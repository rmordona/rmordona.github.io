<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.3 The Significance of Difference  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="6.3 The Significance of Difference  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.3 The Significance of Difference  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="6.2-inferential-statistics.html"/>
<link rel="next" href="6.4-post-hoc-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-appendix.html"><a href="15-appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="15.3-appendix-c.html"><a href="15.3-appendix-c.html"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="the-significance-of-difference" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.3</span> The Significance of Difference <a href="6.3-the-significance-of-difference.html#the-significance-of-difference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, let us <strong>evaluate data</strong> that is <strong>sampled from a population</strong>. In some cases, it is virtually impractical to derive parameters such as averages, variances, and standard deviation from a large population. Therefore, in this circumstance, our approach is to sample the population instead and evaluate if the sampled data is a good representation of the entire population data. The phrase <strong>good representation</strong> can be quantified based on <strong>Statistical Significance</strong>. First, we compare the average (the mean) of the sample data against the average (the mean) of the population data. Then conclude to see if there is any significant difference between the two data sets in terms of a chosen set of statistics such as the mean, variance, and others.</p>
<p>It is important to note that some literature tends to explain <strong>Statistical Significance</strong> in the context of a legal setting in which a defendant is assumed to be innocent until sufficient evidence is gathered to prove the defendant guilty beyond a reasonable doubt. Here, we show how that works using a simple example. We base our discussion on additional references, namely Sternstein M. <span class="citation">(<a href="bibliography.html#ref-ref1338s">1996</a>)</span>, Parkhurst D. F. <span class="citation">(<a href="bibliography.html#ref-ref764a">2006</a>)</span>, De Groot A. D. <span class="citation">(<a href="bibliography.html#ref-ref764a">2006</a>)</span>, Lempert R.O <span class="citation">(<a href="bibliography.html#ref-ref748r">2008</a>)</span>, Sabri F. and Gyateng T. <span class="citation">(<a href="bibliography.html#ref-ref756f">2015</a>)</span>.</p>
<div id="hypothesis" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.1</span> Hypothesis<a href="6.3-the-significance-of-difference.html#hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we review the concept of <strong>Hypothesis</strong>. The term <strong>Hypothesis</strong> is typically associated with other familiar terms such as <strong>supposition</strong>, <strong>proposition</strong>, <strong>assumption</strong>, and <strong>claim</strong>. Here, we declare a statement based on observed data and make a <strong>claim</strong>. For example, we can derive a hypothetical estimate if we cannot possibly know the exact average IQ level of a large population. Suppose we are given a <strong>hypothetical IQ level average</strong> of a large population to be 100. Note that the average IQ level of 100 is just hypothetical, and thus the population mean is unknown. Nonetheless, let us use that to formulate our <strong>Hypothesis</strong>.</p>
<p>There are two types of hypotheses (both need to be stated in our test):</p>
<ul>
<li><strong>Null Hypothesis</strong> - this hypothesis is denoted by <span class="math inline">\(\mathbf{H_0}\)</span>. In this <strong>Hypothesis</strong>, we claim that there is <strong>no significant difference</strong> between the sample and the population ( or between samples from the same population) based on a statistic. For example, we can make a <strong>claim</strong> that the average IQ level of our sample, <span class="math inline">\(\mu\)</span>, is equal to the given <strong>hypothetical IQ level</strong>, <span class="math inline">\(\mu_0 = 100\)</span>. Meaning we claim that there is no difference. </li>
</ul>
<p><span class="math display" id="eq:equate1080001">\[\begin{align}
\mathbf{H_0: \mu = \mu_0}\ \ \ \ \ \ \ \ \ \ where\ \mu_0 = 100 \tag{6.1} 
\end{align}\]</span></p>
<ul>
<li><strong>Alternative Hypothesis</strong> - this hypothesis is denoted by <span class="math inline">\(\mathbf{H_1}\)</span>. In this <strong>hypothesis</strong>, using statistics, we claim a <strong>significant difference</strong> between a sample and a population ( or between samples from the same population). For example, as an alternative hypothesis, we may believe instead that the average IQ level of our sample is greater than the given <strong>hypothetical IQ level</strong> of 100. That means we claim that there is a difference. </li>
</ul>
<p><span class="math display" id="eq:equate1080002">\[\begin{align}
\mathbf{H_1: \mu \ne \mu_0}\ \ \ \ \ \ \ \ \ \ where\ \mu_0 = 100 \tag{6.2} 
\end{align}\]</span></p>
<p>Both null and alternative hypotheses are mutually exclusive, meaning that if one hypothesis is <strong>valid</strong>, then the other must be <strong>invalid</strong>.</p>
<p>Note that <strong>alternative hypothesis</strong> can either be directional or non-directional. For example, if we claim that observation group A is greater than or lesser than observation group B, then this is a <strong>directional alternative hypothesis</strong>. On the other hand, if we compare group A and group B only to know if there is a difference between the two regardless of whether one is better or worse or greater than or lesser than the other, then this is a <strong>non-directional alternative hypothesis</strong>. In the case of our IQ level case above, our <strong>alternative hypothesis</strong> is directional because we claim that our sample mean is greater than the population mean in terms of IQ level.</p>
<p>After formulating our null and alternative hypotheses, the next step is to disprove one of the claims, effectively proving the other.</p>
<p>Our strategy is to <strong>test our claim</strong> on the alternative hypothesis, <span class="math inline">\(\mathbf{H_1}\)</span>, in order to decide on what to do with the null hypothesis based on two outcomes:</p>
<ul>
<li><p>An outcome that <strong>rejects the claim</strong> made on the null hypothesis. It suggests that the sampled data, otherwise our approach, may need more investigation. In this case, the <strong>Alternative Hypothesis</strong> holds.</p></li>
<li><p>An outcome that <strong>fails to reject the claim</strong> made on the null hypothesis. It suggests that the sampled data is a good representation of the population data. Here, if we <strong>fail to reject the claim</strong>, then the <strong>null Hypothesis</strong> holds.</p></li>
</ul>
<p>Also, it helps to be familiar with two types of errors in testing for the <strong>hypothesis</strong>:</p>
<ul>
<li><p><strong>Type I error</strong> - We get this error if our test ends up <strong>rejecting a claim</strong> even though the <strong>null hypothesis</strong> - the claim - is actually <strong>true</strong>. We call this <strong>false positive</strong>. </p></li>
<li><p><strong>Type II error</strong> - We get this error if our test ends up <strong>failing to reject a claim</strong> even though the <strong>null hypothesis</strong> - the claim - is <strong>false</strong>. We call this <strong>false-negative</strong>. </p></li>
</ul>
<p>In the next sections, we discuss methods of testing our hypothetical estimates.</p>
<p>Before we jump to the next sections, however, it may help to introduce three terms that will often describe the relationships of data points:</p>
<ul>
<li><p><strong>Regression</strong> - When we sample a population, we tend to measure our observation based on estimates only. We may not know the exact value of the samples we have observed. Therefore, we can use as many available methods of measurements and perform adjustments if our goal is to get our estimated value as close to the actual value. We rely on <strong>Regression</strong> to see how close our estimated value is to the actual value - if our estimate <strong>regresses to</strong> the actual value. In other words, <strong>Regression</strong> measures the relationship between the dependent and independent variables. Our discussion will primarily focus on coefficients to find any significant effect or the lack thereof.</p></li>
<li><p><strong>Deviance</strong> - In contrast, a population sample with estimates deviating from the true values demonstrates a <strong>deviation</strong>. <strong>Deviation</strong> happens every time our estimate value gets farther away from the true value. Our discussion will also primarily focus on coefficients. </p></li>
<li><p><strong>Variance</strong> - On the other hand, <strong>Variance</strong> is not a measure of whether estimated values regress to or deviate from the true value. Instead, it characterizes the distance between estimates. That means that we are not comparing the estimated and true values. Here, we are comparing two estimate values to see if they are different or not, are related or not. Our discussion will primarily focus on averages to find any significant difference or the lack thereof. </p></li>
</ul>
<p>Note that in the following sections, we will deal with two major types of tests (of estimates):</p>
<ul>
<li>Test to determine the significance of difference - this deals mainly with <strong>Variance</strong>.</li>
<li>Test to determine the significance of Regression - this deals mainly with <strong>Regression</strong> and <strong>Deviation</strong>.</li>
</ul>
<p>Let us now discuss some of the standard tests used in statistics to determine the significance of difference.</p>
</div>
<div id="t-test-true-variance-unknown" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.2</span> T-Test (True Variance unknown) <a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a <strong>T-Test</strong> to test a given hypothesis by computing the <strong>t-value</strong> (or <strong>t-score</strong>) to evaluate our claim <span class="citation">(Ugoni A. and F. Walker B.F. <a href="bibliography.html#ref-ref800a">1995</a>; Abebe T. H. <a href="bibliography.html#ref-ref780t">2020</a>)</span>. This <strong>t-value</strong> is a value of <strong>T statistic</strong> which follows a <strong>Student’s T-distribution</strong> (W.S. Gossett 1876-1937).</p>
<p>Let us consider three types of samples for our <strong>T-Test</strong>:</p>
<p><strong>One-Sample T-Test</strong>:</p>
<p>A <strong>One-Sample T-Test</strong> uses a sample from a population to determine the significance of the sample mean against a given <strong>known</strong> mean, but with an <strong>unknown</strong> variance. Here, we perform the following test statistic equation against the sample:</p>
<p><span class="math display" id="eq:equate1080003">\[\begin{align}
t = \frac{observed-expected}{standard\ error}  = \frac{\mu_s - \mu_0} { \sigma_s / \sqrt{n_s}} \tag{6.3} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{t}\)</span> is the test statistic, following a T distribution.</li>
<li><span class="math inline">\(\mathbf{\mu_s}\)</span> is the sample mean - the average (mean) of the sample, <span class="math inline">\(\bar{x}_s\)</span>.</li>
<li><span class="math inline">\(\mathbf{\sigma}_s\)</span> is the sample standard deviation - the standard deviation of the sample.</li>
<li><span class="math inline">\(\mathbf{n_s}\)</span> is the size of the sample data.</li>
<li><span class="math inline">\(\mathbf{\mu_0}\)</span> is a given <strong>hypothesized</strong> population mean. (Note that <span class="math inline">\(\mathbf{\sigma}_p\)</span> - the population standard deviation - is unknown). See <strong>Conjugacy</strong> section in <strong>Bayesian Computation</strong> for <strong>unknown variance</strong>.</li>
</ul>
<p>The sample standard deviation (along with its corresponding sample variance) is expressed as:</p>
<p><span class="math display" id="eq:equate1080004">\[\begin{align}
\sigma_s = \sqrt{\frac{1}{n-1}\left(\sum_{i=1}^n (x_i - \bar{x})^2\right)}
\ \ \ \ \ \ \ \ \ \ \ where\ \ \sigma_s^2\ \leftarrow\ sample\ variance \tag{6.4} 
\end{align}\]</span></p>
<p>The standard error is expressed as:</p>
<p><span class="math display" id="eq:equate1080005">\[\begin{align}
SE = \frac{\sigma_{s}}{\sqrt{n}} \tag{6.5} 
\end{align}\]</span></p>
<p>The margin of error is expressed as:</p>
<p><span class="math display" id="eq:equate1080006">\[\begin{align}
me = t \times SE \tag{6.6} 
\end{align}\]</span></p>
<p>The confidence interval is expressed as:</p>
<p><span class="math display" id="eq:equate1080007">\[\begin{align}
C.I. = \mu_{s} \pm me \tag{6.7} 
\end{align}\]</span></p>
<p>To illustrate our example of IQ levels once again, suppose we take a sample of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Assume an average IQ level of 100.</p>
<p>Here is what the <strong>t-value</strong> looks like in R-code:</p>

<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb303-2" data-line-number="2">statistic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb303-3" data-line-number="3">    s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb303-4" data-line-number="4">    m =<span class="st"> </span><span class="kw">mean</span>(x)</a>
<a class="sourceLine" id="cb303-5" data-line-number="5">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb303-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) { s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>(x[i] <span class="op">-</span><span class="st"> </span>m)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb303-7" data-line-number="7">    sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span><span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>s)</a>
<a class="sourceLine" id="cb303-8" data-line-number="8">    <span class="kw">list</span>(<span class="st">&quot;mean&quot;</span>=m, <span class="st">&quot;sd&quot;</span>=sd, <span class="st">&quot;sum_squared&quot;</span>=s, <span class="st">&quot;n&quot;</span>=n )</a>
<a class="sourceLine" id="cb303-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb303-10" data-line-number="10">t_test &lt;-<span class="st"> </span><span class="cf">function</span>(x1, <span class="dt">x2 =</span> <span class="ot">NULL</span>, mu, <span class="dt">paired=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb303-11" data-line-number="11">    <span class="cf">if</span> (<span class="kw">is.null</span>(x2)) {  <span class="co"># one sample t-test</span></a>
<a class="sourceLine" id="cb303-12" data-line-number="12">        stat =<span class="st"> </span><span class="kw">statistic</span>(x1)</a>
<a class="sourceLine" id="cb303-13" data-line-number="13">        t =<span class="st"> </span>(stat<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(stat<span class="op">$</span>sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(stat<span class="op">$</span>n))</a>
<a class="sourceLine" id="cb303-14" data-line-number="14">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb303-15" data-line-number="15">    <span class="cf">if</span> (paired <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) { <span class="co"># two sample t-test</span></a>
<a class="sourceLine" id="cb303-16" data-line-number="16">        stat1 =<span class="st"> </span><span class="kw">statistic</span>(x1)</a>
<a class="sourceLine" id="cb303-17" data-line-number="17">        stat2 =<span class="st"> </span><span class="kw">statistic</span>(x2)</a>
<a class="sourceLine" id="cb303-18" data-line-number="18">        pooled_variance =<span class="st"> </span></a>
<a class="sourceLine" id="cb303-19" data-line-number="19"><span class="st">            </span>( stat1<span class="op">$</span>sum_squared <span class="op">+</span><span class="st"> </span>stat2<span class="op">$</span>sum_squared ) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb303-20" data-line-number="20"><span class="st">            </span>( stat1<span class="op">$</span>n <span class="op">+</span><span class="st"> </span>stat2<span class="op">$</span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb303-21" data-line-number="21">        t =<span class="st"> </span>(stat1<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>stat2<span class="op">$</span>mean) <span class="op">/</span></a>
<a class="sourceLine" id="cb303-22" data-line-number="22"><span class="st">            </span>(<span class="kw">sqrt</span>( pooled_variance <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span><span class="op">/</span>stat1<span class="op">$</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>stat2<span class="op">$</span>n)))</a>
<a class="sourceLine" id="cb303-23" data-line-number="23">    } <span class="cf">else</span> { <span class="co"># paired t-test</span></a>
<a class="sourceLine" id="cb303-24" data-line-number="24">        stat =<span class="st"> </span><span class="kw">statistic</span>(x1 <span class="op">-</span><span class="st"> </span>x2) <span class="co"># get the difference</span></a>
<a class="sourceLine" id="cb303-25" data-line-number="25">        t =<span class="st"> </span>(stat<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(stat<span class="op">$</span>sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(stat<span class="op">$</span>n))</a>
<a class="sourceLine" id="cb303-26" data-line-number="26">    }</a>
<a class="sourceLine" id="cb303-27" data-line-number="27">    <span class="kw">list</span>(<span class="st">&quot;statistic&quot;</span>=t)</a>
<a class="sourceLine" id="cb303-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb303-29" data-line-number="29"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb303-30" data-line-number="30">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb303-31" data-line-number="31">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb303-32" data-line-number="32"><span class="co"># we take a sample of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb303-33" data-line-number="33">x =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb303-34" data-line-number="34"><span class="co"># we use a hypothesized average of 100</span></a>
<a class="sourceLine" id="cb303-35" data-line-number="35">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x, <span class="dt">mu=</span><span class="dv">100</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] 3.535</code></pre>

<p>To validate if we get the same result, we use the built-in R function called <strong>“t.test()”</strong> from <strong>stats</strong> package:</p>

<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(x, <span class="dt">y =</span> <span class="ot">NULL</span>, <span class="dt">mu =</span> <span class="dv">100</span>, </a>
<a class="sourceLine" id="cb305-2" data-line-number="2">                 <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] 3.535</code></pre>

<p><strong>Two-Sample T-Test</strong>:</p>
<p>A <strong>Two-Sample T-Test</strong> uses two samples from a population and performs the following test statistic equation against the two samples:</p>
<p><span class="math display" id="eq:equate1080008">\[\begin{align}
t = \frac{\bar{x}_1 - \bar{x}_2 } { \sqrt{\sigma_{ss}^2\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}} \tag{6.8} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{t}\)</span> is the test statistic.</li>
<li><span class="math inline">\(\bar{x}_2, \bar{x}_2\)</span> are two sample means</li>
<li><span class="math inline">\(\sigma_{ss}^2\)</span> is the pooled sample variance.</li>
<li><span class="math inline">\(n_1\ and\ n_2\)</span> are the sizes of the two samples respectively.</li>
</ul>
<p>The <strong>pooled sample variance</strong> is a merge of the individual variance computed as: </p>
<p><span class="math display" id="eq:equate1080009">\[\begin{align}
\sigma_{ss}^2 = \frac{
  \sum_{i=0}^{n_1} (x_i - \bar{x}_1)^2 + \sum_{j=0}^{n_2} (x_j - \bar{x}_2)^2
}{n_1 + n_2 - 2}
\ \ \ \ \ \ \ \ \ \ \ where\ \ \sigma_{ss}^2\ \leftarrow\ pooled\ variance \tag{6.9} 
\end{align}\]</span></p>
<p>To illustrate, suppose we take two separate samples of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Here, we do not have a given “<strong>hypothesized</strong>” population mean, <span class="math inline">\(\mathbf{\mu}\)</span>, because we are comparing the mean of two separate samples. Here is what the <strong>t-value</strong> looks like:</p>

<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb307-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb307-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb307-4" data-line-number="4"><span class="co"># assume a population (though we use the sample)</span></a>
<a class="sourceLine" id="cb307-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb307-6" data-line-number="6"><span class="co"># we take two samples of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb307-7" data-line-number="7">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb307-8" data-line-number="8">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb307-9" data-line-number="9">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x1, <span class="dt">x2=</span>x2)<span class="op">$</span>statistic)</a></code></pre></div>
<pre><code>## [1] 0.441</code></pre>

<p>Let us use <strong>“t.test()”</strong> function to validate:</p>

<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, </a>
<a class="sourceLine" id="cb309-2" data-line-number="2">                 <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] 0.441</code></pre>

<p><strong>Paired T-Test</strong>:</p>
<p>A <strong>Paired T-Test</strong> uses two samples from a population. Here, the two samples are taken from the same population. However, instead of performing a <strong>Two-Sample T-Test</strong>, we perform a <strong>One-Sample T-Test</strong>. To do that, we generate new sample data using the difference between the two samples.</p>
<p>For example, let us generate two samples of 25 individuals from a population and get the difference. Assume an average IQ level of 100.</p>

<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb311-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb311-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb311-4" data-line-number="4"><span class="co"># hypothesized population</span></a>
<a class="sourceLine" id="cb311-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb311-6" data-line-number="6"><span class="co"># we take two samples of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb311-7" data-line-number="7">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb311-8" data-line-number="8">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb311-9" data-line-number="9"><span class="co"># we use a hypothesized average of 100</span></a>
<a class="sourceLine" id="cb311-10" data-line-number="10">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x1, <span class="dt">x2=</span>x2, <span class="dt">mu=</span><span class="dv">100</span>, <span class="dt">paired=</span><span class="ot">TRUE</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>That is equivalent to:</p>

<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" data-line-number="1">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>(x1 <span class="op">-</span><span class="st"> </span>x2), <span class="dt">mu=</span><span class="dv">100</span>, <span class="dt">paired=</span><span class="ot">FALSE</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>and using <strong>“t.test()”</strong> to validate, we get:</p>

<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(<span class="dt">x =</span> x1,<span class="dt">y =</span> x2, <span class="dt">mu=</span><span class="dv">100</span>,  </a>
<a class="sourceLine" id="cb315-2" data-line-number="2">                        <span class="dt">paired=</span><span class="ot">TRUE</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>So far, all we did in running the three types of T-tests is to get the <strong>t-value</strong>. The next step is to use the <strong>t-value</strong> to evaluate our <strong>Hypothesis</strong>. </p>
<p>However, let us first take a careful look at Figure <a href="6.3-the-significance-of-difference.html#fig:twotailtest">6.7</a> to visualize a two-tail test and Figure <a href="6.3-the-significance-of-difference.html#fig:onetailtest">6.8</a> to visualize a one-tail test.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twotailtest"></span>
<img src="DS_files/figure-html/twotailtest-1.png" alt="Critical Value vs Significance Level - Two-Tail Test" width="70%" />
<p class="caption">
Figure 6.7: Critical Value vs Significance Level - Two-Tail Test
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onetailtest"></span>
<img src="DS_files/figure-html/onetailtest-1.png" alt="Critical Value vs Significance Level - One-Tail Test" width="70%" />
<p class="caption">
Figure 6.8: Critical Value vs Significance Level - One-Tail Test
</p>
</div>

<p>In the figures, we introduce five concepts.</p>
<p><strong>First</strong>, we introduce the concept of <strong>Confidence level</strong>. There are three commonly used levels of confidence: 99%, 95%, and 90%. For example, a confidence level of 90% means that we are 90% confident that repeated sampling of a population renders the same outcome. </p>
<p><strong>Second</strong>, we introduce the concept of <strong>Significance level</strong> which may complement <strong>confidence level</strong> only in so far as, for example, if the <strong>confidence level</strong> is 99%, then the <strong>significance level</strong> is 1% - or if the confidence level is 90%, then the significance level is 10%. In other words, there are also three commonly used levels of significance: 1%, 5%, 10%. <strong>Significance level</strong> is also denoted as the <strong>alpha</strong> - <span class="math inline">\(\mathbf{\alpha}\)</span> - which corresponds to 0.01, 0.05, 0.10. </p>
<p>For example, an alpha value - or significant level - of 0.10 means a 10% probability that the observed result is at least as extreme as the computed test statistic - e.g., the <strong>t-value</strong> - when the <strong>null hypothesis</strong> is true.</p>
<p>Except just perhaps being a common practice, there is no rule to prevent us from using our own <strong>confidence level</strong> and <strong>significance level</strong> depending on our domain or area of expertise. We may, however, prefer a more stringent level. However, for illustration, let us continue to keep those three levels in our discussions.</p>
<p><strong>Third</strong>, we introduce the concept of a <strong>Critical value</strong>. A <strong>Critical value</strong> can be computed based on the <strong>Significance level</strong>. For example, a significance level of 0.01, 0.05, or 0.10 has the following corresponding computed critical value for a one-tail test: </p>

<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" data-line-number="1">alpha=<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb317-2" data-line-number="2"><span class="co"># compute for quartile</span></a>
<a class="sourceLine" id="cb317-3" data-line-number="3">lt =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(alpha<span class="op">/</span><span class="dv">2</span>, <span class="dt">df=</span><span class="ot">Inf</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb317-4" data-line-number="4"><span class="co"># reverse alpha then compute for quartile</span></a>
<a class="sourceLine" id="cb317-5" data-line-number="5">ut =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">rev</span>(alpha)<span class="op">/</span><span class="dv">2</span>, <span class="dt">df=</span><span class="ot">Inf</span>),<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb317-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;lower_tail&quot;</span>=<span class="st"> </span>lt, <span class="st">&quot;upper_tail&quot;</span>=ut)</a></code></pre></div>
<pre><code>## $lower_tail
## [1] -2.58 -1.96 -1.64
## 
## $upper_tail
## [1] 1.64 1.96 2.58</code></pre>

<p>A significance level of 0.01, 0.05, and 0.10 respectively for a two-tail test have the following computed critical values:</p>

<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1">alpha=<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb319-2" data-line-number="2"><span class="co"># compute for quartile</span></a>
<a class="sourceLine" id="cb319-3" data-line-number="3">lt =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(alpha, <span class="dt">df=</span><span class="ot">Inf</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb319-4" data-line-number="4"><span class="co"># reverse alpha then compute for quartile</span></a>
<a class="sourceLine" id="cb319-5" data-line-number="5">ut =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">rev</span>(alpha), <span class="dt">df=</span><span class="ot">Inf</span>),<span class="dv">2</span>) </a>
<a class="sourceLine" id="cb319-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;lower_tail&quot;</span>=<span class="st"> </span>lt, <span class="st">&quot;upper_tail&quot;</span>=ut)</a></code></pre></div>
<pre><code>## $lower_tail
## [1] -2.33 -1.64 -1.28
## 
## $upper_tail
## [1] 1.28 1.64 2.33</code></pre>

<p><strong>Fourth</strong>, we introduce the concept of <strong>p-value</strong>. Let us recall the discussion around <strong>CDF</strong> - cumulative density function. Here, the <strong>p-value</strong> uses the <strong>CDF</strong> of a <strong>T-distribution</strong>. </p>
<p>As an example, let us compute for the <strong>CDF</strong> using the <span class="math inline">\(\mathbf{H_1}\)</span> - our <strong>alternative hypothesis</strong>:</p>
<p><span class="math display">\[
P(\mu &gt; 100 | \mu_0 = 100)
\]</span></p>
<p>We interpret that as the probability that the sample mean is greater than 100, given a true value (true mean) equal to 100.</p>
<p>Let us use our T-distribution <strong>CDF</strong> function <strong>t_cdf(.)</strong> to compute for the <strong>p-value</strong> using the generated <strong>t-value</strong> (note that we sampled 25 individuals, n=25):</p>

<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb321-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb321-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb321-4" data-line-number="4">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">5000</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb321-5" data-line-number="5"><span class="co"># we take a sample of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb321-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb321-7" data-line-number="7">tvalue =<span class="st"> </span><span class="kw">t_test</span>(<span class="dt">x1=</span>x, <span class="dt">mu=</span><span class="dv">100</span>)<span class="op">$</span>statistic </a>
<a class="sourceLine" id="cb321-8" data-line-number="8">cdf =<span class="st"> </span><span class="kw">t_cdf</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>)</a>
<a class="sourceLine" id="cb321-9" data-line-number="9">pvalue_left =<span class="st"> </span>cdf</a>
<a class="sourceLine" id="cb321-10" data-line-number="10">pvalue_right =<span class="st">  </span>pvalue =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>cdf</a>
<a class="sourceLine" id="cb321-11" data-line-number="11">tvalue  <span class="co"># T value</span></a></code></pre></div>
<pre><code>## [1] 4.519</code></pre>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" data-line-number="1">pvalue_left <span class="co"># P value (area) towards left of T value</span></a></code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" data-line-number="1">pvalue_right <span class="co"># P value (area) towards right of T value</span></a></code></pre></div>
<pre><code>## [1] 0.00007054</code></pre>

<p>To validate, let us use the built-in R functions <strong>t.test()</strong> and <strong>pt()</strong> to compute for the <strong>p-value</strong>:</p>

<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">tvalue =<span class="st"> </span><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]]</a>
<a class="sourceLine" id="cb327-2" data-line-number="2">pvalue_left =<span class="st"> </span><span class="kw">pt</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb327-3" data-line-number="3">pvalue_right =<span class="st"> </span><span class="kw">pt</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb327-4" data-line-number="4">tvalue  <span class="co"># T value</span></a></code></pre></div>
<pre><code>## [1] 4.519</code></pre>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1">pvalue_left <span class="co"># P value (area) towards left of T value</span></a></code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1">pvalue_right <span class="co"># P value (area) towards right of T value</span></a></code></pre></div>
<pre><code>## [1] 0.00007054</code></pre>

<p>As we can see from the output, we have both <strong>t-value</strong> and <strong>p-value</strong>.</p>
<p><strong>Fifth and last</strong>, we re-introduce the directional and non-directional alternative hypothesis, which we evaluate based on whether we are performing a two-tail or one-tail test. A one-tail test corresponds to evaluating a one-directional hypothesis that tends to compare whether an observation is better or worse, greater or lesser than another observation. A two-tail test evaluates a non-directional alternative that tends to determine if there is a difference between the two observations regardless of direction.</p>
<p>With all those five introductory concepts and to now evaluate our <strong>null hypothesis</strong>, we can use two methods:</p>
<ul>
<li>Use a <strong>t-value</strong> based on a chosen <strong>critical value</strong> as our comfortable threshold (establishing our rejection region).
<ul>
<li>For a two-tail test, a common choice for <strong>critical value</strong> is based on: <span class="math inline">\(\pm 2.58, \pm 1.96, \pm 1.65\)</span>.<br />
</li>
<li>For a one-tail test, we use any of the following: <span class="math inline">\(\pm 2.33, \pm 1.64, \pm 1.28\)</span>.</li>
</ul></li>
<li>Use a <strong>p-value</strong> based on a chosen <strong>significance level</strong> as our comfortable threshold (establishing the area - the alpha - under the curve as our rejection region).
<ul>
<li>For a two-tail test, we can choose any of the given <strong>significance level</strong>: <span class="math inline">\(0.005, 0.025, 0.05\)</span>.<br />
</li>
<li>For a one-tail test, we use any of the following: <span class="math inline">\(0.01, 0.05, 0.10\)</span>.</li>
</ul></li>
</ul>
<p>To illustrate, let us continue to use our <strong>IQ level</strong> samples. Suppose that the average IQ level is 100. Now, let us make a claim, <span class="math inline">\(\mathbf{H_1}\)</span>, that our sample mean is greater than the average IQ level, <span class="math inline">\(\mu\)</span>. Here is what it looks like:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \mu= 100,\ \ \ \ \ \leftarrow \ \ \ \text{null hypothesis}\\
H_1 &amp;: \mu &gt; 100
\end{align*}\]</span></p>
<p>Our computed <strong>t-value</strong> is 4.5191 and our computed <strong>p-value</strong> is 7.054310^{-5}.</p>
<p>If our confidence level is 99% and we are performing a one-tail test, and our alternative hypothesis tends toward the right because we are claiming an IQ level greater than 100, then we have the following:</p>
<p>4.5191 &lt; 2.58</p>
<p>This means that our <strong>t-value</strong> is not in the extreme right-side rejection region (See Figure <a href="6.3-the-significance-of-difference.html#fig:onetailtest">6.8</a>). The extreme right-side rejection region is the region in which we reject our <strong>null hypothesis</strong>. Moreover, because our <strong>t-value</strong> is not anywhere in that region, we can conclude that we <strong>fail to reject our claim</strong> that there is no difference given the <strong>null hypothesis</strong> is true. Our alternative hypothesis holds that the IQ level in the sample is greater than 100.</p>
<p>On the other hand, we also see that our <strong>p-value</strong> has the following:</p>
<p>7.054310^{-5} &gt; 0.01</p>
<p>Because our <strong>p-value</strong> is greater than the <strong>significance level</strong> of 0.01, we get the same conclusion - that is, <strong>failing to reject our claim</strong> that there is no difference given the <strong>null hypothesis</strong> is true.</p>
<p>Our three confidence levels for a right-side one-tail fail to reject our claim, given our <strong>null hypothesis</strong> is true. See Table <a href="6.3-the-significance-of-difference.html#tab:significancelevel">6.4</a>.</p>

<table>
<caption><span id="tab:significancelevel">Table 6.4: </span>Significance Level</caption>
<thead>
<tr class="header">
<th align="left">Conf Level</th>
<th align="left">T Value</th>
<th align="left">P Value</th>
<th align="left">Significance</th>
<th align="left">Critical</th>
<th align="left">Direction</th>
<th align="left">Analysis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">99%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.01</td>
<td align="left">t &lt; 2.58</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
<tr class="even">
<td align="left">95%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.05</td>
<td align="left">t &lt; 1.96</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
<tr class="odd">
<td align="left">90%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.10</td>
<td align="left">t &lt; 1.65</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
</tbody>
</table>

<p>Using the built-in R function <strong>t.test()</strong>, we can implement the same IQ level case in many ways with a summary:</p>

<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, tends toward left</span></a>
<a class="sourceLine" id="cb333-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt; U0)</span></a>
<a class="sourceLine" id="cb333-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 1
## alternative hypothesis: true mean is less than 100
## 99 percent confidence interval:
##   -Inf 123.5
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, tends toward right</span></a>
<a class="sourceLine" id="cb335-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &gt; U0)</span></a>
<a class="sourceLine" id="cb335-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 7e-05
## alternative hypothesis: true mean is greater than 100
## 99 percent confidence interval:
##  106.8   Inf
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, two-sided</span></a>
<a class="sourceLine" id="cb337-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb337-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 0.0001
## alternative hypothesis: true mean is not equal to 100
## 99 percent confidence interval:
##  105.8 124.5
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="co"># Two Sample T-Test, tends toward the right</span></a>
<a class="sourceLine" id="cb339-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &gt; U0)</span></a>
<a class="sourceLine" id="cb339-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.3
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -6.168    Inf
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1"><span class="co"># Two Sample T-Test with pooled variances  </span></a>
<a class="sourceLine" id="cb341-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb341-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2,  <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.7
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -7.831 12.231
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1"><span class="co"># Two Sample T-Test, two-sided, with pooled variances</span></a>
<a class="sourceLine" id="cb343-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb343-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.7
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -7.831 12.231
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>

<p>A Student’s T table is provided (See Table <a href="15.3-appendix-c.html#tab:ttable">15.4</a>) as a reference in the Appendix. Given degrees of freedom and significance level, one can cross-reference both parameters to arrive at a <strong>t-value</strong>.</p>
<p>For example:</p>

<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">alpha =<span class="st"> </span><span class="fl">0.01</span></a>
<a class="sourceLine" id="cb345-2" data-line-number="2">n =<span class="st"> </span><span class="dv">5</span>  <span class="co"># df = n - 1</span></a>
<a class="sourceLine" id="cb345-3" data-line-number="3"><span class="kw">qt</span>(<span class="dt">p=</span>alpha, <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)  <span class="co"># left-side one-tail crit value</span></a></code></pre></div>
<pre><code>## [1] -3.747</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1"><span class="co"># We can use: 1 - qt(p = alpha, df=n-1, lower.tail=TRUE)</span></a>
<a class="sourceLine" id="cb347-2" data-line-number="2"><span class="co"># or we can use the following:</span></a>
<a class="sourceLine" id="cb347-3" data-line-number="3"><span class="kw">qt</span>(<span class="dt">p=</span>alpha, <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)  <span class="co"># right-side one-tail crit value</span></a></code></pre></div>
<pre><code>## [1] 3.747</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1"><span class="co"># As for two-tail, we split into two areas:  0.01/2 = 0.005</span></a>
<a class="sourceLine" id="cb349-2" data-line-number="2">two_tail =<span class="st"> </span><span class="kw">qt</span>(<span class="dt">p =</span> alpha <span class="op">/</span><span class="st"> </span><span class="dv">2</span> , <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb349-3" data-line-number="3"><span class="kw">c</span>(<span class="op">-</span>two_tail, two_tail)  <span class="co"># left and right critical values</span></a></code></pre></div>
<pre><code>## [1] -4.604  4.604</code></pre>

<p>For samples based on multivariate normal distribution, we leave readers to investigate Hotelling’s T^2 statistic.</p>
</div>
<div id="z-test-true-variance-known" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.3</span> Z-Test (True Variance known)<a href="6.3-the-significance-of-difference.html#z-test-true-variance-known" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a <strong>Z-Test</strong> to test a given hypothesis by computing for the <strong>z-score</strong> (or <strong>standard score</strong>) to evaluate our claim. In <strong>Z-Test</strong>, we assume that our <strong>z-score</strong> follows a normal distribution and that the population mean and variance are known. Our <strong>z-score</strong> statistic is therefore expressed as:</p>
<p><span class="math display" id="eq:equate1080010">\[\begin{align}
z = \frac{observed-expected}{standard\ error} = \frac{ (x - \mu_p)}{\sigma_p } \tag{6.10} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{z}\)</span> is the test statistic, following a normal distribution,</li>
<li><span class="math inline">\(\mathbf{\mu_p}\)</span> is a given <strong>hypothesized</strong> population mean, <span class="math inline">\(\mu_p\)</span> = <span class="math inline">\(\mu_0\)</span>,</li>
<li><span class="math inline">\(\mathbf{\sigma_p}\)</span> is a given <strong>hypothesized</strong> population standard deviation.</li>
</ul>
<p>Here, our statistic is based on an individual observation, <span class="math inline">\(\mathbf{x}\)</span>. Equivalently, we can also use the following equation for a group of observations:</p>
<p><span class="math display" id="eq:equate1080011">\[\begin{align}
z = \frac{ (\mu_s - \mu_p)}{ SE(\mu_s )} \tag{6.11} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{z}\)</span> is the test statistic, following a normal distribution,</li>
<li><span class="math inline">\(\mathbf{\bar{u}_s}\)</span> is the sample mean, <span class="math inline">\(\bar{x} = \mu_s\)</span>,</li>
<li><span class="math inline">\(\mathbf{\mu_p}\)</span> is a given <strong>hypothesized</strong> population mean, <span class="math inline">\(\mu_p\)</span> = <span class="math inline">\(\mu_0\)</span>.</li>
</ul>
<p>This alternative equation is based on the idea of the <strong>Central Limit Theorem</strong> which states that as the sample size increases (see Figure <a href="5.9-distributiontypes.html#fig:tdist">5.30</a>), it comes to a point where the sample mean follows a normal distribution <span class="citation">(Kwak S. G., Kim J. H. <a href="bibliography.html#ref-ref790s">2016</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>. That means that the sampling distribution starts to follow the population distribution closely. The difference is that the <strong>standard deviation</strong> of the sample is computed based on the <strong>standard error</strong> equation below:</p>
<p><span class="math display" id="eq:equate1080012">\[\begin{align}
SE(\bar{x}) = \frac{\sigma_p^2}{n_s} = \frac{\sigma_p}{\sqrt{n_s}} \tag{6.12} 
\end{align}\]</span></p>
<p>Figure <a href="6.3-the-significance-of-difference.html#fig:stdnormaldist">6.9</a> shows a standard normal distribution. The <strong>x-axis</strong> represents the standard deviation which is where the <strong>z-score</strong> is measured - it is a measure of the distance between our standard deviations and the mean, meaning how many standard deviations away we are from the mean. For example, if the <strong>z-score</strong> is positive two, then we are two standard deviations away from the right to the mean, <span class="math inline">\(\bar{x}_s = \mu_s = 0\)</span>. And if our <strong>z-score</strong> is negative two, then we are two standard deviations away from the left to the mean, <span class="math inline">\(\bar{x}_s = \mu_s = 0\)</span>. Therefore the unit of measurement for a <strong>z-score</strong> is in standard deviations.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stdnormaldist"></span>
<img src="DS_files/figure-html/stdnormaldist-1.png" alt="Standard Normal Distribution" width="70%" />
<p class="caption">
Figure 6.9: Standard Normal Distribution
</p>
</div>

<p>To illustrate, suppose we have a list of 40 observations sampled from a population. We can derive the <strong>z-score</strong> for each data in the sample as such:</p>

<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb351-2" data-line-number="2">z_score &lt;-<span class="st"> </span><span class="cf">function</span>(x, mu, sd) {</a>
<a class="sourceLine" id="cb351-3" data-line-number="3">  (x <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>sd</a>
<a class="sourceLine" id="cb351-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb351-5" data-line-number="5"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb351-6" data-line-number="6">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb351-7" data-line-number="7"><span class="co"># hypothesized mean and sd</span></a>
<a class="sourceLine" id="cb351-8" data-line-number="8">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb351-9" data-line-number="9">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">1000</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb351-10" data-line-number="10"><span class="co"># we take a sample of 40 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb351-11" data-line-number="11"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb351-12" data-line-number="12">( <span class="dt">sample_data =</span> <span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">40</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) )[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] 116 116 120 119  86 124 121 130 131 121</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1"><span class="co"># The equivalent z-scores for each element in sample x.</span></a>
<a class="sourceLine" id="cb353-2" data-line-number="2"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb353-3" data-line-number="3">(<span class="dt">zscore =</span> <span class="kw">round</span>( <span class="kw">z_score</span>(<span class="dt">x=</span>sample_data, <span class="dt">mu=</span>mu, <span class="dt">sd =</span> sd), <span class="dv">2</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1]  0.96  0.96  1.20  1.14 -0.84  1.43  1.26  1.79  1.85  1.26</code></pre>

<p>Similarly, we can derive back the raw data given a <strong>z-score</strong> using the following formula:</p>
<p><span class="math display" id="eq:equate1080013">\[\begin{align}
x = z  \sigma + \mu \tag{6.13} 
\end{align}\]</span></p>
<p>For example, to validate the <strong>z-score</strong> we derived, we use the formula above to compare it with our sample data.</p>

<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb355-2" data-line-number="2">( <span class="kw">round</span>( zscore <span class="op">*</span><span class="st"> </span>sd <span class="op">+</span><span class="st"> </span>mu, <span class="dv">0</span>) <span class="op">==</span><span class="st"> </span>sample_data )[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE</code></pre>

<p>Now for a better illustration, let us again use the IQ level case where we continue to assume an IQ level average of 100 and a standard deviation of 16.73. Let us determine the proportion of people with IQ levels greater than 90, lesser than 110, and between 90 and 110.</p>
<p><strong>First</strong>, let us get the z-scores.</p>

<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" data-line-number="1">x =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;min&quot;</span>=<span class="dv">90</span>, <span class="st">&quot;max&quot;</span>=<span class="dv">110</span>)</a>
<a class="sourceLine" id="cb357-2" data-line-number="2">(<span class="dt">zscore =</span> <span class="kw">round</span>( <span class="kw">z_score</span>(<span class="dt">x=</span> <span class="kw">c</span>(x<span class="op">$</span>min, x<span class="op">$</span>max), <span class="dt">mu=</span>mu, <span class="dt">sd =</span> sd), <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] -0.6  0.6</code></pre>

<p><strong>Second</strong>, let us use the build-in R function <strong>pnorm</strong> to get the proportions (or percentage) of the population having an IQ level greater than 90.</p>
<p><span class="math display">\[
P(z &gt; 90) 
\]</span></p>

<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb359-2" data-line-number="2"><span class="co"># Get proportions using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb359-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb359-4" data-line-number="4">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">1</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb359-5" data-line-number="5"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.57%&quot;</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" data-line-number="1"><span class="co"># Get proportions using normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb361-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb361-3" data-line-number="3">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">90</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb361-4" data-line-number="4"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.5%&quot;</code></pre>

<p><strong>Third</strong>, let us get the proportion (or percentage) of population having an IQ level lesser than 110.</p>
<p><span class="math display">\[
P(z &lt; 110)
\]</span></p>

<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb363-2" data-line-number="2"><span class="co"># Get proportions using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb363-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb363-4" data-line-number="4"><span class="co"># One way to do it</span></a>
<a class="sourceLine" id="cb363-5" data-line-number="5">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb363-6" data-line-number="6"><span class="co"># Another way to do it</span></a>
<a class="sourceLine" id="cb363-7" data-line-number="7">proportion =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb363-8" data-line-number="8"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.57%&quot;</code></pre>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" data-line-number="1"><span class="co"># Get proportions using normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb365-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb365-3" data-line-number="3"><span class="co"># One way to do it</span></a>
<a class="sourceLine" id="cb365-4" data-line-number="4">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb365-5" data-line-number="5"><span class="co"># Another way to do it</span></a>
<a class="sourceLine" id="cb365-6" data-line-number="6">proportion =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb365-7" data-line-number="7"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.5%&quot;</code></pre>

<p><strong>Lastly</strong>, let us get the proportion (or percentage) of population having IQ levels between 90 and 110.</p>
<p><span class="math display">\[
P( 90 &lt; z &lt;  110)
\]</span></p>

<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb367-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb367-2" data-line-number="2"><span class="co"># Get proportion using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb367-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb367-4" data-line-number="4">proportion1 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb367-5" data-line-number="5">proportion2 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">1</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb367-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion1 <span class="op">-</span><span class="st"> </span>proportion2)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , </a>
<a class="sourceLine" id="cb367-7" data-line-number="7">                           <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;45.15%&quot;</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" data-line-number="1"><span class="co"># Get proportion using = normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb369-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb369-3" data-line-number="3">proportion1 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb369-4" data-line-number="4">proportion2 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">90</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb369-5" data-line-number="5"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion1 <span class="op">-</span><span class="st"> </span>proportion2)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , </a>
<a class="sourceLine" id="cb369-6" data-line-number="6">                           <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;45%&quot;</code></pre>

<p>Consequently, we can also derive the z-score and raw data using the built-in R function <strong>qnorm()</strong> given the probability (proportion). For example:</p>

<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" data-line-number="1"><span class="co"># Get Z-Score using standard normal distribution</span></a>
<a class="sourceLine" id="cb371-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb371-3" data-line-number="3"><span class="kw">round</span>( <span class="kw">qnorm</span>(<span class="dt">p =</span> proportion2, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>), <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] -0.6</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb373-1" data-line-number="1"><span class="kw">round</span>( <span class="kw">qnorm</span>(<span class="dt">p =</span> proportion1, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>), <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.6</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" data-line-number="1"><span class="co"># Get Raw Data using  normal distribution</span></a>
<a class="sourceLine" id="cb375-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb375-3" data-line-number="3"><span class="kw">qnorm</span>(<span class="dt">p =</span> proportion2, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 90</code></pre>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" data-line-number="1"><span class="kw">qnorm</span>(<span class="dt">p =</span> proportion1, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 110</code></pre>

<p>There are two Z-tables for Z-Score that can be found in the Appendix. One table ranges from -3.9 to 0, and another table ranges from 0 to 3.9. The cross-section represents the probability. See Table <a href="15.3-appendix-c.html#tab:ztable1">15.5</a> and Table <a href="15.3-appendix-c.html#tab:ztable2">15.6</a> in the Appendix.</p>
<p>So far, we have not discussed <strong>null hypothesis and alternative hypothesis</strong> in this section. Testing a hypothesis using Z-test also applies given a computation of <strong>z-score</strong> and computation of proportions. The <strong>Critical Values</strong> and <strong>Significance Level</strong> also apply. The <strong>z-score</strong> (similar to t-value) is compared against the chosen <strong>Critical Value</strong> and the <strong>proportion</strong> (similar to p-value) is compared against the chosen <strong>significance Level</strong>. From there, one can conclude whether to reject the claim, the <strong>null hypothesis</strong> - <span class="math inline">\(\mathbf{H_0}\)</span>, stating that there is no difference between observations and population; otherwise, to indicate a failure to reject the claim, <span class="math inline">\(\mathbf{H_0}\)</span>.</p>
<p>Similar to <strong>T-test</strong>, we also can compute for margin of error, which is expressed as:</p>
<p><span class="math display" id="eq:equate1080014">\[\begin{align}
me = Z \times SE,\ \ \ \ \ where\ SE = \frac{\sigma_p}{\sqrt{n}} \tag{6.14} 
\end{align}\]</span></p>
<p>The confidence interval is expressed as:</p>
<p><span class="math display" id="eq:equate1080015">\[\begin{align}
C.I. = \bar{x}_{s} \pm me \tag{6.15} 
\end{align}\]</span></p>
<p>We leave readers to investigate the <strong>Two-Sample Z-Test</strong> and <strong>Paired Z-Test</strong> given the following equations:</p>
<p><strong>Two-Sample Z-Test</strong></p>
<p><span class="math display">\[
z = \frac{ (\bar{x}_s - \mu_p)}{\sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma_2^2}{N_2}}} 
\]</span></p>
<p><strong>Paired Z-Test</strong></p>
<p><span class="math display">\[
z = \frac{ (\bar{x}_{s1} - \bar{x}_{s2}) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma_2^2}{N_2}}} 
\]</span></p>
</div>
<div id="f-test-using-f-ratio" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.4</span> F-Test using F-ratio  <a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>F-Test</strong> follows an F-distribution <span class="citation">(Ardelean F. A. <a href="bibliography.html#ref-ref809f">2017</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>. It is a test commonly used to compare variances of multiple groups independently sampled from a population (e.g., or from separate experiments) using an <strong>F statistic</strong>. To compute for the <strong>F statistic</strong>, we use the following formula (also known as <strong>ratio of variances</strong> or simply <strong>F-ratio</strong>):</p>
<p><span class="math display" id="eq:equate1080016">\[\begin{align}
F = \frac{\sigma_1^2} {\sigma_2^2}\ \ \ \ \ where\ \sigma_1 &gt; \sigma_2 \tag{6.16} 
\end{align}\]</span></p>
<p>Note that we force a right-side tail by having the variance with greater value be the numerator, making it easier to interpret. For inference, our <strong>null hypothesis</strong> claims that there is no significant difference between the variance of the first sample and the variance of the second sample. That is expressed as:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \sigma_1^2 = \sigma_2^2\ \ \ \ \ \leftarrow \ \ \ \text{null hypothesis}\\
H_1 &amp;: \sigma_1^2 \ne \sigma_2^2\ \ \ \ \ \leftarrow \ \ \ \text{alternative}\\
\end{align*}\]</span></p>
<p>In terms of ratio, the <strong>null hypothesis</strong> is true only if <strong>F-ratio</strong> is 1.</p>
<p>To illustrate, let us review the following naive implementation of <strong>F test</strong> in R code:</p>

<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1">f_test &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2) {</a>
<a class="sourceLine" id="cb379-2" data-line-number="2">  v1 =<span class="st"> </span><span class="kw">var</span>(x1)</a>
<a class="sourceLine" id="cb379-3" data-line-number="3">  v2 =<span class="st"> </span><span class="kw">var</span>(x2)</a>
<a class="sourceLine" id="cb379-4" data-line-number="4">  n1 =<span class="st"> </span><span class="kw">length</span>(x1)  </a>
<a class="sourceLine" id="cb379-5" data-line-number="5">  n2 =<span class="st"> </span><span class="kw">length</span>(x2) </a>
<a class="sourceLine" id="cb379-6" data-line-number="6">  fratio =<span class="st"> </span><span class="dv">0</span>;  p =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb379-7" data-line-number="7">  num =<span class="st"> </span>v1; denom =<span class="st"> </span>v2</a>
<a class="sourceLine" id="cb379-8" data-line-number="8">  df1 =<span class="st"> </span>n1 <span class="op">-</span><span class="st"> </span><span class="dv">1</span>; df2 =<span class="st"> </span>n2 <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb379-9" data-line-number="9">  greater =<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb379-10" data-line-number="10">  <span class="cf">if</span> (v2 <span class="op">&gt;</span><span class="st"> </span>v1) {</a>
<a class="sourceLine" id="cb379-11" data-line-number="11">    num =<span class="st"> </span>v2; denom =<span class="st"> </span>v1</a>
<a class="sourceLine" id="cb379-12" data-line-number="12">    df1 =<span class="st"> </span>n2 <span class="op">-</span><span class="st"> </span><span class="dv">1</span>; df2 =<span class="st"> </span>n1 <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb379-13" data-line-number="13">    greater =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb379-14" data-line-number="14">  } </a>
<a class="sourceLine" id="cb379-15" data-line-number="15">    fratio =<span class="st"> </span>num <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb379-16" data-line-number="16">    p =<span class="st"> </span><span class="kw">pf</span>(fratio, df1, df2, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb379-17" data-line-number="17">  <span class="kw">list</span>(<span class="st">&quot;greater&quot;</span>=greater, <span class="st">&quot;fratio&quot;</span>=fratio, </a>
<a class="sourceLine" id="cb379-18" data-line-number="18">       <span class="st">&quot;num df1&quot;</span> =<span class="st"> </span>df1, <span class="st">&quot;denom df2&quot;</span> =<span class="st"> </span>df2, <span class="st">&quot;pvalue&quot;</span>=p)</a>
<a class="sourceLine" id="cb379-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb379-20" data-line-number="20"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb379-21" data-line-number="21">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb379-22" data-line-number="22"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb379-23" data-line-number="23">population1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb379-24" data-line-number="24"><span class="kw">set.seed</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb379-25" data-line-number="25">population2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb379-26" data-line-number="26"><span class="co"># we take two samples and record their IQ level</span></a>
<a class="sourceLine" id="cb379-27" data-line-number="27"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb379-28" data-line-number="28">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population1, <span class="dt">size=</span><span class="dv">11</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) <span class="co"># group 1</span></a>
<a class="sourceLine" id="cb379-29" data-line-number="29">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population2, <span class="dt">size=</span><span class="dv">26</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) <span class="co"># group 2</span></a>
<a class="sourceLine" id="cb379-30" data-line-number="30"><span class="co"># get the F statistic</span></a>
<a class="sourceLine" id="cb379-31" data-line-number="31">fscore =<span class="st"> </span><span class="kw">f_test</span>(x1, x2); <span class="kw">t</span>(fscore)</a></code></pre></div>
<pre><code>##      greater fratio num df1 denom df2 pvalue
## [1,] TRUE    1.313  10      25        0.2768</code></pre>

<p>Alternatively, we can use the built-in R function <strong>var.test(.)</strong>. Note that the function always expects the first parameter to be the numerator. Our own <strong>f_test(.)</strong> function uses the greater sample as our numerator. Our result shows that the first sample is the lesser one; therefore, in using <strong>var.test(.)</strong>, we pass the value <strong>less</strong> to the alternative parameter given we pass the second sample to the first parameter.</p>

<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" data-line-number="1"><span class="co"># first parameter is the second sample, x2</span></a>
<a class="sourceLine" id="cb381-2" data-line-number="2"><span class="co"># therefore, alternative=less</span></a>
<a class="sourceLine" id="cb381-3" data-line-number="3">v1 =<span class="st"> </span><span class="kw">var</span>(x1); v2 =<span class="st"> </span><span class="kw">var</span>(x2)</a>
<a class="sourceLine" id="cb381-4" data-line-number="4"><span class="cf">if</span> (v1 <span class="op">&gt;</span><span class="st"> </span>v2) {</a>
<a class="sourceLine" id="cb381-5" data-line-number="5">  <span class="kw">var.test</span>(x1, x2, <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.90</span>)</a>
<a class="sourceLine" id="cb381-6" data-line-number="6">} <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb381-7" data-line-number="7">  <span class="kw">var.test</span>(x2, x1, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.90</span>)  </a>
<a class="sourceLine" id="cb381-8" data-line-number="8">}</a></code></pre></div>
<pre><code>## 
##  F test to compare two variances
## 
## data:  x1 and x2
## F = 1.3, num df = 10, denom df = 25, p-value = 0.7
## alternative hypothesis: true ratio of variances is less than 1
## 90 percent confidence interval:
##  0.000 2.854
## sample estimates:
## ratio of variances 
##              1.313</code></pre>

<p>We can reference the <strong>F table</strong>, Table <a href="15.3-appendix-c.html#tab:ftable">15.7</a>, in the Appendix to derive the <strong>critical value</strong>. Here, we see that x2 is the greater sample with degrees of freedom at 35, which becomes our numerator. Sample x1 has df at ten, and it becomes our denominator for our <strong>F ratio</strong>. Therefore, our <strong>critical value</strong> in the <strong>F table</strong> corresponds to <strong>2.730</strong> granting our confidence level is at 90% - equivalent to <span class="math inline">\(\alpha = 0.10\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fdist"></span>
<img src="DS_files/figure-html/fdist-1.png" alt="F Distribution" width="70%" />
<p class="caption">
Figure 6.10: F Distribution
</p>
</div>

<p>Using Figure <a href="6.3-the-significance-of-difference.html#fig:fdist">6.10</a>, we see that <strong>F</strong> is lesser than the <strong>critical value</strong>; meaning, <strong>f-score</strong> is outside the <strong>rejection region</strong>. Therefore, we fail to reject the claim, given the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>, is true.</p>
<p>Similarly, if we instead use a <strong>significance level</strong> of 0.05, e.g. <span class="math inline">\(\mathbf{\alpha=0.05}\)</span>, then we see that <strong>p-value</strong> 0.28 is greater than <span class="math inline">\(\alpha = 0.05\)</span>. Therefore, we fail to reject the claim, given the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>, is true.</p>
<p>Let us now discuss a case in which we can derive the <strong>F ratio</strong> by using <strong>ANOVA</strong>.</p>
</div>
<div id="f-test-with-one-way-anova" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.5</span> F-Test with One-Way ANOVA <a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Anova</strong> stands for <strong>analysis of variance</strong> <span class="citation">(Ardelean F. A. <a href="bibliography.html#ref-ref809f">2017</a>)</span>. There are two ways of performing <strong>ANOVA</strong>.</p>
<ul>
<li>One-Way Anova - we use this to analyze the variation of multiple groups involving one independent variable (one factor) and a dependent variable. The goal is to compare variations between groups (e.g., comparing means if they are closer or farther apart) and within groups.</li>
<li>Two-Way Anova - we use this to analyze the variation of multiple groups involving two independent variables (two factors) and a dependent variable. The goal is to examine the effect of one factor, the other, or both factors (via interaction) against the dependent variable.</li>
</ul>
<p>Note that an <strong>F-Test using ANOVA</strong> is an <strong>Omnibus Test</strong>.</p>
<p>To perform a one-way analysis of variance (One-Way ANOVA), we are to compute a few formulas:</p>
<p><strong>Sum of Squares Total (SST):</strong>  </p>
<p><span class="math display" id="eq:equate1080017">\[\begin{align}
SS_T = \sum_{j=1}^m \sum_{i=1}^{n_j} (x_{ji} - \bar{x})^2 \tag{6.17} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>m</strong> is the total number of groups sampled from a population.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{x_{ji}}\)</span> is the ith observation in sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}}\)</span> is the overall grand mean.</li>
</ul>
<p><strong>Sum of Squares between samples (SSB):</strong>  </p>
<p><span class="math display" id="eq:equate1080018">\[\begin{align}
SS_B = SS_{(treatment)} = \sum_{j=1}^m \sum_{i=1}^{n_j} (\bar{x}_j - \bar{x})^2  = \sum_{j=1}^m n_j  (\bar{x}_j - \bar{x})^2 \tag{6.18} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{SS_B}\)</span> is also called the sum of squares treatment.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}_j}\)</span> is the jth sample mean.</li>
<li><span class="math inline">\(\mathbf{\bar{x}}\)</span> is the overall grand mean.</li>
</ul>
<p><strong>Sum of Squares within samples (SSW):</strong>  </p>
<p><span class="math display" id="eq:equate1080019">\[\begin{align}
SS_W = SS_{error} = \sum_{j=1}^m \sum_{i=1}^{n_j} (x_{ji} - \bar{x}_j)^2  \tag{6.19} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{SS_W}\)</span> is also called the sum of squares error.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{x_{ji}}\)</span> is the ith observation in sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}_j}\)</span> is the jth sample mean.</li>
</ul>
<p>Note here that <strong>SST</strong> can also be computed as:</p>
<p><span class="math display" id="eq:equate1080020">\[\begin{align}
SS_T = SS_B + SS_W \tag{6.20} 
\end{align}\]</span></p>
<p><strong>Mean Squares between samples (MSB):</strong>  </p>
<p><span class="math display" id="eq:equate1080021">\[\begin{align}
MS_B = s^2_B = \frac{SS_B}{df_B},\ \ \ \ \ \ df_B =  m - 1 \tag{6.21} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{MS_B}\)</span> is also called the mean of squares treatment.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{df_B}\)</span> is the degrees of freedom between samples.</li>
</ul>
<p><strong>Mean Squares within samples (MSW):</strong>  </p>
<p><span class="math display" id="eq:equate1080022">\[\begin{align}
MS_W = MS_E = s^2_w = \frac{SS_W}{df_W},\ \ \ \ \ \ df_W = \sum_j^m (n_j - 1) \tag{6.22} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{MS_W}\)</span> is also called the within mean square (error).</li>
<li><strong>N</strong> is the overall total observations, e.g., N = mn.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{df_W}\)</span> is the degrees of freedom within samples.</li>
</ul>
<p>Finally, we compute for the <strong>F statistic</strong>: </p>
<p><span class="math display" id="eq:equate1080023">\[\begin{align}
F = \frac{explained}{unexplained} = \frac{s^2_B}{s^2_W} = \frac{MS_B}{MS_W}  \tag{6.23} 
\end{align}\]</span></p>
<p>All the computation is reflected in the following <strong>One-Way ANOVA</strong> table <span class="citation">(Larson M. G. <a href="bibliography.html#ref-ref827m">2008</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>: </p>

<table>
<caption><span id="tab:onewayanova">Table 6.5: </span>One-Way ANOVA</caption>
<thead>
<tr class="header">
<th align="left">Source of Variation</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Sum of Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Between</td>
<td align="left"><span class="math inline">\(df_{B}: m- 1\)</span></td>
<td align="left"><span class="math inline">\(SS_B\)</span></td>
<td align="left"><span class="math inline">\(MS_B: \frac{SSB}{df_B}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MSB}{MSW}\)</span></td>
</tr>
<tr class="even">
<td align="left">Within (Error)</td>
<td align="left"><span class="math inline">\(df_{W}: \sum_j^m (n_j - 1)\)</span></td>
<td align="left"><span class="math inline">\(SS_W\)</span></td>
<td align="left"><span class="math inline">\(MS_W: \frac{SS_W}{df_W}\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(df_T: \sum_j^m (n_j) - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>Below is a naive implementation of <strong>One-Way ANOVA</strong> in R code:</p>

<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" data-line-number="1"><span class="co"># Supports different group sizes</span></a>
<a class="sourceLine" id="cb383-2" data-line-number="2">one_way_anova &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, <span class="dt">factor =</span> <span class="ot">NULL</span>, <span class="dt">size =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb383-3" data-line-number="3">  <span class="cf">if</span> (<span class="kw">is.null</span>(factor)) {</a>
<a class="sourceLine" id="cb383-4" data-line-number="4">      m =<span class="st"> </span><span class="kw">length</span>(dependent)</a>
<a class="sourceLine" id="cb383-5" data-line-number="5">      x =<span class="st"> </span>dependent</a>
<a class="sourceLine" id="cb383-6" data-line-number="6">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb383-7" data-line-number="7">      groups =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb383-8" data-line-number="8">      m =<span class="st"> </span><span class="kw">length</span>(groups)  <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb383-9" data-line-number="9">      x =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb383-10" data-line-number="10">      <span class="co"># Group factors</span></a>
<a class="sourceLine" id="cb383-11" data-line-number="11">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-12" data-line-number="12">          idx =<span class="st"> </span><span class="kw">which</span>(factor <span class="op">==</span><span class="st"> </span>groups[j])</a>
<a class="sourceLine" id="cb383-13" data-line-number="13">          x[[j]] =<span class="st"> </span>dependent[idx]</a>
<a class="sourceLine" id="cb383-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb383-15" data-line-number="15">  } </a>
<a class="sourceLine" id="cb383-16" data-line-number="16">  <span class="co"># Get the grand mean</span></a>
<a class="sourceLine" id="cb383-17" data-line-number="17">  grand_mean =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-18" data-line-number="18">  r =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-20" data-line-number="20">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-21" data-line-number="21">      r =<span class="st"> </span>r <span class="op">+</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb383-22" data-line-number="22">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-23" data-line-number="23">        grand_mean =<span class="st"> </span>grand_mean <span class="op">+</span><span class="st"> </span>x[[j]][i]</a>
<a class="sourceLine" id="cb383-24" data-line-number="24">      }</a>
<a class="sourceLine" id="cb383-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb383-26" data-line-number="26">  grand_mean =<span class="st"> </span>grand_mean <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb383-27" data-line-number="27">  <span class="co"># sum squared total</span></a>
<a class="sourceLine" id="cb383-28" data-line-number="28">  SST =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-29" data-line-number="29">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-30" data-line-number="30">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-31" data-line-number="31">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-32" data-line-number="32">        SST =<span class="st"> </span>SST <span class="op">+</span><span class="st"> </span>(x[[j]][i] <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-33" data-line-number="33">      }</a>
<a class="sourceLine" id="cb383-34" data-line-number="34">  }</a>
<a class="sourceLine" id="cb383-35" data-line-number="35">  <span class="co"># sum square between</span></a>
<a class="sourceLine" id="cb383-36" data-line-number="36">  SSB =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-37" data-line-number="37">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-38" data-line-number="38">      grp_mean =<span class="st"> </span><span class="kw">mean</span>(x[[j]])</a>
<a class="sourceLine" id="cb383-39" data-line-number="39">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]])  <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-40" data-line-number="40">      SSB =<span class="st"> </span>SSB <span class="op">+</span><span class="st"> </span>n<span class="op">*</span>(grp_mean <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb383-42" data-line-number="42">  <span class="co"># sum square within</span></a>
<a class="sourceLine" id="cb383-43" data-line-number="43">  SSW =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-44" data-line-number="44">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-45" data-line-number="45">      grp_mean =<span class="st"> </span><span class="kw">mean</span>(x[[j]])</a>
<a class="sourceLine" id="cb383-46" data-line-number="46">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-47" data-line-number="47">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-48" data-line-number="48">        SSW =<span class="st"> </span>SSW <span class="op">+</span><span class="st"> </span>(x[[j]][i] <span class="op">-</span><span class="st"> </span>grp_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-49" data-line-number="49">      }</a>
<a class="sourceLine" id="cb383-50" data-line-number="50">  }</a>
<a class="sourceLine" id="cb383-51" data-line-number="51">  <span class="co"># mean squared between</span></a>
<a class="sourceLine" id="cb383-52" data-line-number="52">  dfB =<span class="st"> </span>m <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb383-53" data-line-number="53">  MSB =<span class="st"> </span>SSB <span class="op">/</span><span class="st"> </span>dfB</a>
<a class="sourceLine" id="cb383-54" data-line-number="54">  <span class="co"># mean squared within</span></a>
<a class="sourceLine" id="cb383-55" data-line-number="55">  dfW =<span class="st"> </span><span class="dv">0</span>  <span class="co"># assume different group sizes; otherwise, use m * (n - 1)</span></a>
<a class="sourceLine" id="cb383-56" data-line-number="56">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-57" data-line-number="57">    dfW =<span class="st"> </span>dfW <span class="op">+</span><span class="st"> </span>( <span class="kw">length</span>(x[[j]]) <span class="op">-</span><span class="st"> </span><span class="dv">1</span> )</a>
<a class="sourceLine" id="cb383-58" data-line-number="58">  }</a>
<a class="sourceLine" id="cb383-59" data-line-number="59">  MSW =<span class="st"> </span>SSW <span class="op">/</span><span class="st">  </span>dfW</a>
<a class="sourceLine" id="cb383-60" data-line-number="60">  <span class="co"># F statistic</span></a>
<a class="sourceLine" id="cb383-61" data-line-number="61">  F =<span class="st"> </span>MSB <span class="op">/</span><span class="st"> </span>MSW</a>
<a class="sourceLine" id="cb383-62" data-line-number="62">  <span class="co"># return statistics</span></a>
<a class="sourceLine" id="cb383-63" data-line-number="63">  <span class="kw">c</span>( <span class="st">&quot;SSB&quot;</span>=SSB, <span class="st">&quot;SSW&quot;</span>=SSW, <span class="st">&quot;dfB&quot;</span>=dfB,<span class="st">&quot;dfW&quot;</span>=dfW, </a>
<a class="sourceLine" id="cb383-64" data-line-number="64">     <span class="st">&quot;MSB&quot;</span>=MSB, <span class="st">&quot;MSW&quot;</span>=MSW, <span class="st">&quot;F&quot;</span>=F)</a>
<a class="sourceLine" id="cb383-65" data-line-number="65">}</a></code></pre></div>

<p>Let us consider here two cases. One case would be three nearly distinct populations - forcing three sample means to be farther apart. See below:</p>

<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1"><span class="co"># Now let us generate three groups (of population)</span></a>
<a class="sourceLine" id="cb384-2" data-line-number="2"><span class="co"># with different ranges to force different means.</span></a>
<a class="sourceLine" id="cb384-3" data-line-number="3"><span class="co"># The use of different random seed helps to</span></a>
<a class="sourceLine" id="cb384-4" data-line-number="4"><span class="co"># generate independent groups</span></a>
<a class="sourceLine" id="cb384-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb384-6" data-line-number="6">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">70</span>, <span class="dv">120</span>)</a>
<a class="sourceLine" id="cb384-7" data-line-number="7">population1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-8" data-line-number="8"><span class="kw">set.seed</span>(<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb384-9" data-line-number="9">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">130</span>)</a>
<a class="sourceLine" id="cb384-10" data-line-number="10">population2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">30</span>)</a>
<a class="sourceLine" id="cb384-12" data-line-number="12">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">90</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb384-13" data-line-number="13">population3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-14" data-line-number="14"><span class="co"># let us now sample observations from each group</span></a>
<a class="sourceLine" id="cb384-15" data-line-number="15"><span class="co"># with same sample sizes</span></a>
<a class="sourceLine" id="cb384-16" data-line-number="16"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb384-17" data-line-number="17">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb384-18" data-line-number="18">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population1, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-19" data-line-number="19">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population2, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-20" data-line-number="20">x3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population3, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-21" data-line-number="21"><span class="co"># Generate One-Way Anova</span></a>
<a class="sourceLine" id="cb384-22" data-line-number="22">x =<span class="st"> </span><span class="kw">list</span>() ; x[[<span class="dv">1</span>]] =<span class="st"> </span>x1 ; x[[<span class="dv">2</span>]] =<span class="st"> </span>x2; x[[<span class="dv">3</span>]] =<span class="st"> </span>x3</a>
<a class="sourceLine" id="cb384-23" data-line-number="23">(<span class="dt">anova =</span> <span class="kw">one_way_anova</span>(x))</a></code></pre></div>
<pre><code>##      SSB      SSW      dfB      dfW      MSB      MSW        F 
##  540.867 5124.500    2.000   27.000  270.433  189.796    1.425</code></pre>

<p>And to see if we reject the null hypothesis, we evaluate our f-value:</p>

<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb386-1" data-line-number="1">(<span class="dt">Fcrit =</span> <span class="kw">qf</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">df1 =</span> anova[<span class="st">&quot;dfB&quot;</span>], <span class="dt">df2 =</span> anova[<span class="st">&quot;dfW&quot;</span>], </a>
<a class="sourceLine" id="cb386-2" data-line-number="2">            <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 3.354</code></pre>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb388-1" data-line-number="1">(<span class="dt">Fvalue =</span> <span class="kw">as.numeric</span>(anova[<span class="st">&quot;F&quot;</span>])) <span class="op">&gt;</span><span class="st"> </span>Fcrit</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>and our p-value:</p>

<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb390-1" data-line-number="1">(<span class="dt">Pvalue =</span> <span class="kw">pf</span>(Fvalue, anova[<span class="st">&quot;dfB&quot;</span>], anova[<span class="st">&quot;dfW&quot;</span>], <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.2581</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb392-1" data-line-number="1">Pvalue <span class="op">&lt;</span><span class="st"> </span>( <span class="dt">alpha =</span> <span class="fl">0.05</span> )</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>Both outcomes show as <strong>TRUE</strong> meaning, we reject the null hypothesis - there is a significant difference between the groups.</p>
<p>We can also validate using the built-in function called <strong>aov(.)</strong>:</p>

<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb394-1" data-line-number="1">A=<span class="st"> </span><span class="kw">cbind</span>(x1, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb394-2" data-line-number="2">B=<span class="st"> </span><span class="kw">cbind</span>(x2, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb394-3" data-line-number="3">C=<span class="st"> </span><span class="kw">cbind</span>(x3, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb394-4" data-line-number="4">data =<span class="st"> </span>A</a>
<a class="sourceLine" id="cb394-5" data-line-number="5">data =<span class="st"> </span><span class="kw">rbind</span>(data, B)</a>
<a class="sourceLine" id="cb394-6" data-line-number="6">data =<span class="st"> </span><span class="kw">rbind</span>(data, C)</a>
<a class="sourceLine" id="cb394-7" data-line-number="7"><span class="kw">colnames</span>(data) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x&quot;</span>)</a>
<a class="sourceLine" id="cb394-8" data-line-number="8">aov.model =<span class="st"> </span><span class="kw">aov</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(x), <span class="dt">data =</span> <span class="kw">as.data.frame</span>(data))</a>
<a class="sourceLine" id="cb394-9" data-line-number="9"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## as.factor(x)  2    541     270    1.42   0.26
## Residuals    27   5124     190</code></pre>

<p>We can use <strong>df.residual(.)</strong> to get the <strong>degrees of freedom (dfW)</strong> for the within-group:</p>

<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" data-line-number="1"><span class="kw">df.residual</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 27</code></pre>

<p>We can use <strong>deviance(.)</strong> to get the <strong>sum square (SSW)</strong> for the within-group:</p>

<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb398-1" data-line-number="1"><span class="kw">deviance</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 5124</code></pre>

<p>Finally, we can get <strong>MSW</strong> by dividing deviance by degrees of freedom:</p>

<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" data-line-number="1"><span class="kw">deviance</span>(aov.model)<span class="op">/</span><span class="kw">df.residual</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 189.8</code></pre>

<p>The other case would be to take three samples from the same population - forcing sample means between samples to be closer together. An example of this scenario is when performing regression against a repeated sampling of the same population.</p>

<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb402-2" data-line-number="2">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb402-3" data-line-number="3"><span class="co"># Now let us generate a single population ( a single group )</span></a>
<a class="sourceLine" id="cb402-4" data-line-number="4">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb402-5" data-line-number="5"><span class="co"># let us now sample observations from each group</span></a>
<a class="sourceLine" id="cb402-6" data-line-number="6"><span class="co"># with same sample sizes</span></a>
<a class="sourceLine" id="cb402-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb402-8" data-line-number="8">sample_size =<span class="st"> </span><span class="dv">300</span></a>
<a class="sourceLine" id="cb402-9" data-line-number="9">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-10" data-line-number="10">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-11" data-line-number="11">x3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-12" data-line-number="12"><span class="co"># Generate One-Way Anova</span></a>
<a class="sourceLine" id="cb402-13" data-line-number="13">x =<span class="st"> </span><span class="kw">list</span>() ; x[[<span class="dv">1</span>]] =<span class="st"> </span>x1 ; x[[<span class="dv">2</span>]] =<span class="st"> </span>x2; x[[<span class="dv">3</span>]] =<span class="st"> </span>x3</a>
<a class="sourceLine" id="cb402-14" data-line-number="14">(<span class="dt">anova =</span> <span class="kw">round</span>(<span class="kw">one_way_anova</span>(x),<span class="dv">3</span>))</a></code></pre></div>
<pre><code>##        SSB        SSW        dfB        dfW        MSB        MSW          F 
##    422.909 266582.687      2.000    897.000    211.454    297.194      0.712</code></pre>

<p>And to see if we reject the null hypothesis, we verify critical value first:</p>

<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" data-line-number="1">(<span class="dt">Fcrit =</span> <span class="kw">qf</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">df1 =</span> anova[<span class="st">&quot;dfB&quot;</span>], <span class="dt">df2 =</span> anova[<span class="st">&quot;dfW&quot;</span>], </a>
<a class="sourceLine" id="cb404-2" data-line-number="2">            <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 3.006</code></pre>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" data-line-number="1">(<span class="dt">Fvalue =</span> <span class="kw">as.numeric</span>(anova[<span class="st">&quot;F&quot;</span>])) <span class="op">&gt;</span><span class="st"> </span>Fcrit</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>and validate with P-value:</p>

<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1">(<span class="dt">Pvalue =</span> <span class="kw">pf</span>(Fvalue, anova[<span class="st">&quot;dfB&quot;</span>], anova[<span class="st">&quot;dfW&quot;</span>], <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.4909</code></pre>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" data-line-number="1">Pvalue <span class="op">&lt;</span><span class="st"> </span>( <span class="dt">alpha =</span> <span class="fl">0.05</span> )</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>Both outcomes show as <strong>FALSE</strong> meaning, we fail to reject the null hypothesis - there is no difference between the groups.</p>
<p>As can be seen, the <strong>F statistic</strong> is a ratio that reflects the different distributions. Our case demonstrates that if the means are farther from each other between groups, meaning that MSB is greater than MSW, then our <strong>F ratio</strong> is greater than one. On the other hand, if the samples are almost overlapping, meaning that MSB and MSW are almost identical, we see <strong>F ratio</strong> to be closer to one. And if MSW is greater than MSB, it gets closer to zero.</p>
<p>The differences in means are further discussed in the <strong>Post-HOC section</strong>.</p>
<p>In terms of evaluation of <strong>hypothesis</strong>, it is clear that if the <strong>F ratio</strong> is farther away from one, then we reject the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>. Otherwise, if <strong>F ratio</strong> is close to or equal to one, we fail to reject the <strong>null hypothesis</strong>, which claims that there is no difference between samples - that their means are somehow equal. Alternatively, the <span class="math inline">\(\mathbf{H_1}\)</span> has <strong>at least one</strong> mean to be different from the others.</p>
<p><span class="math display" id="eq:equate1080024">\[\begin{align}
H_0: \mu_1 = \mu_2 = \mu_3,\ \ \ \ \ \ \  H_1: \mu_1 \ne \mu_2 \ne \mu_3 \tag{6.24} 
\end{align}\]</span></p>
<p>Let us revisit <strong>One-Way ANOVA</strong> when we cover <strong>Tukey’s method</strong> under the <strong>Post-Hoc Analysis</strong> section using a more practical dataset called <strong>mtcars</strong>. </p>
</div>
<div id="f-test-with-two-way-anova" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.6</span> F-Test with Two-Way ANOVA <a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>One-Way ANOVA</strong>, we show how to perform a test with one independent <strong>nominal</strong> variable. In <strong>Two-Way ANOVA</strong>, we show how to analyze with two independent <strong>nominal</strong> variables (or factors) and a scale-level dependent variable.</p>
<p>For example, suppose we are a dog food company, and we have invited dog owners across the country to participate in our marketing campaign to try our three new brands of dog food (call it A, B, C) specifically made for tiny, adorable cross-breed dogs of the following breed (ref: <a href="https://dogtime.com/dog-breeds/profiles" class="uri">https://dogtime.com/dog-breeds/profiles</a>):</p>
<ul>
<li>Cavachon (Cavalier King Charles Spaniel and Bichon Frise)</li>
<li>Cavapoo (Cavalier King Charles Spaniel and Poodle)</li>
<li>Maltipoo (Maltese and Poodle)</li>
<li>Pomchi (Pomeranian and Chihuahua)</li>
<li>Shichon (Shih Tzu and Bichon Frise) </li>
<li>Shih-Poo (Shih Tzu and Toy Poodle)</li>
<li>Shorkie (Shih Tzhu and Yorkshire Terrier) </li>
<li>Westiepoo (West Highland White Terrier and Poodle)</li>
</ul>
<p>There are 24 combinations to test. We scheme to have four trials for each combination. In other words, we need four subjects from each cross-breed dog type to try one brand of dog food. That makes around 96 volunteered dogs. Each dog is given a 100-gram sample of the new brand (assume no bias - e.g., dog age, dog weight, dog mood, dog health, etc.). Each combination, therefore, makes around 400 grams. Here, we are to measure how many grams are consumed.</p>
<p>Here is what we get (fictitious outcome):</p>

<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb412-1" data-line-number="1"><span class="co"># let us use a fix range of consumed grams between 0 to 1000 grams.</span></a>
<a class="sourceLine" id="cb412-2" data-line-number="2">grams =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb412-3" data-line-number="3">a =<span class="st"> </span><span class="dv">3</span>; b =<span class="st"> </span><span class="dv">8</span>; r =<span class="st"> </span><span class="dv">4</span>;  n =<span class="st"> </span>a <span class="op">*</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb412-4" data-line-number="4"><span class="co"># let us create three brands of food, with 4 replications  for</span></a>
<a class="sourceLine" id="cb412-5" data-line-number="5"><span class="co"># each cross-breed per brand.</span></a>
<a class="sourceLine" id="cb412-6" data-line-number="6"><span class="kw">set.seed</span>(<span class="dv">1</span>); A =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">2</span>); B =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-8" data-line-number="8"><span class="kw">set.seed</span>(<span class="dv">3</span>); C =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-9" data-line-number="9">subjects =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb412-10" data-line-number="10"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb412-11" data-line-number="11">    subjects[i,<span class="dv">1</span>] =<span class="st"> </span><span class="kw">paste</span>(A[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-12" data-line-number="12">    subjects[i,<span class="dv">2</span>] =<span class="st"> </span><span class="kw">paste</span>(B[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-13" data-line-number="13">    subjects[i,<span class="dv">3</span>] =<span class="st"> </span><span class="kw">paste</span>(C[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb412-15" data-line-number="15"><span class="kw">colnames</span>(subjects) =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb412-16" data-line-number="16">            <span class="st">&#39;Brand A&#39;</span>, <span class="st">&#39;Brand B&#39;</span>, <span class="st">&#39;Brand C&#39;</span></a>
<a class="sourceLine" id="cb412-17" data-line-number="17">            )</a>
<a class="sourceLine" id="cb412-18" data-line-number="18"><span class="kw">rownames</span>(subjects) =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb412-19" data-line-number="19">            <span class="st">&#39;Cavachon&#39;</span>, <span class="st">&#39;Cavapoo&#39;</span>, <span class="st">&#39;Maltipoo&#39;</span>, <span class="st">&#39;Pomchi&#39;</span>,</a>
<a class="sourceLine" id="cb412-20" data-line-number="20">            <span class="st">&#39;Shichon&#39;</span>, <span class="st">&#39;Shih-Poo&#39;</span>, <span class="st">&#39;Shorkie&#39;</span>, <span class="st">&#39;Westiepoo&#39;</span></a>
<a class="sourceLine" id="cb412-21" data-line-number="21">          )</a>
<a class="sourceLine" id="cb412-22" data-line-number="22">knitr<span class="op">::</span><span class="kw">kable</span>( subjects, </a>
<a class="sourceLine" id="cb412-23" data-line-number="23">    <span class="dt">caption =</span> <span class="st">&#39;Grams consumed by 4 dogs per cross-breed per brand&#39;</span>, </a>
<a class="sourceLine" id="cb412-24" data-line-number="24">    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-169">Table 6.6: </span>Grams consumed by 4 dogs per cross-breed per brand</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Brand A</th>
<th align="left">Brand B</th>
<th align="left">Brand C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="left">26,63,72,26</td>
<td align="left">18,47,98,35</td>
<td align="left">16,58,11,23</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="left">37,6,100,38</td>
<td align="left">70,55,22,49</td>
<td align="left">81,63,71,79</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="left">57,20,38,1</td>
<td align="left">57,55,44,15</td>
<td align="left">38,51,90,60</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="left">91,17,78,38</td>
<td align="left">16,24,7,36</td>
<td align="left">33,51,28,91</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="left">20,69,94,87</td>
<td align="left">95,76,66,97</td>
<td align="left">60,53,23,56</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="left">90,38,21,34</td>
<td align="left">95,18,39,13</td>
<td align="left">61,56,1,76</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="left">95,77,65,48</td>
<td align="left">13,40,84,1</td>
<td align="left">12,87,13,38</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="left">66,50,12,60</td>
<td align="left">84,86,15,16</td>
<td align="left">29,83,9,37</td>
</tr>
</tbody>
</table>

<p>Here, we deal with two factors which we label as <strong>A</strong> and <strong>B</strong>:</p>
<ul>
<li><strong>A</strong> - Dog Food Type</li>
<li><strong>B</strong> - Dog Breed</li>
</ul>
<p>with the following size of each factor:</p>
<ul>
<li><strong>a</strong> - number of food types, where a = 3</li>
<li><strong>b</strong> - number of dog breeds, where b = 8</li>
<li><strong>r</strong> - number of subjects (trials) per combination, where r = 4</li>
<li><strong>n</strong> - grand total number of subjects, where n = abr = <span class="math inline">\(3\times 8 \times 4 = 96\)</span>.</li>
</ul>
<p>With that, let us step through the <strong>Two-Way ANOVA</strong> process.</p>
<p><strong>First</strong>, we need to solve for five <strong>sum of squared values</strong>:</p>
<ul>
<li>Solving sum of squared values for factor A</li>
</ul>

<p><span class="math display" id="eq:eqnnumber29">\[\begin{align}
S_A {}&amp;= \frac{ \sum^a (\sum^b\sum^r A)^2 }{b r}  \tag{6.25} \\
&amp;= \frac{(\sum\text{Brand A})^2 + (\sum\text{Brand B})^2 + (\sum\text{Brand C})^2}{8 \times 4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
   (67 + 58 + 72 + 88 + ... + 81 +  6 + 33 + 73)^2 + \\
   (84 + 80 + 95 + 42 + ... + 92 + 79 + 99 + 82)^2 + \\
   ( 4 + 73 +  4 + 39 + ... + 19 + 28 + 11 + 73)^2
  \end{array}
  \right)
  }
{8 \times 4} \nonumber
\end{align}\]</span>
</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb413-1" data-line-number="1">(<span class="dt">S_A =</span> ( <span class="kw">sum</span>(A)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>(b <span class="op">*</span><span class="st"> </span>r)) </a></code></pre></div>
<pre><code>## [1] 226362</code></pre>
<ul>
<li>Solving sum of squared values for factor B</li>
</ul>

<p><span class="math display" id="eq:eqnnumber26">\[\begin{align}
S_B {}&amp;= \frac{ \sum^b (\sum^a\sum^r B)^2 }{a r}  \tag{6.26} \\
&amp;= \frac{(\sum\text{Cavachon})^2 + (\sum\text{Cavapoo})^2 + ... + (\sum\text{ Westiepoo})^2}{3 \times 4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
  (67 + 58 + 72 + 88 + 84 + 80 + 95 + 42 + 4 + 73 + 4 + 39)^2 + \\
  (38 + 50 + 78 + 43 + 78 + 75 + 49 + 37 + 57 + 54 + 36 + 21)^2 + \\
   ... + \\
  (81 + 6 + 33 + 73 + 92 + 79 + 99 + 82 + 19 + 28 + 11 + 73)^2
  \end{array}
  \right)
  }
{3 \times 4} \nonumber
\end{align}\]</span>
</p>

<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" data-line-number="1">S_B =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb415-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb415-3" data-line-number="3">    S_B =<span class="st"> </span>S_B <span class="op">+</span><span class="st"> </span>( <span class="kw">sum</span>(A[i,]) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B[i,]) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,]) )<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb415-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb415-5" data-line-number="5">(<span class="dt">S_B =</span> S_B <span class="op">/</span><span class="st"> </span>(a <span class="op">*</span><span class="st"> </span>r))</a></code></pre></div>
<pre><code>## [1] 232082</code></pre>

<ul>
<li>Solving sum of squared values for interaction of A and B</li>
</ul>

<p><span class="math display" id="eq:eqnnumber27">\[\begin{align}
S_{AB} {}&amp;= \frac{ \sum^b \sum^a(\sum^r AB)^2 }{r}  \tag{6.27} \\
&amp;= \frac{
  \left(
  \begin{array}{c}
   (\sum\text{Brand A,Cavachon})^2 + (\sum\text{Brand B, Cavachon})^2 + \\
    ... +\\
    (\sum\text{Brand B,Westiepoo})^2 + (\sum\text{Brand C,Westiepoo})^2
  \end{array}
  \right)
    }{4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
    (67 + 58 + 72 + 88)^2 +  (84 + 80 + 95 + 42)^2  + \\
    ... + \\
    (92 + 79 + 99 + 82)^2 + (19 + 28 + 11 + 73)^2
  \end{array}
  \right)
    }
    {4} \nonumber
\end{align}\]</span>
</p>

<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb417-1" data-line-number="1">S_AB =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb417-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb417-3" data-line-number="3">        S_AB =<span class="st"> </span>S_AB <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(A[i,])<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B[i,])<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,])<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb417-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb417-5" data-line-number="5">(<span class="dt">S_AB =</span> S_AB <span class="op">/</span><span class="st"> </span>r)</a></code></pre></div>
<pre><code>## [1] 246172</code></pre>

<ul>
<li>Solving sum of squared values for each subject</li>
</ul>
<p><span class="math display" id="eq:equate1080025">\[\begin{align}
S_W {}&amp;= \sum^b \sum^a \sum^r (W)^2  \tag{6.28} \\
&amp;=  (\text{Brand A,Cavachon, Dog1})^2 + ... + (\text{Brand C,Westiepoo,Dog4})^2  \nonumber \\
&amp;= 67^2 + 58^2 + 72^2 + 88^2 + ... + 19^2 + 28^2 + 11^2 + 73^2 \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1">S_W =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb419-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb419-3" data-line-number="3">    S_W =<span class="st"> </span>S_W <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>( A[i,]<span class="op">^</span><span class="dv">2</span> ) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span> (B[i,]<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,]<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb419-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb419-5" data-line-number="5">S_W</a></code></pre></div>
<pre><code>## [1] 303280</code></pre>

<ul>
<li>Solving sum of squared values for total</li>
</ul>
<p><span class="math display" id="eq:equate1080026">\[\begin{align}
S_T {}&amp;= (\sum^b \sum^a \sum^r T)^2  \tag{6.29} \\
&amp;=  \frac{(\text{Brand A,Cavachon, Dog1} + ... + \text{Brand C,Westiepoo, Dog4})^2}
{ 3 \times 8 \times 4} \nonumber \\
&amp;= \frac{(67 + 58 + 72 + 88 + ... + 19 + 28 + 11 + 73)^2}{ 3 \times 8 \times 4 } \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" data-line-number="1">(<span class="dt">S_T =</span> ( <span class="kw">sum</span>(A) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(a <span class="op">*</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>r))</a></code></pre></div>
<pre><code>## [1] 226010</code></pre>

<p><strong>Second</strong>, let us use the <strong>Two-Way ANOVA Table</strong> (See Table <a href="6.3-the-significance-of-difference.html#tab:twowayanova">6.7</a>) <span class="citation">(Larson M. G. <a href="bibliography.html#ref-ref827m">2008</a>; Natoli C. Cory <a href="bibliography.html#ref-ref837c">2017</a>)</span>:</p>

<table>
<caption><span id="tab:twowayanova">Table 6.7: </span>Two-Way ANOVA</caption>
<colgroup>
<col width="15%" />
<col width="17%" />
<col width="20%" />
<col width="27%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source of Variation</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Sum of Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Between Grps (A)</td>
<td align="left"><span class="math inline">\(df_A: a - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_A: S_A - S_T\)</span></td>
<td align="left"><span class="math inline">\(MS_A: \frac{SS_A}{df_A}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_A}{MS_E}\)</span></td>
</tr>
<tr class="even">
<td align="left">Between Grps (B)</td>
<td align="left"><span class="math inline">\(df_B: b - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_B: S_B - S_T\)</span></td>
<td align="left"><span class="math inline">\(MS_B: \frac{SS_B}{df_B}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_B}{MS_E}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Interaction (AB)</td>
<td align="left"><span class="math inline">\(df_{AB}: (a-1)(b-1)\)</span></td>
<td align="left"><span class="math inline">\(SS_{AB}: S_{AB} - S_A\)</span></td>
<td align="left"><span class="math inline">\(MS_{AB}: \frac{SS_{AB}}{df_{AB}}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_{AB}}{MS_E}\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(\text{   }\)</span>- <span class="math inline">\(S_B + S_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Within Grps (Err)</td>
<td align="left"><span class="math inline">\(df_E: ab(r - 1)\)</span></td>
<td align="left"><span class="math inline">\(SS_E: S_W - S_{AB}\)</span></td>
<td align="left"><span class="math inline">\(MS_E: \frac{SS_E}{df_E}\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(df_T: abr - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_T: S_W - S_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>With that, we can patch the values into the table (including the critical values at alpha=0.05):</p>

<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" data-line-number="1"><span class="co"># Compute for Sum of Squares</span></a>
<a class="sourceLine" id="cb423-2" data-line-number="2">SSA =<span class="st"> </span><span class="kw">round</span>(S_A <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfA =<span class="st"> </span>a <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-3" data-line-number="3">SSB =<span class="st"> </span><span class="kw">round</span>(S_B <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfB =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-4" data-line-number="4">SSAB =<span class="st"> </span><span class="kw">round</span>(S_AB <span class="op">-</span><span class="st"> </span>S_A <span class="op">-</span><span class="st"> </span>S_B <span class="op">+</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfAB =<span class="st"> </span>(a <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>(b <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb423-5" data-line-number="5">SSW =<span class="st"> </span><span class="kw">round</span>(S_W <span class="op">-</span><span class="st"> </span>S_AB,<span class="dv">2</span>); dfW =<span class="st"> </span>a<span class="op">*</span>b<span class="op">*</span>(r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb423-6" data-line-number="6">SST =<span class="st"> </span><span class="kw">round</span>(S_W <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfT =<span class="st"> </span>a<span class="op">*</span>b<span class="op">*</span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-7" data-line-number="7"><span class="co"># Compute for Mean Squares</span></a>
<a class="sourceLine" id="cb423-8" data-line-number="8">MSA =<span class="st"> </span><span class="kw">round</span>(SSA<span class="op">/</span>dfA,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-9" data-line-number="9">MSB =<span class="st"> </span><span class="kw">round</span>(SSB<span class="op">/</span>dfB,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-10" data-line-number="10">MSAB =<span class="st"> </span><span class="kw">round</span>(SSAB<span class="op">/</span>dfAB,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-11" data-line-number="11">MSE =<span class="st"> </span><span class="kw">round</span>(SSW <span class="op">/</span><span class="st"> </span>dfW,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-12" data-line-number="12"><span class="co"># Compute for F-statistics</span></a>
<a class="sourceLine" id="cb423-13" data-line-number="13">F_A =<span class="st"> </span><span class="kw">round</span>(MSA<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-14" data-line-number="14">F_B =<span class="st"> </span><span class="kw">round</span>(MSB<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-15" data-line-number="15">F_AB =<span class="st"> </span><span class="kw">round</span>(MSAB<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-16" data-line-number="16"><span class="co"># Compute for critical values at alpha=0.05 (95% confidence level).</span></a>
<a class="sourceLine" id="cb423-17" data-line-number="17">cv_A =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfA, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-18" data-line-number="18">cv_B =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-19" data-line-number="19">cv_AB =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfAB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-20" data-line-number="20"><span class="co"># Compute for p-value at  alpha=0.05 (95% confidence level).</span></a>
<a class="sourceLine" id="cb423-21" data-line-number="21">pv_A =<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_A, dfA, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-22" data-line-number="22">pv_B=<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_B, dfB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-23" data-line-number="23">pv_AB =<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_AB, dfAB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-24" data-line-number="24">listing =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb423-25" data-line-number="25">        <span class="st">&quot;Between Groups (A)&quot;</span>, dfA,   SSA, MSA, F_A, cv_A, pv_A,</a>
<a class="sourceLine" id="cb423-26" data-line-number="26">        <span class="st">&quot;Between Groups (B)&quot;</span>, dfB,   SSB, MSB, F_B, cv_B, pv_B,</a>
<a class="sourceLine" id="cb423-27" data-line-number="27">        <span class="st">&quot;Interaction (AB)&quot;</span>, dfAB, SSAB, MSAB, F_AB, cv_AB, pv_AB,</a>
<a class="sourceLine" id="cb423-28" data-line-number="28">        <span class="st">&quot;Error (Within)&quot;</span>, dfW, SSW, MSE,  <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb423-29" data-line-number="29">        <span class="st">&quot;Total&quot;</span>, dfT ,SST, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb423-30" data-line-number="30">            )</a>
<a class="sourceLine" id="cb423-31" data-line-number="31">m =<span class="st"> </span><span class="kw">matrix</span>(listing, <span class="dt">nrow=</span><span class="kw">length</span>(listing)<span class="op">/</span><span class="dv">7</span>, <span class="dt">ncol=</span><span class="dv">7</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb423-32" data-line-number="32"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Source of Variation&quot;</span>, <span class="st">&quot;DF&quot;</span>, </a>
<a class="sourceLine" id="cb423-33" data-line-number="33">                  <span class="st">&quot;SS&quot;</span>, <span class="st">&quot;MS&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;Critical Value&quot;</span>, <span class="st">&quot;P-value&quot;</span> )</a>
<a class="sourceLine" id="cb423-34" data-line-number="34">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Two-Way ANOVA&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb423-35" data-line-number="35">    <span class="dt">align=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>,<span class="dt">times=</span><span class="dv">6</span>)), <span class="dt">digits=</span><span class="dv">3</span>, <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:twowayanova1">Table 6.8: </span>Two-Way ANOVA</caption>
<thead>
<tr class="header">
<th align="right">Source of Variation</th>
<th align="right">DF</th>
<th align="right">SS</th>
<th align="right">MS</th>
<th align="right">F</th>
<th align="right">Critical Value</th>
<th align="right">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Between Groups (A)</td>
<td align="right">2</td>
<td align="right">352.33</td>
<td align="right">176.16</td>
<td align="right">0.2221</td>
<td align="right">3.124</td>
<td align="right">0.801</td>
</tr>
<tr class="even">
<td align="right">Between Groups (B)</td>
<td align="right">7</td>
<td align="right">6071.96</td>
<td align="right">867.42</td>
<td align="right">1.0936</td>
<td align="right">2.14</td>
<td align="right">0.377</td>
</tr>
<tr class="odd">
<td align="right">Interaction (AB)</td>
<td align="right">14</td>
<td align="right">13738.17</td>
<td align="right">981.3</td>
<td align="right">1.2372</td>
<td align="right">1.832</td>
<td align="right">0.269</td>
</tr>
<tr class="even">
<td align="right">Error (Within)</td>
<td align="right">72</td>
<td align="right">57107.5</td>
<td align="right">793.16</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">Total</td>
<td align="right">95</td>
<td align="right">77269.96</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>

<p><strong>Finally</strong>, with the three computed <strong>F-statistics</strong>, let us now review our <strong>hypothesis</strong>. At an alpha value of 0.05, we have the following evaluation:</p>
<p>The first <strong>F-statistic</strong> at 0.2221 is greater than 3.124, which is <strong>significant</strong>. Alternatively, our <strong>P-value</strong> at 0.801 is lesser than alpha=0.05; thus it is also <strong>significant</strong>. The other two <strong>F-statistic</strong>, at 1.0936 and 1.2372 respectively, are not significant as they are lesser than their corresponding critical values. That is also proven by their <strong>P-values</strong>, which are both greater than the alpha=0.05.</p>
<p>Therefore, suppose our <strong>null hypotheses</strong> have the corresponding claims:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between brands (between groups in A),</li>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between cross-breeds (between groups in B),</li>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between the interaction of brands and cross-breeds.</li>
</ul>
<p>In that case, our <strong>Two-way ANOVA</strong> analysis shows that the first <strong>null hypothesis</strong> is statistically significant. That rejects the <strong>null hypothesis</strong>, which shows a difference between the brands. Also, there is no significant difference in the gram consumption between cross-breeds; neither is there any difference between the interaction of brand and cross-breeds. So to then know which of the brands all the dogs prefer, a starting point is to compare the means of each brand.</p>
<p>The <strong>F distribution</strong> in Figure <a href="6.3-the-significance-of-difference.html#fig:fhypo">6.11</a> shows that The <strong>f-value</strong> for <strong>“between groups” of A</strong> is greater than the <strong>critical value</strong> and therefore it falls within the <strong>rejection region</strong>. That means that the <span class="math inline">\(\mathbf{H_0}\)</span>, <strong>null hypothesis</strong>, is rejected. On the other hand, we fail to reject the <span class="math inline">\(\mathbf{H_0}\)</span>, <strong>null hypothesis</strong>, for both <strong>between-groups of B</strong> and <strong>intersection of A and B</strong>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fhypo"></span>
<img src="DS_files/figure-html/fhypo-1.png" alt="F Distribution (for Two-Way ANOVA)" width="80%" />
<p class="caption">
Figure 6.11: F Distribution (for Two-Way ANOVA)
</p>
</div>

<p>A faster way to perform a <strong>Two-way ANOVA</strong> without the manual calculations we recently demonstrated is to use the built-in R <strong>aov()</strong>. Here, we use one of the standard datasets in R that comes with library <strong>MASS</strong>: mtcars; other datasets can be listed using the function <strong>data()</strong>. To get information about mtcars, we can use the “?” in R like so:</p>

<blockquote>
<p>&gt; ? mtcars</p>
<p>Description:</p>
<pre><code> The data was extracted from the 1974 _Motor Trend_ US magazine,
 and comprises fuel consumption and 10 aspects of automobile design
 and performance for 32 automobiles (1973-74 models).}</code></pre>
<p>Format:</p>
<pre><code> A data frame with 32 observations on 11 (numeric) variables.

   [, 1]  mpg   Miles/(US) gallon                        
   [, 2]  cyl   Number of cylinders                      
   [, 3]  disp  Displacement (cu.in.)                    
   [, 4]  hp    Gross horsepower                         
   [, 5]  drat  Rear axle ratio                          
   [, 6]  wt    Weight (1000 lbs)                        
   [, 7]  qsec  1/4 mile time                            
   [, 8]  vs    Engine (0 = V-shaped, 1 = straight)      
   [, 9]  am    Transmission (0 = automatic, 1 = manual) 
   [,10]  gear  Number of forward gears                  
   [,11]  carb  Number of carburetors</code></pre>
…
</blockquote>

<p>To view the first five records of mtcars, we can use the build-in R function, <strong>head(.)</strong>:</p>

<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb426-1" data-line-number="1"><span class="kw">head</span>(mtcars)</a></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>

<p>To view the internal structure:</p>

<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb428-1" data-line-number="1"><span class="kw">str</span>(mtcars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>

<p>The goal is to examine if each factor, such as the number of cylinders in the car (cyl) or the type of transmission (am), influences the rate of fuel consumption (mpg). Additionally, we examine interactions among factors.</p>
<p>To do that, we use <strong>aov</strong> with an <strong>additive</strong> and an <strong>interaction</strong> formula. Below is an example of an <strong>additive</strong> formula (note that we are using only two factors here - <strong>cyl</strong> and <strong>am</strong>):</p>

<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb430-1" data-line-number="1">(<span class="dt">aov.model =</span> <span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(cyl) <span class="op">+</span><span class="st"> </span><span class="kw">as.factor</span>(am), <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## Call:
##    aov(formula = mpg ~ as.factor(cyl) + as.factor(am), data = mtcars)
## 
## Terms:
##                 as.factor(cyl) as.factor(am) Residuals
## Sum of Squares           824.8          36.8     264.5
## Deg. of Freedom              2             1        28
## 
## Residual standard error: 3.073
## Estimated effects may be unbalanced</code></pre>

<p>It is notable to mention that independent variables have to be <strong>factor</strong> variables; otherwise, they are treated as continuous and not discrete. See below:</p>

<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb432-1" data-line-number="1">(<span class="dt">cyl_factor =</span> <span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4
## Levels: 4 6 8</code></pre>

<p>We can use the built-in R function called <strong>levels()</strong> to show the list of discrete values of the category (the factor):</p>

<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb434-1" data-line-number="1"><span class="kw">levels</span>(cyl_factor)</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p>Because the variable <strong>cyl</strong> is made into factor variable, there are three distinct levels (n=3), <span class="math inline">\((4,\ 6,\ 8)\)</span> which renders a degree of freedom of 2 = (n - 1).</p>
<p>The other variable has two discrete levels (n = 2): 0 and 1, with a degree of freedom of 1 = (n - 1).</p>

<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb436-1" data-line-number="1">(<span class="dt">am_factor =</span> <span class="kw">as.factor</span>(mtcars<span class="op">$</span>am))</a></code></pre></div>
<pre><code>##  [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1
## Levels: 0 1</code></pre>

<p>Using the <strong>ANOVA</strong> outcome, a way to summarize the statistic is with the use of the built-in R function, <strong>summary</strong>:</p>

<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb438-1" data-line-number="1"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##                Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## as.factor(cyl)  2    825     412   43.66 2.5e-09 ***
## as.factor(am)   1     37      37    3.89   0.058 .  
## Residuals      28    264       9                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>The summary shows factor <strong>cyl</strong> to be significant with three asterisks &quot;***&quot; significant code only if alpha = 0.001 and that factor <strong>am</strong> is significant with “.” code only if alpha = 0.1.</p>
<p>To also see if there is an interaction between the groups, we use the following <strong>interaction</strong> formula instead:</p>

<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor <span class="op">*</span><span class="st"> </span>am_factor, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb440-2" data-line-number="2"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##                      Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## cyl_factor            2    825     412   44.85 3.7e-09 ***
## am_factor             1     37      37    4.00   0.056 .  
## cyl_factor:am_factor  2     25      13    1.38   0.269    
## Residuals            26    239       9                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Notice a third entry in the outcome, <strong>cyl_factor:am_factor</strong>, representing the interaction. However, note that the P-value, <strong>Pr(&gt;F)</strong>, shows no significance. Therefore, we can drop this model and use the <strong>additive</strong> formula instead.</p>
<p>On the other hand, we see that if our alpha is 0.001, then only the <strong>cyl_factor</strong> is significant; therefore, we also can change our formula to only focus on the effect of that single factor variable:</p>

<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb442-2" data-line-number="2"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>If our assumption of a <strong>null hypothesis</strong> is that no factors affect the rate of fuel consumption, then we can use the following formula instead:</p>

<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1">(<span class="dt">aov.model =</span> <span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## Call:
##    aov(formula = mpg ~ 1, data = mtcars)
## 
## Terms:
##                 Residuals
## Sum of Squares       1126
## Deg. of Freedom        31
## 
## Residual standard error: 6.027</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals   31   1126    36.3</code></pre>

</div>
<div id="pearsons-chi-square-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.7</span> Pearson’s Chi-square Test <a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Chi-square Test</strong>, we deal with multiple independent <strong>nominal or ordinal</strong> variables (or factors). We can use samples from Normal, Binomial, or Poisson distributions. We then compute for the Chi-Square to arrive at a resulting Chi-square distribution for analysis.</p>
<p>Now, to illustrate, let us discuss two types of <strong>Chi-square Test</strong>, namely <span class="math inline">\(\text{One-Factor}\ X^2\ {Test}\)</span> and <span class="math inline">\(\text{Two-Factor}\ X^2\ {Test}\)</span>.</p>
<p><span class="math inline">\(\mathbf{\text{One-Factor}\ X^2\ {Test}}\)</span> (Goodness of Fit Test):</p>
<p>A <span class="math inline">\(\mathbf{\text{One-Factor}\ X^2\ {Test}}\)</span> performs test statistics using the following equation <span class="citation">(McHugh M.L. <a href="bibliography.html#ref-ref847m">2013</a>)</span>:</p>
<p><span class="math display" id="eq:equate1080027">\[\begin{align}
X^2 = \sum \frac{(observed - expected)^2}{expected} = \sum_i \frac{(O_i - E_i)^2}{E_i} \tag{6.30} 
\end{align}\]</span></p>
<p>Suppose we want to survey the cutest cross-breed dogs preferred by dog owners. So we go out across the country and invite dog owners to participate in our survey. We narrow down the cross-breed type based on the same list of cross-breed dogs as before, but this time with the following <strong>hypothetical</strong> datasets:</p>

<table>
<caption><span id="tab:unnamed-chunk-185">Table 6.9: </span>Observed Cross-Breed Preference</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Observation</th>
<th align="right">Probability</th>
<th align="right">Expected</th>
<th align="right">X^2 = (O - E)^2/E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="right">310</td>
<td align="right">0.12</td>
<td align="right">333.6</td>
<td align="right">1.67</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="right">400</td>
<td align="right">0.13</td>
<td align="right">361.4</td>
<td align="right">4.123</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="right">240</td>
<td align="right">0.09</td>
<td align="right">250.2</td>
<td align="right">0.416</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="right">400</td>
<td align="right">0.16</td>
<td align="right">444.8</td>
<td align="right">4.512</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="right">420</td>
<td align="right">0.16</td>
<td align="right">444.8</td>
<td align="right">1.383</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="right">430</td>
<td align="right">0.14</td>
<td align="right">389.2</td>
<td align="right">4.277</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="right">300</td>
<td align="right">0.11</td>
<td align="right">305.8</td>
<td align="right">0.11</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="right">280</td>
<td align="right">0.09</td>
<td align="right">250.2</td>
<td align="right">3.549</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="right">2780</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right">20.039</td>
</tr>
</tbody>
</table>

<p>Note that we have assumed some prior probabilities in our calculation which we need to calculate the expected values. For example, the expected value for Cavachon dogs is calculated as such:</p>
<p>E = Total <span class="math inline">\(\times\)</span> Probability = 2780 <span class="math inline">\(\times\)</span> 0.10 = 278.</p>
<p>Relying on the <strong>Chi-square table</strong> (see the Appendix), we see that the <strong>Critical Value</strong> is 20.278 for a confidence level of 95% (alpha=0.05) with a degree of freedom at 7, (n-1).</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" data-line-number="1">preference =<span class="st"> </span><span class="kw">c</span>(<span class="dv">310</span>, <span class="dv">400</span>, <span class="dv">240</span>, <span class="dv">400</span>, <span class="dv">420</span>, <span class="dv">430</span>, <span class="dv">300</span>, <span class="dv">280</span>)</a>
<a class="sourceLine" id="cb448-2" data-line-number="2">probability =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.13</span>, <span class="fl">0.09</span>, <span class="fl">0.16</span>, <span class="fl">0.16</span>, <span class="fl">0.14</span>, <span class="fl">0.11</span>,<span class="fl">0.09</span>)</a>
<a class="sourceLine" id="cb448-3" data-line-number="3"><span class="kw">chisq.test</span>(preference, <span class="dt">p=</span>probability)</a></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  preference
## X-squared = 20, df = 7, p-value = 0.005</code></pre>
<p>The P-value is also given. To validate, let us use <strong>pchisq(.)</strong> using a lower.tail=FALSE, which means that we are interested in the upper tail given the probability condition <span class="math inline">\(P(X^2 &gt; CV)\)</span>; meaning we are looking for the probability that the <strong>chi-square</strong> statistic is greater than the critical value (tabled value).</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" data-line-number="1">(<span class="dt">pvalue =</span> <span class="kw">pchisq</span>(<span class="dt">q =</span> chisqr, <span class="dt">df =</span> <span class="dv">7</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.005486</code></pre>
<p>Here, we reject the <strong>null hypothesis</strong> because our P-value is 0.0055 less than alpha=0.05 based on our confidence level. Alternatively, this conclusion is true because the <strong>Chi-square statistic</strong> is greater than the <strong>critical value</strong> (tabled value), <span class="math inline">\(\mathbf{X^2 &gt; CV}\)</span>, which is 20.039 &gt; 14.067. That is more clear as we discuss the next type of test in which we go further with the analysis by using the <strong>Chi-square distribution</strong>.</p>
<p><span class="math inline">\(\mathbf{\text{Two-Factor}\ X^2\ {Test}}\)</span> (Test of Independence):</p>
<p>Let us now have <span class="math inline">\(\mathbf{\text{Two-Factor}\ X^2\ {Test}}\)</span> and compute for the test statistics.</p>
<p>Suppose we want to survey if there is a relationship between the gender of dog owners and the type of dogs preferred. That is to test if two samples are independent (IID). Again, we narrow down to the same list of cross-breed dogs as before, but this time with the following datasets in a <strong>contingency table</strong>:</p>

<table>
<caption><span id="tab:unnamed-chunk-188">Table 6.10: </span>Cross-Breed Preference</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Male</th>
<th align="right">Female</th>
<th align="right">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="right">140</td>
<td align="right">170</td>
<td align="right">310</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="right">120</td>
<td align="right">280</td>
<td align="right">400</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="right">108</td>
<td align="right">132</td>
<td align="right">240</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="right">200</td>
<td align="right">200</td>
<td align="right">400</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="right">210</td>
<td align="right">210</td>
<td align="right">420</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="right">258</td>
<td align="right">172</td>
<td align="right">430</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="right">75</td>
<td align="right">225</td>
<td align="right">300</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="right">14</td>
<td align="right">266</td>
<td align="right">280</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="right">1125</td>
<td align="right">1655</td>
<td align="right">2780</td>
</tr>
</tbody>
</table>

<p>Let us now tabulate the data to calculate our <strong>Chi-square</strong>. The <strong>expected</strong> column is calculated based on:</p>
<p><span class="math display" id="eq:equate1080028">\[\begin{align}
E_{cell} = \frac{O_j \times O_i}{T} \tag{6.31} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{O_j}\)</span> is the marginal row total</li>
<li><span class="math inline">\(\mathbf{O_i}\)</span> is the marginal column total</li>
<li><span class="math inline">\(T\)</span> is the grand total</li>
</ul>
<p>For example, to compute the expected value for the Male-Cavachon combination, we perform the following:</p>
<p><span class="math display">\[
E_{male-cavachon} = \frac{310 \times 1125}{2780} = 125.4496
\]</span>
</p>
<table>
<caption><span id="tab:unnamed-chunk-189">Table 6.11: </span>Cross-Breed Preference by Gender</caption>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(Obs_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(F-Obs_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(Expected_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(Expected_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(X^2_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(X^2_{(F)}\)</span></th>
<th align="right"><span class="math inline">\(X^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">140</td>
<td align="right">170</td>
<td align="right">125.45</td>
<td align="right">184.6</td>
<td align="right">1.688</td>
<td align="right">1.147</td>
<td align="right">2.835</td>
</tr>
<tr class="even">
<td align="right">120</td>
<td align="right">280</td>
<td align="right">161.87</td>
<td align="right">238.1</td>
<td align="right">10.831</td>
<td align="right">7.362</td>
<td align="right">18.193</td>
</tr>
<tr class="odd">
<td align="right">108</td>
<td align="right">132</td>
<td align="right">97.12</td>
<td align="right">142.9</td>
<td align="right">1.218</td>
<td align="right">0.828</td>
<td align="right">2.046</td>
</tr>
<tr class="even">
<td align="right">200</td>
<td align="right">200</td>
<td align="right">161.87</td>
<td align="right">238.1</td>
<td align="right">8.982</td>
<td align="right">6.105</td>
<td align="right">15.087</td>
</tr>
<tr class="odd">
<td align="right">210</td>
<td align="right">210</td>
<td align="right">169.96</td>
<td align="right">250.0</td>
<td align="right">9.431</td>
<td align="right">6.411</td>
<td align="right">15.841</td>
</tr>
<tr class="even">
<td align="right">258</td>
<td align="right">172</td>
<td align="right">174.01</td>
<td align="right">256.0</td>
<td align="right">40.539</td>
<td align="right">27.557</td>
<td align="right">68.095</td>
</tr>
<tr class="odd">
<td align="right">75</td>
<td align="right">225</td>
<td align="right">121.40</td>
<td align="right">178.6</td>
<td align="right">17.736</td>
<td align="right">12.056</td>
<td align="right">29.793</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">266</td>
<td align="right">113.31</td>
<td align="right">166.7</td>
<td align="right">87.039</td>
<td align="right">59.166</td>
<td align="right">146.205</td>
</tr>
<tr class="odd">
<td align="right">1125</td>
<td align="right">1655</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">177.463</td>
<td align="right">120.632</td>
<td align="right">298.095</td>
</tr>
</tbody>
</table>

<p>Our calculation for the <strong>Degrees of Freedom</strong> is expressed as: <span class="math inline">\(DF = (row - 1)\times(col - 1) = (8-1)\times(2-1) = 7\)</span></p>
<p>Relying on the <strong>Chi-square table</strong> (see the Appendix), the <strong>critical value</strong> is 14.067 for a confidence level of 95% with a degree of freedom at 7. Our <strong>Chi-square statistic</strong> is at 298.0947.</p>
<p>We can use the built-in R function <strong>pchisq</strong> to derive the probability of finding <span class="math inline">\(\mathbf{X^2} \ge CV\)</span>. Here , we have 298.0947 &gt; 14.067:</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" data-line-number="1"><span class="kw">pchisq</span>(<span class="dt">q =</span> ct[<span class="dv">9</span>,<span class="dv">7</span>], <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##     $X^2$ 
## 1.544e-60</code></pre>
<p>The probability is at the extreme right side (right tail), which can also be shown using <strong>PDF</strong> of <strong>Chi-square distribution</strong>.</p>

<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" data-line-number="1">area &lt;-<span class="st"> </span><span class="cf">function</span>(df, a, b, col) {</a>
<a class="sourceLine" id="cb454-2" data-line-number="2">    area =<span class="st"> </span><span class="kw">seq</span>(a, b, <span class="dt">length.out=</span><span class="dv">50</span>) </a>
<a class="sourceLine" id="cb454-3" data-line-number="3">    x =<span class="st"> </span><span class="kw">c</span>(a, area , b)</a>
<a class="sourceLine" id="cb454-4" data-line-number="4">    y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">chi_pdf</span>(area, df), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb454-5" data-line-number="5">    <span class="kw">polygon</span>(x, y, <span class="dt">col=</span>col, <span class="dt">border=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb454-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb454-7" data-line-number="7">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">40</span>)</a>
<a class="sourceLine" id="cb454-8" data-line-number="8"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">0.12</span>), </a>
<a class="sourceLine" id="cb454-9" data-line-number="9">     <span class="dt">xlab=</span><span class="st">&quot;Chi-square value&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;probability density&quot;</span>,</a>
<a class="sourceLine" id="cb454-10" data-line-number="10">     <span class="dt">main=</span><span class="st">&quot;Chi-square Distribution (PDF)&quot;</span>)</a>
<a class="sourceLine" id="cb454-11" data-line-number="11"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb454-12" data-line-number="12"><span class="co"># Using our CDF implementation of Chi-square PDF</span></a>
<a class="sourceLine" id="cb454-13" data-line-number="13"><span class="kw">curve</span>(<span class="kw">chi_pdf</span>(x, df), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb454-14" data-line-number="14"><span class="co"># Using built-in R function &quot;dchisq&quot;</span></a>
<a class="sourceLine" id="cb454-15" data-line-number="15"><span class="co"># curve(dchisq(x, df), col=&quot;navyblue&quot;, lwd=2, add=TRUE)</span></a>
<a class="sourceLine" id="cb454-16" data-line-number="16"><span class="co"># Chi-square value </span></a>
<a class="sourceLine" id="cb454-17" data-line-number="17"><span class="kw">abline</span>(<span class="dt">v =</span> x2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb454-18" data-line-number="18"><span class="kw">text</span>(x2, <span class="fl">0.06</span>, <span class="dt">label=</span><span class="kw">paste</span>(<span class="st">&quot;X^2=&quot;</span>, x2), <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-19" data-line-number="19"><span class="co"># Critical Value</span></a>
<a class="sourceLine" id="cb454-20" data-line-number="20"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">c</span>(cv, <span class="dv">40</span>), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb454-21" data-line-number="21"><span class="kw">text</span>(cv, <span class="fl">0.06</span>, <span class="dt">label=</span><span class="kw">paste</span>(<span class="st">&quot;cv=&quot;</span>, cv), <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-22" data-line-number="22"><span class="co"># draw the alpha = 0.05</span></a>
<a class="sourceLine" id="cb454-23" data-line-number="23"><span class="kw">area</span>(df, cv, <span class="dv">40</span>, <span class="dt">col=</span><span class="st">&quot;lightblue&quot;</span>)</a>
<a class="sourceLine" id="cb454-24" data-line-number="24"><span class="co"># alpha</span></a>
<a class="sourceLine" id="cb454-25" data-line-number="25"><span class="kw">text</span>(<span class="dv">7</span>, <span class="fl">0.005</span>, <span class="dt">label=</span><span class="st">&quot;alpha=0.05&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-26" data-line-number="26"><span class="kw">arrows</span>(<span class="fl">9.5</span>, <span class="fl">0.005</span>, <span class="fl">12.5</span>, <span class="fl">0.005</span>, <span class="dt">length=</span><span class="fl">0.08</span>)</a>
<a class="sourceLine" id="cb454-27" data-line-number="27"><span class="co"># rejection region</span></a>
<a class="sourceLine" id="cb454-28" data-line-number="28"><span class="kw">arrows</span>(cv, <span class="fl">0.02</span>, <span class="dv">40</span>, <span class="fl">0.02</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">code=</span><span class="dv">3</span>, <span class="dt">length=</span><span class="fl">0.08</span>)</a>
<a class="sourceLine" id="cb454-29" data-line-number="29"><span class="kw">text</span>(<span class="dv">31</span>, <span class="fl">0.025</span>, <span class="dt">label=</span><span class="st">&quot;rejection region&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chitest1"></span>
<img src="DS_files/figure-html/chitest1-1.png" alt="Chi-square Distribution (PDF)" width="70%" />
<p class="caption">
Figure 6.12: Chi-square Distribution (PDF)
</p>
</div>

<p>It shows that the <strong>Chi-square</strong> value is within the <strong>rejection region</strong> (<span class="math inline">\(\mathbf{X^2} \ge\)</span> 14.067) past greater than the critical value, given a confidence level of 95%; therefore, we reject the <strong>null hypothesis</strong>. Therefore, the <strong>alternative hypothesis</strong> holds.</p>
<p>Our <strong>null hypothesis</strong> as below is rejected:</p>
<p><span class="math display">\[
H_0\ \ \rightarrow \text{there is no association between gender of owner and cross-breed type}
\]</span></p>
<p>Indeed, there is an association which is what the <strong>alternative hypothesis</strong>, <span class="math inline">\(\mathbf{H_1}\)</span>, claims.</p>
</div>
<div id="wilcoxon-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.8</span> Wilcoxon Test  <a href="6.3-the-significance-of-difference.html#wilcoxon-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we discuss non-parametric tests. The idea is that we do not rely on known parameters such as <strong>mean</strong> or <strong>variance</strong>; thus, we cannot assume that our data follows some normal distribution. Here, we do not assume any distribution. Instead, our test is based on rank.</p>
<p>Here, we discuss two kinds of <strong>Wilcoxon tests</strong>, namely <strong>Wilcoxon Rank Sum Test</strong> and <strong>Wilcoxon Signed-Rank Test</strong> <span class="citation">(Harris T and Hardin J.W. <a href="bibliography.html#ref-ref856t">2013</a>)</span>:</p>
<p><strong>Wilcoxon Rank Sum Test</strong>:</p>
<p><strong>Wilcoxon Rank Sum Test</strong> is a non-parametric test. It is also called the <strong>Mann-Whitney test</strong>. To illustrate, let us use the following dataset:</p>

<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb455-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb455-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb455-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">2</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb455-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb455-6" data-line-number="6"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>)</a>
<a class="sourceLine" id="cb455-7" data-line-number="7">groups</a></code></pre></div>
<pre><code>##         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A    9   10   14   19    8   19   20   15   15     5
## Group B    8    7   15   11   17   12   16   20   11    17</code></pre>

<p>We combine both groups, sort, and then rank each observation.</p>

<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">rank =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,]) ) </a>
<a class="sourceLine" id="cb457-2" data-line-number="2">rank =<span class="st"> </span><span class="kw">rbind</span>( rank, <span class="kw">seq</span>(<span class="dv">1</span>, sample_size<span class="op">*</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb457-3" data-line-number="3"><span class="kw">rownames</span>(rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted&quot;</span>, <span class="st">&quot;Ranked&quot;</span> )</a>
<a class="sourceLine" id="cb457-4" data-line-number="4">rank[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display only first 10 columns</span></a></code></pre></div>
<pre><code>##        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Sorted    5    7    8    8    9   10   11   11   12    14
## Ranked    1    2    3    4    5    6    7    8    9    10</code></pre>

<p>Consecutively identical observations get to share the average of the rank. For example, the first two observations have identical values of 5 and are consecutively ranked 1 and 2; therefore, both will receive a rank of 1.5, derived from (1+2)/2. Moreover, there are three observations with values 11 ranked 10,11,12, respectively; therefore, they will be assigned a rank of 11, derived from (10+11+12)/3.</p>
<p>We then list only the unique observations with their corresponding ranks (this is our rank template):</p>

<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1">adjusted_rank =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">aggregate</span>(rank[<span class="dv">2</span>,], <span class="kw">list</span>(rank[<span class="dv">1</span>,]), mean))</a>
<a class="sourceLine" id="cb459-2" data-line-number="2"><span class="kw">colnames</span>(adjusted_rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted Observ.&quot;</span>, <span class="st">&quot;Adjusted Rank&quot;</span>)</a>
<a class="sourceLine" id="cb459-3" data-line-number="3"><span class="kw">t</span>(adjusted_rank)[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display only first 10 columns</span></a></code></pre></div>
<pre><code>##                [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Sorted Observ.    5    7  8.0    9   10 11.0   12   14   15    16
## Adjusted Rank     1    2  3.5    5    6  7.5    9   10   12    14</code></pre>

<p>We then assign the adjusted rank back to the original group. An easier way to rank is using the built-in R function <strong>rank()</strong> instead:</p>

<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1">builtin.rank =<span class="st"> </span><span class="kw">rank</span>(<span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,]), <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">ranks =<span class="st"> </span><span class="kw">matrix</span>(builtin.rank, <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb462-2" data-line-number="2">sum_rank1 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb462-3" data-line-number="3">sum_rank2 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb462-4" data-line-number="4">ranked_groups =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], ranks[,<span class="dv">1</span>], groups[<span class="dv">2</span>,], </a>
<a class="sourceLine" id="cb462-5" data-line-number="5">        ranks[,<span class="dv">2</span>]),  <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb462-6" data-line-number="6">ranked_groups =<span class="st"> </span><span class="kw">rbind</span>(ranked_groups, <span class="kw">c</span>(<span class="ot">NA</span>, sum_rank1, <span class="ot">NA</span>, sum_rank2))</a>
<a class="sourceLine" id="cb462-7" data-line-number="7"><span class="kw">colnames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Rank A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Rank B&quot;</span>)</a>
<a class="sourceLine" id="cb462-8" data-line-number="8"><span class="kw">rownames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>, sample_size), <span class="st">&quot;Sum&quot;</span>)</a>
<a class="sourceLine" id="cb462-9" data-line-number="9">ranked_groups</a></code></pre></div>
<pre><code>##     Group A Rank A Group B Rank B
## 1         9    5.0       8    3.5
## 2        10    6.0       7    2.0
## 3        14   10.0      15   12.0
## 4        19   17.5      11    7.5
## 5         8    3.5      17   15.5
## 6        19   17.5      12    9.0
## 7        20   19.5      16   14.0
## 8        15   12.0      20   19.5
## 9        15   12.0      11    7.5
## 10        5    1.0      17   15.5
## Sum      NA  104.0      NA  106.0</code></pre>

<p>We compute for the <strong>U statistic</strong> for each group using the following formula:</p>
<p><span class="math display" id="eq:equate1080029">\[\begin{align}
U_{grp} = R_{grp} - \frac{n(n+1)}{2},\ \ \ \ \ where\ R_{grp} = \sum^n_{i=1} rank(x_i) \tag{6.32} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">n =<span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb464-2" data-line-number="2">U_a =<span class="st"> </span>sum_rank1 <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb464-3" data-line-number="3">U_b =<span class="st"> </span>sum_rank2 <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb464-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;U_a&quot;</span> =<span class="st"> </span>U_a, <span class="st">&quot;U_b&quot;</span> =<span class="st"> </span>U_b)</a></code></pre></div>
<pre><code>## U_a U_b 
##  49  51</code></pre>

<p>The final U-statistic is based on the group with lesser U-statistic.</p>


<p>Therefore, <span class="math inline">\(U_{stat}\)</span> = 49</p>
<p>As for the hypothesis, let us use a confidence level of 95%.</p>
<p>Given an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A &gt; Group\ B,
\]</span>
our <strong>critical value</strong> is computed using <strong>qwilcox()</strong>:</p>

<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1">(<span class="dt">U_crit =</span> <span class="dt">U_upper_crit =</span> <span class="kw">qwilcox</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">m =</span> n, <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb466-2" data-line-number="2">                                 <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 72</code></pre>

<p>which shows that <span class="math inline">\(U_{stat}\)</span> &lt; <span class="math inline">\(U_{crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>On the other hand, with an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A &lt; Group\ B,
\]</span></p>
<p>our <strong>critical value</strong> using <strong>qwilcox()</strong> becomes:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1">(<span class="dt">U_crit =</span> <span class="dt">U_lower_crit =</span> <span class="kw">qwilcox</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">m =</span> n, <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb468-2" data-line-number="2">                                 <span class="dt">lower.tail=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## [1] 28</code></pre>
<p>which shows that <span class="math inline">\(U_{stat}\)</span> &gt; <span class="math inline">\(U_{crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>Lastly, with an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A \ne Group\ B,
\]</span></p>
<p>our <strong>critical value</strong> using <strong>qwilcox()</strong> becomes:</p>

<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;min crit&quot;</span>=U_lower_crit, <span class="st">&quot;max crit&quot;</span>=U_upper_crit)</a></code></pre></div>
<pre><code>## min crit max crit 
##       28       72</code></pre>

<p>which shows that <span class="math inline">\(U_{min\_crit} \le U_{stat} \le U_{max\_crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>In terms of pvalue, for a one-sided upper-tail preference, we get:</p>

<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1">(<span class="dt">pvalue =</span> <span class="kw">pwilcox</span>(<span class="dt">q =</span> U_stat, <span class="dt">m =</span> n, <span class="dt">n =</span> n) <span class="op">*</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.9705</code></pre>

<p>To validate, we can use the built-in R function <strong>wilcox.test()</strong>: </p>

<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb474-1" data-line-number="1"><span class="kw">wilcox.test</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], </a>
<a class="sourceLine" id="cb474-2" data-line-number="2">            <span class="dt">correct=</span><span class="ot">FALSE</span>, <span class="dt">paired=</span><span class="ot">FALSE</span>, <span class="dt">exact=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test
## 
## data:  groups[1, ] and groups[2, ]
## W = 49, p-value = 0.9
## alternative hypothesis: true location shift is not equal to 0</code></pre>

<p>Note that we set exact = FALSE. That allows us to perform normal approximation for discrete observations less than 50. Additionally, the function <strong>wilcox.test()</strong> may show message about <strong>P-Value</strong> approximation if observations have ties.</p>
<p><strong>Wilcoxon Signed Rank Test</strong>:</p>
<p><strong>Wilcoxon Signed Rank Test</strong> is the second <strong>Wilcoxon test</strong> we discuss next, and it is also a non-parametric test equivalent to paired <strong>T-Test</strong>. This test utilizes signed-ranks to evaluate the U statistics.  </p>
<p>To illustrate, let us use the same dataset as before but compute for the difference.</p>

<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb476-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb476-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb476-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">2</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb476-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb476-6" data-line-number="6">diff =<span class="st"> </span>groups[<span class="dv">2</span>,] <span class="op">-</span><span class="st"> </span>groups[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb476-7" data-line-number="7">groups =<span class="st"> </span><span class="kw">rbind</span>(groups, diff)</a>
<a class="sourceLine" id="cb476-8" data-line-number="8"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Difference&quot;</span>)</a>
<a class="sourceLine" id="cb476-9" data-line-number="9">groups</a></code></pre></div>
<pre><code>##            [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A       9   10   14   19    8   19   20   15   15     5
## Group B       8    7   15   11   17   12   16   20   11    17
## Difference   -1   -3    1   -8    9   -7   -4    5   -4    12</code></pre>

<p>We then sort and rank the <strong>absolute difference</strong>. The original groups are discarded.</p>

<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1">diff =<span class="st"> </span><span class="kw">sort</span>( groups[<span class="dv">1</span>,] <span class="op">-</span><span class="st"> </span>groups[<span class="dv">2</span>,])</a>
<a class="sourceLine" id="cb478-2" data-line-number="2">builtin.rank =<span class="st"> </span><span class="kw">rank</span>( <span class="kw">abs</span>( diff ), <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb478-3" data-line-number="3">rank =<span class="st"> </span><span class="kw">rbind</span>( diff, builtin.rank)</a>
<a class="sourceLine" id="cb478-4" data-line-number="4"><span class="kw">rownames</span>(rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Difference&quot;</span>, <span class="st">&quot;Rank&quot;</span> )</a>
<a class="sourceLine" id="cb478-5" data-line-number="5">rank</a></code></pre></div>
<pre><code>##            [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Difference  -12   -9   -5 -1.0  1.0    3  4.0  4.0    7     8
## Rank         10    9    6  1.5  1.5    3  4.5  4.5    7     8</code></pre>

<p>Now, for the <strong>signed rank test</strong>, we sum negative groups and positive groups.</p>

<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">mu =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb480-2" data-line-number="2">rank_neg =<span class="st"> </span>rank [<span class="dv">2</span>, <span class="kw">which</span>( diff <span class="op">&lt;</span><span class="st"> </span>mu ) ]</a>
<a class="sourceLine" id="cb480-3" data-line-number="3">U_neg =<span class="st">  </span><span class="kw">sum</span>(rank_neg) </a>
<a class="sourceLine" id="cb480-4" data-line-number="4">rank_pos =<span class="st"> </span>rank [<span class="dv">2</span>, <span class="kw">which</span>( diff <span class="op">&gt;</span><span class="st"> </span>mu ) ]</a>
<a class="sourceLine" id="cb480-5" data-line-number="5">U_pos =<span class="st"> </span><span class="kw">sum</span>(rank_pos) </a>
<a class="sourceLine" id="cb480-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;U_neg&quot;</span> =<span class="st"> </span>U_neg, <span class="st">&quot;U_pos&quot;</span> =<span class="st"> </span>U_pos)</a></code></pre></div>
<pre><code>## U_neg U_pos 
##  26.5  28.5</code></pre>

<p>The U-statistic is based on the group with lesser U statistics.</p>


<p>Therefore, <span class="math inline">\(U_{stat}\)</span> = 26.5</p>
<p>As for the hypothesis, let us use a confidence level of 95% with the following claims for a two-tail test:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \text{there is no difference between Group A and B}\\
H_1 &amp;:  \text{there is a difference between  Group A and B}
\end{align*}\]</span></p>
<p>Let us use <strong>qsignrank()</strong> to compute for critical value:</p>

<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" data-line-number="1">U_min =<span class="st"> </span><span class="kw">qsignrank</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">n =</span> n, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb482-2" data-line-number="2">U_max =<span class="st"> </span><span class="kw">qsignrank</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">n =</span> n, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb482-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;lower&quot;</span>=U_min, <span class="st">&quot;upper&quot;</span>=U_max)</a></code></pre></div>
<pre><code>## lower upper 
##     9    46</code></pre>

<p>which shows that <span class="math inline">\(U_{min\_crit} \le U_{stat} \le U_{max\_crit}\)</span>, resulting to a rejection of <span class="math inline">\(H_0\)</span>.</p>
<p>Let us now compute for the <strong>z-value</strong> by normalizing and standardizing our U statistic:</p>
<p><span class="math display" id="eq:equate1080030">\[\begin{align}
Z = \frac{U - E_{H_0}(U)}{\sqrt{VAR_{H_0}(U)}} = \frac{U - \mu_u}{\sigma_u} \tag{6.33} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1080031">\[\begin{align}
\mu_u = \frac{n(n+1)}{4}\ \ \ \ \ \ \ \ \sigma_u = \frac{n(n+1)(2n+1)}{24} \tag{6.34} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" data-line-number="1">n =<span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb484-2" data-line-number="2">mu =<span class="st"> </span>( n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">4</span> )</a>
<a class="sourceLine" id="cb484-3" data-line-number="3">var =<span class="st"> </span>(n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))<span class="op">/</span><span class="st"> </span><span class="dv">24</span></a>
<a class="sourceLine" id="cb484-4" data-line-number="4">(<span class="dt">zvalue =</span> (U_stat <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var))</a></code></pre></div>
<pre><code>## [1] -0.1019</code></pre>

<p>And for our <strong>P-value</strong>, we use <strong>pmon()</strong> instead with <strong>Z-value</strong>:</p>

<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb486-1" data-line-number="1"><span class="kw">pnorm</span>( <span class="dt">q =</span> zvalue, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span> ) <span class="op">*</span><span class="st"> </span><span class="dv">2</span></a></code></pre></div>
<pre><code>## [1] 0.9188</code></pre>

<p>To validate, we can use the built-in R function <strong>wilcox.test()</strong>:</p>

<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1"><span class="kw">wilcox.test</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, </a>
<a class="sourceLine" id="cb488-2" data-line-number="2">            <span class="dt">correct=</span><span class="ot">FALSE</span>, <span class="dt">paired=</span><span class="ot">TRUE</span>, <span class="dt">exact=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test
## 
## data:  groups[1, ] and groups[2, ]
## V = 28, p-value = 0.9
## alternative hypothesis: true location shift is not equal to 0</code></pre>

<p>We set the <strong>exact</strong> parameter of the function equal to FALSE to force approximation of the distribution to a normal distribution; though, we do not depend on the distribution. As we can see, the approximation is close to the result of <strong>pnorm(.)</strong>.</p>
<p>There are other <strong>Wilcoxon Tests</strong> available to use. We leave the rest for readers to investigate:</p>
<ul>
<li>Wilcoxon matched-pairs signed-rank test</li>
</ul>
</div>
<div id="kruskal-wallis-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.9</span> Kruskal-Wallis Test <a href="6.3-the-significance-of-difference.html#kruskal-wallis-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kruskal-Wallis</strong> test is also a non-parametric ranked sum test similar to <strong>Mann-Whitney</strong> in that we cannot assume about the distribution (for example, using mean and standard deviation). However, we can also compare two or more groups and rank each observation <span class="citation">(Chan Y. and Walmsley R.P. <a href="bibliography.html#ref-ref865y">1997</a>)</span>.</p>
<p>To illustrate, let us use the same dataset as before.</p>

<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb490-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb490-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb490-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">3</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb490-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb490-6" data-line-number="6"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Group C&quot;</span>)</a>
<a class="sourceLine" id="cb490-7" data-line-number="7">groups</a></code></pre></div>
<pre><code>##         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A   15   11   14   12    7    6    7   11    5    14
## Group B   17   16   18   11   11   13   20   15   13     9
## Group C    8    6   18   20   19    7   10   16   12    13</code></pre>

<p>We then rank the groups in the same manner as shown in <strong>Mann-Whitney test</strong>:</p>

<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1">builtin.rank =<span class="st"> </span><span class="kw">rank</span>(<span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], groups[<span class="dv">3</span>,]), </a>
<a class="sourceLine" id="cb492-2" data-line-number="2">                    <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb492-3" data-line-number="3">ranks =<span class="st"> </span><span class="kw">matrix</span>(builtin.rank, <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb492-4" data-line-number="4">R1 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">1</span>]); R2 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">2</span>]); R3 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb492-5" data-line-number="5">ranked_groups =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], ranks[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb492-6" data-line-number="6">                          groups[<span class="dv">2</span>,], ranks[,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb492-7" data-line-number="7">                          groups[<span class="dv">3</span>,], ranks[,<span class="dv">3</span>] ),</a>
<a class="sourceLine" id="cb492-8" data-line-number="8">        <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">6</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb492-9" data-line-number="9">ranked_groups =<span class="st"> </span><span class="kw">rbind</span>(ranked_groups,  <span class="kw">c</span>(<span class="ot">NA</span>, R1, <span class="ot">NA</span>, R2, <span class="ot">NA</span>, R3))</a>
<a class="sourceLine" id="cb492-10" data-line-number="10"><span class="kw">colnames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Rank A&quot;</span>, </a>
<a class="sourceLine" id="cb492-11" data-line-number="11">                            <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Rank B&quot;</span>,</a>
<a class="sourceLine" id="cb492-12" data-line-number="12">                            <span class="st">&quot;Group C&quot;</span>, <span class="st">&quot;Rank C&quot;</span>)</a>
<a class="sourceLine" id="cb492-13" data-line-number="13"><span class="kw">rownames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>, sample_size), <span class="st">&quot;Sum&quot;</span>)</a>
<a class="sourceLine" id="cb492-14" data-line-number="14">ranked_groups</a></code></pre></div>
<pre><code>##     Group A Rank A Group B Rank B Group C Rank C
## 1        15   21.5      17   25.0       8    7.0
## 2        11   11.5      16   23.5       6    2.5
## 3        14   19.5      18   26.5      18   26.5
## 4        12   14.5      11   11.5      20   29.5
## 5         7    5.0      11   11.5      19   28.0
## 6         6    2.5      13   17.0       7    5.0
## 7         7    5.0      20   29.5      10    9.0
## 8        11   11.5      15   21.5      16   23.5
## 9         5    1.0      13   17.0      12   14.5
## 10       14   19.5       9    8.0      13   17.0
## Sum      NA  111.5      NA  191.0      NA  162.5</code></pre>

<p>The formula to compute for the K-statistic is as follows:</p>
<p><span class="math display" id="eq:equate1080032">\[\begin{align}
H = \left(\frac{12}{N(N+1)}\sum^k_{i=1}{\frac{R^2_i}{n_i}}\right) - 3(N + 1) \tag{6.35} 
\end{align}\]</span></p>
<p>Note that the K-statistic assumes a chi-square distribution as shown when using <strong>kruskal.test(.)</strong> function later.</p>

<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1">k =<span class="st"> </span><span class="dv">3</span> <span class="co"># total number of groups</span></a>
<a class="sourceLine" id="cb494-2" data-line-number="2">n =<span class="st"> </span>sample_size <span class="co"># size per group</span></a>
<a class="sourceLine" id="cb494-3" data-line-number="3">N =<span class="st"> </span>n <span class="op">*</span><span class="st"> </span>k</a>
<a class="sourceLine" id="cb494-4" data-line-number="4">(<span class="dt">H_stat =</span> (<span class="dv">12</span><span class="op">/</span>(N <span class="op">*</span><span class="st"> </span>(N<span class="op">+</span><span class="dv">1</span>))) <span class="op">*</span><span class="st"> </span>(R1<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>R2<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>R3<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n) <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>(N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 4.186</code></pre>

<p>We use <strong>Chi-Square functions</strong> to generate the <strong>Critical value</strong> and <strong>P-value</strong>:</p>

<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb496-1" data-line-number="1">alpha=<span class="fl">0.05</span></a>
<a class="sourceLine" id="cb496-2" data-line-number="2">df =<span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb496-3" data-line-number="3">(<span class="dt">H_crit  =</span> <span class="kw">qchisq</span>(<span class="dt">p=</span>alpha, <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 5.991</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb498-1" data-line-number="1">(<span class="dt">p_value =</span> <span class="kw">pchisq</span>(H_stat, <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.1233</code></pre>

<p>If our <strong>null hypothesis</strong> is expressed as below (with a confidence level of 95%):</p>
<p><span class="math display">\[
H_0: \text{there is no difference between groups}
\]</span>
then, we reject <span class="math inline">\(H_0\)</span> because the result shows that <span class="math inline">\(H_{stat}\)</span> &lt; <span class="math inline">\(H_{crit}\)</span>. There is significant difference between groups.</p>
<p>To validate, we can use the built-in R function <strong>kruskal.test()</strong>:</p>

<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb500-1" data-line-number="1"><span class="co"># Perform Rank Sum Test</span></a>
<a class="sourceLine" id="cb500-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(groups[<span class="dv">1</span>,])</a>
<a class="sourceLine" id="cb500-3" data-line-number="3">data =<span class="st"> </span><span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], groups[<span class="dv">3</span>,])</a>
<a class="sourceLine" id="cb500-4" data-line-number="4">grouping =<span class="st"> </span><span class="kw">as.factor</span> (<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="dv">2</span>, n), <span class="kw">rep</span>(<span class="dv">3</span>, n)) )</a>
<a class="sourceLine" id="cb500-5" data-line-number="5"><span class="kw">kruskal.test</span>(data, grouping)</a></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  data and grouping
## Kruskal-Wallis chi-squared = 4.2, df = 2, p-value = 0.1</code></pre>

</div>
<div id="friedman-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.10</span> Friedman Test <a href="6.3-the-significance-of-difference.html#friedman-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Friedman</strong> Test is non-parametric test with an <strong>F-statistic</strong> following a <strong>Chi-square distribution</strong>.</p>
<p>We leave readers to investigate <strong>Friedman Test</strong>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="6.2-inferential-statistics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="6.4-post-hoc-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
