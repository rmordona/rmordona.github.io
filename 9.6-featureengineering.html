<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.6 Feature Engineering | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="9.6 Feature Engineering | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.6 Feature Engineering | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="9.5-exploratory-data-analysis.html"/>
<link rel="next" href="9.7-general-modeling.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="featureengineering" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.6</span> Feature Engineering<a href="9.6-featureengineering.html#featureengineering" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The essential primary ingredient in computational learning - or machine learning <strong>(ML)</strong> - is the input. We define <strong>input</strong> as any observable and measurable property of an entity or phenomenon (Wikipedia). Our goal is to study and analyze the <strong>input</strong> - a raw material (a raw metric) - and determine its importance and relevance. Any <strong>input</strong> extracted and selected based on domain knowledge is called a candidate <strong>feature</strong>. That is where <strong>Feature Engineering</strong> comes into play. </p>
<div id="machine-learning-features" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.1</span> Machine Learning Features<a href="9.6-featureengineering.html#machine-learning-features" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Machine Learning, we introduce terms such as <strong>Feature(s)</strong> and <strong>Label(s)</strong>, <strong>Input(s)</strong> and <strong>output(s)</strong>. See Table <a href="9.6-featureengineering.html#tab:nicevariables1">9.34</a> for equivalent terms.</p>
<table>
<caption><span id="tab:nicevariables1">Table 9.34: </span>Variables</caption>
<thead>
<tr class="header">
<th align="left">Answer</th>
<th align="left">Question</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Dependent variable</td>
<td align="left">Independent variables</td>
</tr>
<tr class="even">
<td align="left">Response variable (Variate)</td>
<td align="left">Predictor variables (Regressor / Covariate)</td>
</tr>
<tr class="odd">
<td align="left">Explained variable</td>
<td align="left">Explanatory variables</td>
</tr>
<tr class="even">
<td align="left">Label</td>
<td align="left">Features</td>
</tr>
<tr class="odd">
<td align="left">Output</td>
<td align="left">Inputs</td>
</tr>
</tbody>
</table>
<p>Barring statistics for a moment, we can treat features as traits, characteristics, or properties of entities. They describe the profile or personality of entities. For example, the weather has properties such as humidity, temperature, and wind velocity. Another example is a person with unique traits and characteristics that embody the personality of the individual. With machine learning data, we also need to give a name for the person. In other words, we need to label the personality. In essence, a label represents an objectâs attributes. While we explain this concept using objects, machine learning data is not limited to just characterizing an object. We can treat features also in terms of inputs that result in an output (the label). As in statistics, any input may have the quality of being observable and measurable and thus can result in a definitive output that can be labeled. For example, humidity, temperature, and wind velocity are inputs (features) in our study of the effect of climate change. The effect of climate change represents the focus of our study and our labeling efforts.</p>
<p>Recall that while we are interested in data that is only observable and measurable entities, we prefer to narrow down data into features that contribute significance and relevance for analysis in data science. Here, it is important to note that not all such attributes or features can <strong>contribute</strong> or <strong>influence</strong>; therefore, we have algorithms that reduce the number of <strong>Features</strong> to those more <strong>relevant</strong>. </p>
<p>There seems to be a consensus about the lack of a unified formal definition of <strong>Feature Engineering</strong>. If so, perhaps we can prescribe our overarching definition with no intention to solicit merit. Let us first describe the basic premise of <strong>Engineering</strong>, which is a systematic and creative application of scientific, mathematical, theoretical, and empirical knowledge (sourced from britannica.com, linkengineering.org, bccampus.ca) to produce consumables (usable products) from raw materials. With that said, it seems sensible to regard <strong>Feature Engineering</strong> as both methodological (bearing the application of math and science) and ideological (bearing the demand for artful creativity) in the context of creating, selecting or extracting, and transforming features, bearing in mind the domain for which we apply such processes. One has to develop a system of methodologies describing the creation and reduction of features from a set of raw inputs unique to a domain. The goal is to eliminate <strong>redundant</strong> and <strong>irrelevant</strong> information. Furthermore, the intent is to enhance the predictive power of models, increase the accuracy of output, and enhance the performance of algorithms, among many other benefits.</p>
<p>In this section, our view of <strong>Feature Engineering</strong> covers a system of three overlapping methodologies: <strong>Construction</strong>, <strong>Selection</strong>, and <strong>Transformation</strong> of <strong>Features</strong>. In some cases, these methodologies are more ideological, and thus other literature follows specific descriptions, categories, or order. However, ultimately, we follow the <strong>GIGO</strong> philosophy - garbage in, garbage out. In other words, we are more unified to believe that a solid, relevant feature provides more valuable and informative data that guides us toward better decision choices.</p>
</div>
<div id="dimensionality-reduction" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.2</span> Dimensionality Reduction <a href="9.6-featureengineering.html#dimensionality-reduction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dimension</strong> in <strong>ML</strong> refers to the number of features in a dataset (whether they are relevant, irrelevant, or redundant). A highly dimensional dataset means that the dataset contains a high number of features. Therefore, it is sensical to reduce the number of dimensions in our dataset to a minimum - this is called <strong>dimensionality reduction</strong>.</p>
<p>The following sections cover strategies that allow us to perform such dimensionality reduction, starting with <strong>PCA</strong>.</p>
</div>
<div id="principal-component-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.3</span> Principal Component Analysis  <a href="9.6-featureengineering.html#principal-component-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Principal Component Analysis (PCA)</strong> is widely discussed and explained on the net as one of the many dimensionality reduction methods. Here, we may have to recall concepts such as <strong>Eigenvectors</strong>, <strong>EigenValues</strong>, and <strong>Singular Valued Decomposition (SVD)</strong> discussed in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). In addition, the concept of <strong>Covariance</strong> is also needed. It is briefly covered under the <strong>EDA</strong> section in this Chapter and Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) under the <strong>Types of Matrices</strong> Section.</p>
<p>To get the intuition behind <strong>PCA</strong>, it helps to compare it with <strong>Ordinary Least Square (OLS)</strong>, which we covered in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) (see Figure <a href="9.6-featureengineering.html#fig:olspca">9.49</a>). In <strong>OLS</strong>, we try to find the best fit through the data points by computing the <strong>minimum sum square</strong> of all <strong>vertical</strong> distances between the data points and the line. In other words, we <strong>compute for the least residual</strong>. The line becomes the <strong>model</strong> upon which we predict values of <strong>Y</strong> based on a given <strong>X</strong>. In <strong>PCA</strong>, it is essential to note in the figure that we are using <strong>X1</strong> and <strong>X2</strong> axes to indicate that we are instead of comparing two independent variables. Conceptually, in <strong>PCA</strong>, we also try to find the best fit of the line through the data points; however, the <strong>fitted line</strong> is a projection that represents the spread (or variance) of the data points being projected. The goal is to find the largest variance by computing the <strong>maximum sum square</strong> of all distances between the projected points and the point of origin, with the corresponding <strong>Principal axis</strong>.</p>
<p>Equivalently, we can also compute the least square distance of the orthogonal projection to the corresponding <strong>Principal axis</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:olspca"></span>
<img src="ols_pca.png" alt="OLS vs PCA" width="100%" />
<p class="caption">
Figure 9.49: OLS vs PCA
</p>
</div>
<p>To illustrate, let us first generate our own dataset:</p>

<div class="sourceCode" id="cb1078"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1078-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb1078-2" data-line-number="2">N =<span class="st"> </span><span class="dv">25</span></a>
<a class="sourceLine" id="cb1078-3" data-line-number="3">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1078-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">sample</span>(range, <span class="dt">size=</span>N<span class="op">*</span><span class="dv">4</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>), <span class="dt">nrow=</span>N, <span class="dt">ncol=</span><span class="dv">4</span>) </a>
<a class="sourceLine" id="cb1078-5" data-line-number="5"><span class="kw">colnames</span>(dataset) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>, <span class="st">&quot;feature4&quot;</span>)</a>
<a class="sourceLine" id="cb1078-6" data-line-number="6"><span class="kw">head</span>(dataset)</a></code></pre></div>
<pre><code>##      feature1 feature2 feature3 feature4
## [1,]       77        2       31      100
## [2,]       72       60        9       13
## [3,]       31       95        4       52
## [4,]       62       63       35       34
## [5,]        6       46       52       77
## [6,]        5       65       38        5</code></pre>

<p><strong>Second</strong>, we perform centering and scaling for our dataset by standardization. Note that our scaling and centering are column-wise. See <strong>standardization</strong> in <strong>Primitive Analytical Methods</strong> section. Hereafter, we only use centering. We defer scaling for now and illustrate the need to manually scale to <strong>unit eigenvector</strong> in further discussion later.</p>
<div class="sourceCode" id="cb1080"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1080-1" data-line-number="1"><span class="kw">head</span>((<span class="dt">std.dataset =</span> <span class="kw">scale</span>(dataset, <span class="dt">center=</span><span class="ot">TRUE</span>, <span class="dt">scale=</span><span class="ot">FALSE</span>))[,])</a></code></pre></div>
<pre><code>##      feature1 feature2 feature3 feature4
## [1,]    32.24   -46.24   -10.52    43.16
## [2,]    27.24    11.76   -32.52   -43.84
## [3,]   -13.76    46.76   -37.52    -4.84
## [4,]    17.24    14.76    -6.52   -22.84
## [5,]   -38.76    -2.24    10.48    20.16
## [6,]   -39.76    16.76    -3.52   -51.84</code></pre>
<p><strong>Third</strong>, for simple explanation of <strong>PCA</strong>, we use feature1 and feature2.</p>
<div class="sourceCode" id="cb1082"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1082-1" data-line-number="1">X =<span class="st"> </span>std.sample =<span class="st"> </span>std.dataset[,<span class="kw">c</span>(<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>)]</a>
<a class="sourceLine" id="cb1082-2" data-line-number="2"><span class="kw">head</span>(X)</a></code></pre></div>
<pre><code>##      feature1 feature2
## [1,]    32.24   -46.24
## [2,]    27.24    11.76
## [3,]   -13.76    46.76
## [4,]    17.24    14.76
## [5,]   -38.76    -2.24
## [6,]   -39.76    16.76</code></pre>
<p><strong>Fourth</strong>, we compute for the <strong>Principal components</strong> using the <strong>svd(.)</strong> function. Now, recall that we covered <strong>Single Valued Decomposition (SVD)</strong> under <strong>Matrix Factorization</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). Here, we use the following <strong>SVD</strong> equation:</p>
<p><span class="math display" id="eq:equate1110040">\[\begin{align}
X = UDV^T \tag{9.42} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li>X is the matrix (e.g., our dataset - centered and scaled).</li>
<li>U holds the left <strong>eigenvectors</strong>, also called the <strong>loading</strong> matrix.</li>
<li>D is the standard deviation (diagonal) for rotation.</li>
<li><span class="math inline">\(V\)</span> holds the right <strong>eigenvectors</strong>, also called the <strong>scores</strong> matrix.</li>
</ul>

<div class="sourceCode" id="cb1084"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1084-1" data-line-number="1">svd.model =<span class="st"> </span><span class="kw">svd</span>(std.sample)</a>
<a class="sourceLine" id="cb1084-2" data-line-number="2">(<span class="dt">D =</span> svd.model<span class="op">$</span>d)</a></code></pre></div>
<pre><code>## [1] 159.9 140.2</code></pre>
<div class="sourceCode" id="cb1086"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1086-1" data-line-number="1">(<span class="dt">U =</span> svd.model<span class="op">$</span>u)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##          [,1]     [,2]
## [1,]  0.20133  0.32999
## [2,] -0.12804  0.15314
## [3,] -0.24445 -0.20766
## [4,] -0.12396  0.07883
## [5,]  0.09718 -0.25373</code></pre>
<div class="sourceCode" id="cb1088"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1088-1" data-line-number="1">(<span class="dt">V =</span> svd.model<span class="op">$</span>v)</a></code></pre></div>
<pre><code>##         [,1]    [,2]
## [1,] -0.3467  0.9380
## [2,] -0.9380 -0.3467</code></pre>

<p>We then compute the <strong>PCs</strong> using the following formula:</p>

<p><span class="math display" id="eq:equate1110041">\[\begin{align}
PC = X \cdot V  \tag{9.43} 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1090"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1090-1" data-line-number="1">(<span class="dt">PC =</span> X <span class="op">%*%</span><span class="st"> </span>V)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>

<p>The second way is to compute for the <strong>covariance</strong> of our dataset and then use the <strong>eigen(.)</strong> function to compute for the <strong>EigenVectors</strong> and <strong>EigenValues</strong> using the <strong>covariance matrix</strong>. The covariance between the two features is computed using the following formula (for a large number of features, it may help to decompose the matrix using <strong>Cholesky Factorization</strong>): </p>
<p><span class="math display" id="eq:equate1110042">\[\begin{align}
Cov(X) = \frac{(X - \bar{X})^T(X - \bar{X})}{N - 1} \tag{9.44} 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1092"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1092-1" data-line-number="1">X.mean =<span class="st"> </span><span class="kw">apply</span>(X, <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1092-2" data-line-number="2">(<span class="dt">cov.features =</span> ( <span class="kw">t</span>(X <span class="op">-</span><span class="st"> </span>X.mean) <span class="op">%*%</span><span class="st"> </span>(X <span class="op">-</span><span class="st"> </span>X.mean)  ) <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>##          feature1 feature2
## feature1   848.86    80.06
## feature2    80.06  1035.86</code></pre>
<p>The same covariance can be computed using the <strong>cov(.)</strong> function.</p>
<div class="sourceCode" id="cb1094"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1094-1" data-line-number="1">(<span class="dt">cov.features =</span> <span class="kw">cov</span>(std.sample))</a></code></pre></div>
<pre><code>##          feature1 feature2
## feature1   848.86    80.06
## feature2    80.06  1035.86</code></pre>
<p>We first compute for the <strong>EigenValues</strong> and <strong>EigenVectors</strong>.</p>

<div class="sourceCode" id="cb1096"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1096-1" data-line-number="1">p.eigen =<span class="st"> </span><span class="kw">eigen</span>(cov.features)</a>
<a class="sourceLine" id="cb1096-2" data-line-number="2">(<span class="dt">eigenVal =</span> p.eigen<span class="op">$</span>values)  <span class="co"># EigenValues</span></a></code></pre></div>
<pre><code>## [1] 1065.4  819.3</code></pre>
<div class="sourceCode" id="cb1098"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1098-1" data-line-number="1">(<span class="dt">eigenVec =</span> p.eigen<span class="op">$</span>vectors) <span class="co"># EigenVectors</span></a></code></pre></div>
<pre><code>##        [,1]    [,2]
## [1,] 0.3467 -0.9380
## [2,] 0.9380  0.3467</code></pre>

<p>Then we compute for the <strong>PCs</strong>:</p>
<div class="sourceCode" id="cb1100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1100-1" data-line-number="1">(<span class="dt">PC =</span> X <span class="op">%*%</span><span class="st"> </span>eigenVec)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##        [,1]   [,2]
## [1,] -32.19 -46.27
## [2,]  20.47 -21.47
## [3,]  39.09  29.12
## [4,]  19.82 -11.05
## [5,] -15.54  35.58</code></pre>
<p>Note that the expanded version of the <strong>PCA</strong> formula is as such:</p>
<p><span class="math display" id="eq:equate1110043">\[\begin{align}
PC = X \cdot V = \sum_i^n X_i \times V_i  \tag{9.45} 
\end{align}\]</span></p>
<p>In this case, it is easy to just perform the computation with two features:</p>

<div class="sourceCode" id="cb1102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1102-1" data-line-number="1">V =<span class="st"> </span>loadings =<span class="st"> </span>eigenVec</a>
<a class="sourceLine" id="cb1102-2" data-line-number="2">PC1 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1102-3" data-line-number="3">PC2 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1102-4" data-line-number="4">(<span class="dt">PC =</span> <span class="kw">cbind</span>(PC1, PC2))[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,] -32.19 -46.27
## [2,]  20.47 -21.47
## [3,]  39.09  29.12
## [4,]  19.82 -11.05
## [5,] -15.54  35.58</code></pre>

<p>The third way is using the <strong>prcomp(.)</strong> function which readily computes for the <strong>PCs</strong>.</p>
<div class="sourceCode" id="cb1104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1104-1" data-line-number="1">(<span class="dt">pca.model =</span> <span class="kw">prcomp</span>(X))</a></code></pre></div>
<pre><code>## Standard deviations (1, .., p=2):
## [1] 32.64 28.62
## 
## Rotation (n x k) = (2 x 2):
##              PC1     PC2
## feature1 -0.3467  0.9380
## feature2 -0.9380 -0.3467</code></pre>
<p>We can get the <strong>PCs</strong> via the <strong>PCA model</strong> produced:</p>
<div class="sourceCode" id="cb1106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1106-1" data-line-number="1">(<span class="dt">scores =</span> pca.model<span class="op">$</span>x)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,] <span class="co"># show only first 5 rows</span></a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>
<p>The same <strong>PCA model</strong> has the <strong>Eigenvectors</strong> which we can use to also compute for <strong>PCA</strong>:</p>
<div class="sourceCode" id="cb1108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1108-1" data-line-number="1">V =<span class="st"> </span>loadings =<span class="st"> </span>pca.model<span class="op">$</span>rotation <span class="co"># eigenvectors</span></a>
<a class="sourceLine" id="cb1108-2" data-line-number="2">PC1 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1108-3" data-line-number="3">PC2 =<span class="st"> </span>X[,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1108-4" data-line-number="4">(<span class="dt">PC =</span> <span class="kw">cbind</span>(PC1, PC2))[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##         PC1    PC2
## [1,]  32.19  46.27
## [2,] -20.47  21.47
## [3,] -39.09 -29.12
## [4,] -19.82  11.05
## [5,]  15.54 -35.58</code></pre>
<p>We can now see that a <strong>Principal Component</strong> makes up of a linear combination - a dot product of the data points and corresponding eigenvectors. We will expound on this idea later.</p>
<p><strong>Fifth</strong>, let us plot the model (see Figure <a href="9.6-featureengineering.html#fig:pca1">9.50</a>). The idea is to find the maximum deviation (or variance) of the two features. We maximize the distance between the <strong>projected points</strong> to the point of origin.</p>

<div class="sourceCode" id="cb1110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1110-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>)</a>
<a class="sourceLine" id="cb1110-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1110-3" data-line-number="3"><span class="co">### Plotting the first Principal Component (PC1)</span></a>
<a class="sourceLine" id="cb1110-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1110-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1110-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;Maximum Variance (PC1)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1110-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1110-8" data-line-number="8"><span class="co">### Intercept and Slope of PC1</span></a>
<a class="sourceLine" id="cb1110-9" data-line-number="9">pc1.intrcpt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1110-10" data-line-number="10">pc1.slope =<span class="st"> </span>eigenVec[<span class="dv">2</span>,<span class="dv">1</span>]<span class="op">/</span>eigenVec[<span class="dv">1</span>,<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1110-11" data-line-number="11"><span class="kw">lines</span>(X[,<span class="dv">1</span>], pc1.slope <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>], <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1110-12" data-line-number="12"><span class="co">### Projection to PC1</span></a>
<a class="sourceLine" id="cb1110-13" data-line-number="13">slope.orthog =<span class="st"> </span><span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>pc1.slope)</a>
<a class="sourceLine" id="cb1110-14" data-line-number="14">intrcpt.orthog =<span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>slope.orthog <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1110-15" data-line-number="15">x.proj =<span class="st"> </span>pc1.x.proj =<span class="st"> </span>(pc1.intrcpt <span class="op">-</span><span class="st"> </span>intrcpt.orthog) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1110-16" data-line-number="16"><span class="st">                      </span>(slope.orthog <span class="op">-</span><span class="st"> </span>pc1.slope) </a>
<a class="sourceLine" id="cb1110-17" data-line-number="17">y.proj =<span class="st"> </span>pc1.y.proj =<span class="st"> </span>pc1.slope <span class="op">*</span><span class="st"> </span>x.proj  <span class="op">+</span><span class="st"> </span>pc1.intrcpt</a>
<a class="sourceLine" id="cb1110-18" data-line-number="18"><span class="kw">segments</span>(X[,<span class="dv">1</span>], X[,<span class="dv">2</span>], x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1110-19" data-line-number="19"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1110-20" data-line-number="20"><span class="kw">points</span>(X, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-21" data-line-number="21"><span class="kw">points</span>(x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-22" data-line-number="22"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1110-23" data-line-number="23"><span class="co">### Legend and PC1 label</span></a>
<a class="sourceLine" id="cb1110-24" data-line-number="24"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1110-25" data-line-number="25">    <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Projected Points&quot;</span>, <span class="st">&quot;Point of Origin&quot;</span>),</a>
<a class="sourceLine" id="cb1110-26" data-line-number="26">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-27" data-line-number="27"><span class="kw">text</span>(<span class="op">-</span><span class="dv">55</span>,<span class="dv">60</span>, <span class="dt">label=</span><span class="st">&quot;PC1&quot;</span>)</a>
<a class="sourceLine" id="cb1110-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1110-29" data-line-number="29"><span class="co">### Plotting the second Principal Component (PC2)</span></a>
<a class="sourceLine" id="cb1110-30" data-line-number="30"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1110-31" data-line-number="31">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1110-32" data-line-number="32">     <span class="dt">main=</span><span class="st">&quot;Maximum Variance (PC2)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1110-33" data-line-number="33"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1110-34" data-line-number="34"><span class="co">### Intercept and Slope of PC2</span></a>
<a class="sourceLine" id="cb1110-35" data-line-number="35">pc2.intrcpt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1110-36" data-line-number="36">pc2.slope =<span class="st"> </span>eigenVec[<span class="dv">2</span>,<span class="dv">2</span>]<span class="op">/</span>eigenVec[<span class="dv">1</span>,<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1110-37" data-line-number="37"><span class="kw">lines</span>(X[,<span class="dv">2</span>], pc2.slope <span class="op">*</span><span class="st"> </span>X[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1110-38" data-line-number="38"><span class="co">### Projection to PC2</span></a>
<a class="sourceLine" id="cb1110-39" data-line-number="39">slope.orthog =<span class="st"> </span><span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>pc2.slope)</a>
<a class="sourceLine" id="cb1110-40" data-line-number="40">intrcpt.orthog =<span class="st"> </span>X[,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>slope.orthog <span class="op">*</span><span class="st"> </span>X[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1110-41" data-line-number="41">x.proj =<span class="st"> </span>pc2.x.proj =<span class="st"> </span>(pc2.intrcpt <span class="op">-</span><span class="st"> </span>intrcpt.orthog) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1110-42" data-line-number="42"><span class="st">                      </span>(slope.orthog <span class="op">-</span><span class="st"> </span>pc2.slope) </a>
<a class="sourceLine" id="cb1110-43" data-line-number="43">y.proj =<span class="st"> </span>pc2.y.proj =<span class="st"> </span>pc2.slope <span class="op">*</span><span class="st"> </span>x.proj  <span class="op">+</span><span class="st"> </span>pc2.intrcpt</a>
<a class="sourceLine" id="cb1110-44" data-line-number="44"><span class="kw">segments</span>(X[,<span class="dv">1</span>], X[,<span class="dv">2</span>], x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1110-45" data-line-number="45"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1110-46" data-line-number="46"><span class="kw">points</span>(X, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-47" data-line-number="47"><span class="kw">points</span>(x.proj, y.proj, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-48" data-line-number="48"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1110-49" data-line-number="49"><span class="co">### Legend and PC2 label</span></a>
<a class="sourceLine" id="cb1110-50" data-line-number="50"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1110-51" data-line-number="51">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Projected Points&quot;</span>, <span class="st">&quot;Point of Origin&quot;</span>),</a>
<a class="sourceLine" id="cb1110-52" data-line-number="52">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1110-53" data-line-number="53"><span class="kw">text</span>(<span class="op">-</span><span class="dv">50</span>,<span class="op">-</span><span class="dv">40</span>, <span class="dt">label=</span><span class="st">&quot;PC2&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca1"></span>
<img src="DS_files/figure-html/pca1-1.png" alt="Principal Component Analysis (PCA)" width="100%" />
<p class="caption">
Figure 9.50: Principal Component Analysis (PCA)
</p>
</div>

<p>Here, it helps to know the terminologies used in <strong>SVD</strong> and <strong>PCA</strong>. We start by describing <strong>Principal Component</strong>. Geometrically, a <strong>Principal Component (PC1)</strong> is represented by a <strong>Principal axis</strong>, oriented in the direction that maximizes the variance (given two features). We also term this <strong>Principal direction</strong>. Data points are projected to this <strong>Principal axis</strong>. Then, the <strong>sum of squared distance (SSD)</strong> of all the projected points from the point of origin along the <strong>Principal axis</strong> is calculated. See below:</p>
<p><span class="math display" id="eq:equate1110044">\[\begin{align}
\text{SSD}_{max} = \underset{ssd}{max} \left\{ \sum_i^n d_i^2 \right\}=  \underset{ssd}{max} \left\{d_1^2 + d_2^2 + d_3^2 + ... + d_n^2 \right\} \tag{9.46} 
\end{align}\]</span></p>
<p>To get the maximum SSD, we may need to visualize the geometric representation of the <strong>Principal Component</strong> as it rotates around the point of origin until such that we find the maximum variance.</p>
<p>A second <strong>Principal Component (PC2)</strong> inherently gets created, which is also geometrically represented by a <strong>Principal axis</strong> that is orthogonal (or perpendicular) to the primary <strong>Principal axis (PC1)</strong>. Equivalently, we get the sum squared distance of the <strong>projected points</strong> from the point of origin along the <strong>orthogonal axis</strong> by using the SSD formula.</p>
<p><strong>Sixth</strong>, let us compute for the <strong>unit vectors</strong>. Note that the orientation of the segments that make up both <strong>PC1</strong> and<strong>PC2</strong> is described by the slopes of their respective lines (axis). We know that a slope consists of the <strong>rise</strong> over the <strong>run</strong>, which corresponds to the y vector and x vector (also called <strong>PC Scores</strong> or <strong>Loading Scores</strong> ).</p>
<p><span class="math display" id="eq:equate1110045">\[\begin{align}
\text{slope} = \frac{\text{rise}}{\text{run}} = \frac{\text{y vector}}{\text{x vector}}  \tag{9.47} 
\end{align}\]</span></p>
<p>In this specific case, we only focus on our two dimensions (e.g.Â two features).</p>

<div class="sourceCode" id="cb1111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1111-1" data-line-number="1"><span class="kw">colnames</span>(V) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;EigenVec for PC1&quot;</span>, <span class="st">&quot;EigenVec for PC2&quot;</span>)</a>
<a class="sourceLine" id="cb1111-2" data-line-number="2"><span class="kw">rownames</span>(V) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Score 1 (a)&quot;</span>, <span class="st">&quot;Score 2 (b)&quot;</span>)</a>
<a class="sourceLine" id="cb1111-3" data-line-number="3">V</a></code></pre></div>
<pre><code>##             EigenVec for PC1 EigenVec for PC2
## Score 1 (a)          -0.3467           0.9380
## Score 2 (b)          -0.9380          -0.3467</code></pre>

<p>The slope of our linear combinations (our PCs) becomes:</p>

<div class="sourceCode" id="cb1113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1113-1" data-line-number="1">pc1.slope =<span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">/</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1113-2" data-line-number="2">pc2.slope =<span class="st"> </span>V[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>V[<span class="dv">2</span>,<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1113-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Slope for PC1&quot;</span> =<span class="st"> </span>pc1.slope, <span class="st">&quot;Slope for PC2&quot;</span> =<span class="st"> </span>pc2.slope)</a></code></pre></div>
<pre><code>## Slope for PC1 Slope for PC2 
##        0.3696       -2.7054</code></pre>

<p>Now suppose for <strong>PC1</strong>, we have a=-0.3467 and b=-0.938; therefore, a classic <strong>Pythagorean Theorem</strong> gives us the linear combination as <span class="math inline">\(c = \sqrt{a^2 + b^2} =\)</span> 1. This is the length (or magnitude value) of our Eigenvector for <strong>PC1</strong>.</p>
<p>To compute the <strong>unit vector</strong> of the x and y vectors, including the <strong>Eigenvector</strong>, we divide all the vector components by the <strong>Eigenvector</strong> length. This makes the <strong>Eigenvector</strong> a <strong>Unit Eigenvector</strong> equal to 1, e.g. <span class="math inline">\(\text{unit eigenvector } =\)</span> 1/1. The y and x unit vectors have -0.938 and -0.3467, respectively. It means that for every 0.3467 unit decrease of <strong>feature1</strong>, we get -0.938 unit increase of <strong>feature2</strong>.</p>
<pre><code>##       X-vec       Y-vec Eigenvector 
##     -0.3467     -0.9380      1.0000</code></pre>
<p>Similarly, for <strong>PC2</strong>, we have the following:</p>
<pre><code>##       X-vec       Y-vec Eigenvector 
##      0.9380     -0.3467      1.0000</code></pre>
<p>If an <strong>Eigenvector</strong> is scaled down to a <strong>unit vector</strong>, we call this <strong>Singular vector</strong> for the <strong>PC1</strong>. On the other hand, the <strong>Singular value</strong> for <strong>PC1</strong> is computed by taking the root square of its SSD.</p>
<p><span class="math display" id="eq:equate1110046">\[\begin{align}
\text{singular value} = \sqrt{\text{SSD}} \tag{9.48} 
\end{align}\]</span></p>
<p>Below is the implementation of the <strong>singular value</strong> computation in R code:</p>

<div class="sourceCode" id="cb1117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1117-1" data-line-number="1">SSD &lt;-<span class="st"> </span><span class="cf">function</span>(a, b, eigenvector) {</a>
<a class="sourceLine" id="cb1117-2" data-line-number="2">  s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1117-3" data-line-number="3">  a.unit =<span class="st"> </span>a <span class="op">/</span><span class="st"> </span>eigenvector</a>
<a class="sourceLine" id="cb1117-4" data-line-number="4">  b.unit =<span class="st"> </span>b <span class="op">/</span><span class="st"> </span>eigenvector</a>
<a class="sourceLine" id="cb1117-5" data-line-number="5">  c =<span class="st"> </span><span class="kw">sqrt</span>(a.unit<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>b.unit<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1117-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) { s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>(c[i] <span class="op">-</span><span class="st"> </span><span class="dv">0</span>)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb1117-7" data-line-number="7">  s  </a>
<a class="sourceLine" id="cb1117-8" data-line-number="8">}</a></code></pre></div>

<p>The <strong>singular values</strong> for <strong>PC1</strong> and <strong>PC2</strong> are the following:</p>

<div class="sourceCode" id="cb1118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1118-1" data-line-number="1">PC1.ssd =<span class="st"> </span><span class="kw">SSD</span>(pc1.x.proj, pc1.y.proj, pc1.eigenvector)</a>
<a class="sourceLine" id="cb1118-2" data-line-number="2">PC2.ssd =<span class="st"> </span><span class="kw">SSD</span>(pc2.x.proj, pc2.y.proj, pc2.eigenvector)</a>
<a class="sourceLine" id="cb1118-3" data-line-number="3">SV =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1118-4" data-line-number="4">  <span class="dt">ssd =</span> <span class="kw">rbind</span>(PC1.ssd, PC2.ssd),</a>
<a class="sourceLine" id="cb1118-5" data-line-number="5">  <span class="dt">singular.value =</span> <span class="kw">rbind</span>(<span class="kw">sqrt</span>(PC1.ssd), <span class="kw">sqrt</span>(PC2.ssd)),</a>
<a class="sourceLine" id="cb1118-6" data-line-number="6">  <span class="dt">variance =</span> <span class="kw">rbind</span>(PC1.ssd<span class="op">/</span>(N<span class="dv">-1</span>), PC2.ssd<span class="op">/</span>(N<span class="dv">-1</span>)),</a>
<a class="sourceLine" id="cb1118-7" data-line-number="7">  <span class="dt">std.dev =</span> <span class="kw">rbind</span>(<span class="kw">sqrt</span>(PC1.ssd<span class="op">/</span>(N<span class="dv">-1</span>)), <span class="kw">sqrt</span>(PC2.ssd<span class="op">/</span>(N<span class="dv">-1</span>))),</a>
<a class="sourceLine" id="cb1118-8" data-line-number="8">  <span class="dt">proportion =</span> pca.model<span class="op">$</span>sdev <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(pca.model<span class="op">$</span>sdev) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1118-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb1118-10" data-line-number="10"><span class="kw">rownames</span>(SV) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;PC1&quot;</span>, <span class="st">&quot;PC2&quot;</span>)</a>
<a class="sourceLine" id="cb1118-11" data-line-number="11">SV</a></code></pre></div>
<pre><code>##       ssd singular.value variance std.dev proportion
## PC1 25571          159.9   1065.4   32.64      53.28
## PC2 19662          140.2    819.3   28.62      46.72</code></pre>

<p>We can validate the <strong>standard deviation</strong> using the <strong>PCA.model</strong> we generated previously:</p>

<div class="sourceCode" id="cb1120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1120-1" data-line-number="1">pca.model<span class="op">$</span>sdev</a></code></pre></div>
<pre><code>## [1] 32.64 28.62</code></pre>

<p>For the proportionality of the <strong>Principal Components</strong>, we see that <strong>PC1</strong> has a higher proportion at 53.2796%. See Figure <a href="9.6-featureengineering.html#fig:pcaproportion1">9.51</a>.</p>

<div class="sourceCode" id="cb1122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1122-1" data-line-number="1">PC.proportions =<span class="st"> </span>SV<span class="op">$</span>proportion</a>
<a class="sourceLine" id="cb1122-2" data-line-number="2"><span class="kw">barplot</span>(PC.proportions, <span class="dt">las=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1122-3" data-line-number="3">        <span class="dt">xlab=</span><span class="st">&quot;Principal Components&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Variance&quot;</span>,</a>
<a class="sourceLine" id="cb1122-4" data-line-number="4">        <span class="dt">col=</span><span class="st">&quot;white&quot;</span>,</a>
<a class="sourceLine" id="cb1122-5" data-line-number="5">        <span class="dt">names.arg=</span><span class="kw">paste</span>(<span class="st">&quot;PC&quot;</span>,<span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(PC.proportions)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion1"></span>
<img src="DS_files/figure-html/pcaproportion1-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.51: PC Proportions
</p>
</div>

<p><strong>Seventh</strong>, for two features, we can rotate the respective <strong>Principal axis</strong> of their <strong>PCs</strong> to align them along the vertical and horizontal axes. The data is also rotated. See Figure <a href="9.6-featureengineering.html#fig:pca2">9.52</a>.</p>

<div class="sourceCode" id="cb1123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1123-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">80</span>,<span class="dv">80</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">100</span>,<span class="dv">80</span>), </a>
<a class="sourceLine" id="cb1123-2" data-line-number="2">      <span class="dt">xlab=</span><span class="st">&quot;feature1 (X1)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;feature2 (X2)&quot;</span>, </a>
<a class="sourceLine" id="cb1123-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Rotated Principal Components&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1123-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1123-5" data-line-number="5"><span class="co">### Plot the Points</span></a>
<a class="sourceLine" id="cb1123-6" data-line-number="6"><span class="kw">points</span>(PC, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">1.0</span>)</a>
<a class="sourceLine" id="cb1123-7" data-line-number="7"><span class="kw">points</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb1123-8" data-line-number="8"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1123-9" data-line-number="9"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1123-10" data-line-number="10"><span class="co">### Legend and PC2 label</span></a>
<a class="sourceLine" id="cb1123-11" data-line-number="11"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>, </a>
<a class="sourceLine" id="cb1123-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;Principal Axis (PC1)&quot;</span>, <span class="st">&quot;Principal Axis (PC2)&quot;</span>),</a>
<a class="sourceLine" id="cb1123-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>, <span class="dv">20</span>, <span class="dv">20</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1123-14" data-line-number="14"><span class="kw">text</span>(<span class="dv">5</span>, <span class="dv">-80</span>, <span class="dt">label=</span><span class="st">&quot;(PC2)&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1123-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">60</span>,  <span class="dv">5</span>, <span class="dt">label=</span><span class="st">&quot;(PC1)&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pca2"></span>
<img src="DS_files/figure-html/pca2-1.png" alt="Rotated Principal Components" width="70%" />
<p class="caption">
Figure 9.52: Rotated Principal Components
</p>
</div>

<p>It is important to note that, for a dataset with three features, we get three <strong>Principal axes</strong> that are all orthogonal to each other; thus, geometrically, we see a 3D representation of the <strong>Principal components</strong>.</p>
<p>For larger datasets with more features, we can quickly generate a covariance matrix derived from our original dataset to review deviations between features quickly. Additionally, because covariance is commutative, the matrix is therefore symmetric in which the upper and lower triangular areas are equal. Because of this, we can use <strong>Cholesky Decomposition</strong>, which we introduced in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) to extract the upper portion of the matrix. </p>
<div class="sourceCode" id="cb1124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1124-1" data-line-number="1"><span class="co"># Get the covariance, then transform to cholesky matrix</span></a>
<a class="sourceLine" id="cb1124-2" data-line-number="2">(<span class="dt">chol.matrix =</span> <span class="kw">chol</span>( <span class="kw">cov</span>(dataset)) )</a></code></pre></div>
<pre><code>##          feature1 feature2 feature3 feature4
## feature1    29.14    2.748   -2.249   0.6465
## feature2     0.00   32.067    3.051  -3.8405
## feature3     0.00    0.000   30.959   8.1306
## feature4     0.00    0.000    0.000  27.1806</code></pre>
<p>We can then use <strong>eigen(.)</strong> function as before. However, here, we show two functions that we can use for <strong>PCA</strong>, namely <strong>prcomp(.)</strong> and <strong>princomp(.)</strong>.</p>
<div class="sourceCode" id="cb1126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1126-1" data-line-number="1">(<span class="dt">pca.model =</span> <span class="kw">prcomp</span>(dataset, <span class="dt">tol=</span><span class="fl">0.10</span>, <span class="dt">center =</span> <span class="ot">TRUE</span>, <span class="dt">scale =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## Standard deviations (1, .., p=4):
## [1] 1.1305 1.0455 0.9975 0.7962
## 
## Rotation (n x k) = (4 x 4):
##              PC1      PC2     PC3     PC4
## feature1  0.1670  0.46545 -0.8299 -0.2584
## feature2  0.1668  0.81206  0.3543  0.4327
## feature3 -0.6672  0.35077  0.2515 -0.6070
## feature4 -0.7065 -0.02958 -0.3500  0.6144</code></pre>
<p>Below is the summary of the <strong>PCA</strong> outcome:</p>
<div class="sourceCode" id="cb1128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1128-1" data-line-number="1"><span class="kw">summary</span>(pca.model)</a></code></pre></div>
<pre><code>## Importance of components:
##                         PC1   PC2   PC3   PC4
## Standard deviation     1.13 1.046 0.997 0.796
## Proportion of Variance 0.32 0.273 0.249 0.158
## Cumulative Proportion  0.32 0.593 0.842 1.000</code></pre>
<p>We also use <strong>screeplot(.)</strong> function from <strong>graphics</strong> library to visualize <strong>PCA</strong>. See Figure <a href="9.6-featureengineering.html#fig:pcaproportion2">9.53</a> for the screeplot.</p>
<div class="sourceCode" id="cb1130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1130-1" data-line-number="1"><span class="kw">require</span>(graphics)</a>
<a class="sourceLine" id="cb1130-2" data-line-number="2"><span class="kw">screeplot</span>(pca.model, <span class="dt">npcs =</span> <span class="dv">5</span>, <span class="dt">type =</span> <span class="kw">c</span>(<span class="st">&quot;lines&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion2"></span>
<img src="DS_files/figure-html/pcaproportion2-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.53: PC Proportions
</p>
</div>
<p>Alternatively, we can use <strong>princomp(.)</strong> function. See Figure <a href="9.6-featureengineering.html#fig:pcaproportion2">9.53</a> for the screeplot.</p>
<div class="sourceCode" id="cb1131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1131-1" data-line-number="1">std.dataset =<span class="st"> </span><span class="kw">scale</span>(dataset, <span class="dt">center=</span><span class="ot">TRUE</span>, <span class="dt">scale=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1131-2" data-line-number="2">(pca.model &lt;-<span class="st"> </span><span class="kw">princomp</span>(std.dataset, <span class="dt">cor =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## Call:
## princomp(x = std.dataset, cor = TRUE)
## 
## Standard deviations:
## Comp.1 Comp.2 Comp.3 Comp.4 
## 1.1305 1.0455 0.9975 0.7962 
## 
##  4  variables and  25 observations.</code></pre>
<div class="sourceCode" id="cb1133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1133-1" data-line-number="1"><span class="kw">screeplot</span>(pca.model, <span class="dt">npcs =</span> <span class="dv">5</span>, <span class="dt">type =</span> <span class="st">&quot;lines&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pcaproportion3"></span>
<img src="DS_files/figure-html/pcaproportion3-1.png" alt="PC Proportions" width="70%" />
<p class="caption">
Figure 9.54: PC Proportions
</p>
</div>
<p><strong>Independent Component Analysis (ICA)</strong> is similar to <strong>PCA</strong>. We leave readers to investigate further on this topic.</p>
</div>
<div id="linear-discriminant-analysis-lda" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.4</span> Linear Discriminant Analysis (LDA)  <a href="9.6-featureengineering.html#linear-discriminant-analysis-lda" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend the concept of <strong>Dimensionality Reduction</strong> compared to <strong>PCA</strong> by introducing <strong>Linear Discriminant Analysis (LDA)</strong>. With <strong>LDA</strong>, we assume that our dataset is classified (or labeled). Our goal is to determine the maximum separation of the classes. In other words, we <strong>discriminate</strong> data points such that they are separated into groups relative to their respective <strong>moments</strong>, e.g., mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma\)</span>). Here, we use Figure <a href="9.6-featureengineering.html#fig:lda">9.55</a> to illustrate the idea.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lda"></span>
<img src="lda.png" alt="Linear Discriminant Analysis (LDA)" width="70%" />
<p class="caption">
Figure 9.55: Linear Discriminant Analysis (LDA)
</p>
</div>
<p>The figure shows an <strong>Eigenvector</strong> representing the space unto which data points are projected. The projected data points are assumed to follow a multimodal distribution with multinomial outcomes. In the case of our figure, we see a projection of two <strong>Gaussian distribution</strong> with respective means (<span class="math inline">\(\mu_1, \mu_2\)</span>) and covariances (<span class="math inline">\(\sigma_1, \sigma_2\)</span>). In <strong>LDA</strong>, similar to <strong>PCA</strong>, our goal is to find the best <strong>fit</strong>. However, a <strong>best fit</strong> represents a line (a projected vector) with maximum separability and a minimum overlap of classes.</p>
<p>Note that in <strong>PCA</strong>, our goal is to derive the <strong>Principal components</strong>, e.g. <strong>PC1</strong> and <strong>PC2</strong>. In <strong>LDA</strong>, our goal is to derive the <strong>Linear Discriminants</strong>, e.g. <strong>LD1</strong> and <strong>LD2</strong>.</p>
<p>To illustrate, we use the <strong>iris</strong> dataset, which is common in explaining <strong>LDA</strong>.</p>

<div class="sourceCode" id="cb1134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1134-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>,<span class="st">&quot;Sepal.Width&quot;</span>,<span class="st">&quot;Petal.Length&quot;</span>,<span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1134-2" data-line-number="2">lda.dataset =<span class="st"> </span>iris[,features]</a>
<a class="sourceLine" id="cb1134-3" data-line-number="3">lda.dataset =<span class="st"> </span><span class="kw">as.matrix</span>(lda.dataset)</a>
<a class="sourceLine" id="cb1134-4" data-line-number="4">N =<span class="st"> </span><span class="kw">nrow</span>(lda.dataset)</a>
<a class="sourceLine" id="cb1134-5" data-line-number="5"><span class="kw">head</span>(lda.dataset) <span class="co"># show only first 5 observations</span></a></code></pre></div>
<pre><code>##      Sepal.Length Sepal.Width Petal.Length Petal.Width
## [1,]          5.1         3.5          1.4         0.2
## [2,]          4.9         3.0          1.4         0.2
## [3,]          4.7         3.2          1.3         0.2
## [4,]          4.6         3.1          1.5         0.2
## [5,]          5.0         3.6          1.4         0.2
## [6,]          5.4         3.9          1.7         0.4</code></pre>

<p>Here, our <strong>iris</strong> dataset is labeled - in other words, each observation is already classified as either <strong>Setosa</strong>, <strong>Versicolor</strong>, or <strong>Virginica</strong> depending on the <strong>Sepal</strong> and <strong>Petal</strong> properties.</p>

<div class="sourceCode" id="cb1136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1136-1" data-line-number="1">labels =<span class="st"> </span><span class="kw">levels</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)])</a>
<a class="sourceLine" id="cb1136-2" data-line-number="2">K =<span class="st"> </span><span class="kw">length</span>(labels)</a>
<a class="sourceLine" id="cb1136-3" data-line-number="3">labels</a></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>

<p><strong>First</strong>, using the <strong>Fisher Linear Discriminant</strong> method, our initial step is to compute for the mean of each features based on labels:</p>

<p><span class="math display" id="eq:eqnnumber401">\[\begin{align}
\mu_k = \frac{1}{n_k} \sum_{x \in D_k} x
\ \ \ \ \ \ \ \ where:\ 
\begin{array}{ll}
\mathbf{k} &amp; \text{kth class}\\
\mathbf{D} &amp; \text{dataset}\\
\end{array} \tag{9.49}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1138-1" data-line-number="1">mu.k =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>K, <span class="dt">ncol=</span><span class="kw">length</span>(features), <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1138-2" data-line-number="2"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1138-3" data-line-number="3">  idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>labels[k])</a>
<a class="sourceLine" id="cb1138-4" data-line-number="4">  mu.k[k,] =<span class="st"> </span><span class="kw">apply</span>(lda.dataset[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1138-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1138-6" data-line-number="6"><span class="kw">colnames</span>(mu.k) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1138-7" data-line-number="7"><span class="kw">rownames</span>(mu.k) =<span class="st"> </span>labels</a>
<a class="sourceLine" id="cb1138-8" data-line-number="8">mu.k</a></code></pre></div>
<pre><code>##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa            5.006       3.428        1.462       0.246
## versicolor        5.936       2.770        4.260       1.326
## virginica         6.588       2.974        5.552       2.026</code></pre>

<p>Note that the whole dataset assumes to follow a multi-modal Gaussian distribution with three modes, representing three means, e.g., <span class="math inline">\(\mu_1, \mu_2, \mu_3\)</span>, and three variances. Our goal is to project the data points to lower-dimensional spaces - a set of projections (represented by eigenvectors).</p>
<p><strong>Second</strong>, we compute for two types of <strong>variances</strong>. The first variance is the <strong>variance within each class</strong>. The second variance is the <strong>variance between classes</strong>. The idea is to maximize the separation of classes. That can be achieved if the variance between classes is larger than the variance within each class, allowing each data point to be closer together in a class and further away from other classes.</p>
<p>Here, we compute for the <strong>within-class scatter matrix</strong> <span class="math inline">\(\mathbf{S_w}\)</span> like so:</p>

<p><span class="math display" id="eq:equate1110047">\[\begin{align}
\mathbf{S_w} = \sum_{k=1}^{K} S_k
\ \ \ \ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ 
S_k = \sum_{x \in D_k} \left(x - \mu_k\right)\left(x - \mu_k\right)^T  \tag{9.50} 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1140-1" data-line-number="1">within.class &lt;-<span class="st"> </span><span class="cf">function</span>(x, labels) {</a>
<a class="sourceLine" id="cb1140-2" data-line-number="2">  s_k =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1140-3" data-line-number="3">  <span class="cf">for</span> (k <span class="cf">in</span> labels) {</a>
<a class="sourceLine" id="cb1140-4" data-line-number="4">    idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1140-5" data-line-number="5">    mu =<span class="st"> </span><span class="kw">apply</span>(x[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1140-6" data-line-number="6">    s_i =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1140-7" data-line-number="7">    <span class="cf">for</span> (i <span class="cf">in</span> idx) {</a>
<a class="sourceLine" id="cb1140-8" data-line-number="8">      s_i =<span class="st"> </span>s_i <span class="op">+</span><span class="st">  </span>(x[i,] <span class="op">-</span><span class="st"> </span>mu) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x[i,] <span class="op">-</span><span class="st"> </span>mu) </a>
<a class="sourceLine" id="cb1140-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb1140-10" data-line-number="10">    s_k =<span class="st"> </span>s_k <span class="op">+</span><span class="st"> </span>s_i</a>
<a class="sourceLine" id="cb1140-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb1140-12" data-line-number="12">  s_k</a>
<a class="sourceLine" id="cb1140-13" data-line-number="13">}</a>
<a class="sourceLine" id="cb1140-14" data-line-number="14">S_w =<span class="st"> </span><span class="kw">within.class</span>(lda.dataset, labels)</a>
<a class="sourceLine" id="cb1140-15" data-line-number="15"><span class="kw">rownames</span>(S_w) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1140-16" data-line-number="16">S_w</a></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length       38.956      13.630       24.625       5.645
## Sepal.Width        13.630      16.962        8.121       4.808
## Petal.Length       24.625       8.121       27.223       6.272
## Petal.Width         5.645       4.808        6.272       6.157</code></pre>

<p><strong>Third</strong>, we compute for the <strong>between-class scatter matrix</strong> <span class="math inline">\(\mathbf{S_b}\)</span> with the given total mean per class.</p>

<p><span class="math display" id="eq:equate1110048">\[\begin{align}
\mathbf{S_b} = \sum_{k=1}^{K} n_k  \left(\mu_k - \mu\right)\left(\mu_k - \mu\right)^T
\ \ \ \ \ \ where: \ \ \mu\ \text{is total mean per class} \tag{9.51} 
\end{align}\]</span></p>
<div class="sourceCode" id="cb1142"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1142-1" data-line-number="1">(<span class="dt">mu =</span> <span class="kw">apply</span>(lda.dataset, <span class="dv">2</span>, mean)) <span class="co"># mean(lda.dataset))</span></a></code></pre></div>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##        5.843        3.057        3.758        1.199</code></pre>
<div class="sourceCode" id="cb1144"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1144-1" data-line-number="1">between.class &lt;-<span class="st"> </span><span class="cf">function</span>(x, labels) {</a>
<a class="sourceLine" id="cb1144-2" data-line-number="2">  s_k =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1144-3" data-line-number="3">  <span class="cf">for</span> (k <span class="cf">in</span> labels) {</a>
<a class="sourceLine" id="cb1144-4" data-line-number="4">    idx =<span class="st"> </span><span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)] <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1144-5" data-line-number="5">    n.k =<span class="st"> </span><span class="kw">length</span>(idx)</a>
<a class="sourceLine" id="cb1144-6" data-line-number="6">    mu.k =<span class="st"> </span><span class="kw">apply</span>(lda.dataset[idx,] , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1144-7" data-line-number="7">    s_k =<span class="st"> </span>s_k <span class="op">+</span><span class="st"> </span>n.k <span class="op">*</span><span class="st"> </span>(mu.k <span class="op">-</span><span class="st"> </span>mu) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(mu.k <span class="op">-</span><span class="st"> </span>mu)</a>
<a class="sourceLine" id="cb1144-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1144-9" data-line-number="9">  s_k</a>
<a class="sourceLine" id="cb1144-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1144-11" data-line-number="11">S_b =<span class="st"> </span><span class="kw">between.class</span>(lda.dataset, labels)</a>
<a class="sourceLine" id="cb1144-12" data-line-number="12"><span class="kw">rownames</span>(S_b) =<span class="st"> </span>features</a>
<a class="sourceLine" id="cb1144-13" data-line-number="13">S_b</a></code></pre></div>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length Petal.Width
## Sepal.Length        63.21      -19.95       165.25       71.28
## Sepal.Width        -19.95       11.34       -57.24      -22.93
## Petal.Length       165.25      -57.24       437.10      186.77
## Petal.Width         71.28      -22.93       186.77       80.41</code></pre>

<p><strong>Fourth</strong>, to derive the best fit, we need to minimize the following objective function:</p>

<p><span class="math display" id="eq:eqnnumber150a">\[\begin{align}
\mathcal{J}(\mathbf{\bar{v}}) = \frac{(\bar{\mu}_1 - \bar{\mu}_2)^2}{s_1^2 + s_2^2}  
= \frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}} \tag{9.52}
\end{align}\]</span>
</p>
<p>For a good understanding of Equation (<a href="9.6-featureengineering.html#eq:eqnnumber150a">(9.52)</a>) and to explain its formulation, let us consider discussing projections and how to find the best projection. For that, we start with the idea that any data point can be projected in the direction of a vector, e.g., <span class="math inline">\(\mathbf{\vec{v}}\)</span>, using the following formula:</p>
<p><span class="math display" id="eq:equate1110049">\[\begin{align}
\tilde{x}_i^{(\text{projected point)}} = \mathbf{\vec{v}}^T x_i  \tag{9.53} 
\end{align}\]</span></p>
<p>A class of data points has a mean that can also be projected like so:</p>
<p><span class="math display" id="eq:equate1110050">\[\begin{align}
\tilde{\mu}_k^{(\text{projected mean)}} = \mathbf{\vec{v}}^T \mu_k   \tag{9.54} 
\end{align}\]</span></p>
<p>where <strong>k</strong> is a class and <span class="math inline">\(\mathbf{\vec{v}}\)</span> is the projected line.</p>
<p>We can compare such projected mean <span class="math inline">\(\tilde{\mu}_1\)</span> of a class <strong>k=1</strong> with the projected mean <span class="math inline">\(\tilde{\mu}_2\)</span> of another class <strong>k=2</strong> in terms of distance. The best projection is based on the largest distance between classes (with respect to their means).</p>
<p>Next, we also account for the variance of each class using the following formula (with basic derivation included):</p>

<p><span class="math display" id="eq:equate1110054" id="eq:equate1110053" id="eq:equate1110052" id="eq:equate1110051">\[\begin{align}
\tilde{s}_k^2 &amp;= \sum_{x \in D_k} \left(\tilde{x}_i - \tilde{\mu}_k\right)^2 \tag{9.55} \\
&amp;=  \sum_{x \in D_k} \left(\mathbf{\vec{v}}^T x_i - \mathbf{\vec{v}}^T \mu_k\right)^2 \tag{9.56} \\
&amp;=\sum_{x \in D_k} \mathbf{\vec{v}}^T  \left(x - \mu_k\right)\left(x - \mu_k\right)^T \mathbf{\vec{v}} \tag{9.57} \\
&amp; = \mathbf{\vec{v}}^T S_k \mathbf{\vec{v}} \tag{9.58} 
\end{align}\]</span>
</p>
<p>Therefore:</p>
<p><span class="math display" id="eq:equate1110055">\[\begin{align}
\sum \tilde{s}_k^2 = \mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}  \tag{9.59} 
\end{align}\]</span></p>
<p>Similarly, starting with a 2 class configuration, we can derive the following:</p>
<p><span class="math display" id="eq:equate1110056">\[\begin{align}
\left(\tilde{\mu}_1 - \tilde{\mu}_2\right)^2 = \left( \mathbf{\vec{v}}^T \mu_1 - \mathbf{\vec{v}}^T\tilde{\mu}_2\right)^2 = \mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}  \tag{9.60} 
\end{align}\]</span></p>
<p>Now to minimize the objective function, namely <span class="math inline">\(\mathbf{\mathcal{J}}(\mathbf{\vec{v}})\)</span>, with respect to vector <span class="math inline">\(\mathbf{\vec{v}}\)</span>, we use partial derivatives:</p>

<p><span class="math display" id="eq:equate1110057">\[\begin{align}
\frac{ \partial \mathbf{\mathcal{J}}(\mathbf{\vec{v}}) } {\partial \mathbf{\vec{v}}}
=  
\frac{ \partial  } {\partial \mathbf{\vec{v}}}
\left[\frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}}\right] \tag{9.61} 
\end{align}\]</span>
</p>
<p>Setting the derivative of the objective function with respect to <span class="math inline">\(\mathbf{\vec{v}}\)</span> to zero, we get:</p>

<p><span class="math display" id="eq:equate1110058">\[\begin{align}
\underbrace{S_b \mathbf{\vec{v}} = \lambda S_w \mathbf{\vec{v}} }_{\text{Generalized Eigenvalue Problem}}
\ \ \ \ \leftarrow \ \ \ \ \ \
S_b \mathbf{\vec{v}} - \lambda S_w \mathbf{\vec{v}} = 0
\ \ \ \ \ \ \ where\ \  \lambda = \frac{\mathbf{\vec{v}}^T S_b \mathbf{\vec{v}}}{\mathbf{\vec{v}}^T S_w \mathbf{\vec{v}}} \tag{9.62} 
\end{align}\]</span>
</p>
<p>For a two-class configuration, the derived projection becomes:</p>
<p><span class="math display" id="eq:equate1110059">\[\begin{align}
\mathbf{\vec{v}} = S_w^{-1} \left(\mu_1 - \mu_2\right) \tag{9.63} 
\end{align}\]</span></p>
<p>For multi-class configuration, recall in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) the <strong>Eigen equation</strong> and the derivation of the <strong>Eigenvalue</strong> <span class="math inline">\(\lambda\)</span> using determinant and echelon form:</p>
<p><span class="math display" id="eq:equate1110060">\[\begin{align}
(A - \lambda I) = 0\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ A \mathbf{\vec{v}} = \lambda \mathbf{\vec{v}} \tag{9.64} 
\end{align}\]</span></p>
<p>Equivalently, we have the following expression:</p>
<p><span class="math display" id="eq:equate1110061">\[\begin{align}
\underbrace{(S_w^{-1}S_b )\mathbf{\vec{v}} = \lambda \mathbf{\vec{v}}}_{A \mathbf{\vec{v}} = \lambda \mathbf{\vec{v}}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ \ S_b \mathbf{\vec{v}} = \lambda S_w \mathbf{\vec{v}}  \tag{9.65} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, to then find the projection of datasets to the new axis, we use the following expression:</p>
<p><span class="math display" id="eq:equate1110062">\[\begin{align}
y = \mathbf{\vec{v}}^T x  \tag{9.66} 
\end{align}\]</span></p>
<p>Let us compute for the eigenvalues and corresponding eigenvectors (we choose the two highest eigenvalues):</p>
<div class="sourceCode" id="cb1146"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1146-1" data-line-number="1">e =<span class="st"> </span><span class="kw">eigen</span>(<span class="kw">solve</span>(S_w) <span class="op">%*%</span><span class="st"> </span>S_b)</a>
<a class="sourceLine" id="cb1146-2" data-line-number="2">idx =<span class="st"> </span><span class="kw">sort</span>( e<span class="op">$</span>values, <span class="dt">index.return =</span> <span class="ot">TRUE</span>, <span class="dt">decreasing=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1146-3" data-line-number="3">(<span class="dt">eigenvalues =</span> idx<span class="op">$</span>x)</a></code></pre></div>
<pre><code>## [1]  3.219e+01  2.854e-01 -2.801e-15 -4.881e-15</code></pre>
<div class="sourceCode" id="cb1148"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1148-1" data-line-number="1">(<span class="dt">eigenvectors =</span>  e<span class="op">$</span>vectors[idx<span class="op">$</span>ix,]) </a></code></pre></div>
<pre><code>##         [,1]      [,2]      [,3]    [,4]
## [1,] -0.2087 -0.006532  0.850126 -0.6859
## [2,] -0.3862 -0.586611 -0.373448  0.4384
## [3,]  0.7074 -0.769453  0.002199 -0.3476
## [4,]  0.5540  0.252562 -0.371237  0.4653</code></pre>
<p>Finally, to plot, let us compute the projections of the datasets (see Figure <a href="9.6-featureengineering.html#fig:ldareduction1">9.56</a>).</p>

<div class="sourceCode" id="cb1150"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1150-1" data-line-number="1">coeff =<span class="st"> </span>eigenvectors</a>
<a class="sourceLine" id="cb1150-2" data-line-number="2">LD1 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-3" data-line-number="3"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1150-4" data-line-number="4"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-5" data-line-number="5"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a>
<a class="sourceLine" id="cb1150-6" data-line-number="6">LD2 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-7" data-line-number="7"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1150-8" data-line-number="8"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1150-9" data-line-number="9"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a></code></pre></div>
<div class="sourceCode" id="cb1151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1151-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(LD1), <span class="dt">ylim=</span><span class="kw">range</span>(LD2),</a>
<a class="sourceLine" id="cb1151-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;LD2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;LD1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Linear Discriminant Analysis&quot;</span>)</a>
<a class="sourceLine" id="cb1151-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1151-4" data-line-number="4">col =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, <span class="kw">length</span>(LD1))</a>
<a class="sourceLine" id="cb1151-5" data-line-number="5">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>) ] =<span class="st"> &quot;red&quot;</span></a>
<a class="sourceLine" id="cb1151-6" data-line-number="6">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;versicolor&quot;</span>) ] =<span class="st"> &quot;blue&quot;</span></a>
<a class="sourceLine" id="cb1151-7" data-line-number="7">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;virginica&quot;</span>) ] =<span class="st"> &quot;green&quot;</span></a>
<a class="sourceLine" id="cb1151-8" data-line-number="8"><span class="kw">points</span>(LD1,LD2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span>col)</a>
<a class="sourceLine" id="cb1151-9" data-line-number="9"><span class="kw">legend</span>(<span class="op">-</span><span class="fl">0.5</span>, <span class="dv">-5</span>,   <span class="kw">c</span>( <span class="st">&quot;Setosa&quot;</span>, <span class="st">&quot;Versicolor&quot;</span>, <span class="st">&quot;Virginica&quot;</span> ),</a>
<a class="sourceLine" id="cb1151-10" data-line-number="10">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>), </a>
<a class="sourceLine" id="cb1151-11" data-line-number="11">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldareduction1"></span>
<img src="DS_files/figure-html/ldareduction1-1.png" alt="Linear Discriminant Analysis" width="70%" />
<p class="caption">
Figure 9.56: Linear Discriminant Analysis
</p>
</div>

<p><strong>Sixth</strong>, to validate, we use <strong>lda(.)</strong> from library <strong>MASS</strong>:</p>

<div class="sourceCode" id="cb1152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1152-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb1152-2" data-line-number="2">(<span class="dt">lda.model =</span> <span class="kw">lda</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data=</span>iris, <span class="dt">prior=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>)<span class="op">/</span><span class="dv">3</span>, </a>
<a class="sourceLine" id="cb1152-3" data-line-number="3">                 <span class="dt">method=</span><span class="st">&quot;moment&quot;</span>)) </a></code></pre></div>
<pre><code>## Call:
## lda(Species ~ ., data = iris, prior = c(1, 1, 1)/3, method = &quot;moment&quot;)
## 
## Prior probabilities of groups:
##     setosa versicolor  virginica 
##     0.3333     0.3333     0.3333 
## 
## Group means:
##            Sepal.Length Sepal.Width Petal.Length Petal.Width
## setosa            5.006       3.428        1.462       0.246
## versicolor        5.936       2.770        4.260       1.326
## virginica         6.588       2.974        5.552       2.026
## 
## Coefficients of linear discriminants:
##                  LD1     LD2
## Sepal.Length  0.8294  0.0241
## Sepal.Width   1.5345  2.1645
## Petal.Length -2.2012 -0.9319
## Petal.Width  -2.8105  2.8392
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9912 0.0088</code></pre>

<p>The result gives us the <strong>Coefficients of the linear discriminants</strong> and the <strong>LD1</strong> and <strong>LD2</strong>. The calculations of <strong>LD1</strong> and <strong>LD2</strong> are based on the <strong>Coefficients</strong>:</p>

<div class="sourceCode" id="cb1154"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1154-1" data-line-number="1">coeff =<span class="st"> </span>lda.model<span class="op">$</span>scaling</a>
<a class="sourceLine" id="cb1154-2" data-line-number="2">LD1 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-3" data-line-number="3"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1154-4" data-line-number="4"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-5" data-line-number="5"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a>
<a class="sourceLine" id="cb1154-6" data-line-number="6">LD2 =<span class="st"> </span>coeff[<span class="dv">1</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-7" data-line-number="7"><span class="st">      </span>coeff[<span class="dv">2</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Sepal.Width&quot;</span>)]  <span class="op">+</span></a>
<a class="sourceLine" id="cb1154-8" data-line-number="8"><span class="st">      </span>coeff[<span class="dv">3</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Length&quot;</span>)] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1154-9" data-line-number="9"><span class="st">      </span>coeff[<span class="dv">4</span>,<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>iris[,<span class="kw">c</span>(<span class="st">&quot;Petal.Width&quot;</span>)]</a></code></pre></div>

<p>We can then use the result to plot a 2D representation of our dataset (see Figure <a href="9.6-featureengineering.html#fig:ldareduction2">9.57</a>):</p>

<div class="sourceCode" id="cb1155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1155-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(LD1), <span class="dt">ylim=</span><span class="kw">range</span>(LD2),</a>
<a class="sourceLine" id="cb1155-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;LD2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;LD1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Linear Discriminant Analysis&quot;</span>)</a>
<a class="sourceLine" id="cb1155-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1155-4" data-line-number="4">col =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, <span class="kw">length</span>(LD1))</a>
<a class="sourceLine" id="cb1155-5" data-line-number="5">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;setosa&quot;</span>) ] =<span class="st"> &quot;red&quot;</span></a>
<a class="sourceLine" id="cb1155-6" data-line-number="6">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;versicolor&quot;</span>) ] =<span class="st"> &quot;blue&quot;</span></a>
<a class="sourceLine" id="cb1155-7" data-line-number="7">col[ <span class="kw">which</span>(iris[,<span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>) ] <span class="op">==</span><span class="st"> &quot;virginica&quot;</span>) ] =<span class="st"> &quot;green&quot;</span></a>
<a class="sourceLine" id="cb1155-8" data-line-number="8"><span class="kw">points</span>(LD1,LD2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span>col)</a>
<a class="sourceLine" id="cb1155-9" data-line-number="9"><span class="kw">legend</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">9</span>,   <span class="kw">c</span>( <span class="st">&quot;Setosa&quot;</span>, <span class="st">&quot;Versicolor&quot;</span>, <span class="st">&quot;Virginica&quot;</span> ),</a>
<a class="sourceLine" id="cb1155-10" data-line-number="10">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;green&quot;</span>), </a>
<a class="sourceLine" id="cb1155-11" data-line-number="11">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldareduction2"></span>
<img src="DS_files/figure-html/ldareduction2-1.png" alt="Linear Discriminant Analysis" width="70%" />
<p class="caption">
Figure 9.57: Linear Discriminant Analysis
</p>
</div>

<p>Note that <strong>LDA</strong>, as shown in the figure, is also used as a basic <strong>classifier</strong>.</p>
<p>For non-linear discriminants, we leave readers to investigate <strong>Quadratic Discriminant Analysis (QDA)</strong>.</p>
</div>
<div id="feature-construction" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.5</span> Feature Construction <a href="9.6-featureengineering.html#feature-construction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Construction</strong> is a method to create or filter features. It helps to discuss this in the context of pattern detection and pattern recognition, especially if working with images, texts, sounds, and signals. In this literature, we may also regard this <strong>Feature Engineering</strong> component as <strong>Feature Extraction by Construction</strong>.</p>
<p>To illustrate, let us use the MNIST database made available by Y. LeCun, C. Cortes, and C. Burges, which contains over 60,000 grayscale images of handwritten digits. The database comes with four files with the following contents: a training set of images, a training set of labels, a test set of images, and a test set of labels.</p>
<p>In our case, we use the training set to exemplify.</p>
<p><strong>First</strong>, we read the file that contains the digit labels.</p>

<div class="sourceCode" id="cb1156"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1156-1" data-line-number="1">minst_file =<span class="st"> </span><span class="kw">paste0</span>(dir, <span class="st">&quot;train-labels-idx1-ubyte&quot;</span>)</a>
<a class="sourceLine" id="cb1156-2" data-line-number="2">minst.handler =<span class="st"> </span><span class="kw">file</span>( minst_file, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb1156-3" data-line-number="3">magic.number   =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-4" data-line-number="4">image.count    =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-5" data-line-number="5">image.labels =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="kw">integer</span>(), <span class="dt">size=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1156-6" data-line-number="6">                       <span class="dt">n=</span> image.count, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1156-7" data-line-number="7"><span class="kw">close</span>(minst.handler)</a>
<a class="sourceLine" id="cb1156-8" data-line-number="8"><span class="kw">c</span>(<span class="st">&quot;Magic Number&quot;</span>  =<span class="st"> </span>magic.number,  <span class="st">&quot;Number of Images&quot;</span>    =<span class="st"> </span>image.count)</a></code></pre></div>
<pre><code>##     Magic Number Number of Images 
##             2049            60000</code></pre>

<p>We display the first 20 digit labels:</p>
<div class="sourceCode" id="cb1158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1158-1" data-line-number="1">image.labels[<span class="dv">1</span><span class="op">:</span><span class="dv">20</span>]</a></code></pre></div>
<pre><code>##  [1] 5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9</code></pre>
<p>Because the digit images are collected randomly, let us find the first occurrence of digits 0 through 9 in ascending order.</p>
<div class="sourceCode" id="cb1160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1160-1" data-line-number="1">digits =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb1160-2" data-line-number="2">( <span class="dt">indices.of.1st.occurrences =</span> <span class="kw">match</span>(digits, image.labels) )</a></code></pre></div>
<pre><code>##  [1]  2  4  6  8  3  1 14 16 18  5</code></pre>
<p><strong>Second</strong>, the number of digit images and the constant size of the images are encoded into the first few bytes of the MNIST database. Therefore, we need to extract the information as so:</p>

<div class="sourceCode" id="cb1162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1162-1" data-line-number="1">minst_file =<span class="st"> </span><span class="kw">paste0</span>(dir, <span class="st">&quot;train-images-idx3-ubyte&quot;</span>)</a>
<a class="sourceLine" id="cb1162-2" data-line-number="2">minst.handler =<span class="st"> </span><span class="kw">file</span>( minst_file, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb1162-3" data-line-number="3"><span class="co"># Get metadata ( number of images, rows, columns)</span></a>
<a class="sourceLine" id="cb1162-4" data-line-number="4">magic.number =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-5" data-line-number="5">                       <span class="dt">size =</span> <span class="dv">4</span>, <span class="dt">endian =</span> <span class="st">&#39;big&#39;</span>)</a>
<a class="sourceLine" id="cb1162-6" data-line-number="6">image.count   =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-7" data-line-number="7">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-8" data-line-number="8">image.rows    =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-9" data-line-number="9">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-10" data-line-number="10">image.columns =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="st">&#39;integer&#39;</span>, <span class="dt">n =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1162-11" data-line-number="11">                        <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1162-12" data-line-number="12"><span class="kw">c</span>(  <span class="st">&quot;Number of Images&quot;</span>  =<span class="st"> </span>image.count, </a>
<a class="sourceLine" id="cb1162-13" data-line-number="13">    <span class="st">&quot;Number of Rows&quot;</span>    =<span class="st"> </span>image.rows,</a>
<a class="sourceLine" id="cb1162-14" data-line-number="14">    <span class="st">&quot;Number of Columns&quot;</span> =<span class="st"> </span>image.columns)</a></code></pre></div>
<pre><code>##  Number of Images    Number of Rows Number of Columns 
##             60000                28                28</code></pre>

<p><strong>Third</strong>, for our purpose, we read only the first 25 28x28 digit images and filter the first occurrence of digits 0 through 9 into a list structure:</p>

<div class="sourceCode" id="cb1164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1164-1" data-line-number="1">images =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1164-2" data-line-number="2"><span class="cf">for</span> (idx <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">25</span>) {</a>
<a class="sourceLine" id="cb1164-3" data-line-number="3">  img.hex.vector =<span class="st"> </span><span class="kw">readBin</span>(minst.handler, <span class="dt">what =</span> <span class="st">&quot;raw&quot;</span>, <span class="dt">n =</span> <span class="dv">784</span> , </a>
<a class="sourceLine" id="cb1164-4" data-line-number="4">                           <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb1164-5" data-line-number="5">  <span class="cf">if</span> (idx <span class="op">%in%</span><span class="st"> </span>indices.of<span class="fl">.1</span>st.occurrences) {</a>
<a class="sourceLine" id="cb1164-6" data-line-number="6">    img.dec.vector =<span class="st"> </span><span class="kw">as.integer</span>(img.hex.vector )</a>
<a class="sourceLine" id="cb1164-7" data-line-number="7">    images[[idx]] =<span class="st"> </span><span class="kw">matrix</span>(img.dec.vector, <span class="dt">nrow=</span><span class="dv">28</span>, <span class="dt">ncol=</span><span class="dv">28</span>)</a>
<a class="sourceLine" id="cb1164-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1164-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1164-10" data-line-number="10"><span class="kw">close</span>(minst.handler)</a></code></pre></div>

<p>Now, herein lies the construction of features. We intentionally show 25 iterations, each reading 784 bytes and storing them into a 784-length vector. The entire 784 bytes of data represent one image of a digit. In other words, each byte is a feature. The representation of a digit depends upon the value of each byte. Therefore, it becomes more apparent to store the vector in a 28x28 matrix for visualization. Each cell in a matrix represents a grayscale code between 0-255 (bytesize) that is mapped to one pixel. However, for classification, a vector of features is more convenient for computation. Certain image perturbations (such as thinning, swelling, thickening, and fractures) of each digit are then evaluated for feature matching (for whatever intended purpose).</p>
<p><strong>Fourth</strong>, for visualization, we use a 3rd-party R package called <strong>imager</strong> and use its built-in function <strong>as.cimg(.)</strong> to convert the image list into <strong>cimage</strong> format so that we can plot the actual image (see Figure <a href="9.6-featureengineering.html#fig:digits">9.58</a>). Note that each image in the figure represents 784 features.</p>

<div class="sourceCode" id="cb1165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1165-1" data-line-number="1"><span class="kw">library</span>(imager)</a>
<a class="sourceLine" id="cb1165-2" data-line-number="2">digits =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb1165-3" data-line-number="3"><span class="cf">for</span> (n <span class="cf">in</span> digits.index) {</a>
<a class="sourceLine" id="cb1165-4" data-line-number="4"> digits =<span class="st"> </span><span class="kw">rbind</span>(digits, <span class="kw">rep</span>(<span class="dv">255</span>, <span class="dv">28</span>), images[[n]])</a>
<a class="sourceLine" id="cb1165-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1165-6" data-line-number="6"><span class="kw">plot</span>(<span class="kw">as.cimg</span>(digits), <span class="dt">axes=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:digits"></span>
<img src="mnist_digits.png" alt="MNIST digits" width="80%" />
<p class="caption">
Figure 9.58: MNIST digits
</p>
</div>

<p>Other types of datasets, such as the one used by <strong>Natural Language Processing (NLP)</strong>, use <strong>word embeddings</strong> generated by tools such as <strong>word2vec</strong> using <strong>CBOW</strong> and <strong>Skip-gram</strong>. We defer this topic towards the end around <strong>Attention</strong> and <strong>Deep Learning</strong>.</p>
<p><strong>Gesture and Speech Recognition</strong> are two other areas that demand different representations and interpretations of their datasets and corresponding features. For example, <strong>Speech Recognition</strong> uses <strong>spectrogram</strong>.</p>
<p>Datasets that do not fall under the category of images, sound, signals, or texts have simpler features. For example, the dataset <strong>mtcars</strong> deals with <strong>tuples</strong> that are transactional and uses the following simpler features (e.g., cyl, disp, hp, drt, wt, asec, and so on ):</p>
<div class="sourceCode" id="cb1166"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1166-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1166-2" data-line-number="2"><span class="kw">attributes</span>(mtcars)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;mpg&quot;  &quot;cyl&quot;  &quot;disp&quot; &quot;hp&quot;   &quot;drat&quot; &quot;wt&quot;   &quot;qsec&quot; &quot;vs&quot;   &quot;am&quot;  
## [10] &quot;gear&quot; &quot;carb&quot;</code></pre>
<p>As we have shown, <strong>Feature Construction</strong> demands some expert knowledge in a specific domain. We need to understand the structure and mechanics involved in translating and converting inputs to their corresponding interpretable representation for a specific domain to construct features. In the next section, we expand the knowledge of <strong>Feature Engineering</strong> from the concept of construction to the concept of selection.</p>
</div>
<div id="featureselection" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.6</span> Feature Selection<a href="9.6-featureengineering.html#featureselection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Selection</strong> is a method of selecting salient features based on the strength of relevance. We favor relevantly strong features over those that are relevantly weak. </p>
<p>There are three common categories:</p>
<p><strong>Filter-based:</strong></p>
<p>This category selects features based on chosen scoring and ranking system. Scoring (or rating) and ranking may come in many shapes and flavors. Data is given a score and ordered in rank, after which a recommendation is carried out. We favor top-ranked features over low-ranks in the case of <strong>Filter-based</strong> feature selection.</p>
<p>The computation of the result is based upon which result is required. Therefore, we can use a combination of the following measures to compare the outcome. Some measures are already discussed in the <strong>Distance Metrics</strong> section. Others are discussed in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>) under the <strong>Information Theory</strong> Section.</p>

<table>
<tbody>
<tr class="odd">
<td align="left">Precision</td>
<td align="left">Support</td>
<td align="left">Distance</td>
</tr>
<tr class="even">
<td align="left">Recall</td>
<td align="left">Error Rate</td>
<td align="left">Loss vs Gain</td>
</tr>
<tr class="odd">
<td align="left">Sensitivity</td>
<td align="left">Accuracy</td>
<td align="left">Entropy (Mutual Information)</td>
</tr>
<tr class="even">
<td align="left">Specificity</td>
<td align="left">Similarity</td>
<td align="left">Correlation (Collinearity)</td>
</tr>
</tbody>
</table>

<p>For selection, one common comparison of features is using <strong>Correlation</strong> which we cover under <strong>linear regression analysis</strong> in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>). We also extend the concept of <strong>Correlation or Collinearity</strong> under the <strong>Exploratory Data Analysis</strong> section. Additionally, we have a long discussion on <strong>ANOVA</strong>, <strong>Chi-Square</strong>, and <strong>RMSE</strong> in Chapter <strong>6</strong>. More importantly, it helps to understand <strong>Statistical Interaction</strong> under the <strong>Linear Regression Modeling</strong> section in Chapter <strong>6</strong>. The idea is to evaluate the <strong>additive and interactive</strong> strengths of features when combined, along with the interaction of <strong>dummy or indicator variables</strong>.</p>
<p>Here, we use <strong>AUC on ROC</strong> as one of the many tools that can be used as a scoring mechanism for <strong>Filter-based Feature Selection</strong>. We use an actual <strong>ML</strong> training set based on using one of the five folds (5 randomly generated samples from the <strong>BreastCancer</strong> dataset from <strong>mlbench</strong> library). </p>

<div class="sourceCode" id="cb1168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1168-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1168-2" data-line-number="2"><span class="kw">library</span>(mlbench)</a>
<a class="sourceLine" id="cb1168-3" data-line-number="3"><span class="kw">data</span>(BreastCancer)</a>
<a class="sourceLine" id="cb1168-4" data-line-number="4">BC =<span class="st"> </span><span class="kw">na.omit</span>( BreastCancer )</a>
<a class="sourceLine" id="cb1168-5" data-line-number="5"><span class="co"># 5-fold (creates 5 groups)</span></a>
<a class="sourceLine" id="cb1168-6" data-line-number="6">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(BC<span class="op">$</span>Class, <span class="dt">k =</span> <span class="dv">5</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1168-7" data-line-number="7"><span class="co"># choose the first fold for our test group.</span></a>
<a class="sourceLine" id="cb1168-8" data-line-number="8">test =<span class="st"> </span>BC[fold.indices<span class="op">$</span>Fold1,]</a>
<a class="sourceLine" id="cb1168-9" data-line-number="9"><span class="co"># choose the other folds for training group.</span></a>
<a class="sourceLine" id="cb1168-10" data-line-number="10">train =<span class="st"> </span>BC[<span class="op">-</span>fold.indices<span class="op">$</span>Fold1,]</a>
<a class="sourceLine" id="cb1168-11" data-line-number="11"><span class="co"># preserve test label for comparison later</span></a>
<a class="sourceLine" id="cb1168-12" data-line-number="12">test.class =<span class="st"> </span>test<span class="op">$</span>Class </a>
<a class="sourceLine" id="cb1168-13" data-line-number="13">test<span class="op">$</span>Class =<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>

<p>We have the following Training set attributes:</p>
<div class="sourceCode" id="cb1169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1169-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1169-2" data-line-number="2"><span class="kw">attributes</span>(train)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;Id&quot;              &quot;Cl.thickness&quot;    &quot;Cell.size&quot;      
##  [4] &quot;Cell.shape&quot;      &quot;Marg.adhesion&quot;   &quot;Epith.c.size&quot;   
##  [7] &quot;Bare.nuclei&quot;     &quot;Bl.cromatin&quot;     &quot;Normal.nucleoli&quot;
## [10] &quot;Mitoses&quot;         &quot;Class&quot;</code></pre>
<p>We also have the Test set attributes (omitting the Label or Class):</p>
<div class="sourceCode" id="cb1171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1171-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1171-2" data-line-number="2"><span class="kw">attributes</span>(test)<span class="op">$</span>names</a></code></pre></div>
<pre><code>##  [1] &quot;Id&quot;              &quot;Cl.thickness&quot;    &quot;Cell.size&quot;      
##  [4] &quot;Cell.shape&quot;      &quot;Marg.adhesion&quot;   &quot;Epith.c.size&quot;   
##  [7] &quot;Bare.nuclei&quot;     &quot;Bl.cromatin&quot;     &quot;Normal.nucleoli&quot;
## [10] &quot;Mitoses&quot;</code></pre>
<p>Then we use a <strong>randomForest</strong> algorithm to model the fit with the <strong>multinomial</strong> dataset (in this case, we are using only the <strong>binomial</strong> dataset).</p>
<div class="sourceCode" id="cb1173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1173-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1173-2" data-line-number="2"><span class="co"># Random Forest Model</span></a>
<a class="sourceLine" id="cb1173-3" data-line-number="3">model.rf &lt;-<span class="st"> </span><span class="kw">randomForest</span>(Class <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1173-4" data-line-number="4">                         <span class="dt">ntree=</span><span class="dv">3</span>)</a></code></pre></div>
<p>Finally, we make a prediction and generate the <strong>AUC</strong> score using the <strong>performance(.)</strong> function from the <strong>ROCR</strong> library. For the TEST set, we use the following R code:</p>

<div class="sourceCode" id="cb1174"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1174-1" data-line-number="1"><span class="kw">library</span>(ROCR)</a>
<a class="sourceLine" id="cb1174-2" data-line-number="2"><span class="co"># Calculate AUC for test set</span></a>
<a class="sourceLine" id="cb1174-3" data-line-number="3">test.pred  =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model.rf, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>,<span class="dt">newdata =</span> test)[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb1174-4" data-line-number="4">test.pred  =<span class="st"> </span><span class="kw">prediction</span>(test.pred, test.class)</a>
<a class="sourceLine" id="cb1174-5" data-line-number="5">(<span class="dt">test.auc  =</span> <span class="kw">performance</span>(test.pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="op">@</span>y.values[[<span class="dv">1</span>]] )</a></code></pre></div>
<pre><code>## [1] 0.9631</code></pre>

<p>We calculate the performance of prediction (in the context of predicting the class) by comparing its <strong>Sensitivity</strong> and <strong>Specificity</strong> corresponding to their <strong>true-positive rates</strong> and <strong>false-positive rates</strong>. See Figure <a href="9.6-featureengineering.html#fig:rocrf1">9.59</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf1"></span>
<img src="DS_files/figure-html/rocrf1-1.png" alt="Prediction Performance" width="70%" />
<p class="caption">
Figure 9.59: Prediction Performance
</p>
</div>

<p>We also calculate the accuracy of the prediction. See Figure <a href="9.6-featureengineering.html#fig:rocrf2">9.60</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf2"></span>
<img src="DS_files/figure-html/rocrf2-1.png" alt="Prediction Performance" width="70%" />
<p class="caption">
Figure 9.60: Prediction Performance
</p>
</div>
<p>The <strong>AUC</strong> score and accuracy show that using the training set for the <strong>Random Forest</strong> algorithm makes a strong prediction. Here, our training and test sets contain relevant features. One has to manually mix and match the different combinations of the features to rank predictions and select the combination with higher <strong>AUC</strong> and <strong>Accuracy</strong> scores than other combinations.</p>
<p>In a previous section, we introduce the concept of <strong>AUC on ROC</strong> with details on comparing <strong>AUC</strong> of each feature (or their corresponding predictions). It helps to review the section on how we did a simple feature selection using <strong>AUC on ROC</strong>.</p>
<p>Lastly, we use dataset <strong>iris</strong> as an example to illustrate <strong>Feature Selection</strong>. Here, we use <strong>Random Forest</strong> to model our classification and use a few <strong>measures of importance</strong> such as <strong>MSE</strong> and <strong>Gini Index</strong>.</p>

<div class="sourceCode" id="cb1176"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1176-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1176-2" data-line-number="2"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1176-3" data-line-number="3"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1176-4" data-line-number="4"><span class="kw">data</span>(iris)</a>
<a class="sourceLine" id="cb1176-5" data-line-number="5">model.rf =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">importance =</span> <span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1176-6" data-line-number="6">pred.rf =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model.rf)</a>
<a class="sourceLine" id="cb1176-7" data-line-number="7"><span class="kw">confusionMatrix</span>(pred.rf, iris<span class="op">$</span>Species)</a></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##             Reference
## Prediction   setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         47         3
##   virginica       0          3        47
## 
## Overall Statistics
##                                         
##                Accuracy : 0.96          
##                  95% CI : (0.915, 0.985)
##     No Information Rate : 0.333         
##     P-Value [Acc &gt; NIR] : &lt;2e-16        
##                                         
##                   Kappa : 0.94          
##                                         
##  Mcnemar&#39;s Test P-Value : NA            
## 
## Statistics by Class:
## 
##                      Class: setosa Class: versicolor Class: virginica
## Sensitivity                  1.000             0.940            0.940
## Specificity                  1.000             0.970            0.970
## Pos Pred Value               1.000             0.940            0.940
## Neg Pred Value               1.000             0.970            0.970
## Prevalence                   0.333             0.333            0.333
## Detection Rate               0.333             0.313            0.313
## Detection Prevalence         0.333             0.333            0.333
## Balanced Accuracy            1.000             0.955            0.955</code></pre>

<p>To extract importance of features from <strong>random Forest</strong>, we use a function called <strong>varImpPlot(.)</strong> and rank features based on <strong>Mean Decrease Accuracy</strong> and <strong>Mean Decrease Gini</strong>. See Figure <a href="9.6-featureengineering.html#fig:rocrf3">9.61</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rocrf3"></span>
<img src="DS_files/figure-html/rocrf3-1.png" alt="Variable Importance" width="70%" />
<p class="caption">
Figure 9.61: Variable Importance
</p>
</div>
<p><strong>Mean Decrease in Accuracy (MDA)</strong>, also called <strong>Percent increase in MSE</strong>, as measured by <strong>Mean Squared Error (MSE)</strong>, ranks features based on how worst if variable is present. For example, <strong>Petal.width</strong> is important (or significant), <strong>Sepal.Width</strong> is worst in importance.    </p>
<p>Equivalently, <strong>Mean Decrease in Gini (MDG)</strong> ranks features based on how worst if variable is not present. For example, <strong>Petal.width</strong> is important, <strong>Sepal.Width</strong> is worst.  </p>
<p>The <strong>importance(.)</strong> function shows the importance of features. </p>

<div class="sourceCode" id="cb1178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1178-1" data-line-number="1">importance.model =<span class="st"> </span><span class="kw">importance</span>(model.rf)</a>
<a class="sourceLine" id="cb1178-2" data-line-number="2"><span class="kw">colnames</span>(importance.model) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;setosa&quot;</span>, <span class="st">&quot;versicolor&quot;</span>, <span class="st">&quot;virginica&quot;</span>, </a>
<a class="sourceLine" id="cb1178-3" data-line-number="3">                               <span class="st">&quot;MDAccuracy&quot;</span>, <span class="st">&quot;MDGini&quot;</span>)</a>
<a class="sourceLine" id="cb1178-4" data-line-number="4">importance.model</a></code></pre></div>
<pre><code>##              setosa versicolor virginica MDAccuracy MDGini
## Sepal.Length  6.169      8.087     7.862      10.78  9.557
## Sepal.Width   4.619      1.322     4.584       5.16  2.299
## Petal.Length 20.785     32.482    27.886      32.89 42.613
## Petal.Width  23.520     32.978    30.729      33.69 44.785</code></pre>

<p>The <strong>varUsed(.)</strong> function shows how frequently each variable is used. Notice below an imbalance of feature usage.</p>

<div class="sourceCode" id="cb1180"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1180-1" data-line-number="1"><span class="kw">varUsed</span>(model.rf)</a></code></pre></div>
<pre><code>## [1]  786  567 1255 1222</code></pre>

<p>Both <strong>MDA</strong> and <strong>MDG</strong> are farther discussed under <strong>Decision Trees</strong> subsection in <strong>Regression</strong> and <strong>Classification</strong> sections respectively.</p>
<p><strong>Wrapper-based:</strong></p>
<p>This category starts with a subset of features evaluated against a model based on performance measures. In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we have discussed <strong>Aikike Information Criterion (AIC)</strong> and <strong>Bayesian Information Criterion (BIC)</strong> in great detail. We encourage readers to review the algorithm in that Chapter and carefully review the <strong>forward, backward, and exhaustive steps and bidirectional steps</strong>. While the topic focuses on <strong>model selection</strong>, the underlying concept and method altogether apply to <strong>feature selection</strong> in that the best model chosen is the result of mixing and matching different combinations of features based on the <strong>AIC</strong> score.</p>
<p>Below is a summarized outcome of <strong>AIC-based</strong> scoring. We start with a formula that includes the following features: disp, hp, drat, wt, qsec. Then, after performing a <strong>bidirectional step</strong> for our <strong>Feature Selection</strong>, the final formula lists a reduced number of features, namely drat, wt, qsec.</p>

<div class="sourceCode" id="cb1182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1182-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st">  </span>disp <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec , <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb1182-2" data-line-number="2">(<span class="dt">selected.model =</span> <span class="kw">step</span>(initial.model, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, <span class="dt">k=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1182-3" data-line-number="3">                       <span class="dt">trace =</span> <span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ drat + wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)         drat           wt         qsec  
##      11.394        1.656       -4.398        0.946</code></pre>

<p>Given our formulaâs initial number of features, we start with a high <strong>AIC</strong> score of 65.4663, After the <strong>bidirectional AIC-based selection</strong>, the above formula with a reduced list of features gives us the lowest <strong>AIC</strong> score of 63.8911. See below:</p>
<div class="sourceCode" id="cb1184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1184-1" data-line-number="1">selected.model<span class="op">$</span>anova<span class="op">$</span>AIC</a></code></pre></div>
<pre><code>## [1] 65.47 64.21 63.89</code></pre>
<p><strong>Embedded-based:</strong></p>
<p>The feature selection in this category is part of model fitting. There is no need to subset features or to score features. Features are instead rewarded or penalized. That involves tuning <strong>coefficients</strong>, which gives weights to features - we call this <strong>Regularization</strong>. A detailed discussion of <strong>Regularization</strong> is discussed in a few sections ahead when we cover <strong>General Modeling</strong> in which we introduce <strong>Ridge</strong>, <strong>Lasso</strong>, and <strong>Elasticnet</strong> Regularization. The idea is that features are weighted and dropped from the list of relevant features as their weights reach zero.</p>
<p>The discussion of <strong>Embedded-based</strong> feature selection is covered primarily in Chapter <strong>13</strong> (<strong>Computational Deep Learning II</strong>).</p>
</div>
<div id="feature-transformation" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.7</span> Feature Transformation <a href="9.6-featureengineering.html#feature-transformation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Feature Transformation</strong> is a method to improve the interpretability of features by transforming features into a somewhat more interpretable <strong>synthetic</strong> form. In this literature, we regard this <strong>Feature Engineering</strong> component as <strong>Feature Extraction by Transformation</strong>.</p>
<p>Among many other transformations, we give about six transformations that may help us to improve interpretability. In fact, a few of them are derived from <strong>Econometrics</strong> dealing with <strong>Pooled Models</strong> and <strong>Panel Models</strong>, including <strong>homogeneity</strong> and <strong>heterogeneity</strong> of observations which we introduced in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>). We wrap the <strong>models</strong> around the context of <strong>Feature Transformation</strong> in a sense because certain raw data can be merged cross-sectionally or cross-sequentially and arrived at a newly formed <strong>synthetic</strong> set of features. <strong>Synthesized</strong> features may undergo different modeling altogether. That brings us to models such as <strong>Pooled OLS model</strong>, <strong>Fixed Effect Model (FEM)</strong>, and <strong>Random Effect Model (REM)</strong>, which we introduce under the <strong>Time-Series Forecasting</strong> section in Chapter <strong>11</strong> (<strong>Computational Learning III</strong>).</p>
<p><strong>One Hot Encoding:</strong></p>
<p>This transformation allows the creation of a new separate feature for each category in a feature expressed in the form of a one-hot vector. We have covered <strong>One Hot encoding</strong> in detail in the <strong>Exploratory Data Analysis</strong> section.</p>
<p><strong>Bucketize Feature Columns:</strong></p>
<p>This transformation allows assigning feature values into corresponding buckets. That is almost equivalent to <strong>binning</strong>; however, instead of <strong>discretizing</strong> numerical values into <strong>discrete</strong> values, we put values (whether numerical or discrete) into bounded buckets - or a set of ranges. For example, one good feature to extract from a date feature and then bucketize is the year. Our example below shows that essential events are categorized into two buckets based on dates representing a decade between 2001-2010 and another decade between 2011-2020 (note that the events are fictitious except for the COVID-19 pandemic). Here, a new feature called <strong>Decade</strong> is added.</p>

<table>
<caption><span id="tab:bucketize">Table 9.35: </span>Bucketize Feature (Yearly)</caption>
<tbody>
<tr class="odd">
<td align="left">Important Event</td>
<td align="left">Date</td>
<td align="left">Decade</td>
</tr>
<tr class="even">
<td align="left">Discovery of Time Machine</td>
<td align="left">01-Jan-2009</td>
<td align="left">2001-2010</td>
</tr>
<tr class="odd">
<td align="left">First Encounter of the 4th kind</td>
<td align="left">23-Feb-2010</td>
<td align="left">2001-2010</td>
</tr>
<tr class="even">
<td align="left">Covid Pandemic</td>
<td align="left">11-Mar-2020</td>
<td align="left">2011-2020</td>
</tr>
</tbody>
</table>

<p><strong>Crossed Feature Columns:</strong></p>
<p>This transformation allows multiple features to be combined into one feature. Each feature crosses the other to form one new feature. That is different from <strong>correlation</strong> in which we drop duplicates. That is also different from <strong>interactive</strong> of multiple features as discussed in <strong>Statistical Computation</strong> in which we multiply the weights of two features.</p>
<p>An example of a <strong>crossed feature</strong> is when combining <strong>longitude</strong> and <strong>latitude</strong> to develop a new feature called <strong>global position</strong>. The value of the new feature can be represented by hashing the longitude and latitude together as an example, and then perhaps implement <strong>bucketize feature transformation</strong> to further process the feature into buckets - such as into <strong>regions, zones, or hemispheres</strong>.</p>
<p><strong>Cross-Sectional vs Longitudinal Columns:</strong> </p>
<p>Both <strong>Cross-Sectional Feature Columns (CSFC)</strong> and <strong>Longitudinal Feature Columns (LFC)</strong>, also called <strong>Panel Feature Columns (PFC)</strong>, do extract data from time-series data. <strong>CSFC</strong> differs from <strong>LFC</strong> in that <strong>CSFC</strong> analyzes subjects of varying categories at only one moment (a snapshot), whereas <strong>LFC</strong> analyzes subjects in the same category across the varying period. See Figure <a href="9.6-featureengineering.html#fig:crosssectional">9.62</a>. For cross-sectional analysis, the data of interest contains subjects under age brackets 20, 30, 40, 50, and 60 in 2016. Moreover, for longitudinal analysis, the data of interest contains subjects under Cohorts D across the years 2015, 2016, 2017, and 2018.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:crosssectional"></span>
<img src="cross_sectional.png" alt="Cross-Sectional, Longitudinal, Cross-Sequential" width="90%" />
<p class="caption">
Figure 9.62: Cross-Sectional, Longitudinal, Cross-Sequential
</p>
</div>
<p><strong>Cross-Sequential Feature Columns:</strong> </p>
<p><strong>Cross-Sequential Features</strong> combines both <strong>Cross-Sectional</strong> and <strong>Longitudinal</strong> features in that, for example, subjects under age 50 across Cohorts A through D is being analyzed across the year 2015, 2016, 2017, 2018. We may also perform the same analysis for subjects under age 40 and so on.</p>
<p><strong>Cross-Sectional Windowing:</strong></p>
<p>Now, in terms of transformation, the tables in Figure <a href="9.6-featureengineering.html#fig:crosssectional">9.62</a> are assumed to be coming from time-series data. The transformation of <strong>CSFC</strong> from time-series data into another <strong>synthetic</strong> form relies on <strong>Windowing</strong>. As an example, in Figure <a href="9.6-featureengineering.html#fig:windowing">9.63</a>, if we are to account for seasonality on monthly sales, perhaps we can stretch an overlapping 4-month window that covers Holiday sales and see if sales are higher than non-Holiday sales.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:windowing"></span>
<img src="windowing.png" alt="Windowing (Cross-Sectional)" width="100%" />
<p class="caption">
Figure 9.63: Windowing (Cross-Sectional)
</p>
</div>
<p><strong>Pooled Data Model vs Panel Data Model:</strong>  </p>
<p><strong>Pooled Model</strong> and <strong>Panel Model</strong> are terms that can be interchanged with <strong>Cross-Sectional Model</strong> and <strong>Longitudinal Model</strong>. In concept, <strong>Pooled model</strong> refers to sampling different subjects of the same population that are independently collected at different time intervals and pooled together, whereas <strong>Panel model</strong> refers to sampling the same set of subjects collected at different times intervals.</p>
<p>One reason for analyzing the panel dataset is to determine resiliency and sustainability.</p>
</div>
<div id="model-specification-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.6.8</span> Model Specification <a href="9.6-featureengineering.html#model-specification-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we introduce the concept of <strong>Statistical Model Specification</strong>, or <strong>Model Specification</strong> in general. In specifying a model, we need to be able to describe and define the relationships of observations and features and what they represent. Not having a correct specification may result in an imbalance of <strong>bias</strong> and <strong>variance</strong>.</p>
<p>In fitting a poorly specified model, we may consequently obtain inaccurate results. We can say that inaccurate results can either be <strong>underspecified</strong> or <strong>overspecified</strong>. To mitigate such situations, part of the goal of <strong>feature engineering</strong> as discussed in the previous section and which we discuss next in subsequent sections around <strong>model building</strong> is to understand how different <strong>machine learning</strong> algorithms introduce regularization as a way to measure the importance of observations and features. The existence of a feature that may be deemed not necessary or relevant may create a misrepresentation of our model. Similarly, a missing feature may create <strong>data shift</strong> or <strong>data perturbation</strong> - observations that are not accounted for yet necessary or relevant.</p>
<p>In the context of <strong>anomalies</strong> and <strong>outliers</strong>, it helps to identify, distinguish and describe the existence of such observations - such rare phenomena - and specify their requirements, representations, and relationships.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="9.5-exploratory-data-analysis.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9.7-general-modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
