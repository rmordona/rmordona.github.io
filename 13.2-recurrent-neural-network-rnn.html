<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.2 Recurrent Neural Network (RNN)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="13.2 Recurrent Neural Network (RNN)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.2 Recurrent Neural Network (RNN)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="13.1-residual-network-resnet.html"/>
<link rel="next" href="13.3-deep-stacked-rnn.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><em>Bibliography</em></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="recurrent-neural-network-rnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.2</span> Recurrent Neural Network (RNN)  <a href="13.2-recurrent-neural-network-rnn.html#recurrent-neural-network-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now switch context to another type of <strong>Neural Network</strong> that can handle sequential data. In the context of <strong>Recurrent Neural Network (RNN)</strong>, our emphasis is on the <strong>recurrence</strong> of a sequence of data points. While <strong>CNN</strong> commonly intends to learn patterns from a static list of images to perform image classification, <strong>RNN</strong> intends to recognize and learn <strong>recurring</strong> patterns from a set of sequential data points to perform prediction for the next sequence of patterns. This ability to predict based on a recurring sequence of events showcases the power of <strong>RNN</strong>, which opens up even wider possible applications such as speech recognition, language translation, self-driving automobiles, and self-serve robots in restaurants.</p>
<p>To build up our intuition on <strong>RNN</strong>, let us first understand the two diagrams shown in Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:unrolledrnn">13.3</a>. We illustrate an <strong>RNN</strong> box known as <strong>RNN Cell</strong> or <strong>RNN Unit</strong>. The cell takes <strong>X</strong> as input and produces <span class="math inline">\(\mathbf{\hat{Y}}\)</span> as output. Notice a second output in the form of <strong>H</strong> comes out of the unit and gets fed back through it. This output is called <strong>Hidden State</strong> (also called <strong>Cell State</strong>), and it exists as a memory state preserving information.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unrolledrnn"></span>
<img src="unrolledrnn.png" alt="RNN (Unrolled Representation)" width="80%" />
<p class="caption">
Figure 13.3: RNN (Unrolled Representation)
</p>
</div>
<p>Here, we note that the <strong>X</strong> input ideally requires a recurrent sequential pattern. A straightforward example of a recurring input is a sentence formed based on a specific language, e.g., the English language. Because a language follows a certain syntactic and semantic structure, it suggests some level of sequential pattern in that a sentence is made up of a sequence of words as input. Note here that we do not simply consider a bag of words with no order. The sequence affects the semantics or meaning of a sentence if otherwise unordered. For example, let us use one sentence derived from <strong>Og Mandino</strong>âs The Greatest Salesman in the world:</p>
<p><span class="math display">\[
\text{Today, I begin a new life.}
\]</span>
Our simple goal is to feed the sample sentence into <strong>RNN</strong> in the form of individual words such that the word <strong>Today</strong> is fed first through RNN at time <span class="math inline">\(\mathbf{t_0}\)</span>, then the word <strong>I</strong> is fed next at time <span class="math inline">\(\mathbf{t_1}\)</span> followed by the word <strong>begin</strong> at <span class="math inline">\(\mathbf{t_2}\)</span>, and so on. The words are fed through the <strong>RNN cell</strong> recursively. If we <strong>unroll</strong> the <strong>RNN cell</strong>, we see the operations across time as depicted in the <strong>unrolled representation</strong> in Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:unrolledrnn">13.3</a>. The word <strong>Today</strong> is represented by <span class="math inline">\(\mathbf{X_0}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_0}\)</span> output and <span class="math inline">\(\mathbf{H_0}\)</span> cell state. The word <strong>I</strong> is represented by <span class="math inline">\(\mathbf{X_1}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_1}\)</span> output and <span class="math inline">\(\mathbf{H_1}\)</span> cell state. The word <strong>begin</strong> is represented by <span class="math inline">\(\mathbf{X_2}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_1}\)</span> output and <span class="math inline">\(\mathbf{H_2}\)</span> cell state. And so on. We can then stop feeding <strong>RNN</strong> after the word <strong>new</strong> and try to allow <strong>RNN</strong> to predict what is the last word. In our case, the last word is <strong>life</strong>.</p>
<p>Note that we do not feed the exact form of the sentence into <strong>RNN</strong>. Instead, we take each word in the sentence and convert it into a unique numeric representation. We discuss this technique of casting words to numbers more in the <strong>Attention</strong> section.</p>
<p>Now in <strong>RNN</strong>, we may encounter four general types of sequence models discussed. Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:rnnsequence">13.4</a> provides a diagram of each type <span class="citation">(Karpathy A. <a href="bibliography.html#ref-ref1186a">2015</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnsequence"></span>
<img src="rnnsequence.png" alt="RNN (Sequence Models)" width="80%" />
<p class="caption">
Figure 13.4: RNN (Sequence Models)
</p>
</div>
<p>The <strong>RNN</strong> algorithm may differ depending on the sequence (and application) used based on Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:rnnsequence">13.4</a>. For example, using the <strong>One to One</strong> sequence model for a <strong>binary classification</strong> of a single image may be appropriate. The <strong>One to Many</strong> sequence model is appropriate for a single image such as a <strong>cifar-10</strong> image, as the case may be in our <strong>CNN</strong> example. The <strong>Many to One</strong> sequence model may apply in sentiment analysis in which we feed <strong>RNN</strong> with a sequence of input (words or images) and determine sentimentally if the sequence exudes a positive vibe. The <strong>Many to Many</strong> design makes it possible to perform language translation <span class="citation">(Karpathy A. <a href="bibliography.html#ref-ref1186a">2015</a>)</span>. Alternatively (not included in the figure), a combined model of the <strong>Many to One</strong> and <strong>One to Many</strong> can be used as encoder and decoder, respectively, as commonly depicted in other literature.</p>
<p>In the next few sections, let us introduce three <strong>RNN</strong> variances, namely <strong>Vanilla RNN</strong>, <strong>LSTM</strong>, and <strong>GRU</strong>.</p>
<div id="vanilla-rnn" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.1</span> Vanilla RNN<a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us recall that a linear equation is made up of the following general example:</p>
<p><span class="math display" id="eq:equate1150001">\[\begin{align}
\hat{y} = f(x) = \omega_0 + x_1 \omega_1 + x_2 \omega_2 +\ ...\ + x_p \omega_p \tag{13.1} 
\end{align}\]</span></p>
<p>where the omega (<span class="math inline">\(\omega\)</span>) symbol represents parameters or coefficients required for network learning. Additionally, considering Figure <a href="12.1-simple-perceptron.html#fig:perceptron">12.3</a>, we also use an activation function to achieve non-linearity. In this case, we use <strong>tanh</strong>. </p>
<p><span class="math display" id="eq:equate1150002">\[\begin{align}
\hat{y} = f_a(x) = \text{tanh}(\omega_0 + x_1 \omega_1 + x_2 \omega_2 +\ ...\ + x_p \omega_p) \tag{13.2} 
\end{align}\]</span></p>
<p>We can easily modify the diagram in Figure <a href="12.1-simple-perceptron.html#fig:perceptron">12.3</a> to reflect an <strong>RNN</strong> cell with the existence of <strong>cell states</strong>. See Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:vanillarnn">13.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vanillarnn"></span>
<img src="vanillarnn.png" alt="Vanilla RNN Cell" width="80%" />
<p class="caption">
Figure 13.5: Vanilla RNN Cell
</p>
</div>
<p>The figure shows that <strong>RNN</strong> uses two equations. The first equation is expressed as such:</p>
<p><span class="math display" id="eq:equate1150003">\[\begin{align}
\mathbf{H}_{nxh}^{(t)} = \mathbf{f_a}\left(\mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right) = \mathbf{\text{tanh}}\left( \mathbf{X}_{nxp}^{(t)} \cdotp  \mathbf{W}_{pxh} +  \mathbf{H}_{nxh}^{(t-1)} \cdotp \mathbf{U}_{hxh} + \mathbf{b}_{1xh}^{(h)} \right) \tag{13.3} 
\end{align}\]</span></p>
<p>The <strong>cell state</strong> (<span class="math inline">\(\mathbf{H^{(t)}}\)</span>) is preserved as a memory state to be used by the next iteration. Note here that the input <span class="math inline">\(\mathbf{X^{(t)}}\)</span> and previous cell state <span class="math inline">\(\mathbf{H^{(t-1)}}\)</span> are multiplied by their respective weights then summed together along with a bias (<span class="math inline">\(\mathbf{b}_{1xh}\)</span>); after which, the result is then fed through the <strong>tanh</strong> activation function.</p>
<p>The second equation is expressed as such:</p>
<p><span class="math display" id="eq:equate1150004">\[\begin{align}
\mathbf{\hat{Y}}_{nxo}^{(t)} = \mathbf{f}\left(\mathbf{H}_{nxh}^{(t)}\right) = 
\mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \times \mathbf{V}_{hxo} + \mathbf{b}_{1xo}^{(y)}\right)  \tag{13.4} 
\end{align}\]</span></p>
<p>This yields the <span class="math inline">\(\mathbf{\hat{Y}_{nxo}^{(t)}}\)</span> output produced by a <strong>softmax</strong> function, in this case, performing classification.</p>
<p>In terms of dimensions, Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:rnnforward">13.6</a> illustrates how the dot-product and summation operations handle the dimension of the input, output, cell state, and parameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnforward"></span>
<img src="rnnforward.png" alt="Vanilla RNN Dimensions" width="80%" />
<p class="caption">
Figure 13.6: Vanilla RNN Dimensions
</p>
</div>
<p>Given the following activation functions, the structure in Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:vanillarnn">13.5</a> has the following implementation:</p>

<div class="sourceCode" id="cb2085"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2085-1" data-line-number="1">rnn.tanh     &lt;-<span class="st"> </span><span class="cf">function</span>(x) { (<span class="kw">exp</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) <span class="op">/</span><span class="st"> </span>( <span class="kw">exp</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb2085-2" data-line-number="2">rnn.sigmoid         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb2085-3" data-line-number="3">rnn.softmax  &lt;-<span class="st"> </span><span class="cf">function</span>(x) { p =<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, max); x =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>p; p =<span class="st"> </span><span class="kw">exp</span>(x) </a>
<a class="sourceLine" id="cb2085-4" data-line-number="4">                              s =<span class="st"> </span><span class="kw">apply</span>(p, <span class="dv">1</span>, sum); <span class="kw">sweep</span>(p, <span class="dv">1</span>, s, <span class="st">&quot;/&quot;</span>) </a>
<a class="sourceLine" id="cb2085-5" data-line-number="5">}</a></code></pre></div>

<p>Now suppose we have an <span class="math inline">\(\mathbf{n \times p}\)</span> <strong>input</strong> (<strong>X</strong>) (e.g., it has <strong>n</strong> samples and <strong>p</strong> features) with its corresponding <span class="math inline">\(\mathbf{p \times h}\)</span> <strong>weight</strong> (<strong>W</strong>), a <strong>hidden state</strong> (<strong>H</strong>) with <span class="math inline">\(\mathbf{n \times h}\)</span> dimension (e.g., <strong>h</strong> neurons) and its corresponding <span class="math inline">\(\mathbf{h \times h}\)</span> ** weight** (<strong>U</strong>), we need to calculate the <span class="math inline">\(\mathbf{n \times o}\)</span> <strong>output</strong> (<span class="math inline">\(\mathbf{\hat{Y}}\)</span>). Below is a sample use of the functions and structure:</p>

<div class="sourceCode" id="cb2086"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2086-1" data-line-number="1">n     =<span class="st"> </span><span class="dv">5</span>    <span class="co"># number of samples</span></a>
<a class="sourceLine" id="cb2086-2" data-line-number="2">p     =<span class="st"> </span><span class="dv">30</span>   <span class="co"># number of features per sample (could also mean number of </span></a>
<a class="sourceLine" id="cb2086-3" data-line-number="3">             <span class="co"># probabilities of a word embedding)</span></a>
<a class="sourceLine" id="cb2086-4" data-line-number="4">h     =<span class="st"> </span><span class="dv">20</span>   <span class="co"># number of neurons in a hidden state</span></a>
<a class="sourceLine" id="cb2086-5" data-line-number="5">o     =<span class="st"> </span><span class="dv">3</span>    <span class="co"># number of output neurons in an output layer</span></a>
<a class="sourceLine" id="cb2086-6" data-line-number="6">X     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-7" data-line-number="7">H     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-8" data-line-number="8">W     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(p <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-9" data-line-number="9">U     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(h <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-10" data-line-number="10">V     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(h <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span>h, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-11" data-line-number="11">bh    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-12" data-line-number="12">by    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-13" data-line-number="13">H     =<span class="st"> </span><span class="kw">rnn.tanh</span>(<span class="kw">sweep</span>(X <span class="op">%*%</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>U, <span class="dv">2</span>, bh, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2086-14" data-line-number="14">Y     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>

<p>Below is the result showing the dimensions of <strong>H</strong> and <strong>Y</strong>:</p>

<div class="sourceCode" id="cb2087"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2087-1" data-line-number="1"><span class="kw">str</span>(<span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)) <span class="co"># put in a list for  better display</span></a></code></pre></div>
<pre><code>## List of 2
##  $ H: num [1:5, 1:20] 0.968 0.992 -0.88 0.999 0.975 ...
##  $ Y: num [1:5, 1:3] 0.13742 2.11004 -0.00548 -0.75795 -0.89994 ...</code></pre>

<p>Similar to <strong>MLP</strong> and <strong>CNN</strong>, it should be noted that <strong>RNN</strong> also considers the use of <strong>forward feed</strong> and <strong>backpropagation</strong>.</p>
<p><strong>Forward Feed</strong></p>
<p>Here, we follow a straightforward implementation of <strong>RNN</strong> forward feed based on the immediate equations above.</p>

<div class="sourceCode" id="cb2089"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2089-1" data-line-number="1">forward.unit.RNN &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, params) {</a>
<a class="sourceLine" id="cb2089-2" data-line-number="2">    W     =<span class="st"> </span>params<span class="op">$</span>W</a>
<a class="sourceLine" id="cb2089-3" data-line-number="3">    U     =<span class="st"> </span>params<span class="op">$</span>U</a>
<a class="sourceLine" id="cb2089-4" data-line-number="4">    V     =<span class="st"> </span>params<span class="op">$</span>V</a>
<a class="sourceLine" id="cb2089-5" data-line-number="5">    bh    =<span class="st"> </span>params<span class="op">$</span>bh</a>
<a class="sourceLine" id="cb2089-6" data-line-number="6">    by    =<span class="st"> </span>params<span class="op">$</span>by</a>
<a class="sourceLine" id="cb2089-7" data-line-number="7">    Ht    =<span class="st"> </span><span class="kw">rnn.tanh</span>(<span class="kw">sweep</span>(X <span class="op">%*%</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>U, <span class="dv">2</span>, bh, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2089-8" data-line-number="8">    Y.hat =<span class="st"> </span><span class="kw">rnn.softmax</span>(<span class="kw">sweep</span>(H <span class="op">%*%</span><span class="st"> </span>V, <span class="dv">2</span>, by, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2089-9" data-line-number="9">    <span class="kw">list</span>(<span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;Y.hat&quot;</span> =<span class="st"> </span>Y.hat)</a>
<a class="sourceLine" id="cb2089-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb2089-11" data-line-number="11">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;W&quot;</span> =<span class="st"> </span>W, <span class="st">&quot;U&quot;</span> =<span class="st"> </span>U, <span class="st">&quot;V&quot;</span> =<span class="st"> </span>V, <span class="st">&quot;bh&quot;</span> =<span class="st"> </span>bh, <span class="st">&quot;by&quot;</span> =<span class="st"> </span>by)</a>
<a class="sourceLine" id="cb2089-12" data-line-number="12">model =<span class="st"> </span><span class="kw">forward.unit.RNN</span>(X, H,  params)</a></code></pre></div>

<p>Below is the result showing the dimension of <strong>H</strong> and <span class="math inline">\(\mathbf{\hat{Y}}\)</span>:</p>

<div class="sourceCode" id="cb2090"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2090-1" data-line-number="1"><span class="kw">str</span>(model)</a></code></pre></div>
<pre><code>## List of 2
##  $ Ht   : num [1:5, 1:20] 1 1 0.845 0.998 0.901 ...
##  $ Y.hat: num [1:5, 1:3] 0.895 0.565 0.939 1 0.998 ...</code></pre>

<p><strong>Backpropagation</strong></p>
<p>We start our discussion of <strong>RNN backpropagation</strong> by using Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:rnnbackprop">13.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnbackprop"></span>
<img src="rnnbackprop.png" alt="RNN (Softmax)" width="90%" />
<p class="caption">
Figure 13.7: RNN (Softmax)
</p>
</div>
<p>Similar to <strong>CNN</strong>, our implementation of <strong>RNN</strong> uses <strong>Cross-Entropy Loss</strong> for <strong>Softmax function</strong> (which is also the case for <strong>LSTM</strong> and <strong>GRU</strong> in next sections ahead). If we recall <strong>Delta Rule</strong> in <strong>MLP</strong> section, we start by obtaining <strong>Delta o</strong> (<span class="math inline">\(\mathbf{\delta o}\)</span>) which is the gradient of our loss with respect to the activation function (in this is case, we use softmax). Here, it helps to recall the derivation of the equation under <strong>activation function</strong> subsection in <strong>MLP</strong> section. Note that in <strong>MLP</strong> we use the <strong>hat-o</strong> (<span class="math inline">\(\hat{\mathbf{o}}\)</span>) symbol for the <strong>linear function</strong> of our output and <strong>no-hat</strong> (<span class="math inline">\(\mathbf{o}\)</span>) symbol for our <strong>non-linear function</strong>. See below:</p>
<p><span class="math display" id="eq:equate1150005">\[\begin{align}
\delta_o = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right) \left(\frac{\partial o}{\partial \hat{o}}\right) = (o - t) \tag{13.5} 
\end{align}\]</span></p>
<p>This should not be confused now about our use of <strong>y-hat</strong> (<span class="math inline">\(\hat{\mathbf{y}}\)</span>) symbol for our <strong>softmax</strong> and <strong>y</strong> for our target (<strong>t</strong>) in <strong>RNN</strong>; whereas, the <strong>h-hat</strong> (<span class="math inline">\(\hat{\mathbf{h}}\)</span>) symbol represents output of our linear function. Therefore, in our case for <strong>RNN</strong>, we have the following equation:</p>
<p><span class="math display" id="eq:equate1150006">\[\begin{align}
\delta_{y} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{h}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{y}}\right) \left(\frac{\partial \hat{y}}{\partial \hat{h}}\right) = (\hat{y} - y) \tag{13.6} 
\end{align}\]</span></p>
<p>It should then be straightforward to derive the gradient of <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>, <span class="math inline">\(\mathbf{V}_{hxo}\)</span>, and <span class="math inline">\(\mathbf{b}_{1xh}^{(y)}\)</span> (in the order shown below):</p>
<p><span class="math display" id="eq:equate1150009" id="eq:equate1150008" id="eq:equate1150007">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} 
  &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} + \delta \mathbf{H}_{nxh}^{(t+1)}  
  =   \delta y 
    \left(\frac{\partial \hat{h}} {\partial  \mathbf{H}_{nxh}^{(t)}} 
    \right) + \delta \mathbf{H}_{nxh}^{(t+1)} 
  = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \delta \mathbf{H}_{nxh}^{(t+1)}  \tag{13.7} \\
\nabla \mathbf{V_{hxo}} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{V}_{hxo}} 
    = 
    \left(\frac{\partial \hat{h}} {\partial \mathbf{V_{hxo}} }  
    \right)  \delta y 
    =  \left(\mathbf{H}_{nxh}^{(t)}\right)^{\text{T}} \cdot \delta y  \tag{13.8} \\
\nabla \mathbf{b}_{1xo}^{(y)} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{b}_{1xo}^{(y)} }  = \sum_{column-wise}{\delta y}   \tag{13.9} 
\end{align}\]</span></p>
<p>Also, notice here the addition of the second term, namely <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t+1)}\right)\)</span>, which we also refer to as <strong>dH.next</strong> in our implementation. Because there is no time step <strong>t+1</strong> at the start of backpropagation, <strong>dH.next</strong> is initially zero.</p>
<p>Next, we then solve for gradient of our activation function (the <strong>tangent</strong>) with respect to <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>. Here, we know that the first derivative of tangent is written as:</p>
<p><span class="math display" id="eq:equate1150010">\[\begin{align}
\mathbf{\text{tanh}}&#39;(a) = \left(1 - \mathbf{tanh}^2(a)\right) \tag{13.10} 
\end{align}\]</span></p>
<p>We use the derivative to construct our <strong>Delta tanh</strong> like so:</p>
<p><span class="math display" id="eq:equate1150011">\[\begin{align}
\delta\ \mathbf{\text{tanh}} = ( 1 - \mathbf{\text{tanh}}^2(a)) \odot \delta a = ( 1 - \mathbf{\text{tanh}}^2(\mathbf{H}_{nxh}^{(t)})) \odot \delta \mathbf{H}_{nxh}^{(t)} \tag{13.11} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(a = \mathbf{H}_{nxh}^{(t)}\)</span>.</p>
<p>From there, we can derive the gradient of our loss with respect to <span class="math inline">\(\mathbf{W}_{pxh}\)</span>, <span class="math inline">\(\mathbf{U}_{hxh}\)</span>, and <span class="math inline">\(\mathbf{b}_{1xh}^{(y)}\)</span>.</p>
<p><span class="math display" id="eq:equate1150012">\[\begin{align}
\nabla  \mathbf{W}_{pxh}  = \left(\mathbf{X}_{nxp}^{(t-1)}\right)^\text{T} \cdotp  \delta \mathbf{\text{tanh}}
\ \ \ \ \ \ \ \ \ 
 \nabla  \mathbf{U}_{hxh}  = \left( \mathbf{H}_{nxh}^{(t-1)}\right)^\text{T} \cdotp  \delta \mathbf{\text{tanh}} \tag{13.12} 
\end{align}\]</span>
<span class="math display" id="eq:equate1150013">\[\begin{align}
 \nabla \mathbf{b}_{1xh}^{(y)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{b}_{1xh}^{(h)} } 
     =  \delta\mathbf{\text{tanh}} \tag{13.13} 
\end{align}\]</span></p>
<p>For the gradient of the loss with respect to previous <strong>X</strong> and <strong>H</strong>, we perform the following equations:</p>
<p><span class="math display" id="eq:eqnnumber802" id="eq:eqnnumber801">\[\begin{align}
\underbrace{\delta \mathbf{H}_{nxh}^{(t-1)}}_{
     \begin{array}{ll}
     \mathbf{\text{dH.prev becomes}}\\
     \mathbf{\text{new dH.next later}}
     \end{array}
     }  
  = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t-1)}} 
     = \delta\mathbf{\text{tanh}} \cdotp \left(\mathbf{U}_{hxh}\right)^\text{T} \tag{13.14}\\
\delta \mathbf{X}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{X}_{nxh}^{(t)}} 
     = \delta\mathbf{\text{tanh}} \cdotp \left(\mathbf{W}_{pxh}\right)^\text{T} \tag{13.15}
\end{align}\]</span></p>
<p>Below is our example implementation of <strong>RNN backpropagation</strong>:</p>

<div class="sourceCode" id="cb2092"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2092-1" data-line-number="1">backward.unit.RNN &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next, X, Y, model, params) {</a>
<a class="sourceLine" id="cb2092-2" data-line-number="2">    W           =<span class="st"> </span>params<span class="op">$</span>W</a>
<a class="sourceLine" id="cb2092-3" data-line-number="3">    U           =<span class="st"> </span>params<span class="op">$</span>U</a>
<a class="sourceLine" id="cb2092-4" data-line-number="4">    V           =<span class="st"> </span>params<span class="op">$</span>V</a>
<a class="sourceLine" id="cb2092-5" data-line-number="5">    bh          =<span class="st"> </span>params<span class="op">$</span>bh</a>
<a class="sourceLine" id="cb2092-6" data-line-number="6">    by          =<span class="st"> </span>params<span class="op">$</span>by</a>
<a class="sourceLine" id="cb2092-7" data-line-number="7">    Y.hat       =<span class="st"> </span>model<span class="op">$</span>Y.hat</a>
<a class="sourceLine" id="cb2092-8" data-line-number="8">    dy          =<span class="st"> </span>(Y.hat <span class="op">-</span><span class="st"> </span>Y) </a>
<a class="sourceLine" id="cb2092-9" data-line-number="9">    dH          =<span class="st"> </span>dy <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V) <span class="op">+</span><span class="st"> </span>dH.next  <span class="co"># dealing with one-hot encoding</span></a>
<a class="sourceLine" id="cb2092-10" data-line-number="10">    dV          =<span class="st"> </span><span class="kw">t</span>(H) <span class="op">%*%</span><span class="st"> </span>dy </a>
<a class="sourceLine" id="cb2092-11" data-line-number="11">    dby         =<span class="st"> </span><span class="kw">apply</span>(dy, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb2092-12" data-line-number="12">    dtanh       =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>model<span class="op">$</span>Ht<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>dH  <span class="co"># Gradient of Tanh wrt H</span></a>
<a class="sourceLine" id="cb2092-13" data-line-number="13">    dX          =<span class="st"> </span>dtanh <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(W)         <span class="co"># delta X for BP through time</span></a>
<a class="sourceLine" id="cb2092-14" data-line-number="14">    dH          =<span class="st"> </span>dtanh <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(U)         <span class="co"># delta H for BP throught time</span></a>
<a class="sourceLine" id="cb2092-15" data-line-number="15">    dW          =<span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>dtanh         <span class="co"># Gradient of Loss wrt W</span></a>
<a class="sourceLine" id="cb2092-16" data-line-number="16">    dU          =<span class="st"> </span><span class="kw">t</span>(H) <span class="op">%*%</span><span class="st"> </span>dtanh         <span class="co"># Gradient of Loss wrt U</span></a>
<a class="sourceLine" id="cb2092-17" data-line-number="17">    dbh         =<span class="st"> </span><span class="kw">apply</span>(dtanh, <span class="dv">2</span>, sum)   <span class="co"># Gradient of Loss wrt Bias</span></a>
<a class="sourceLine" id="cb2092-18" data-line-number="18">    <span class="kw">list</span>(<span class="st">&quot;dX&quot;</span> =<span class="st"> </span>dX, <span class="st">&quot;dH&quot;</span> =<span class="st"> </span>dH, <span class="st">&quot;dW&quot;</span> =<span class="st"> </span>dW,  <span class="st">&quot;dU&quot;</span> =<span class="st"> </span>dU, <span class="st">&quot;dbh&quot;</span> =<span class="st"> </span>dbh,</a>
<a class="sourceLine" id="cb2092-19" data-line-number="19">         <span class="st">&quot;dV&quot;</span> =<span class="st"> </span>dV, <span class="st">&quot;dby&quot;</span> =<span class="st"> </span>dby)</a>
<a class="sourceLine" id="cb2092-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb2092-21" data-line-number="21">params     =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;W&quot;</span> =<span class="st"> </span>W, <span class="st">&quot;U&quot;</span> =<span class="st"> </span>U, <span class="st">&quot;V&quot;</span> =<span class="st"> </span>V)</a>
<a class="sourceLine" id="cb2092-22" data-line-number="22">dH.next    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2092-23" data-line-number="23">gradients  =<span class="st"> </span><span class="kw">backward.unit.RNN</span>(dH.next, X, Y, model, params)</a>
<a class="sourceLine" id="cb2092-24" data-line-number="24"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 7
##  $ dX : num [1:5, 1:30] 2.4484 -0.0373 1.2621 4.4449 2.2843 ...
##  $ dH : num [1:5, 1:20] -0.795 7.254 -0.411 -3.118 0.463 ...
##  $ dW : num [1:30, 1:20] 0.271 0.413 0.219 0.198 0.146 ...
##  $ dU : num [1:20, 1:20] -0.339 0.413 0.439 -0.442 0.43 ...
##  $ dbh: num [1:20] 0.442 1.185 0.499 1.524 0.403 ...
##  $ dV : num [1:20, 1:3] 1.9744 -1.4219 0.0203 -3.8125 3.7388 ...
##  $ dby: num [1:3] 3.812 0.142 5.587</code></pre>

<p>Training our <strong>RNN</strong> iterates between the <strong>forward.unit.RNN(.)</strong> function and <strong>backward.unit.RNN(.)</strong> function, along with the usual cost computation and an optimized update of the learnable parameters similar to <strong>CNN</strong>. See our generic <strong>RNN algorithm</strong> next.</p>
<p><strong>RNN Algorithm</strong></p>
<p>It should be noted that the recurrent nature of <strong>RNN</strong> preserves the previous state at each time step. If we view the <strong>unrolled representation</strong> of <strong>RNN</strong>, we should be able to trace back the gradients through time.</p>
<p>Below is a generic algorithm for <strong>RNN</strong> showing the use of <strong>forward feed</strong> and <strong>backpropagation</strong> through time. The algorithm is based on the <strong>Many to Many</strong> sequence model.</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{W, U, V, bh, by}\ \leftarrow \text{(params)}\ \ \ \ \ \ \ \ \ \ \ \   (\text{randomly generated}) \\
\text{H}_{nxh}, \text{cost}_{1xepoch}\ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \  \text{(initialize to zero)}\\
\text{dH.next}  \ \ \ \ \ \ \ \ \ \ \ \ \  \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \text{(initialize to zero)}\\
\text{dW, dU, dbh, dby}\ \leftarrow \text{(gradients)}\ \ \ \  \ \ \text{(initialize to zero)}\\
\mathbf{\text{for}}\ iter\ \text{    in    } 1...epoch\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \mathbf{\text{for}}\ t\ \text{    in    } 1...T\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{model} = \mathbf{\text{forward.unit}}(X_{nxp}^{(t)}, H_{nxh}^{(t)}, \text{params})\\
\ \ \ \ \ \ \ \ \ \ \ \ \ H_{nxh}^{(t)}, \hat{Y}_{nxo}^{(t)} = \text{model}\\
\ \ \ \ \ \ \mathbf{\text{end loop}}\\
\ \ \ \ \ \ \text{cost[iter] }  = \text{mean}\left(\mathbf{\text{  cost.estimate}}(\hat{Y}_{nxo}, Y_{nxo})\right)  \ \ \ \ \ \ \ \ (\text{cross-entropy cost})\\
\ \ \ \ \ \ \mathbf{\text{for}}\ t\ \text{    in    } T...1\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{gradients} = \mathbf{\text{backward.unit}} (\text{dH.next}, X_{nxh}^{(t)}, \text{model}, \text{params})\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;W&#39;] = params[&#39;W&#39;] + gradients[&#39;dW&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;U&#39;] = params[&#39;U&#39;] + gradients[&#39;dU&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;bh&#39;] = params[&#39;bh&#39;] + gradients[&#39;dbh&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{dH.next = dH} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params} = \mathbf{\text{optimize.update}}(\text{params, gradients})\\
\ \ \ \ \ \ \ \mathbf{\text{end loop}}\\
\mathbf{\text{end loop}}\\
\mathbf{\text{output : }}\text{ cost, params}\ \ \leftarrow \text{(trained model)}
\end{array}
\]</span></p>
<p>where <strong>cross-entropy cost</strong> is calculated as <span class="math inline">\(\left(-\sum\left(\log_e\left(Y_n^{(t)} \odot \hat{Y}_n^{(t)}\right)\right)\right)\)</span>.</p>
<p>A common problem with <strong>Vanilla RNN</strong> is the <strong>vanishing or exploding gradient</strong>. In such a case, the more critical concern is that <strong>RNN</strong>, by its very nature, attempts to preserve (in memory) the state of the previous time step in that such state information tends to <strong>vanish</strong> in time so that the early memory tends to be <strong>forgotten</strong> as we keep progressing in future time steps. This concern is mitigated using <strong>LSTM</strong>.</p>
</div>
<div id="long-short-term-memory-lstm" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.2</span> Long Short-Term Memory (LSTM)  <a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>LSTM</strong> is a variant of <strong>RNN</strong> introduced by Sepp Hochreiter and JÃ¼rgen Schmidhuber in <span class="citation">(<a href="bibliography.html#ref-ref1154h">1997</a>)</span>. <strong>LSTM</strong> is designed as a <strong>gated RNN</strong> with a <strong>forget gate</strong>, <strong>update gate</strong>, and <strong>output gate</strong>. See the design in Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:lstm">13.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lstm"></span>
<img src="lstm.png" alt="LSTM Cell" width="100%" />
<p class="caption">
Figure 13.8: LSTM Cell
</p>
</div>
<p>The intuition behind the gates becomes apparent after reviewing the <strong>forward feed</strong>.</p>
<p><strong>LSTM Forward Feed</strong></p>
<p>Based on Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:lstm">13.8</a>, the <strong>LSTM</strong> design uses the below equations where <span class="math inline">\(\odot\)</span> is an element-wise (<strong>Hadamard</strong>) multiplication:</p>
<p><span class="math display" id="eq:equate1150021" id="eq:equate1150020" id="eq:equate1150019" id="eq:equate1150018" id="eq:equate1150017" id="eq:equate1150016" id="eq:equate1150015" id="eq:equate1150014">\[\begin{align}
[\mathbf{X,H}]_{nx[p,h]}^{(t)} = \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right] &amp;  \tag{13.16} \\
A_f^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(f)}    + \mathbf{b}_{1xh}^{(f)} &amp;\ \ \ \ \ \mathbf{F}_{nxh}^{(t)} = \sigma \left(A_f^{(t)}\right)  \tag{13.17} \\
A_i^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(i)}   + \mathbf{b}_{1xh}^{(i)} &amp;\ \ \ \ \ \mathbf{I}_{nxh}^{(t)} = \sigma \left(A_i^{(t)} \right)  \tag{13.18} \\
A_g^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(g)}  + \mathbf{b}_{1xh}^{(g)} &amp;\ \ \ \ \ \mathbf{G}_{nxh}^{(t)} = \mathbf{\text{tanh}} \left(A_g^{(t)}\right)  \tag{13.19} \\
A_o^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(o)}   + \mathbf{b}_{1xh}^{(o)} &amp;\ \ \ \ \ \mathbf{O}_{nxh}^{(t)} = \sigma \left(A_o^{(t)}\right)  \tag{13.20} \\
&amp;\ \ \ \ \ \mathbf{C}_{nxh}^{(t)} = \mathbf{F}_{nxh}^{(t)} \odot \mathbf{C}_{nxh}^{(t-1)} + \mathbf{I}_{nxh}^{(t)} \odot \mathbf{G}_{nxh}^{(t)}  \tag{13.21} \\
&amp;\ \ \ \ \ \mathbf{H}_{nxh}^{(t)} = \mathbf{\text{tanh}}\left( \mathbf{C}_{nxh}^{(t)}\right) \odot \mathbf{O}_{nxh}^{(t)} \tag{13.22} \\
&amp;\ \ \ \ \ \mathbf{\hat{Y}}_{nxo}^{(t)} = \mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \cdotp \mathbf{V}_{hxo}+ \mathbf{b}_{1xo}^{(y)}\right)  \tag{13.23} 
\end{align}\]</span></p>
<p>Note that the equation for <strong>vanilla RNN</strong> has the following:</p>
<p><span class="math display" id="eq:equate1150022">\[\begin{align}
 \mathbf{X}_{nxp}^{(t)} \cdotp  \mathbf{W}_{pxh} +  \mathbf{H}_{nxh}^{(t-1)} \cdotp \mathbf{U}_{hxh}  \tag{13.24} 
\end{align}\]</span></p>
<p>In <strong>LSTM</strong>, we concatenate <strong>X</strong> and <strong>H</strong> instead which is denoted by <span class="math inline">\(\mathbf{\text{[X, H]}}\)</span>. Then multiplied by an already concatenated weight using the following notation:</p>
<p><span class="math display" id="eq:equate1150023">\[\begin{align}
[\mathbf{\text{X}}_{nxp}^{(t)}, \mathbf{\text{H}}_{nxh}^{(t-1)}] \cdot W_{[p,h]xh} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(f)}  \tag{13.25} 
\end{align}\]</span></p>
<p>Now, because the <strong>forget gate</strong>, denoted by <span class="math inline">\(\mathbf{F}_{nxh}^{(t)}\)</span>, gets multiplied by <span class="math inline">\(\mathbf{C}_{nxh}^{(t-1)}\)</span>, mathematically when it comes to the gates, any value of <span class="math inline">\(\mathbf{F}_{nxh}^{(t)}\)</span>, therefore, affects the previous time step. For example, if the value gets closer to zero, then the more limited information (in the form of <span class="math inline">\(\mathbf{C}_{nxh}^{(t-1)}\)</span>) gets allowed to the next time step. On the other hand, the <strong>candidate state</strong>, denoted by <span class="math inline">\(\mathbf{G}_{nxh}^{(t)}\)</span>, is negated if the value of the <strong>input gate</strong>, denoted by <span class="math inline">\(\mathbf{I}_{nxh}^{(t)}\)</span>, becomes zero. In other words, <strong>gates</strong> in <strong>LSTM</strong> regulate the flow of information such that a complementary intuition here is to avoid long-term memory, which, in effect, mitigates the <strong>vanishing</strong> and <strong>exploding</strong> gradient condition.</p>
<p>Here, we have an example implementation of <strong>LSTM forward feed</strong> based on the above equations: </p>

<div class="sourceCode" id="cb2094"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2094-1" data-line-number="1">forward.unit.LSTM &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, C, params) {</a>
<a class="sourceLine" id="cb2094-2" data-line-number="2">    Wi    =<span class="st"> </span>params<span class="op">$</span>Wi<span class="op">$</span>weight;  Wf  =<span class="st"> </span>params<span class="op">$</span>Wf<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-3" data-line-number="3">    Wg    =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight;  Wo  =<span class="st"> </span>params<span class="op">$</span>Wo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-4" data-line-number="4">    bf    =<span class="st"> </span>params<span class="op">$</span>bf<span class="op">$</span>weight;  bi  =<span class="st"> </span>params<span class="op">$</span>bi<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-5" data-line-number="5">    bg    =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight;  bo  =<span class="st"> </span>params<span class="op">$</span>bo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-6" data-line-number="6">    XH    =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2094-7" data-line-number="7">    Ft    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wf, <span class="dv">2</span>, bf, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-8" data-line-number="8">    It    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wi, <span class="dv">2</span>, bi, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-9" data-line-number="9">    Gt    =<span class="st"> </span><span class="kw">rnn.tanh</span>   (<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wg, <span class="dv">2</span>, bg, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-10" data-line-number="10">    Ot    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wo, <span class="dv">2</span>, bo, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-11" data-line-number="11">    Ct    =<span class="st"> </span>Ft <span class="op">*</span><span class="st"> </span>C <span class="op">+</span><span class="st">  </span>It <span class="op">*</span><span class="st"> </span>Gt </a>
<a class="sourceLine" id="cb2094-12" data-line-number="12">    Ht    =<span class="st"> </span><span class="kw">rnn.tanh</span>(Ct) <span class="op">*</span><span class="st"> </span>Ot</a>
<a class="sourceLine" id="cb2094-13" data-line-number="13">    <span class="kw">list</span>( <span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;Ct&quot;</span> =<span class="st"> </span>Ct,</a>
<a class="sourceLine" id="cb2094-14" data-line-number="14">          <span class="st">&quot;Ft&quot;</span> =<span class="st"> </span>Ft, <span class="st">&quot;It&quot;</span> =<span class="st"> </span>It, <span class="st">&quot;Gt&quot;</span> =<span class="st"> </span>Gt, <span class="st">&quot;Ot&quot;</span> =<span class="st"> </span>Ot</a>
<a class="sourceLine" id="cb2094-15" data-line-number="15">         )</a>
<a class="sourceLine" id="cb2094-16" data-line-number="16">}</a></code></pre></div>

<p><strong>LSTM Backpropagation</strong></p>
<p>Derived from the <strong>cross-entropy</strong> loss for <strong>softmax</strong> to start the backpropagation, we obtain the <strong>Delta y</strong> (<span class="math inline">\(\delta y\)</span>) similar to the <strong>Vanilla RNN</strong>. We also derive the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span> which we also reference as <strong>dH.next</strong> in our implementation. See Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:lstmbackprop">13.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lstmbackprop"></span>
<img src="lstmbackprop.png" alt="RNN (Softmax)" width="80%" />
<p class="caption">
Figure 13.9: RNN (Softmax)
</p>
</div>
<p>We know that the gradient of our loss with respect to <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> is the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span>. Recalling the example derivation from Vanilla RNN backpropagation, we also add <strong>dH.next</strong> which is initially zero.</p>
<p><span class="math display" id="eq:equate1150024">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} +  \delta \mathbf{H}_{nxh}^{(t+1)} = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \underbrace{\delta \mathbf{H}_{nxh}^{(t+1)}}_{\mathbf{\text{dH.next}}} \tag{13.26} 
\end{align}\]</span></p>
<p>Our gradients fo the weight <strong>V</strong> <span class="math inline">\(\left(\nabla \mathbf{V_{hxo}}\right)\)</span> and bias <span class="math inline">\(\left(\nabla \mathbf{b}_{1xh}^{(y)}\right)\)</span> follow similar derivation from <strong>Vanilla RNN</strong>.</p>
<p>Now, to calculate <span class="math inline">\(\left(\delta \mathbf{C}_{nxh}^{(t)}\right)\)</span>, let us first obtain the gradient of <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> with respect to <span class="math inline">\(\mathbf{C}_{nxh}^{(t)}\)</span> using Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:lstmbackprop">13.9</a> as reference:</p>
<p><span class="math display" id="eq:equate1150025">\[\begin{align}
\left(\frac{\partial \mathbf{H}_{nxh}^{(t)}} {\partial \mathbf{C}_{nxh}^{(t)}}\right) = \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot \mathbf{O}_{nxh}^{(t)}  \tag{13.27} 
\end{align}\]</span></p>
<p>Therefore, we obtain the following formulation:</p>
<p><span class="math display" id="eq:equate1150027" id="eq:equate1150026">\[\begin{align}
\delta \mathbf{C}_{nxh}^{(t)} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{C}_{nxh}^{(t)}} 
 = \left(\frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} \right)
    \left(\frac{\partial \mathbf{H}_{nxh}^{(t)}} {\partial \mathbf{C}_{nxh}^{(t)}}  \right)
+  \delta \mathbf{C}_{nxh}^{(t+1)} \tag{13.28} \\
&amp;= \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}} \tag{13.29} 
\end{align}\]</span></p>
<p>That becomes our <strong>initial</strong> <strong>Delta C</strong> or <strong>dC.Next</strong> which is used along with the <strong>initial</strong> <strong>dH.next</strong> to calculate other gradients for our <strong>LSTM</strong>.</p>
<p>We follow this with the calculation of gradients for the gates:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{F}_{nxh}^{(t)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display" id="eq:equate1150028">\[\begin{align}
\delta f =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{F}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{F}_{nxh}^{(t)}} 
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{C}_{nxh}^{(t-1)} \tag{13.30} 
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display" id="eq:equate1150029">\[\begin{align}
\delta f = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{C}_{nxh}^{(t-1)} \tag{13.31} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{I}_{nxh}^{(t)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display" id="eq:equate1150030">\[\begin{align}
\delta i =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{I}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{I}_{nxh}^{(t)}}  
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{G}_{nxh}^{(t)} \tag{13.32} 
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display" id="eq:equate1150031">\[\begin{align}
\delta i = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{G}_{nxh}^{(t)} \tag{13.33} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{G}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display" id="eq:equate1150032">\[\begin{align}
\delta g =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{I}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}} 
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{I}_{nxh}^{(t)} \tag{13.34} 
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display" id="eq:equate1150033">\[\begin{align}
\delta g = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{I}_{nxh}^{(t)} \tag{13.35} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{O}_{nxh}^{(t)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150034">\[\begin{align}
\delta o =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{O}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{O}_{nxh}^{(t)}}
= \delta \mathbf{H}_{nxh}^{(t)} \odot \mathbf{tanh}\left(\mathbf{C}_{nxh}^{(t)}\right) \tag{13.36} 
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to the linear functions. Now, in <strong>Vanilla RNN</strong>, we require the first derivative of the tangent function. Here, we also include the first derivative of the sigmoid function, which is required in calculating gradients for the three gates.</p>
<p><span class="math display" id="eq:equate1150035">\[\begin{align}
\mathbf{\text{tanh}}&#39;(a) = \left(1 - \mathbf{tanh}^2(a)\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\sigma&#39;(a) = \sigma(a)\left(1 - \sigma(a)\right) \tag{13.37} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{F}}_{nxh}^{(t)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display" id="eq:equate1150036">\[\begin{align}
\delta \hat{f} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{F}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{F}_{nxh}^{(t)}}
\frac{\partial \mathbf{F}_{nxh}^{(t)}}{\partial \mathbf{\hat{F}}_{nxh}^{(t)}}
=\delta f \odot \mathbf{F}_{nxh}^{(t)} \odot ( 1 - \mathbf{F}_{nxh}^{(t)})  \tag{13.38} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{f}\)</span> can also be represented as:</p>
<p><span class="math display" id="eq:equate1150037">\[\begin{align}
\delta \hat{f} = \delta f \odot \sigma(A_f) \odot ( 1 - \sigma(A_f))  \tag{13.39} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{I}}_{nxh}^{(t)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display" id="eq:equate1150038">\[\begin{align}
\delta \hat{i} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{I}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{I}_{nxh}^{(t)}}
\frac{\partial \mathbf{I}_{nxh}^{(t)}}{\partial \mathbf{\hat{I}}_{nxh}^{(t)}}
=\delta i \odot \mathbf{I}_{nxh}^{(t)} \odot ( 1 - \mathbf{I}_{nxh}^{(t)})  \tag{13.40} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{i}\)</span> can also be represented as:</p>
<p><span class="math display" id="eq:equate1150039">\[\begin{align}
\delta \hat{i} = \delta i \odot \sigma(A_i) \odot ( 1 - \sigma(A_i))  \tag{13.41} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{G}}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display" id="eq:equate1150040">\[\begin{align}
\delta \hat{g} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}}
=\delta g \odot  \left( 1 - \left({\mathbf{G}_{nxh}^{(t)}}\right)^2\right)  \tag{13.42} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{g}\)</span> can also be represented as:</p>
<p><span class="math display" id="eq:equate1150041">\[\begin{align}
\delta \hat{g} = \delta g \odot ( 1 - \tanh^2(A_g))  \tag{13.43} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{O}}_{nxh}^{(t)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150042">\[\begin{align}
\delta \hat{o} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{O}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{O}_{nxh}^{(t)}}
\frac{\partial \mathbf{O}_{nxh}^{(t)}}{\partial \mathbf{\hat{O}}_{nxh}^{(t)}}
=\delta o \odot \mathbf{O}_{nxh}^{(t)} \odot ( 1 - \mathbf{O}_{nxh}^{(t)})  \tag{13.44} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{o}\)</span> can also be represented as:</p>
<p><span class="math display" id="eq:equate1150043">\[\begin{align}
\delta \hat{o} = \delta o \odot \sigma(A_o) \odot ( 1 - \sigma(A_o))  \tag{13.45} 
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to weights and biases. Note that, because we concatenated <strong>X</strong> and <strong>H</strong>, we can choose to calculate the gradient for each of their weights. In that case, we only use the <span class="math inline">\(\mathbf{p \times h}\)</span> (dimension-wise) portion of the concatenated <span class="math inline">\(\mathbf{W_{[p,h]xh}}\)</span> for the weights for <strong>X</strong> and <span class="math inline">\(\mathbf{h \times h}\)</span> portion for the weights for <strong>H</strong>. For example:</p>
<p><span class="math display" id="eq:equate1150044">\[\begin{align}
\underbrace{\mathbf{W_{pxh}}}_{\text{X portion}}\ \ \ \ \ \ \ \ \ \ \ \
\underbrace{\mathbf{W_{hxh}}}_{\text{H portion}}\ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\mathbf{W_{[p,h]xh}}}_{\text{concatenated weights for X and H}}  \tag{13.46} 
\end{align}\]</span></p>
<p>But only for notation convenience, let us use the concatenation format instead:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(f)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(f)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display" id="eq:equate1150045">\[\begin{align}
\nabla W_{[p,h]xh}^{(f)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(f)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{f}
  \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(f)} =  \sum_{column-wise}{\delta \hat{f}}  \tag{13.47} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(i)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(i)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display" id="eq:equate1150046">\[\begin{align}
\nabla W_{[p,h]xh}^{(i)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(i)}}
 =  \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{i}
 \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(i)} = \sum_{column-wise}{ \delta \hat{i}} \tag{13.48} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(g)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(g)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display" id="eq:equate1150047">\[\begin{align}
\nabla W_{[p,h]xh}^{(g)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(g)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{g}
   \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(g)} =  \sum_{column-wise}{\delta \hat{g}} \tag{13.49} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(o)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(o)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150048">\[\begin{align}
\nabla W_{[p,h]xh}^{(o)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(o)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{o}
    \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(o)} =  \sum_{column-wise}{\delta \hat{o}} \tag{13.50} 
\end{align}\]</span></p>
<p>Lastly, we calculate gradients with respect to <span class="math inline">\(\left(\mathbf{C}_{nxh}^{(t-1)}\right)\)</span>, <span class="math inline">\(\left(\mathbf{H}_{nxh}^{(t-1)}\right)\)</span>, and <span class="math inline">\(\left(\mathbf{X}_{nxh}^{(t)}\right)\)</span>.</p>
<p><span class="math display" id="eq:equate1150051" id="eq:equate1150050" id="eq:equate1150049">\[\begin{align}
\nabla \mathbf{X}_{nxp}^{(t)} &amp;= 
   \delta f \cdot   \left(\mathbf{W_{pxh}^{(f)}}\right)^{\text{T}} 
 + \delta i \cdot   \left(\mathbf{W_{pxh}^{(i)}}\right)^{\text{T}}
 + \delta g \cdot   \left(\mathbf{W_{pxh}^{(g)}}\right)^{\text{T}}  
 + \delta o \cdot   \left(\mathbf{W_{pxh}^{(o)}}\right)^{\text{T}}  \tag{13.51} \\
\underbrace{\nabla \mathbf{H}_{nxh}^{(t-1)}}_{\mathbf{\text{new dH.next}}}
   &amp;=  \delta f \cdot   \left(\mathbf{W_{hxh}^{(f)}}\right)^{\text{T}} 
 + \delta i \cdot   \left(\mathbf{W_{hxh}^{(i)}}\right)^{\text{T}}
 + \delta g \cdot   \left(\mathbf{W_{hxh}^{(g)}}\right)^{\text{T}}  
 + \delta o \cdot   \left(\mathbf{W_{hxh}^{(o)}}\right)^{\text{T}}  \tag{13.52} \\
\underbrace{\nabla \mathbf{C}_{nxh}^{(t-1)}}_{\mathbf{\text{new dC.next}}}  
   &amp;=  \delta \mathbf{C}_{nxh}^{(t)} \odot F_{nxh}^{(t)} \tag{13.53} 
\end{align}\]</span></p>
<p>Both <span class="math inline">\(\left(\nabla \mathbf{H}_{nxh}^{(t-1)}\right)\)</span> and <span class="math inline">\(\left(\nabla \mathbf{C}_{nxh}^{(t-1)}\right)\)</span> are calculated to be the gradients of previous time step; but they become the new <strong>dH.next</strong> and new <strong>dC.next</strong> respectively which we use to propagate back to the next previous time step.</p>
<p><span class="math display" id="eq:equate1150052">\[\begin{align}
\text{dH.next} = \delta \mathbf{H}_{nxh}^{(t-1)}
\ \ \ \ \ \ \ \ \ \ 
\text{dC.next} = \delta \mathbf{C}_{nxh}^{(t-1)} \tag{13.54} 
\end{align}\]</span></p>
<p>Let us now review our example implementation of <strong>LSTM backpropagation</strong> based on the gradient formulations above (and for additional consistency, we reference an LSTM Python code from Fisseha Berhane <span class="citation">(<a href="bibliography.html#ref-ref2060f">n.d.</a>)</span>): </p>

<div class="sourceCode" id="cb2095"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2095-1" data-line-number="1">backward.unit.LSTM &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next, dC.next, X, Y, H, C, model, </a>
<a class="sourceLine" id="cb2095-2" data-line-number="2">                               params, grad) {</a>
<a class="sourceLine" id="cb2095-3" data-line-number="3">    p       =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb2095-4" data-line-number="4">    h       =<span class="st"> </span><span class="kw">ncol</span>(H)</a>
<a class="sourceLine" id="cb2095-5" data-line-number="5">    Ft      =<span class="st"> </span>model<span class="op">$</span>Ft;          It     =<span class="st"> </span>model<span class="op">$</span>It</a>
<a class="sourceLine" id="cb2095-6" data-line-number="6">    Gt      =<span class="st"> </span>model<span class="op">$</span>Gt;          Ot     =<span class="st"> </span>model<span class="op">$</span>Ot</a>
<a class="sourceLine" id="cb2095-7" data-line-number="7">    Ct      =<span class="st"> </span>model<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2095-8" data-line-number="8">    Ht      =<span class="st"> </span>model<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2095-9" data-line-number="9">    Wi      =<span class="st"> </span>params<span class="op">$</span>Wi<span class="op">$</span>weight;  Wf     =<span class="st"> </span>params<span class="op">$</span>Wf<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-10" data-line-number="10">    Wg      =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight;  Wo     =<span class="st"> </span>params<span class="op">$</span>Wo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-11" data-line-number="11">    bf      =<span class="st"> </span>params<span class="op">$</span>bf<span class="op">$</span>weight;  bi     =<span class="st"> </span>params<span class="op">$</span>bi<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-12" data-line-number="12">    bg      =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight;  bo     =<span class="st"> </span>params<span class="op">$</span>bo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-13" data-line-number="13">    dC      =<span class="st"> </span>(Ot <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">rnn.tanh</span>(Ct)<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>dH.next) <span class="op">+</span><span class="st"> </span>dC.next</a>
<a class="sourceLine" id="cb2095-14" data-line-number="14">    dFt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>C) <span class="op">*</span><span class="st"> </span>Ft <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Ft)</a>
<a class="sourceLine" id="cb2095-15" data-line-number="15">    dIt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>Gt) <span class="op">*</span><span class="st"> </span>It <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>It)</a>
<a class="sourceLine" id="cb2095-16" data-line-number="16">    dGt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>It) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Gt<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2095-17" data-line-number="17">    dOt     =<span class="st"> </span>dH.next <span class="op">*</span><span class="st"> </span><span class="kw">rnn.tanh</span>(Ct) <span class="op">*</span><span class="st"> </span>Ot <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Ot)</a>
<a class="sourceLine" id="cb2095-18" data-line-number="18">    XH      =<span class="st"> </span><span class="kw">cbind</span>(X, H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2095-19" data-line-number="19">    <span class="co"># Calculate gradients wrt to shared Weights and Biases</span></a>
<a class="sourceLine" id="cb2095-20" data-line-number="20">    dWft    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dFt;       grad<span class="op">$</span>dF  =<span class="st"> </span>grad<span class="op">$</span>dF <span class="op">+</span><span class="st"> </span>dWft</a>
<a class="sourceLine" id="cb2095-21" data-line-number="21">    dWit    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dIt;       grad<span class="op">$</span>dI  =<span class="st"> </span>grad<span class="op">$</span>dI <span class="op">+</span><span class="st"> </span>dWit</a>
<a class="sourceLine" id="cb2095-22" data-line-number="22">    dWgt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dGt;       grad<span class="op">$</span>dG  =<span class="st"> </span>grad<span class="op">$</span>dG <span class="op">+</span><span class="st"> </span>dWgt</a>
<a class="sourceLine" id="cb2095-23" data-line-number="23">    dWot    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dOt;       grad<span class="op">$</span>dO  =<span class="st"> </span>grad<span class="op">$</span>dO <span class="op">+</span><span class="st"> </span>dWot</a>
<a class="sourceLine" id="cb2095-24" data-line-number="24">    dbf     =<span class="st"> </span><span class="kw">apply</span>(dFt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbf =<span class="st"> </span>grad<span class="op">$</span>dbf <span class="op">+</span><span class="st"> </span>dbf</a>
<a class="sourceLine" id="cb2095-25" data-line-number="25">    dbi     =<span class="st"> </span><span class="kw">apply</span>(dIt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbi =<span class="st"> </span>grad<span class="op">$</span>dbi <span class="op">+</span><span class="st"> </span>dbi</a>
<a class="sourceLine" id="cb2095-26" data-line-number="26">    dbg     =<span class="st"> </span><span class="kw">apply</span>(dGt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbg =<span class="st"> </span>grad<span class="op">$</span>dbg <span class="op">+</span><span class="st"> </span>dbg</a>
<a class="sourceLine" id="cb2095-27" data-line-number="27">    dbo     =<span class="st"> </span><span class="kw">apply</span>(dOt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbo =<span class="st"> </span>grad<span class="op">$</span>dbo <span class="op">+</span><span class="st"> </span>dbo</a>
<a class="sourceLine" id="cb2095-28" data-line-number="28">    dX      =<span class="st"> </span>dFt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wf[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dIt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wi[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2095-29" data-line-number="29"><span class="st">              </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dOt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wo[<span class="dv">1</span><span class="op">:</span>p,])</a>
<a class="sourceLine" id="cb2095-30" data-line-number="30">    dH      =<span class="st"> </span>dFt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wf[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span>dIt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wi[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span></a>
<a class="sourceLine" id="cb2095-31" data-line-number="31"><span class="st">              </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span>dOt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wo[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),])</a>
<a class="sourceLine" id="cb2095-32" data-line-number="32">    dC      =<span class="st"> </span>dC <span class="op">*</span><span class="st"> </span>Ft </a>
<a class="sourceLine" id="cb2095-33" data-line-number="33">    grad<span class="op">$</span>dX =<span class="st"> </span>dX</a>
<a class="sourceLine" id="cb2095-34" data-line-number="34">    grad<span class="op">$</span>dH =<span class="st"> </span>dH</a>
<a class="sourceLine" id="cb2095-35" data-line-number="35">    grad<span class="op">$</span>dC =<span class="st"> </span>dC</a>
<a class="sourceLine" id="cb2095-36" data-line-number="36">    grad</a>
<a class="sourceLine" id="cb2095-37" data-line-number="37">}</a></code></pre></div>

<p>For a simple use, let us concoct a simple dataset and initialize our parameters:</p>

<div class="sourceCode" id="cb2096"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2096-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2096-2" data-line-number="2">n     =<span class="st"> </span><span class="dv">5</span>    <span class="co"># number of samples</span></a>
<a class="sourceLine" id="cb2096-3" data-line-number="3">p     =<span class="st"> </span><span class="dv">30</span>   <span class="co"># number of features per sample (could also mean number of </span></a>
<a class="sourceLine" id="cb2096-4" data-line-number="4">             <span class="co"># probabilities of a word embedding)</span></a>
<a class="sourceLine" id="cb2096-5" data-line-number="5">h     =<span class="st"> </span><span class="dv">20</span>   <span class="co"># number of neurons in a hidden state</span></a>
<a class="sourceLine" id="cb2096-6" data-line-number="6">o     =<span class="st"> </span><span class="dv">3</span>    <span class="co"># number of output neurons in an output layer</span></a>
<a class="sourceLine" id="cb2096-7" data-line-number="7">X     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-8" data-line-number="8">H     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-9" data-line-number="9">C     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-10" data-line-number="10">W     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p <span class="op">+</span><span class="st"> </span>h) <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p<span class="op">+</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-11" data-line-number="11"><span class="co"># copy same structure for other weights</span></a>
<a class="sourceLine" id="cb2096-12" data-line-number="12">Wf    =<span class="st"> </span>Wi =<span class="st"> </span>Wg =<span class="st"> </span>Wo =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>W)</a>
<a class="sourceLine" id="cb2096-13" data-line-number="13">bias  =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-14" data-line-number="14"><span class="co"># copy same structure for other biases</span></a>
<a class="sourceLine" id="cb2096-15" data-line-number="15">bf    =<span class="st"> </span>bi =<span class="st"> </span>bg =<span class="st"> </span>bo =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>bias) </a></code></pre></div>

<p>We then run our <strong>forward feed</strong> like so:</p>

<div class="sourceCode" id="cb2097"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2097-1" data-line-number="1">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wf&quot;</span> =<span class="st"> </span>Wf, <span class="st">&quot;Wi&quot;</span> =<span class="st"> </span>Wi, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wo&quot;</span> =<span class="st"> </span>Wo,</a>
<a class="sourceLine" id="cb2097-2" data-line-number="2">              <span class="st">&quot;bf&quot;</span> =<span class="st"> </span>bf, <span class="st">&quot;bi&quot;</span> =<span class="st"> </span>bi, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;bo&quot;</span> =<span class="st"> </span>bo)</a>
<a class="sourceLine" id="cb2097-3" data-line-number="3">model  =<span class="st"> </span><span class="kw">forward.unit.LSTM</span>(X, H, C, params)</a>
<a class="sourceLine" id="cb2097-4" data-line-number="4"><span class="kw">str</span>(model, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## List of 6
## $ Ht: num [1:5, 1:20] 3.06e-02 -3.26e-03 -6.87e-05
##    7.50e-01 -2.87e-07 ...
## $ Ct: num [1:5, 1:20] 0.085496 -0.0067803 -0.0072869
##    1.0755145 -0.0000626 ...
## $ Ft: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...
## $ It: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...
## $ Gt: num [1:5, 1:20] -0.5236 -0.0763 -0.9998 0.9939 -1
##    ...
## $ Ot: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...</code></pre>

<p>Afterwhich, we follow that by running our <strong>backward pass</strong> like so:</p>

<div class="sourceCode" id="cb2099"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2099-1" data-line-number="1">gradients =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dC&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2099-2" data-line-number="2">                 <span class="st">&quot;dF&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dI&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dO&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2099-3" data-line-number="3">                 <span class="st">&quot;dbf&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbi&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbo&quot;</span> =<span class="st"> </span><span class="dv">0</span> )</a>
<a class="sourceLine" id="cb2099-4" data-line-number="4">dH.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2099-5" data-line-number="5">dC.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2099-6" data-line-number="6">gradients =<span class="st"> </span><span class="kw">backward.unit.LSTM</span>(dH.next, dC.next, X, Y, H, C, </a>
<a class="sourceLine" id="cb2099-7" data-line-number="7">                               model, params, gradients)</a>
<a class="sourceLine" id="cb2099-8" data-line-number="8"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 11
##  $ dX : num [1:5, 1:30] -0.119 2.364 1 -2.511 0.851 ...
##  $ dH : num [1:5, 1:20] 0.443 0.249 -0.344 -1.452 0.811 ...
##  $ dC : num [1:5, 1:20] -0.05873 0.07422 0.00317 -0.49132 0.00114 ...
##  $ dF : num [1:50, 1:20] -0.00558 -0.00869 -0.01672 -0.02838 -0.00533 ...
##  $ dI : num [1:50, 1:20] -0.00631 0.00258 -0.00859 -0.00632 -0.0213 ...
##  $ dG : num [1:50, 1:20] 0.02282 0.02802 0.00815 -0.03022 0.04779 ...
##  $ dO : num [1:50, 1:20] -0.00717 -0.00439 -0.01699 -0.02315 -0.01799 ...
##  $ dbf: num [1:20] -0.02811 -0.13134 -0.00873 0.00139 -0.07315 ...
##  $ dbi: num [1:20] -0.0131 0.171491 0.041701 0.000973 -0.131988 ...
##  $ dbg: num [1:20] 2.52e-02 -4.17e-01 -3.78e-02 -5.07e-09 -7.10e-01 ...
##  $ dbo: num [1:20] -2.75e-02 2.11e-01 1.19e-03 7.13e-06 -4.29e-01 ...</code></pre>

<p>Note that the calculation of our predicted output (<strong>Y.hat</strong>) using <strong>softmax</strong> and the calculation of the gradients are removed from the implementation. The calculations are relocated to another function. The reason becomes apparent in the <strong>Deep Stacked Bidirectional RNN</strong> section.</p>
</div>
<div id="gated-recurrent-units-gru" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.3</span> Gated Recurrent Units (GRU)  <a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>GRU</strong> is another variant of <strong>RNN</strong> introduced by Kyunghyun Cho et al. <span class="citation">(<a href="bibliography.html#ref-ref1173k">2014</a>)</span> with an evaluation paper by Junyoung Chung et al. <span class="citation">(<a href="bibliography.html#ref-ref1158j">2014</a>)</span>. It is regarded as a simplified version of <strong>LSTM</strong>. See the design in Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:gru">13.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gru"></span>
<img src="gru.png" alt="GRU Cell" width="100%" />
<p class="caption">
Figure 13.10: GRU Cell
</p>
</div>
<p>In <strong>GRU</strong>, we do not have a <strong>forget gate</strong> and an <strong>input gate</strong>. Instead, it is the <strong>update gate</strong>, <span class="math inline">\(\mathbf{Z}_{nxh}^{(t)}\)</span>, that regulates the flow of information from state to state or layer to layer.</p>
<p><strong>GRU Forward Feed</strong></p>
<p>Based on Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:gru">13.10</a>, the <strong>GRU</strong> design uses five equations for <strong>forward feed</strong> like so:</p>
<p><span class="math display" id="eq:equate1150057" id="eq:equate1150056" id="eq:equate1150055" id="eq:equate1150054" id="eq:equate1150053">\[\begin{align}
\mathbf{Z}_{nxh}^{(t)} &amp;= \sigma \left(\mathbf{W}_{[p,h]xh}^{(z)} \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}  \right]  + \mathbf{b}_{1xh}^{(z)} \right)  \tag{13.55} \\
\mathbf{R}_{nxh}^{(t)} &amp;= \sigma \left(\mathbf{W}_{[p,h]xh}^{(r)}  \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right]  + \mathbf{b}_{1xh}^{(r)}\right)  \tag{13.56} \\
\mathbf{G}_{nxh}^{(t)} &amp;= \mathbf{\text{tanh}} \left(\mathbf{W}_{[p,h]xh}^{(g)}  \left[ \mathbf{X}_{nxp}^{(t)},\ \mathbf{R}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)}\right] + \mathbf{b}_{1xh}^{(g)}\right)  \tag{13.57} \\
\mathbf{H}_{nxh}^{(t)} &amp;=  \left(1 - \mathbf{Z}_{nxh}^{(t)}\right) \odot \mathbf{H}_{nxh}^{(t-1)} + \mathbf{Z}_{nxh}^{(t)} \odot \mathbf{G}_{nxh}^{(t)}  \tag{13.58} \\
\mathbf{\hat{Y}}_{nxo}^{(t)} &amp;=\mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \cdot \mathbf{V}_{hxo}+ \mathbf{b}_{1xo}^{(y)}\right)  \tag{13.59} 
\end{align}\]</span></p>
<p>While we reference the paper by JunYoung Chung et al <span class="citation">(<a href="bibliography.html#ref-ref1158j">2014</a>)</span> which reflects more of the above formulation, it is notable to mention that other literature may have the following alternative formulation for <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> which references a variant of Figure <a href="13.2-recurrent-neural-network-rnn.html#fig:gru">13.10</a>:</p>
<p><span class="math display" id="eq:equate1150058">\[\begin{align}
\mathbf{H}_{nxh}^{(t)} =  \left(1 - \mathbf{Z}_{nxh}^{(t)}\right) \odot  \mathbf{G}_{nxh}^{(t)}  + \mathbf{Z}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)} \tag{13.60} 
\end{align}\]</span></p>
<p>We leave readers to investigate the difference in performance, accuracy, etc. That said, we have our example implementation of <strong>GRU forward feed</strong> based on the equations above but using the latter formulation for <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>. Note that our implementation is motivated by a MATLAB code from Minchen Li and partly in reference to our previous implementation of <strong>LSTM</strong>. </p>

<div class="sourceCode" id="cb2101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2101-1" data-line-number="1">forward.unit.GRU &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, params) {</a>
<a class="sourceLine" id="cb2101-2" data-line-number="2">    Wz    =<span class="st"> </span>params<span class="op">$</span>Wz<span class="op">$</span>weight; Wr  =<span class="st"> </span>params<span class="op">$</span>Wr<span class="op">$</span>weight; Wg  =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2101-3" data-line-number="3">    bz    =<span class="st"> </span>params<span class="op">$</span>bz<span class="op">$</span>weight; br  =<span class="st"> </span>params<span class="op">$</span>br<span class="op">$</span>weight; bg  =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2101-4" data-line-number="4">    XH    =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2101-5" data-line-number="5">    Zt    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wz, <span class="dv">2</span>, bz, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2101-6" data-line-number="6">    Rt    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wr, <span class="dv">2</span>, br, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2101-7" data-line-number="7">    rXH   =<span class="st"> </span><span class="kw">cbind</span>(X, Rt <span class="op">*</span><span class="st"> </span>H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2101-8" data-line-number="8">    G.hat =<span class="st"> </span><span class="kw">sweep</span>(rXH <span class="op">%*%</span><span class="st"> </span>Wg, <span class="dv">2</span>, bg, <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb2101-9" data-line-number="9">    Gt    =<span class="st"> </span><span class="kw">rnn.tanh</span>(G.hat)</a>
<a class="sourceLine" id="cb2101-10" data-line-number="10">    Ht    =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt) <span class="op">*</span><span class="st"> </span>Gt <span class="op">+</span><span class="st"> </span>Zt <span class="op">*</span><span class="st"> </span>H</a>
<a class="sourceLine" id="cb2101-11" data-line-number="11">    <span class="kw">list</span>(<span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;G.hat&quot;</span> =<span class="st"> </span>G.hat,</a>
<a class="sourceLine" id="cb2101-12" data-line-number="12">         <span class="st">&quot;Zt&quot;</span> =<span class="st"> </span>Zt, <span class="st">&quot;Rt&quot;</span>    =<span class="st"> </span>Rt,    <span class="st">&quot;Gt&quot;</span>    =<span class="st"> </span>Gt)</a>
<a class="sourceLine" id="cb2101-13" data-line-number="13">}</a></code></pre></div>

<p><strong>GRU Backpropagation</strong></p>
<p>Similar to the <strong>Vanilla RNN</strong> and <strong>LSTM</strong>, we derive our <strong>cross-entropy</strong> loss for <strong>softmax</strong> to start the backpropagation. We obtain the <strong>Delta y</strong> (<span class="math inline">\(\delta y\)</span>) similar to the <strong>Vanilla RNN</strong>. We also derive the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span> which we also reference as <strong>dH.next</strong> in our implementation. This has an initial value of zero.</p>
<p><span class="math display" id="eq:equate1150059">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} +  \delta \mathbf{H}_{nxh}^{(t+1)} = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \underbrace{\delta \mathbf{H}_{nxh}^{(t+1)}}_{\mathbf{\text{dH.next}}} \tag{13.61} 
\end{align}\]</span></p>
<p>Our gradients fo the weight <strong>V</strong> <span class="math inline">\(\left(\nabla \mathbf{V_{hxo}}\right)\)</span> and bias <span class="math inline">\(\left(\nabla \mathbf{b}_{1xh}^{(y)}\right)\)</span> follow similar derivation from <strong>Vanilla RNN</strong>.</p>
<p>Next, we calculate the gradients of each gate, namely:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{Z}_{nxh}^{(t)}\right)\)</span> in the <strong>update gate</strong>,</p>
<p><span class="math display" id="eq:equate1150060">\[\begin{align}
\delta z =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{Z}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{Z}_{nxh}^{(t)}}  
= \delta \mathbf{H}_{nxh}^{(t)} \odot \left( \mathbf{H}_{nxh}^{(t-1)} - \mathbf{G}_{nxh}^{(t)} \right) \tag{13.62} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{G}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>,</p>
<p><span class="math display" id="eq:equate1150061">\[\begin{align}
\delta g =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{G}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}  
= \delta \mathbf{H}_{nxh}^{(t)} \odot \left(1 - \mathbf{Z}_{nxh}^{(t)} \right) \tag{13.63} 
\end{align}\]</span></p>
<p>and <strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{R}_{nxh}^{(t)}\right)\)</span> in the <strong>reset gate</strong>,</p>
<p><span class="math display" id="eq:equate1150062">\[\begin{align}
\delta r =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{R}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  
= \delta g \odot \left(\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  \right) \tag{13.64} 
\end{align}\]</span></p>
<p>where <span class="citation">(Ahlad Kumar <a href="bibliography.html#ref-ref1174a">2021</a>)</span>:</p>
<p><span class="math display" id="eq:equate1150063">\[\begin{align}
\left(\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  \right) =
\left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right) \odot \left( 1 - \mathbf{\text{tanh}}^2 \left( \hat{\mathbf{G}}_{nxh}^{(t)} \right)\right) \tag{13.65} 
\end{align}\]</span></p>
<p>and where:</p>
<p><span class="math display" id="eq:equate1150064">\[\begin{align}
\hat{\mathbf{G}}_{nxh}^{(t)} = \mathbf{W}_{[p,h]xh}^{(g)}  \left[ \mathbf{X}_{nxp}^{(t)},\ \mathbf{R}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)}\right] + \mathbf{b}_{1xh}^{(g)}. \tag{13.66} 
\end{align}\]</span></p>
<p>For the partial derivative of <strong>tanh</strong>, let us recall calculus:</p>
<p><span class="math display" id="eq:equate1150065">\[\begin{align}
\frac{\partial\ tanh(ax + by + z)}{\partial x} = sec^2(ax + by + z)(a) = a(1 - tan^2(ax + by + z)) \tag{13.67} 
\end{align}\]</span></p>
<p>such that we have:</p>
<p><span class="math display" id="eq:equate1150066">\[\begin{align}
a = \left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right) \tag{13.68} 
\end{align}\]</span></p>
<p>Also, it is important to note that the input <strong>X</strong> denoted by <span class="math inline">\(\left(\mathbf{X}_{nxp}^{(t)}\right)\)</span> is canceled out from the equation (see calculus); and therefore, its corresponding weight is also canceled out from the concatenation so that we have:</p>
<p><span class="math display" id="eq:equate1150067">\[\begin{align}
\mathbf{W}_{[p,h]xh}^{(g)} \ \ \ \ \ \rightarrow\ \ \ \ \ \ \mathbf{W}_{hxh}^{(g)}  \tag{13.69} 
\end{align}\]</span></p>
<p>Therefore, our gradient in the <strong>reset gate</strong> becomes:</p>
<p><span class="math display" id="eq:equate1150068">\[\begin{align}
\delta r = \delta g \odot  \left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right) \odot \left( 1 - \mathbf{\text{tanh}}^2 \left( \hat{\mathbf{G}}_{nxh}^{(t)} \right)\right) \tag{13.70} 
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to the linear functions, namely:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{Z}}_{nxh}^{(t)}\right)\)</span> in the <strong>update gate</strong>,</p>
<p><span class="math display" id="eq:equate1150069">\[\begin{align}
\delta \hat{z} =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{Z}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{Z}_{nxh}^{(t)}}   
\frac{\partial \mathbf{Z}_{nxh}^{(t)}}{\partial \mathbf{\hat{Z}}_{nxh}^{(t)}}  
= \delta z \odot \mathbf{Z}_{nxh}^{(t)} \odot \left(1 - \mathbf{Z}_{nxh}^{(t)}\right) \tag{13.71} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{G}}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>,</p>
<p><span class="math display" id="eq:equate1150070">\[\begin{align}
\delta \hat{g} =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{G}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}   
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}}  
= \delta g \odot \left(1 - \left(\mathbf{G}_{nxh}^{(t)}\right)^2\right)    \tag{13.72} 
\end{align}\]</span></p>
<p>and <strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{R}}_{nxh}^{(t)}\right)\)</span> in the <strong>reset gate</strong>.</p>
<p><span class="math display" id="eq:equate1150071">\[\begin{align}
\delta \hat{r} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{R}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}   
\frac{\partial \mathbf{R}_{nxh}^{(t)}}{\partial \mathbf{\hat{R}}_{nxh}^{(t)}}  
= \delta r \odot  \mathbf{R}_{nxh}^{(t)} \left(1 -  \mathbf{R}_{nxh}^{(t)}\right) \tag{13.73} 
\end{align}\]</span></p>
<p>Lastly, let us get the gradients with respect to the weights and biases. Similarly, only for notation convenience, let us use the concatenation format instead:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(z)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(z)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150072">\[\begin{align}
\nabla W_{[p,h]xh}^{(z)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(z)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{z}
    \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(z)} =  \sum_{column-wise}{\delta \hat{z}} \tag{13.74} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(g)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(g)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150073">\[\begin{align}
\nabla W_{[p,h]xh}^{(g)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(g)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{g}
    \ \ \ \ \ \ \ \ \ 
\nabla b_{1xh}^{(g)} =  \sum_{column-wise}{\delta \hat{g}} \tag{13.75} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(r)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(r)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display" id="eq:equate1150074">\[\begin{align}
\nabla W_{[p,h]xh}^{(r)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(r)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{r}
    \ \ \ \ \ \ \ \ \ 
\nabla b_{1xh}^{(r)} =  \sum_{column-wise}{\delta \hat{r}} \tag{13.76} 
\end{align}\]</span></p>
<p>Lastly, we calculate gradients with respect to <span class="math inline">\(\left(\mathbf{H}_{nxh}^{(t-1)}\right)\)</span>, and <span class="math inline">\(\left(\mathbf{X}_{nxh}^{(t)}\right)\)</span>.</p>
<p><span class="math display" id="eq:equate1150076" id="eq:equate1150075">\[\begin{align}
\nabla \mathbf{X}_{nxp}^{(t)} &amp;= 
   \delta z \cdot   \left(\mathbf{W_{pxh}^{(z)}}\right)^{\text{T}} 
 + \delta g \cdot   \left(\mathbf{W_{pxh}^{(g)}}\right)^{\text{T}}
 + \delta r \cdot   \left(\mathbf{W_{pxh}^{(r)}}\right)^{\text{T}}  \tag{13.77} \\
\underbrace{\nabla \mathbf{H}_{nxh}^{(t-1)}}_{\text{new dH.next}} &amp;= 
   \delta z \cdot   \left(\mathbf{W_{hxh}^{(z)}}\right)^{\text{T}} 
 + \delta g \cdot   \left(\mathbf{W_{hxh}^{(g)}}\right)^{\text{T}}
 + \delta r \cdot   \left(\mathbf{W_{hxh}^{(r)}}\right)^{\text{T}}   \tag{13.78} 
\end{align}\]</span></p>
<p>The <span class="math inline">\(\left(\nabla \mathbf{H}_{nxh}^{(t-1)}\right)\)</span> becomes our new <strong>dH.next</strong> which we use to propagate back to the next previous time step.</p>
<p><span class="math display" id="eq:equate1150077">\[\begin{align}
\text{dH.next} = \delta \mathbf{H}_{nxh}^{(t-1)} \tag{13.79} 
\end{align}\]</span></p>
<p>Let us now review our example implementation of <strong>GRU backpropagation</strong> based on the formulations above: </p>

<div class="sourceCode" id="cb2102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2102-1" data-line-number="1">backward.unit.GRU &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next,  X, Y, H, model, params, grad) {</a>
<a class="sourceLine" id="cb2102-2" data-line-number="2">  p       =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb2102-3" data-line-number="3">  h       =<span class="st"> </span><span class="kw">ncol</span>(H)</a>
<a class="sourceLine" id="cb2102-4" data-line-number="4">  Ht      =<span class="st"> </span>model<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2102-5" data-line-number="5">  Zt      =<span class="st"> </span>model<span class="op">$</span>Zt;          Rt =<span class="st"> </span>model<span class="op">$</span>Rt;         Gt =<span class="st"> </span>model<span class="op">$</span>Gt</a>
<a class="sourceLine" id="cb2102-6" data-line-number="6">  Wz      =<span class="st"> </span>params<span class="op">$</span>Wz<span class="op">$</span>weight;  Wr =<span class="st"> </span>params<span class="op">$</span>Wr<span class="op">$</span>weight; Wg =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2102-7" data-line-number="7">  bz      =<span class="st"> </span>params<span class="op">$</span>bz<span class="op">$</span>weight;  br =<span class="st"> </span>params<span class="op">$</span>br<span class="op">$</span>weight; bg =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2102-8" data-line-number="8">  G.hat   =<span class="st"> </span>model<span class="op">$</span>G.hat</a>
<a class="sourceLine" id="cb2102-9" data-line-number="9">  dH      =<span class="st"> </span>dH.next</a>
<a class="sourceLine" id="cb2102-10" data-line-number="10">  dZt     =<span class="st"> </span>dH <span class="op">*</span><span class="st"> </span>( Ht <span class="op">-</span><span class="st"> </span>Gt) <span class="op">*</span><span class="st"> </span>Zt <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt)</a>
<a class="sourceLine" id="cb2102-11" data-line-number="11">  dGt     =<span class="st"> </span>dH <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt) <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Gt<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2102-12" data-line-number="12">  dRt     =<span class="st"> </span>( dGt <span class="op">*</span><span class="st"> </span>(Ht <span class="op">%*%</span><span class="st"> </span>Wr[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">rnn.tanh</span>(G.hat)<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span></a>
<a class="sourceLine" id="cb2102-13" data-line-number="13"><span class="st">              </span>Rt <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Rt)</a>
<a class="sourceLine" id="cb2102-14" data-line-number="14">  XH      =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2102-15" data-line-number="15">  <span class="co"># Calculate gradient wrt to shared Weights and Biases</span></a>
<a class="sourceLine" id="cb2102-16" data-line-number="16">  dWzt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dZt;       grad<span class="op">$</span>dZ  =<span class="st"> </span>grad<span class="op">$</span>dZ <span class="op">+</span><span class="st"> </span>dWzt</a>
<a class="sourceLine" id="cb2102-17" data-line-number="17">  dWgt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dGt;       grad<span class="op">$</span>dG  =<span class="st"> </span>grad<span class="op">$</span>dG <span class="op">+</span><span class="st"> </span>dWgt</a>
<a class="sourceLine" id="cb2102-18" data-line-number="18">  dWrt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dRt;       grad<span class="op">$</span>dR  =<span class="st"> </span>grad<span class="op">$</span>dR <span class="op">+</span><span class="st"> </span>dWrt</a>
<a class="sourceLine" id="cb2102-19" data-line-number="19">  dbz     =<span class="st"> </span><span class="kw">apply</span>(dZt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbz =<span class="st"> </span>grad<span class="op">$</span>dbz <span class="op">+</span><span class="st"> </span>dbz    </a>
<a class="sourceLine" id="cb2102-20" data-line-number="20">  dbg     =<span class="st"> </span><span class="kw">apply</span>(dGt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbg =<span class="st"> </span>grad<span class="op">$</span>dbg <span class="op">+</span><span class="st"> </span>dbg</a>
<a class="sourceLine" id="cb2102-21" data-line-number="21">  dbr     =<span class="st"> </span><span class="kw">apply</span>(dRt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbr =<span class="st"> </span>grad<span class="op">$</span>dbr <span class="op">+</span><span class="st"> </span>dbr</a>
<a class="sourceLine" id="cb2102-22" data-line-number="22">  dX      =<span class="st"> </span>dZt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wz[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st">  </span>dRt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wr[<span class="dv">1</span><span class="op">:</span>p,])</a>
<a class="sourceLine" id="cb2102-23" data-line-number="23">  dH      =<span class="st"> </span>dZt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wz[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2102-24" data-line-number="24"><span class="st">            </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span></a>
<a class="sourceLine" id="cb2102-25" data-line-number="25"><span class="st">            </span>dRt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wr[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) </a>
<a class="sourceLine" id="cb2102-26" data-line-number="26">  grad<span class="op">$</span>dX =<span class="st"> </span>dX</a>
<a class="sourceLine" id="cb2102-27" data-line-number="27">  grad<span class="op">$</span>dH =<span class="st"> </span>dH</a>
<a class="sourceLine" id="cb2102-28" data-line-number="28">  grad</a>
<a class="sourceLine" id="cb2102-29" data-line-number="29">}</a></code></pre></div>

<p>For a simple use, let us concoct a simple dataset and initialize our parameters:</p>

<div class="sourceCode" id="cb2103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2103-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2103-2" data-line-number="2">n    =<span class="st"> </span><span class="dv">5</span>; p =<span class="st"> </span><span class="dv">30</span>; h =<span class="st"> </span><span class="dv">20</span>; o =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2103-3" data-line-number="3">X    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-4" data-line-number="4">H    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-5" data-line-number="5">W    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p <span class="op">+</span><span class="st"> </span>h) <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p<span class="op">+</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-6" data-line-number="6">Wz   =<span class="st"> </span>Wg =<span class="st"> </span>Wr =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span>=W)    <span class="co">#copy same structure for other weights</span></a>
<a class="sourceLine" id="cb2103-7" data-line-number="7">bias =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-8" data-line-number="8">bz   =<span class="st"> </span>bg =<span class="st"> </span>br =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span>=bias) <span class="co">#copy same structure for other biases</span></a></code></pre></div>

<p>We then run our <strong>forward feed</strong> like so:</p>

<div class="sourceCode" id="cb2104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2104-1" data-line-number="1">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wz&quot;</span> =<span class="st"> </span>Wz, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wr&quot;</span> =<span class="st"> </span>Wr,</a>
<a class="sourceLine" id="cb2104-2" data-line-number="2">              <span class="st">&quot;bz&quot;</span> =<span class="st"> </span>bz, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;br&quot;</span> =<span class="st"> </span>br)</a>
<a class="sourceLine" id="cb2104-3" data-line-number="3">model  =<span class="st"> </span><span class="kw">forward.unit.GRU</span>(X, H, params)</a>
<a class="sourceLine" id="cb2104-4" data-line-number="4"><span class="kw">str</span>(model)</a></code></pre></div>
<pre><code>## List of 5
##  $ Ht   : num [1:5, 1:20] -0.988 0.474 0.615 0.953 0.513 ...
##  $ G.hat: num [1:5, 1:20] -4.472 0.137 0.606 1.184 1.049 ...
##  $ Zt   : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ...
##  $ Rt   : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ...
##  $ Gt   : num [1:5, 1:20] -1 0.136 0.541 0.829 0.782 ...</code></pre>

<p>Afterwhich, we follow that by running our <strong>backward pass</strong> like so:</p>

<div class="sourceCode" id="cb2106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2106-1" data-line-number="1">gradients =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2106-2" data-line-number="2">                 <span class="st">&quot;dZ&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dR&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2106-3" data-line-number="3">                 <span class="st">&quot;dbz&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbr&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2106-4" data-line-number="4">dH.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2106-5" data-line-number="5">gradients =<span class="st"> </span><span class="kw">backward.unit.GRU</span>(dH.next, X, Y, H, model, params, gradients)</a>
<a class="sourceLine" id="cb2106-6" data-line-number="6"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 8
##  $ dX : num [1:5, 1:30] 1.0933 -0.8241 0.2523 -0.1001 0.0675 ...
##  $ dH : num [1:5, 1:20] 1.653 -1.46 -0.101 1.061 0.117 ...
##  $ dZ : num [1:50, 1:20] -0.03323 -0.05663 -0.0364 -0.00337 -0.06603 ...
##  $ dG : num [1:50, 1:20] 0.6443 0.0415 0.2329 0.233 0.2817 ...
##  $ dR : num [1:50, 1:20] 0.0561 0.1294 0.0907 0.0223 0.1606 ...
##  $ dbz: num [1:20] -0.073512 0.004256 -0.000381 0.180441 -0.070078 ...
##  $ dbg: num [1:20] 0.52703 -0.00109 -0.00139 0.13647 0.09256 ...
##  $ dbr: num [1:20] 1.79e-01 -4.04e-07 -1.25e-06 -3.29e-02 -3.01e-02 ...</code></pre>

<p>We leave readers to investigate <strong>Minimal Gated Unit (MGU)</strong>, a variant of <strong>GRU</strong>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="13.1-residual-network-resnet.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="13.3-deep-stacked-rnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
