<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.2 Approximating Eigenvalues and EigenVectors by Iteration (\(Av = \lambda v\)) | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.2 Approximating Eigenvalues and EigenVectors by Iteration (\(Av = \lambda v\)) | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.2 Approximating Eigenvalues and EigenVectors by Iteration (\(Av = \lambda v\)) | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3.1-iteration-and-convergence.html"/>
<link rel="next" href="3.3-approximating-root-and-fixed-point-by-iteration.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.2</span> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our discussion about matrix decomposition, we aim to find the <strong>Eigenvalues</strong> and corresponding <strong>Eigenvectors</strong> of a matrix. In such an <strong>analytical</strong> process, we rely on evaluating the <strong>determinant</strong> of a <strong>characteristic matrix</strong>, in the form of <span class="math inline">\(det(A - \lambda I) = 0\)</span> to generate a <strong>characteristic polynomial</strong> that leads to a set of solutions for our <strong>Eigenvalues</strong>. It may seem simple and easy to take that approach, but if we begin to perform the same steps against an extensive matrix - which is practically common - then deriving the <strong>characteristic equation</strong> becomes extremely complex and unimaginable. For this reason, we seek other alternatives. <strong>Numerically</strong>, we can use <strong>iterative</strong> methods to approximate <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>. And that is what we shall cover in this section.</p>
<p>We may accept only a reasonable finite number of loops or iterations for any process involving iteration before we stop the iteration. Therefore, for most discussions around iterations, let us use a tolerance level (our tolerance threshold) such as the value below:</p>

<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb23-1" data-line-number="1">tol =<span class="st"> </span><span class="fl">1e-5</span></a></code></pre></div>

<p>Before we begin, let us use <strong>eigen(.)</strong> function to generate the actual <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>, which we will use to validate our solutions later. Our sample matrix and the result of the <strong>eigen(.)</strong> function are shown below:</p>

<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">(<span class="dt">A =</span> <span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5</code></pre>
<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1"><span class="kw">eigen</span>(A)</a></code></pre></div>
<pre><code>## eigen() decomposition
## $values
## [1] 10.6771903  1.9109438 -0.5881341
## 
## $vectors
##            [,1]        [,2]        [,3]
## [1,] -0.4836669 -0.91825423  0.07759288
## [2,] -0.6128191 -0.05832852 -0.74984844
## [3,] -0.6249152  0.39167200  0.65704388</code></pre>

<div id="power-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.1</span> Power Method <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to use iteration to approximate an <strong>Eigenvalue</strong> of a matrix using an initial arbitrary vector until the iteration stops at a tolerance level.</p>
<p>The <strong>Power Method</strong> estimates only the dominant <strong>Eigenvalue</strong> - or the maximum absolute <strong>Eigenvalue</strong> <span class="citation">(Atkinson K. E. <a href="bibliography.html#ref-ref288k">1989</a>; Bai Z. et al <a href="bibliography.html#ref-ref26z">2000</a>)</span>.</p>
<p><span class="math display">\[
|\lambda_1| &gt; |\lambda_2|\ge |\lambda_3| \ge ... \ge |\lambda_n|
\]</span></p>
<p>The restriction imposed by this method is to find only the one largest <strong>dominant Eigenvalue</strong> along with its corresponding <strong>Eigenvector</strong> for a matrix with multiple <strong>Eigenvalues</strong> or even with one <strong>Eigenvalue</strong> but having <strong>Multiplicity</strong> greater than one. Additionally, the condition above applies to <strong>Diagonalizable</strong> matrices in which any column is a linear combination of <strong>Eigenvectors</strong>. Below illustrates the sequence against which we iterate until convergence.</p>
<p><span class="math display">\[
Av_0, A^2v_1, A^3v_2,\ ...\ \ \ \ \ \text{where } v_j \text{ is an Eigenvector}.
\]</span></p>
<p>We normalize the <strong>Eigenvector</strong> at each iteration using the below equation:</p>
<p><span class="math display" id="eq:equate1050001">\[\begin{align}
v_j =  \frac{v_j} {\|v_j\|_{L2}} = \frac{v_j} {\sqrt{\sum{v_j^2}}} \tag{3.1} 
\end{align}\]</span></p>
<p>For the sake of illustration, we use the <strong>Rayleigh Equation</strong> below to derive the <strong>EigenValue</strong> <span class="citation">(B. N. Parlett <a href="bibliography.html#ref-ref7b">1974</a>)</span>. We also use the value to compute the error.</p>
<p><span class="math display" id="eq:equate1050002">\[\begin{align}
\lambda = \frac{v^T \cdotp Av}{v^Tv} \tag{3.2} 
\end{align}\]</span></p>
<p>Here, <span class="math inline">\(\lambda\)</span> is a natural approximation of an <strong>Eigenvalue</strong> if <span class="math inline">\(\mathbf{v}\)</span> is close to the actual <strong>Eigenvector</strong>.</p>
<p>That said, let us now introduce the normalized <strong>Power Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ v_j = A \cdotp v_{j-1} \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalized Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(\lambda_j, \lambda_{j-1}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{\vec{v}}\)</span> is the <strong>approximate eigenvector</strong> and <span class="math inline">\(\lambda\)</span> is the <strong>approximate (dominant) eigenvalue</strong>.</p>
<p>Our naive implementation of normalized <strong>Power Method</strong> in R code shows the following (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">power_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb28-2" data-line-number="2">    n          =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb28-3" data-line-number="3">    sequence   =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb28-4" data-line-number="4">    vj =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(A)<span class="op">-</span><span class="dv">1</span>)) <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb28-5" data-line-number="5">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb28-6" data-line-number="6">    limit      =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb28-7" data-line-number="7">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb28-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb28-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb28-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb28-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb28-12" data-line-number="12">            vj       =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>vj</a>
<a class="sourceLine" id="cb28-13" data-line-number="13">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>))  <span class="co"># normalized Eigenvector</span></a>
<a class="sourceLine" id="cb28-14" data-line-number="14">            evalue   =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj))<span class="op">/</span></a>
<a class="sourceLine" id="cb28-15" data-line-number="15"><span class="st">                        </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj) <span class="co"># Eigvenvalue</span></a>
<a class="sourceLine" id="cb28-16" data-line-number="16">            err      =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span>evalue </a>
<a class="sourceLine" id="cb28-17" data-line-number="17">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb28-18" data-line-number="18">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>   </a>
<a class="sourceLine" id="cb28-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb28-20" data-line-number="20">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb28-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb28-22" data-line-number="22">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb28-23" data-line-number="23">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb28-24" data-line-number="24">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb28-25" data-line-number="25">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb28-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb28-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb28-28" data-line-number="28">(<span class="dt">E.normalized =</span> <span class="kw">power_method</span>(A))</a></code></pre></div>
<pre><code>## $Iteration
##       J  eigenval eigenvec1 eigenvec2 eigenvec3        error
##  [1,] 0  0.000000 1.0000000 0.0000000 0.0000000 0.000000e+00
##  [2,] 1  7.857143 0.8017837 0.5345225 0.2672612 1.000000e+00
##  [3,] 2 10.368682 0.5666657 0.5981471 0.5666657 2.422236e-01
##  [4,] 3 10.633900 0.4992259 0.6111735 0.6141991 2.494084e-02
##  [5,] 4 10.669882 0.4864789 0.6125083 0.6230344 3.372238e-03
##  [6,] 5 10.675896 0.4841708 0.6127657 0.6245773 5.633452e-04
##  [7,] 6 10.676959 0.4837571 0.6128095 0.6248548 9.957788e-05
##  [8,] 7 10.677149 0.4836831 0.6128174 0.6249044 1.777917e-05
##  [9,] 8 10.677183 0.4836698 0.6128188 0.6249133 3.180791e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##          [,1]
## [1,] 10.67718
## 
## $eigenvector
## eigenvec1 eigenvec2 eigenvec3 
## 0.4836698 0.6128188 0.6249133</code></pre>

<p>The iteration stops after the tolerance is reached, giving us an <strong>approximate Eigenvalue</strong> of <span class="math inline">\(\lambda\)</span>=10.6771829 with the corresponding <strong>approximate Eigenvector</strong>, v=(0.4836698, 0.6128188, 0.6249133).</p>
</div>
<div id="inverse-power-method-using-lu-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.2</span> Inverse Power Method (using LU Decomposition)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In contrast to the <strong>Power Method</strong>, the <strong>Inverse Power Method</strong> restricts us to the smallest <strong>Eigenvalue</strong> <span class="citation">(Bai Z. et al. <a href="bibliography.html#ref-ref26z">2000</a>)</span>.</p>
<p>The method is similar to the <strong>Power Method</strong>. We follow the same iteration, except we use an inverse matrix this time. To avoid the cost of calculating the inverse of a matrix, we decompose the matrix into <strong>LU form</strong> and use the decomposed components. In our case, we use <strong>LU decomposition by Doolittle</strong> (See Section <strong>LU Decomposition</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>)) to extract the <strong>LU</strong> form, then perform substitution to solve for the system.</p>
<p>That said, let us now introduce the <strong>Inverse Power Method</strong> algorithm (a modified version with Doolittle Decomposition):</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
LU = lu\_decomposition\_by\_doolittle(A)\\
\text{loop}\ j\ in\ 1:\ ... \\
\ \ \ \ u_y = forward\_sub(L,  v_{j-1} ) \\
\ \ \ \ v_j = backward\_sub(U,  u_y ) \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalize Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(\lambda_j, \lambda_{j-1}) &lt; \text{tolerance then break} \\
\text{end loop}
\end{array}
\]</span></p>
<p>Note in our algorithm that we have not introduced the use of <strong>shift</strong> which we cover in the next method.</p>
<p>Below is a naive implementation of the <strong>Inverse Power Method</strong> in R (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1">inverse_power_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb30-2" data-line-number="2">    LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A) <span class="co">#derived from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb30-3" data-line-number="3">    n  =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb30-4" data-line-number="4">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb30-5" data-line-number="5">    vj =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>,<span class="kw">ncol</span>(A)<span class="op">-</span><span class="dv">1</span>)) <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb30-6" data-line-number="6">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb30-7" data-line-number="7">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb30-8" data-line-number="8">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb30-9" data-line-number="9">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb30-10" data-line-number="10">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb30-11" data-line-number="11">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb30-12" data-line-number="12">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb30-13" data-line-number="13">            uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, vj)</a>
<a class="sourceLine" id="cb30-14" data-line-number="14">            vj =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb30-15" data-line-number="15">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>)) <span class="co"># Normalize Eigenvector</span></a>
<a class="sourceLine" id="cb30-16" data-line-number="16">            evalue   =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj))<span class="op">/</span></a>
<a class="sourceLine" id="cb30-17" data-line-number="17"><span class="st">                        </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj) <span class="co"># Eigvenvalue</span></a>
<a class="sourceLine" id="cb30-18" data-line-number="18">            err =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span><span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb30-19" data-line-number="19">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb30-20" data-line-number="20">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb30-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb30-22" data-line-number="22">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb30-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb30-24" data-line-number="24">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb30-25" data-line-number="25">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb30-26" data-line-number="26">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb30-27" data-line-number="27">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb30-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb30-29" data-line-number="29">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb30-30" data-line-number="30">(<span class="dt">E.scaled =</span> <span class="kw">inverse_power_method</span>(A))</a></code></pre></div>
<pre><code>## $Iteration
##        J   eigenval    eigenvec1  eigenvec2  eigenvec3         error
##  [1,]  0  0.0000000  1.000000000  0.0000000  0.0000000  0.000000e+00
##  [2,]  1  0.6976744  0.539163866  0.5391639 -0.6469966  1.000000e+00
##  [3,]  2 -0.5317854  0.361791913 -0.7488251  0.5553085  2.311947e+00
##  [4,]  3 -0.5177750  0.006673751  0.7368606 -0.6760117 -2.705868e-02
##  [5,]  4 -0.6025525  0.104197718 -0.7527992  0.6499509  1.406972e-01
##  [6,]  5 -0.5829676 -0.069460484  0.7488318 -0.6591101 -3.359522e-02
##  [7,]  6 -0.5896563  0.080101260 -0.7501514  0.6563967  1.134351e-02
##  [8,]  7 -0.5876591 -0.076821393  0.7497542 -0.6572420 -3.398616e-03
##  [9,]  8 -0.5882797  0.077830375 -0.7498773  0.6569828  1.054838e-03
## [10,]  9 -0.5880892 -0.077519795  0.7498395 -0.6570627 -3.238135e-04
## [11,] 10 -0.5881479  0.077615379 -0.7498512  0.6570381  9.973976e-05
## [12,] 11 -0.5881298 -0.077585960  0.7498476 -0.6570457 -3.068956e-05
## [13,] 12 -0.5881354  0.077595014 -0.7498487  0.6570433  9.446082e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##            [,1]
## [1,] -0.5881354
## 
## $eigenvector
##   eigenvec1   eigenvec2   eigenvec3 
##  0.07759501 -0.74984870  0.65704333</code></pre>

</div>
<div id="rayleigh-quotient-method-using-lu-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.3</span> Rayleigh Quotient Method (using LU Decomposition)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Rayleigh Quotient Method</strong> is a variant of <strong>Inverse Power Method</strong> <span class="citation">(B. N. Parlett <a href="bibliography.html#ref-ref7b">1974</a>)</span>.</p>
<p>Here, the <strong>LU decomposition</strong> takes a shifted matrix using the calculated <strong>Eigenvector</strong>, <span class="math inline">\(\lambda_{j-1}\)</span>, (via Rayleigh Quotient) at every iteration. The shift is expressed as such:</p>
<p><span class="math display">\[
A - \lambda_{j-1} I
\]</span></p>
<p>Here, the calculated <strong>Eigenvector</strong> is called the <strong>shift</strong>, which allows for faster convergence <span class="citation">(Heath M.T. p.176, <a href="bibliography.html#ref-ref187m">2002</a>)</span>.</p>
<p>That said, let us now introduce the <strong>Rayleigh Quotient Iteration</strong> algorithm (a modified version with Doolittle Decomposition):</p>
<p><span class="math display">\[
\begin{array}{l}
v_0 \leftarrow \text{initial arbitrary nonzero vector}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ LU = lu\_decomposition\_by\_doolittle(A - \lambda_{j-1} I)\\
\ \ \ \ u_y = forward\_sub(L, v_{j-1}  ) \\
\ \ \ \ v_j = backward\_sub(U,  u_y ) \\
\ \ \ \ v_j =  v_j / \|v_j\|_{L2}\ \ \ \leftarrow \text{normalized Eigenvector}\\
\ \ \ \ \lambda_j = v_j^TAv_j / v_j^Tv_j \ \ \ \leftarrow \text{Eigenvalue} \\
\ \ \ \ if\ err(v_j, v_{j-1}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>Below is a naive implementation of <strong>Rayleigh Quotient Method</strong> in R (note that the eigenvector output is scaled):</p>

<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">rayleighquotient_method &lt;-<span class="cf">function</span>(A, eigenvector) {</a>
<a class="sourceLine" id="cb32-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb32-3" data-line-number="3">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb32-4" data-line-number="4">    vj =<span class="st"> </span>eigenvector <span class="co"># initial arbitrary nonzero vector</span></a>
<a class="sourceLine" id="cb32-5" data-line-number="5">    old_evalue =<span class="st"> </span><span class="dv">0</span>; evalue =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb32-6" data-line-number="6">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb32-7" data-line-number="7">    tol=<span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb32-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb32-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb32-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb32-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb32-12" data-line-number="12">            B  =<span class="st"> </span>A <span class="op">-</span><span class="st"> </span><span class="kw">c</span>(evalue) <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(n)</a>
<a class="sourceLine" id="cb32-13" data-line-number="13">            LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(B) <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb32-14" data-line-number="14">            uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, vj)</a>
<a class="sourceLine" id="cb32-15" data-line-number="15">            vj =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb32-16" data-line-number="16">            vj =<span class="st"> </span>vj <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(vj<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb32-17" data-line-number="17">            evalue =<span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>(A <span class="op">%*%</span><span class="st"> </span>vj)) <span class="op">/</span><span class="st"> </span>(<span class="kw">t</span>(vj) <span class="op">%*%</span><span class="st"> </span>vj)</a>
<a class="sourceLine" id="cb32-18" data-line-number="18">            err =<span class="st"> </span>(evalue <span class="op">-</span><span class="st"> </span>old_evalue)<span class="op">/</span><span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb32-19" data-line-number="19">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalue, vj, err))</a>
<a class="sourceLine" id="cb32-20" data-line-number="20">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb32-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb32-22" data-line-number="22">        old_evalue =<span class="st"> </span>evalue </a>
<a class="sourceLine" id="cb32-23" data-line-number="23">    }</a>
<a class="sourceLine" id="cb32-24" data-line-number="24">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="st">&quot;eigenval&quot;</span>, </a>
<a class="sourceLine" id="cb32-25" data-line-number="25">          <span class="kw">paste</span>(<span class="st">&quot;eigenvec&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb32-26" data-line-number="26">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;eigenvalue&quot;</span>=evalue, </a>
<a class="sourceLine" id="cb32-27" data-line-number="27">         <span class="st">&quot;eigenvector&quot;</span>=sequence[<span class="kw">nrow</span>(sequence),<span class="dv">3</span><span class="op">:</span>(<span class="kw">ncol</span>(A)<span class="op">+</span><span class="dv">2</span>)])</a>
<a class="sourceLine" id="cb32-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb32-29" data-line-number="29">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb32-30" data-line-number="30">neigenvector =<span class="st"> </span>E.scaled<span class="op">$</span>eigenvector   </a>
<a class="sourceLine" id="cb32-31" data-line-number="31"><span class="co">#eigenvector = E.normalized$eigenvector  </span></a>
<a class="sourceLine" id="cb32-32" data-line-number="32"><span class="kw">rayleighquotient_method</span>(A,neigenvector )</a></code></pre></div>
<pre><code>## $Iteration
##      J   eigenval   eigenvec1  eigenvec2  eigenvec3        error
## [1,] 0  0.0000000  0.07759501 -0.7498487  0.6570433 0.000000e+00
## [2,] 1 -0.5881337 -0.07759223  0.7498484 -0.6570441 1.000000e+00
## [3,] 2 -0.5881341  0.07759288 -0.7498484  0.6570439 6.841798e-07
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalue
##            [,1]
## [1,] -0.5881341
## 
## $eigenvector
##   eigenvec1   eigenvec2   eigenvec3 
##  0.07759288 -0.74984844  0.65704388</code></pre>

<p>Given an actual <strong>Eigenvector</strong>, we can use the <strong>Rayleigh Quotient Method</strong> to discover the corresponding unknown <strong>Eigenvalue</strong>. More importantly, the method allows discovering interior <strong>Eigenvalues</strong>, which is an advantage over <strong>Power Method</strong>, which only discovers the largest <strong>Eigenvalue</strong> and over <strong>Inverse Power Method</strong>, which only finds the smallest <strong>Eigenvalue</strong>.</p>
</div>
<div id="qr-method-using-qr-decomposition-by-givens" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.4</span> QR Method (using QR Decomposition by Givens)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us now look into the <strong>QR Method</strong>, which allows us to find the list of <strong>Eigenvalues</strong> of a given matrix.</p>
<p>In <strong>Linear Algebra</strong>, we have discussed how to decompose a matrix into its <strong>QR form</strong>. We use <strong>QR decomposition by Givens</strong> in the algorithm to derive all the <strong>Eigenvalues</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}
loop\ j\ in\ 1:\ ... \\
\ \ \ \ QR = qr\_decomposition\_by\_givens(A_{j-1})\\
\ \ \ \ A_j = Q^T A_{j-1} Q\\
\ \ \ \ \lambda_j = diag(Q) \\
\ \ \ \ if\ err(\|\lambda_j\|_{L2}, \|\lambda_{j-1}\|_{L2}) &lt; \text{tolerance then break} \\
end\ loop
\end{array}
\]</span></p>
<p>We implement the <strong>QR Method</strong> in R code like so:</p>

<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1">qr_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb34-2" data-line-number="2">    n          =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb34-3" data-line-number="3">    sequence   =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb34-4" data-line-number="4">    old_evalue =<span class="st"> </span>evalues =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb34-5" data-line-number="5">    limit      =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb34-6" data-line-number="6">    tol        =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb34-7" data-line-number="7">    A_ =<span class="st"> </span>A</a>
<a class="sourceLine" id="cb34-8" data-line-number="8">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb34-9" data-line-number="9">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb34-10" data-line-number="10">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalues, err))</a>
<a class="sourceLine" id="cb34-11" data-line-number="11">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb34-12" data-line-number="12">            QR =<span class="st"> </span><span class="kw">qr_decomposition_by_givens</span>(A) <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb34-13" data-line-number="13">            A        =<span class="st"> </span><span class="kw">t</span>(QR<span class="op">$</span>Q) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>QR<span class="op">$</span>Q</a>
<a class="sourceLine" id="cb34-14" data-line-number="14">            evalues  =<span class="st"> </span><span class="kw">diag</span>(QR<span class="op">$</span>R)</a>
<a class="sourceLine" id="cb34-15" data-line-number="15">            ls_ev    =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(evalues<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb34-16" data-line-number="16">            ls_ov    =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(old_evalues<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb34-17" data-line-number="17">            err      =<span class="st"> </span>(ls_ev <span class="op">-</span><span class="st"> </span>ls_ov) <span class="op">/</span><span class="st"> </span>ls_ev</a>
<a class="sourceLine" id="cb34-18" data-line-number="18">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, evalues,  err))</a>
<a class="sourceLine" id="cb34-19" data-line-number="19">            <span class="cf">if</span> (<span class="kw">abs</span>(err) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span>  </a>
<a class="sourceLine" id="cb34-20" data-line-number="20">        }</a>
<a class="sourceLine" id="cb34-21" data-line-number="21">        old_evalues =<span class="st"> </span>evalues </a>
<a class="sourceLine" id="cb34-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb34-23" data-line-number="23">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;J&quot;</span>, <span class="kw">paste</span>(<span class="st">&quot;eigenval&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  </a>
<a class="sourceLine" id="cb34-24" data-line-number="24">                           <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb34-25" data-line-number="25">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=<span class="st"> </span>A_, <span class="st">&quot;eigenvalues&quot;</span>=evalues)</a>
<a class="sourceLine" id="cb34-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb34-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb34-28" data-line-number="28"><span class="kw">qr_method</span>(A)</a></code></pre></div>
<pre><code>## $Iteration
##       J eigenval1 eigenval2  eigenval3        error
##  [1,] 0  0.000000  0.000000  0.0000000 0.000000e+00
##  [2,] 1  3.741657  3.927922 -0.8164966 1.000000e+00
##  [3,] 2  8.489489  2.227705 -0.6345152 3.765876e-01
##  [4,] 3 10.404981  1.993459 -0.5785391 1.706118e-01
##  [5,] 4 10.635189  1.907776 -0.5914371 1.950956e-02
##  [6,] 5 10.669923  1.915441 -0.5871527 3.247539e-03
##  [7,] 6 10.675897  1.910184 -0.5884393 4.617532e-04
##  [8,] 7 10.676959  1.911289 -0.5880405 1.120004e-04
##  [9,] 8 10.677149  1.910858 -0.5881629 1.078783e-05
## [10,] 9 10.677183  1.910974 -0.5881252 4.771086e-06
## 
## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $eigenvalues
## [1] 10.6771829  1.9109740 -0.5881252</code></pre>

<p>As long as <strong>QR Method</strong> yields all the <strong>Eigenvalues</strong>, this saves us from composing the <strong>characteristic polynomials</strong> using determinants. From here, we plug the individual <strong>Eigenvalues</strong> into the original matrix and form each corresponding <strong>characteristic matrix</strong>. Then, we derive the <strong>RREF</strong> of each <strong>characteristic matrix</strong> and perform a substitution to get the corresponding <strong>Eigenvectors</strong>.</p>
<p>Note that <strong>QR iteration</strong> may not necessarily converge for <strong>Eigenvalues</strong> that are negative or for matrices with complex numbers. For this, workarounds such as <strong>conjugate shifts</strong> are being used.</p>
<p>Also, note that other <strong>QR decomposition</strong> such as <strong>Householder</strong> can be used and evaluated, though best to play around with the “-sign” in <strong>Householder</strong> as that tends to vary the signs of <strong>Eigenvalues</strong> <span class="citation">(Anley E. F. <a href="bibliography.html#ref-ref39e">2016</a>)</span>.</p>
</div>
<div id="jacobi-eigenvalue-method-using-jacobi-rotation" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.5</span> Jacobi Eigenvalue Method (using Jacobi Rotation)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Jacobi Eigenvalue Method</strong> reduces a <strong>real symmetric</strong> matrix into its <strong>diagonal</strong> form. This reduction is an orthogonalization transformation called <strong>Jacobi rotation</strong> based on <strong>Givens rotation</strong> <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>.</p>
<p><span class="math display">\[
G = 
\left[
\begin{array}{rr}
cos &amp; -sin \\
sin &amp; cos
\end{array}
\right]
\]</span></p>
<p>Recall that <strong>Givens rotation</strong> is used in <strong>QR decomposition</strong> to transform a matrix into its upper triangular form by annihilating its lower triangular region, excluding the diagonal entries. In the same fashion, the <strong>Jacobi Eigenvalue Method</strong> uses <strong>Jacobi rotation</strong> to annihilate non-diagonal entries, leaving a diagonal with <strong>Eigenvalues</strong> as entries.</p>
<p>To illustrate, here is a simple symmetric matrix:</p>
<p><span class="math display">\[
A = 
\left[
\begin{array}{rrrrr}
a_{ii} &amp; a_{ij}\\
a_{ji} &amp; a_{jj}
\end{array}
\right] =
\left[
\begin{array}{rrrrr}
9 &amp; 1\\
1 &amp; 9
\end{array}
\right]
\]</span></p>
<p>First, we compute for the angle - <span class="math inline">\(\theta\)</span> - that we need to plug into our <strong>cos</strong> and <strong>sin</strong>:</p>
<p><span class="math display">\[\begin{align*}
tan 2\theta {}&amp;= \frac{2A_{ij}}{(A_{jj} - A_{ii})} 
= \frac{2(1)}{9 - 9)}  = \infty \\
\theta &amp;= \frac{1}{2} arctan(\infty) = -0.7853982
\end{align*}\]</span></p>
<p>thus, we get:</p>
<p><span class="math display">\[\begin{align*}
cos (\theta) {}&amp;= cos(-0.7853982) = 0.7071068 \\
sin (\theta) &amp;= sin(-0.7853982) = -0.7071068
\end{align*}\]</span></p>
<p>We then construct the <strong>Jacobi rotation matrix</strong>:</p>
<p><span class="math display">\[
J(i,j) = 
\left[
\begin{array}{rr}
cos &amp; sin \\
-sin &amp; cos
\end{array}
\right] = 
\left[
\begin{array}{rrrrr}
0.7071608 &amp; -0.7071608 \\
0.7071608 &amp; 0.7071608
\end{array}
\right]
\]</span></p>
<p>Then, using <strong>Jacobi rotation matrix</strong>, we transform matrix <span class="math inline">\(A\)</span>, into a <strong>diagonal matrix</strong>, <span class="math inline">\(A&#39;\)</span>:</p>
<p><span class="math display">\[\begin{align*}
A&#39; = J^TAJ {}&amp;= 
\left[
\begin{array}{rrrrr}
cos &amp; -sin \\
sin &amp; cos
\end{array}
\right]_{J^T}
\left[
\begin{array}{rrrrr}
9 &amp; 1 \\
1 &amp; 9
\end{array}
\right]_A
\left[
\begin{array}{rrrrr}
cos &amp; sin \\
-sin &amp; cos
\end{array}
\right]_J \\
&amp;=
\left[
\begin{array}{rrrrr}
0.7071608 &amp; 0.7071608 \\
-0.7071608 &amp; 0.7071608
\end{array}
\right]
\left[
\begin{array}{rrrrr}
9 &amp; 1 \\
1 &amp; 9
\end{array}
\right]
\left[
\begin{array}{rrrrr}
0.7071608 &amp; -0.7071608 \\
0.7071608 &amp; 0.7071608
\end{array}
\right] \\
A&#39; = D &amp;=
\left[
\begin{array}{rrrrr}
10 &amp; 0 \\
0 &amp; 8
\end{array}
\right]
\end{align*}\]</span></p>
<p><strong>In general</strong>, the algorithm is described simply by the following equation:</p>
<p><span class="math display" id="eq:eqnnumber6">\[\begin{align}
A&#39; = J^TAJ\ \leftarrow \text{iterate until}\ A&#39; = D \tag{3.3}
\end{align}\]</span></p>
<p>We iterate until <span class="math inline">\(A&#39;\)</span> transforms into a diagonal matrix.</p>
<p>Here <strong>J</strong> is the <strong>Jacobi rotation matrix</strong>:</p>
<p><span class="math display">\[
J(i,j) = 
\left[
\begin{array}{rrrrr}
1 &amp; . &amp; . &amp; . &amp; . \\
. &amp; c_{ii} &amp; . &amp; s_{ji} &amp; . \\
. &amp; . &amp; 1 &amp; . &amp; . \\
. &amp; -s_{ij} &amp; . &amp; c_{jj} &amp; . \\
. &amp; . &amp; . &amp; . &amp; 1 \\
\end{array}
\right]
\]</span></p>
<p>To construct the <strong>Jacobi rotation matrix</strong>, we need to identify or select a region in the matrix using <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span> indices. Below are six choices we can make to choose a region from which to base our <strong>rotation matrix</strong>.</p>
<p><span class="math display">\[
\begin{array}{l}
\left[\begin{array}{ccccc}
\square  &amp; \square  &amp; . &amp; .\\
\square  &amp; \square  &amp;. &amp; .\\
.  &amp; . &amp; . &amp; .\\
. &amp; .  &amp; . &amp; .
\end{array}\right]_{i=1, j=2}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
. &amp; \square  &amp; \square &amp; . \\
. &amp; \square &amp; \square &amp; . \\
.  &amp; . &amp; . &amp; .
\end{array}\right]_{i=2,j=2}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
.  &amp; . &amp; . &amp; .\\
. &amp;. &amp; \square &amp; \square \\
. &amp; . &amp; \square &amp; \square  \\
\end{array}\right]_{i=3,j=4} \\
\\
\left[\begin{array}{ccccc}
\square  &amp; \square &amp; \square &amp;.\\
\square &amp; . &amp;   \square &amp; .\\
\square &amp; \square&amp;  \square &amp; . \\
.  &amp; . &amp; . &amp; .
\end{array}\right]_{i=1,j=3}
\left[\begin{array}{ccccc}
.  &amp; . &amp; . &amp; .\\
.&amp; \square &amp; \square &amp; \square \\
. &amp; \square  &amp; . &amp;   \square \\
. &amp; \square &amp; \square &amp;  \square  \\
\end{array}\right]_{i=2,j=4}
\left[\begin{array}{ccccc}
\square &amp; \square &amp; \square &amp; \square  \\
\square  &amp; . &amp; . &amp;  \square\\
\square &amp; .  &amp; . &amp; \square\\
\square &amp; \square&amp; \square &amp;  \square \\
\end{array}\right]_{i=1,j=4}
\end{array}
\]</span></p>
<p>For example, if we are to select a region with <span class="math inline">\(i=1, j=3\)</span>, then our <strong>Jacobi rotation matrix</strong> becomes:</p>
<p><span class="math display">\[
J(i=1,j=3) = 
\left[
\begin{array}{rrrrr}
c_{ii} &amp; . &amp; s_{ji} &amp; . \\ 
. &amp; 1 &amp; . &amp; . \\
-s_{ij} &amp; . &amp; c_{jj} &amp; . \\
. &amp; . &amp; . &amp; 1
\end{array}
\right]_{i=1, j=3}
\]</span></p>
<p>The most simple selection is based on searching for a region with the largest off-diagonal entry:</p>
<p><span class="math display">\[
max\{A\} = max_{i&lt;j}\{\|A_{ij}\|\}
\]</span></p>
<p>This allows for faster convergence. The <strong>stop</strong> is when the max is less or equal to a tolerance level (e.g., in our case <span class="math inline">\(\text{1e-5}\)</span>). Unfortunately, it becomes a challenge to perform a search against a matrix with extreme high-dimension. In that case, we can use the <strong>Cyclic Jacobi Method</strong>. We leave this method to the reader to investigate. In our illustration below, we use a simple selection algorithm for the region for the <strong>Jacobi rotation matrix</strong>.</p>
<p>Now, assume a selected region with indices <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, our next step is then to compute for both <span class="math inline">\(c = cos\)</span> and <span class="math inline">\(s=sin\)</span>:</p>
<p><span class="math display">\[\begin{align*}
tan 2\theta {}&amp;= \left(\frac{2A_{ij}}{A_{jj}-A_{ii}}\right) \\
\theta  &amp;= \frac{1}{2} arctan\left(\frac{2A_{ij}}{A_{jj}-A_{ii}}\right) \\
c  &amp;= cos(\theta) \\
s  &amp;= sin(\theta) \\
\end{align*}\]</span></p>
<p>For a more stable solution, we can also perform trigonometric manipulation to compute for <strong>cos</strong> and <strong>sin</strong> given <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[\begin{align*}
 \\
t {}&amp;= \frac{sign(\theta)}{|\theta| + \sqrt{\theta^2 + 1}}\ \ \ \leftarrow \ \ \ t^2 + 2\theta t - 1 = 0\\
c &amp;= \frac{1}{\sqrt{t^2 + 1}} \\
s &amp;= ct
\end{align*}\]</span></p>
<p>From there, we construct the <span class="math inline">\(J\)</span> matrix and use that to approximate a <strong>diagonal</strong> matrix using Equation <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#eq:eqnnumber6">(3.3)</a> or perhaps to validate, we can use the following list of equations to construct <span class="math inline">\(A&#39;\)</span>:</p>
<p><span class="math display" id="eq:equate1050007" id="eq:equate1050006" id="eq:equate1050005" id="eq:equate1050004" id="eq:equate1050003">\[\begin{align}
A&#39;_{ii} {}&amp;= c^2A_{ii} - 2csA_{ij} + s^2A_{jj}  \tag{3.4} \\
A&#39;_{jj} &amp;= s^2A_{ii} + 2csA_{ij} + c^2A_{jj}  \tag{3.5} \\
A&#39;_{ij} &amp;= A&#39;_{ji} = (c^2 - s^2)A_{ij} + cs(A_{ii} - A_{jj}) = 0 \tag{3.6} \\
A&#39;_{ik} &amp;= A&#39;_{ki} = cA_{ki} - sA_{kj} \rightarrow for\ k\ \neq i\ and\ k \neq j  \tag{3.7} \\
A&#39;_{jk} &amp;= A&#39;_{kj} = sA_{ki} + cA_{kj} \rightarrow for\ k\ \neq i\ and\ k \neq j   \tag{3.8} 
\end{align}\]</span></p>
<p>Note that the diagonal form may not readily be apparent the first time we compute for <span class="math inline">\(A&#39;\)</span>. Hence, it may take a few iterations to get an <strong>approximation</strong> of a diagonal form in which the entries are the <strong>Eigenvalues</strong> of the symmetric matrix. For example:</p>
<p><span class="math display">\[\begin{align*}
A_1 {}&amp;= J^T_0A_0J_0 \rightarrow choose\ i,j\ to\ construct\ J_0\\
A_2 &amp;= J_1^TA_1J_1 \rightarrow choose\ i,j\ to\ construct\ J_1\\
&amp;\vdots \\
D &amp;= J_k^TA_kJ_k \rightarrow choose\ i,j\ to\ construct\ J_k 
\end{align*}\]</span></p>
<p>After every step in the <strong>Jacobi iteration</strong> method, the off-diagonal elements, <span class="math inline">\(A&#39;_{ij}\)</span> and <span class="math inline">\(A&#39;_{ji}\)</span>, are annihilated (turn into zeros).</p>
<p>We illustrate using the following symmetric matrix:</p>
<p><span class="math display">\[
A = 
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]
\]</span></p>
<p><strong>For the first Iteration</strong>, let us formulate our first <span class="math inline">\(J_1\)</span> matrix and <span class="math inline">\(A_1\)</span> matrix. We see six being a max off-diagonal entry in <span class="math inline">\(A_{i=2,j=3}\)</span>. The corresponding <strong>Jacobi rotation matrix</strong> is:</p>
<p><span class="math display">\[
J_1 = 
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; 0.7071068 &amp; 0.7071068 &amp; . \\
. &amp; -0.7071068 &amp; 0.7071068 &amp; . \\
. &amp; . &amp; . &amp; 1
\end{array}
\right]
\]</span></p>
<p>We end up with matrix <span class="math inline">\(A_1\)</span>:</p>

<p><span class="math display">\[
A_1 = J_1^TAJ_1 = 
\left[
\begin{array}{rrrr}
1.0000000 &amp; -7.071068e-01 &amp; 3.535534e+00 &amp; 4.0000000 \\
-0.7071068 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 0.7071068 \\
3.5355339 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 3.5355339 \\
4.0000000 &amp; 7.071068e-01 &amp; 3.535534e+00 &amp; 1.0000000
\end{array}
\right]
\]</span>
</p>
<p><strong>For the second Iteration</strong>, we construct our next <span class="math inline">\(J_2\)</span> matrix and <span class="math inline">\(A_2\)</span> matrix. We see four being a max off-diagonal entry in <span class="math inline">\(A_{i=1,j=4}\)</span>. The corresponding <strong>Jacobi rotation matrix</strong> is:</p>
<p><span class="math display">\[
J_2 = 
\left[
\begin{array}{rrrr}
0.7071068  &amp; . &amp; . &amp; 0.7071068  \\
. &amp; 1 &amp; . &amp; . \\
. &amp; . &amp; 1 &amp; . \\
-0.7071068  &amp; . &amp; . &amp; 0.7071068 
\end{array}
\right]
\]</span></p>
<p>We end up with matrix <span class="math inline">\(A_2\)</span>:</p>

<p><span class="math display">\[
A_2 = J_2^TA_1J_2 = 
\left[
\begin{array}{rrrr}
-3.000000e+00 &amp; -1.000000e+00 &amp; 4.440892e-16 &amp; 4.440892e-16 \\
-1.000000e+00 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 5.551115e-16 \\
4.440892e-16 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 5.000000e+00 \\
8.881784e-16 &amp; 5.551115e-16 &amp; 5.000000e+00 &amp; 5.000000e+00
\end{array}
\right]
\]</span>
</p>
<p>We continue until we hit the fifth iteration. The <strong>stop</strong> is when the search for max off-diagonal entry ends at zero.</p>
<p>We end up with the final matrix <span class="math inline">\(A_5\)</span>:</p>
<p><span class="math display">\[
A_5 = J_5^TA_4J_5 = 
\left[
\begin{array}{rrrr}
-3.162278 &amp; . &amp; . &amp; . \\
. &amp; 3.162278 &amp; . &amp; . \\
. &amp; . &amp; 17.07107 &amp; . \\
. &amp; . &amp; . &amp; 2.928932
\end{array}
\right]
\]</span></p>
<p>Hence, the approximate <strong>Eigenvalues</strong> are (sorted in descending order):</p>
<p><span class="math display">\[
\tilde \lambda_1=17.07107 \ \ \ \ 
\tilde \lambda_2=3.162278\ \ \ \ \ 
\tilde \lambda_3=2.928932\ \ \ \ \ 
\tilde \lambda_4=-3.162278
\]</span>
It is notable to mention that there are applications of the algorithm in which we may not require to seek all of the eigenvalues. For example, suppose a matrix is extremely large. It may be enough to get the first few of them using a different <strong>stopping</strong> criterion versus just evaluating a max off-diagonal entry.</p>
<p>Finally, we illustrate a naive implementation of the <strong>Jacobi Eigenvalue Method</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">offdiagonal_maxsearch &lt;-<span class="st"> </span><span class="cf">function</span>(A, tol) {</a>
<a class="sourceLine" id="cb36-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">    m =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb36-4" data-line-number="4">    max =<span class="st"> </span>tol</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">    index =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb36-7" data-line-number="7">        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb36-8" data-line-number="8">            <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb36-9" data-line-number="9">                <span class="cf">if</span> ( max <span class="op">&lt;</span><span class="st"> </span><span class="kw">abs</span>( A[i,j]) ) {</a>
<a class="sourceLine" id="cb36-10" data-line-number="10">                    <span class="cf">if</span> (i <span class="op">&lt;</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb36-11" data-line-number="11">                        max =<span class="st"> </span><span class="kw">abs</span>( A[i,j] )</a>
<a class="sourceLine" id="cb36-12" data-line-number="12">                        index =<span class="st"> </span><span class="kw">c</span>(i,j)</a>
<a class="sourceLine" id="cb36-13" data-line-number="13">                    }</a>
<a class="sourceLine" id="cb36-14" data-line-number="14">                }</a>
<a class="sourceLine" id="cb36-15" data-line-number="15">            }</a>
<a class="sourceLine" id="cb36-16" data-line-number="16">        }</a>
<a class="sourceLine" id="cb36-17" data-line-number="17">    }</a>
<a class="sourceLine" id="cb36-18" data-line-number="18">    i =<span class="st"> </span><span class="kw">min</span>(index)</a>
<a class="sourceLine" id="cb36-19" data-line-number="19">    j =<span class="st"> </span><span class="kw">max</span>(index)</a>
<a class="sourceLine" id="cb36-20" data-line-number="20">    <span class="kw">return</span>( <span class="kw">list</span>(<span class="st">&quot;max&quot;</span>=max, <span class="st">&quot;i&quot;</span>=i, <span class="st">&quot;j&quot;</span>=j) )</a>
<a class="sourceLine" id="cb36-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb36-22" data-line-number="22">rotation_matrix &lt;-<span class="st"> </span><span class="cf">function</span>(i,j, A) {</a>
<a class="sourceLine" id="cb36-23" data-line-number="23">    R =<span class="st">  </span><span class="kw">diag</span>(<span class="kw">ncol</span>(A))</a>
<a class="sourceLine" id="cb36-24" data-line-number="24">    theta =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">atan</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>A[i,j] <span class="op">/</span><span class="st"> </span>( A[j,j] <span class="op">-</span><span class="st"> </span>A[i,i]))</a>
<a class="sourceLine" id="cb36-25" data-line-number="25">    cos =<span class="st"> </span><span class="kw">cos</span>(theta) </a>
<a class="sourceLine" id="cb36-26" data-line-number="26">    sin =<span class="st"> </span><span class="kw">sin</span>(theta)</a>
<a class="sourceLine" id="cb36-27" data-line-number="27">    R[i,i] =<span class="st"> </span>cos; R[i,j] =<span class="st"> </span>sin; R[j,i] =<span class="st"> </span><span class="op">-</span>sin; R[j,j] =<span class="st"> </span>cos</a>
<a class="sourceLine" id="cb36-28" data-line-number="28">    <span class="kw">list</span>(<span class="st">&quot;J&quot;</span>=R, <span class="st">&quot;c&quot;</span>=cos, <span class="st">&quot;s&quot;</span>=sin)</a>
<a class="sourceLine" id="cb36-29" data-line-number="29">}</a>
<a class="sourceLine" id="cb36-30" data-line-number="30">jacobi_eigenvalue_method &lt;-<span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb36-31" data-line-number="31">    iterate =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb36-32" data-line-number="32">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb36-33" data-line-number="33">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb36-34" data-line-number="34">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb36-35" data-line-number="35">      S =<span class="st"> </span><span class="kw">offdiagonal_maxsearch</span>(A, tol)</a>
<a class="sourceLine" id="cb36-36" data-line-number="36">      <span class="cf">if</span> (S<span class="op">$</span>max <span class="op">&lt;=</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb36-37" data-line-number="37">      R =<span class="st"> </span><span class="kw">rotation_matrix</span>(S<span class="op">$</span>i,S<span class="op">$</span>j, A)</a>
<a class="sourceLine" id="cb36-38" data-line-number="38">      A =<span class="st"> </span><span class="kw">t</span>(R<span class="op">$</span>J) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>R<span class="op">$</span>J</a>
<a class="sourceLine" id="cb36-39" data-line-number="39">      iterate =<span class="st"> </span>k</a>
<a class="sourceLine" id="cb36-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb36-41" data-line-number="41">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;iterate&quot;</span>=k, </a>
<a class="sourceLine" id="cb36-42" data-line-number="42">         <span class="st">&quot;eigenvalues&quot;</span>=<span class="kw">sort</span>(<span class="kw">diag</span>(A),  <span class="dt">decreasing=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb36-43" data-line-number="43">}</a>
<a class="sourceLine" id="cb36-44" data-line-number="44">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,  <span class="dv">2</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">3</span>,  <span class="dv">3</span>,<span class="dv">6</span>,<span class="dv">9</span>,<span class="dv">2</span>,  <span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb36-45" data-line-number="45"><span class="kw">jacobi_eigenvalue_method</span>(A)</a></code></pre></div>
<pre><code>## $matrix
##               [,1]         [,2]         [,3]          [,4]
## [1,] -3.162278e+00 0.000000e+00 7.710670e-16  2.513307e-16
## [2,]  0.000000e+00 3.162278e+00 1.129214e-15  4.835791e-17
## [3,]  9.059580e-16 8.994983e-16 1.707107e+01 -8.881784e-16
## [4,]  6.699290e-16 6.651312e-17 0.000000e+00  2.928932e+00
## 
## $iterate
## [1] 5
## 
## $eigenvalues
## [1] 17.071068  3.162278  2.928932 -3.162278</code></pre>

<p>We validate the result using <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb38"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb38-1" data-line-number="1"><span class="kw">eigen</span>(A)<span class="op">$</span>values</a></code></pre></div>
<pre><code>## [1] 17.071068  3.162278  2.928932 -3.162278</code></pre>

</div>
<div id="arnoldi-method-using-gram-schmidt-in-krylov-subspace" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.6</span> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Arnoldi Method</strong> is one of the orthogonal projection methods of finding the approximation of both <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>. To understand the idea behind this method, let us discuss what we are projecting orthogonally using Figure <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fig:krylovspace">3.1</a>. The left side portion of the figure covers generalized minimal residual (GMRES) and Conjugate Gradient(CG) for solving systems of linear equations. But here, our discussion is about the right side portion, which covers solving for <strong>Eigenvalue</strong> problems <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Sleijpen G. <a href="bibliography.html#ref-ref61g">2014</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:krylovspace"></span>
<img src="krylovspace.png" alt="Orthogonal Projection unto Krylov Space" width="90%" />
<p class="caption">
Figure 3.1: Orthogonal Projection unto Krylov Space
</p>
</div>
<p>Recall the <strong>Eigen</strong> equation (<span class="math inline">\(A \cdotp \mathbf{\vec{v}} = \lambda \times \mathbf{\vec{v}}\)</span>) under Section <strong>Eigenvectors and Eigenvalues</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>). Here, we emphasis the actual <strong>Eigenvectors</strong> denoted as <strong>v</strong> and <strong>Eigenvalues</strong>, <span class="math inline">\(\lambda\)</span>. Equivalently, we can designate a notation denoting approximation of <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>:</p>
<p><span class="math display" id="eq:equate1050008">\[\begin{align}
A \tilde u = \tilde \lambda \tilde u  \tag{3.9} 
\end{align}\]</span></p>
<p>Then we can find a subspace, <strong>K</strong>, into which we can project <span class="math inline">\(A \tilde u\)</span> such that the equation becomes:</p>
<p><span class="math display" id="eq:equate1050009">\[\begin{align}
Proj_{(K)}(A \tilde u)\ \ \ \ \ \rightarrow \ \ \ \  (A \tilde u - \tilde \lambda \tilde u) \perp K \tag{3.10} 
\end{align}\]</span></p>
<p>Here, we introduce the <strong>Krylov subspace</strong> , K, such that <span class="math inline">\((A \tilde u - \tilde \lambda \tilde u)\)</span> is orthogonal to K. For more intuition about <strong>Krylov subspace</strong>, it may also help to review Section <strong>Krylov Method</strong>.</p>
<p>Given a system of linear equations in matrix form, <span class="math inline">\(Ax = b\)</span>, the <strong>Krylov subspace</strong> is a subspace spanned by {Ab + AAb + AAAb + … + AAA…Ab} , which can be expressed this way (m-th order of Krylov sequence):</p>
<p><span class="math display" id="eq:equate1050010">\[\begin{align}
K_m(A,b) = span\ \{b,\ Ab,\ A^2b,\ A^3b,\ ...,\ A^{m-1}b\}  \tag{3.11} 
\end{align}\]</span></p>
<p>In the same manner, in this particular case for eigenvalue problems, <strong>Krylov subspace</strong> is spanned by a linear combination like so (using an orthogonal basis, <strong>Q</strong>):</p>
<p><span class="math display" id="eq:equate1050011">\[\begin{align}
K_m(A,q) =  span\ \{ q_1,\ Aq_2,\ A^2q_3, ...,\ A^{m-1}q_m \} \tag{3.12} 
\end{align}\]</span></p>
<p>We derive a change of <strong>basis</strong> called the <strong>Arnoldi basis</strong> from the subspace, <strong>K</strong>. And we let this basis be an orthogonal matrix denoted as <strong>Q</strong>:</p>
<p><span class="math display" id="eq:equate1050012">\[\begin{align}
Q_{basis} = \{ \ q_1,\ q_2,\ q_3, ...,\ q_{m}\ \}  \tag{3.13} 
\end{align}\]</span></p>
<p>See <strong>Krylov Methods</strong> and <strong>GMRES</strong> in a few sections ahead, covering an example of the construction of the Q basis for the <strong>K</strong> space.</p>
<p>The <strong>basis</strong> is derived by <strong>Arnoldi Method</strong>, decomposing an <span class="math inline">\(n \times n\)</span> square matrix, <strong>A</strong>, into the form <span class="math inline">\(QHQ^T\)</span>. The algorithm also shows the combined <span class="math inline">\(AQ\)</span> equal to a combined upper Hessenberg matrix, <strong>H</strong>, and a basis matrix, <strong>Q</strong>, expressed as:</p>
<p><span class="math display" id="eq:equate1050013">\[\begin{align}
AQ_m = Q_{m+1}H_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QH) \perp K \tag{3.14} 
\end{align}\]</span></p>
<p>We show the equation in matrix forms:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrr}
&amp; &amp;   \\
&amp; &amp;  \\
 &amp; A_{mxm} &amp; \\
&amp; &amp;  \\
&amp; &amp;  \\
\end{array}
\right]
\left[
\begin{array}{rrrr}
&amp; &amp; \\
&amp; &amp; \\
q1 &amp; ... &amp;  q_m \\
&amp; &amp; \\
&amp; &amp; \\
\end{array}
\right] =
\left[
\begin{array}{rrrr}
&amp; &amp; \\
&amp; &amp; \\
q1 &amp; ... &amp; q_{m+1} \\
&amp; &amp; \\
&amp; &amp; \\
\end{array}
\right]
\left[
\begin{array}{llll}
h_{1,1} &amp; h_{1,2} &amp;  \ldots &amp; h_{1,m}  \\
\alpha_{2,1} &amp; h_{2,2} &amp;  \ldots &amp; h_{2,m} \\
. &amp;\alpha_{3,2} &amp;  \ldots &amp; h_{3,m}  \\
. &amp; . &amp;  \ddots &amp; \vdots \\
. &amp; . &amp;  . &amp; \alpha_{m+1,m}  \\
\end{array}
\right]
\]</span>
</p>
<p>Based on the equation, we can derive the equation for any column. For example, for the 2nd column and 3rd columns, we can use the following equations to compute for <span class="math inline">\(Aq_2\)</span> and <span class="math inline">\(Aq_3\)</span> respectively:</p>
<p><span class="math display">\[
Aq_2 =  h_{1,2}q_1 +  h_{2,2}q_2 + \alpha_{3,2}q_3\ \ \ \ \ \ \ \ \ \
Aq_3 =  h_{1,3}q_1 +  h_{2,3}q_2 + h_{3,3}q_3 + \alpha_{4,3}q_4
\]</span></p>
<p>where <span class="math inline">\(\alpha_{j+1,j} = \| Aq_j - \sum_k(h_{k,j}q_j) \|_{L2}\)</span>.</p>
<p>On the other hand, using the <strong>Arnoldi basis</strong>, <strong>Q</strong>, we can also generate the <strong>upper Hessenberg</strong> matrix like so:</p>
<p><span class="math display" id="eq:eqnnumber7">\[\begin{align}
H_m = Q_{m}^TAQ_m \tag{3.15}
\end{align}\]</span></p>
<p>This can be interpreted as <strong>H</strong> being the projection of <strong>A</strong> onto <strong>K</strong>.</p>
<p>Now, given <span class="math inline">\(Q\)</span> and the approximation of <strong>Eigenvalues</strong> and <strong>Eigenvectors</strong>, we can perform a mathematical conversion:</p>
<p><span class="math display" id="eq:equate1050014">\[\begin{align}
A \tilde u = \tilde \lambda \tilde u\ \rightarrow\ \ \ \ Q^TAQ \tilde u = \lambda Q^TQ \tilde u \rightarrow\ \ \ \ H \tilde u = \tilde \lambda \tilde u  \tag{3.16} 
\end{align}\]</span></p>
<p>This pair <span class="math inline">\(\tilde \lambda \tilde u\)</span> is called the <strong>Ritz</strong> pair. We can find the <strong>Ritz value</strong>, <span class="math inline">\(\tilde \lambda\)</span>, and <strong>Ritz vector</strong>, <span class="math inline">\(\tilde u\)</span>, using <strong>QR Method</strong> to solve the equation below (recall the equation <span class="math inline">\((A - \lambda I) \cdotp v = 0\)</span> under Section <strong>Eigenvectors and Eigenvalues</strong> in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>)):</p>
<p><span class="math display">\[
(H - \tilde \lambda I)\tilde u = 0
\]</span></p>
<p>That said, let us now review the following <strong>Arnoldi Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
v \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = v / \|v\|_{L2}\\
loop\ j\ in\ 1:m \\
\ \ \ \ v = Aq_j\\
\ \ \ \ loop\ k\ in\ 1:j \rightarrow \text{Gram-Schmidt} \\
\ \ \ \ \ \ \ \ h_{kj} = q_k^T v\\
\ \ \ \ \ \ \ \ v = v - h_{kj}q_k \\
\ \ \ \ end\ loop \\
\ \ \ \ h_{j+1,j} = \|v\|_{L2}\\
\ \ \ \ if\ (h_{j+1,j} &lt; tol\ )\ break \\
\ \ \ \ q_{j+1} = v / h_{j+1,j}\\
end\ loop
\end{array}
\]</span></p>
<p>We follow this with a naive implementation of <strong>Arnoldi Method</strong> in R code:</p>

<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">arnoldi_method &lt;-<span class="st"> </span><span class="cf">function</span>(A, v) {</a>
<a class="sourceLine" id="cb40-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb40-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb40-4" data-line-number="4">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>(m<span class="op">+</span><span class="dv">1</span>)), n, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Q n x n+1</span></a>
<a class="sourceLine" id="cb40-5" data-line-number="5">  h =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, (m<span class="op">+</span><span class="dv">1</span>)<span class="op">*</span>m), (m<span class="op">+</span><span class="dv">1</span>), <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># H n+1 x n </span></a>
<a class="sourceLine" id="cb40-6" data-line-number="6">  q[,<span class="dv">1</span>] =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-7" data-line-number="7">  tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb40-8" data-line-number="8">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb40-9" data-line-number="9">    v =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb40-10" data-line-number="10">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) { <span class="co"># Gram-Schmidt Orthogonalization</span></a>
<a class="sourceLine" id="cb40-11" data-line-number="11">        h[k,j] =<span class="st"> </span><span class="kw">t</span>(q[,k]) <span class="op">%*%</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb40-12" data-line-number="12">        v =<span class="st"> </span>v <span class="op">-</span><span class="st">  </span>h[k,j] <span class="op">*</span><span class="st"> </span>q[,k] <span class="co"># subtracting projection</span></a>
<a class="sourceLine" id="cb40-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb40-14" data-line-number="14">    h[j<span class="op">+</span><span class="dv">1</span>,j] =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb40-15" data-line-number="15">    <span class="cf">if</span> (<span class="kw">abs</span>( h[j<span class="op">+</span><span class="dv">1</span>,j]) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb40-16" data-line-number="16">    q[,j<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>v <span class="op">/</span><span class="st"> </span>h[j<span class="op">+</span><span class="dv">1</span>,j]</a>
<a class="sourceLine" id="cb40-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb40-18" data-line-number="18">  h_m =<span class="st"> </span>h[<span class="dv">1</span><span class="op">:</span>m,<span class="dv">1</span><span class="op">:</span>m]; q_m =<span class="st"> </span>q[<span class="dv">1</span><span class="op">:</span>m,<span class="dv">1</span><span class="op">:</span>m]</a>
<a class="sourceLine" id="cb40-19" data-line-number="19">  <span class="kw">list</span>(<span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q_m , <span class="st">&quot;H&quot;</span> =<span class="st"> </span>h_m,  </a>
<a class="sourceLine" id="cb40-20" data-line-number="20">       <span class="st">&quot;AQ&quot;</span> =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q_m, <span class="st">&quot;QH&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>h, </a>
<a class="sourceLine" id="cb40-21" data-line-number="21">       <span class="st">&quot;H=QtAQ&quot;</span> =<span class="st"> </span><span class="kw">t</span>(q_m) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q_m,</a>
<a class="sourceLine" id="cb40-22" data-line-number="22">       <span class="st">&quot;A=QHQt&quot;</span>=<span class="st"> </span>q_m <span class="op">%*%</span><span class="st"> </span>h_m <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(q_m) )</a>
<a class="sourceLine" id="cb40-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb40-24" data-line-number="24">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb40-25" data-line-number="25">b=<span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb40-26" data-line-number="26">(<span class="dt">arnoldi =</span> <span class="kw">arnoldi_method</span>(A, b))</a></code></pre></div>
<pre><code>## $Q
##           [,1]        [,2]       [,3]
## [1,] 0.6092077 -0.65018500 -0.4540104
## [2,] 0.5076731  0.75958120 -0.4065765
## [3,] 0.6092077  0.01720066  0.7928241
## 
## $H
##           [,1]     [,2]      [,3]
## [1,] 10.123711 3.127356 1.5019617
## [2,]  1.521378 1.194132 1.2436289
## [3,]  0.000000 1.649767 0.6821563
## 
## $AQ
##          [,1]      [,2]       [,3]
## [1,] 5.178265 0.3797906 -0.2032882
## [2,] 6.295146 1.8239581  1.4297940
## [3,] 6.193612 3.2337243  1.4772279
## 
## $QH
##          [,1]      [,2]       [,3]
## [1,] 5.178265 0.3797906 -0.2032882
## [2,] 6.295146 1.8239581  1.4297940
## [3,] 6.193612 3.2337243  1.4772279
## 
## $`H=QtAQ`
##              [,1]     [,2]      [,3]
## [1,] 1.012371e+01 3.127356 1.5019617
## [2,] 1.521378e+00 1.194132 1.2436289
## [3,] 3.941292e-15 1.649767 0.6821563
## 
## $`A=QHQt`
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5</code></pre>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb42-1" data-line-number="1">(<span class="dt">eigenvalues =</span> <span class="kw">qr_method</span>(arnoldi<span class="op">$</span>H)<span class="op">$</span>eigenvalues)</a></code></pre></div>
<pre><code>## [1] 10.6771883  1.9107948 -0.5881801</code></pre>

<p>We validate the result using <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb44-1" data-line-number="1"><span class="kw">eigen</span>(A)<span class="op">$</span>values</a></code></pre></div>
<pre><code>## [1] 10.6771903  1.9109438 -0.5881341</code></pre>

<p>We can easily validate using the following equations (See also Equation <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#eq:eqnnumber7">(3.15)</a>) below:</p>
<p><span class="math display" id="eq:equate1050015">\[\begin{align}
H = Q^TAQ\ \ \ \ \ \ \ \ A=QHQ^T  \tag{3.17} 
\end{align}\]</span></p>
<p>It is notable to mention that the <strong>Arnoldi Method</strong> tends to converge towards the largest <strong>Eigenvector</strong> similar to that of the <strong>Power Method</strong> and so is ill-conditioned. To work around that, <strong>Arnoldi Method</strong> uses <strong>Gram Schmidt orthogonalization</strong>, resulting in a more stable convergence, arising in a better conditioned Hessenberg matrix. Other literature may illustrate using other orthogonalization methods in place of <strong>Gram-Schmidt</strong> orthogonalization.</p>
</div>
<div id="lanczos-method-using-gram-schmidt-in-krylov-subspace" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.7</span> Lanczos Method (using Gram-Schmidt in Krylov Subspace)<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Lanczos Method</strong> method <span class="citation">(Lanczos, C. <a href="bibliography.html#ref-ref299c">1950</a>)</span> is useful for <strong>Symmetric Positive Definite (SPD) matrices</strong> and <strong>Tridiagonal Hermitian</strong> types of matrices, including highly sparse matrices. This method is also an orthogonal projection method similar to <strong>Arnoldi Method</strong> but it handles <strong>Hermitian matrices</strong>.  </p>
<p>We illustrate the method by starting with a <strong>Tridiagonal Hermitian matrix</strong> with the (super/sub)diagonal entries denoted by <span class="math inline">\(\beta\)</span>, and the main diagonal entries denoted by <span class="math inline">\(\alpha\)</span>.</p>
<p><span class="math display">\[
A = \left[
\begin{array}{rrrrr}
\alpha_1 &amp; \beta_1 &amp; . &amp; . &amp; . \\
\beta_1 &amp; \alpha_2 &amp; \beta_2 &amp; . &amp; . \\
. &amp; \beta_2 &amp;  \alpha_3  &amp; \ddots &amp; . \\
. &amp; . &amp; \ddots &amp; \ddots &amp; \beta_{j-1} \\
. &amp; . &amp; . &amp; \beta_{j-1} &amp; \alpha_j \\
\end{array}
\right]
\]</span></p>
<p>Now recall the equation below from <strong>Arnoldi</strong> method:</p>
<p><span class="math display" id="eq:equate1050016">\[\begin{align}
AQ_m = Q_{m+1}H_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QH) \perp K  \tag{3.18} 
\end{align}\]</span></p>
<p>In <strong>Lanczos</strong> method, because we deal with <strong>SPD</strong> matrix, we have the following equation instead:</p>
<p><span class="math display" id="eq:equate1050017">\[\begin{align}
AQ_m = Q_{m}T_m \ \ \ \ \ \rightarrow \ \ \ \ \ \ (AQ - QT) \perp K \tag{3.19} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(T\)</span> is an <strong>SPD Tridiagonal</strong> matrix.</p>
<p>Because we deal with a symmetric tridiagonal matrix, <span class="math inline">\(T\)</span>, and an orthogonal matrix <span class="math inline">\(Q^TQ = I\)</span>, we can therefore use the 3-term recurrence <span class="citation">(Parlett B. N. <a href="bibliography.html#ref-ref72b">1994</a>)</span>:</p>
<p><span class="math display">\[\begin{align*}
Aq_j {}&amp;= \beta_{(j-1)}q_{(j-1)} + \alpha_{(j)} q_{(j)} + \beta_{(j)} q_{(j+1)}
\end{align*}\]</span></p>
<p>Here follows the <strong>Lanczos Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Standard Lanczos}}\\
\\
\beta_0 = 0, q_0 = 0  \leftarrow \text{initial zero } \\
b \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = b / \|b\|_{L2}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ r = Aq_j \\
\ \ \ \ \alpha_j = q_j^T r \\
\ \ \ \ r = r -  \alpha_j q_j - \beta_{(j)}q_{(j-1)} \\ 
\ \ \ \ \beta_j = \|r\|_{L2} \\
\ \ \ \ if\ (\beta_j &lt; tol)\ break \\
\ \ \ \ q_{j+1} = r / \beta_j\\
end\ loop
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Chris Paige Proposal}}\\
\\
\beta_0 = 0, q_0 = 0  \leftarrow \text{initial zero } \\
b \leftarrow \text{initial arbitrary nonzero vector} \\
q_1 = b / \|b\|_{L2}\\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ r = Aq_j - \beta_{(j-1)}q_{(j-1)}\\
\ \ \ \ \alpha_j = q_j^T r \\
\ \ \ \ r = r -  \alpha_j q_j  \\ 
\ \ \ \ \beta_j = \|r\|_{L2} \\
\ \ \ \ if\ (\beta_j &lt; tol)\ break \\
\ \ \ \ q_{j+1} = r / \beta_j\\
end\ loop
\end{array}
\right.
\]</span></p>
<p>Note that, at each iteration in the vanilla Lanczos method, the equation <span class="math inline">\(r = Aq_j\)</span> creates a new basis vector and the equation <span class="math inline">\(r = r - \alpha_j q_j - \beta_{(j)}q_{(j-1)}\)</span> subtracts the new vector from its projection. The subtraction is a remedy for the loss of orthogonality for the Lanczos Method infinite precision <span class="citation">(Hoppe T., <a href="bibliography.html#ref-ref123t">n.d.</a>; Hoppe T., <a href="bibliography.html#ref-ref131t">n.d.</a>; Qianqian Yang <a href="bibliography.html#ref-ref1127y">2019</a>)</span></p>
<p>To illustrate, let us use our naive implementation of the <strong>Lanczos Method</strong> algorithm based on Paige’s Proposal <span class="citation">(<a href="bibliography.html#ref-ref92c">1975</a>)</span>:</p>

<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb46-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb46-2" data-line-number="2">lanczos_method &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb46-3" data-line-number="3">  n      =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb46-4" data-line-number="4">  m     =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb46-5" data-line-number="5">  q     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb46-6" data-line-number="6">  beta  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb46-7" data-line-number="7">  alpha =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb46-8" data-line-number="8">  b     =<span class="st"> </span><span class="kw">rnorm</span>(n)  <span class="co"># initialize randomly</span></a>
<a class="sourceLine" id="cb46-9" data-line-number="9">  q[,<span class="dv">1</span>] =<span class="st"> </span>b <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(b<span class="op">^</span><span class="dv">2</span>)) <span class="co"># normalize</span></a>
<a class="sourceLine" id="cb46-10" data-line-number="10">  tol   =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb46-11" data-line-number="11">  prev.beta =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb46-12" data-line-number="12">  prev.q =<span class="st"> </span>q[,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb46-13" data-line-number="13">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n)) {</a>
<a class="sourceLine" id="cb46-14" data-line-number="14">    r         =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q[,j] <span class="op">-</span><span class="st"> </span>prev.beta <span class="op">*</span><span class="st"> </span>prev.q</a>
<a class="sourceLine" id="cb46-15" data-line-number="15">    alpha[j]  =<span class="st"> </span><span class="kw">t</span>(q[,j]) <span class="op">%*%</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb46-16" data-line-number="16">    r =<span class="st"> </span>r <span class="op">-</span><span class="st"> </span>alpha[j] <span class="op">*</span><span class="st"> </span>q[,j]  <span class="co"># subtract from its projection</span></a>
<a class="sourceLine" id="cb46-17" data-line-number="17">    beta[j]   =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(r<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb46-18" data-line-number="18">    <span class="cf">if</span> (j <span class="op">&gt;=</span>n <span class="op">||</span><span class="st"> </span><span class="kw">abs</span>(beta[j]) <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb46-19" data-line-number="19">    q[,j<span class="op">+</span><span class="dv">1</span>]   =<span class="st"> </span>r <span class="op">/</span><span class="st"> </span>beta[j]   <span class="co"># then scale</span></a>
<a class="sourceLine" id="cb46-20" data-line-number="20">    prev.beta =<span class="st"> </span>beta[j]</a>
<a class="sourceLine" id="cb46-21" data-line-number="21">    prev.q    =<span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb46-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb46-23" data-line-number="23">  T =<span class="st"> </span><span class="kw">t</span>(q) <span class="op">%*%</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>q</a>
<a class="sourceLine" id="cb46-24" data-line-number="24">  T[<span class="kw">which</span>(<span class="kw">abs</span>(T) <span class="op">&lt;</span><span class="st"> </span>tol) ] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb46-25" data-line-number="25">  <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=<span class="st"> </span>A , <span class="st">&quot;T&quot;</span>=<span class="st"> </span>T, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q , <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>beta, <span class="st">&quot;alpha&quot;</span> =<span class="st"> </span>alpha )</a>
<a class="sourceLine" id="cb46-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb46-27" data-line-number="27">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>, <span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">5</span>,<span class="dv">5</span>),<span class="dv">5</span>,</a>
<a class="sourceLine" id="cb46-28" data-line-number="28">           <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb46-29" data-line-number="29">(<span class="dt">lanczos =</span> <span class="kw">lanczos_method</span>(A))</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    2    0    0    0
## [2,]    2    3    3    0    0
## [3,]    0    3    3    4    0
## [4,]    0    0    4    4    5
## [5,]    0    0    0    5    5
## 
## $T
##          [,1]     [,2]     [,3]      [,4]     [,5]
## [1,] 1.726379 5.939271 0.000000 0.0000000 0.000000
## [2,] 5.939271 5.223253 2.542124 0.0000000 0.000000
## [3,] 0.000000 2.542124 6.710107 1.2006169 0.000000
## [4,] 0.000000 0.000000 1.200617 0.8125495 1.296137
## [5,] 0.000000 0.000000 0.000000 1.2961366 1.527711
## 
## $Q
##             [,1]        [,2]       [,3]        [,4]        [,5]
## [1,] -0.32230088  0.07123367  0.3943700 -0.85660737 -0.04176164
## [2,]  0.09448167 -0.30542846  0.7022612  0.28996500 -0.56615912
## [3,] -0.42991811  0.50829179  0.4628082  0.39608594  0.43096916
## [4,]  0.82074751  0.16736559  0.3444932 -0.15551579  0.39433690
## [5,]  0.16952670  0.78438985 -0.1357740 -0.03278521 -0.58007225
## 
## $beta
## [1] 5.939271e+00 2.542124e+00 1.200617e+00 1.296137e+00 4.913656e-14
## 
## $alpha
## [1] 1.7263794 5.2232530 6.7101074 0.8125495 1.5277108</code></pre>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" data-line-number="1">(<span class="dt">eigenvalues =</span> <span class="kw">qr_method</span>(lanczos<span class="op">$</span>T)<span class="op">$</span>eigenvalues)</a></code></pre></div>
<pre><code>## [1] 10.7729509  6.1028726  2.9759549  2.3984001  0.2983397</code></pre>

<p>We validate the result (in decreasing order using) <strong>eigen(.)</strong> function like so:</p>

<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" data-line-number="1"><span class="kw">sort</span>(<span class="kw">abs</span>(<span class="kw">eigen</span>(lanczos<span class="op">$</span>T)<span class="op">$</span>values), <span class="dt">decreasing=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 10.7729806  6.1028984  2.9759370  2.3983977  0.2983397</code></pre>

<p>We can easily use <span class="math inline">\(Q\)</span> to construct a tridiagonal symmetric matrix:</p>
<p><span class="math display" id="eq:equate1050018">\[\begin{align}
T = Q^TAQ \tag{3.20} 
\end{align}\]</span></p>
<p>We can derive a tridiagonal symmetric matrix, <span class="math inline">\(T = Q^TAQ\)</span>, from <span class="math inline">\(Q\)</span> to then find the approximate <strong>Eigenvalues</strong>, and <strong>Eigenvectors</strong> - the <strong>Ritz</strong> pair.</p>
<p>Like <strong>Arnoldi Method</strong>, we use <strong>QR Method</strong> to generate the <strong>Eigenvalues</strong> but this time using the <strong>tridiagonal symmetric matrix</strong> derived from <strong>Lanczos Method</strong>.</p>
<p>Given <span class="math inline">\(Q = (q_1, q_2, ..., q_j)\)</span>, <strong>Laczos basis</strong>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix derived from <strong>Lanczos Method</strong>, we can then perform the following equations.</p>
<p><span class="math display" id="eq:equate1050019">\[\begin{align}
Av = \lambda v\ \rightarrow\ \ \ \ Q^TAQv = \lambda Q^TQv \rightarrow\ \ \ \ Tv = \lambda v \tag{3.21} 
\end{align}\]</span></p>
<p>Improvements of the <strong>Lanczos</strong> method have evolved through the years. We leave readers to investigate <strong>Lanczos algorithm for SVD</strong> and <strong>Randomized Block Lanczos</strong> <span class="citation">(Yuan Q. et al <a href="bibliography.html#ref-ref71y">2018</a>)</span>.</p>
<p>Other recent methods such as <strong>Jacobi-Davidson Method </strong> uses <strong>Ritz-Galerkin</strong> procedure instead of <strong>Krylov subspace</strong>. And like <strong>Power Method</strong>, the <strong>Jacobi-Davidson</strong> method converges to the largest <strong>Eigenvalue</strong>.</p>
</div>
<div id="fine-tuning-of-iteration-and-convergence" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.2.8</span> Fine-Tuning of Iteration and Convergence<a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we close the topic around iteration and convergence for <strong>Eigenvalue problems</strong>, it is notable, for further reading, to introduce procedures that are essential for a more stable and faster convergence.</p>
<ul>
<li><strong>Restarting</strong></li>
</ul>
<p>Restarting is simply just restarting the iteration in the middle of it after a certain threshold - usually when we reach a predetermined set of vectors. This allows us to use a new set of initial vectors at restart.</p>
<ul>
<li><strong>Shifting</strong></li>
</ul>
<p>Shifts are commonly seen in <strong>Iteration</strong> methods in the form of:</p>
<p><span class="math display" id="eq:equate1050020">\[\begin{align}
(A - \alpha I) \tag{3.22} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> shifts a matrix. We use shift in <strong>Eigenvalue problems</strong> to get an approximate <strong>Eigenvalue</strong> much closer to one actual <strong>Eigenvalue</strong> than to another in the case in which a matrix has multiple (interior) <strong>Eigenvalues</strong>. While we have not covered <strong>shifting</strong>, the methods we previously discussed can implement shifting, e.g. <strong>Inverse Power Method with shift</strong>, <strong>QR Method with shift</strong>, etc.</p>
<ul>
<li><strong>Preconditioning</strong> </li>
</ul>
<p><strong>Preconditioning</strong> transforms a system of equations to a new form as a way to reduce or eliminate the system from being ill-conditioned. A common equation with <strong>preconditioner</strong> is applied on systems of equations where <span class="math inline">\(M^{-1}\)</span> denotes a <strong>preconditioner</strong>:</p>
<p><span class="math display" id="eq:equate1050021">\[\begin{align}
Ax = b \rightarrow M^{-1}Ax = M^{-1}b  \tag{3.23} 
\end{align}\]</span></p>
<p>There are three ways to use preconditioners:</p>
<ul>
<li>Left preconditioning</li>
</ul>
<p><span class="math display" id="eq:equate1050022">\[\begin{align}
Ax = b \rightarrow M^{-1}Ax = M^{-1}b  \tag{3.24} 
\end{align}\]</span></p>
<ul>
<li>Right preconditioning</li>
</ul>
<p><span class="math display" id="eq:equate1050023">\[\begin{align}
Ax = b \rightarrow AM^{-1}u = b \ \ \  where\ u = Mx \tag{3.25} 
\end{align}\]</span></p>
<ul>
<li>Split or Symmetric preconditioning</li>
</ul>
<p><span class="math display" id="eq:equate1050024">\[\begin{align}
Ax = b \rightarrow M^{-1}AM^{-1}u = M^{-1}b  \ \ \ where\ u = Mx  \tag{3.26} 
\end{align}\]</span></p>
<p>A simple preconditioner is in the form of diagonal of a matrix:</p>
<p>e.g.</p>
<p><span class="math display" id="eq:equate1050025">\[\begin{align}
M^{-1} = diag(A) \tag{3.27} 
\end{align}\]</span></p>
<p>We leave readers to investigate other preconditioners.</p>
<p>We also leave readers to investigate <strong>Deflation</strong> and <strong>Augmentation</strong>, which are techniques used to optimize convergence (in the context of <strong>Krylov subspace</strong>). For <strong>Deflation</strong>, we have <strong>Hotelling’s Deflation</strong> and <strong>Wielandt deflation</strong> to be familiar.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3.1-iteration-and-convergence.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3.3-approximating-root-and-fixed-point-by-iteration.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
