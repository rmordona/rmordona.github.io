<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>12.4 Convolutional Neural Network (CNN)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="12.4 Convolutional Neural Network (CNN)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="12.4 Convolutional Neural Network (CNN)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="12.3-multi-layer-perceptron-mlp.html"/>
<link rel="next" href="13-deeplearning2.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="15-appendix.html"><a href="15-appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="15.1-appendix-a.html"><a href="15.1-appendix-a.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="15.2-appendix-b.html"><a href="15.2-appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="15.3-appendix-c.html"><a href="15.3-appendix-c.html"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="15.4-appendix-d.html"><a href="15.4-appendix-d.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="convolutional-neural-network-cnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">12.4</span> Convolutional Neural Network (CNN)  <a href="12.4-convolutional-neural-network-cnn.html#convolutional-neural-network-cnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the 1980s, Yann Lecun laid out the underpinnings of what is known today as the <strong>Convolutional Neural Network (CNN)</strong> used for <strong>Image Recognition and Classification</strong>.</p>
<div id="computer-graphics" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.1</span> Computer Graphics<a href="12.4-convolutional-neural-network-cnn.html#computer-graphics" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The main goal of working on <strong>CNN</strong> is to process and classify images. However, to classify images, we need to know first the type of computer graphics used.</p>
<p>There are essentially two types of computer graphics: <strong>vector graphics</strong> and <strong>raster graphics</strong>:</p>
<p><strong>Raster graphics</strong> are composed of pixels (whether greyscaled or colored). For example, the left image in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:raster">12.22</a> represents the lower right portion of a circle projected to show the individual pixels.</p>
<p><strong>Vector graphics</strong> are rendered based on vertices and paths. For example, the right image in the figure shows a rectangle with four vertices (points) and four paths (lines) connecting the vertices.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:raster"></span>
<img src="raster.png" alt="Raster and Vector Graphics" width="70%" />
<p class="caption">
Figure 12.22: Raster and Vector Graphics
</p>
</div>
<p>For our purposes, we use a rasterized image in the format of <strong>jpeg</strong> (<strong>png</strong> and <strong>gif</strong> are as equally popular). Here, we use image operations made available in <strong>R</strong> by third-party libraries such as <strong>jpeg</strong>. Below is the code to read an image. See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange">12.23</a> for the actual image.</p>

<div class="sourceCode" id="cb1981"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1981-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;jpeg&quot;</span>)</a>
<a class="sourceLine" id="cb1981-2" data-line-number="2">image      =<span class="st"> </span>jpeg<span class="op">::</span><span class="kw">readJPEG</span>(<span class="dt">source =</span> <span class="st">&quot;appleorange.jpg&quot;</span>)</a>
<a class="sourceLine" id="cb1981-3" data-line-number="3">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1981-4" data-line-number="4">draw.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">main=</span><span class="st">&quot;Apple, Oranges, and Banana&quot;</span> ) {</a>
<a class="sourceLine" id="cb1981-5" data-line-number="5">  di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1981-6" data-line-number="6">  img =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(di[<span class="dv">1</span>], di[<span class="dv">2</span>], di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb1981-7" data-line-number="7">  sz =<span class="st"> </span><span class="kw">ncol</span>(img)</a>
<a class="sourceLine" id="cb1981-8" data-line-number="8">  <span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>) </a>
<a class="sourceLine" id="cb1981-9" data-line-number="9">  <span class="kw">plot</span>(<span class="ot">NA</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), </a>
<a class="sourceLine" id="cb1981-10" data-line-number="10">     <span class="dt">xlab=</span><span class="st">&quot;Image Width&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Image Height&quot;</span>,  <span class="dt">main=</span>main)</a>
<a class="sourceLine" id="cb1981-11" data-line-number="11">  <span class="kw">rasterImage</span>(img,<span class="dt">xleft=</span><span class="dv">1</span>, <span class="dt">ybottom=</span>sz, <span class="dt">xright=</span>sz, <span class="dt">ytop=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1981-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb1981-13" data-line-number="13"><span class="kw">draw.image</span>(image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange"></span>
<img src="plotappleorange.png" alt="Apple, Oranges, and Banana" width="50%" />
<p class="caption">
Figure 12.23: Apple, Oranges, and Banana
</p>
</div>

<p>We use the <strong>rasterImage(.)</strong> function to project the image to a pixelized format - this is also called <strong>rasterization</strong>.</p>
<p>While our scope does not cover <strong>image processing</strong>, it helps to leave readers to investigate <strong>Graphics Pipelines</strong>, which is covered in <strong>Data Visualization</strong> courses. In addition, the topic touches on <strong>vertex processing</strong> such as <strong>tesselation</strong>, <strong>transformation</strong>, <strong>clipping</strong>, <strong>thresholding</strong>, and <strong>filtering or masking</strong>, which are mostly the processes we may use for illustration purposes when we get to <strong>Kernels</strong>.</p>
<p>Additionally, we know that <strong>3D Games</strong> perform heavy <strong>image processing</strong> and thus rely heavily on <strong>Graphics Processing Unit (GPU)</strong> for faster processing. Similarly, some, if not most, <strong>CNN</strong> standard packages are already built to have <strong>GPU</strong> support.</p>
<p>Part of <strong>Computer Graphics</strong> also touches on <strong>Object Segmentation</strong>, <strong>Object Detection or Localization</strong>, and <strong>Object Selection</strong>, which are essential when dealing with <strong>Autonomous Vehicles</strong> in a more dynamic time-series fashion (which we cover in the <strong>RNN</strong> section). In the same line of thinking, recall our discussion on <strong>Lidar Sensors</strong> and <strong>Kalman Filters</strong> in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>) for dynamic systems.</p>
<p>Now, going back to our image of a handful of fruits, to manipulate the image, we first have to note that the image is stored in a 3D tensor form with a 500x500x3 dimension. It can easily be shown like so:</p>

<div class="sourceCode" id="cb1982"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1982-1" data-line-number="1"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 500 500   3</code></pre>

<p>The three dimensions correspondingly represent the height, width, and depth of the image. The depth in a <strong>JPEG</strong> image also represents the three channels of the RGB color scheme. Each element in the tensor represents the color intensity of an RGB color scheme which is normalized by a max value of 255. For example, we can get the hexadecimal equivalent of red color like so:</p>

<div class="sourceCode" id="cb1984"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1984-1" data-line-number="1"><span class="kw">rgb</span>(<span class="dv">255</span><span class="op">/</span><span class="dv">255</span>, <span class="dv">0</span>, <span class="dv">0</span>)</a></code></pre></div>
<pre><code>## [1] &quot;#FF0000&quot;</code></pre>

<p>To change the color of the image in order to show only the red color scheme, we can manipulate the tensor like so:</p>

<div class="sourceCode" id="cb1986"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1986-1" data-line-number="1">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">3</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># red</span></a>
<a class="sourceLine" id="cb1986-2" data-line-number="2"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<p>We can apply the same changes to isolate the green and blue color schemes of the image.</p>

<div class="sourceCode" id="cb1987"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1987-1" data-line-number="1">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># green</span></a>
<a class="sourceLine" id="cb1987-2" data-line-number="2"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1987-3" data-line-number="3"></a>
<a class="sourceLine" id="cb1987-4" data-line-number="4">image.copy =<span class="st"> </span>image; image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)] =<span class="st"> </span><span class="dv">0</span>  <span class="co"># blue</span></a>
<a class="sourceLine" id="cb1987-5" data-line-number="5"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<p>Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange1">12.24</a> gives us the original image and the three images in separate RGB color schemes.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange1"></span>
<img src="appleorange1.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.24: Apple, Oranges, and Banana
</p>
</div>
<p>Equivalently, Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange2">12.25</a> gives us an enhanced version of the original image and three images in separate grayscale color schemes using the following tricks:</p>

<div class="sourceCode" id="cb1988"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1988-1" data-line-number="1">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-2" data-line-number="2">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span><span class="kw">ifelse</span>(image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="dv">0</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1988-3" data-line-number="3"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-4" data-line-number="4"></a>
<a class="sourceLine" id="cb1988-5" data-line-number="5">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-6" data-line-number="6">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-7" data-line-number="7"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1988-9" data-line-number="9">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-10" data-line-number="10">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">2</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-11" data-line-number="11"><span class="kw">draw.image</span>(image.copy)</a>
<a class="sourceLine" id="cb1988-12" data-line-number="12"></a>
<a class="sourceLine" id="cb1988-13" data-line-number="13">image.copy =<span class="st"> </span>image</a>
<a class="sourceLine" id="cb1988-14" data-line-number="14">image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,] =<span class="st"> </span>image.copy [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">3</span>]  <span class="op">*</span><span class="st"> </span>( <span class="dv">250</span> <span class="op">/</span><span class="st"> </span><span class="dv">255</span> )</a>
<a class="sourceLine" id="cb1988-15" data-line-number="15"><span class="kw">draw.image</span>(image.copy)</a></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange2"></span>
<img src="appleorange2.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.25: Apple, Oranges, and Banana
</p>
</div>
<p>Given the ability to manipulate the image tensor by hand, there are other more creative ways to apply effects to the image. In the context of <strong>CNN</strong>, we use <strong>Filters</strong> to manipulate images. The purpose is to <strong>highlight</strong> image properties that are readily identifiable and classifiable. This is where we discuss <strong>Convolution</strong>.</p>
</div>
<div id="convolution" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.2</span> Convolution <a href="12.4-convolutional-neural-network-cnn.html#convolution" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>convolutional</strong> nature of <strong>CNN</strong> comes with the fact that <strong>CNN</strong> assumes an input image, performs an <strong>element-wise</strong> multiplication with a <strong>filter</strong>, then sums the product to produce a <strong>feature map</strong>. We demonstrate the operation using Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:convolution">12.26</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolution"></span>
<img src="convolution.png" alt="Convolution" width="90%" />
<p class="caption">
Figure 12.26: Convolution
</p>
</div>
<p>The idea is to use a <strong>Filter</strong>, also called <strong>Kernel</strong>, which is a 2D matrix (in the case of Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:convolution">12.26</a>). We slide the Filter over the input image in the direction from left to right and top to bottom, one step at a time. A <strong>step</strong> is called a <strong>stride</strong>, which dictates the pace or number of <strong>pixels</strong> to skip horizontally and vertically. As the <strong>Kernel</strong> lands on a <strong>patch</strong> which has the same dimension as the Kernel, we perform an <strong>element-wise</strong> multiplication and then sum the product. For example, the first patch (2x2) consists of the following pixels (1,2,5,6). We then multiply the patch with the Kernel like so:</p>
<p><span class="math display">\[
1 \times 1 + 2\times 0 + 5 \times 0 + 6 \times 1  = 7
\]</span></p>
<p>A <strong>Patch</strong> may also reference the terms <strong>receptive field</strong>, <strong>image patch</strong>, and <strong>convolution patch</strong>. The next patch (2x2), after 1st stride, consists of the following pixels (5,6,9,10). We then multiply the patch with the kernel like so:   </p>
<p><span class="math display">\[
5 \times 1 + 6 \times 0 + 9 \times 0 + 10 \times 1  = 15
\]</span></p>
<p>We perform the same operation for subsequent patches until we form a new matrix called <strong>Feature Map</strong>, also called <strong>Convolved Feature</strong>. </p>
<p>We can repeat the process by performing the same steps, yielding a new <strong>Feature Map</strong>. For example, in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:convolved">12.27</a>, our <strong>Feature Map</strong> becomes an input and goes through another round of convolution using a smaller filter (2x2) with one stride.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolved"></span>
<img src="convolved.png" alt="Convolution (Repeated)" width="70%" />
<p class="caption">
Figure 12.27: Convolution (Repeated)
</p>
</div>
<p>It is also possible to perform convolution, resulting in a new <strong>Feature Map</strong> with the same dimension as the original Input Image. To achieve this, where the kernel crosses over the edges of an image, we use <strong>padding</strong> with zero values. See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:convolved1">12.28</a>. The operation uses one padding and one stride. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolved1"></span>
<img src="convolved1.png" alt="Repeated Convolution with Padding" width="70%" />
<p class="caption">
Figure 12.28: Repeated Convolution with Padding
</p>
</div>
<p>Convolution is expressed using the following general formulation (with padding and a stride of one):</p>
<p><span class="math display" id="eq:equate1140164">\[\begin{align}
M_{(\text{feature.map})} = \sum_d^D \sum_{(i=-p)}^{(H+p)} \sum_{(j=-p)}^{(W+p)} I(h_s: h_e, w_s : w_e, d) * K(,,d) \tag{12.174} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>H</strong>, <strong>,W</strong>, <strong>,D</strong> are the dimensions of an image (e.g. height, width, depth) ,</li>
<li><strong>k</strong> is the size of the kernel,</li>
<li><strong>I</strong> is the input image with dim. (<span class="math inline">\(H \times W \times D\)</span>),</li>
<li><strong>K</strong> is the kernel with dim. (<span class="math inline">\(kr \times kc \times D\)</span>),</li>
<li><strong>p</strong> is the padding,</li>
<li><strong>s</strong> is the stride.</li>
</ul>
<p>For the rest of <strong>CNN</strong> discussions, we assume <strong>symmetric or equal</strong> paddings along the edges if required.</p>
<p>The dimension (<span class="math inline">\(O \times O\)</span>) of the <strong>feature map</strong> has the <strong>O</strong> size calculated as such <span class="citation">(Dumoulin V., Visin F. <a href="bibliography.html#ref-ref1144v">2018</a>)</span>:</p>
<p><span class="math display" id="eq:equate1140165">\[\begin{align}
O = (W - K + 2p) / s + 1 \tag{12.175} 
\end{align}\]</span></p>
<p>For our purposes, especially if implemented in <strong>R</strong>, we assume one (and not zero) as starting index. Therefore, to construct the <strong>receptive field</strong>, we use the following pseudocode and formulation:</p>
<p><span class="math display" id="eq:equate1140166">\[\begin{align}
\{\ h_i = (s + 1) \times i \ \}_{i=1}^{(H+2p)} \ \ \ \ \ \ \ \ \  \{\ w_j = (s + 1) \times j \ \}_{j=1}^{(W+2p)} \tag{12.176} 
\end{align}\]</span></p>
<p><span class="math display">\[
\begin{array}{ll}
\text{for i in }\{1, ..., O\}: \\
\ \ \ \ \ \text{for j in }\{1, ..., O\}: \\
\ \ \ \ \ \ \ \ \ \  h_s=h_i; \ \ \ \ he = (hs + r - 1)\\
\ \ \ \ \ \ \ \ \ \ w_s=w_j;\ \ \ \ we = (ws + r - 1)  \\ 
\ \ \ \ \ \ \ \ \ \ ...\  I[h_s: h_e, w_s, w_e]\ ...
\end{array}
\]</span></p>
<p>The <strong>receptive field</strong> is bounded by the four hyperparameters, namely <span class="math inline">\(\mathbf{h_s}\)</span>, <span class="math inline">\(\mathbf{h_e}\)</span>, <span class="math inline">\(\mathbf{w_s}\)</span>, and <span class="math inline">\(\mathbf{w_e}\)</span> which respectively represent the starting and ending indices of its height and width.</p>
<p>Finally, let us review our example implementation of <strong>Convolution</strong>. Our implementation allows the ability to dilate both the image and the kernel and cache the indices and size that form the receptive fields. This capability is essential during backpropagation in the <strong>Pooling Layer</strong>, discussed later.</p>

<div class="sourceCode" id="cb1989"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1989-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1989-2" data-line-number="2">convolution &lt;-<span class="st"> </span><span class="cf">function</span>(images, filter, </a>
<a class="sourceLine" id="cb1989-3" data-line-number="3">                 <span class="dt">cur.fmaps =</span> <span class="ot">NULL</span>, <span class="dt">dw.kernel =</span> <span class="ot">NULL</span>, <span class="dt">pw.kernel =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1989-4" data-line-number="4">                 <span class="dt">Dout=</span><span class="ot">NULL</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding =</span> <span class="dv">0</span>, <span class="dt">dil_rate=</span><span class="dv">0</span>, <span class="dt">dil_input=</span><span class="dv">0</span>, </a>
<a class="sourceLine" id="cb1989-5" data-line-number="5">                 <span class="dt">autopad =</span> <span class="ot">NULL</span>, <span class="dt">auto.pad=</span><span class="dv">0</span>, <span class="dt">normalize=</span><span class="ot">FALSE</span>, <span class="dt">clipping=</span><span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb1989-6" data-line-number="6">                 <span class="dt">afunc=</span><span class="ot">NULL</span>, <span class="dt">pool.cache =</span> <span class="ot">NULL</span>, <span class="dt">pool =</span> <span class="ot">NULL</span>, <span class="dt">bias =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1989-7" data-line-number="7">                 <span class="dt">dw.bias =</span> <span class="ot">NULL</span>, <span class="dt">pw.bias =</span> <span class="ot">NULL</span>, <span class="dt">ptype=</span><span class="st">&quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-8" data-line-number="8">  gaps      =<span class="st"> </span>dil_input</a>
<a class="sourceLine" id="cb1989-9" data-line-number="9">  d         =<span class="st"> </span><span class="kw">dim</span>(images)</a>
<a class="sourceLine" id="cb1989-10" data-line-number="10">  img.h     =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1989-11" data-line-number="11">  padded    =<span class="st"> </span><span class="kw">pad</span>(<span class="kw">dilate</span>(images, dil_input), filter, padding)</a>
<a class="sourceLine" id="cb1989-12" data-line-number="12">  image     =<span class="st"> </span>padded<span class="op">$</span>image</a>
<a class="sourceLine" id="cb1989-13" data-line-number="13">  padding   =<span class="st"> </span>padded<span class="op">$</span>padding</a>
<a class="sourceLine" id="cb1989-14" data-line-number="14">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(autopad)) {</a>
<a class="sourceLine" id="cb1989-15" data-line-number="15">      extra     =<span class="st"> </span><span class="kw">autopad</span>(image, filter, padding, stride, <span class="dt">edge=</span>autopad,</a>
<a class="sourceLine" id="cb1989-16" data-line-number="16">                          auto.pad)</a>
<a class="sourceLine" id="cb1989-17" data-line-number="17">      image     =<span class="st"> </span>extra<span class="op">$</span>image</a>
<a class="sourceLine" id="cb1989-18" data-line-number="18">      auto.pad  =<span class="st"> </span>extra<span class="op">$</span>pad</a>
<a class="sourceLine" id="cb1989-19" data-line-number="19">  } </a>
<a class="sourceLine" id="cb1989-20" data-line-number="20">  di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1989-21" data-line-number="21">  new.h =<span class="st"> </span>di[<span class="dv">1</span>]; new.w =<span class="st"> </span>di[<span class="dv">2</span>]; new.d =<span class="st"> </span>di[<span class="dv">3</span>]; new.s =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1989-22" data-line-number="22">  K         =<span class="st"> </span><span class="kw">dilate</span>(filter, dil_rate)</a>
<a class="sourceLine" id="cb1989-23" data-line-number="23">  r         =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb1989-24" data-line-number="24">  c         =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb1989-25" data-line-number="25">  bkprop.cache =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1989-26" data-line-number="26">  O =<span class="st">   </span><span class="kw">floor</span>( (new.w <span class="op">-</span><span class="st"> </span>c) <span class="op">/</span><span class="st"> </span>stride <span class="op">+</span><span class="st"> </span><span class="dv">1</span> ) </a>
<a class="sourceLine" id="cb1989-27" data-line-number="27">  <span class="cf">if</span> (new.h <span class="op">&lt;=</span><span class="st"> </span>r) <span class="kw">error</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1989-28" data-line-number="28">  <span class="cf">if</span> (O <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(<span class="ot">NULL</span>)}</a>
<a class="sourceLine" id="cb1989-29" data-line-number="29">  h =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>new.h, <span class="dt">by=</span>stride)</a>
<a class="sourceLine" id="cb1989-30" data-line-number="30">  w =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>new.w, <span class="dt">by=</span>stride)</a>
<a class="sourceLine" id="cb1989-31" data-line-number="31">  I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-32" data-line-number="32">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1989-33" data-line-number="33">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span> <span class="op">||</span><span class="st"> </span>ptype <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) </a>
<a class="sourceLine" id="cb1989-34" data-line-number="34">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-35" data-line-number="35">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-36" data-line-number="36">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1989-37" data-line-number="37">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1989-38" data-line-number="38">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1989-39" data-line-number="39">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r, c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb1989-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb1989-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb1989-42" data-line-number="42">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-43" data-line-number="43">      feature.map =<span class="st"> </span><span class="kw">convolve.image</span>(image, dw.kernel, pw.kernel, </a>
<a class="sourceLine" id="cb1989-44" data-line-number="44">                                   dw.bias, pw.bias, bias, dil_rate, </a>
<a class="sourceLine" id="cb1989-45" data-line-number="45">                                   O, h, w, r, c)</a>
<a class="sourceLine" id="cb1989-46" data-line-number="46">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-47" data-line-number="47">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.I.wrt.K&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-48" data-line-number="48">      feature.map =<span class="st"> </span><span class="kw">gradient.I.wrt.K</span>(image, K, O, h, w, r, c)</a>
<a class="sourceLine" id="cb1989-49" data-line-number="49">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-50" data-line-number="50">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.K.wrt.I&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-51" data-line-number="51">      feature.map =<span class="st"> </span><span class="kw">gradient.K.wrt.I</span>(image, Dout, cur.fmaps, pw.kernel, </a>
<a class="sourceLine" id="cb1989-52" data-line-number="52">                                     dil_rate, O, h, w)</a>
<a class="sourceLine" id="cb1989-53" data-line-number="53">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-54" data-line-number="54">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;gradient.I.wrt.P&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-55" data-line-number="55">      feature.map =<span class="st"> </span><span class="kw">gradient.I.wrt.P</span>(image, Dout, K, O, pool, h, w,</a>
<a class="sourceLine" id="cb1989-56" data-line-number="56">                                     pool.cache)</a>
<a class="sourceLine" id="cb1989-57" data-line-number="57">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-58" data-line-number="58">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-59" data-line-number="59">      di =<span class="st"> </span><span class="kw">dim</span>(I[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1989-60" data-line-number="60">      img.s   =<span class="st"> </span>di[<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb1989-61" data-line-number="61">      im2col  =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],  di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb1989-62" data-line-number="62">      im2max  =<span class="st"> </span><span class="kw">apply</span>(im2col, <span class="dv">2</span>, max)</a>
<a class="sourceLine" id="cb1989-63" data-line-number="63">      pool.cache =<span class="st"> </span><span class="kw">apply</span>(im2col, <span class="dv">2</span>, which.max)</a>
<a class="sourceLine" id="cb1989-64" data-line-number="64">      im2col  =<span class="st"> </span><span class="kw">array</span>( im2max, <span class="kw">c</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s, O, O))</a>
<a class="sourceLine" id="cb1989-65" data-line-number="65">      max.channel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-66" data-line-number="66">      <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s)) {</a>
<a class="sourceLine" id="cb1989-67" data-line-number="67">          max.channel[[d]] =<span class="st"> </span><span class="kw">t</span>(im2col[d,,])</a>
<a class="sourceLine" id="cb1989-68" data-line-number="68">      }</a>
<a class="sourceLine" id="cb1989-69" data-line-number="69">      max.channel =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(max.channel), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], img.s))</a>
<a class="sourceLine" id="cb1989-70" data-line-number="70">      feature.map =<span class="st"> </span>max.channel</a>
<a class="sourceLine" id="cb1989-71" data-line-number="71">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1989-72" data-line-number="72">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-73" data-line-number="73">      di      =<span class="st"> </span><span class="kw">dim</span>(I[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb1989-74" data-line-number="74">      img.s   =<span class="st"> </span>di[<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb1989-75" data-line-number="75">      im2col  =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],  di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb1989-76" data-line-number="76">      im2mean =<span class="st"> </span>pool.cache =<span class="st"> </span><span class="kw">colMeans</span>(im2col)</a>
<a class="sourceLine" id="cb1989-77" data-line-number="77">      im2col  =<span class="st"> </span><span class="kw">array</span>( im2mean, <span class="kw">c</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s, O, O))</a>
<a class="sourceLine" id="cb1989-78" data-line-number="78">      mean.channel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1989-79" data-line-number="79">      <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>img.s)) {</a>
<a class="sourceLine" id="cb1989-80" data-line-number="80">          mean.channel[[d]] =<span class="st"> </span><span class="kw">t</span>(im2col[d,,])</a>
<a class="sourceLine" id="cb1989-81" data-line-number="81">      }</a>
<a class="sourceLine" id="cb1989-82" data-line-number="82">      mean.channel =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(mean.channel), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], img.s))</a>
<a class="sourceLine" id="cb1989-83" data-line-number="83">      feature.map =<span class="st"> </span>mean.channel</a>
<a class="sourceLine" id="cb1989-84" data-line-number="84">  } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1989-85" data-line-number="85">  <span class="cf">if</span> (ptype <span class="op">==</span><span class="st"> &quot;mask&quot;</span>) {</a>
<a class="sourceLine" id="cb1989-86" data-line-number="86">      img.copy =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, di)</a>
<a class="sourceLine" id="cb1989-87" data-line-number="87">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-88" data-line-number="88">        <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb1989-89" data-line-number="89">            hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1989-90" data-line-number="90">            ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1989-91" data-line-number="91">            img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] =<span class="st"> </span></a>
<a class="sourceLine" id="cb1989-92" data-line-number="92"><span class="st">               </span><span class="kw">sum</span>( <span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r, c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) <span class="op">*</span><span class="st"> </span>K)</a>
<a class="sourceLine" id="cb1989-93" data-line-number="93">        }</a>
<a class="sourceLine" id="cb1989-94" data-line-number="94">      }</a>
<a class="sourceLine" id="cb1989-95" data-line-number="95">      feature.map =<span class="st"> </span>img.copy</a>
<a class="sourceLine" id="cb1989-96" data-line-number="96">  } </a>
<a class="sourceLine" id="cb1989-97" data-line-number="97">  <span class="cf">if</span> (normalize<span class="op">==</span><span class="ot">TRUE</span>) {  </a>
<a class="sourceLine" id="cb1989-98" data-line-number="98">     mx =<span class="st"> </span><span class="kw">max</span>(feature.map)</a>
<a class="sourceLine" id="cb1989-99" data-line-number="99">     mn =<span class="st"> </span><span class="kw">min</span>(feature.map)</a>
<a class="sourceLine" id="cb1989-100" data-line-number="100">     feature.map =<span class="st"> </span>(feature.map <span class="op">-</span><span class="st"> </span>mn) <span class="op">/</span><span class="st"> </span>(mx <span class="op">-</span><span class="st"> </span>mn)   </a>
<a class="sourceLine" id="cb1989-101" data-line-number="101">  } </a>
<a class="sourceLine" id="cb1989-102" data-line-number="102">  <span class="cf">if</span> (clipping<span class="op">==</span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb1989-103" data-line-number="103">     feature.map =<span class="st">  </span><span class="kw">pmax</span>(<span class="kw">pmin</span>( feature.map, <span class="dv">1</span>), <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1989-104" data-line-number="104">  }</a>
<a class="sourceLine" id="cb1989-105" data-line-number="105">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(afunc)) {</a>
<a class="sourceLine" id="cb1989-106" data-line-number="106">     feature.map =<span class="st"> </span><span class="kw">activation</span>(feature.map, <span class="kw">get</span>(afunc))</a>
<a class="sourceLine" id="cb1989-107" data-line-number="107">  }</a>
<a class="sourceLine" id="cb1989-108" data-line-number="108">  bkprop.cache =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;stride&quot;</span>   =<span class="st"> </span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1989-109" data-line-number="109">      <span class="st">&quot;padding&quot;</span>    =<span class="st"> </span><span class="kw">max</span>(r <span class="op">-</span><span class="st"> </span>(padding <span class="op">+</span><span class="st"> </span>auto.pad <span class="op">+</span><span class="st"> </span><span class="dv">1</span>), <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1989-110" data-line-number="110">      <span class="st">&quot;dil_rate&quot;</span>   =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;dil_input&quot;</span>  =<span class="st"> </span>stride<span class="dv">-1</span>, <span class="st">&quot;auto.pad&quot;</span> =<span class="st"> </span>auto.pad,</a>
<a class="sourceLine" id="cb1989-111" data-line-number="111">      <span class="st">&quot;autopad&quot;</span>    =<span class="st"> &quot;left&quot;</span>,</a>
<a class="sourceLine" id="cb1989-112" data-line-number="112">      <span class="st">&quot;wstride&quot;</span>    =<span class="st"> </span><span class="dv">1</span>,   <span class="st">&quot;wpadding&quot;</span> =<span class="st"> </span>padding, <span class="st">&quot;wdil_rate&quot;</span> =<span class="st"> </span>stride <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1989-113" data-line-number="113">      <span class="st">&quot;wdil_input&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;wauto.pad&quot;</span>  =<span class="st"> </span>auto.pad,</a>
<a class="sourceLine" id="cb1989-114" data-line-number="114">      <span class="st">&quot;wautopad&quot;</span>   =<span class="st"> </span><span class="kw">ifelse</span>(auto.pad <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;right.backprop&quot;</span>, <span class="st">&quot;left.backprop&quot;</span>)</a>
<a class="sourceLine" id="cb1989-115" data-line-number="115">         )</a>
<a class="sourceLine" id="cb1989-116" data-line-number="116">  <span class="kw">list</span>(<span class="st">&quot;feature.map&quot;</span> =<span class="st"> </span>feature.map, <span class="st">&quot;bkprop.cache&quot;</span> =<span class="st"> </span>bkprop.cache, </a>
<a class="sourceLine" id="cb1989-117" data-line-number="117">       <span class="st">&quot;pool.cache&quot;</span>  =<span class="st"> </span>pool.cache, <span class="st">&quot;auto.pad&quot;</span> =<span class="st"> </span>auto.pad, <span class="st">&quot;image&quot;</span> =<span class="st"> </span>image)</a>
<a class="sourceLine" id="cb1989-118" data-line-number="118">}</a></code></pre></div>

<p>Note that in the context of graphical images in which we need to respect the color intensity boundary, we implement the option of normalizing or scaling the feature map back to the allowable range between 0 and 1 after convolution and after the option of clipping to a boundary. This option is only for visual or graphical interpretation - for example, if only just because we need to see the visual effect experimentally. On the other hand, when we discuss <strong>feed-forward</strong> in <strong>CNN</strong>, we primarily use <strong>RELU</strong> or <strong>Leaky RELU</strong> as activation function right after the convolution operation - another way of clipping a value to a lower bound.</p>
</div>
<div id="stride-and-padding" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.3</span> Stride and Padding  <a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In determining the optimal stride and padding for our <strong>CNN</strong>, we need to consider how we should avoid losing information which happens in a case in which an input of size <span class="math inline">\(4\times 4\)</span> and a filter of size <span class="math inline">\(3\times 3\)</span> force us to drop the right side edge and bottom edge of the input if using stride equal to two. Ideally, we can pad the edges with zeroes. Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:paddingstride">12.29</a> shows an example of a convolution with no padding and two convolutions with paddings.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:paddingstride"></span>
<img src="paddingstride.png" alt="Stride and Padding" width="80%" />
<p class="caption">
Figure 12.29: Stride and Padding
</p>
</div>
<p>Based on the figure, each choice of padding and stride affects the size of the feature map and thus also the information it produces. We may opt to use a stride equal to one with no padding in our case. That gives us a smaller <span class="math inline">\(2 \times 2\)</span> feature map while preserving information.</p>
<p>Below is our example implementation of <strong>Convolution</strong> with <strong>Padding</strong> following the equation. In the implementation, we include <strong>Auto Padding</strong> and <strong>Dilation</strong>, which we cover in a section up ahead.</p>

<div class="sourceCode" id="cb1990"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1990-1" data-line-number="1"><span class="co"># Assumes symmetric matrix for the kernels</span></a>
<a class="sourceLine" id="cb1990-2" data-line-number="2">pad &lt;-<span class="st"> </span><span class="cf">function</span>(image, filter, padding) {</a>
<a class="sourceLine" id="cb1990-3" data-line-number="3">  <span class="cf">if</span> (padding <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>) { <span class="kw">return</span> ( <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>image, <span class="st">&quot;padding&quot;</span> =<span class="st"> </span>padding))}</a>
<a class="sourceLine" id="cb1990-4" data-line-number="4">  di        =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1990-5" data-line-number="5">  fi        =<span class="st"> </span><span class="kw">dim</span>(filter)</a>
<a class="sourceLine" id="cb1990-6" data-line-number="6">  img.h     =<span class="st"> </span>di[<span class="dv">1</span>]; kernel.h =<span class="st"> </span>fi[<span class="dv">1</span>]  </a>
<a class="sourceLine" id="cb1990-7" data-line-number="7">  img.w     =<span class="st"> </span>di[<span class="dv">2</span>]; kernel.w =<span class="st"> </span>fi[<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1990-8" data-line-number="8">  img.d     =<span class="st"> </span>di[<span class="dv">3</span>]; kernel.d =<span class="st"> </span>fi[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1990-9" data-line-number="9">  img.s     =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb1990-10" data-line-number="10">  <span class="cf">if</span> (padding <span class="op">&gt;=</span><span class="st"> </span>kernel.h) { <span class="co"># limit paddings</span></a>
<a class="sourceLine" id="cb1990-11" data-line-number="11">      padding =<span class="st"> </span>kernel.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1990-12" data-line-number="12">  } </a>
<a class="sourceLine" id="cb1990-13" data-line-number="13">  h =<span class="st"> </span>(img.h <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>padding)  </a>
<a class="sourceLine" id="cb1990-14" data-line-number="14">  w =<span class="st"> </span>(img.w <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>padding)  </a>
<a class="sourceLine" id="cb1990-15" data-line-number="15">  img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb1990-16" data-line-number="16">  img[(<span class="dv">1</span><span class="op">+</span>padding)<span class="op">:</span>(img.h<span class="op">+</span>padding),(<span class="dv">1</span><span class="op">+</span>padding)<span class="op">:</span>(img.w<span class="op">+</span>padding),,] =<span class="st"> </span>image </a>
<a class="sourceLine" id="cb1990-17" data-line-number="17">  <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>img, <span class="st">&quot;padding&quot;</span> =<span class="st"> </span>padding)</a>
<a class="sourceLine" id="cb1990-18" data-line-number="18">}</a></code></pre></div>

<p>In our implementation, we restrict the number of zero paddings allowed in our <strong>CNN</strong> such that the number of zero paddings cannot be greater than the size of the kernel size, which is a waste of space.</p>
<p><span class="math display" id="eq:eqnnumber609">\[\begin{align}
p = \begin{cases}
kr - 1 &amp; p \ge kr\\
p &amp; otherwise
\end{cases} \tag{12.177}
\end{align}\]</span></p>
<p>Additionally, to preserve information, we automatically add extra paddings to the right and bottom of the image to ensure that we can filter pixels along the edges of the image. In our example implementation below, we allow forcing extra paddings to the right or left edges of the image (as required by backpropagation, for example). Furthermore, if the overall zero paddings exceed the size of the kernel, then we disregard the extra paddings.</p>

<div class="sourceCode" id="cb1991"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1991-1" data-line-number="1">autopad &lt;-<span class="st"> </span><span class="cf">function</span>(image, filter, padding, stride, <span class="dt">edge=</span><span class="st">&quot;right&quot;</span>, </a>
<a class="sourceLine" id="cb1991-2" data-line-number="2">                    <span class="dt">auto.pad =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1991-3" data-line-number="3">  di        =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb1991-4" data-line-number="4">  fi        =<span class="st"> </span><span class="kw">dim</span>(filter)</a>
<a class="sourceLine" id="cb1991-5" data-line-number="5">  img.h     =<span class="st"> </span>di[<span class="dv">1</span>]; kernel.h =<span class="st"> </span>fi[<span class="dv">1</span>]  </a>
<a class="sourceLine" id="cb1991-6" data-line-number="6">  img.w     =<span class="st"> </span>di[<span class="dv">2</span>]; kernel.w =<span class="st"> </span>fi[<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb1991-7" data-line-number="7">  img.d     =<span class="st"> </span>di[<span class="dv">3</span>]; kernel.d =<span class="st"> </span>fi[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1991-8" data-line-number="8">  img.s     =<span class="st"> </span>di[<span class="dv">4</span>];</a>
<a class="sourceLine" id="cb1991-9" data-line-number="9">  <span class="cf">if</span> (edge <span class="op">==</span><span class="st"> &quot;right&quot;</span>) {</a>
<a class="sourceLine" id="cb1991-10" data-line-number="10">        extra.pad =<span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span>kernel.h) <span class="op">%%</span><span class="st"> </span>stride</a>
<a class="sourceLine" id="cb1991-11" data-line-number="11">        extra.pad =<span class="st"> </span><span class="kw">ifelse</span>(extra.pad <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, stride <span class="op">-</span><span class="st"> </span>extra.pad, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1991-12" data-line-number="12">  } <span class="cf">else</span>  {</a>
<a class="sourceLine" id="cb1991-13" data-line-number="13">        extra.pad =<span class="st"> </span>auto.pad</a>
<a class="sourceLine" id="cb1991-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1991-15" data-line-number="15">  <span class="cf">if</span> (extra.pad <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>extra.pad <span class="op">+</span><span class="st"> </span>padding <span class="op">&gt;=</span><span class="st"> </span>kernel.h) {</a>
<a class="sourceLine" id="cb1991-16" data-line-number="16">    <span class="kw">return</span>(<span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>image, <span class="st">&quot;pad&quot;</span> =<span class="st"> </span><span class="dv">0</span>))  </a>
<a class="sourceLine" id="cb1991-17" data-line-number="17">  }     </a>
<a class="sourceLine" id="cb1991-18" data-line-number="18">  h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>extra.pad</a>
<a class="sourceLine" id="cb1991-19" data-line-number="19">  w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>extra.pad</a>
<a class="sourceLine" id="cb1991-20" data-line-number="20">  img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h, w, img.d, img.s))</a>
<a class="sourceLine" id="cb1991-21" data-line-number="21">  <span class="cf">if</span> (edge <span class="op">==</span><span class="st"> &quot;right&quot;</span> <span class="op">||</span><span class="st"> </span>edge<span class="op">==</span><span class="st">&quot;right.backprop&quot;</span>) {</a>
<a class="sourceLine" id="cb1991-22" data-line-number="22">    img[<span class="dv">1</span><span class="op">:</span>img.h,<span class="dv">1</span><span class="op">:</span>img.w,,] =<span class="st"> </span>image    </a>
<a class="sourceLine" id="cb1991-23" data-line-number="23">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1991-24" data-line-number="24">    img[(<span class="dv">1</span><span class="op">+</span>extra.pad)<span class="op">:</span>(img.h<span class="op">+</span>extra.pad),(<span class="dv">1</span><span class="op">+</span>extra.pad)<span class="op">:</span>(img.w<span class="op">+</span>extra.pad),,]=<span class="st"> </span></a>
<a class="sourceLine" id="cb1991-25" data-line-number="25"><span class="st">      </span>image</a>
<a class="sourceLine" id="cb1991-26" data-line-number="26">  }</a>
<a class="sourceLine" id="cb1991-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;image&quot;</span> =<span class="st"> </span>img, <span class="st">&quot;pad&quot;</span> =<span class="st"> </span>extra.pad)</a>
<a class="sourceLine" id="cb1991-28" data-line-number="28">}</a></code></pre></div>

</div>
<div id="kernels-and-filters" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.4</span> Kernels And Filters<a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>filter</strong> is a composition of kernels. A filter may only consist of one Kernel, and thus perhaps the terms are interchangeable in that respect based on such cases. Applying a kernel or filter to an image is called <strong>masking</strong> or <strong>filtering</strong>.</p>
<p>In this section, we cover a few of the <strong>masking or filtering</strong> tricks based on the type of <strong>Kernel</strong>.</p>
<p>While graphically, we use <strong>kernels</strong> to mask an image, which results in <strong>blurring</strong>, <strong>smoothing</strong>, <strong>sharpening</strong>, <strong>embossing</strong>, <strong>edge detection</strong>, or even <strong>noise reduction</strong>, among many other effects, in <strong>CNN</strong>, we use <strong>kernels</strong> to preserve the <strong>spatial relationship</strong> of individual pixels, knowing that during convolution, the process may reduce the dimension of the image if we choose to in our architecture design.</p>
<p>To illustrate, let us form a list of <strong>kernels</strong> to use. Below, we list a few examples of a <strong>kernel</strong>:</p>

<p><span class="math display">\[
 \underbrace{\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 1 \\ 2 &amp; 0 &amp; -2 \\ 1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Sobel Kernel}}
 \ \ \ \ \ 
\underbrace{\left[ \begin{array}{rrr} -1 &amp; 0 &amp; -1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Negative Photo }}
\ \ \ \ \ \
\underbrace{ \left[ \begin{array}{rrr} 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \\ 1 &amp; 0 &amp; -1 \end{array} \right]}_{\text{Embossed (Raised) }}
\ \ \ \ \ \ 
\underbrace{ \left[ \begin{array}{rrr} 1 &amp; 1 &amp; 1 \\ 0 &amp; 0 &amp; 0 \\ -1 &amp; -1 &amp; -1 \end{array} \right]}_{\text{Embossed (Recessed) }}
\]</span>
</p>
<p>We can implement the kernels as follows:</p>

<div class="sourceCode" id="cb1992"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1992-1" data-line-number="1">sobel.kernel    =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>,  <span class="dv">1</span>, <span class="dv">-2</span>, <span class="dv">0</span>,  <span class="dv">2</span>, <span class="dv">-1</span>,  <span class="dv">0</span>,  <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1992-2" data-line-number="2">negative.photo  =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">0</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1992-3" data-line-number="3">raised.emboss   =<span class="st"> </span><span class="kw">c</span>( <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">-1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1992-4" data-line-number="4">recessed.emboss =<span class="st"> </span><span class="kw">c</span>( <span class="dv">1</span>, <span class="dv">1</span>,  <span class="dv">1</span>,  <span class="dv">0</span>, <span class="dv">0</span>,  <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">-1</span>, <span class="dv">-1</span>)</a></code></pre></div>

<p>Finally, let us demonstrate the use of the <strong>convolution(.)</strong> function with a kernel. Below, we perform convolution against the image using different kernels (see the following ). Recall that because convolution may result in a color intensity beyond the range of 0 and 1, we use normalization as an option to scale the feature map back to the same range as required by a <strong>JPEG</strong> format; otherwise, we also can use clipping.</p>

<div class="sourceCode" id="cb1993"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1993-1" data-line-number="1">kernel       =<span class="st"> </span>sobel.kernel</a>
<a class="sourceLine" id="cb1993-2" data-line-number="2">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-3" data-line-number="3">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-4" data-line-number="4">sobel.img    =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-5" data-line-number="5">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-6" data-line-number="6"><span class="co"># crude way of converting greyscale to white-black colors.</span></a>
<a class="sourceLine" id="cb1993-7" data-line-number="7">sobel.img [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,,] =<span class="st"> </span><span class="kw">ifelse</span>(sobel.img [<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">500</span>,,] <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.50</span>, <span class="dv">1</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1993-8" data-line-number="8"></a>
<a class="sourceLine" id="cb1993-9" data-line-number="9">kernel       =<span class="st"> </span>negative.photo</a>
<a class="sourceLine" id="cb1993-10" data-line-number="10">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-11" data-line-number="11">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-12" data-line-number="12">negative.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-13" data-line-number="13">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-14" data-line-number="14"></a>
<a class="sourceLine" id="cb1993-15" data-line-number="15">kernel =<span class="st"> </span>raised.emboss</a>
<a class="sourceLine" id="cb1993-16" data-line-number="16">filter =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-17" data-line-number="17">raised.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-18" data-line-number="18">                                <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1993-19" data-line-number="19"></a>
<a class="sourceLine" id="cb1993-20" data-line-number="20">kernel       =<span class="st"> </span>recessed.emboss</a>
<a class="sourceLine" id="cb1993-21" data-line-number="21">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1993-22" data-line-number="22">recessed.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1993-23" data-line-number="23">                       <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange3">12.30</a> for the effects of the kernels ordered as follows: Sobel kernel, negative photo, embossed (raised), embossed (recessed).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange3"></span>
<img src="appleorange3.png" alt="Apple, Oranges, and Banana" width="80%" />
<p class="caption">
Figure 12.30: Apple, Oranges, and Banana
</p>
</div>
<p>Notice how the <strong>Sobel kernel</strong> can be used for <strong>edge detection</strong>; albeit, there may be other kernels that can mask just the same. For example, test the following kernel:</p>

<div class="sourceCode" id="cb1994"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1994-1" data-line-number="1">edge.kernel =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">4</span>, <span class="dv">-1</span>, <span class="dv">0</span>, <span class="dv">-1</span>, <span class="dv">0</span>)</a></code></pre></div>

<p>One of the important techniques in <strong>CNN</strong> is the ability to perform multiple convolution operations, each using a different kernel like so:</p>

<div class="sourceCode" id="cb1995"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1995-1" data-line-number="1">kernel       =<span class="st"> </span>sobel.kernel</a>
<a class="sourceLine" id="cb1995-2" data-line-number="2">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-3" data-line-number="3">apple.image  =<span class="st"> </span><span class="kw">array</span>(image, <span class="kw">c</span>(<span class="dv">500</span>,<span class="dv">500</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-4" data-line-number="4">sobel.img    =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-5" data-line-number="5">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1995-6" data-line-number="6"></a>
<a class="sourceLine" id="cb1995-7" data-line-number="7">kernel       =<span class="st"> </span>negative.photo</a>
<a class="sourceLine" id="cb1995-8" data-line-number="8">filter       =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-9" data-line-number="9">negative.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter,  <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-10" data-line-number="10">                           <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb1995-11" data-line-number="11"></a>
<a class="sourceLine" id="cb1995-12" data-line-number="12">kernel   =<span class="st"> </span>edge.kernel</a>
<a class="sourceLine" id="cb1995-13" data-line-number="13">filter   =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1995-14" data-line-number="14">edge.img =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">normalize=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1995-15" data-line-number="15">                       <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>We can then combine the convolved result to form a new input image like so:</p>

<div class="sourceCode" id="cb1996"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1996-1" data-line-number="1">d =<span class="st"> </span><span class="kw">dim</span>(edge.img)</a>
<a class="sourceLine" id="cb1996-2" data-line-number="2">new.input.image =<span class="st"> </span><span class="kw">array</span>( <span class="kw">cbind</span>(sobel.img, negative.img, edge.img), </a>
<a class="sourceLine" id="cb1996-3" data-line-number="3">                         <span class="kw">c</span>(d[<span class="dv">1</span>], d[<span class="dv">2</span>], <span class="dv">3</span>, <span class="dv">1</span>))</a></code></pre></div>

<p>which yields the following dimension.</p>

<div class="sourceCode" id="cb1997"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1997-1" data-line-number="1"><span class="kw">dim</span>(new.input.image)</a></code></pre></div>
<pre><code>## [1] 500 500   3   1</code></pre>

<p>Along with the new image, we can also concoct another separate filter:</p>

<div class="sourceCode" id="cb1999"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1999-1" data-line-number="1">pattern =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>( <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dv">15</span>), <span class="kw">rep</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">0</span>), <span class="dv">15</span>)), <span class="dt">nrow=</span><span class="dv">30</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1999-2" data-line-number="2">new.filter =<span class="st"> </span><span class="kw">array</span>( <span class="kw">cbind</span>(pattern, pattern, pattern), <span class="kw">c</span>(<span class="dv">30</span>, <span class="dv">30</span>, <span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1999-3" data-line-number="3"><span class="kw">dim</span>(new.filter)</a></code></pre></div>
<pre><code>## [1] 30 30  3  1</code></pre>

<p>Finally, we perform convolution against the new input image with the new filter (see Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange4">12.31</a>).</p>

<div class="sourceCode" id="cb2001"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2001-1" data-line-number="1">new.image =<span class="st"> </span><span class="kw">convolution</span>(new.input.image , <span class="dt">filter =</span> new.filter,  </a>
<a class="sourceLine" id="cb2001-2" data-line-number="2">                    <span class="dt">normalize=</span><span class="ot">TRUE</span>, <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2001-3" data-line-number="3"><span class="kw">draw.image</span>(new.image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange4"></span>
<img src="appleorange4.png" alt="Apple, Oranges, and Banana" width="60%" />
<p class="caption">
Figure 12.31: Apple, Oranges, and Banana
</p>
</div>

<p>Note that while we use a <span class="math inline">\(30 \times 30\)</span> kernel size for illustration purposes, literatures tend to use <span class="math inline">\(3 \times 3\)</span>, <span class="math inline">\(5 \times 5\)</span>, and <span class="math inline">\(7 \times 7\)</span> kernel sizes. Determining an optimal kernel size can be done heuristically, or it can be done by automatic or adaptive means. However, a notable emphasis is that as long as the kernel can preserve some level of correlation within local features (e.g., highlighting specific features of an image such as edges and others), then we can perhaps say that we are on the right track, at least for the sake of classification or pattern recognition. Hereafter, let us begin to use <strong>highlight</strong> to also refer to a <strong>feature</strong>.</p>
<p>On the other hand, we may start losing the highlights if we are not careful in choosing the correct kernel (let alone its size) - as an example of this is shown in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange4">12.31</a> in which the image starts to blur too much. Irrespective of that, visualization becomes irrelevant as we start training <strong>CNN</strong> to optimize our kernels.</p>
</div>
<div id="dilation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.5</span> Dilation <a href="12.4-convolutional-neural-network-cnn.html#dilation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dilation</strong> is the method of stretching or inflating a <strong>receptive field</strong> such that it skips or leaves gaps in between elements. For example, in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:dilating">12.32</a>, we use a <strong>dilate rate</strong> of two, resulting in a feature map element of 453 after convolution. In general, a <strong>dilate rate</strong> of one is equivalent to a normal convolution with no gaps. Moreover, a <strong>dilate rate</strong> of two corresponds to a one-pixel gap.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dilating"></span>
<img src="dilating.png" alt="Dilation" width="80%" />
<p class="caption">
Figure 12.32: Dilation
</p>
</div>
<p>Below is our example implementation of <strong>Dilation</strong>. Note that for our own purposes only, we use a <strong>dilate rate</strong> equal to zero for normal convolution with no gaps. And a rate of one corresponds to a one-pixel gap.</p>

<div class="sourceCode" id="cb2002"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2002-1" data-line-number="1">dilate  &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">dil_rate=</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2002-2" data-line-number="2">   <span class="cf">if</span> (dil_rate <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(image) }  </a>
<a class="sourceLine" id="cb2002-3" data-line-number="3">   d       =<span class="st"> </span><span class="kw">dim</span>(image) </a>
<a class="sourceLine" id="cb2002-4" data-line-number="4">   img.h   =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2002-5" data-line-number="5">   h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2002-6" data-line-number="6">   w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.w <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2002-7" data-line-number="7">   img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb2002-8" data-line-number="8">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.h) {</a>
<a class="sourceLine" id="cb2002-9" data-line-number="9">     <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.w) {</a>
<a class="sourceLine" id="cb2002-10" data-line-number="10">       img[i<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,j<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,,] =<span class="st"> </span>image[i,j,,]</a>
<a class="sourceLine" id="cb2002-11" data-line-number="11">     }</a>
<a class="sourceLine" id="cb2002-12" data-line-number="12">   }</a>
<a class="sourceLine" id="cb2002-13" data-line-number="13">   img</a>
<a class="sourceLine" id="cb2002-14" data-line-number="14">}</a></code></pre></div>
<div class="sourceCode" id="cb2003"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2003-1" data-line-number="1">dilate.filters &lt;-<span class="st"> </span><span class="cf">function</span>(filters, <span class="dt">dil_rate =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2003-2" data-line-number="2">   <span class="cf">if</span> (dil_rate <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="kw">return</span>(filters) }  </a>
<a class="sourceLine" id="cb2003-3" data-line-number="3">   len =<span class="st"> </span><span class="kw">length</span>(filters)</a>
<a class="sourceLine" id="cb2003-4" data-line-number="4">   <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2003-5" data-line-number="5">       filter =<span class="st"> </span>filters[[l]]</a>
<a class="sourceLine" id="cb2003-6" data-line-number="6">       d       =<span class="st"> </span><span class="kw">dim</span>(filter) </a>
<a class="sourceLine" id="cb2003-7" data-line-number="7">       img.h   =<span class="st"> </span>d[<span class="dv">1</span>]; img.w =<span class="st"> </span>d[<span class="dv">2</span>]; img.d =<span class="st"> </span>d[<span class="dv">3</span>]; img.s =<span class="st"> </span>d[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2003-8" data-line-number="8">       h =<span class="st"> </span>img.h <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.h <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2003-9" data-line-number="9">       w =<span class="st"> </span>img.w <span class="op">+</span><span class="st"> </span>dil_rate <span class="op">*</span><span class="st"> </span>(img.w <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2003-10" data-line-number="10">       img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(h,w, img.d, img.s))</a>
<a class="sourceLine" id="cb2003-11" data-line-number="11">       <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.h) {</a>
<a class="sourceLine" id="cb2003-12" data-line-number="12">         <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>img.w) {</a>
<a class="sourceLine" id="cb2003-13" data-line-number="13">           img[i<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,j<span class="op">*</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span>,,] =<span class="st"> </span>image[i,j,,]</a>
<a class="sourceLine" id="cb2003-14" data-line-number="14">         }</a>
<a class="sourceLine" id="cb2003-15" data-line-number="15">       }</a>
<a class="sourceLine" id="cb2003-16" data-line-number="16">       filters[[l]] =<span class="st"> </span>img</a>
<a class="sourceLine" id="cb2003-17" data-line-number="17">   }</a>
<a class="sourceLine" id="cb2003-18" data-line-number="18">   filters</a>
<a class="sourceLine" id="cb2003-19" data-line-number="19">}</a></code></pre></div>

<p>A simple example of using the <strong>dilate(.)</strong> function is shown below:</p>

<div class="sourceCode" id="cb2004"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2004-1" data-line-number="1">size =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2004-2" data-line-number="2">m =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,size <span class="op">*</span><span class="st"> </span>size), <span class="kw">c</span>(size, size, <span class="dv">1</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2004-3" data-line-number="3">m[,,,<span class="dv">1</span>]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    4    7
## [2,]    2    5    8
## [3,]    3    6    9</code></pre>
<div class="sourceCode" id="cb2006"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2006-1" data-line-number="1"><span class="kw">dilate</span>(m, <span class="dt">dil_rate=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## , , 1, 1
## 
##      [,1] [,2] [,3] [,4] [,5]
## [1,]    1    0    4    0    7
## [2,]    0    0    0    0    0
## [3,]    2    0    5    0    8
## [4,]    0    0    0    0    0
## [5,]    3    0    6    0    9</code></pre>

<p>To illustrate, let us use the edge kernel, but this time, we pass a new hyperparameter to the <strong>convolution(.)</strong> function called <strong>Dilation Rate</strong> with a value equal to one.</p>

<div class="sourceCode" id="cb2008"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2008-1" data-line-number="1">kernel =<span class="st"> </span>edge.kernel</a>
<a class="sourceLine" id="cb2008-2" data-line-number="2">filter =<span class="st"> </span><span class="kw">array</span>( kernel, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2008-3" data-line-number="3">new.image =<span class="st"> </span><span class="kw">convolution</span>(apple.image , <span class="dt">filter =</span> filter, <span class="dt">dil_rate=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2008-4" data-line-number="4">                    <span class="dt">normalize=</span><span class="ot">TRUE</span>, <span class="dt">ptype=</span><span class="st">&quot;mask&quot;</span>)<span class="op">$</span>feature.map</a></code></pre></div>

<p>Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:appleorange5">12.33</a> shows the new image using a dilated filter.</p>

<div class="sourceCode" id="cb2009"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2009-1" data-line-number="1"><span class="kw">draw.image</span>(new.image)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:appleorange5"></span>
<img src="appleorange5.png" alt="Apple, Oranges, and Banana" width="60%" />
<p class="caption">
Figure 12.33: Apple, Oranges, and Banana
</p>
</div>

</div>
<div id="pooling" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.6</span> Pooling <a href="12.4-convolutional-neural-network-cnn.html#pooling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Pooling</strong> is also regarded as <strong>SubSampling</strong> or <strong>DownSampling</strong>, of which the primary intent is for dimensionality reduction by summarizing the feature map.</p>
<p>Similar to convolution, pooling is an operation that slides a <span class="math inline">\(P \times P\)</span> filter over a feature map in the direction from left to right and top to bottom. We then perform one of two common pooling methods, namely <strong>maximum pooling</strong> and <strong>average pooling</strong> (see Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:pooling">12.34</a>).  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pooling"></span>
<img src="pooling.png" alt="Pooling" width="70%" />
<p class="caption">
Figure 12.34: Pooling
</p>
</div>
<p>In Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:pooling">12.34</a>, we compute the value of the first cell in the pool for either the maximum pooling or average pooling. Below is the result:</p>

<div class="sourceCode" id="cb2010"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2010-1" data-line-number="1">p1 =<span class="st"> </span><span class="kw">max</span>(<span class="kw">c</span>(<span class="dv">224</span>, <span class="dv">161</span>, <span class="dv">325</span>, <span class="dv">83</span>))</a>
<a class="sourceLine" id="cb2010-2" data-line-number="2">p2 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">c</span>(<span class="dv">224</span>, <span class="dv">161</span>,<span class="dv">325</span>, <span class="dv">83</span>))</a>
<a class="sourceLine" id="cb2010-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MaxPool&quot;</span> =<span class="st"> </span>p1, <span class="st">&quot;AvePool&quot;</span> =<span class="st"> </span>p2)</a></code></pre></div>
<pre><code>## MaxPool AvePool 
##  325.00  198.25</code></pre>

<p>We then slide the filter to the next patch to perform the same calculation. We repeat the process until we slide to the last patch.</p>

<div class="sourceCode" id="cb2012"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2012-1" data-line-number="1">p1 =<span class="st"> </span><span class="kw">max</span>(<span class="kw">c</span>(<span class="dv">161</span>, <span class="dv">164</span>, <span class="dv">83</span>, <span class="dv">65</span>))</a>
<a class="sourceLine" id="cb2012-2" data-line-number="2">p2 =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">c</span>(<span class="dv">161</span>, <span class="dv">164</span>, <span class="dv">83</span>, <span class="dv">65</span>))</a>
<a class="sourceLine" id="cb2012-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;MaxPool&quot;</span> =<span class="st"> </span>p1, <span class="st">&quot;AvePool&quot;</span> =<span class="st"> </span>p2)</a></code></pre></div>
<pre><code>## MaxPool AvePool 
##  164.00  118.25</code></pre>

<p>Notice that the resulting matrix is reduced in size to a smaller dimension. It means a reduction in the number of parameters and computation. Note that for a tensor, the depth is preserved.</p>
<p>Our implementation of <strong>Pooling</strong> piggybacks on <strong>Convolution</strong> in our case. Therefore, we need a corresponding function, e.g., <strong>pooling(.)</strong>. It is notable to mention that other implementations may avoid overlapping, which can be done by adjusting the strides.</p>

<div class="sourceCode" id="cb2014"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2014-1" data-line-number="1">pooling &lt;-<span class="cf">function</span>(<span class="dt">ptype =</span> <span class="st">&quot;maxpool&quot;</span>, ...) { <span class="kw">convolution</span>( <span class="dt">ptype=</span>ptype, ...)}</a>
<a class="sourceLine" id="cb2014-2" data-line-number="2">pool.backprop &lt;-<span class="st"> </span>pooling</a></code></pre></div>

<p>As an example of its application:</p>

<div class="sourceCode" id="cb2015"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2015-1" data-line-number="1">m =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">48</span>), <span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2015-2" data-line-number="2">f =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">27</span>), <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2015-3" data-line-number="3"><span class="kw">dim</span>(m)</a></code></pre></div>
<pre><code>## [1] 4 4 3 1</code></pre>
<div class="sourceCode" id="cb2017"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2017-1" data-line-number="1">m[,,,<span class="dv">1</span>]</a></code></pre></div>
<pre><code>## , , 1
## 
##      [,1] [,2] [,3] [,4]
## [1,]    1    5    9   13
## [2,]    2    6   10   14
## [3,]    3    7   11   15
## [4,]    4    8   12   16
## 
## , , 2
## 
##      [,1] [,2] [,3] [,4]
## [1,]   17   21   25   29
## [2,]   18   22   26   30
## [3,]   19   23   27   31
## [4,]   20   24   28   32
## 
## , , 3
## 
##      [,1] [,2] [,3] [,4]
## [1,]   33   37   41   45
## [2,]   34   38   42   46
## [3,]   35   39   43   47
## [4,]   36   40   44   48</code></pre>
<div class="sourceCode" id="cb2019"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2019-1" data-line-number="1">p =<span class="st"> </span><span class="kw">pooling</span>(<span class="dt">image=</span>m, <span class="dt">filter=</span>f, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2019-2" data-line-number="2"><span class="kw">dim</span>(p<span class="op">$</span>feature.map)</a></code></pre></div>
<pre><code>## [1] 2 2 3 1</code></pre>
<div class="sourceCode" id="cb2021"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2021-1" data-line-number="1">p<span class="op">$</span>feature.map[,,,<span class="dv">1</span>]  <span class="co"># default: maxpool</span></a></code></pre></div>
<pre><code>## , , 1
## 
##      [,1] [,2]
## [1,]   11   15
## [2,]   12   16
## 
## , , 2
## 
##      [,1] [,2]
## [1,]   27   31
## [2,]   28   32
## 
## , , 3
## 
##      [,1] [,2]
## [1,]   43   47
## [2,]   44   48</code></pre>

<p>There are other types of pooling used in other literature. We leave readers to investigate the others.</p>
<p>In the next section, we now discuss how convolution works in a deep convolutional neural network.</p>
</div>
<div id="cnn-architectures" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.7</span> CNN Architectures<a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A few <strong>CNN</strong> architectures have earned their rightful place and can keep their respective prestigious names, namely <strong>LeNet-5</strong>, <strong>AlexNet</strong>, <strong>VGG-16</strong>, and <strong>ResNet</strong>. Such architectures have become classic models for object and image detection in popular competitions - one competition, in particular, is the <strong>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</strong> which promotes annual competition, eyeing for deserving architectures that may find their place in the top ranks based on performance, accuracy, and other criteria.    </p>
<p>Designing <strong>CNN</strong> architecture involves determining the number of convolution layers to use, the number of filters to use, the number of pooling layers to use, the size of the minibatch to use, and the use of batch normalization and dropouts, among many other factors. Ultimately, we desire to reduce computation costs while achieving reasonable stability and accuracy across general cases.</p>
<p>In this section, let us use a <strong>toy architecture</strong> to discuss the major components of a <strong>CNN</strong> architecture as shown in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnarch">12.35</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnarch"></span>
<img src="cnnarchitecture.png" alt="CNN (Toy Architecture)" width="90%" />
<p class="caption">
Figure 12.35: CNN (Toy Architecture)
</p>
</div>
<p>For <strong>CNN</strong> to work, we ensure it knows how to learn (to be trained) first. We go through the same methods such as <strong>FeedForward</strong> and <strong>BackPropagation</strong> as covered in previous sections about <strong>MLP</strong>. Moreover, we go through the possible use of optimizers such as <strong>AdaGrad</strong>, <strong>Adam</strong>, and <strong>Batch Normalization</strong>.</p>
</div>
<div id="forward-feed-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.8</span> Forward Feed <a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We mostly cover the <strong>Forward Feed</strong> technique in the <strong>MLP</strong> section. It is indeed a straightforward process. Recall that in the <strong>Forward Pass</strong> portion of the algorithm, we are given the option to choose which activation function to use. In the context of <strong>CNN</strong>, we have additional options to choose from, a couple of them being the size and number of kernels. Essentially, this dictates the complexity of our <strong>CNN</strong>, driven by the number of learnable parameters to use. There are two considerations to note in determining the size and number of kernels:</p>
<p><strong>Local Connectivity</strong> </p>
<p>The core premise of <strong>convolution</strong> is to form a local grouping of pixels that may carry unique highlights or patterns. Such a group of pixels is projected in a summarized form to what we call <strong>activation map</strong>; otherwise known as <strong>feature map</strong>. Each cell in a <strong>feature map</strong> corresponds to a single <strong>neuron</strong> due to convolving a portion of an input image using a filter. As the filter slides over the input image, we form a collection of <strong>neurons</strong> in the form of the <strong>activation map</strong>. The term <strong>local connectivity</strong> emphasizes that a <strong>neuron</strong> is built upon (or is connected to) only a subset (a small region) of an input image. It should be noted that the effect of such <strong>local connectivity</strong> unavoidably diminishes the quality of the original image as we move deeper down the neural network via <strong>feed-forward</strong>. This effect can be regarded as smoothing and a way to offset overfitting. On the other hand, <strong>local connectivity</strong> tries to discover and preserve unique patterns per neuron in a <strong>feature map</strong>. The entire <strong>feature map</strong> is a collection of unique local patterns, forming the important, relevant components of a class - possibly a unique unified global information that can be classified. Preserving while siphoning this information from one layer to the next is challenging. For intuition, the ears, eyes, or nose of a facial image are all highlights at a high level, produced by a convolution of the facial image with different kernels. It is an onus upon us to design a <strong>CNN</strong> that effectively allows the highlights to percolate through the network down a final classification layer without losing relevant information. This final layer is the fully connected (FC) neural network layer, as shown in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnarch">12.35</a> responsible for performing softmax for classification. It is at the output layer of the fully connected neural network where we also connect all highlights in a more global setting to finally classify the image in its entirety based on softmax probability. </p>
<p><strong>Weight Sharing</strong> </p>
<p><strong>Activation maps</strong> contain neurons (cells) that share common <strong>weights</strong> in the form of <strong>kernels</strong>. Similar to neurons in <strong>MLP</strong>, <strong>activation maps</strong> are the <strong>net output</strong> of a <strong>dot product</strong> between a set of inputs and weights. Here, <strong>kernels</strong> - being the <strong>weights</strong> - hold learnable parameters that need to be optimized during <strong>BackPropagation</strong>. Therefore, if our kernel size is <span class="math inline">\(3 \times 3\)</span>, we have nine learnable parameters to tune. Likewise, if we build a filter of <span class="math inline">\(3 \times 3\)</span> kernels with a depth size of 10, we have ninety learnable parameters.</p>
<p><strong>Feedforward</strong></p>
<p>Now, in terms of <strong>FeedForward</strong>, we need to be able to construct a set of randomly initialized kernels. Similar to <strong>deep.neural.layer(.)</strong> based on our <strong>MLP</strong> implementation, let us also write a <strong>CNN</strong> version named <strong>deep.cnn.layer(.)</strong>. Both contain a structure of learnable <strong>weights</strong>, which are trained and updated during <strong>Backpropagation</strong> and <strong>Backward Pass</strong>.</p>

<div class="sourceCode" id="cb2023"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2023-1" data-line-number="1">error &lt;-<span class="st"> </span><span class="cf">function</span>(e) {</a>
<a class="sourceLine" id="cb2023-2" data-line-number="2">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2023-3" data-line-number="3">       <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Feature map convolvs into 1x1.&quot;</span>,</a>
<a class="sourceLine" id="cb2023-4" data-line-number="4">                  <span class="st">&quot; Input size is too small or kernel size is too large&quot;</span>,</a>
<a class="sourceLine" id="cb2023-5" data-line-number="5">                  <span class="st">&quot; or there are too many layers.&quot;</span>)) </a>
<a class="sourceLine" id="cb2023-6" data-line-number="6">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2023-7" data-line-number="7">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">2</span> ) {</a>
<a class="sourceLine" id="cb2023-8" data-line-number="8">        <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Backprop Convolution does not trace to correct path.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-9" data-line-number="9">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-10" data-line-number="10">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">3</span> ) {</a>
<a class="sourceLine" id="cb2023-11" data-line-number="11">        <span class="kw">stop</span>(<span class="kw">paste0</span>(<span class="st">&quot;Full Convolution does not trace to correct path.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-12" data-line-number="12">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-13" data-line-number="13">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb2023-14" data-line-number="14">        <span class="kw">stop</span>(<span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb2023-15" data-line-number="15">          <span class="st">&quot;Size of kernel cannot be the same or greater than input size.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-16" data-line-number="16">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2023-17" data-line-number="17">    <span class="cf">if</span> (e <span class="op">==</span><span class="st"> </span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb2023-18" data-line-number="18">        <span class="kw">stop</span>(<span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb2023-19" data-line-number="19">          <span class="st">&quot;Size of Stride cannot be the same or greater than kernel size.&quot;</span>))</a>
<a class="sourceLine" id="cb2023-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb2023-21" data-line-number="21">}</a></code></pre></div>
<div class="sourceCode" id="cb2024"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2024-1" data-line-number="1">deep.cnn.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X,  ...) { </a>
<a class="sourceLine" id="cb2024-2" data-line-number="2">   di        =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2024-3" data-line-number="3">   <span class="cf">if</span> (<span class="kw">length</span>(di) <span class="op">&lt;</span><span class="st"> </span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb2024-4" data-line-number="4">     di =<span class="st"> </span><span class="kw">c</span>(di, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2024-5" data-line-number="5">     X =<span class="st"> </span><span class="kw">array</span>(X, di)</a>
<a class="sourceLine" id="cb2024-6" data-line-number="6">   }</a>
<a class="sourceLine" id="cb2024-7" data-line-number="7">   input.size  =<span class="st"> </span>di[<span class="dv">1</span>]; img.s =<span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2024-8" data-line-number="8">   depth       =<span class="st"> </span>di[<span class="dv">3</span>] </a>
<a class="sourceLine" id="cb2024-9" data-line-number="9">   layers      =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb2024-10" data-line-number="10">   structure   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2024-11" data-line-number="11">   l         =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2024-12" data-line-number="12">   <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb2024-13" data-line-number="13">      l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2024-14" data-line-number="14">      <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) { </a>
<a class="sourceLine" id="cb2024-15" data-line-number="15">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>stride))     { layer<span class="op">$</span>stride     =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-16" data-line-number="16">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>padding))    { layer<span class="op">$</span>padding    =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-17" data-line-number="17">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_rate))   { layer<span class="op">$</span>dil_rate   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-18" data-line-number="18">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>filters))    { layer<span class="op">$</span>filters    =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-19" data-line-number="19">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>auto.pad))   { layer<span class="op">$</span>auto.pad   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-20" data-line-number="20">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>afunc))      { layer<span class="op">$</span>afunc      =<span class="st"> </span><span class="ot">NULL</span> }  </a>
<a class="sourceLine" id="cb2024-21" data-line-number="21">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>drop))       { layer<span class="op">$</span>drop       =<span class="st"> </span><span class="ot">NULL</span> }</a>
<a class="sourceLine" id="cb2024-22" data-line-number="22">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>bias))       { layer<span class="op">$</span>bias       =<span class="st"> </span><span class="ot">TRUE</span> }</a>
<a class="sourceLine" id="cb2024-23" data-line-number="23">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>normalize))  { layer<span class="op">$</span>normalize  =<span class="st"> </span><span class="ot">FALSE</span> }</a>
<a class="sourceLine" id="cb2024-24" data-line-number="24">        <span class="cf">if</span> (layer<span class="op">$</span>size <span class="op">&lt;=</span><span class="st"> </span>layer<span class="op">$</span>stride) { <span class="kw">error</span>(<span class="dv">5</span>) }</a>
<a class="sourceLine" id="cb2024-25" data-line-number="25">           </a>
<a class="sourceLine" id="cb2024-26" data-line-number="26">        batch.gamma      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2024-27" data-line-number="27">        batch.beta       =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2024-28" data-line-number="28">        layer<span class="op">$</span>batchnorm  =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;gamma&quot;</span>  =<span class="st"> </span>batch.gamma, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>batch.beta, </a>
<a class="sourceLine" id="cb2024-29" data-line-number="29">                                <span class="st">&quot;moving.mu&quot;</span>  =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>filters,<span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb2024-30" data-line-number="30">                                <span class="st">&quot;moving.var&quot;</span> =<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(layer<span class="op">$</span>filters,<span class="dv">1</span>)),</a>
<a class="sourceLine" id="cb2024-31" data-line-number="31">                                <span class="st">&quot;normalize&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>normalize)  </a>
<a class="sourceLine" id="cb2024-32" data-line-number="32">        <span class="co"># For Depthwise Separable convolution</span></a>
<a class="sourceLine" id="cb2024-33" data-line-number="33">        weights =<span class="st"> </span><span class="kw">net.initialization</span>(layer<span class="op">$</span>size <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>size, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb2024-34" data-line-number="34">                                 layer<span class="op">$</span>size <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>size, <span class="dt">itype=</span><span class="st">&quot;he&quot;</span>, </a>
<a class="sourceLine" id="cb2024-35" data-line-number="35">                                 <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2024-36" data-line-number="36">        layer<span class="op">$</span>dw.kernel  =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb2024-37" data-line-number="37">                <span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">array</span>(weights, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)), </a>
<a class="sourceLine" id="cb2024-38" data-line-number="38">                <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)),</a>
<a class="sourceLine" id="cb2024-39" data-line-number="39">                <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(layer<span class="op">$</span>size, layer<span class="op">$</span>size, depth)) )</a>
<a class="sourceLine" id="cb2024-40" data-line-number="40">        layer<span class="op">$</span>dw.bias    =<span class="st"> </span><span class="kw">list</span>(</a>
<a class="sourceLine" id="cb2024-41" data-line-number="41">                <span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>depth, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb2024-42" data-line-number="42">                <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, depth),</a>
<a class="sourceLine" id="cb2024-43" data-line-number="43">                <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, depth) )     </a>
<a class="sourceLine" id="cb2024-44" data-line-number="44">        <span class="co"># For Pointwise convolution</span></a>
<a class="sourceLine" id="cb2024-45" data-line-number="45">        weights =<span class="st"> </span><span class="kw">net.initialization</span>(depth <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>filters, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2024-46" data-line-number="46">                                 depth <span class="op">*</span><span class="st"> </span>layer<span class="op">$</span>filters, <span class="dt">itype=</span><span class="st">&quot;he&quot;</span>, </a>
<a class="sourceLine" id="cb2024-47" data-line-number="47">                                 <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2024-48" data-line-number="48">        layer<span class="op">$</span>pw.kernel =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-49" data-line-number="49"><span class="st">                     </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">array</span>(weights, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)),</a>
<a class="sourceLine" id="cb2024-50" data-line-number="50">                          <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)),</a>
<a class="sourceLine" id="cb2024-51" data-line-number="51">                          <span class="st">&quot;nu&quot;</span>     =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(depth, layer<span class="op">$</span>filters)) )</a>
<a class="sourceLine" id="cb2024-52" data-line-number="52">        layer<span class="op">$</span>pw.bias   =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-53" data-line-number="53"><span class="st">                     </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>layer<span class="op">$</span>filters, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb2024-54" data-line-number="54">                          <span class="st">&quot;rho&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, layer<span class="op">$</span>filters),</a>
<a class="sourceLine" id="cb2024-55" data-line-number="55">                           <span class="st">&quot;nu&quot;</span>    =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, layer<span class="op">$</span>filters) )  </a>
<a class="sourceLine" id="cb2024-56" data-line-number="56"></a>
<a class="sourceLine" id="cb2024-57" data-line-number="57">        input.size         =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2024-58" data-line-number="58">        depth              =<span class="st"> </span>layer<span class="op">$</span>filters</a>
<a class="sourceLine" id="cb2024-59" data-line-number="59">        structure[[l]]     =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-60" data-line-number="60"><span class="st">          </span><span class="kw">list</span>(<span class="st">&quot;stride&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>stride,    <span class="st">&quot;padding&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>padding,   </a>
<a class="sourceLine" id="cb2024-61" data-line-number="61">               <span class="st">&quot;dil_rate&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>dil_rate,  <span class="st">&quot;auto.pad&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>auto.pad,   </a>
<a class="sourceLine" id="cb2024-62" data-line-number="62">               <span class="st">&quot;afunc&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>afunc,     <span class="st">&quot;istype&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>type,</a>
<a class="sourceLine" id="cb2024-63" data-line-number="63">               <span class="st">&quot;dw.kernel&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>dw.kernel, <span class="st">&quot;pw.kernel&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>pw.kernel,</a>
<a class="sourceLine" id="cb2024-64" data-line-number="64">               <span class="st">&quot;dw.bias&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>dw.bias,   <span class="st">&quot;pw.bias&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>pw.bias,</a>
<a class="sourceLine" id="cb2024-65" data-line-number="65">               <span class="st">&quot;batchnorm&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>batchnorm, <span class="st">&quot;drop&quot;</span>       =<span class="st"> </span>layer<span class="op">$</span>drop,</a>
<a class="sourceLine" id="cb2024-66" data-line-number="66">               <span class="st">&quot;bias&quot;</span>      =<span class="st"> </span>layer<span class="op">$</span>bias)</a>
<a class="sourceLine" id="cb2024-67" data-line-number="67">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2024-68" data-line-number="68">     <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2024-69" data-line-number="69">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>stride))  { layer<span class="op">$</span>stride  =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2024-70" data-line-number="70">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>padding)) { layer<span class="op">$</span>padding =<span class="st"> </span><span class="dv">0</span> }   </a>
<a class="sourceLine" id="cb2024-71" data-line-number="71">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_rate))   { layer<span class="op">$</span>dil_rate   =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-72" data-line-number="72">        <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>dil_input))  { layer<span class="op">$</span>dil_input  =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2024-73" data-line-number="73">        wsiz =<span class="st"> </span>layer<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2024-74" data-line-number="74">        window =<span class="st"> </span><span class="kw">array</span>(<span class="kw">rep</span>(<span class="dv">1</span>, wsiz <span class="op">*</span><span class="st"> </span>wsiz), <span class="kw">c</span>(wsiz, wsiz, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2024-75" data-line-number="75">        structure[[l]] =<span class="st"> </span></a>
<a class="sourceLine" id="cb2024-76" data-line-number="76"><span class="st">          </span><span class="kw">list</span>(<span class="st">&quot;window&quot;</span>    =<span class="st"> </span>window,         <span class="st">&quot;stride&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2024-77" data-line-number="77">               <span class="st">&quot;padding&quot;</span>   =<span class="st"> </span>layer<span class="op">$</span>padding,  <span class="st">&quot;auto.pad&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>auto.pad,</a>
<a class="sourceLine" id="cb2024-78" data-line-number="78">               <span class="st">&quot;dil_rate&quot;</span>  =<span class="st"> </span>layer<span class="op">$</span>dil_rate, <span class="st">&quot;dil_input&quot;</span> =<span class="st"> </span>layer<span class="op">$</span>dil_input,</a>
<a class="sourceLine" id="cb2024-79" data-line-number="79">               <span class="st">&quot;ptype&quot;</span>     =<span class="st"> </span>layer<span class="op">$</span>ptype,    <span class="st">&quot;istype&quot;</span>    =<span class="st"> </span>layer<span class="op">$</span>type)</a>
<a class="sourceLine" id="cb2024-80" data-line-number="80">      } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2024-81" data-line-number="81">      <span class="cf">if</span> (layer<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2024-82" data-line-number="82">       structure[[l]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;fc.layers&quot;</span>     =<span class="st"> </span>layer[layer <span class="op">!=</span><span class="st"> </span>layer<span class="op">$</span>type],</a>
<a class="sourceLine" id="cb2024-83" data-line-number="83">                              <span class="st">&quot;init&quot;</span>          =<span class="st"> </span><span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb2024-84" data-line-number="84">                              <span class="st">&quot;istype&quot;</span>        =<span class="st"> </span>layer<span class="op">$</span>type)</a>
<a class="sourceLine" id="cb2024-85" data-line-number="85">      }</a>
<a class="sourceLine" id="cb2024-86" data-line-number="86">  }</a>
<a class="sourceLine" id="cb2024-87" data-line-number="87">  structure</a>
<a class="sourceLine" id="cb2024-88" data-line-number="88">}</a></code></pre></div>

<p>To illustrate, let us use a <strong>synthetic</strong> image with the following dimension.</p>

<div class="sourceCode" id="cb2025"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2025-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb2025-2" data-line-number="2">size =<span class="st"> </span><span class="dv">7</span></a>
<a class="sourceLine" id="cb2025-3" data-line-number="3">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2025-4" data-line-number="4">input =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb2025-5" data-line-number="5">mx =<span class="st"> </span>size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth</a>
<a class="sourceLine" id="cb2025-6" data-line-number="6">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,mx), <span class="kw">c</span>(size,size,depth, input))</a>
<a class="sourceLine" id="cb2025-7" data-line-number="7">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2025-8" data-line-number="8">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">2</span>, <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>),</a>
<a class="sourceLine" id="cb2025-9" data-line-number="9">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2025-10" data-line-number="10">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2025-11" data-line-number="11">        )</a>
<a class="sourceLine" id="cb2025-12" data-line-number="12"><span class="kw">str</span>(cnn.layers, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## List of 3
## $ :List of 13
## ..$ stride : num 1
## ..$ padding : num 0
## ..$ dil_rate : num 0
## ..$ auto.pad : num 0
## ..$ afunc : NULL
## ..$ istype : chr &quot;convolv&quot;
## ..$ dw.kernel:List of 3
## .. ..$ weight: num [1:3, 1:3, 1:3] 0.236 0.215 -0.293
##    -0.183 0.326 ...
## .. ..$ rho : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0
##    ...
## .. ..$ nu : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0 ...
## ..$ pw.kernel:List of 3
## .. ..$ weight: num [1:3, 1:2] -0.0192 0.0585 -0.1033
##    -0.33 0.3465 ...
## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0
## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0
## ..$ dw.bias :List of 3
## .. ..$ weight: num [1:3] 0.84 0.397 0.393
## .. ..$ rho : num [1:3] 0 0 0
## .. ..$ nu : num [1:3] 0 0 0
## ..$ pw.bias :List of 3
## .. ..$ weight: num [1:2] 0.552 0.102
## .. ..$ rho : num [1:2] 0 0
## .. ..$ nu : num [1:2] 0 0
## ..$ batchnorm:List of 5
## .. ..$ gamma :List of 3
## .. .. ..$ weight: num 1
## .. .. ..$ rho : num 0
## .. .. ..$ nu : num 0
## .. ..$ beta :List of 3
## .. .. ..$ weight: num 0
## .. .. ..$ rho : num 0
## .. .. ..$ nu : num 0
## .. ..$ moving.mu : num [1:2, 1] 0 0
## .. ..$ moving.var: num [1:2, 1] 1 1
## .. ..$ normalize : chr &quot;batch&quot;
## ..$ drop : NULL
## ..$ bias : logi TRUE
## $ :List of 8
## ..$ window : num [1:2, 1:2, 1] 1 1 1 1
## ..$ stride : num 1
## ..$ padding : num 0
## ..$ auto.pad : NULL
## ..$ dil_rate : num 0
## ..$ dil_input: num 0
## ..$ ptype : chr &quot;maxpool&quot;
## ..$ istype : chr &quot;pooling&quot;
## $ :List of 3
## ..$ fc.layers:List of 3
## .. ..$ :List of 1
## .. .. ..$ size: num 3
## .. ..$ :List of 1
## .. .. ..$ size: num 3
## .. ..$ :List of 1
## .. .. ..$ size: num 10
## ..$ init : logi TRUE
## ..$ istype : chr &quot;dense&quot;</code></pre>

<p>Here, we use a depth size of three, corresponding to the <strong>RGB channels</strong>. The kernels then inherit the input depth in the first layer in our <strong>CNN</strong>. Now, suppose that we use only two convolutional layers in building our <strong>CNN</strong>. For the first layer, we use a kernel size of <span class="math inline">\(3 \times 3\)</span>. We also specify three kernels (based on the <strong>kernels</strong> parameter). After convolution, it will produce a feature map with a depth of three, based on specifying three kernels. The second layer also uses a kernel size of <span class="math inline">\(3 \times 3\)</span> but with only two kernels. In other words, the first <strong>feature map</strong> to be generated will have a depth of 3, and the second <strong>feature map</strong> will have a depth of two. Let us use our <strong>deep.cnn.layers(.)</strong> function to construct the structure. Here, we assume a default stride of 1 and padding of 0. The first layer uses <strong>Leaky RELU</strong> at the end of the convolution.</p>

<div class="sourceCode" id="cb2027"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2027-1" data-line-number="1">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X,</a>
<a class="sourceLine" id="cb2027-2" data-line-number="2">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">kernels=</span><span class="dv">3</span>),</a>
<a class="sourceLine" id="cb2027-3" data-line-number="3">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">kernels=</span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb2027-4" data-line-number="4">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2027-5" data-line-number="5">        <span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">3</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2027-6" data-line-number="6">        )</a></code></pre></div>

<p>With the structure of the <strong>CNN</strong> layers in place, let us now have our example implementation of <strong>forward pass</strong> for our <strong>CNN</strong>, passing the input and the filters, and yielding a flattened vector as output.</p>

<div class="sourceCode" id="cb2028"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2028-1" data-line-number="1">forward.pass.cnn &lt;-<span class="st"> </span><span class="cf">function</span>(X, layers,  <span class="dt">train=</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-2" data-line-number="2">  cnn.output   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-3" data-line-number="3">  cache.output =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-4" data-line-number="4">  pool.cache   =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2028-5" data-line-number="5">  feature.map =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2028-6" data-line-number="6">  H =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2028-7" data-line-number="7">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb2028-8" data-line-number="8">    layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2028-9" data-line-number="9">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-10" data-line-number="10">      fmaps   =<span class="st"> </span><span class="ot">NULL</span>; v =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2028-11" data-line-number="11">      conv.output      =<span class="st"> </span><span class="kw">convolution</span>(feature.map,                   </a>
<a class="sourceLine" id="cb2028-12" data-line-number="12">             <span class="dt">filter    =</span> layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-13" data-line-number="13">             <span class="dt">dw.kernel =</span> layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-14" data-line-number="14">             <span class="dt">pw.kernel =</span> layer<span class="op">$</span>pw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-15" data-line-number="15">             <span class="dt">dw.bias   =</span> layer<span class="op">$</span>dw.bias<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-16" data-line-number="16">             <span class="dt">pw.bias   =</span> layer<span class="op">$</span>pw.bias<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2028-17" data-line-number="17">             <span class="dt">bias      =</span> layer<span class="op">$</span>bias,</a>
<a class="sourceLine" id="cb2028-18" data-line-number="18">             <span class="dt">stride    =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2028-19" data-line-number="19">             <span class="dt">padding   =</span> layer<span class="op">$</span>padding,</a>
<a class="sourceLine" id="cb2028-20" data-line-number="20">             <span class="dt">dil_rate  =</span> layer<span class="op">$</span>dil_rate,</a>
<a class="sourceLine" id="cb2028-21" data-line-number="21">             <span class="dt">autopad   =</span> <span class="st">&quot;right&quot;</span>,</a>
<a class="sourceLine" id="cb2028-22" data-line-number="22">             <span class="dt">auto.pad  =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2028-23" data-line-number="23">             <span class="dt">afunc     =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb2028-24" data-line-number="24">      feature.map =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2028-25" data-line-number="25">      <span class="co"># Dropout Before Activation</span></a>
<a class="sourceLine" id="cb2028-26" data-line-number="26">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>drop) <span class="op">&amp;&amp;</span><span class="st"> </span>train<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-27" data-line-number="27">          feature.map =<span class="st"> </span><span class="kw">drop.out</span>(feature.map, layer<span class="op">$</span>drop)</a>
<a class="sourceLine" id="cb2028-28" data-line-number="28">      }</a>
<a class="sourceLine" id="cb2028-29" data-line-number="29">      <span class="co"># Activation Before Normalization</span></a>
<a class="sourceLine" id="cb2028-30" data-line-number="30">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>afunc)) {</a>
<a class="sourceLine" id="cb2028-31" data-line-number="31">        feature.map =<span class="st"> </span><span class="kw">activation</span>(feature.map, <span class="kw">get</span>(layer<span class="op">$</span>afunc))</a>
<a class="sourceLine" id="cb2028-32" data-line-number="32">      }</a>
<a class="sourceLine" id="cb2028-33" data-line-number="33">      <span class="co"># Normalization After Activation</span></a>
<a class="sourceLine" id="cb2028-34" data-line-number="34">      <span class="cf">if</span> (layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) { </a>
<a class="sourceLine" id="cb2028-35" data-line-number="35">        normalize    =<span class="st"> </span><span class="kw">normalize.forward</span>(feature.map, layer, train)</a>
<a class="sourceLine" id="cb2028-36" data-line-number="36">        feature.map  =<span class="st"> </span>normalize<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2028-37" data-line-number="37">        layer        =<span class="st"> </span>normalize<span class="op">$</span>layer</a>
<a class="sourceLine" id="cb2028-38" data-line-number="38">        layers[[L]]  =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2028-39" data-line-number="39">      }</a>
<a class="sourceLine" id="cb2028-40" data-line-number="40">      conv.output<span class="op">$</span>featuremap<span class="op">$</span>pw.fmap =<span class="st"> </span>feature.map</a>
<a class="sourceLine" id="cb2028-41" data-line-number="41">      cnn.output[[L]]   =<span class="st"> </span>conv.output<span class="op">$</span>feature.map</a>
<a class="sourceLine" id="cb2028-42" data-line-number="42"></a>
<a class="sourceLine" id="cb2028-43" data-line-number="43">      cache.output[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>bkprop.cache</a>
<a class="sourceLine" id="cb2028-44" data-line-number="44">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2028-45" data-line-number="45">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-46" data-line-number="46">      conv.output =<span class="st"> </span><span class="kw">pooling</span>(feature.map, </a>
<a class="sourceLine" id="cb2028-47" data-line-number="47">                           <span class="dt">filter  =</span> layer<span class="op">$</span>window,</a>
<a class="sourceLine" id="cb2028-48" data-line-number="48">                           <span class="dt">stride  =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2028-49" data-line-number="49">                           <span class="dt">padding =</span> layer<span class="op">$</span>padding,</a>
<a class="sourceLine" id="cb2028-50" data-line-number="50">                           <span class="dt">ptype   =</span> layer<span class="op">$</span>ptype)</a>
<a class="sourceLine" id="cb2028-51" data-line-number="51">      pmaps             =<span class="st"> </span>conv.output<span class="op">$</span>feature.map </a>
<a class="sourceLine" id="cb2028-52" data-line-number="52">      feature.map       =<span class="st"> </span>pmaps</a>
<a class="sourceLine" id="cb2028-53" data-line-number="53">      cnn.output[[L]]   =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;pw.fmap&quot;</span> =<span class="st"> </span>feature.map )</a>
<a class="sourceLine" id="cb2028-54" data-line-number="54">      cache.output[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>bkprop.cache</a>
<a class="sourceLine" id="cb2028-55" data-line-number="55">      pool.cache[[L]]   =<span class="st"> </span>conv.output<span class="op">$</span>pool.cache</a>
<a class="sourceLine" id="cb2028-56" data-line-number="56">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2028-57" data-line-number="57">    <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2028-58" data-line-number="58">      di =<span class="st"> </span><span class="kw">dim</span>(feature.map)</a>
<a class="sourceLine" id="cb2028-59" data-line-number="59">      feature.map =<span class="st">  </span><span class="kw">t</span>(<span class="kw">array</span>(feature.map, <span class="kw">c</span>( di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">3</span>], di[<span class="dv">4</span>])))</a>
<a class="sourceLine" id="cb2028-60" data-line-number="60">      <span class="cf">if</span> (layer<span class="op">$</span>init <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2028-61" data-line-number="61">          fclayers             =<span class="st"> </span>layer<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2028-62" data-line-number="62">          fclayers[[<span class="st">&quot;X&quot;</span>]]      =<span class="st"> </span>feature.map</a>
<a class="sourceLine" id="cb2028-63" data-line-number="63">          dnn =<span class="st"> </span><span class="kw">do.call</span>(deep.neural.layers, fclayers)</a>
<a class="sourceLine" id="cb2028-64" data-line-number="64">          layers[[L]]<span class="op">$</span>fc.layers =<span class="st"> </span>dnn<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2028-65" data-line-number="65">          layers[[L]]<span class="op">$</span>init      =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb2028-66" data-line-number="66">      } </a>
<a class="sourceLine" id="cb2028-67" data-line-number="67">      fc.layers =<span class="st"> </span>layers[[L]]<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2028-68" data-line-number="68">      fc.model  =<span class="st"> </span><span class="kw">forward.pass</span>(feature.map, fc.layers, <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>) </a>
<a class="sourceLine" id="cb2028-69" data-line-number="69">    }</a>
<a class="sourceLine" id="cb2028-70" data-line-number="70">  }  </a>
<a class="sourceLine" id="cb2028-71" data-line-number="71">  <span class="kw">list</span>(<span class="st">&quot;cnn.output&quot;</span>   =<span class="st"> </span>cnn.output,        <span class="st">&quot;flattened.output&quot;</span> =<span class="st"> </span>feature.map,</a>
<a class="sourceLine" id="cb2028-72" data-line-number="72">       <span class="st">&quot;cache.output&quot;</span> =<span class="st"> </span>cache.output,      <span class="st">&quot;pool.cache&quot;</span>       =<span class="st"> </span>pool.cache,</a>
<a class="sourceLine" id="cb2028-73" data-line-number="73">       <span class="st">&quot;fc.model&quot;</span>    =<span class="st"> </span>fc.model ,          <span class="st">&quot;layers&quot;</span>           =<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb2028-74" data-line-number="74">}</a></code></pre></div>

<p>Assume a tiny <span class="math inline">\(15 \times 15\)</span> image of depth 3 (RGB channels). Let us run a forward pass using the generated layers.</p>

<div class="sourceCode" id="cb2029"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2029-1" data-line-number="1">image =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dv">675</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">3</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2029-2" data-line-number="2"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 15 15  3  1</code></pre>
<div class="sourceCode" id="cb2031"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2031-1" data-line-number="1">model =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(image, cnn.layers)</a>
<a class="sourceLine" id="cb2031-2" data-line-number="2">len   =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2031-3" data-line-number="3"><span class="kw">str</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output)</a></code></pre></div>
<pre><code>##  num [1, 1:10] 0.0246 0.0714 0.0825 0.3705 0.1133 ...</code></pre>

<p>The final set of <strong>feature maps</strong> is flattened as one vector, and the vector is fed to the fully connected layer. Thus, if we have <span class="math inline">\(15 \times 15 \times 3\)</span> feature maps, all three maps are flattened sequentially into one vector.</p>
</div>
<div id="backpropagation-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.9</span> BackPropagation <a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We start the discussion of <strong>CNN backpropagation</strong> using Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnbackprop">12.36</a>. Note that the diagram uses <strong>Max Pooling</strong>. <strong>Average Pooling</strong> is another option to use.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnbackprop"></span>
<img src="cnnbackprop.png" alt="CNN (BackPropagation)" width="100%" />
<p class="caption">
Figure 12.36: CNN (BackPropagation)
</p>
</div>
<p>Here, <strong>CNN backpropagation</strong> consists of two parts. The first part performs backpropagation from a regular <strong>MLP (dense network)</strong> referred to as a fully connected neural network. The second part takes the gradient output from the <strong>dense network</strong> and uses that as input to perform the <strong>convolutional backpropagation</strong>.</p>
<p>Let us have a quick review of the first part.</p>
<p><strong>First</strong>, we take the derivative of the <strong>Loss Function</strong> with respect to the outputs in the output layer:</p>
<p><span class="math display" id="eq:equate1140167">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial o_1} = o_1 - t_1
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}}{\partial o_2} = o_2 - t_2
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\frac{\partial \mathcal{L}}{\partial o_3} = o_3 - t_3 \tag{12.178} 
\end{align}\]</span></p>
<p><strong>Second</strong>, we then calculate for the <strong>deltas</strong> (<span class="math inline">\(\delta o\)</span>):</p>
<p><span class="math display" id="eq:equate1140168">\[\begin{align}
\delta o_1 = \frac{\partial \mathcal{L}}{\partial o_1} \left( \frac{\partial o_1}{\partial \hat{o}_1}\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta o_2 = \frac{\partial \mathcal{L}}{\partial o_2} \left( \frac{\partial o_2}{\partial \hat{o}_2}\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\delta o_3 = \frac{\partial \mathcal{L}}{\partial o_3} \left( \frac{\partial o_3}{\partial \hat{o}_3}\right) \tag{12.179} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140169">\[\begin{align}
\delta_o = \sum_i \frac{\partial \mathcal{L}}{\partial o_i} \left( \frac{\partial o_i}{\partial \hat{o}_i}\right) =  \delta o_1 + \delta o_2 + \delta o_3 \tag{12.180} 
\end{align}\]</span></p>
<p><strong>Third</strong>, we calculate the derivative of the <strong>Loss Function</strong> with respect to weights:</p>
<p><span class="math display" id="eq:equate1140170">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \varphi_{1,1}} = \delta o_1 \left( \frac{\partial \hat{o}_1}{\partial \varphi_{1,1}}\right) = \delta {o_1} h_1
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}}{\partial \varphi_{2,3}} = \delta o_3 \left( \frac{\partial \hat{o}_3}{\partial \varphi_{2,3}}\right) = \delta {o_3} h_2 \tag{12.181} 
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we calculate the derivative of the <strong>Loss Function</strong> with respect to <strong>activation output</strong>:</p>
<p><span class="math display" id="eq:equate1140171">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial h_{1}} =  \sum_i 
 \left(
  \frac{\partial \mathcal{L}_i}{\partial o_i}
  \frac{\partial o_i}{\partial \hat{o}_i}
 \frac{\partial \hat{o}_i}{\partial h_1}
 \right) = \sum_i \delta o_i \varphi_{1,i} =
 \delta o_1 \varphi_{1,1} + \delta o_2 \varphi_{1,2} + \delta o_3 \varphi_{1,3} \tag{12.182} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140172">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial h_{2}} =  \sum_i 
 \left(
  \frac{\partial \mathcal{L}_i}{\partial o_i}
  \frac{\partial o_i}{\partial \hat{o}_i}
 \frac{\partial \hat{o}_i}{\partial h_2}
 \right)  = \sum_i \delta o_i \varphi_{2,i} =
 \delta o_1 \varphi_{2,1} + \delta o_2 \varphi_{2,2}  + \delta o_3 \varphi_{2,3}  \tag{12.183} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, we continue to the next hidden FC layer and perform the same, taking derivative with respect to next previous weight:</p>
<p><span class="math display" id="eq:equate1140173">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \omega_{1,1}} = \delta h_1 \left(\frac{\partial \hat{h}_1}{\partial \omega_{1,1}}\right ) = \delta h_1\ p_1 
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial \mathcal{L}}{\partial \omega_{8,2}} = \delta h_2 \left(\frac{\partial \hat{h}_2}{\partial \omega_{8,2}}\right )  = \delta h_2\ p_8  \tag{12.184} 
\end{align}\]</span></p>
<p>From:</p>
<p><span class="math display" id="eq:eqnnumber610">\[\begin{align}
\delta {h_1} = \frac{\partial \mathcal{L}}{\partial h_{1}} \frac{\partial h_1}{\partial \hat{h}_1} = \left(\sum_i \delta o_i \varphi_{1,i} \right) \left(\nabla \text{RELU} = \begin{cases} 
0 &amp; if\ h_1 \le 0 \\
1 &amp; if\ h_1 &gt; 0
\end{cases} \right) \tag{12.185}
\end{align}\]</span></p>
<p><span class="math display" id="eq:eqnnumber611">\[\begin{align}
\delta {h_2} = \frac{\partial \mathcal{L}}{\partial h_{2}} \frac{\partial h_2}{\partial \hat{h}_2} = \left(\sum_i \delta o_i \varphi_{2,i} \right)  \left(\nabla \text{RELU} = \begin{cases} 
0 &amp; if\ h_2 \le 0 \\
1 &amp; if\ h_2 &gt; 0
\end{cases} \right) \tag{12.186}
\end{align}\]</span></p>
<p><strong>Sixth</strong>, we use the flattened vector - this is the common <strong>X input</strong> as we know it in <strong>MLP</strong>. The flattened vector shows eight input features (p1, …, p8) - equivalent to (x1, …, x8). We take the derivative of the <strong>Loss function</strong> with respect to <span class="math inline">\(\mathbf{x_1}\)</span>. In our case, that is the <span class="math inline">\(\mathbf{p_1}\)</span>.</p>
<p><span class="math display" id="eq:equate1140174">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p_1} = \sum_i \left( \delta h_i\  \frac{\partial \hat{h}_i}{\partial p_1}\right)
=  \sum_i \left( \delta h_i\  \omega_{1,i} \right)
= (\delta h_1 \omega_{1,1} + \delta \hat{h}_2 \omega_{1,2} )  \tag{12.187} 
\end{align}\]</span></p>
<p><span class="math display">\[
...
\]</span></p>
<p><span class="math display" id="eq:equate1140175">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial p_8} = \sum_i \left( \delta h_i\  \frac{\partial \hat{h}_i}{\partial p_8}\right)
=  \sum_i \left( \delta h_i\  \omega_{8,i} \right)
= (\delta h_1 \omega_{8,1} + \delta \hat{h}_2 \omega_{8,2} )  \tag{12.188} 
\end{align}\]</span></p>
<p><strong>Seventh</strong>, we move to the second part of the <strong>CNN backpropagation</strong>, from the <strong>FC</strong> layer and into the <strong>Max Pooling</strong> layer. Here, we take the gradients as output from the <strong>FC layer</strong> and use them as backpropagation input to the <strong>Max Pooling</strong> layer. Each gradient output corresponds to a maximum value derived from individual <strong>Patches</strong> in subsequent previous <strong>feature maps</strong>. For example, let us consider the first gradient output from the previous layer, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial p_1}\)</span> or <span class="math inline">\(\nabla_{p_1}\mathcal{L}\)</span>. This gradient represents the effect or influence of <span class="math inline">\(\mathbf{p_1}\)</span> to the <strong>Loss function</strong>. At the same time, <span class="math inline">\(\mathbf{p_1}\)</span> also represents the maximum value, e.g., 409, derived from the <strong>1st Patch</strong> of the succeeding previous <strong>first feature map</strong> produced by the convolution with the first kernel set (still in reference to Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnbackprop">12.36</a>).</p>
<p><span class="math display" id="eq:eqnnumber612">\[\begin{align}
\left[ \begin{array}{ll}  f_1 = 307 &amp; f_2 = 226\\ f_4 = 409 &amp; f_5 = 170 \end{array}
\right]\ \ \ \ \rightarrow 
\left[ \begin{array}{ll}  \frac{\partial \mathcal{L}}{\partial f_1} = 0 &amp; \frac{\partial \mathcal{L}}{\partial f_2} = 0\\ \frac{\partial \mathcal{L}}{\partial f_4} = \frac{\partial \mathcal{L}}{\partial p_1} &amp; \frac{\partial \mathcal{L}}{\partial f_5} = 0 \end{array}
\right] \rightarrow \text{(influence to }\mathcal{L} )   \tag{12.189}
\end{align}\]</span></p>
<p>Let us take another example. We review the <strong>4th Patch</strong> of the <strong>2nd feature map</strong> produced by the convolution with the second kernel set. It has 158 as the maximum value.</p>
<p><span class="math display" id="eq:eqnnumber613">\[\begin{align}
\left[ \begin{array}{ll}  f_{14} = 4 &amp; f_{15} = 88\\ f_{17} = 140 &amp; f_{18} = 158 \end{array}
\right]\ \ \ \ \rightarrow 
\left[ \begin{array}{ll}  \frac{\partial \mathcal{L}}{\partial  f_{14}} = 0 &amp; \frac{\partial \mathcal{L}}{\partial f_{15} } = 0\\ \frac{\partial \mathcal{L}}{\partial f_{17}} = 0  &amp; \frac{\partial \mathcal{L}}{\partial f_{18}} = \frac{\partial \mathcal{L}}{\partial p_8} \end{array}
\right] \rightarrow \text{(influence to }\mathcal{L} )    \tag{12.190}
\end{align}\]</span></p>
<p>We, therefore, should be able to construct the gradients of the <strong>Loss Function</strong> with respect to all the <strong>activation outputs</strong> in the two feature maps based on the <strong>Max Pool</strong>.</p>
<p><span class="math display" id="eq:eqnnumber614">\[\begin{align}
\left[ \begin{array}{lll}  
\frac{\partial \mathcal{L}}{\partial  f_{1}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{2} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{3} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{4}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{5} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{6} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{7}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{8} } &amp;
\frac{\partial \mathcal{L}}{\partial f_{9} }\\ 
\end{array}
\right] = 
\left[ \begin{array}{lll}  
0 &amp; 0 &amp; \frac{\partial \mathcal{L}}{\partial p_{2} }\\ 
\left(
\frac{\partial \mathcal{L}}{\partial  p_{1}} + \frac{\partial \mathcal{L}}{\partial  p_{3}}\right) &amp; 0 &amp; 0\\ 
0 &amp; 0 &amp;  \frac{\partial \mathcal{L}}{\partial p_{4} }\\ 
\end{array}
\right]  \tag{12.191}
\end{align}\]</span></p>
<p>Notice that the gradients with respect to <span class="math inline">\(\mathbf{p_1}\)</span> and <span class="math inline">\(\mathbf{p_3}\)</span> are overlapping as they both claim the same maximum value for <span class="math inline">\(\mathbf{f_4}\)</span>, but each comes from separate patches. For this case of overlap, we aggregate the gradients by addition.</p>
<p>The second feature map has the following corresponding gradients.</p>
<p><span class="math display" id="eq:eqnnumber615">\[\begin{align}
\left[ \begin{array}{lll}  
\frac{\partial \mathcal{L}}{\partial  f_{10}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{11} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{12} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{13}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{14} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{15} }\\ 
\frac{\partial \mathcal{L}}{\partial  f_{16}} &amp; 
\frac{\partial \mathcal{L}}{\partial f_{17} } &amp; 
\frac{\partial \mathcal{L}}{\partial f_{18} }\\ 
\end{array}
\right] = 
\left[ \begin{array}{lll}  
0 &amp; 0 &amp; 0\\ 
0 &amp; \frac{\partial \mathcal{L}}{\partial  p_{5}}  &amp; \frac{\partial \mathcal{L}}{\partial  p_{6}} \\ 
0 &amp; \frac{\partial \mathcal{L}}{\partial  p_{7}}  &amp;  \frac{\partial \mathcal{L}}{\partial p_{8} }\\ 
\end{array}
\right]  \tag{12.192}
\end{align}\]</span></p>
<p>Overall, the individual contribution for each cell to the <strong>Loss function</strong> can be expressed in this simple equation:</p>
<p><span class="math display" id="eq:eqnnumber616">\[\begin{align}
\nabla f_i \mathcal{L} = \sum_{j}
\begin{cases}  
\frac{\partial \mathcal{L}}{\partial p_j} &amp; if\ f_i =  p_j \\
0 &amp; otherwise 
\end{cases}
\ \ \ \ \ \ \
 where\ p_j = \text{max}(\text{patch}_j\text{)}  \tag{12.193}
\end{align}\]</span></p>
<p>Our implementation of the <strong>Pooling Layer</strong> uses cached indices from the convolution operation to trace the image pixels (or neurons from previous feature maps) responsible for bearing the influence (see the use of <strong>pool.cache</strong> from <strong>convolution(.)</strong> function). Each neuron from the <strong>gradient map</strong> corresponds to a patch region in the image.</p>
<p>Now, it is important to emphasize that if we do not have a <strong>Pooling layer</strong> between a <strong>Convolutional layer</strong> and an <strong>FC layer</strong>, then the <strong>gradient</strong> with respect to each element in the <strong>flattened vector</strong> maps directly to the <strong>first previous</strong> <strong>feature map</strong> without having to deal with <strong>maximum or average values</strong>.</p>
<p>Nonetheless, the <strong>output</strong> in <strong>CNN</strong> from a <strong>convolution</strong> operation (which may include using an <strong>activation function</strong> such as <strong>RELU</strong> or <strong>Leaky RELU</strong>) maps to a cell in a <strong>feature map</strong>. Such a cell can also be regarded as <strong>neuron</strong> receiving an <strong>activation output</strong> similar to that in <strong>MLP</strong>. Here, the gradient of the <strong>Loss</strong> with respect to the <strong>activation output</strong> is backpropagated.</p>
<p><strong>Eight</strong>, we then depend on the gradients above to solve for the gradient of the <strong>Loss function</strong> with respect to the input image (or previous feature maps). Now, when dealing with <strong>backpropagation</strong>, we need to trace the influence of each component to the <strong>Loss Function</strong> from layer to layer backward. Sometimes, the needed operations are quite involved, requiring some effort. However, from time to time, a few tricks are discovered to allow certain operations to use computations that are mathematically convenient, especially, as an example, when computing for the gradients of the weights. Here, we can use <strong>Full Convolution</strong> and <span class="math inline">\(\mathbf{180^{\circ}}\)</span> matrix rotation as introduced in other literature. To illustrate, let us use Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:fullconvolv0">12.37</a>. The convolution in the figure forms a <span class="math inline">\(2 \times 2\)</span> feature map.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv0"></span>
<img src="fullconvolv0.png" alt="CNN (Full Convolution without Padding)" width="90%" />
<p class="caption">
Figure 12.37: CNN (Full Convolution without Padding)
</p>
</div>
<p>Below is the complete influence of I to individual <strong>neurons</strong> when multiplied to weights:</p>
<p><span class="math display" id="eq:eqnnumber617">\[\begin{align}
\begin{array}{ll}
f_{11} &amp;= I_{11} \times k_1 + I_{12} \times k_2  + I_{21} \times k_3  + I_{22} \times k_4 \\
f_{12} &amp;= I_{12} \times k_1 + I_{13} \times k_2  + I_{22} \times k_3  + I_{23} \times k_4 \\
f_{21} &amp;= I_{21} \times k_1 + I_{22} \times k_2  + I_{31} \times k_3  + I_{32} \times k_4 \\
f_{22} &amp;= I_{22} \times k_1 + I_{23} \times k_2  + I_{32} \times k_3  + I_{33} \times k_4 \\
\end{array}  \tag{12.194}
\end{align}\]</span></p>
<p>We can observe in the formulations above that the influence of one specific image feature, namely <span class="math inline">\(I_{11}\)</span>, only applies to <span class="math inline">\(f_{11}\)</span>. In terms of gradient, we can therefore show that as:</p>
<p><span class="math display" id="eq:equate1140176">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{11}}  = \frac{\partial \mathcal{L}}{\partial f_{11}} \frac{\partial f_{11}}{\partial I_{11}} = \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_1  \tag{12.195} 
\end{align}\]</span></p>
<p>The other image features, such as <span class="math inline">\(I_{13}\)</span>, <span class="math inline">\(I_{31}\)</span>, and <span class="math inline">\(I_{33}\)</span>, follow the same gradient formulation. Additionally, <span class="math inline">\(I_{12}\)</span> influences two neurons, and so the gradients are aggregated like so:</p>
<p><span class="math display" id="eq:equate1140177">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{12}}  = \frac{\partial \mathcal{L}}{\partial f_{11}} \frac{\partial f_{11}}{\partial I_{12}} + \frac{\partial \mathcal{L}}{\partial f_{12}} \frac{\partial f_{12}}{\partial I_{12}} = \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_2 + \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_1  \tag{12.196} 
\end{align}\]</span></p>
<p>The others follow the same formulation.</p>
<p>Now, to derive them all, we use <strong>Full Convolution</strong> for the gradients of all <strong>activation outputs</strong>, e.g. <span class="math inline">\(\frac{\partial \mathcal{L}}{\partial f_{11}}, ..., \frac{\partial \mathcal{L}}{\partial f_{18}}\)</span>, with the corresponding kernels which are rotated about <span class="math inline">\(\mathbf{180}^{\circ}\)</span>. From there, we end up with the following:</p>
<p><span class="math display" id="eq:eqnnumber618">\[\begin{align}
\begin{array}{ll}
\frac{\partial \mathcal{L}}{\partial I_{11}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_1  \\ 
\frac{\partial \mathcal{L}}{\partial I_{13}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_2  \\ 
\frac{\partial \mathcal{L}}{\partial I_{31}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_3  \\ 
\frac{\partial \mathcal{L}}{\partial I_{33}} &amp;= \frac{\partial \mathcal{L}}{\partial f_{22}} \times k_4  \\ 
\end{array}
\ \ \ \ \ \ \ \ 
\begin{array}{ll}
\frac{\partial \mathcal{L}}{\partial I_{12}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_2 + \frac{\partial \mathcal{L}}{f_{12}} \times k_1 \\ 
\frac{\partial \mathcal{L}}{\partial I_{21}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_3 + \frac{\partial \mathcal{L}}{f_{21}} \times k_1 \\ 
\frac{\partial \mathcal{L}}{\partial I_{23}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_4 + \frac{\partial \mathcal{L}}{f_{22}} \times k_2 \\ 
\frac{\partial \mathcal{L}}{\partial I_{32}} &amp;=  \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_4 + \frac{\partial \mathcal{L}}{f_{22}} \times k_3  \\ 
\end{array}  \tag{12.197}
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1140178">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial I_{22}} =  \frac{\partial \mathcal{L}}{\partial f_{11}} \times k_4 + \frac{\partial \mathcal{L}}{\partial f_{12}} \times k_3  + \frac{\partial \mathcal{L}}{\partial f_{21}} \times k_2  + \frac{\partial \mathcal{L}}{\partial f_{22}} \times k_1 \tag{12.198} 
\end{align}\]</span></p>
<p>Notice that the gradients of individual <strong>activation output</strong> with respect to image features result in individual weights:</p>
<p><span class="math display" id="eq:equate1140179">\[\begin{align}
\frac{\partial f_{11}} {\partial I_{11}} = k_1
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial f_{11}} {\partial I_{12}} =  k_2
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial f_{22}} {\partial I_{33}} = k_4 \tag{12.199} 
\end{align}\]</span></p>
<p>Equivalently, from individual neuron perspective, the gradient of an <strong>activation output</strong>, e.g. <span class="math inline">\(\mathbf{f_{11}}\)</span>, with respect to a weight, e.g. <span class="math inline">\(\mathbf{k_1}\)</span>, in the filter is:</p>
<p><span class="math display" id="eq:equate1140180">\[\begin{align}
\frac{\partial f_{11}} {\partial k_1} = I_{11}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{\partial f_{11}} {\partial k_2} = I_{12}
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
\frac{\partial f_{22}} {\partial k_4} = I_{33} \tag{12.200} 
\end{align}\]</span></p>
<p>Let us take another case in which our convolution includes padding equal to 1. See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:fullconvolv1">12.38</a>. The convolution in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:fullconvolv1">12.38</a> forms a <span class="math inline">\(4 \times 4\)</span> feature map because of the extra padding.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv1"></span>
<img src="fullconvolv1.png" alt="CNN (Full Convolution with Padding)" width="90%" />
<p class="caption">
Figure 12.38: CNN (Full Convolution with Padding)
</p>
</div>
<p>Notice this time that the <strong>Full Convolution</strong> does not require padding during backpropagation, whereas during the <strong>feedforward</strong>, we have padding set to 1.</p>
<p>Therefore, we should notice that performing a <strong>Full Convolution</strong> with a rotated kernel at <span class="math inline">\(180^\circ\)</span> requires careful tracing of the influence of every neuron and weight to the <strong>Loss Function</strong>. With the many permutations of image size, kernel size, and the number of stride and paddings, it requires a bit of heuristic iteration to arrive at the correct combination of knobs to use (in terms of stride, padding, dilation, auto padding) for our full convolution. Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:heuristicbp">12.39</a> illustrates a combination table as a result of heuristically iterating over a few adjustments of the hyperparameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heuristicbp"></span>
<img src="heuristicbp.png" alt="BackProp Combination (Heuristic)" width="70%" />
<p class="caption">
Figure 12.39: BackProp Combination (Heuristic)
</p>
</div>
<p>For example, if we use stride=1 with padding=0, e.g. 1/0, against an image of size <span class="math inline">\(4\times4\)</span> with kernel size <span class="math inline">\(3 \times 3\)</span>, e.g. 4/3, then our <strong>full convolution</strong> should use <strong>1200</strong> (stride=1, padding=2, dilated image=0, auto.pad=0). Programmatically, that is equivalent to the following code:</p>

<div class="sourceCode" id="cb2033"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2033-1" data-line-number="1"><span class="kw">convolution</span>(<span class="dt">image   =</span> Dout,   <span class="dt">filter  =</span> rotated.filters, </a>
<a class="sourceLine" id="cb2033-2" data-line-number="2">            <span class="dt">stride  =</span> <span class="dv">1</span>,      <span class="dt">padding =</span> <span class="dv">2</span>,  <span class="dt">dil_input =</span> <span class="dv">0</span>,  <span class="dt">auto.pad =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2033-3" data-line-number="3">            <span class="dt">autopad =</span> <span class="st">&quot;left&quot;</span>, <span class="dt">ptype   =</span> <span class="st">&quot;gradient.I.wrt.K&quot;</span>)</a></code></pre></div>

<p>To automatically determine the proper combination, a few patterns in the table expose the below set of formulas:</p>
<p><span class="math display" id="eq:eqnnumber619">\[\begin{align}
\begin{array}{ll}
\text{stride} &amp;= 1\\
\text{padding} &amp;= \text{max}(\text{kernel.ht} - (\text{padding} + \text{auto.pad}  + 1), 0)\\
\text{dilate}\_\text{input} &amp;= \text{stride} - 1 \\
\text{auto.pad} &amp;= \text{auto.pad} \\
\text{auto.pad} &amp;= \text{left}
\end{array}  \tag{12.201}
\end{align}\]</span></p>
<p>Note here that we can pass <strong>dilation rate</strong> (e.g., dil_input) to dilate the jacobian matrix of the gradients, and we can also pass <strong>dilation rate</strong> hyperparameter (e.g., dil_rate) to dilate receptive fields (which is always set to zero). Additionally, auto padding is done to the left and top edges.</p>
<p>Also, our brief experimentation does show that an image size of 4 and a kernel size of 3 does not produce a combination for our full convolution if our stride and paddings are set to 2, as shown in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:heuristicbp">12.39</a>. This experiment tells us that while the formulas above work in reasonably general cases, there are configurations that may not apply. Here, a decent general case implies that a kernel size of 3 and stride of 1 is a good choice versus a stride of 4, which leaves a gap between receptive fields, causing loss of information. Thus, in our later implementation of <strong>CNN</strong>, we constrain the stride size not to be equal to or greater than the kernel size. We leave readers to investigate this combination and re-evaluate the general case above.</p>
<p><strong>Ninth</strong>, in terms of solving for the gradient of the <strong>Loss function</strong> with respect to the individual weights (parameters in the kernels) in a <strong>convolutional layer</strong>, we take one of the learnable weights as an example, e.g., <span class="math inline">\(\mathbf{k_1}\)</span>, from Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnbackprop">12.36</a>. In such a case, we have the following equation:</p>
<p><span class="math display" id="eq:equate1140181">\[\begin{align}
\nabla_{k_1} \mathcal{L} = \frac{\partial \mathcal{L}} {\partial k_1} = 
\left(\frac{\partial \mathcal{L}}{\partial f_1}\right)
\left(\frac{\partial f_1} {\partial k_1}\right)   \tag{12.202} 
\end{align}\]</span></p>
<p>If an <strong>activation function</strong> such as <strong>RELU</strong> or <strong>Leaky RELU</strong> is involved, then we can also use the following equation:</p>
<p><span class="math display" id="eq:equate1140182">\[\begin{align}
\nabla_{k_1} \mathcal{L} = \frac{\partial \mathcal{L}} {\partial k_1} = 
\left(\frac{\partial \mathcal{L}}{\partial f_1}\right)
\left(\frac{\partial f_1} {\partial \hat{f_1}}\right)  
\left(\frac{\partial \hat{f_1}} {\partial k_1}\right)  \tag{12.203} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\hat{f}_i\)</span> is a <strong>net input</strong> as result of the convolution prior to invoking an <strong>activation function</strong>.</p>
<p>Here, we use <strong>convolution</strong> this time to perform backpropagation. Similar to <strong>full convolution</strong>, we need the correct knobs to use. Below is a set of formulas that can be applied in general cases.</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\text{stride} &amp;= 1\\
\text{padding} &amp;= \text{padding}\\
\text{dilate}\_\text{rate} &amp;= \text{stride} - 1\\
\text{auto.pad} &amp;= \text{auto.pad} \\
\text{autapad} &amp;= \begin{cases}
\text{right} &amp; \text{auto.pad} &gt; 0  \\
\text{left} &amp; \text{otherwise}
\end{cases}
\end{array} 
\end{align*}\]</span></p>
<p>For example, suppose we use a stride=2 and padding=0 against an image with size <span class="math inline">\(4 \times 4\)</span> and a kernel with size <span class="math inline">\(3 \times 3\)</span>, we arrive at five knobs, e.g. (stride=1, padding=0, dilate_rate=1, auto.pad = 0, autopad=right), to adjust for our backpropagation that also goes through its convolution process (which we tag it with type equal to <strong>gradient.K.wrt.I</strong> in our implementation). Programmatically, that can be written as such:</p>

<div class="sourceCode" id="cb2034"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2034-1" data-line-number="1">cv =<span class="st"> </span><span class="kw">convolution</span>(<span class="dt">image=</span>image, <span class="dt">filters=</span>Dout, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">0</span>, <span class="dt">dil_rate=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2034-2" data-line-number="2">     <span class="dt">dil_input=</span><span class="dv">0</span>, <span class="dt">auto.pad =</span> <span class="dv">1</span>, <span class="dt">autopad =</span> <span class="st">&quot;right&quot;</span>, <span class="dt">ptype=</span><span class="st">&quot;gradient.K.wrt.I&quot;</span>)</a></code></pre></div>

<p>Note here that we can pass the <strong>dilation rate</strong> parameter (e.g., dil_rate) to dilate receptive fields, and we can also pass <strong>dilation rate</strong> (e.g., dil_input) to dilate images/feature maps (which is always set to zero).</p>
<p>Let us now see our example implementation of <strong>CNN Backpropagation</strong>. For that, let us show a summarized form of backpropagation (see Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnbackpropsummary">12.40</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnbackpropsummary"></span>
<img src="cnnbackpropsummary.png" alt="CNN (Backpropagation Summary)" width="100%" />
<p class="caption">
Figure 12.40: CNN (Backpropagation Summary)
</p>
</div>

<div class="sourceCode" id="cb2035"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2035-1" data-line-number="1">get.gradient &lt;-<span class="st"> </span><span class="cf">function</span>(map, afunc) { <span class="co"># @P1@^P1</span></a>
<a class="sourceLine" id="cb2035-2" data-line-number="2">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(afunc)) {</a>
<a class="sourceLine" id="cb2035-3" data-line-number="3">        di =<span class="st"> </span><span class="kw">dim</span>(map)</a>
<a class="sourceLine" id="cb2035-4" data-line-number="4">        afunc =<span class="st"> </span><span class="kw">get</span>(afunc)</a>
<a class="sourceLine" id="cb2035-5" data-line-number="5">        gradient =<span class="st"> </span>map</a>
<a class="sourceLine" id="cb2035-6" data-line-number="6">        <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">4</span>])</a>
<a class="sourceLine" id="cb2035-7" data-line-number="7">        <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2035-8" data-line-number="8">            gradient[,,d,s] =<span class="st">  </span><span class="kw">gradient.activation</span>(map[,,d,s], afunc) </a>
<a class="sourceLine" id="cb2035-9" data-line-number="9">        }</a>
<a class="sourceLine" id="cb2035-10" data-line-number="10">        gradient =<span class="st"> </span><span class="kw">array</span>(gradient, di) </a>
<a class="sourceLine" id="cb2035-11" data-line-number="11">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2035-12" data-line-number="12">        gradient =<span class="st"> </span>map  </a>
<a class="sourceLine" id="cb2035-13" data-line-number="13">    } </a>
<a class="sourceLine" id="cb2035-14" data-line-number="14">    gradient  </a>
<a class="sourceLine" id="cb2035-15" data-line-number="15">}</a></code></pre></div>
<div class="sourceCode" id="cb2036"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2036-1" data-line-number="1">back.propagation.cnn &lt;-<span class="cf">function</span> (X, Y, model) {</a>
<a class="sourceLine" id="cb2036-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2036-3" data-line-number="3">    layers    =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2036-4" data-line-number="4">    H         =<span class="st"> </span><span class="kw">length</span>(layers) <span class="op">-</span><span class="st"> </span><span class="dv">1</span> <span class="co">#remove FC layer and focus on CONV layers</span></a>
<a class="sourceLine" id="cb2036-5" data-line-number="5">    fc.layers =<span class="st"> </span>model<span class="op">$</span>fc.model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2036-6" data-line-number="6">    backprop  =<span class="st"> </span><span class="kw">back.propagation</span>(model<span class="op">$</span>flattened.output, Y, model<span class="op">$</span>fc.model, </a>
<a class="sourceLine" id="cb2036-7" data-line-number="7">                                       <span class="dt">afunc=</span><span class="st">&quot;softmax&quot;</span>) </a>
<a class="sourceLine" id="cb2036-8" data-line-number="8">    Dout      =<span class="st"> </span>backprop<span class="op">$</span>delta.output[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb2036-9" data-line-number="9">    di        =<span class="st"> </span><span class="kw">dim</span>(model<span class="op">$</span>cnn.output[[H]]<span class="op">$</span>pw.fmap)  </a>
<a class="sourceLine" id="cb2036-10" data-line-number="10">    gradient.loss =<span class="st"> </span>Dout <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(fc.layers[[<span class="dv">1</span>]]<span class="op">$</span>omega<span class="op">$</span>weight) <span class="co"># @L/@P1</span></a>
<a class="sourceLine" id="cb2036-11" data-line-number="11">    gradient.loss =<span class="st"> </span>gradient.loss[,<span class="op">-</span><span class="dv">1</span>] <span class="co">#remove bias</span></a>
<a class="sourceLine" id="cb2036-12" data-line-number="12"></a>
<a class="sourceLine" id="cb2036-13" data-line-number="13">    loss.I           =<span class="st"> </span><span class="kw">array</span>(<span class="kw">t</span>(gradient.loss), di) </a>
<a class="sourceLine" id="cb2036-14" data-line-number="14">    delta.normparams =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-15" data-line-number="15">    delta.dws        =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-16" data-line-number="16">    delta.pws        =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-17" data-line-number="17">    delta.dw.biases  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-18" data-line-number="18">    delta.pw.biases  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2036-19" data-line-number="19">    <span class="cf">for</span> (L <span class="cf">in</span> H<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-20" data-line-number="20">        layer     =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2036-21" data-line-number="21">        cache     =<span class="st"> </span>model<span class="op">$</span>cache.output[[L]]</a>
<a class="sourceLine" id="cb2036-22" data-line-number="22">        cur.fmaps =<span class="st"> </span>model<span class="op">$</span>cnn.output[[L]]</a>
<a class="sourceLine" id="cb2036-23" data-line-number="23">        <span class="cf">if</span> (L <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-24" data-line-number="24">            prev.fmaps =<span class="st"> </span>model<span class="op">$</span>cnn.output[[L<span class="dv">-1</span>]]<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2036-25" data-line-number="25">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2036-26" data-line-number="26">        <span class="cf">if</span> (L<span class="op">==</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2036-27" data-line-number="27">            prev.fmaps =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2036-28" data-line-number="28">        }</a>
<a class="sourceLine" id="cb2036-29" data-line-number="29">        <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;pooling&quot;</span>) {</a>
<a class="sourceLine" id="cb2036-30" data-line-number="30">            conv.output =<span class="st"> </span><span class="kw">pool.backprop</span>(prev.fmaps, </a>
<a class="sourceLine" id="cb2036-31" data-line-number="31">                           <span class="dt">filter  =</span> layer<span class="op">$</span>window,</a>
<a class="sourceLine" id="cb2036-32" data-line-number="32">                           <span class="dt">Dout    =</span> loss.I,</a>
<a class="sourceLine" id="cb2036-33" data-line-number="33">                           <span class="dt">stride  =</span> layer<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2036-34" data-line-number="34">                           <span class="dt">pool.cache =</span> model<span class="op">$</span>pool.cache[[L]],</a>
<a class="sourceLine" id="cb2036-35" data-line-number="35">                           <span class="dt">ptype   =</span> <span class="st">&quot;gradient.I.wrt.P&quot;</span>,</a>
<a class="sourceLine" id="cb2036-36" data-line-number="36">                           <span class="dt">pool    =</span> layer<span class="op">$</span>ptype)</a>
<a class="sourceLine" id="cb2036-37" data-line-number="37">           loss.I             =<span class="st"> </span>conv.output<span class="op">$</span>feature.map </a>
<a class="sourceLine" id="cb2036-38" data-line-number="38">        } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2036-39" data-line-number="39">        <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) { </a>
<a class="sourceLine" id="cb2036-40" data-line-number="40">           <span class="co"># activation gradient first before normalization</span></a>
<a class="sourceLine" id="cb2036-41" data-line-number="41">           <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(layer<span class="op">$</span>afunc)) { </a>
<a class="sourceLine" id="cb2036-42" data-line-number="42">                gradient.I =<span class="st"> </span><span class="kw">get.gradient</span>(cur.fmaps<span class="op">$</span>pw.fmap, layer<span class="op">$</span>afunc)</a>
<a class="sourceLine" id="cb2036-43" data-line-number="43">                Dout       =<span class="st"> </span>loss.I <span class="op">*</span><span class="st"> </span>gradient.I </a>
<a class="sourceLine" id="cb2036-44" data-line-number="44">            } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2036-45" data-line-number="45">                Dout       =<span class="st"> </span>loss.I</a>
<a class="sourceLine" id="cb2036-46" data-line-number="46">            }</a>
<a class="sourceLine" id="cb2036-47" data-line-number="47">            <span class="cf">if</span> (layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2036-48" data-line-number="48">                normalize             =<span class="st"> </span><span class="kw">normalize.backward</span>(Dout, layer)</a>
<a class="sourceLine" id="cb2036-49" data-line-number="49">                Dout                  =<span class="st"> </span>normalize<span class="op">$</span>gradient.loss</a>
<a class="sourceLine" id="cb2036-50" data-line-number="50">                delta.normparams[[L]] =<span class="st"> </span>normalize<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2036-51" data-line-number="51">            }</a>
<a class="sourceLine" id="cb2036-52" data-line-number="52"></a>
<a class="sourceLine" id="cb2036-53" data-line-number="53">            conv.output   =<span class="st"> </span><span class="kw">convolution</span>(prev.fmaps, </a>
<a class="sourceLine" id="cb2036-54" data-line-number="54">                        <span class="dt">filter    =</span> Dout, <span class="co"># used only to calculate shape</span></a>
<a class="sourceLine" id="cb2036-55" data-line-number="55">                        <span class="dt">Dout      =</span> Dout, </a>
<a class="sourceLine" id="cb2036-56" data-line-number="56">                        <span class="dt">cur.fmaps =</span> cur.fmaps,</a>
<a class="sourceLine" id="cb2036-57" data-line-number="57">                        <span class="dt">pw.kernel =</span> layer<span class="op">$</span>pw.kernel<span class="op">$</span>weight,</a>
<a class="sourceLine" id="cb2036-58" data-line-number="58">                        <span class="dt">stride    =</span> cache<span class="op">$</span>wstride, </a>
<a class="sourceLine" id="cb2036-59" data-line-number="59">                        <span class="dt">padding   =</span> cache<span class="op">$</span>wpadding,  </a>
<a class="sourceLine" id="cb2036-60" data-line-number="60">                        <span class="dt">dil_rate  =</span> cache<span class="op">$</span>wdil_rate,</a>
<a class="sourceLine" id="cb2036-61" data-line-number="61">                        <span class="dt">dil_input =</span> cache<span class="op">$</span>wdil_input,</a>
<a class="sourceLine" id="cb2036-62" data-line-number="62">                        <span class="dt">auto.pad  =</span> cache<span class="op">$</span>wauto.pad,</a>
<a class="sourceLine" id="cb2036-63" data-line-number="63">                        <span class="dt">autopad   =</span> cache<span class="op">$</span>wautopad,</a>
<a class="sourceLine" id="cb2036-64" data-line-number="64">                        <span class="dt">ptype     =</span> <span class="st">&quot;gradient.K.wrt.I&quot;</span>)</a>
<a class="sourceLine" id="cb2036-65" data-line-number="65">            delta.dws[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>delta.dw</a>
<a class="sourceLine" id="cb2036-66" data-line-number="66">            delta.pws[[L]] =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>delta.pw</a>
<a class="sourceLine" id="cb2036-67" data-line-number="67">            dw.Dout        =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>dw.Dout</a>
<a class="sourceLine" id="cb2036-68" data-line-number="68">            pw.Dout        =<span class="st"> </span>conv.output<span class="op">$</span>feature.map<span class="op">$</span>pw.Dout</a>
<a class="sourceLine" id="cb2036-69" data-line-number="69">            </a>
<a class="sourceLine" id="cb2036-70" data-line-number="70">            <span class="co"># Compute Biases</span></a>
<a class="sourceLine" id="cb2036-71" data-line-number="71">            di.dw          =<span class="st"> </span><span class="kw">dim</span>(dw.Dout)</a>
<a class="sourceLine" id="cb2036-72" data-line-number="72">            di.pw          =<span class="st"> </span><span class="kw">dim</span>(pw.Dout)</a>
<a class="sourceLine" id="cb2036-73" data-line-number="73">            delta.dw.bias  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, di.dw[<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb2036-74" data-line-number="74">            delta.pw.bias  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, di.pw[<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb2036-75" data-line-number="75">            <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.dw[<span class="dv">3</span>]) { delta.dw.bias[d] =<span class="st"> </span><span class="kw">sum</span>(dw.Dout[,,d,]) }</a>
<a class="sourceLine" id="cb2036-76" data-line-number="76">            <span class="cf">for</span> (f <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.pw[<span class="dv">3</span>]) { delta.pw.bias[f] =<span class="st"> </span><span class="kw">sum</span>(pw.Dout[,,f,]) }</a>
<a class="sourceLine" id="cb2036-77" data-line-number="77"></a>
<a class="sourceLine" id="cb2036-78" data-line-number="78">            delta.dw.biases[[L]] =<span class="st"> </span>delta.dw.bias</a>
<a class="sourceLine" id="cb2036-79" data-line-number="79">            delta.pw.biases[[L]] =<span class="st"> </span>delta.pw.bias</a>
<a class="sourceLine" id="cb2036-80" data-line-number="80"></a>
<a class="sourceLine" id="cb2036-81" data-line-number="81">            rotated.kernel =<span class="st"> </span><span class="kw">rotate.matrix.180</span>(layer<span class="op">$</span>dw.kernel<span class="op">$</span>weight)</a>
<a class="sourceLine" id="cb2036-82" data-line-number="82">            conv.output   =<span class="st"> </span><span class="kw">convolution</span>(dw.Dout, </a>
<a class="sourceLine" id="cb2036-83" data-line-number="83">                        <span class="dt">filter    =</span> rotated.kernel, </a>
<a class="sourceLine" id="cb2036-84" data-line-number="84">                        <span class="dt">stride    =</span> cache<span class="op">$</span>stride, </a>
<a class="sourceLine" id="cb2036-85" data-line-number="85">                        <span class="dt">padding   =</span> cache<span class="op">$</span>padding,  </a>
<a class="sourceLine" id="cb2036-86" data-line-number="86">                        <span class="dt">dil_rate  =</span> cache<span class="op">$</span>dil_rate,</a>
<a class="sourceLine" id="cb2036-87" data-line-number="87">                        <span class="dt">dil_input =</span> cache<span class="op">$</span>dil_input,</a>
<a class="sourceLine" id="cb2036-88" data-line-number="88">                        <span class="dt">auto.pad  =</span> cache<span class="op">$</span>auto.pad,</a>
<a class="sourceLine" id="cb2036-89" data-line-number="89">                        <span class="dt">autopad   =</span> cache<span class="op">$</span>autopad,</a>
<a class="sourceLine" id="cb2036-90" data-line-number="90">                        <span class="dt">ptype     =</span> <span class="st">&quot;gradient.I.wrt.K&quot;</span>)</a>
<a class="sourceLine" id="cb2036-91" data-line-number="91">            loss.I =<span class="st"> </span>conv.output<span class="op">$</span>feature.map             </a>
<a class="sourceLine" id="cb2036-92" data-line-number="92">        }</a>
<a class="sourceLine" id="cb2036-93" data-line-number="93">    } </a>
<a class="sourceLine" id="cb2036-94" data-line-number="94">    <span class="kw">list</span>(<span class="st">&quot;fc.delta.params&quot;</span>      =<span class="st"> </span>backprop<span class="op">$</span>delta.params,</a>
<a class="sourceLine" id="cb2036-95" data-line-number="95">         <span class="st">&quot;cnn.delta.normparams&quot;</span> =<span class="st"> </span>delta.normparams,</a>
<a class="sourceLine" id="cb2036-96" data-line-number="96">         <span class="st">&quot;cnn.delta.dws&quot;</span>        =<span class="st"> </span>delta.dws,</a>
<a class="sourceLine" id="cb2036-97" data-line-number="97">         <span class="st">&quot;cnn.delta.pws&quot;</span>        =<span class="st"> </span>delta.pws,</a>
<a class="sourceLine" id="cb2036-98" data-line-number="98">         <span class="st">&quot;cnn.delta.dw.biases&quot;</span>  =<span class="st"> </span>delta.dw.biases,</a>
<a class="sourceLine" id="cb2036-99" data-line-number="99">         <span class="st">&quot;cnn.delta.pw.biases&quot;</span>  =<span class="st"> </span>delta.pw.biases)</a>
<a class="sourceLine" id="cb2036-100" data-line-number="100">}</a></code></pre></div>

<p>Our implementation of <strong>full convolution</strong> rotates the filter. Below is a script to perform matrix rotation.</p>

<div class="sourceCode" id="cb2037"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2037-1" data-line-number="1">rotate<span class="fl">.90</span> &lt;-<span class="st"> </span><span class="cf">function</span>(filters) {</a>
<a class="sourceLine" id="cb2037-2" data-line-number="2">    len =<span class="st"> </span><span class="kw">length</span>(filters)</a>
<a class="sourceLine" id="cb2037-3" data-line-number="3">    <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2037-4" data-line-number="4">       a =<span class="st"> </span>filters[[l]]</a>
<a class="sourceLine" id="cb2037-5" data-line-number="5">       dl =<span class="st"> </span><span class="kw">dim</span>(a); h =<span class="st"> </span>dl[<span class="dv">1</span>]; w =<span class="st"> </span>dl[<span class="dv">2</span>]; d =<span class="st"> </span>dl[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2037-6" data-line-number="6">       mx =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2037-7" data-line-number="7">       <span class="cf">for</span> (depth <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>d) {</a>
<a class="sourceLine" id="cb2037-8" data-line-number="8">         m =<span class="st"> </span>a[,,depth]</a>
<a class="sourceLine" id="cb2037-9" data-line-number="9">         mx =<span class="st"> </span><span class="kw">cbind</span>(mx, <span class="kw">t</span>(m[<span class="kw">nrow</span>(m)<span class="op">:</span><span class="dv">1</span>,,<span class="dt">drop =</span> <span class="ot">FALSE</span>]))</a>
<a class="sourceLine" id="cb2037-10" data-line-number="10">       }</a>
<a class="sourceLine" id="cb2037-11" data-line-number="11">       filters[[l]] =<span class="st"> </span><span class="kw">array</span>(mx, <span class="kw">c</span>(h, w, d))</a>
<a class="sourceLine" id="cb2037-12" data-line-number="12">    }</a>
<a class="sourceLine" id="cb2037-13" data-line-number="13">    filters</a>
<a class="sourceLine" id="cb2037-14" data-line-number="14">}</a></code></pre></div>
<div class="sourceCode" id="cb2038"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2038-1" data-line-number="1">rotate<span class="fl">.180</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) { <span class="kw">rotate.90</span>(<span class="kw">rotate.90</span>(a)) }</a>
<a class="sourceLine" id="cb2038-2" data-line-number="2">rotate.matrix<span class="fl">.90</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) {</a>
<a class="sourceLine" id="cb2038-3" data-line-number="3">  dl =<span class="st"> </span><span class="kw">dim</span>(a)</a>
<a class="sourceLine" id="cb2038-4" data-line-number="4">  mx =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2038-5" data-line-number="5">  <span class="cf">for</span> (depth <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>dl[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2038-6" data-line-number="6">    m =<span class="st"> </span>a[,,depth]</a>
<a class="sourceLine" id="cb2038-7" data-line-number="7">    mx =<span class="st"> </span><span class="kw">cbind</span>(mx, <span class="kw">t</span>(m[<span class="kw">nrow</span>(m)<span class="op">:</span><span class="dv">1</span>,,<span class="dt">drop =</span> <span class="ot">FALSE</span>]))</a>
<a class="sourceLine" id="cb2038-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb2038-9" data-line-number="9">  <span class="kw">array</span>(mx, dl)</a>
<a class="sourceLine" id="cb2038-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb2038-11" data-line-number="11">rotate.matrix<span class="fl">.180</span> &lt;-<span class="st"> </span><span class="cf">function</span>(a) {</a>
<a class="sourceLine" id="cb2038-12" data-line-number="12">    <span class="kw">rotate.matrix.90</span>(<span class="kw">rotate.matrix.90</span>(a)) </a>
<a class="sourceLine" id="cb2038-13" data-line-number="13">}</a></code></pre></div>

</div>
<div id="optimization-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.10</span> Optimization<a href="12.4-convolutional-neural-network-cnn.html#optimization-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Finally</strong>, reviewing Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnbackprop">12.36</a> once again, we come down to the update rules. For the <strong>Backward Pass</strong>, let us use one of the kernel weights, e.g. <span class="math inline">\(k_1\)</span>, to illustrate.</p>
<p><span class="math display" id="eq:equate1140183">\[\begin{align}
k_1 = k_1 - \eta \times \nabla k_1 \mathcal{L}
\ \ \ \ \ \ \ 
...
\ \ \ \ \ \ \ 
k_4 = k_4 - \eta \times \nabla k_4 \mathcal{L} \tag{12.204} 
\end{align}\]</span></p>
<p>Our implementation of the backward pass is encapsulated into the <strong>optimizer(.)</strong> function. Recall our discussion of <strong>optimization</strong> in <strong>MLP</strong> section. Here, we implement a few of the known optimizers. See below:</p>

<div class="sourceCode" id="cb2039"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2039-1" data-line-number="1">optimizer   &lt;-<span class="st"> </span><span class="cf">function</span>(backprop, layers, <span class="dt">optimize=</span><span class="st">&quot;momentum&quot;</span>, t, eta) {</a>
<a class="sourceLine" id="cb2039-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2039-3" data-line-number="3">  optimize.sgd =<span class="st"> </span>sgd &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-4" data-line-number="4">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-5" data-line-number="5">    param</a>
<a class="sourceLine" id="cb2039-6" data-line-number="6">  }</a>
<a class="sourceLine" id="cb2039-7" data-line-number="7">         </a>
<a class="sourceLine" id="cb2039-8" data-line-number="8">  optimize.momentum =<span class="st"> </span>momentum &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-9" data-line-number="9">    gamma        =<span class="st"> </span><span class="fl">0.90</span></a>
<a class="sourceLine" id="cb2039-10" data-line-number="10">    param<span class="op">$</span>nu     =<span class="st"> </span>gamma <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-11" data-line-number="11">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>param<span class="op">$</span>nu</a>
<a class="sourceLine" id="cb2039-12" data-line-number="12">    param</a>
<a class="sourceLine" id="cb2039-13" data-line-number="13">  }</a>
<a class="sourceLine" id="cb2039-14" data-line-number="14">                  </a>
<a class="sourceLine" id="cb2039-15" data-line-number="15">  optimize.rmsprop =<span class="st"> </span>rmsprop &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, <span class="dt">t =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2039-16" data-line-number="16">    beta1 =<span class="st"> </span><span class="fl">0.90</span> ; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-17" data-line-number="17">    param<span class="op">$</span>nu     =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-18" data-line-number="18">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(param<span class="op">$</span>nu) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2039-19" data-line-number="19">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-20" data-line-number="20">    param</a>
<a class="sourceLine" id="cb2039-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb2039-22" data-line-number="22">         </a>
<a class="sourceLine" id="cb2039-23" data-line-number="23">optimize.adam =<span class="st"> </span>adam &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-24" data-line-number="24">    beta1 =<span class="st"> </span><span class="fl">0.90</span>; beta2 =<span class="st"> </span><span class="fl">0.999</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-25" data-line-number="25">    param<span class="op">$</span>rho    =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2039-26" data-line-number="26">    param<span class="op">$</span>nu     =<span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-27" data-line-number="27">    rho.hat      =<span class="st"> </span>param<span class="op">$</span>rho <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2039-28" data-line-number="28">    nu.hat       =<span class="st"> </span>param<span class="op">$</span>nu <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2039-29" data-line-number="29">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(nu.hat) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2039-30" data-line-number="30">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>rho.hat</a>
<a class="sourceLine" id="cb2039-31" data-line-number="31">    param</a>
<a class="sourceLine" id="cb2039-32" data-line-number="32">}</a>
<a class="sourceLine" id="cb2039-33" data-line-number="33">             </a>
<a class="sourceLine" id="cb2039-34" data-line-number="34">  optimize.adadelta =<span class="st"> </span>adadelta &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-35" data-line-number="35">    rho =<span class="st"> </span><span class="fl">0.90</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2039-36" data-line-number="36">    param<span class="op">$</span>rho    =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-37" data-line-number="37">    phi          =<span class="st"> </span><span class="kw">sqrt</span>(param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>eps) <span class="op">*</span><span class="st"> </span>(gradient <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>eps))</a>
<a class="sourceLine" id="cb2039-38" data-line-number="38">    param<span class="op">$</span>nu     =<span class="st"> </span>rho <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho) <span class="op">*</span><span class="st"> </span>phi<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2039-39" data-line-number="39">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi</a>
<a class="sourceLine" id="cb2039-40" data-line-number="40">    param</a>
<a class="sourceLine" id="cb2039-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb2039-42" data-line-number="42"></a>
<a class="sourceLine" id="cb2039-43" data-line-number="43">  optimizing &lt;-<span class="st"> </span><span class="cf">function</span>(func, param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2039-44" data-line-number="44">      func =<span class="st"> </span><span class="kw">get</span>(func)</a>
<a class="sourceLine" id="cb2039-45" data-line-number="45">      <span class="kw">func</span>(param, gradient, eta, t)</a>
<a class="sourceLine" id="cb2039-46" data-line-number="46">  }</a>
<a class="sourceLine" id="cb2039-47" data-line-number="47">    </a>
<a class="sourceLine" id="cb2039-48" data-line-number="48">  delta.params     =<span class="st"> </span>backprop<span class="op">$</span>fc.delta.params</a>
<a class="sourceLine" id="cb2039-49" data-line-number="49">  delta.normparams =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.normparams</a>
<a class="sourceLine" id="cb2039-50" data-line-number="50">  delta.dws        =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.dws</a>
<a class="sourceLine" id="cb2039-51" data-line-number="51">  delta.pws        =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.pws</a>
<a class="sourceLine" id="cb2039-52" data-line-number="52">  delta.dw.biases  =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.dw.biases</a>
<a class="sourceLine" id="cb2039-53" data-line-number="53">  delta.pw.biases  =<span class="st"> </span>backprop<span class="op">$</span>cnn.delta.pw.biases</a>
<a class="sourceLine" id="cb2039-54" data-line-number="54">  H =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2039-55" data-line-number="55"></a>
<a class="sourceLine" id="cb2039-56" data-line-number="56">  <span class="cf">for</span> (L <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>H) {</a>
<a class="sourceLine" id="cb2039-57" data-line-number="57">     layer =<span class="st"> </span>layers[[L]]</a>
<a class="sourceLine" id="cb2039-58" data-line-number="58">     <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;dense&quot;</span>) {</a>
<a class="sourceLine" id="cb2039-59" data-line-number="59">        fc.layers =<span class="st"> </span>layer<span class="op">$</span>fc.layers</a>
<a class="sourceLine" id="cb2039-60" data-line-number="60">        len  =<span class="st"> </span><span class="kw">length</span>(delta.params)</a>
<a class="sourceLine" id="cb2039-61" data-line-number="61">        <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2039-62" data-line-number="62">            omega =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>omega, </a>
<a class="sourceLine" id="cb2039-63" data-line-number="63">                               delta.params[[l]]<span class="op">$</span>omega, eta, t)</a>
<a class="sourceLine" id="cb2039-64" data-line-number="64">            layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>omega =<span class="st"> </span>omega</a>
<a class="sourceLine" id="cb2039-65" data-line-number="65">            </a>
<a class="sourceLine" id="cb2039-66" data-line-number="66">            <span class="cf">if</span> (fc.layers[[l]]<span class="op">$</span>batchnorm <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb2039-67" data-line-number="67">              batch.gamma =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>batch.gamma, </a>
<a class="sourceLine" id="cb2039-68" data-line-number="68">                               delta.params[[l]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb2039-69" data-line-number="69">              batch.beta =<span class="st"> </span><span class="kw">optimizing</span>(optimize, fc.layers[[l]]<span class="op">$</span>batch.beta, </a>
<a class="sourceLine" id="cb2039-70" data-line-number="70">                               delta.params[[l]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb2039-71" data-line-number="71">              layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>batch.gamma =<span class="st"> </span>batch.gamma</a>
<a class="sourceLine" id="cb2039-72" data-line-number="72">              layer<span class="op">$</span>fc.layers[[l]]<span class="op">$</span>batch.beta =<span class="st"> </span>batch.beta</a>
<a class="sourceLine" id="cb2039-73" data-line-number="73">            }</a>
<a class="sourceLine" id="cb2039-74" data-line-number="74">        }</a>
<a class="sourceLine" id="cb2039-75" data-line-number="75">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2039-76" data-line-number="76">     <span class="cf">if</span> (layer<span class="op">$</span>istype <span class="op">==</span><span class="st"> &quot;convolv&quot;</span>) {</a>
<a class="sourceLine" id="cb2039-77" data-line-number="77">        delta.dw        =<span class="st"> </span>delta.dws[[L]]</a>
<a class="sourceLine" id="cb2039-78" data-line-number="78">        delta.pw        =<span class="st"> </span>delta.pws[[L]]</a>
<a class="sourceLine" id="cb2039-79" data-line-number="79">        delta.dw.bias   =<span class="st"> </span>delta.dw.biases[[L]]</a>
<a class="sourceLine" id="cb2039-80" data-line-number="80">        delta.pw.bias   =<span class="st"> </span>delta.pw.biases[[L]]</a>
<a class="sourceLine" id="cb2039-81" data-line-number="81"></a>
<a class="sourceLine" id="cb2039-82" data-line-number="82">        layer<span class="op">$</span>dw.kernel =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>dw.kernel, </a>
<a class="sourceLine" id="cb2039-83" data-line-number="83">                                     delta.dw, eta, t)</a>
<a class="sourceLine" id="cb2039-84" data-line-number="84">        layer<span class="op">$</span>pw.kernel =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>pw.kernel, </a>
<a class="sourceLine" id="cb2039-85" data-line-number="85">                                     delta.pw, eta, t)</a>
<a class="sourceLine" id="cb2039-86" data-line-number="86">        layer<span class="op">$</span>dw.bias   =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>dw.bias, </a>
<a class="sourceLine" id="cb2039-87" data-line-number="87">                                     delta.dw.bias, eta, t)</a>
<a class="sourceLine" id="cb2039-88" data-line-number="88">        layer<span class="op">$</span>pw.bias   =<span class="st"> </span><span class="kw">optimizing</span>(optimize, layer<span class="op">$</span>pw.bias, </a>
<a class="sourceLine" id="cb2039-89" data-line-number="89">                                     delta.pw.bias, eta, t)</a>
<a class="sourceLine" id="cb2039-90" data-line-number="90"></a>
<a class="sourceLine" id="cb2039-91" data-line-number="91">        <span class="cf">if</span> (layers[[L]]<span class="op">$</span>batchnorm<span class="op">$</span>normalize <span class="op">!=</span><span class="st"> </span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2039-92" data-line-number="92">          batchnorm          =<span class="st"> </span>layer<span class="op">$</span>batchnorm</a>
<a class="sourceLine" id="cb2039-93" data-line-number="93">          batchnorm<span class="op">$</span>gamma =<span class="st"> </span><span class="kw">optimizing</span>(optimize, batchnorm<span class="op">$</span>gamma, </a>
<a class="sourceLine" id="cb2039-94" data-line-number="94">                                       delta.normparams[[L]]<span class="op">$</span>gamma, eta, t)</a>
<a class="sourceLine" id="cb2039-95" data-line-number="95">          batchnorm<span class="op">$</span>beta  =<span class="st"> </span><span class="kw">optimizing</span>(optimize, batchnorm<span class="op">$</span>beta, </a>
<a class="sourceLine" id="cb2039-96" data-line-number="96">                                       delta.normparams[[L]]<span class="op">$</span>beta, eta, t)</a>
<a class="sourceLine" id="cb2039-97" data-line-number="97">          layer<span class="op">$</span>batchnorm =<span class="st"> </span>batchnorm</a>
<a class="sourceLine" id="cb2039-98" data-line-number="98">        }</a>
<a class="sourceLine" id="cb2039-99" data-line-number="99">     } </a>
<a class="sourceLine" id="cb2039-100" data-line-number="100">     layers[[L]] =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2039-101" data-line-number="101">  } </a>
<a class="sourceLine" id="cb2039-102" data-line-number="102">  layers</a>
<a class="sourceLine" id="cb2039-103" data-line-number="103">}</a></code></pre></div>

</div>
<div id="normalization" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.11</span> Normalization<a href="12.4-convolutional-neural-network-cnn.html#normalization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>MLP</strong> section, we introduced the concept of <strong>Batch normalization</strong>, accompanied by the math behind it and its implementation. In this section, we continue to show <strong>Batch normalization</strong> for <strong>CNN</strong>, with a brief introduction to the concept of <strong>Layer normalization</strong>. While <strong>Layer normalization</strong> may not be as common in usage for <strong>CNN</strong>, it helps show that the idea is identical. Furthermore, the implementation is the same. In <strong>Batch normalization</strong>, we group the operation as a Depthwise operation. <strong>Layer normalization</strong> groups the operation as a Sample-wise operation.</p>

<div class="sourceCode" id="cb2040"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2040-1" data-line-number="1">normalize.forward &lt;-<span class="st"> </span><span class="cf">function</span>(H, layer, train, <span class="dt">eps=</span><span class="fl">1e-10</span>, <span class="dt">momentum =</span> <span class="fl">0.90</span>) {</a>
<a class="sourceLine" id="cb2040-2" data-line-number="2">    ntype             =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize</a>
<a class="sourceLine" id="cb2040-3" data-line-number="3">    gamma             =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2040-4" data-line-number="4">    beta              =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>beta<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2040-5" data-line-number="5">    moving.mu         =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.mu</a>
<a class="sourceLine" id="cb2040-6" data-line-number="6">    moving.variance   =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.var </a>
<a class="sourceLine" id="cb2040-7" data-line-number="7">    <span class="cf">if</span> (ntype <span class="op">==</span><span class="st"> &quot;layer&quot;</span>) { </a>
<a class="sourceLine" id="cb2040-8" data-line-number="8">        <span class="co"># Layer normalization ( Height, Width, Depth, Sample)</span></a>
<a class="sourceLine" id="cb2040-9" data-line-number="9">        <span class="co"># normalize across Samples per Layer</span></a>
<a class="sourceLine" id="cb2040-10" data-line-number="10">        mu     =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">4</span>, mean)</a>
<a class="sourceLine" id="cb2040-11" data-line-number="11">        H.mu   =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">4</span>, mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-12" data-line-number="12">        var    =<span class="st"> </span><span class="kw">apply</span>(H.mu<span class="op">^</span><span class="dv">2</span>, <span class="dv">4</span>, mean)</a>
<a class="sourceLine" id="cb2040-13" data-line-number="13">        istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-14" data-line-number="14">        H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">4</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-15" data-line-number="15">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2040-16" data-line-number="16">    <span class="cf">if</span> (ntype <span class="op">==</span><span class="st"> &quot;batch&quot;</span>) {</a>
<a class="sourceLine" id="cb2040-17" data-line-number="17">        <span class="co"># Batch normalization ( Height, Width, Depth, Sample)</span></a>
<a class="sourceLine" id="cb2040-18" data-line-number="18">        <span class="co"># normalize across Depth per Layer</span></a>
<a class="sourceLine" id="cb2040-19" data-line-number="19">        <span class="cf">if</span> (train <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2040-20" data-line-number="20">            mu     =<span class="st"> </span><span class="kw">apply</span>(H, <span class="dv">3</span>, mean)</a>
<a class="sourceLine" id="cb2040-21" data-line-number="21">            H.mu   =<span class="st"> </span><span class="kw">sweep</span>(H, <span class="dv">3</span>, mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-22" data-line-number="22">            var    =<span class="st"> </span><span class="kw">apply</span>(H.mu<span class="op">^</span><span class="dv">2</span>, <span class="dv">3</span>, mean)</a>
<a class="sourceLine" id="cb2040-23" data-line-number="23">            istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-24" data-line-number="24">            H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">3</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-25" data-line-number="25">            <span class="co"># We can use moving average &amp; variance for this normalization</span></a>
<a class="sourceLine" id="cb2040-26" data-line-number="26">            <span class="co"># because normalization is across the entire mini-batch.</span></a>
<a class="sourceLine" id="cb2040-27" data-line-number="27">            moving.mu       =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.mu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>momentum) <span class="op">*</span><span class="st"> </span>mu</a>
<a class="sourceLine" id="cb2040-28" data-line-number="28">            moving.variance =<span class="st"> </span>momentum <span class="op">*</span><span class="st"> </span>moving.variance <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>momentum)<span class="op">*</span>var</a>
<a class="sourceLine" id="cb2040-29" data-line-number="29">            layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.mu   =<span class="st"> </span>moving.mu</a>
<a class="sourceLine" id="cb2040-30" data-line-number="30">            layer<span class="op">$</span>batchnorm<span class="op">$</span>moving.var  =<span class="st"> </span>moving.variance</a>
<a class="sourceLine" id="cb2040-31" data-line-number="31">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2040-32" data-line-number="32">            H.mu   =<span class="st"> </span><span class="kw">sweep</span>( H, <span class="dv">3</span>, moving.mu, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb2040-33" data-line-number="33">            istd   =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(moving.variance <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2040-34" data-line-number="34">            H.norm =<span class="st"> </span><span class="kw">sweep</span>(H.mu, <span class="dv">3</span>, istd, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2040-35" data-line-number="35">        }</a>
<a class="sourceLine" id="cb2040-36" data-line-number="36">    }</a>
<a class="sourceLine" id="cb2040-37" data-line-number="37">    layer<span class="op">$</span>batchnorm<span class="op">$</span>H.norm  =<span class="st"> </span>H.norm</a>
<a class="sourceLine" id="cb2040-38" data-line-number="38">    layer<span class="op">$</span>batchnorm<span class="op">$</span>H.mu    =<span class="st"> </span>H.mu</a>
<a class="sourceLine" id="cb2040-39" data-line-number="39">    layer<span class="op">$</span>batchnorm<span class="op">$</span>istd    =<span class="st"> </span>istd</a>
<a class="sourceLine" id="cb2040-40" data-line-number="40">    H.hat =<span class="st"> </span>H.norm <span class="op">*</span><span class="st"> </span>gamma <span class="op">+</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb2040-41" data-line-number="41">    <span class="kw">list</span>(<span class="st">&quot;feature.map&quot;</span> =<span class="st"> </span>H.hat, <span class="st">&quot;layer&quot;</span> =<span class="st"> </span>layer)</a>
<a class="sourceLine" id="cb2040-42" data-line-number="42">}</a></code></pre></div>
<div class="sourceCode" id="cb2041"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2041-1" data-line-number="1">normalize.backward &lt;-<span class="st"> </span><span class="cf">function</span>(Dout, layer) {</a>
<a class="sourceLine" id="cb2041-2" data-line-number="2">   ntype           =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>normalize</a>
<a class="sourceLine" id="cb2041-3" data-line-number="3">   H.norm          =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>H.norm</a>
<a class="sourceLine" id="cb2041-4" data-line-number="4">   H.mu            =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>H.mu </a>
<a class="sourceLine" id="cb2041-5" data-line-number="5">   istd            =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>istd</a>
<a class="sourceLine" id="cb2041-6" data-line-number="6">   gamma           =<span class="st"> </span>layer<span class="op">$</span>batchnorm<span class="op">$</span>gamma<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2041-7" data-line-number="7">    </a>
<a class="sourceLine" id="cb2041-8" data-line-number="8">   s =<span class="st"> </span><span class="kw">ifelse</span>(ntype <span class="op">==</span><span class="st"> &quot;batch&quot;</span>, <span class="dv">3</span>, <span class="dv">4</span>) <span class="co"># batchnorm (3) or layernorm (4)</span></a>
<a class="sourceLine" id="cb2041-9" data-line-number="9">   m               =<span class="st"> </span><span class="kw">apply</span>(H.norm, s, length)</a>
<a class="sourceLine" id="cb2041-10" data-line-number="10">   delta.gamma     =<span class="st"> </span><span class="kw">apply</span>(Dout <span class="op">*</span><span class="st"> </span>H.norm, s, sum)  </a>
<a class="sourceLine" id="cb2041-11" data-line-number="11">   delta.beta      =<span class="st"> </span><span class="kw">apply</span>(Dout, s, sum) </a>
<a class="sourceLine" id="cb2041-12" data-line-number="12">   delta.H.norm    =<span class="st"> </span>Dout <span class="op">*</span><span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb2041-13" data-line-number="13">   delta.std      =<span class="st"> </span><span class="kw">apply</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>H.mu, s, sum) </a>
<a class="sourceLine" id="cb2041-14" data-line-number="14">   delta.var      =<span class="st"> </span>delta.std <span class="op">*</span><span class="st"> </span><span class="fl">-0.5</span> <span class="op">*</span><span class="st"> </span>istd<span class="op">^</span><span class="dv">3</span> </a>
<a class="sourceLine" id="cb2041-15" data-line-number="15">   delta.Hmu1     =<span class="st"> </span><span class="kw">apply</span>( delta.H.norm <span class="op">*</span><span class="st"> </span><span class="op">-</span>istd, s, sum)</a>
<a class="sourceLine" id="cb2041-16" data-line-number="16">   delta.Hmu2     =<span class="st"> </span>delta.var <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>( <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span>H.mu,s, mean)</a>
<a class="sourceLine" id="cb2041-17" data-line-number="17">   delta.mu       =<span class="st"> </span>delta.Hmu1 <span class="op">+</span><span class="st"> </span>delta.Hmu2</a>
<a class="sourceLine" id="cb2041-18" data-line-number="18">   delta.out      =<span class="st"> </span><span class="kw">sweep</span>(delta.H.norm <span class="op">*</span><span class="st"> </span>istd <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2041-19" data-line-number="19"><span class="st">                    </span>delta.var <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>H.mu) <span class="op">/</span><span class="st"> </span>m, s, (delta.mu <span class="op">/</span><span class="st"> </span>m), <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb2041-20" data-line-number="20">  params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;gamma&quot;</span> =<span class="st"> </span>delta.gamma, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>delta.beta)</a>
<a class="sourceLine" id="cb2041-21" data-line-number="21">  <span class="kw">list</span>(<span class="st">&quot;gradient.loss&quot;</span>    =<span class="st"> </span>delta.out, <span class="st">&quot;params&quot;</span> =<span class="st"> </span>params)</a>
<a class="sourceLine" id="cb2041-22" data-line-number="22">}</a></code></pre></div>

</div>
<div id="step-decay" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.12</span> Step Decay<a href="12.4-convolutional-neural-network-cnn.html#step-decay" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There is one other pointer to consider in <strong>CNN</strong>, which has to do with adjusting the learning rate during training. While other approaches may use different exponential formulations, here, we recall from <strong>MLP</strong> the use of the <strong>step decay</strong> approach using a typical formulation below:</p>
<p><span class="math display" id="eq:equate1140184">\[\begin{align}
\eta^{(t+1)} = \eta^{(initial)} \times DF^{floor\left(\frac{t}{step size}\right)} \tag{12.205} 
\end{align}\]</span></p>
<p>where <strong>t</strong> is the epoch and <span class="math inline">\(\mathbf{\eta}\)</span> is the learning rate. The equivalent implementation follows:</p>

<div class="sourceCode" id="cb2042"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2042-1" data-line-number="1">step.decay &lt;-<span class="st"> </span><span class="cf">function</span>(eta, epoch, <span class="dt">decay.factor=</span><span class="fl">0.55</span>, <span class="dt">step.size=</span><span class="dv">5</span>) { </a>
<a class="sourceLine" id="cb2042-2" data-line-number="2">    <span class="co"># default decay.factor=0.55 and step.size=5 for cifar-10 dataset</span></a>
<a class="sourceLine" id="cb2042-3" data-line-number="3">    eta <span class="op">*</span><span class="st"> </span>(decay.factor <span class="op">^</span><span class="st"> </span><span class="kw">floor</span>(epoch<span class="op">/</span>step.size)) </a>
<a class="sourceLine" id="cb2042-4" data-line-number="4">}</a></code></pre></div>

<p>To illustrate the use, below is an example of scheduled decay we use for a cifar-10 dataset of 50,000 images for training. The idea is to try to get the most (fastest) learning during the first five epochs using a large learning rate, then decay at a slightly lower learning rate between 5 and 10 epochs before we conveniently settle below <span class="math inline">\(\text{1e-3}\)</span> range past 15 epochs.</p>

<div class="sourceCode" id="cb2043"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2043-1" data-line-number="1"><span class="kw">step.decay</span>(<span class="fl">0.05</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">20</span>), <span class="dt">decay.factor=</span><span class="fl">0.80</span>, <span class="dt">step.size=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>##  [1] 0.04000000000 0.03200000000 0.02560000000
##  [4] 0.02048000000 0.01638400000 0.01310720000
##  [7] 0.01048576000 0.00838860800 0.00671088640
## [10] 0.00536870912 0.00429496730 0.00343597384
## [13] 0.00274877907 0.00219902326 0.00175921860
## [16] 0.00140737488 0.00112589991 0.00090071993
## [19] 0.00072057594 0.00057646075</code></pre>

<p>Before we finally discuss the implementation of the <strong>CNN</strong> function, let us first cover <strong>GEMM</strong> and <strong>Depthwise Separable Convolution</strong>.</p>
</div>
<div id="gemm-matrix-multiplication" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.13</span> GEMM (Matrix Multiplication) <a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Convolution</strong> is computationally expensive if we use the common <strong>dot-product</strong> operation against the raw image input. Doing so is otherwise known as <strong>Direct Convolution</strong>. Alternatively, we can use what is called <strong>GEMM-based Convolution</strong>. This convolution involves three consecutive techniques to help boost performance. The first technique is to perform <strong>image-to-column (im2col)</strong> conversion, which stacks all the receptive fields in a matrix. The second technique <strong>GEneral Matrix to Matrix Multiplication (GEMM)</strong> operation performs the actual <strong>dot-product</strong>. The third technique is a <strong>column-to-image (col2im)</strong> conversion which transforms the resultant <strong>Toeplitz Matrix</strong> back to its convolution form. </p>
<p>We use Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:convolv">12.41</a> to illustrate a feed-forward convolution using the three methods above. The main idea here is to stack up all the receptive fields into a matrix form and perform the same conversion for the kernels such that the two newly formed matrices can convolve to form the feature map. Note that in the figure, both ker2col and im2col are arranged horizontally to fit our text, labeled as <span class="math inline">\(2 \times 28\)</span> and <span class="math inline">\(9 \times 28\)</span>, respectively. However, we have it vertically in such an implementation, namely <span class="math inline">\(28 \times 2\)</span> and <span class="math inline">\(28 \times 9\)</span>. We then convolve so that <span class="math inline">\(t(28 \times 2) * (28 \times 9) = (2 \times 9)\)</span>. Also, note that we assume a tensor of (HxWxCxI) dimension. On that note, we intend to reduce the dimension into a 2-dimension matrix.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:convolv"></span>
<img src="convolv.png" alt="CNN (Main Convolution)" width="100%" />
<p class="caption">
Figure 12.41: CNN (Main Convolution)
</p>
</div>
<p>In terms of backpropagation, we use Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:fullconvolv">12.42</a> to illustrate the use of a similar <strong>GEMM-based</strong> method to perform full convolution, computing the gradient with respect to the filters to arrive at our delta gradients for the input. Similarly, the arrangement is plotted horizontally, where our filters and input tensors are converted into 2-dimensional matrices for convolution.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fullconvolv"></span>
<img src="fullconvolv.png" alt="CNN (Gradient with respect to Filter)" width="100%" />
<p class="caption">
Figure 12.42: CNN (Gradient with respect to Filter)
</p>
</div>
<p>Similarly, Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:bpconvolv">12.43</a> illustrates the use of <strong>GEMM-based</strong> convolution to compute the gradient with respect to the input to arrive at the delta gradients for our filters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpconvolv"></span>
<img src="bpconvolv.png" alt="CNN (Gradient with respect to Input)" width="100%" />
<p class="caption">
Figure 12.43: CNN (Gradient with respect to Input)
</p>
</div>
<p>Finally, we also can use the <strong>GEMM-based</strong> method to perform the same operation for our max pool. See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:bpmaxpool">12.44</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bpmaxpool"></span>
<img src="bpmaxpool.png" alt="CNN (Gradient with respect to MaxPool)" width="90%" />
<p class="caption">
Figure 12.44: CNN (Gradient with respect to MaxPool)
</p>
</div>
<p>While the use of the <strong>GEMM-based</strong> method benefits from slow convolution computation, one alternative method for machines or gadgets with low resources is called <strong>Depthwise Separable Convolution</strong> complemented by <strong>Pointwise Convolution</strong>. The next section discusses the idea in more detail.</p>
</div>
<div id="depthwise-separable-convolution-dsc" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.14</span> Depthwise Separable Convolution (DSC)  <a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our implementation of <strong>CNN</strong> uses <strong>Depthwise Separable Convolution (DSC)</strong> along with <strong>Pointwise Convolution (PC)</strong> and an intermediate <strong>GEMM</strong> implementation within the loop. </p>
<p>The idea is to <strong>separate</strong> the convolution operation for individual channels (depthwise) as the first step.
Each channel from the Input is separately convolved with each channel from the filter such that we have <span class="math inline">\((5\times5)_{(\text{input})} * (3\times3)_{(\text{kernel})} = (3\times3)_{(\text{feature map})}\)</span> for each Image input. We then conjoin to form a <span class="math inline">\((3\times 3\times 1)\)</span> feature map. This is followed by performing <strong>pointwise</strong> convolution with a set of N <span class="math inline">\((1\times C)\)</span> filters. See Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:depthwise">12.45</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:depthwise"></span>
<img src="depthwise.png" alt="CNN (Depthwise)" width="90%" />
<p class="caption">
Figure 12.45: CNN (Depthwise)
</p>
</div>
<p>Below are the implementations of convolving matrices starting with the <strong>convolve.image(.)</strong> function, which handles the main convolution using <strong>DSC</strong> and <strong>PC</strong>. Our <strong>DSC</strong> implementation performs the <strong>ker2col</strong> conversion first as a one-time step, then a set of <strong>im2col</strong> conversions after accumulating the receptive fields row-wise. We then use the converted <strong>im2col</strong> matrix for every row to perform <strong>GEMM</strong> convolution with <strong>ker2col</strong>. This extra step assumes handling a decent mini-batch size (labeled as I in our text and S in our implementation). The convolution result is conjoined to form an <span class="math inline">\((O\times O\times C \times S)\)</span> matrix that is then convolved with a <span class="math inline">\(1\times C\)</span> of N filters which are simply represented as <span class="math inline">\(C\times N\)</span> in our implementation. Looping through <strong>S</strong>, we end up with the expected <span class="math inline">\((O\times O\times N \times S)\)</span> feature map.</p>

<div class="sourceCode" id="cb2045"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2045-1" data-line-number="1">convolve.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, dw.kernel, pw.kernel, </a>
<a class="sourceLine" id="cb2045-2" data-line-number="2">                           dw.bias, pw.bias, bias, dil_rate, O, h, w, r, c) {</a>
<a class="sourceLine" id="cb2045-3" data-line-number="3">  dil.filters =<span class="st"> </span><span class="kw">dilate.filters</span>(dw.kernel, dil_rate)</a>
<a class="sourceLine" id="cb2045-4" data-line-number="4">  len =<span class="st"> </span><span class="kw">length</span>(dil.filters)</a>
<a class="sourceLine" id="cb2045-5" data-line-number="5">  di   =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2045-6" data-line-number="6">  di.k =<span class="st"> </span><span class="kw">dim</span>(dw.kernel)</a>
<a class="sourceLine" id="cb2045-7" data-line-number="7">  len =<span class="st"> </span><span class="kw">dim</span>(pw.kernel)[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2045-8" data-line-number="8">  ker2col =<span class="st"> </span><span class="kw">array</span>(dw.kernel, <span class="kw">c</span>(di.k[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.k[<span class="dv">2</span>], di.k[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2045-9" data-line-number="9">  <span class="co">#ker2col = rbind(dw.bias, ker2col) # add bias</span></a>
<a class="sourceLine" id="cb2045-10" data-line-number="10">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2045-11" data-line-number="11">  I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-12" data-line-number="12">  dw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2045-13" data-line-number="13">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2045-14" data-line-number="14">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2045-15" data-line-number="15">    I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-16" data-line-number="16">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2045-17" data-line-number="17">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2045-18" data-line-number="18">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2045-19" data-line-number="19">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2045-20" data-line-number="20">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2045-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb2045-22" data-line-number="22">    D =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb2045-23" data-line-number="23">    <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) { <span class="co"># Depthwise Separable Convolution (KxKxM filter)</span></a>
<a class="sourceLine" id="cb2045-24" data-line-number="24">      im2col =<span class="st"> </span>D[,d,]</a>
<a class="sourceLine" id="cb2045-25" data-line-number="25">      <span class="co">#im2col = rbind(rep(1,  di[4] * n), im2col) # Add bias constant</span></a>
<a class="sourceLine" id="cb2045-26" data-line-number="26">      conv =<span class="st">  </span><span class="kw">t</span>(ker2col[,d]) <span class="op">%*%</span><span class="st"> </span>im2col</a>
<a class="sourceLine" id="cb2045-27" data-line-number="27">      dw.fmap[i,,d,] =<span class="st"> </span><span class="kw">t</span>(<span class="kw">array</span>(conv, <span class="kw">c</span>(di[<span class="dv">4</span>], n)))</a>
<a class="sourceLine" id="cb2045-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb2045-29" data-line-number="29">  }</a>
<a class="sourceLine" id="cb2045-30" data-line-number="30">  dw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.fmap), <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) <span class="co"># DfxDfxMxS</span></a>
<a class="sourceLine" id="cb2045-31" data-line-number="31">  pw.fmap =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2045-32" data-line-number="32">  <span class="cf">if</span> (bias<span class="op">==</span><span class="ot">TRUE</span>) { pw.kernel =<span class="st"> </span><span class="kw">rbind</span>(pw.bias, pw.kernel) } <span class="co"># add bias</span></a>
<a class="sourceLine" id="cb2045-33" data-line-number="33">  sz =<span class="st"> </span>O <span class="op">*</span><span class="st"> </span>O</a>
<a class="sourceLine" id="cb2045-34" data-line-number="34">  <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">4</span>]) { <span class="co"># Pointwise Convolution (MxN filter)</span></a>
<a class="sourceLine" id="cb2045-35" data-line-number="35">      pw.fmap[[s]] =<span class="st"> </span><span class="kw">array</span>( dw.fmap[,,,s], <span class="kw">c</span>(sz, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2045-36" data-line-number="36">      <span class="cf">if</span> (bias<span class="op">==</span><span class="ot">TRUE</span>) { </a>
<a class="sourceLine" id="cb2045-37" data-line-number="37">          pw.fmap[[s]] =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, sz), pw.fmap[[s]]) <span class="co"># add bias constant</span></a>
<a class="sourceLine" id="cb2045-38" data-line-number="38">      }</a>
<a class="sourceLine" id="cb2045-39" data-line-number="39">      pw.fmap[[s]] =<span class="st"> </span>pw.fmap[[s]] <span class="op">%*%</span><span class="st"> </span>pw.kernel </a>
<a class="sourceLine" id="cb2045-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb2045-41" data-line-number="41">  pw.fmap =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(pw.fmap), <span class="kw">c</span>(O, O, len, di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2045-42" data-line-number="42">  <span class="kw">remove</span>(I, ker2col, D)  </a>
<a class="sourceLine" id="cb2045-43" data-line-number="43">  <span class="kw">list</span>(<span class="st">&quot;dw.fmap&quot;</span> =<span class="st"> </span>dw.fmap, <span class="st">&quot;pw.fmap&quot;</span> =<span class="st"> </span>pw.fmap)   </a>
<a class="sourceLine" id="cb2045-44" data-line-number="44">}</a></code></pre></div>

<p>A note to emphasize in the implementation above is the addition of <strong>bias</strong> only to the <strong>Pointwise</strong> convolution. We leave readers to investigate the addition of <strong>bias</strong> to the <strong>Depthwise separable</strong> convolution.</p>
<p>Next, we account for solving the gradient with respect to the filter to generate our Input gradients. Our implementation performs a full convolution by rotating our kernel to 180 degrees. See <strong>back.propagation(.)</strong>.</p>

<div class="sourceCode" id="cb2046"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2046-1" data-line-number="1">gradient.I.wrt.K &lt;-<span class="st"> </span><span class="cf">function</span>(image, dw.kernel, O, h, w, r, c) {</a>
<a class="sourceLine" id="cb2046-2" data-line-number="2">   di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2046-3" data-line-number="3">   fmap =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2046-4" data-line-number="4">   ker2col   =<span class="st"> </span><span class="kw">array</span>(dw.kernel, <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2046-5" data-line-number="5">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2046-6" data-line-number="6">     n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2046-7" data-line-number="7">     I =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2046-8" data-line-number="8">     <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2046-9" data-line-number="9">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2046-10" data-line-number="10">        hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2046-11" data-line-number="11">        ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2046-12" data-line-number="12">        I[[n]] =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2046-13" data-line-number="13">     }</a>
<a class="sourceLine" id="cb2046-14" data-line-number="14">     D =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(I), <span class="kw">c</span>( r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">3</span>], di[<span class="dv">4</span>] <span class="op">*</span><span class="st"> </span>n))</a>
<a class="sourceLine" id="cb2046-15" data-line-number="15">     <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) { <span class="co"># Depthwise Separable Backprop. (KxKxM filter)</span></a>
<a class="sourceLine" id="cb2046-16" data-line-number="16">       conv =<span class="st">  </span><span class="kw">t</span>(ker2col[,d]) <span class="op">%*%</span><span class="st"> </span>D[,d,]</a>
<a class="sourceLine" id="cb2046-17" data-line-number="17">       fmap[i,,d,] =<span class="st"> </span><span class="kw">t</span>(<span class="kw">array</span>(conv, <span class="kw">c</span>(di[<span class="dv">4</span>], n)))</a>
<a class="sourceLine" id="cb2046-18" data-line-number="18">     }</a>
<a class="sourceLine" id="cb2046-19" data-line-number="19">   }</a>
<a class="sourceLine" id="cb2046-20" data-line-number="20">   <span class="kw">remove</span>(I, ker2col, conv)  </a>
<a class="sourceLine" id="cb2046-21" data-line-number="21">   fmap</a>
<a class="sourceLine" id="cb2046-22" data-line-number="22">}</a></code></pre></div>

<p>Next, we also account for solving the gradient with respect to input (or feature map) to generate our filter gradients. Our implementation performs a convolution of the gradient derived from a prior deeper layer (l) with the input of the current layer (l-1). See <strong>back.propagation(.)</strong>.</p>

<div class="sourceCode" id="cb2047"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2047-1" data-line-number="1">gradient.K.wrt.I &lt;-<span class="st"> </span><span class="cf">function</span>(image, Dout, cur.fmaps, pw.kernel, </a>
<a class="sourceLine" id="cb2047-2" data-line-number="2">                             dil_rate, O, h, w) {</a>
<a class="sourceLine" id="cb2047-3" data-line-number="3">    pw.Dout  =<span class="st"> </span>Dout</a>
<a class="sourceLine" id="cb2047-4" data-line-number="4">    dw.fmap  =<span class="st"> </span>cur.fmaps<span class="op">$</span>dw.fmap</a>
<a class="sourceLine" id="cb2047-5" data-line-number="5">    pw.fmap  =<span class="st"> </span>cur.fmaps<span class="op">$</span>pw.fmap</a>
<a class="sourceLine" id="cb2047-6" data-line-number="6">    di.dw    =<span class="st"> </span><span class="kw">dim</span>(dw.fmap)  </a>
<a class="sourceLine" id="cb2047-7" data-line-number="7">    di.pw    =<span class="st"> </span><span class="kw">dim</span>(pw.fmap) </a>
<a class="sourceLine" id="cb2047-8" data-line-number="8">    delta.pw =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2047-9" data-line-number="9">    dw.Dout  =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2047-10" data-line-number="10">    <span class="cf">for</span> (s <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di.dw[<span class="dv">4</span>]) {</a>
<a class="sourceLine" id="cb2047-11" data-line-number="11">       im2col   =<span class="st"> </span><span class="kw">array</span>( dw.fmap[,,,s], <span class="kw">c</span>(di.dw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.dw[<span class="dv">2</span>], di.dw[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-12" data-line-number="12">       Dout2col =<span class="st"> </span><span class="kw">array</span>( pw.Dout[,,,s], <span class="kw">c</span>(di.pw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.pw[<span class="dv">2</span>], di.pw[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-13" data-line-number="13">       delta.pw =<span class="st"> </span>delta.pw <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(im2col) <span class="op">%*%</span><span class="st"> </span>Dout2col</a>
<a class="sourceLine" id="cb2047-14" data-line-number="14">       dw.Dout[[s]] =<span class="st">  </span>(Dout2col) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(pw.kernel)</a>
<a class="sourceLine" id="cb2047-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb2047-16" data-line-number="16">    <span class="co">#delta.pw = delta.pw / di.pw[4] # take average gradient of minibatch</span></a>
<a class="sourceLine" id="cb2047-17" data-line-number="17">    Dout2col =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.Dout), <span class="kw">c</span>(di.pw[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di.pw[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2047-18" data-line-number="18">                                        di.dw[<span class="dv">3</span>], di.dw[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2047-19" data-line-number="19">    dw.Dout =<span class="st"> </span><span class="kw">array</span>(<span class="kw">unlist</span>(dw.Dout), <span class="kw">c</span>(di.pw[<span class="dv">1</span>], di.pw[<span class="dv">2</span>], </a>
<a class="sourceLine" id="cb2047-20" data-line-number="20">                                       di.dw[<span class="dv">3</span>], di.dw[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2047-21" data-line-number="21">    K =<span class="st"> </span><span class="kw">dilate</span>(dw.Dout, dil_rate) <span class="co"># Becomes Filter for gradient.I.wrt.K</span></a>
<a class="sourceLine" id="cb2047-22" data-line-number="22">    r =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb2047-23" data-line-number="23">    c =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb2047-24" data-line-number="24">    di.k =<span class="st"> </span><span class="kw">dim</span>(K)</a>
<a class="sourceLine" id="cb2047-25" data-line-number="25">    di =<span class="st"> </span><span class="kw">dim</span>(image)</a>
<a class="sourceLine" id="cb2047-26" data-line-number="26">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2047-27" data-line-number="27">    delta.dw =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(O, O, di[<span class="dv">3</span>]))</a>
<a class="sourceLine" id="cb2047-28" data-line-number="28">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2047-29" data-line-number="29">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2047-30" data-line-number="30">         n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2047-31" data-line-number="31">         hs =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2047-32" data-line-number="32">         ws =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2047-33" data-line-number="33">         <span class="cf">for</span> (d <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>di[<span class="dv">3</span>]) {</a>
<a class="sourceLine" id="cb2047-34" data-line-number="34">           Dout2col.sub     =<span class="st"> </span><span class="kw">array</span>(K[,,d,] , <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c,  di[<span class="dv">4</span>] )  )</a>
<a class="sourceLine" id="cb2047-35" data-line-number="35">           im2col.sub       =<span class="st"> </span><span class="kw">array</span>(image[hs<span class="op">:</span>he, ws<span class="op">:</span>we,d,], <span class="kw">c</span>(r <span class="op">*</span><span class="st"> </span>c, di[<span class="dv">4</span>])) </a>
<a class="sourceLine" id="cb2047-36" data-line-number="36"></a>
<a class="sourceLine" id="cb2047-37" data-line-number="37">           delta.dw[i,j,d] =<span class="st"> </span><span class="kw">sum</span>(Dout2col.sub <span class="op">*</span><span class="st"> </span>im2col.sub)</a>
<a class="sourceLine" id="cb2047-38" data-line-number="38">         } </a>
<a class="sourceLine" id="cb2047-39" data-line-number="39">      }</a>
<a class="sourceLine" id="cb2047-40" data-line-number="40">    }</a>
<a class="sourceLine" id="cb2047-41" data-line-number="41">    <span class="co">#delta.dw = delta.dw / di[4] # take average gradient of minibatch</span></a>
<a class="sourceLine" id="cb2047-42" data-line-number="42">    <span class="kw">remove</span>(Dout2col, im2col.sub, Dout2col.sub) </a>
<a class="sourceLine" id="cb2047-43" data-line-number="43">    <span class="kw">list</span>(<span class="st">&quot;dw.Dout&quot;</span> =<span class="st"> </span>dw.Dout, <span class="st">&quot;pw.Dout&quot;</span> =<span class="st"> </span>pw.Dout,</a>
<a class="sourceLine" id="cb2047-44" data-line-number="44">         <span class="st">&quot;delta.dw&quot;</span> =<span class="st"> </span>delta.dw, <span class="st">&quot;delta.pw&quot;</span> =<span class="st"> </span>delta.pw)</a>
<a class="sourceLine" id="cb2047-45" data-line-number="45">}</a></code></pre></div>

<p>A note to emphasize in our implementation is the summation of gradients accumulated from all minibatch samples. We leave readers to investigate the effect of averaging the gradients instead, granting that the average does not swing the value too large or too small enough to offset the trajectory.</p>
<p>Lastly, we derive the gradient with respect to a maximum pool. Here, we rely on a cache containing the location of the maximum values.</p>

<div class="sourceCode" id="cb2048"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2048-1" data-line-number="1">gradient.I.wrt.P &lt;-<span class="st"> </span><span class="cf">function</span>(image, Dout, K, O, pool, h, w, pool.cache) {</a>
<a class="sourceLine" id="cb2048-2" data-line-number="2">      r         =<span class="st"> </span><span class="kw">nrow</span>(K)</a>
<a class="sourceLine" id="cb2048-3" data-line-number="3">      c         =<span class="st"> </span><span class="kw">ncol</span>(K)</a>
<a class="sourceLine" id="cb2048-4" data-line-number="4">      img.copy =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">dim</span>(image))</a>
<a class="sourceLine" id="cb2048-5" data-line-number="5">      di  =<span class="st"> </span><span class="kw">dim</span>(Dout)</a>
<a class="sourceLine" id="cb2048-6" data-line-number="6">      gr2col =<span class="st"> </span><span class="kw">array</span>(Dout, <span class="kw">c</span>(<span class="dv">1</span>, di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2048-7" data-line-number="7">      len =<span class="st"> </span><span class="kw">length</span>(gr2col)</a>
<a class="sourceLine" id="cb2048-8" data-line-number="8">      <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-9" data-line-number="9">          one.hot =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(len,r <span class="op">*</span><span class="st"> </span>c))</a>
<a class="sourceLine" id="cb2048-10" data-line-number="10">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {  one.hot[i, pool.cache[i]] =<span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb2048-11" data-line-number="11">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2048-12" data-line-number="12">        all.one =<span class="st"> </span><span class="kw">array</span>(<span class="dv">1</span>, <span class="kw">c</span>(len, r<span class="op">*</span>c))</a>
<a class="sourceLine" id="cb2048-13" data-line-number="13">      }</a>
<a class="sourceLine" id="cb2048-14" data-line-number="14">      n    =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2048-15" data-line-number="15">      skip =<span class="st"> </span>di[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2048-16" data-line-number="16">      k    =<span class="st"> </span><span class="kw">prod</span>(di)</a>
<a class="sourceLine" id="cb2048-17" data-line-number="17">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2048-18" data-line-number="18">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>O) {</a>
<a class="sourceLine" id="cb2048-19" data-line-number="19">        n     =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2048-20" data-line-number="20">        hs    =<span class="st"> </span>h[i];  he =<span class="st"> </span>(hs <span class="op">+</span><span class="st"> </span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb2048-21" data-line-number="21">        ws    =<span class="st"> </span>w[j];  we =<span class="st"> </span>(ws <span class="op">+</span><span class="st"> </span>c <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2048-22" data-line-number="22">        b     =<span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>skip <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2048-23" data-line-number="23">        e     =<span class="st"> </span>n <span class="op">*</span><span class="st"> </span>skip</a>
<a class="sourceLine" id="cb2048-24" data-line-number="24">        a.idx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>b, <span class="dt">to=</span>e)</a>
<a class="sourceLine" id="cb2048-25" data-line-number="25">        b.idx =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span>n,  <span class="dt">to=</span>k, <span class="dt">by=</span>di[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>di[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb2048-26" data-line-number="26">        <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;maxpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-27" data-line-number="27">            z =<span class="st"> </span><span class="kw">sweep</span>(<span class="kw">t</span>(one.hot[a.idx,]), <span class="dv">2</span>, gr2col[,b.idx], <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2048-28" data-line-number="28">        } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2048-29" data-line-number="29">        <span class="cf">if</span> (pool <span class="op">==</span><span class="st"> &quot;avgpool&quot;</span>) {</a>
<a class="sourceLine" id="cb2048-30" data-line-number="30">            z =<span class="st"> </span><span class="kw">sweep</span>(<span class="kw">t</span>(all.one[a.idx,]), <span class="dv">2</span>, gr2col[,b.idx], <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2048-31" data-line-number="31">        }</a>
<a class="sourceLine" id="cb2048-32" data-line-number="32">        dimen.pool =<span class="st"> </span><span class="kw">array</span>(z, <span class="kw">c</span>(r,c, di[<span class="dv">3</span>], di[<span class="dv">4</span>]))</a>
<a class="sourceLine" id="cb2048-33" data-line-number="33">        img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] =<span class="st"> </span>img.copy[hs<span class="op">:</span>he, ws<span class="op">:</span>we,,] <span class="op">+</span><span class="st"> </span>dimen.pool</a>
<a class="sourceLine" id="cb2048-34" data-line-number="34">      }</a>
<a class="sourceLine" id="cb2048-35" data-line-number="35">      }</a>
<a class="sourceLine" id="cb2048-36" data-line-number="36">      img.copy</a>
<a class="sourceLine" id="cb2048-37" data-line-number="37">}</a></code></pre></div>

<p>Other types of <strong>Convolution</strong> cater to improving the computation of <strong>Convolution</strong>, of which two of them are necessary for investigation, namely <strong>FFT (Fast Fourier Transform)</strong> Convolution vs. <strong>WinoGrad</strong> Convolution. However, we leave readers to investigate these algorithms further, along with how the computations are distributed across a set of <strong>GPU</strong> processors in a parallel fashion; for example, having each kernel affined to its dedicated <strong>GPU</strong> unit and performing <strong>GEMM</strong>.</p>
</div>
<div id="cnn-implementation" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.15</span> CNN Implementation<a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Below is a rudimentary implementation of our <strong>CNN</strong> with the forward feed for CNN and forward feed for <strong>MLP</strong> (taking part to support <strong>fully-connected</strong> layers).</p>

<div class="sourceCode" id="cb2049"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2049-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb2049-2" data-line-number="2">accuracy &lt;-<span class="st"> </span><span class="cf">function</span>(t, o) {</a>
<a class="sourceLine" id="cb2049-3" data-line-number="3">    p1 =<span class="st"> </span><span class="kw">apply</span>(t, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb2049-4" data-line-number="4">    p2 =<span class="st"> </span><span class="kw">apply</span>(o, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb2049-5" data-line-number="5">    p =<span class="st"> </span>p1 <span class="op">==</span><span class="st"> </span>p2</a>
<a class="sourceLine" id="cb2049-6" data-line-number="6">    <span class="kw">sum</span>(p)<span class="op">/</span><span class="kw">length</span>(p)</a>
<a class="sourceLine" id="cb2049-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2049-8" data-line-number="8">get.batch &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data, labels, k, t) {  </a>
<a class="sourceLine" id="cb2049-9" data-line-number="9">   <span class="kw">set.seed</span>(t)</a>
<a class="sourceLine" id="cb2049-10" data-line-number="10">   batch.indices =<span class="st"> </span><span class="kw">createFolds</span>(sample.data, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2049-11" data-line-number="11">   batches =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2049-12" data-line-number="12">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb2049-13" data-line-number="13">       indices =<span class="st"> </span>batch.indices[[i]]</a>
<a class="sourceLine" id="cb2049-14" data-line-number="14">       X =<span class="st"> </span><span class="kw">extract.image</span>(images, indices)</a>
<a class="sourceLine" id="cb2049-15" data-line-number="15">       Y =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, indices) </a>
<a class="sourceLine" id="cb2049-16" data-line-number="16">       <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb2049-17" data-line-number="17">         batches[[i]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y )</a>
<a class="sourceLine" id="cb2049-18" data-line-number="18">       } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2049-19" data-line-number="19">         validation =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb2049-20" data-line-number="20">       }</a>
<a class="sourceLine" id="cb2049-21" data-line-number="21">   }</a>
<a class="sourceLine" id="cb2049-22" data-line-number="22">   <span class="kw">list</span>(<span class="st">&quot;train&quot;</span> =<span class="st"> </span>batches, <span class="st">&quot;validation&quot;</span> =<span class="st"> </span>validation)</a>
<a class="sourceLine" id="cb2049-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb2049-24" data-line-number="24">flush.str &lt;-<span class="st"> </span><span class="cf">function</span>(...) {str =<span class="st"> </span><span class="kw">sprintf</span>(...); <span class="kw">print</span>(str); <span class="kw">flush.console</span>()}</a>
<a class="sourceLine" id="cb2049-25" data-line-number="25">transfer.learning &lt;-<span class="st"> </span><span class="cf">function</span>(model.file) {</a>
<a class="sourceLine" id="cb2049-26" data-line-number="26">     my.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(model.file)</a>
<a class="sourceLine" id="cb2049-27" data-line-number="27">     my.cnn.model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2049-28" data-line-number="28">}</a></code></pre></div>
<div class="sourceCode" id="cb2050"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2050-1" data-line-number="1">my.CNN &lt;-<span class="st"> </span><span class="cf">function</span>(target.images, labels, <span class="dt">layers=</span><span class="ot">NULL</span>, optimize, </a>
<a class="sourceLine" id="cb2050-2" data-line-number="2">                   <span class="dt">transfer=</span><span class="ot">NULL</span>, <span class="dt">minibatch=</span><span class="dv">40</span>,  <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">eta =</span> <span class="fl">0.01</span>) {</a>
<a class="sourceLine" id="cb2050-3" data-line-number="3">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">16</span>)  <span class="co"># 16 digits precision </span></a>
<a class="sourceLine" id="cb2050-4" data-line-number="4">  eta            =<span class="st"> </span><span class="kw">c</span>(eta)</a>
<a class="sourceLine" id="cb2050-5" data-line-number="5">  total.cost     =<span class="st"> </span>epoch.cost     =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-6" data-line-number="6">  total.accuracy =<span class="st"> </span>epoch.accuracy =<span class="st"> </span>fc.params =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-7" data-line-number="7">  total.validate =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2050-8" data-line-number="8">    </a>
<a class="sourceLine" id="cb2050-9" data-line-number="9">  <span class="co"># Target images</span></a>
<a class="sourceLine" id="cb2050-10" data-line-number="10">  population     =<span class="st"> </span>target.images<span class="op">$</span>rgb</a>
<a class="sourceLine" id="cb2050-11" data-line-number="11">  population.len =<span class="st"> </span><span class="kw">length</span>(population)</a>
<a class="sourceLine" id="cb2050-12" data-line-number="12">  sample.set     =<span class="st"> </span><span class="dv">250</span> <span class="co"># population.len</span></a>
<a class="sourceLine" id="cb2050-13" data-line-number="13">  shuffled.data  =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n=</span>population.len, <span class="dt">size=</span>population.len, </a>
<a class="sourceLine" id="cb2050-14" data-line-number="14">                              <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2050-15" data-line-number="15">  sample.size    =<span class="st"> </span><span class="kw">ifelse</span>(sample.set <span class="op">&lt;</span><span class="st"> </span>population.len, sample.set, </a>
<a class="sourceLine" id="cb2050-16" data-line-number="16">                          population.len)</a>
<a class="sourceLine" id="cb2050-17" data-line-number="17">  sample.data    =<span class="st"> </span><span class="kw">sample</span>(shuffled.data, sample.size)</a>
<a class="sourceLine" id="cb2050-18" data-line-number="18">  k              =<span class="st"> </span><span class="kw">ceiling</span>(sample.size <span class="op">/</span><span class="st"> </span>minibatch)</a>
<a class="sourceLine" id="cb2050-19" data-line-number="19">  <span class="kw">flush.str</span>( <span class="st">&quot;No of batches: %d&quot;</span>, k )</a>
<a class="sourceLine" id="cb2050-20" data-line-number="20">  <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(transfer)) {</a>
<a class="sourceLine" id="cb2050-21" data-line-number="21">      layers =<span class="st"> </span><span class="kw">transfer.learning</span>(transfer)</a>
<a class="sourceLine" id="cb2050-22" data-line-number="22">  } </a>
<a class="sourceLine" id="cb2050-23" data-line-number="23">  <span class="cf">if</span> (<span class="kw">is.null</span>(layers)) {</a>
<a class="sourceLine" id="cb2050-24" data-line-number="24">      <span class="kw">stop</span>(<span class="st">&quot;No network layers defined.&quot;</span>)</a>
<a class="sourceLine" id="cb2050-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb2050-26" data-line-number="26">  my.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-27" data-line-number="27">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {  </a>
<a class="sourceLine" id="cb2050-28" data-line-number="28">    batch.cost =<span class="st"> </span>batch.accuracy =<span class="st"> </span><span class="ot">NULL</span>     </a>
<a class="sourceLine" id="cb2050-29" data-line-number="29">    batch.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-30" data-line-number="30">    step.eta   =<span class="st"> </span><span class="kw">step.decay</span>(eta, t, <span class="dt">decay.factor=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2050-31" data-line-number="31">    n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2050-32" data-line-number="32">    batches =<span class="st"> </span><span class="kw">get.batch</span>(sample.data, labels, k, t)</a>
<a class="sourceLine" id="cb2050-33" data-line-number="33">    <span class="cf">for</span> (batch <span class="cf">in</span> batches<span class="op">$</span>train) {</a>
<a class="sourceLine" id="cb2050-34" data-line-number="34">        n =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>        </a>
<a class="sourceLine" id="cb2050-35" data-line-number="35">        X =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2050-36" data-line-number="36">        Y =<span class="st"> </span>batch<span class="op">$</span>Y</a>
<a class="sourceLine" id="cb2050-37" data-line-number="37">        model          =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers)</a>
<a class="sourceLine" id="cb2050-38" data-line-number="38">        backprop       =<span class="st"> </span><span class="kw">back.propagation.cnn</span>(X, Y, model)</a>
<a class="sourceLine" id="cb2050-39" data-line-number="39">        layers         =<span class="st"> </span><span class="kw">optimizer</span>(backprop, model<span class="op">$</span>layers, optimize, </a>
<a class="sourceLine" id="cb2050-40" data-line-number="40">                                   (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n, step.eta)</a>
<a class="sourceLine" id="cb2050-41" data-line-number="41">        len            =<span class="st"> </span><span class="kw">length</span>(model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2050-42" data-line-number="42">        softmax.prob   =<span class="st"> </span>model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2050-43" data-line-number="43">        loss           =<span class="st"> </span><span class="kw">softmax.loss</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-44" data-line-number="44">        accurate       =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-45" data-line-number="45">        batch.cost     =<span class="st"> </span><span class="kw">c</span>(batch.cost, <span class="kw">mean</span>(loss))</a>
<a class="sourceLine" id="cb2050-46" data-line-number="46">        batch.accuracy =<span class="st"> </span><span class="kw">c</span>(batch.accuracy, accurate)</a>
<a class="sourceLine" id="cb2050-47" data-line-number="47">        </a>
<a class="sourceLine" id="cb2050-48" data-line-number="48">        <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">50</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2050-49" data-line-number="49">          new.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-50" data-line-number="50">          lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, my.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>) </a>
<a class="sourceLine" id="cb2050-51" data-line-number="51">          my.time  =<span class="st"> </span>new.time</a>
<a class="sourceLine" id="cb2050-52" data-line-number="52">          stime =<span class="st"> </span><span class="kw">format</span>(<span class="kw">Sys.Date</span>(), <span class="st">&quot;%c&quot;</span>)</a>
<a class="sourceLine" id="cb2050-53" data-line-number="53">          <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-54" data-line-number="54">          <span class="st">&quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lagtime (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2050-55" data-line-number="55">               n, <span class="kw">mean</span>(loss),  (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n, accurate, lag.time)</a>
<a class="sourceLine" id="cb2050-56" data-line-number="56">        }</a>
<a class="sourceLine" id="cb2050-57" data-line-number="57">        <span class="cf">if</span> (<span class="kw">is.na</span>(<span class="kw">mean</span>(loss)) <span class="op">||</span><span class="st"> </span><span class="kw">mean</span>(loss) <span class="op">&gt;</span><span class="st"> </span><span class="dv">40</span>) { </a>
<a class="sourceLine" id="cb2050-58" data-line-number="58">            <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-59" data-line-number="59">              <span class="st">&quot;loss NaN/increasing at %d epoch.&quot;</span>,  (t <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>k <span class="op">+</span><span class="st"> </span>n) </a>
<a class="sourceLine" id="cb2050-60" data-line-number="60">            <span class="kw">stop</span>(<span class="st">&quot;&quot;</span>) </a>
<a class="sourceLine" id="cb2050-61" data-line-number="61">        }</a>
<a class="sourceLine" id="cb2050-62" data-line-number="62">     }</a>
<a class="sourceLine" id="cb2050-63" data-line-number="63">      </a>
<a class="sourceLine" id="cb2050-64" data-line-number="64">     <span class="co">## Validate</span></a>
<a class="sourceLine" id="cb2050-65" data-line-number="65">     batches   =<span class="st"> </span>batches<span class="op">$</span>validation</a>
<a class="sourceLine" id="cb2050-66" data-line-number="66">     X =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2050-67" data-line-number="67">     Y =<span class="st"> </span>batch<span class="op">$</span>Y</a>
<a class="sourceLine" id="cb2050-68" data-line-number="68">     valid.model    =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers, <span class="dt">train=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2050-69" data-line-number="69">     len            =<span class="st"> </span><span class="kw">length</span>(valid.model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2050-70" data-line-number="70">     softmax.prob   =<span class="st"> </span>valid.model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2050-71" data-line-number="71">     valid.accurate =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2050-72" data-line-number="72">      </a>
<a class="sourceLine" id="cb2050-73" data-line-number="73">     new.time   =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2050-74" data-line-number="74">     lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, batch.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>)</a>
<a class="sourceLine" id="cb2050-75" data-line-number="75">     epoch.cost      =<span class="st"> </span><span class="kw">c</span>(epoch.cost, <span class="kw">mean</span>(batch.cost))</a>
<a class="sourceLine" id="cb2050-76" data-line-number="76">     epoch.accuracy  =<span class="st"> </span><span class="kw">c</span>(epoch.accuracy, <span class="kw">mean</span>(batch.accuracy))</a>
<a class="sourceLine" id="cb2050-77" data-line-number="77">     total.cost      =<span class="st"> </span><span class="kw">c</span>(total.cost, batch.cost)</a>
<a class="sourceLine" id="cb2050-78" data-line-number="78">     total.accuracy  =<span class="st"> </span><span class="kw">c</span>(total.accuracy, batch.accuracy)</a>
<a class="sourceLine" id="cb2050-79" data-line-number="79">     total.validate  =<span class="st"> </span><span class="kw">c</span>(total.validate, valid.accurate)</a>
<a class="sourceLine" id="cb2050-80" data-line-number="80">     <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-81" data-line-number="81">     <span class="st">&quot;epoch %d: loss %2.3f accuracy %2.3f val %2.3f lag time (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2050-82" data-line-number="82">     t, <span class="kw">mean</span>(batch.cost),  <span class="kw">mean</span>(batch.accuracy), valid.accurate, lag.time)</a>
<a class="sourceLine" id="cb2050-83" data-line-number="83">     <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) { <span class="kw">gc</span>() } <span class="co"># garbage collection</span></a>
<a class="sourceLine" id="cb2050-84" data-line-number="84">     <span class="cf">if</span> (valid.accurate <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.90</span>) {</a>
<a class="sourceLine" id="cb2050-85" data-line-number="85">       <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2050-86" data-line-number="86">       <span class="st">&quot;We have reached a validation accuracy of %2.3f. This is good enough&quot;</span>,</a>
<a class="sourceLine" id="cb2050-87" data-line-number="87">       valid.accurate)</a>
<a class="sourceLine" id="cb2050-88" data-line-number="88">       <span class="cf">break</span></a>
<a class="sourceLine" id="cb2050-89" data-line-number="89">     }</a>
<a class="sourceLine" id="cb2050-90" data-line-number="90">  } </a>
<a class="sourceLine" id="cb2050-91" data-line-number="91">  <span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>model, <span class="st">&quot;layers&quot;</span> =<span class="st"> </span>layers, <span class="st">&quot;eta&quot;</span> =<span class="st"> </span>eta, </a>
<a class="sourceLine" id="cb2050-92" data-line-number="92">       <span class="st">&quot;cost&quot;</span> =<span class="st"> </span>epoch.cost, <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span>epoch.accuracy, </a>
<a class="sourceLine" id="cb2050-93" data-line-number="93">       <span class="st">&quot;total.cost&quot;</span> =<span class="st"> </span>total.cost, <span class="st">&quot;total.accuracy&quot;</span> =<span class="st"> </span>total.accuracy,</a>
<a class="sourceLine" id="cb2050-94" data-line-number="94">       <span class="st">&quot;total.valid&quot;</span> =<span class="st"> </span>total.validate)</a>
<a class="sourceLine" id="cb2050-95" data-line-number="95"></a>
<a class="sourceLine" id="cb2050-96" data-line-number="96">}</a></code></pre></div>

<p>In the following two sections, we discuss the use of <strong>GEMM</strong> in our implementation and a more efficient convolution algorithm, namely <strong>Depthwise Separable Convolution</strong> and <strong>Pointwise Convolution</strong>.</p>
</div>
<div id="cnn-application" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.16</span> CNN Application<a href="12.4-convolutional-neural-network-cnn.html#cnn-application" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us perform image classification using <strong>CIFAR-10</strong> as our dataset, containing 60000 <span class="math inline">\(32 \times 32\)</span> tiny images <span class="citation">(Krizhevsky A. <a href="bibliography.html#ref-ref1350a">2009</a>)</span>. Here, we use the binary version of the dataset. At the time of writing, the currently available dataset is broken down into six files: data_batch{1,2,3,4,5}.bin and a test_batch.bin. However, because the dataset is a collection of images instead of one single <strong>JPEG</strong> image, we have to read differently such that instead of using <strong>readJPEG(.)</strong>, we use <strong>readBin(.)</strong> to read raw data in binary format.</p>
<p>Below is a modified implementation of reading and parsing the <strong>CIFAR-10</strong> dataset (motivated by an R script written by an anonymous <strong>matt</strong> from Stackoverflow (questions/32113942)):</p>

<div class="sourceCode" id="cb2051"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2051-1" data-line-number="1">getNext &lt;-<span class="st"> </span><span class="cf">function</span>(fp, <span class="dt">typ =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb2051-2" data-line-number="2">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(typ) <span class="op">&amp;&amp;</span><span class="st"> </span>typ <span class="op">==</span><span class="st"> &quot;label&quot;</span>) {</a>
<a class="sourceLine" id="cb2051-3" data-line-number="3">        <span class="kw">readBin</span>(fp, <span class="kw">integer</span>(), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>)</a>
<a class="sourceLine" id="cb2051-4" data-line-number="4">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2051-5" data-line-number="5">        <span class="kw">as.integer</span>(<span class="kw">readBin</span>(fp, <span class="kw">raw</span>(), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">n=</span><span class="dv">1024</span>, <span class="dt">endian=</span><span class="st">&quot;big&quot;</span>))      </a>
<a class="sourceLine" id="cb2051-6" data-line-number="6">    }</a>
<a class="sourceLine" id="cb2051-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2051-8" data-line-number="8">readBatch &lt;-<span class="st"> </span><span class="cf">function</span>(fn, images, labels) {</a>
<a class="sourceLine" id="cb2051-9" data-line-number="9">    fp         =<span class="st"> </span><span class="kw">file</span>(fn, <span class="st">&quot;rb&quot;</span>)</a>
<a class="sourceLine" id="cb2051-10" data-line-number="10">    i =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>lab)</a>
<a class="sourceLine" id="cb2051-11" data-line-number="11">    <span class="cf">for</span> (g <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">10000</span>) {</a>
<a class="sourceLine" id="cb2051-12" data-line-number="12">        i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2051-13" data-line-number="13">        images<span class="op">$</span>lab[i] =<span class="st"> </span>labels[ <span class="kw">getNext</span>(fp, <span class="dt">ty =</span> <span class="st">&quot;label&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, ]</a>
<a class="sourceLine" id="cb2051-14" data-line-number="14">        images<span class="op">$</span>rgb[[i]] =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">r =</span> <span class="kw">getNext</span>(fp), <span class="dt">g =</span> <span class="kw">getNext</span>(fp), </a>
<a class="sourceLine" id="cb2051-15" data-line-number="15">                                      <span class="dt">b =</span> <span class="kw">getNext</span>(fp))</a>
<a class="sourceLine" id="cb2051-16" data-line-number="16">    }</a>
<a class="sourceLine" id="cb2051-17" data-line-number="17">    <span class="kw">close</span>(fp)</a>
<a class="sourceLine" id="cb2051-18" data-line-number="18">    <span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span>images<span class="op">$</span>lab, <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span>images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2051-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb2051-20" data-line-number="20">extract.label &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, index) {</a>
<a class="sourceLine" id="cb2051-21" data-line-number="21">    labels[ images<span class="op">$</span>lab[[index]],]</a>
<a class="sourceLine" id="cb2051-22" data-line-number="22">}</a>
<a class="sourceLine" id="cb2051-23" data-line-number="23">extract.label.onehot &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, index) {</a>
<a class="sourceLine" id="cb2051-24" data-line-number="24">    len =<span class="st"> </span><span class="kw">length</span>(index)</a>
<a class="sourceLine" id="cb2051-25" data-line-number="25">    list.label =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(len,<span class="dv">10</span>))</a>
<a class="sourceLine" id="cb2051-26" data-line-number="26">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2051-27" data-line-number="27">      n =<span class="st"> </span><span class="kw">nrow</span>(labels)</a>
<a class="sourceLine" id="cb2051-28" data-line-number="28">      label =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb2051-29" data-line-number="29">      label[images<span class="op">$</span>lab[[index[i]]]] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2051-30" data-line-number="30">      list.label[i,] =<span class="st"> </span>label</a>
<a class="sourceLine" id="cb2051-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb2051-32" data-line-number="32">    list.label</a>
<a class="sourceLine" id="cb2051-33" data-line-number="33">}</a></code></pre></div>
<div class="sourceCode" id="cb2052"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2052-1" data-line-number="1">extract.image &lt;-<span class="st"> </span><span class="cf">function</span>(images, index) {</a>
<a class="sourceLine" id="cb2052-2" data-line-number="2">  len =<span class="st"> </span><span class="kw">length</span>(index)</a>
<a class="sourceLine" id="cb2052-3" data-line-number="3">  list.img =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>, len))</a>
<a class="sourceLine" id="cb2052-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2052-5" data-line-number="5">   img   =<span class="st"> </span>images<span class="op">$</span>rgb[[index[i]]]</a>
<a class="sourceLine" id="cb2052-6" data-line-number="6">   r     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>r, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-7" data-line-number="7">   g     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>g, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-8" data-line-number="8">   b     =<span class="st"> </span><span class="kw">matrix</span>(img<span class="op">$</span>b, <span class="dt">ncol=</span><span class="dv">32</span>, <span class="dt">byrow =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2052-9" data-line-number="9">   norm  =<span class="st"> </span><span class="dv">255</span></a>
<a class="sourceLine" id="cb2052-10" data-line-number="10">   img   =<span class="st"> </span><span class="kw">array</span>(<span class="kw">cbind</span>(r,g,b) <span class="op">/</span><span class="st"> </span>norm, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>))  <span class="co"># normalize</span></a>
<a class="sourceLine" id="cb2052-11" data-line-number="11">   list.img[,,,i] =<span class="st"> </span>img</a>
<a class="sourceLine" id="cb2052-12" data-line-number="12">  }</a>
<a class="sourceLine" id="cb2052-13" data-line-number="13">  list.img</a>
<a class="sourceLine" id="cb2052-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb2052-15" data-line-number="15">search.image &lt;-<span class="st"> </span><span class="cf">function</span>(images, labels, <span class="dt">label =</span> <span class="st">&#39;ship&#39;</span>) {</a>
<a class="sourceLine" id="cb2052-16" data-line-number="16">  idx =<span class="st"> </span><span class="kw">which</span>(labels <span class="op">==</span><span class="st"> </span>label)</a>
<a class="sourceLine" id="cb2052-17" data-line-number="17">  len =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2052-18" data-line-number="18">  what =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb2052-19" data-line-number="19">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>len) {</a>
<a class="sourceLine" id="cb2052-20" data-line-number="20">    <span class="cf">if</span> (images<span class="op">$</span>lab[[i]] <span class="op">==</span><span class="st"> </span>idx) {</a>
<a class="sourceLine" id="cb2052-21" data-line-number="21">        what =<span class="st"> </span><span class="kw">c</span>(what, i)</a>
<a class="sourceLine" id="cb2052-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb2052-23" data-line-number="23">  }</a>
<a class="sourceLine" id="cb2052-24" data-line-number="24">  what</a>
<a class="sourceLine" id="cb2052-25" data-line-number="25">}</a>
<a class="sourceLine" id="cb2052-26" data-line-number="26">draw.image &lt;-<span class="st"> </span><span class="cf">function</span>(image, <span class="dt">main=</span><span class="st">&quot;Apple, Oranges, and Banana&quot;</span>) { </a>
<a class="sourceLine" id="cb2052-27" data-line-number="27">  sz =<span class="st"> </span><span class="kw">nrow</span>(image)</a>
<a class="sourceLine" id="cb2052-28" data-line-number="28">  <span class="kw">par</span>(<span class="dt">pty=</span><span class="st">&quot;s&quot;</span>) </a>
<a class="sourceLine" id="cb2052-29" data-line-number="29">  <span class="kw">plot</span>(<span class="dv">1</span>, <span class="dt">type=</span><span class="st">&quot;n&quot;</span>, <span class="dt">xlim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), <span class="dt">ylim=</span><span class="kw">c</span>(<span class="dv">1</span>, sz), </a>
<a class="sourceLine" id="cb2052-30" data-line-number="30">     <span class="dt">xlab=</span><span class="st">&quot;Image Width&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Image Height&quot;</span>, <span class="dt">main=</span>main)</a>
<a class="sourceLine" id="cb2052-31" data-line-number="31">  <span class="kw">rasterImage</span>(image,<span class="dt">xleft=</span><span class="dv">1</span>, <span class="dt">ybottom=</span>sz, <span class="dt">xright=</span>sz, <span class="dt">ytop=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2052-32" data-line-number="32">}</a></code></pre></div>

<p>We then use our <strong>draw.image(.)</strong> function like so (for example, reading the 20th image in the dataset):</p>

<div class="sourceCode" id="cb2053"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2053-1" data-line-number="1">images =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span><span class="kw">list</span>(), <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span><span class="kw">list</span>())</a>
<a class="sourceLine" id="cb2053-2" data-line-number="2">labels =<span class="st"> </span><span class="kw">read.table</span>(<span class="st">&quot;cifar-10-batches-bin/batches.meta.txt&quot;</span>)</a>
<a class="sourceLine" id="cb2053-3" data-line-number="3"><span class="cf">for</span> (f <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb2053-4" data-line-number="4">    fn       =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;cifar-10-batches-bin/data_batch_&quot;</span>, f, <span class="st">&quot;.bin&quot;</span>)</a>
<a class="sourceLine" id="cb2053-5" data-line-number="5">    images =<span class="st"> </span><span class="kw">readBatch</span>(fn, images, labels ) </a>
<a class="sourceLine" id="cb2053-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb2053-7" data-line-number="7">this.image =<span class="st"> </span><span class="kw">extract.image</span>(images,<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb2053-8" data-line-number="8">this.label =<span class="st"> </span><span class="kw">extract.label</span>(images,labels,<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb2053-9" data-line-number="9"><span class="kw">draw.image</span>(<span class="kw">array</span>(this.image, <span class="kw">c</span>(<span class="dv">32</span>,<span class="dv">32</span>,<span class="dv">3</span>)) , <span class="dt">main=</span>this.label)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cifar1020"></span>
<img src="cifarfrog.png" alt="CIFAR-10 (20th Image)" width="70%" />
<p class="caption">
Figure 12.46: CIFAR-10 (20th Image)
</p>
</div>

<p>The image has the following dimension:</p>

<div class="sourceCode" id="cb2054"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2054-1" data-line-number="1">image =<span class="st"> </span><span class="kw">extract.image</span>(images,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2054-2" data-line-number="2"><span class="kw">dim</span>(image)</a></code></pre></div>
<pre><code>## [1] 32 32  3  1</code></pre>

<p>The corresponding label is translated into a one-hot vector with the 7th index matching the label for a <strong>frog</strong>:</p>

<div class="sourceCode" id="cb2056"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2056-1" data-line-number="1">label =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2056-2" data-line-number="2">label</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    1    0    0     0</code></pre>
<div class="sourceCode" id="cb2058"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2058-1" data-line-number="1"><span class="kw">t</span>(labels)</a></code></pre></div>
<pre><code>##    [,1]       [,2]         [,3]   [,4]  [,5]   [,6] 
## V1 &quot;airplane&quot; &quot;automobile&quot; &quot;bird&quot; &quot;cat&quot; &quot;deer&quot; &quot;dog&quot;
##    [,7]   [,8]    [,9]   [,10]  
## V1 &quot;frog&quot; &quot;horse&quot; &quot;ship&quot; &quot;truck&quot;</code></pre>

<p>With that, let us now design our <strong>CNN layers</strong> like so:</p>

<div class="sourceCode" id="cb2060"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2060-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb2060-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2060-3" data-line-number="3">size =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2060-4" data-line-number="4">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2060-5" data-line-number="5">minibatch =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2060-6" data-line-number="6">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>, size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth <span class="op">*</span><span class="st"> </span>minibatch), </a>
<a class="sourceLine" id="cb2060-7" data-line-number="7">          <span class="kw">c</span>(size, size, depth, minibatch))</a>
<a class="sourceLine" id="cb2060-8" data-line-number="8"><span class="kw">dim</span>(X)</a></code></pre></div>
<pre><code>## [1] 32 32  3 32</code></pre>
<div class="sourceCode" id="cb2062"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2062-1" data-line-number="1">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2062-2" data-line-number="2"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb2062-3" data-line-number="3">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),     </a>
<a class="sourceLine" id="cb2062-4" data-line-number="4"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">2</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2062-5" data-line-number="5"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">64</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2062-6" data-line-number="6">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2062-7" data-line-number="7"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;pooling&quot;</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">stride=</span><span class="dv">2</span>, <span class="dt">ptype=</span><span class="st">&quot;maxpool&quot;</span>),</a>
<a class="sourceLine" id="cb2062-8" data-line-number="8"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>,  <span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">256</span>, <span class="dt">drop=</span><span class="fl">0.05</span> ),<span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">10</span> ))</a>
<a class="sourceLine" id="cb2062-9" data-line-number="9">)</a></code></pre></div>

<p>We then invoke our <strong>my.CNN(.)</strong> function, feeding a minibatch=32 and epoch=10. See plot in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:cnnplot">12.47</a>.</p>

<div class="sourceCode" id="cb2063"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2063-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2063-2" data-line-number="2">cnn.model =<span class="st"> </span><span class="kw">my.CNN</span>(images, labels, <span class="dt">layers =</span> cnn.layers, <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,</a>
<a class="sourceLine" id="cb2063-3" data-line-number="3">                               <span class="dt">minibatch=</span>minibatch, <span class="dt">epoch=</span><span class="dv">10</span>, <span class="dt">eta=</span><span class="fl">0.001</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 8&quot;
## [1] &quot;epoch 1: loss 6.246 accuracy 0.146 val 0.161 lag time (sec): 13.789&quot;
## [1] &quot;epoch 2: loss 3.078 accuracy 0.273 val 0.194 lag time (sec): 13.122&quot;
## [1] &quot;epoch 3: loss 1.599 accuracy 0.508 val 0.000 lag time (sec): 12.193&quot;
## [1] &quot;epoch 4: loss 1.251 accuracy 0.580 val 0.258 lag time (sec): 12.990&quot;
## [1] &quot;epoch 5: loss 0.982 accuracy 0.689 val 0.000 lag time (sec): 12.321&quot;
## [1] &quot;epoch 6: loss 0.842 accuracy 0.736 val 0.125 lag time (sec): 12.453&quot;
## [1] &quot;epoch 7: loss 0.602 accuracy 0.845 val 0.065 lag time (sec): 12.604&quot;
## [1] &quot;epoch 8: loss 0.408 accuracy 0.916 val 0.094 lag time (sec): 12.377&quot;
## [1] &quot;epoch 9: loss 0.307 accuracy 0.959 val 0.194 lag time (sec): 13.160&quot;
## [1] &quot;epoch 10: loss 0.262 accuracy 0.955 val 0.219 lag time (sec): 13.669&quot;</code></pre>
<div class="sourceCode" id="cb2065"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2065-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(cnn.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2065-2" data-line-number="2">y =<span class="st"> </span>cnn.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2065-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2065-4" data-line-number="4">y2 =<span class="st"> </span>cnn.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2065-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2065-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2065-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;CNN (250, 32 - 32dm, 64dm, fc256)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2065-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2065-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2065-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cnnplot"></span>
<img src="DS_files/figure-html/cnnplot-1.png" alt="CNN PLOT" width="70%" />
<p class="caption">
Figure 12.47: CNN PLOT
</p>
</div>

<p>Notice in the figure that we trained only 250 images. It is worth noting that our <strong>CNN</strong> implementation indeed demonstrates the ability to train. We can shoot to 96.8% (even up to 100%) <strong>top-1</strong> train accuracy. However, if we perform prediction, we may see our <strong>top-1</strong> test accuracy as low as 20%. That is because there is insufficient data to train; thus, we are overfitting the train data.</p>
<p>Over time, we performed several iterations to achieve a better train and test accuracy using the entire 50,000 images. Our model was trained with a <strong>top-1</strong> train accuracy of 94.20% after epoch 22 for 50000 cifar-10 images, which took 19.8 hours with an average lag time of 3750 seconds per epoch using only four CPUs (from quad-core) with no GPU support. Here, we show how we can use our saved model:</p>

<div class="sourceCode" id="cb2066"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2066-1" data-line-number="1">transfer.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;transfer_model.rds&quot;</span>) </a></code></pre></div>

<p>The model uses the following neural network configuration:</p>

<div class="sourceCode" id="cb2067"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2067-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb2067-2" data-line-number="2">size =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2067-3" data-line-number="3">depth =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2067-4" data-line-number="4">minibatch =<span class="st"> </span><span class="dv">32</span></a>
<a class="sourceLine" id="cb2067-5" data-line-number="5">X =<span class="st"> </span><span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>, size <span class="op">*</span><span class="st"> </span>size <span class="op">*</span><span class="st"> </span>depth <span class="op">*</span><span class="st"> </span>minibatch), </a>
<a class="sourceLine" id="cb2067-6" data-line-number="6">          <span class="kw">c</span>(size, size, depth, minibatch))</a>
<a class="sourceLine" id="cb2067-7" data-line-number="7"><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2067-8" data-line-number="8">cnn.layers =<span class="st"> </span><span class="kw">deep.cnn.layers</span>( X, </a>
<a class="sourceLine" id="cb2067-9" data-line-number="9"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">1</span>, <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-10" data-line-number="10">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-11" data-line-number="11"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">32</span>, <span class="dt">stride=</span><span class="dv">2</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-12" data-line-number="12">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-13" data-line-number="13"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">92</span>, <span class="dt">stride=</span><span class="dv">1</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-14" data-line-number="14">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>), </a>
<a class="sourceLine" id="cb2067-15" data-line-number="15"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;convolv&quot;</span>, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">filters=</span><span class="dv">92</span>, <span class="dt">stride=</span><span class="dv">2</span>,  <span class="dt">padding=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb2067-16" data-line-number="16">      <span class="dt">normalize=</span><span class="st">&quot;batch&quot;</span>, <span class="dt">afunc=</span><span class="st">&quot;leaky.relu&quot;</span>),</a>
<a class="sourceLine" id="cb2067-17" data-line-number="17"><span class="kw">list</span>( <span class="dt">type =</span> <span class="st">&quot;dense&quot;</span>, <span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">1024</span>, <span class="dt">drop=</span><span class="fl">0.95</span>),<span class="kw">list</span>(  <span class="dt">size=</span><span class="dv">10</span> ))</a>
<a class="sourceLine" id="cb2067-18" data-line-number="18">)</a></code></pre></div>

<p>The <strong>CNN</strong> architecture shows only four convolution layers and one dense layer with dropout. Instead of pooling, we adjust the stride to be 2 in the first and fourth convolution layers to reduce dimensionality.</p>
<p>With that, the model gives us a plot of the cost and accuracy in Figure <a href="12.4-convolutional-neural-network-cnn.html#fig:transferred">12.48</a>. Note that the line, in increasing direction (and in navy blue color), represents accuracy.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:transferred"></span>
<img src="DS_files/figure-html/transferred-1.png" alt="Transferred Model (CNN)" width="80%" />
<p class="caption">
Figure 12.48: Transferred Model (CNN)
</p>
</div>

<p>To generate prediction, we use the <strong>my.predict.NN(.)</strong> function below:</p>

<div class="sourceCode" id="cb2068"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2068-1" data-line-number="1">get.test.batch &lt;-<span class="st"> </span><span class="cf">function</span>(sample.data, labels, k, t) {  </a>
<a class="sourceLine" id="cb2068-2" data-line-number="2">   <span class="kw">set.seed</span>(<span class="dv">2019</span>)</a>
<a class="sourceLine" id="cb2068-3" data-line-number="3">   batch.indices =<span class="st"> </span><span class="kw">createFolds</span>(sample.data, <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-4" data-line-number="4">   batches =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2068-5" data-line-number="5">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>k) {</a>
<a class="sourceLine" id="cb2068-6" data-line-number="6">       indices =<span class="st"> </span>batch.indices[[i]]</a>
<a class="sourceLine" id="cb2068-7" data-line-number="7">       X =<span class="st"> </span><span class="kw">extract.image</span>(images, indices)</a>
<a class="sourceLine" id="cb2068-8" data-line-number="8">       Y =<span class="st"> </span><span class="kw">extract.label.onehot</span>(images, labels, indices) </a>
<a class="sourceLine" id="cb2068-9" data-line-number="9">         batches[[i]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;X&quot;</span> =<span class="st"> </span>X, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y )</a>
<a class="sourceLine" id="cb2068-10" data-line-number="10">   }</a>
<a class="sourceLine" id="cb2068-11" data-line-number="11">   batches</a>
<a class="sourceLine" id="cb2068-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb2068-13" data-line-number="13">my.predict.CNN &lt;-<span class="st"> </span><span class="cf">function</span>(test.images, labels,  model, <span class="dt">minibatch=</span><span class="dv">32</span>,</a>
<a class="sourceLine" id="cb2068-14" data-line-number="14">                           <span class="dt">first_batch_only=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2068-15" data-line-number="15">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">16</span>)  <span class="co"># 16 digits precision for our example</span></a>
<a class="sourceLine" id="cb2068-16" data-line-number="16">  layers        =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2068-17" data-line-number="17">  img.len       =<span class="st"> </span><span class="kw">length</span>(images<span class="op">$</span>rgb)</a>
<a class="sourceLine" id="cb2068-18" data-line-number="18">    </a>
<a class="sourceLine" id="cb2068-19" data-line-number="19">  <span class="co"># Test images</span></a>
<a class="sourceLine" id="cb2068-20" data-line-number="20">  population     =<span class="st"> </span>test.images<span class="op">$</span>rgb</a>
<a class="sourceLine" id="cb2068-21" data-line-number="21">  population.len =<span class="st"> </span><span class="kw">length</span>(population)</a>
<a class="sourceLine" id="cb2068-22" data-line-number="22">  sample.set     =<span class="st"> </span>population.len</a>
<a class="sourceLine" id="cb2068-23" data-line-number="23">  shuffled.data  =<span class="st"> </span><span class="kw">sample.int</span>(<span class="dt">n=</span>population.len, <span class="dt">size=</span>population.len, </a>
<a class="sourceLine" id="cb2068-24" data-line-number="24">                              <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-25" data-line-number="25">  sample.size    =<span class="st"> </span><span class="kw">ifelse</span>(sample.set <span class="op">&lt;</span><span class="st"> </span>population.len, sample.set, </a>
<a class="sourceLine" id="cb2068-26" data-line-number="26">                          population.len)</a>
<a class="sourceLine" id="cb2068-27" data-line-number="27">  sample.data    =<span class="st"> </span><span class="kw">sample</span>(shuffled.data, sample.size)</a>
<a class="sourceLine" id="cb2068-28" data-line-number="28">  k              =<span class="st"> </span><span class="kw">ceiling</span>(sample.size <span class="op">/</span><span class="st"> </span>minibatch)    </a>
<a class="sourceLine" id="cb2068-29" data-line-number="29">  my.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2068-30" data-line-number="30">  <span class="kw">flush.str</span>( <span class="st">&quot;No of batches: %d&quot;</span>, k )</a>
<a class="sourceLine" id="cb2068-31" data-line-number="31">  n =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2068-32" data-line-number="32">  total.accuracy =<span class="st"> </span><span class="dv">0</span>; total.loss =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2068-33" data-line-number="33">  <span class="cf">for</span> (batch <span class="cf">in</span> <span class="kw">get.test.batch</span>(sample.data, labels, k, t)) {</a>
<a class="sourceLine" id="cb2068-34" data-line-number="34">    n             =<span class="st"> </span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>        </a>
<a class="sourceLine" id="cb2068-35" data-line-number="35">    X             =<span class="st"> </span>batch<span class="op">$</span>X</a>
<a class="sourceLine" id="cb2068-36" data-line-number="36">    Y             =<span class="st"> </span>batch<span class="op">$</span>Y   </a>
<a class="sourceLine" id="cb2068-37" data-line-number="37">    test.model    =<span class="st"> </span><span class="kw">forward.pass.cnn</span>(X, layers, <span class="dt">train=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2068-38" data-line-number="38">    len           =<span class="st"> </span><span class="kw">length</span>(test.model<span class="op">$</span>fc.model<span class="op">$</span>layers)</a>
<a class="sourceLine" id="cb2068-39" data-line-number="39">    softmax.prob  =<span class="st"> </span>test.model<span class="op">$</span>fc.model<span class="op">$</span>layers[[len]]<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2068-40" data-line-number="40"></a>
<a class="sourceLine" id="cb2068-41" data-line-number="41">    loss          =<span class="st"> </span><span class="kw">softmax.loss</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2068-42" data-line-number="42">    test.accurate =<span class="st"> </span><span class="kw">accuracy</span>(Y, softmax.prob)</a>
<a class="sourceLine" id="cb2068-43" data-line-number="43">    total.loss     =<span class="st"> </span><span class="kw">c</span>(total.loss, <span class="kw">mean</span>(loss))</a>
<a class="sourceLine" id="cb2068-44" data-line-number="44">    total.accuracy =<span class="st"> </span><span class="kw">c</span>(total.accuracy, test.accurate)</a>
<a class="sourceLine" id="cb2068-45" data-line-number="45">      </a>
<a class="sourceLine" id="cb2068-46" data-line-number="46">    <span class="cf">if</span> (n <span class="op">%%</span><span class="st"> </span><span class="dv">10</span> <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb2068-47" data-line-number="47">       new.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2068-48" data-line-number="48">       lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, my.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>) </a>
<a class="sourceLine" id="cb2068-49" data-line-number="49">       my.time  =<span class="st"> </span>new.time</a>
<a class="sourceLine" id="cb2068-50" data-line-number="50">       stime =<span class="st"> </span><span class="kw">format</span>(<span class="kw">Sys.Date</span>(), <span class="st">&quot;%c&quot;</span>)</a>
<a class="sourceLine" id="cb2068-51" data-line-number="51">       <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2068-52" data-line-number="52">      <span class="st">&quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lag time (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2068-53" data-line-number="53">                  n, <span class="kw">mean</span>(loss),   n, test.accurate, lag.time)</a>
<a class="sourceLine" id="cb2068-54" data-line-number="54">       <span class="cf">if</span> (first_batch_only <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) { <span class="cf">break</span> }</a>
<a class="sourceLine" id="cb2068-55" data-line-number="55">    }</a>
<a class="sourceLine" id="cb2068-56" data-line-number="56"></a>
<a class="sourceLine" id="cb2068-57" data-line-number="57">  }</a>
<a class="sourceLine" id="cb2068-58" data-line-number="58">  <span class="kw">list</span>(<span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(total.accuracy), <span class="st">&quot;loss&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(total.loss),</a>
<a class="sourceLine" id="cb2068-59" data-line-number="59">       <span class="st">&quot;prediction&quot;</span> =<span class="st"> </span>softmax.prob, <span class="st">&quot;target&quot;</span> =<span class="st"> </span>Y)</a>
<a class="sourceLine" id="cb2068-60" data-line-number="60">}</a></code></pre></div>

<p>Along with that, we use the test images from the <strong>cifar-10</strong> dataset.</p>

<div class="sourceCode" id="cb2069"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2069-1" data-line-number="1">test.images =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;label&quot;</span> =<span class="st"> </span><span class="kw">list</span>(), <span class="st">&quot;rgb&quot;</span> =<span class="st"> </span><span class="kw">list</span>())</a>
<a class="sourceLine" id="cb2069-2" data-line-number="2">fn       =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;cifar-10-batches-bin/test_batch.bin&quot;</span>)</a>
<a class="sourceLine" id="cb2069-3" data-line-number="3">test.images =<span class="st"> </span><span class="kw">readBatch</span>(fn, test.images, labels ) </a></code></pre></div>

<p>Now, we run prediction and get the result - let us show the result of the first batch only.</p>

<div class="sourceCode" id="cb2070"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2070-1" data-line-number="1">result =<span class="st"> </span><span class="kw">my.predict.CNN</span>(test.images, labels, transfer.cnn.model, </a>
<a class="sourceLine" id="cb2070-2" data-line-number="2">                        <span class="dt">first_batch_only =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 313&quot;
## [1] &quot;batch 10 - loss: 0.170 t: 10, accuracy 0.969 lag time (sec): 7.300&quot;</code></pre>

<p>That gives us a <strong>top-1</strong> test accuracy of 87.78% and a loss of 0.122853209833548.</p>
<p>Below, we see the actual predictions compared to the target:</p>

<div class="sourceCode" id="cb2072"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2072-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2072-2" data-line-number="2"><span class="kw">round</span>(result<span class="op">$</span>prediction[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,],<span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0 0.00 0.00    0 0.00    0    1    0 0.00  0.00
## [2,]    0 0.00 0.00    0 0.00    0    0    1 0.00  0.00
## [3,]    0 0.85 0.05    0 0.01    0    0    0 0.00  0.09
## [4,]    0 0.00 0.99    0 0.00    0    0    0 0.00  0.00
## [5,]    0 0.02 0.00    0 0.00    0    0    0 0.98  0.00</code></pre>
<div class="sourceCode" id="cb2074"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2074-1" data-line-number="1">result<span class="op">$</span>target[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    1    0    0     0
## [2,]    0    0    0    0    0    0    0    1    0     0
## [3,]    0    1    0    0    0    0    0    0    0     0
## [4,]    0    0    1    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    0    0    0    1     0</code></pre>

<p>Heuristically, we have observed that our implementation reaches a <strong>local minima</strong> somewhere at epoch 22 with about 96% train accuracy using a learning rate of 0.0001. If we decay the learning rate by 0.10 to 0.00001 at epoch 20, we get a little bit more increase in accuracy. For example, below, we can use a step decay like so:</p>

<div class="sourceCode" id="cb2076"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2076-1" data-line-number="1"><span class="kw">step.decay</span>(<span class="fl">0.0001</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">30</span>), <span class="dt">decay.factor=</span><span class="fl">0.10</span>, <span class="dt">step.size=</span><span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04
##  [9] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04
## [17] 1e-04 1e-04 1e-04 1e-05 1e-05 1e-05 1e-05 1e-05
## [25] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05</code></pre>

<p>Alternatively, as we have already trained our model, we can instead use the saved <strong>transfer.cnn.model</strong> to be re-trained via <strong>transfer learning</strong> and manually adjust the learning rate to 0.00001.</p>

<div class="sourceCode" id="cb2078"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2078-1" data-line-number="1"><span class="kw">system.time</span>((<span class="dt">retrained_model =</span> <span class="kw">my.CNN</span>(images, labels, <span class="dt">layers =</span> cnn.layers,</a>
<a class="sourceLine" id="cb2078-2" data-line-number="2">              <span class="dt">optimize=</span><span class="st">&quot;adam&quot;</span>,  <span class="dt">minibatch=</span>minibatch, <span class="dt">epoch=</span><span class="dv">10</span>, <span class="dt">eta=</span><span class="fl">0.00001</span>, </a>
<a class="sourceLine" id="cb2078-3" data-line-number="3">              <span class="dt">transfer=</span><span class="st">&quot;transfer_model.rds&quot;</span>)))</a></code></pre></div>

<p>After a few more epochs during the training, we again try to execute our prediction. Note that because training takes much longer, we are now using the saved model instead, namely <strong>retrained.cnn.model</strong>, as shown:</p>

<div class="sourceCode" id="cb2079"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2079-1" data-line-number="1">retrained.cnn.model =<span class="st"> </span><span class="kw">readRDS</span>(<span class="st">&quot;retrained_model.rds&quot;</span>) </a>
<a class="sourceLine" id="cb2079-2" data-line-number="2">result =<span class="st"> </span><span class="kw">my.predict.CNN</span>(test.images, labels, retrained.cnn.model, </a>
<a class="sourceLine" id="cb2079-3" data-line-number="3">                        <span class="dt">first_batch_only=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] &quot;No of batches: 313&quot;
## [1] &quot;batch 10 - loss: 0.037 t: 10, accuracy 1.000 lag time (sec): 7.211&quot;</code></pre>

<p>That gives us a <strong>top-1</strong> test accuracy of 89.49% and a loss of 0.0718042660598904.</p>
<p>Below, we see the actual predictions compared to the target:</p>

<div class="sourceCode" id="cb2081"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2081-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2081-2" data-line-number="2"><span class="kw">round</span>(result<span class="op">$</span>prediction[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,],<span class="dv">2</span>)[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,] 0.00    0 0.00 0.00    0 0.00 0.00 0.00    0     1
## [2,] 0.00    0 0.99 0.00    0 0.00 0.01 0.00    0     0
## [3,] 0.00    0 0.00 0.98    0 0.02 0.00 0.00    0     0
## [4,] 0.99    0 0.00 0.00    0 0.00 0.00 0.00    0     0
## [5,] 0.00    0 0.00 0.00    0 0.96 0.00 0.04    0     0</code></pre>
<div class="sourceCode" id="cb2083"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2083-1" data-line-number="1">result<span class="op">$</span>target[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>,]</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## [1,]    0    0    0    0    0    0    0    0    0     1
## [2,]    0    0    1    0    0    0    0    0    0     0
## [3,]    0    0    0    1    0    0    0    0    0     0
## [4,]    1    0    0    0    0    0    0    0    0     0
## [5,]    0    0    0    0    0    1    0    0    0     0</code></pre>

</div>
<div id="summary-7" class="section level3 hasAnchor">
<h3><span class="header-section-number">12.4.17</span> Summary<a href="12.4-convolutional-neural-network-cnn.html#summary-7" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Convolutional Neural Network</strong> is a topic that is wide and deep. There are more than a handful of knobs to cover. The interplay of these knobs is essential to get where we want to be in train and test accuracy. Each knob deserves a section for illustration and discussion. However, one section alone does not give justice to each. Our previous discussions perhaps only scratch the surface.</p>
<p>Nevertheless, we leave readers to continue investigating novel tricks and techniques to improve <strong>CNN</strong> and to read about <strong>Data Augmentation</strong> and <strong>Adversarial Sampling</strong>. The former - <strong>Data Augmentation</strong> - allows us to use newly added datasets for further training based on existing datasets that are synthetically altered (e.g., image transformation). Given an insufficient dataset, the technique is helpful if we need to improve test accuracy. The latter - <strong>Adversarial Sampling</strong> - performs a similar technique as <strong>Augmentation</strong>; however, we need to be wary about intentionally tampered datasets that could mislead training.</p>
<p>It is also worth mentioning that our <strong>CNN</strong> implementation is tailored based on a machine with minimal computing capability. However, advanced frameworks can perform <strong>distributed</strong> (parallel) processing across powerful machines with <strong>GPU</strong> support, achieving speeds down to seconds instead of hours. We leave readers to explore this next.</p>

</div>
</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="12.3-multi-layer-perceptron-mlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="13-deeplearning2.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
