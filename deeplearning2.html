<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 13 Computational Deep Learning II | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 13 Computational Deep Learning II | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 13 Computational Deep Learning II | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="deeplearning1.html"/>
<link rel="next" href="distributedcomputation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="deeplearning2" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 13</span> Computational Deep Learning II<a href="deeplearning2.html#deeplearning2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '13.'+n}
      } 
  }
});
</script>
<p>In this chapter, we continue to build our intuition around <strong>Neural Networks</strong> by introducing three other types of <strong>ANN</strong>, namely <strong>Residual Network (ResNet)</strong>, <strong>Recurrent Neural Network (RNN)</strong>, and <strong>Transformer Neural Network (TNN)</strong> (or simply <strong>Transformer</strong>).</p>
<div id="residual-network-resnet" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.1</span> Residual Network (ResNet)  <a href="deeplearning2.html#residual-network-resnet" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall in <strong>MLP</strong> and <strong>CNN</strong> that <strong>DropOut</strong> is one of the strategies we use to inject regularization into the network. The idea is to push the output of randomly selected neurons to zero, thus negating their effects on subsequent layers. Doing so also helps to generalize our model. In essence, <strong>DropOut</strong> by its description, is a type of a <strong>skip connection</strong> strategy.</p>
<p>There is another <strong>skip connection</strong> strategy called <strong>residual connection</strong> as introduced in <strong>ResNet</strong> architecture. To illustrate, let us use Figure <a href="deeplearning2.html#fig:resnetunit">13.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resnetunit"></span>
<img src="resnetunit.png" alt="Residual Network (ResNet) Unit" width="70%" />
<p class="caption">
Figure 13.1: Residual Network (ResNet) Unit
</p>
</div>
<p>There are two ways to visualize <strong>ResNet</strong>. In the figure, the left diagram can be perceived in such a way that a connection skips a network block (or a set of network blocks) and directly adds <strong>X</strong> to the output, <strong>f(X)</strong>, of the network block(s). The correct diagram can be perceived such that a residual block is added to the network. The residual output, <strong>R(X)</strong>, is added to <strong>X</strong>, and by doing so, we allow the network to learn the residual.</p>
<p>A slightly complex architecture is shown in Figure <a href="deeplearning2.html#fig:resnet">13.2</a>. However, the architecture is still simpler than known architectures such as <strong>ResNet152</strong>. Nevertheless, there is no limit to our creativity as long as we design a neural network architecture that we see fits our purposes and expectations.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:resnet"></span>
<img src="resnet.png" alt="Residual Network (ResNet) Architecture" width="70%" />
<p class="caption">
Figure 13.2: Residual Network (ResNet) Architecture
</p>
</div>
<p>The placement of the <strong>skip connection</strong> and the <strong>residual block</strong> size varies depending on the design. For example, extending the connection right after the weight layer is possible but before the activation function. The size of the <strong>residual block</strong> is dictated by the number of layers we skip.</p>
<p>We skip showing an implementation of <strong>ResNet</strong> in this section. On the other hand, let us discuss <strong>RNN</strong> next which shows a case of a <strong>skip connection</strong>, but more importantly, in addition to that, it also introduces the concept of <strong>gated connections</strong>, which regulate the flow of information apart from only considering the effect of residuals.</p>
</div>
<div id="recurrent-neural-network-rnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.2</span> Recurrent Neural Network (RNN)  <a href="deeplearning2.html#recurrent-neural-network-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now switch context to another type of <strong>Neural Network</strong> that can handle sequential data. In the context of <strong>Recurrent Neural Network (RNN)</strong>, our emphasis is on the <strong>recurrence</strong> of a sequence of data points. While <strong>CNN</strong> commonly intends to learn patterns from a static list of images to perform image classification, <strong>RNN</strong> intends to recognize and learn <strong>recurring</strong> patterns from a set of sequential data points to perform prediction for the next sequence of patterns. This ability to predict based on a recurring sequence of events showcases the power of <strong>RNN</strong>, which opens up even wider possible applications such as speech recognition, language translation, self-driving automobiles, and self-serve robots in restaurants.</p>
<p>To build up our intuition on <strong>RNN</strong>, let us first understand the two diagrams shown in Figure <a href="deeplearning2.html#fig:unrolledrnn">13.3</a>. We illustrate an <strong>RNN</strong> box known as <strong>RNN Cell</strong> or <strong>RNN Unit</strong>. The cell takes <strong>X</strong> as input and produces <span class="math inline">\(\mathbf{\hat{Y}}\)</span> as output. Notice a second output in the form of <strong>H</strong> comes out of the unit and gets fed back through it. This output is called <strong>Hidden State</strong> (also called <strong>Cell State</strong>), and it exists as a memory state preserving information.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unrolledrnn"></span>
<img src="unrolledrnn.png" alt="RNN (Unrolled Representation)" width="80%" />
<p class="caption">
Figure 13.3: RNN (Unrolled Representation)
</p>
</div>
<p>Here, we note that the <strong>X</strong> input ideally requires a recurrent sequential pattern. A straightforward example of a recurring input is a sentence formed based on a specific language, e.g., the English language. Because a language follows a certain syntactic and semantic structure, it suggests some level of sequential pattern in that a sentence is made up of a sequence of words as input. Note here that we do not simply consider a bag of words with no order. The sequence affects the semantics or meaning of a sentence if otherwise unordered. For example, let us use one sentence derived from <strong>Og Mandino</strong>âs The Greatest Salesman in the world:</p>
<p><span class="math display">\[
\text{Today, I begin a new life.}
\]</span>
Our simple goal is to feed the sample sentence into <strong>RNN</strong> in the form of individual words such that the word <strong>Today</strong> is fed first through RNN at time <span class="math inline">\(\mathbf{t_0}\)</span>, then the word <strong>I</strong> is fed next at time <span class="math inline">\(\mathbf{t_1}\)</span> followed by the word <strong>begin</strong> at <span class="math inline">\(\mathbf{t_2}\)</span>, and so on. The words are fed through the <strong>RNN cell</strong> recursively. If we <strong>unroll</strong> the <strong>RNN cell</strong>, we see the operations across time as depicted in the <strong>unrolled representation</strong> in Figure <a href="deeplearning2.html#fig:unrolledrnn">13.3</a>. The word <strong>Today</strong> is represented by <span class="math inline">\(\mathbf{X_0}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_0}\)</span> output and <span class="math inline">\(\mathbf{H_0}\)</span> cell state. The word <strong>I</strong> is represented by <span class="math inline">\(\mathbf{X_1}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_1}\)</span> output and <span class="math inline">\(\mathbf{H_1}\)</span> cell state. The word <strong>begin</strong> is represented by <span class="math inline">\(\mathbf{X_2}\)</span> with the corresponding <span class="math inline">\(\mathbf{\hat{Y}_1}\)</span> output and <span class="math inline">\(\mathbf{H_2}\)</span> cell state. And so on. We can then stop feeding <strong>RNN</strong> after the word <strong>new</strong> and try to allow <strong>RNN</strong> to predict what is the last word. In our case, the last word is <strong>life</strong>.</p>
<p>Note that we do not feed the exact form of the sentence into <strong>RNN</strong>. Instead, we take each word in the sentence and convert it into a unique numeric representation. We discuss this technique of casting words to numbers more in the <strong>Attention</strong> section.</p>
<p>Now in <strong>RNN</strong>, we may encounter four general types of sequence models discussed. Figure <a href="deeplearning2.html#fig:rnnsequence">13.4</a> provides a diagram of each type <span class="citation">(Karpathy A. <a href="bibliography.html#ref-ref1186a">2015</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnsequence"></span>
<img src="rnnsequence.png" alt="RNN (Sequence Models)" width="80%" />
<p class="caption">
Figure 13.4: RNN (Sequence Models)
</p>
</div>
<p>The <strong>RNN</strong> algorithm may differ depending on the sequence (and application) used based on Figure <a href="deeplearning2.html#fig:rnnsequence">13.4</a>. For example, using the <strong>One to One</strong> sequence model for a <strong>binary classification</strong> of a single image may be appropriate. The <strong>One to Many</strong> sequence model is appropriate for a single image such as a <strong>cifar-10</strong> image, as the case may be in our <strong>CNN</strong> example. The <strong>Many to One</strong> sequence model may apply in sentiment analysis in which we feed <strong>RNN</strong> with a sequence of input (words or images) and determine sentimentally if the sequence exudes a positive vibe. The <strong>Many to Many</strong> design makes it possible to perform language translation <span class="citation">(Karpathy A. <a href="bibliography.html#ref-ref1186a">2015</a>)</span>. Alternatively (not included in the figure), a combined model of the <strong>Many to One</strong> and <strong>One to Many</strong> can be used as encoder and decoder, respectively, as commonly depicted in other literature.</p>
<p>In the next few sections, let us introduce three <strong>RNN</strong> variances, namely <strong>Vanilla RNN</strong>, <strong>LSTM</strong>, and <strong>GRU</strong>.</p>
<div id="vanilla-rnn" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.1</span> Vanilla RNN<a href="deeplearning2.html#vanilla-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us recall that a linear equation is made up of the following general example:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = f(x) = \omega_0 + x_1 \omega_1 + x_2 \omega_2 +\ ...\ + x_p \omega_p
\end{align}\]</span></p>
<p>where the omega (<span class="math inline">\(\omega\)</span>) symbol represents parameters or coefficients required for network learning. Additionally, considering Figure <a href="deeplearning1.html#fig:perceptron">12.3</a>, we also use an activation function to achieve non-linearity. In this case, we use <strong>tanh</strong>. </p>
<p><span class="math display">\[\begin{align}
\hat{y} = f_a(x) = \text{tanh}(\omega_0 + x_1 \omega_1 + x_2 \omega_2 +\ ...\ + x_p \omega_p)
\end{align}\]</span></p>
<p>We can easily modify the diagram in Figure <a href="deeplearning1.html#fig:perceptron">12.3</a> to reflect an <strong>RNN</strong> cell with the existence of <strong>cell states</strong>. See Figure <a href="deeplearning2.html#fig:vanillarnn">13.5</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vanillarnn"></span>
<img src="vanillarnn.png" alt="Vanilla RNN Cell" width="80%" />
<p class="caption">
Figure 13.5: Vanilla RNN Cell
</p>
</div>
<p>The figure shows that <strong>RNN</strong> uses two equations. The first equation is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\mathbf{H}_{nxh}^{(t)} = \mathbf{f_a}\left(\mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right) = \mathbf{\text{tanh}}\left( \mathbf{X}_{nxp}^{(t)} \cdotp  \mathbf{W}_{pxh} +  \mathbf{H}_{nxh}^{(t-1)} \cdotp \mathbf{U}_{hxh} + \mathbf{b}_{1xh}^{(h)} \right)
\end{align}\]</span></p>
<p>The <strong>cell state</strong> (<span class="math inline">\(\mathbf{H^{(t)}}\)</span>) is preserved as a memory state to be used by the next iteration. Note here that the input <span class="math inline">\(\mathbf{X^{(t)}}\)</span> and previous cell state <span class="math inline">\(\mathbf{H^{(t-1)}}\)</span> are multiplied by their respective weights then summed together along with a bias (<span class="math inline">\(\mathbf{b}_{1xh}\)</span>); after which, the result is then fed through the <strong>tanh</strong> activation function.</p>
<p>The second equation is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\hat{Y}}_{nxo}^{(t)} = \mathbf{f}\left(\mathbf{H}_{nxh}^{(t)}\right) = 
\mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \times \mathbf{V}_{hxo} + \mathbf{b}_{1xo}^{(y)}\right) 
\end{align}\]</span></p>
<p>This yields the <span class="math inline">\(\mathbf{\hat{Y}_{nxo}^{(t)}}\)</span> output produced by a <strong>softmax</strong> function, in this case, performing classification.</p>
<p>In terms of dimensions, Figure <a href="deeplearning2.html#fig:rnnforward">13.6</a> illustrates how the dot-product and summation operations handle the dimension of the input, output, cell state, and parameters.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnforward"></span>
<img src="rnnforward.png" alt="Vanilla RNN Dimensions" width="80%" />
<p class="caption">
Figure 13.6: Vanilla RNN Dimensions
</p>
</div>
<p>Given the following activation functions, the structure in Figure <a href="deeplearning2.html#fig:vanillarnn">13.5</a> has the following implementation:</p>

<div class="sourceCode" id="cb2085"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2085-1" data-line-number="1">rnn.tanh     &lt;-<span class="st"> </span><span class="cf">function</span>(x) { (<span class="kw">exp</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) <span class="op">/</span><span class="st"> </span>( <span class="kw">exp</span>(x) <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb2085-2" data-line-number="2">rnn.sigmoid         &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x)) }</a>
<a class="sourceLine" id="cb2085-3" data-line-number="3">rnn.softmax  &lt;-<span class="st"> </span><span class="cf">function</span>(x) { p =<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">1</span>, max); x =<span class="st"> </span>x <span class="op">-</span><span class="st"> </span>p; p =<span class="st"> </span><span class="kw">exp</span>(x) </a>
<a class="sourceLine" id="cb2085-4" data-line-number="4">                              s =<span class="st"> </span><span class="kw">apply</span>(p, <span class="dv">1</span>, sum); <span class="kw">sweep</span>(p, <span class="dv">1</span>, s, <span class="st">&quot;/&quot;</span>) </a>
<a class="sourceLine" id="cb2085-5" data-line-number="5">}</a></code></pre></div>

<p>Now suppose we have an <span class="math inline">\(\mathbf{n \times p}\)</span> <strong>input</strong> (<strong>X</strong>) (e.g., it has <strong>n</strong> samples and <strong>p</strong> features) with its corresponding <span class="math inline">\(\mathbf{p \times h}\)</span> <strong>weight</strong> (<strong>W</strong>), a <strong>hidden state</strong> (<strong>H</strong>) with <span class="math inline">\(\mathbf{n \times h}\)</span> dimension (e.g., <strong>h</strong> neurons) and its corresponding <span class="math inline">\(\mathbf{h \times h}\)</span> ** weight** (<strong>U</strong>), we need to calculate the <span class="math inline">\(\mathbf{n \times o}\)</span> <strong>output</strong> (<span class="math inline">\(\mathbf{\hat{Y}}\)</span>). Below is a sample use of the functions and structure:</p>

<div class="sourceCode" id="cb2086"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2086-1" data-line-number="1">n     =<span class="st"> </span><span class="dv">5</span>    <span class="co"># number of samples</span></a>
<a class="sourceLine" id="cb2086-2" data-line-number="2">p     =<span class="st"> </span><span class="dv">30</span>   <span class="co"># number of features per sample (could also mean number of </span></a>
<a class="sourceLine" id="cb2086-3" data-line-number="3">             <span class="co"># probabilities of a word embedding)</span></a>
<a class="sourceLine" id="cb2086-4" data-line-number="4">h     =<span class="st"> </span><span class="dv">20</span>   <span class="co"># number of neurons in a hidden state</span></a>
<a class="sourceLine" id="cb2086-5" data-line-number="5">o     =<span class="st"> </span><span class="dv">3</span>    <span class="co"># number of output neurons in an output layer</span></a>
<a class="sourceLine" id="cb2086-6" data-line-number="6">X     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-7" data-line-number="7">H     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-8" data-line-number="8">W     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(p <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-9" data-line-number="9">U     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(h <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-10" data-line-number="10">V     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(h <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span>h, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-11" data-line-number="11">bh    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-12" data-line-number="12">by    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2086-13" data-line-number="13">H     =<span class="st"> </span><span class="kw">rnn.tanh</span>(<span class="kw">sweep</span>(X <span class="op">%*%</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>U, <span class="dv">2</span>, bh, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2086-14" data-line-number="14">Y     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>o), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>o, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a></code></pre></div>

<p>Below is the result showing the dimensions of <strong>H</strong> and <strong>Y</strong>:</p>

<div class="sourceCode" id="cb2087"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2087-1" data-line-number="1"><span class="kw">str</span>(<span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H, <span class="st">&quot;Y&quot;</span> =<span class="st"> </span>Y)) <span class="co"># put in a list for  better display</span></a></code></pre></div>
<pre><code>## List of 2
##  $ H: num [1:5, 1:20] 0.968 0.992 -0.88 0.999 0.975 ...
##  $ Y: num [1:5, 1:3] 0.13742 2.11004 -0.00548 -0.75795 -0.89994 ...</code></pre>

<p>Similar to <strong>MLP</strong> and <strong>CNN</strong>, it should be noted that <strong>RNN</strong> also considers the use of <strong>forward feed</strong> and <strong>backpropagation</strong>.</p>
<p><strong>Forward Feed</strong></p>
<p>Here, we follow a straightforward implementation of <strong>RNN</strong> forward feed based on the immediate equations above.</p>

<div class="sourceCode" id="cb2089"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2089-1" data-line-number="1">forward.unit.RNN &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, params) {</a>
<a class="sourceLine" id="cb2089-2" data-line-number="2">    W     =<span class="st"> </span>params<span class="op">$</span>W</a>
<a class="sourceLine" id="cb2089-3" data-line-number="3">    U     =<span class="st"> </span>params<span class="op">$</span>U</a>
<a class="sourceLine" id="cb2089-4" data-line-number="4">    V     =<span class="st"> </span>params<span class="op">$</span>V</a>
<a class="sourceLine" id="cb2089-5" data-line-number="5">    bh    =<span class="st"> </span>params<span class="op">$</span>bh</a>
<a class="sourceLine" id="cb2089-6" data-line-number="6">    by    =<span class="st"> </span>params<span class="op">$</span>by</a>
<a class="sourceLine" id="cb2089-7" data-line-number="7">    Ht    =<span class="st"> </span><span class="kw">rnn.tanh</span>(<span class="kw">sweep</span>(X <span class="op">%*%</span><span class="st"> </span>W <span class="op">+</span><span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>U, <span class="dv">2</span>, bh, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2089-8" data-line-number="8">    Y.hat =<span class="st"> </span><span class="kw">rnn.softmax</span>(<span class="kw">sweep</span>(H <span class="op">%*%</span><span class="st"> </span>V, <span class="dv">2</span>, by, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2089-9" data-line-number="9">    <span class="kw">list</span>(<span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;Y.hat&quot;</span> =<span class="st"> </span>Y.hat)</a>
<a class="sourceLine" id="cb2089-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb2089-11" data-line-number="11">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;W&quot;</span> =<span class="st"> </span>W, <span class="st">&quot;U&quot;</span> =<span class="st"> </span>U, <span class="st">&quot;V&quot;</span> =<span class="st"> </span>V, <span class="st">&quot;bh&quot;</span> =<span class="st"> </span>bh, <span class="st">&quot;by&quot;</span> =<span class="st"> </span>by)</a>
<a class="sourceLine" id="cb2089-12" data-line-number="12">model =<span class="st"> </span><span class="kw">forward.unit.RNN</span>(X, H,  params)</a></code></pre></div>

<p>Below is the result showing the dimension of <strong>H</strong> and <span class="math inline">\(\mathbf{\hat{Y}}\)</span>:</p>

<div class="sourceCode" id="cb2090"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2090-1" data-line-number="1"><span class="kw">str</span>(model)</a></code></pre></div>
<pre><code>## List of 2
##  $ Ht   : num [1:5, 1:20] 1 1 0.845 0.998 0.901 ...
##  $ Y.hat: num [1:5, 1:3] 0.895 0.565 0.939 1 0.998 ...</code></pre>

<p><strong>Backpropagation</strong></p>
<p>We start our discussion of <strong>RNN backpropagation</strong> by using Figure <a href="deeplearning2.html#fig:rnnbackprop">13.7</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rnnbackprop"></span>
<img src="rnnbackprop.png" alt="RNN (Softmax)" width="90%" />
<p class="caption">
Figure 13.7: RNN (Softmax)
</p>
</div>
<p>Similar to <strong>CNN</strong>, our implementation of <strong>RNN</strong> uses <strong>Cross-Entropy Loss</strong> for <strong>Softmax function</strong> (which is also the case for <strong>LSTM</strong> and <strong>GRU</strong> in next sections ahead). If we recall <strong>Delta Rule</strong> in <strong>MLP</strong> section, we start by obtaining <strong>Delta o</strong> (<span class="math inline">\(\mathbf{\delta o}\)</span>) which is the gradient of our loss with respect to the activation function (in this is case, we use softmax). Here, it helps to recall the derivation of the equation under <strong>activation function</strong> subsection in <strong>MLP</strong> section. Note that in <strong>MLP</strong> we use the <strong>hat-o</strong> (<span class="math inline">\(\hat{\mathbf{o}}\)</span>) symbol for the <strong>linear function</strong> of our output and <strong>no-hat</strong> (<span class="math inline">\(\mathbf{o}\)</span>) symbol for our <strong>non-linear function</strong>. See below:</p>
<p><span class="math display">\[\begin{align}
\delta_o = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{o}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial o}\right) \left(\frac{\partial o}{\partial \hat{o}}\right) = (o - t)
\end{align}\]</span></p>
<p>This should not be confused now about our use of <strong>y-hat</strong> (<span class="math inline">\(\hat{\mathbf{y}}\)</span>) symbol for our <strong>softmax</strong> and <strong>y</strong> for our target (<strong>t</strong>) in <strong>RNN</strong>; whereas, the <strong>h-hat</strong> (<span class="math inline">\(\hat{\mathbf{h}}\)</span>) symbol represents output of our linear function. Therefore, in our case for <strong>RNN</strong>, we have the following equation:</p>
<p><span class="math display">\[\begin{align}
\delta_{y} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{h}} = 
\left(\frac{\partial \mathcal{L}^{(CE)}}{\partial \hat{y}}\right) \left(\frac{\partial \hat{y}}{\partial \hat{h}}\right) = (\hat{y} - y)
\end{align}\]</span></p>
<p>It should then be straightforward to derive the gradient of <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>, <span class="math inline">\(\mathbf{V}_{hxo}\)</span>, and <span class="math inline">\(\mathbf{b}_{1xh}^{(y)}\)</span> (in the order shown below):</p>
<p><span class="math display">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} 
  &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} + \delta \mathbf{H}_{nxh}^{(t+1)}  
  =   \delta y 
    \left(\frac{\partial \hat{h}} {\partial  \mathbf{H}_{nxh}^{(t)}} 
    \right) + \delta \mathbf{H}_{nxh}^{(t+1)} 
  = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \delta \mathbf{H}_{nxh}^{(t+1)} \\
\nabla \mathbf{V_{hxo}} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{V}_{hxo}} 
    = 
    \left(\frac{\partial \hat{h}} {\partial \mathbf{V_{hxo}} }  
    \right)  \delta y 
    =  \left(\mathbf{H}_{nxh}^{(t)}\right)^{\text{T}} \cdot \delta y \\
\nabla \mathbf{b}_{1xo}^{(y)} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{b}_{1xo}^{(y)} }  = \sum_{column-wise}{\delta y}  
\end{align}\]</span></p>
<p>Also, notice here the addition of the second term, namely <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t+1)}\right)\)</span>, which we also refer to as <strong>dH.next</strong> in our implementation. Because there is no time step <strong>t+1</strong> at the start of backpropagation, <strong>dH.next</strong> is initially zero.</p>
<p>Next, we then solve for gradient of our activation function (the <strong>tangent</strong>) with respect to <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>. Here, we know that the first derivative of tangent is written as:</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{tanh}}&#39;(a) = \left(1 - \mathbf{tanh}^2(a)\right)
\end{align}\]</span></p>
<p>We use the derivative to construct our <strong>Delta tanh</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\delta\ \mathbf{\text{tanh}} = ( 1 - \mathbf{\text{tanh}}^2(a)) \odot \delta a = ( 1 - \mathbf{\text{tanh}}^2(\mathbf{H}_{nxh}^{(t)})) \odot \delta \mathbf{H}_{nxh}^{(t)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(a = \mathbf{H}_{nxh}^{(t)}\)</span>.</p>
<p>From there, we can derive the gradient of our loss with respect to <span class="math inline">\(\mathbf{W}_{pxh}\)</span>, <span class="math inline">\(\mathbf{U}_{hxh}\)</span>, and <span class="math inline">\(\mathbf{b}_{1xh}^{(y)}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\nabla  \mathbf{W}_{pxh}  = \left(\mathbf{X}_{nxp}^{(t-1)}\right)^\text{T} \cdotp  \delta \mathbf{\text{tanh}}
\ \ \ \ \ \ \ \ \ 
 \nabla  \mathbf{U}_{hxh}  = \left( \mathbf{H}_{nxh}^{(t-1)}\right)^\text{T} \cdotp  \delta \mathbf{\text{tanh}}
\end{align}\]</span>
<span class="math display">\[\begin{align}
 \nabla \mathbf{b}_{1xh}^{(y)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{b}_{1xh}^{(h)} } 
     =  \delta\mathbf{\text{tanh}}
\end{align}\]</span></p>
<p>For the gradient of the loss with respect to previous <strong>X</strong> and <strong>H</strong>, we perform the following equations:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\delta \mathbf{H}_{nxh}^{(t-1)}}_{
     \begin{array}{ll}
     \mathbf{\text{dH.prev becomes}}\\
     \mathbf{\text{new dH.next later}}
     \end{array}
     }  
  = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t-1)}} 
     = \delta\mathbf{\text{tanh}} \cdotp \left(\mathbf{U}_{hxh}\right)^\text{T} \label{eqn:eqnnumber801}\\
\delta \mathbf{X}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{X}_{nxh}^{(t)}} 
     = \delta\mathbf{\text{tanh}} \cdotp \left(\mathbf{W}_{pxh}\right)^\text{T} \label{eqn:eqnnumber802}
\end{align}\]</span></p>
<p>Below is our example implementation of <strong>RNN backpropagation</strong>:</p>

<div class="sourceCode" id="cb2092"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2092-1" data-line-number="1">backward.unit.RNN &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next, X, Y, model, params) {</a>
<a class="sourceLine" id="cb2092-2" data-line-number="2">    W           =<span class="st"> </span>params<span class="op">$</span>W</a>
<a class="sourceLine" id="cb2092-3" data-line-number="3">    U           =<span class="st"> </span>params<span class="op">$</span>U</a>
<a class="sourceLine" id="cb2092-4" data-line-number="4">    V           =<span class="st"> </span>params<span class="op">$</span>V</a>
<a class="sourceLine" id="cb2092-5" data-line-number="5">    bh          =<span class="st"> </span>params<span class="op">$</span>bh</a>
<a class="sourceLine" id="cb2092-6" data-line-number="6">    by          =<span class="st"> </span>params<span class="op">$</span>by</a>
<a class="sourceLine" id="cb2092-7" data-line-number="7">    Y.hat       =<span class="st"> </span>model<span class="op">$</span>Y.hat</a>
<a class="sourceLine" id="cb2092-8" data-line-number="8">    dy          =<span class="st"> </span>(Y.hat <span class="op">-</span><span class="st"> </span>Y) </a>
<a class="sourceLine" id="cb2092-9" data-line-number="9">    dH          =<span class="st"> </span>dy <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V) <span class="op">+</span><span class="st"> </span>dH.next  <span class="co"># dealing with one-hot encoding</span></a>
<a class="sourceLine" id="cb2092-10" data-line-number="10">    dV          =<span class="st"> </span><span class="kw">t</span>(H) <span class="op">%*%</span><span class="st"> </span>dy </a>
<a class="sourceLine" id="cb2092-11" data-line-number="11">    dby         =<span class="st"> </span><span class="kw">apply</span>(dy, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb2092-12" data-line-number="12">    dtanh       =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>model<span class="op">$</span>Ht<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>dH  <span class="co"># Gradient of Tanh wrt H</span></a>
<a class="sourceLine" id="cb2092-13" data-line-number="13">    dX          =<span class="st"> </span>dtanh <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(W)         <span class="co"># delta X for BP through time</span></a>
<a class="sourceLine" id="cb2092-14" data-line-number="14">    dH          =<span class="st"> </span>dtanh <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(U)         <span class="co"># delta H for BP throught time</span></a>
<a class="sourceLine" id="cb2092-15" data-line-number="15">    dW          =<span class="st"> </span><span class="kw">t</span>(X) <span class="op">%*%</span><span class="st"> </span>dtanh         <span class="co"># Gradient of Loss wrt W</span></a>
<a class="sourceLine" id="cb2092-16" data-line-number="16">    dU          =<span class="st"> </span><span class="kw">t</span>(H) <span class="op">%*%</span><span class="st"> </span>dtanh         <span class="co"># Gradient of Loss wrt U</span></a>
<a class="sourceLine" id="cb2092-17" data-line-number="17">    dbh         =<span class="st"> </span><span class="kw">apply</span>(dtanh, <span class="dv">2</span>, sum)   <span class="co"># Gradient of Loss wrt Bias</span></a>
<a class="sourceLine" id="cb2092-18" data-line-number="18">    <span class="kw">list</span>(<span class="st">&quot;dX&quot;</span> =<span class="st"> </span>dX, <span class="st">&quot;dH&quot;</span> =<span class="st"> </span>dH, <span class="st">&quot;dW&quot;</span> =<span class="st"> </span>dW,  <span class="st">&quot;dU&quot;</span> =<span class="st"> </span>dU, <span class="st">&quot;dbh&quot;</span> =<span class="st"> </span>dbh,</a>
<a class="sourceLine" id="cb2092-19" data-line-number="19">         <span class="st">&quot;dV&quot;</span> =<span class="st"> </span>dV, <span class="st">&quot;dby&quot;</span> =<span class="st"> </span>dby)</a>
<a class="sourceLine" id="cb2092-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb2092-21" data-line-number="21">params     =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;W&quot;</span> =<span class="st"> </span>W, <span class="st">&quot;U&quot;</span> =<span class="st"> </span>U, <span class="st">&quot;V&quot;</span> =<span class="st"> </span>V)</a>
<a class="sourceLine" id="cb2092-22" data-line-number="22">dH.next    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2092-23" data-line-number="23">gradients  =<span class="st"> </span><span class="kw">backward.unit.RNN</span>(dH.next, X, Y, model, params)</a>
<a class="sourceLine" id="cb2092-24" data-line-number="24"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 7
##  $ dX : num [1:5, 1:30] 2.4484 -0.0373 1.2621 4.4449 2.2843 ...
##  $ dH : num [1:5, 1:20] -0.795 7.254 -0.411 -3.118 0.463 ...
##  $ dW : num [1:30, 1:20] 0.271 0.413 0.219 0.198 0.146 ...
##  $ dU : num [1:20, 1:20] -0.339 0.413 0.439 -0.442 0.43 ...
##  $ dbh: num [1:20] 0.442 1.185 0.499 1.524 0.403 ...
##  $ dV : num [1:20, 1:3] 1.9744 -1.4219 0.0203 -3.8125 3.7388 ...
##  $ dby: num [1:3] 3.812 0.142 5.587</code></pre>

<p>Training our <strong>RNN</strong> iterates between the <strong>forward.unit.RNN(.)</strong> function and <strong>backward.unit.RNN(.)</strong> function, along with the usual cost computation and an optimized update of the learnable parameters similar to <strong>CNN</strong>. See our generic <strong>RNN algorithm</strong> next.</p>
<p><strong>RNN Algorithm</strong></p>
<p>It should be noted that the recurrent nature of <strong>RNN</strong> preserves the previous state at each time step. If we view the <strong>unrolled representation</strong> of <strong>RNN</strong>, we should be able to trace back the gradients through time.</p>
<p>Below is a generic algorithm for <strong>RNN</strong> showing the use of <strong>forward feed</strong> and <strong>backpropagation</strong> through time. The algorithm is based on the <strong>Many to Many</strong> sequence model.</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{W, U, V, bh, by}\ \leftarrow \text{(params)}\ \ \ \ \ \ \ \ \ \ \ \   (\text{randomly generated}) \\
\text{H}_{nxh}, \text{cost}_{1xepoch}\ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \  \text{(initialize to zero)}\\
\text{dH.next}  \ \ \ \ \ \ \ \ \ \ \ \ \  \  \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \ \ \ \ \text{(initialize to zero)}\\
\text{dW, dU, dbh, dby}\ \leftarrow \text{(gradients)}\ \ \ \  \ \ \text{(initialize to zero)}\\
\mathbf{\text{for}}\ iter\ \text{    in    } 1...epoch\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \mathbf{\text{for}}\ t\ \text{    in    } 1...T\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{model} = \mathbf{\text{forward.unit}}(X_{nxp}^{(t)}, H_{nxh}^{(t)}, \text{params})\\
\ \ \ \ \ \ \ \ \ \ \ \ \ H_{nxh}^{(t)}, \hat{Y}_{nxo}^{(t)} = \text{model}\\
\ \ \ \ \ \ \mathbf{\text{end loop}}\\
\ \ \ \ \ \ \text{cost[iter] }  = \text{mean}\left(\mathbf{\text{  cost.estimate}}(\hat{Y}_{nxo}, Y_{nxo})\right)  \ \ \ \ \ \ \ \ (\text{cross-entropy cost})\\
\ \ \ \ \ \ \mathbf{\text{for}}\ t\ \text{    in    } T...1\ \mathbf{\text{ loop}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{gradients} = \mathbf{\text{backward.unit}} (\text{dH.next}, X_{nxh}^{(t)}, \text{model}, \text{params})\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;W&#39;] = params[&#39;W&#39;] + gradients[&#39;dW&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;U&#39;] = params[&#39;U&#39;] + gradients[&#39;dU&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params[&#39;bh&#39;] = params[&#39;bh&#39;] + gradients[&#39;dbh&#39;]} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{dH.next = dH} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{params} = \mathbf{\text{optimize.update}}(\text{params, gradients})\\
\ \ \ \ \ \ \ \mathbf{\text{end loop}}\\
\mathbf{\text{end loop}}\\
\mathbf{\text{output : }}\text{ cost, params}\ \ \leftarrow \text{(trained model)}
\end{array}
\]</span></p>
<p>where <strong>cross-entropy cost</strong> is calculated as <span class="math inline">\(\left(-\sum\left(\log_e\left(Y_n^{(t)} \odot \hat{Y}_n^{(t)}\right)\right)\right)\)</span>.</p>
<p>A common problem with <strong>Vanilla RNN</strong> is the <strong>vanishing or exploding gradient</strong>. In such a case, the more critical concern is that <strong>RNN</strong>, by its very nature, attempts to preserve (in memory) the state of the previous time step in that such state information tends to <strong>vanish</strong> in time so that the early memory tends to be <strong>forgotten</strong> as we keep progressing in future time steps. This concern is mitigated using <strong>LSTM</strong>.</p>
</div>
<div id="long-short-term-memory-lstm" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.2</span> Long Short-Term Memory (LSTM)  <a href="deeplearning2.html#long-short-term-memory-lstm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>LSTM</strong> is a variant of <strong>RNN</strong> introduced by Sepp Hochreiter and JÃ¼rgen Schmidhuber in <span class="citation">(<a href="bibliography.html#ref-ref1154h">1997</a>)</span>. <strong>LSTM</strong> is designed as a <strong>gated RNN</strong> with a <strong>forget gate</strong>, <strong>update gate</strong>, and <strong>output gate</strong>. See the design in Figure <a href="deeplearning2.html#fig:lstm">13.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lstm"></span>
<img src="lstm.png" alt="LSTM Cell" width="100%" />
<p class="caption">
Figure 13.8: LSTM Cell
</p>
</div>
<p>The intuition behind the gates becomes apparent after reviewing the <strong>forward feed</strong>.</p>
<p><strong>LSTM Forward Feed</strong></p>
<p>Based on Figure <a href="deeplearning2.html#fig:lstm">13.8</a>, the <strong>LSTM</strong> design uses the below equations where <span class="math inline">\(\odot\)</span> is an element-wise (<strong>Hadamard</strong>) multiplication:</p>
<p><span class="math display">\[\begin{align}
[\mathbf{X,H}]_{nx[p,h]}^{(t)} = \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right] &amp; \\
A_f^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(f)}    + \mathbf{b}_{1xh}^{(f)} &amp;\ \ \ \ \ \mathbf{F}_{nxh}^{(t)} = \sigma \left(A_f^{(t)}\right) \\
A_i^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(i)}   + \mathbf{b}_{1xh}^{(i)} &amp;\ \ \ \ \ \mathbf{I}_{nxh}^{(t)} = \sigma \left(A_i^{(t)} \right) \\
A_g^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(g)}  + \mathbf{b}_{1xh}^{(g)} &amp;\ \ \ \ \ \mathbf{G}_{nxh}^{(t)} = \mathbf{\text{tanh}} \left(A_g^{(t)}\right) \\
A_o^{(t)} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(o)}   + \mathbf{b}_{1xh}^{(o)} &amp;\ \ \ \ \ \mathbf{O}_{nxh}^{(t)} = \sigma \left(A_o^{(t)}\right) \\
&amp;\ \ \ \ \ \mathbf{C}_{nxh}^{(t)} = \mathbf{F}_{nxh}^{(t)} \odot \mathbf{C}_{nxh}^{(t-1)} + \mathbf{I}_{nxh}^{(t)} \odot \mathbf{G}_{nxh}^{(t)} \\
&amp;\ \ \ \ \ \mathbf{H}_{nxh}^{(t)} = \mathbf{\text{tanh}}\left( \mathbf{C}_{nxh}^{(t)}\right) \odot \mathbf{O}_{nxh}^{(t)}\\
&amp;\ \ \ \ \ \mathbf{\hat{Y}}_{nxo}^{(t)} = \mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \cdotp \mathbf{V}_{hxo}+ \mathbf{b}_{1xo}^{(y)}\right) 
\end{align}\]</span></p>
<p>Note that the equation for <strong>vanilla RNN</strong> has the following:</p>
<p><span class="math display">\[\begin{align}
 \mathbf{X}_{nxp}^{(t)} \cdotp  \mathbf{W}_{pxh} +  \mathbf{H}_{nxh}^{(t-1)} \cdotp \mathbf{U}_{hxh} 
\end{align}\]</span></p>
<p>In <strong>LSTM</strong>, we concatenate <strong>X</strong> and <strong>H</strong> instead which is denoted by <span class="math inline">\(\mathbf{\text{[X, H]}}\)</span>. Then multiplied by an already concatenated weight using the following notation:</p>
<p><span class="math display">\[\begin{align}
[\mathbf{\text{X}}_{nxp}^{(t)}, \mathbf{\text{H}}_{nxh}^{(t-1)}] \cdot W_{[p,h]xh} = [\mathbf{X,H}]_{nx[p,h]}^{(t)} \cdot \mathbf{W}_{[p,h]xh}^{(f)} 
\end{align}\]</span></p>
<p>Now, because the <strong>forget gate</strong>, denoted by <span class="math inline">\(\mathbf{F}_{nxh}^{(t)}\)</span>, gets multiplied by <span class="math inline">\(\mathbf{C}_{nxh}^{(t-1)}\)</span>, mathematically when it comes to the gates, any value of <span class="math inline">\(\mathbf{F}_{nxh}^{(t)}\)</span>, therefore, affects the previous time step. For example, if the value gets closer to zero, then the more limited information (in the form of <span class="math inline">\(\mathbf{C}_{nxh}^{(t-1)}\)</span>) gets allowed to the next time step. On the other hand, the <strong>candidate state</strong>, denoted by <span class="math inline">\(\mathbf{G}_{nxh}^{(t)}\)</span>, is negated if the value of the <strong>input gate</strong>, denoted by <span class="math inline">\(\mathbf{I}_{nxh}^{(t)}\)</span>, becomes zero. In other words, <strong>gates</strong> in <strong>LSTM</strong> regulate the flow of information such that a complementary intuition here is to avoid long-term memory, which, in effect, mitigates the <strong>vanishing</strong> and <strong>exploding</strong> gradient condition.</p>
<p>Here, we have an example implementation of <strong>LSTM forward feed</strong> based on the above equations: </p>

<div class="sourceCode" id="cb2094"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2094-1" data-line-number="1">forward.unit.LSTM &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, C, params) {</a>
<a class="sourceLine" id="cb2094-2" data-line-number="2">    Wi    =<span class="st"> </span>params<span class="op">$</span>Wi<span class="op">$</span>weight;  Wf  =<span class="st"> </span>params<span class="op">$</span>Wf<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-3" data-line-number="3">    Wg    =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight;  Wo  =<span class="st"> </span>params<span class="op">$</span>Wo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-4" data-line-number="4">    bf    =<span class="st"> </span>params<span class="op">$</span>bf<span class="op">$</span>weight;  bi  =<span class="st"> </span>params<span class="op">$</span>bi<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-5" data-line-number="5">    bg    =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight;  bo  =<span class="st"> </span>params<span class="op">$</span>bo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2094-6" data-line-number="6">    XH    =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2094-7" data-line-number="7">    Ft    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wf, <span class="dv">2</span>, bf, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-8" data-line-number="8">    It    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wi, <span class="dv">2</span>, bi, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-9" data-line-number="9">    Gt    =<span class="st"> </span><span class="kw">rnn.tanh</span>   (<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wg, <span class="dv">2</span>, bg, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-10" data-line-number="10">    Ot    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wo, <span class="dv">2</span>, bo, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2094-11" data-line-number="11">    Ct    =<span class="st"> </span>Ft <span class="op">*</span><span class="st"> </span>C <span class="op">+</span><span class="st">  </span>It <span class="op">*</span><span class="st"> </span>Gt </a>
<a class="sourceLine" id="cb2094-12" data-line-number="12">    Ht    =<span class="st"> </span><span class="kw">rnn.tanh</span>(Ct) <span class="op">*</span><span class="st"> </span>Ot</a>
<a class="sourceLine" id="cb2094-13" data-line-number="13">    <span class="kw">list</span>( <span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;Ct&quot;</span> =<span class="st"> </span>Ct,</a>
<a class="sourceLine" id="cb2094-14" data-line-number="14">          <span class="st">&quot;Ft&quot;</span> =<span class="st"> </span>Ft, <span class="st">&quot;It&quot;</span> =<span class="st"> </span>It, <span class="st">&quot;Gt&quot;</span> =<span class="st"> </span>Gt, <span class="st">&quot;Ot&quot;</span> =<span class="st"> </span>Ot</a>
<a class="sourceLine" id="cb2094-15" data-line-number="15">         )</a>
<a class="sourceLine" id="cb2094-16" data-line-number="16">}</a></code></pre></div>

<p><strong>LSTM Backpropagation</strong></p>
<p>Derived from the <strong>cross-entropy</strong> loss for <strong>softmax</strong> to start the backpropagation, we obtain the <strong>Delta y</strong> (<span class="math inline">\(\delta y\)</span>) similar to the <strong>Vanilla RNN</strong>. We also derive the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span> which we also reference as <strong>dH.next</strong> in our implementation. See Figure <a href="deeplearning2.html#fig:lstmbackprop">13.9</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lstmbackprop"></span>
<img src="lstmbackprop.png" alt="RNN (Softmax)" width="80%" />
<p class="caption">
Figure 13.9: RNN (Softmax)
</p>
</div>
<p>We know that the gradient of our loss with respect to <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> is the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span>. Recalling the example derivation from Vanilla RNN backpropagation, we also add <strong>dH.next</strong> which is initially zero.</p>
<p><span class="math display">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} +  \delta \mathbf{H}_{nxh}^{(t+1)} = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \underbrace{\delta \mathbf{H}_{nxh}^{(t+1)}}_{\mathbf{\text{dH.next}}}
\end{align}\]</span></p>
<p>Our gradients fo the weight <strong>V</strong> <span class="math inline">\(\left(\nabla \mathbf{V_{hxo}}\right)\)</span> and bias <span class="math inline">\(\left(\nabla \mathbf{b}_{1xh}^{(y)}\right)\)</span> follow similar derivation from <strong>Vanilla RNN</strong>.</p>
<p>Now, to calculate <span class="math inline">\(\left(\delta \mathbf{C}_{nxh}^{(t)}\right)\)</span>, let us first obtain the gradient of <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> with respect to <span class="math inline">\(\mathbf{C}_{nxh}^{(t)}\)</span> using Figure <a href="deeplearning2.html#fig:lstmbackprop">13.9</a> as reference:</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial \mathbf{H}_{nxh}^{(t)}} {\partial \mathbf{C}_{nxh}^{(t)}}\right) = \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot \mathbf{O}_{nxh}^{(t)} 
\end{align}\]</span></p>
<p>Therefore, we obtain the following formulation:</p>
<p><span class="math display">\[\begin{align}
\delta \mathbf{C}_{nxh}^{(t)} &amp;= \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{C}_{nxh}^{(t)}} 
 = \left(\frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} \right)
    \left(\frac{\partial \mathbf{H}_{nxh}^{(t)}} {\partial \mathbf{C}_{nxh}^{(t)}}  \right)
+  \delta \mathbf{C}_{nxh}^{(t+1)}\\
&amp;= \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}
\end{align}\]</span></p>
<p>That becomes our <strong>initial</strong> <strong>Delta C</strong> or <strong>dC.Next</strong> which is used along with the <strong>initial</strong> <strong>dH.next</strong> to calculate other gradients for our <strong>LSTM</strong>.</p>
<p>We follow this with the calculation of gradients for the gates:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{F}_{nxh}^{(t)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta f =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{F}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{F}_{nxh}^{(t)}} 
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{C}_{nxh}^{(t-1)}
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display">\[\begin{align}
\delta f = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{C}_{nxh}^{(t-1)}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{I}_{nxh}^{(t)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta i =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{I}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{I}_{nxh}^{(t)}}  
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{G}_{nxh}^{(t)}
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display">\[\begin{align}
\delta i = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{G}_{nxh}^{(t)}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{G}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta g =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{I}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}} 
= \delta \mathbf{C}_{nxh}^{(t)}  \odot\mathbf{I}_{nxh}^{(t)}
\end{align}\]</span></p>
<p>The formulation is also equivalent to:</p>
<p><span class="math display">\[\begin{align}
\delta g = \left( \delta \mathbf{H}_{nxh}^{(t)}  
        \odot \left(1 - \mathbf{tanh}^2(\mathbf{C}_{nxh}^{(t)})\right) \odot
        \mathbf{O}_{nxh}^{(t)} +  \underbrace{\delta \mathbf{C}_{nxh}^{(t+1)}}_{\mathbf{\text{dC.next}}}\right)  \odot\mathbf{I}_{nxh}^{(t)}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{O}_{nxh}^{(t)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta o =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{O}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{O}_{nxh}^{(t)}}
= \delta \mathbf{H}_{nxh}^{(t)} \odot \mathbf{tanh}\left(\mathbf{C}_{nxh}^{(t)}\right)
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to the linear functions. Now, in <strong>Vanilla RNN</strong>, we require the first derivative of the tangent function. Here, we also include the first derivative of the sigmoid function, which is required in calculating gradients for the three gates.</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{tanh}}&#39;(a) = \left(1 - \mathbf{tanh}^2(a)\right)
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\sigma&#39;(a) = \sigma(a)\left(1 - \sigma(a)\right)
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{F}}_{nxh}^{(t)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta \hat{f} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{F}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{F}_{nxh}^{(t)}}
\frac{\partial \mathbf{F}_{nxh}^{(t)}}{\partial \mathbf{\hat{F}}_{nxh}^{(t)}}
=\delta f \odot \mathbf{F}_{nxh}^{(t)} \odot ( 1 - \mathbf{F}_{nxh}^{(t)}) 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{f}\)</span> can also be represented as:</p>
<p><span class="math display">\[\begin{align}
\delta \hat{f} = \delta f \odot \sigma(A_f) \odot ( 1 - \sigma(A_f)) 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{I}}_{nxh}^{(t)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta \hat{i} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{I}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{I}_{nxh}^{(t)}}
\frac{\partial \mathbf{I}_{nxh}^{(t)}}{\partial \mathbf{\hat{I}}_{nxh}^{(t)}}
=\delta i \odot \mathbf{I}_{nxh}^{(t)} \odot ( 1 - \mathbf{I}_{nxh}^{(t)}) 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{i}\)</span> can also be represented as:</p>
<p><span class="math display">\[\begin{align}
\delta \hat{i} = \delta i \odot \sigma(A_i) \odot ( 1 - \sigma(A_i)) 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{G}}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta \hat{g} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{C}_{nxh}^{(t)}}
\frac{\partial \mathbf{C}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}}
=\delta g \odot  \left( 1 - \left({\mathbf{G}_{nxh}^{(t)}}\right)^2\right) 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{g}\)</span> can also be represented as:</p>
<p><span class="math display">\[\begin{align}
\delta \hat{g} = \delta g \odot ( 1 - \tanh^2(A_g)) 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\hat{\mathbf{O}}_{nxh}^{(t)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta \hat{o} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{\hat{O}}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{O}_{nxh}^{(t)}}
\frac{\partial \mathbf{O}_{nxh}^{(t)}}{\partial \mathbf{\hat{O}}_{nxh}^{(t)}}
=\delta o \odot \mathbf{O}_{nxh}^{(t)} \odot ( 1 - \mathbf{O}_{nxh}^{(t)}) 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\delta \hat{o}\)</span> can also be represented as:</p>
<p><span class="math display">\[\begin{align}
\delta \hat{o} = \delta o \odot \sigma(A_o) \odot ( 1 - \sigma(A_o)) 
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to weights and biases. Note that, because we concatenated <strong>X</strong> and <strong>H</strong>, we can choose to calculate the gradient for each of their weights. In that case, we only use the <span class="math inline">\(\mathbf{p \times h}\)</span> (dimension-wise) portion of the concatenated <span class="math inline">\(\mathbf{W_{[p,h]xh}}\)</span> for the weights for <strong>X</strong> and <span class="math inline">\(\mathbf{h \times h}\)</span> portion for the weights for <strong>H</strong>. For example:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\mathbf{W_{pxh}}}_{\text{X portion}}\ \ \ \ \ \ \ \ \ \ \ \
\underbrace{\mathbf{W_{hxh}}}_{\text{H portion}}\ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\mathbf{W_{[p,h]xh}}}_{\text{concatenated weights for X and H}} 
\end{align}\]</span></p>
<p>But only for notation convenience, let us use the concatenation format instead:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(f)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(f)}\right)\)</span> in the <strong>forget gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(f)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(f)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{f}
  \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(f)} =  \sum_{column-wise}{\delta \hat{f}} 
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(i)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(i)}\right)\)</span> in the <strong>input gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(i)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(i)}}
 =  \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{i}
 \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(i)} = \sum_{column-wise}{ \delta \hat{i}}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(g)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(g)}\right)\)</span> for the <strong>candidate state</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(g)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(g)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{g}
   \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(g)} =  \sum_{column-wise}{\delta \hat{g}}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(o)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(o)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(o)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(o)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{o}
    \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(o)} =  \sum_{column-wise}{\delta \hat{o}}
\end{align}\]</span></p>
<p>Lastly, we calculate gradients with respect to <span class="math inline">\(\left(\mathbf{C}_{nxh}^{(t-1)}\right)\)</span>, <span class="math inline">\(\left(\mathbf{H}_{nxh}^{(t-1)}\right)\)</span>, and <span class="math inline">\(\left(\mathbf{X}_{nxh}^{(t)}\right)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\nabla \mathbf{X}_{nxp}^{(t)} &amp;= 
   \delta f \cdot   \left(\mathbf{W_{pxh}^{(f)}}\right)^{\text{T}} 
 + \delta i \cdot   \left(\mathbf{W_{pxh}^{(i)}}\right)^{\text{T}}
 + \delta g \cdot   \left(\mathbf{W_{pxh}^{(g)}}\right)^{\text{T}}  
 + \delta o \cdot   \left(\mathbf{W_{pxh}^{(o)}}\right)^{\text{T}} \\
\underbrace{\nabla \mathbf{H}_{nxh}^{(t-1)}}_{\mathbf{\text{new dH.next}}}
   &amp;=  \delta f \cdot   \left(\mathbf{W_{hxh}^{(f)}}\right)^{\text{T}} 
 + \delta i \cdot   \left(\mathbf{W_{hxh}^{(i)}}\right)^{\text{T}}
 + \delta g \cdot   \left(\mathbf{W_{hxh}^{(g)}}\right)^{\text{T}}  
 + \delta o \cdot   \left(\mathbf{W_{hxh}^{(o)}}\right)^{\text{T}} \\
\underbrace{\nabla \mathbf{C}_{nxh}^{(t-1)}}_{\mathbf{\text{new dC.next}}}  
   &amp;=  \delta \mathbf{C}_{nxh}^{(t)} \odot F_{nxh}^{(t)}
\end{align}\]</span></p>
<p>Both <span class="math inline">\(\left(\nabla \mathbf{H}_{nxh}^{(t-1)}\right)\)</span> and <span class="math inline">\(\left(\nabla \mathbf{C}_{nxh}^{(t-1)}\right)\)</span> are calculated to be the gradients of previous time step; but they become the new <strong>dH.next</strong> and new <strong>dC.next</strong> respectively which we use to propagate back to the next previous time step.</p>
<p><span class="math display">\[\begin{align}
\text{dH.next} = \delta \mathbf{H}_{nxh}^{(t-1)}
\ \ \ \ \ \ \ \ \ \ 
\text{dC.next} = \delta \mathbf{C}_{nxh}^{(t-1)}
\end{align}\]</span></p>
<p>Let us now review our example implementation of <strong>LSTM backpropagation</strong> based on the gradient formulations above (and for additional consistency, we reference an LSTM Python code from Fisseha Berhane <span class="citation">(<a href="bibliography.html#ref-ref2060f">n.d.</a>)</span>): </p>

<div class="sourceCode" id="cb2095"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2095-1" data-line-number="1">backward.unit.LSTM &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next, dC.next, X, Y, H, C, model, </a>
<a class="sourceLine" id="cb2095-2" data-line-number="2">                               params, grad) {</a>
<a class="sourceLine" id="cb2095-3" data-line-number="3">    p       =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb2095-4" data-line-number="4">    h       =<span class="st"> </span><span class="kw">ncol</span>(H)</a>
<a class="sourceLine" id="cb2095-5" data-line-number="5">    Ft      =<span class="st"> </span>model<span class="op">$</span>Ft;          It     =<span class="st"> </span>model<span class="op">$</span>It</a>
<a class="sourceLine" id="cb2095-6" data-line-number="6">    Gt      =<span class="st"> </span>model<span class="op">$</span>Gt;          Ot     =<span class="st"> </span>model<span class="op">$</span>Ot</a>
<a class="sourceLine" id="cb2095-7" data-line-number="7">    Ct      =<span class="st"> </span>model<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2095-8" data-line-number="8">    Ht      =<span class="st"> </span>model<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2095-9" data-line-number="9">    Wi      =<span class="st"> </span>params<span class="op">$</span>Wi<span class="op">$</span>weight;  Wf     =<span class="st"> </span>params<span class="op">$</span>Wf<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-10" data-line-number="10">    Wg      =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight;  Wo     =<span class="st"> </span>params<span class="op">$</span>Wo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-11" data-line-number="11">    bf      =<span class="st"> </span>params<span class="op">$</span>bf<span class="op">$</span>weight;  bi     =<span class="st"> </span>params<span class="op">$</span>bi<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-12" data-line-number="12">    bg      =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight;  bo     =<span class="st"> </span>params<span class="op">$</span>bo<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2095-13" data-line-number="13">    dC      =<span class="st"> </span>(Ot <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">rnn.tanh</span>(Ct)<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>dH.next) <span class="op">+</span><span class="st"> </span>dC.next</a>
<a class="sourceLine" id="cb2095-14" data-line-number="14">    dFt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>C) <span class="op">*</span><span class="st"> </span>Ft <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Ft)</a>
<a class="sourceLine" id="cb2095-15" data-line-number="15">    dIt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>Gt) <span class="op">*</span><span class="st"> </span>It <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>It)</a>
<a class="sourceLine" id="cb2095-16" data-line-number="16">    dGt     =<span class="st"> </span>(dC <span class="op">*</span><span class="st"> </span>It) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Gt<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2095-17" data-line-number="17">    dOt     =<span class="st"> </span>dH.next <span class="op">*</span><span class="st"> </span><span class="kw">rnn.tanh</span>(Ct) <span class="op">*</span><span class="st"> </span>Ot <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Ot)</a>
<a class="sourceLine" id="cb2095-18" data-line-number="18">    XH      =<span class="st"> </span><span class="kw">cbind</span>(X, H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2095-19" data-line-number="19">    <span class="co"># Calculate gradient wrt to shared Weights and Biases</span></a>
<a class="sourceLine" id="cb2095-20" data-line-number="20">    dWft    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dFt;       grad<span class="op">$</span>dF  =<span class="st"> </span>grad<span class="op">$</span>dF <span class="op">+</span><span class="st"> </span>dWft</a>
<a class="sourceLine" id="cb2095-21" data-line-number="21">    dWit    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dIt;       grad<span class="op">$</span>dI  =<span class="st"> </span>grad<span class="op">$</span>dI <span class="op">+</span><span class="st"> </span>dWit</a>
<a class="sourceLine" id="cb2095-22" data-line-number="22">    dWgt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dGt;       grad<span class="op">$</span>dG  =<span class="st"> </span>grad<span class="op">$</span>dG <span class="op">+</span><span class="st"> </span>dWgt</a>
<a class="sourceLine" id="cb2095-23" data-line-number="23">    dWot    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dOt;       grad<span class="op">$</span>dO  =<span class="st"> </span>grad<span class="op">$</span>dO <span class="op">+</span><span class="st"> </span>dWot</a>
<a class="sourceLine" id="cb2095-24" data-line-number="24">    dbf     =<span class="st"> </span><span class="kw">apply</span>(dFt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbf =<span class="st"> </span>grad<span class="op">$</span>dbf <span class="op">+</span><span class="st"> </span>dbf</a>
<a class="sourceLine" id="cb2095-25" data-line-number="25">    dbi     =<span class="st"> </span><span class="kw">apply</span>(dIt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbi =<span class="st"> </span>grad<span class="op">$</span>dbi <span class="op">+</span><span class="st"> </span>dbi</a>
<a class="sourceLine" id="cb2095-26" data-line-number="26">    dbg     =<span class="st"> </span><span class="kw">apply</span>(dGt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbg =<span class="st"> </span>grad<span class="op">$</span>dbg <span class="op">+</span><span class="st"> </span>dbg</a>
<a class="sourceLine" id="cb2095-27" data-line-number="27">    dbo     =<span class="st"> </span><span class="kw">apply</span>(dOt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbo =<span class="st"> </span>grad<span class="op">$</span>dbo <span class="op">+</span><span class="st"> </span>dbo</a>
<a class="sourceLine" id="cb2095-28" data-line-number="28">    dX      =<span class="st"> </span>dFt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wf[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dIt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wi[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2095-29" data-line-number="29"><span class="st">              </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dOt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wo[<span class="dv">1</span><span class="op">:</span>p,])</a>
<a class="sourceLine" id="cb2095-30" data-line-number="30">    dH      =<span class="st"> </span>dFt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wf[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span>dIt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wi[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span></a>
<a class="sourceLine" id="cb2095-31" data-line-number="31"><span class="st">              </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span>dOt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wo[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),])</a>
<a class="sourceLine" id="cb2095-32" data-line-number="32">    dC      =<span class="st"> </span>dC <span class="op">*</span><span class="st"> </span>Ft </a>
<a class="sourceLine" id="cb2095-33" data-line-number="33">    grad<span class="op">$</span>dX =<span class="st"> </span>dX</a>
<a class="sourceLine" id="cb2095-34" data-line-number="34">    grad<span class="op">$</span>dH =<span class="st"> </span>dH</a>
<a class="sourceLine" id="cb2095-35" data-line-number="35">    grad<span class="op">$</span>dC =<span class="st"> </span>dC</a>
<a class="sourceLine" id="cb2095-36" data-line-number="36">    grad</a>
<a class="sourceLine" id="cb2095-37" data-line-number="37">}</a></code></pre></div>

<p>For a simple use, let us concoct a simple dataset and initialize our parameters:</p>

<div class="sourceCode" id="cb2096"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2096-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2096-2" data-line-number="2">n     =<span class="st"> </span><span class="dv">5</span>    <span class="co"># number of samples</span></a>
<a class="sourceLine" id="cb2096-3" data-line-number="3">p     =<span class="st"> </span><span class="dv">30</span>   <span class="co"># number of features per sample (could also mean number of </span></a>
<a class="sourceLine" id="cb2096-4" data-line-number="4">             <span class="co"># probabilities of a word embedding)</span></a>
<a class="sourceLine" id="cb2096-5" data-line-number="5">h     =<span class="st"> </span><span class="dv">20</span>   <span class="co"># number of neurons in a hidden state</span></a>
<a class="sourceLine" id="cb2096-6" data-line-number="6">o     =<span class="st"> </span><span class="dv">3</span>    <span class="co"># number of output neurons in an output layer</span></a>
<a class="sourceLine" id="cb2096-7" data-line-number="7">X     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-8" data-line-number="8">H     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-9" data-line-number="9">C     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-10" data-line-number="10">W     =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p <span class="op">+</span><span class="st"> </span>h) <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p<span class="op">+</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-11" data-line-number="11"><span class="co"># copy same structure for other weights</span></a>
<a class="sourceLine" id="cb2096-12" data-line-number="12">Wf    =<span class="st"> </span>Wi =<span class="st"> </span>Wg =<span class="st"> </span>Wo =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>W)</a>
<a class="sourceLine" id="cb2096-13" data-line-number="13">bias  =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2096-14" data-line-number="14"><span class="co"># copy same structure for other biases</span></a>
<a class="sourceLine" id="cb2096-15" data-line-number="15">bf    =<span class="st"> </span>bi =<span class="st"> </span>bg =<span class="st"> </span>bo =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span>bias) </a></code></pre></div>

<p>We then run our <strong>forward feed</strong> like so:</p>

<div class="sourceCode" id="cb2097"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2097-1" data-line-number="1">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wf&quot;</span> =<span class="st"> </span>Wf, <span class="st">&quot;Wi&quot;</span> =<span class="st"> </span>Wi, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wo&quot;</span> =<span class="st"> </span>Wo,</a>
<a class="sourceLine" id="cb2097-2" data-line-number="2">              <span class="st">&quot;bf&quot;</span> =<span class="st"> </span>bf, <span class="st">&quot;bi&quot;</span> =<span class="st"> </span>bi, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;bo&quot;</span> =<span class="st"> </span>bo)</a>
<a class="sourceLine" id="cb2097-3" data-line-number="3">model  =<span class="st"> </span><span class="kw">forward.unit.LSTM</span>(X, H, C, params)</a>
<a class="sourceLine" id="cb2097-4" data-line-number="4"><span class="kw">str</span>(model, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## List of 6
## $ Ht: num [1:5, 1:20] 3.06e-02 -3.26e-03 -6.87e-05
##    7.50e-01 -2.87e-07 ...
## $ Ct: num [1:5, 1:20] 0.085496 -0.0067803 -0.0072869
##    1.0755145 -0.0000626 ...
## $ Ft: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...
## $ It: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...
## $ Gt: num [1:5, 1:20] -0.5236 -0.0763 -0.9998 0.9939 -1
##    ...
## $ Ot: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756
##    0.00459 ...</code></pre>

<p>Afterwhich, we follow that by running our <strong>backward pass</strong> like so:</p>

<div class="sourceCode" id="cb2099"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2099-1" data-line-number="1">gradients =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dC&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2099-2" data-line-number="2">                 <span class="st">&quot;dF&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dI&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dO&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2099-3" data-line-number="3">                 <span class="st">&quot;dbf&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbi&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbo&quot;</span> =<span class="st"> </span><span class="dv">0</span> )</a>
<a class="sourceLine" id="cb2099-4" data-line-number="4">dH.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2099-5" data-line-number="5">dC.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2099-6" data-line-number="6">gradients =<span class="st"> </span><span class="kw">backward.unit.LSTM</span>(dH.next, dC.next, X, Y, H, C, </a>
<a class="sourceLine" id="cb2099-7" data-line-number="7">                               model, params, gradients)</a>
<a class="sourceLine" id="cb2099-8" data-line-number="8"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 11
##  $ dX : num [1:5, 1:30] -0.119 2.364 1 -2.511 0.851 ...
##  $ dH : num [1:5, 1:20] 0.443 0.249 -0.344 -1.452 0.811 ...
##  $ dC : num [1:5, 1:20] -0.05873 0.07422 0.00317 -0.49132 0.00114 ...
##  $ dF : num [1:50, 1:20] -0.00558 -0.00869 -0.01672 -0.02838 -0.00533 ...
##  $ dI : num [1:50, 1:20] -0.00631 0.00258 -0.00859 -0.00632 -0.0213 ...
##  $ dG : num [1:50, 1:20] 0.02282 0.02802 0.00815 -0.03022 0.04779 ...
##  $ dO : num [1:50, 1:20] -0.00717 -0.00439 -0.01699 -0.02315 -0.01799 ...
##  $ dbf: num [1:20] -0.02811 -0.13134 -0.00873 0.00139 -0.07315 ...
##  $ dbi: num [1:20] -0.0131 0.171491 0.041701 0.000973 -0.131988 ...
##  $ dbg: num [1:20] 2.52e-02 -4.17e-01 -3.78e-02 -5.07e-09 -7.10e-01 ...
##  $ dbo: num [1:20] -2.75e-02 2.11e-01 1.19e-03 7.13e-06 -4.29e-01 ...</code></pre>

<p>Note that the calculation of our predicted output (<strong>Y.hat</strong>) using <strong>softmax</strong> and the gradientsâ calculation are removed from the implementation. The calculations are relocated to another function. The reason becomes apparent in the <strong>Deep Stacked Bidirectional RNN</strong> section.</p>
</div>
<div id="gated-recurrent-units-gru" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.2.3</span> Gated Recurrent Units (GRU)  <a href="deeplearning2.html#gated-recurrent-units-gru" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>GRU</strong> is another variant of <strong>RNN</strong> introduced by Kyunghyun Cho et al. <span class="citation">(<a href="bibliography.html#ref-ref1173k">2014</a>)</span> with an evaluation paper by Junyoung Chung et al. <span class="citation">(<a href="bibliography.html#ref-ref1158j">2014</a>)</span>. It is regarded as a simplified version of <strong>LSTM</strong>. See the design in Figure <a href="deeplearning2.html#fig:gru">13.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gru"></span>
<img src="gru.png" alt="GRU Cell" width="100%" />
<p class="caption">
Figure 13.10: GRU Cell
</p>
</div>
<p>In <strong>GRU</strong>, we do not have a <strong>forget gate</strong> and an <strong>input gate</strong>. Instead, it is the <strong>update gate</strong>, <span class="math inline">\(\mathbf{Z}_{nxh}^{(t)}\)</span>, that regulates the flow of information from state to state or layer to layer.</p>
<p><strong>GRU Forward Feed</strong></p>
<p>Based on Figure <a href="deeplearning2.html#fig:gru">13.10</a>, the <strong>GRU</strong> design uses five equations for <strong>forward feed</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\mathbf{Z}_{nxh}^{(t)} &amp;= \sigma \left(\mathbf{W}_{[p,h]xh}^{(z)} \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}  \right]  + \mathbf{b}_{1xh}^{(z)} \right) \\
\mathbf{R}_{nxh}^{(t)} &amp;= \sigma \left(\mathbf{W}_{[p,h]xh}^{(r)}  \left[ \mathbf{X}_{nxp}^{(t)}, \mathbf{H}_{nxh}^{(t-1)}\right]  + \mathbf{b}_{1xh}^{(r)}\right) \\
\mathbf{G}_{nxh}^{(t)} &amp;= \mathbf{\text{tanh}} \left(\mathbf{W}_{[p,h]xh}^{(g)}  \left[ \mathbf{X}_{nxp}^{(t)},\ \mathbf{R}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)}\right] + \mathbf{b}_{1xh}^{(g)}\right) \\
\mathbf{H}_{nxh}^{(t)} &amp;=  \left(1 - \mathbf{Z}_{nxh}^{(t)}\right) \odot \mathbf{H}_{nxh}^{(t-1)} + \mathbf{Z}_{nxh}^{(t)} \odot \mathbf{G}_{nxh}^{(t)} \\
\mathbf{\hat{Y}}_{nxo}^{(t)} &amp;=\mathbf{\text{softmax}}\left(\mathbf{H}_{nxh}^{(t)} \cdot \mathbf{V}_{hxo}+ \mathbf{b}_{1xo}^{(y)}\right) 
\end{align}\]</span></p>
<p>While we reference the paper by JunYoung Chung et al <span class="citation">(<a href="bibliography.html#ref-ref1158j">2014</a>)</span> which reflects more of the above formulation, it is notable to mention that other literature may have the following alternative formulation for <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span> which references a variant of Figure <a href="deeplearning2.html#fig:gru">13.10</a>:</p>
<p><span class="math display">\[\begin{align}
\mathbf{H}_{nxh}^{(t)} =  \left(1 - \mathbf{Z}_{nxh}^{(t)}\right) \odot  \mathbf{G}_{nxh}^{(t)}  + \mathbf{Z}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)}
\end{align}\]</span></p>
<p>We leave readers to investigate the difference in performance, accuracy, etc. That said, we have our example implementation of <strong>GRU forward feed</strong> based on the equations above but using the latter formulation for <span class="math inline">\(\mathbf{H}_{nxh}^{(t)}\)</span>. Note that our implementation is motivated by a MATLAB code from Minchen Li and partly in reference to our previous implementation of <strong>LSTM</strong>. </p>

<div class="sourceCode" id="cb2101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2101-1" data-line-number="1">forward.unit.GRU &lt;-<span class="st"> </span><span class="cf">function</span>(X, H, params) {</a>
<a class="sourceLine" id="cb2101-2" data-line-number="2">    Wz    =<span class="st"> </span>params<span class="op">$</span>Wz<span class="op">$</span>weight; Wr  =<span class="st"> </span>params<span class="op">$</span>Wr<span class="op">$</span>weight; Wg  =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2101-3" data-line-number="3">    bz    =<span class="st"> </span>params<span class="op">$</span>bz<span class="op">$</span>weight; br  =<span class="st"> </span>params<span class="op">$</span>br<span class="op">$</span>weight; bg  =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2101-4" data-line-number="4">    XH    =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2101-5" data-line-number="5">    Zt    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wz, <span class="dv">2</span>, bz, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2101-6" data-line-number="6">    Rt    =<span class="st"> </span><span class="kw">rnn.sigmoid</span>(<span class="kw">sweep</span>(XH <span class="op">%*%</span><span class="st"> </span>Wr, <span class="dv">2</span>, br, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2101-7" data-line-number="7">    rXH   =<span class="st"> </span><span class="kw">cbind</span>(X, Rt <span class="op">*</span><span class="st"> </span>H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2101-8" data-line-number="8">    G.hat =<span class="st"> </span><span class="kw">sweep</span>(rXH <span class="op">%*%</span><span class="st"> </span>Wg, <span class="dv">2</span>, bg, <span class="st">&#39;+&#39;</span>)</a>
<a class="sourceLine" id="cb2101-9" data-line-number="9">    Gt    =<span class="st"> </span><span class="kw">rnn.tanh</span>(G.hat)</a>
<a class="sourceLine" id="cb2101-10" data-line-number="10">    Ht    =<span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt) <span class="op">*</span><span class="st"> </span>Gt <span class="op">+</span><span class="st"> </span>Zt <span class="op">*</span><span class="st"> </span>H</a>
<a class="sourceLine" id="cb2101-11" data-line-number="11">    <span class="kw">list</span>(<span class="st">&quot;Ht&quot;</span> =<span class="st"> </span>Ht, <span class="st">&quot;G.hat&quot;</span> =<span class="st"> </span>G.hat,</a>
<a class="sourceLine" id="cb2101-12" data-line-number="12">         <span class="st">&quot;Zt&quot;</span> =<span class="st"> </span>Zt, <span class="st">&quot;Rt&quot;</span>    =<span class="st"> </span>Rt,    <span class="st">&quot;Gt&quot;</span>    =<span class="st"> </span>Gt)</a>
<a class="sourceLine" id="cb2101-13" data-line-number="13">}</a></code></pre></div>

<p><strong>GRU Backpropagation</strong></p>
<p>Similar to the <strong>Vanilla RNN</strong> and <strong>LSTM</strong>, we derive our <strong>cross-entropy</strong> loss for <strong>softmax</strong> to start the backpropagation. We obtain the <strong>Delta y</strong> (<span class="math inline">\(\delta y\)</span>) similar to the <strong>Vanilla RNN</strong>. We also derive the <strong>Delta H</strong> denoted by <span class="math inline">\(\left(\delta \mathbf{H}_{nxh}^{(t)}\right)\)</span> which we also reference as <strong>dH.next</strong> in our implementation. This has an initial value of zero.</p>
<p><span class="math display">\[\begin{align}
\delta \mathbf{H}_{nxh}^{(t)} = \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{H}_{nxh}^{(t)}} +  \delta \mathbf{H}_{nxh}^{(t+1)} = \delta y \cdot \left(\mathbf{V}_{hxo}\right)^{\text{T}}  + \underbrace{\delta \mathbf{H}_{nxh}^{(t+1)}}_{\mathbf{\text{dH.next}}}
\end{align}\]</span></p>
<p>Our gradients fo the weight <strong>V</strong> <span class="math inline">\(\left(\nabla \mathbf{V_{hxo}}\right)\)</span> and bias <span class="math inline">\(\left(\nabla \mathbf{b}_{1xh}^{(y)}\right)\)</span> follow similar derivation from <strong>Vanilla RNN</strong>.</p>
<p>Next, we calculate the gradients of each gate, namely:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{Z}_{nxh}^{(t)}\right)\)</span> in the <strong>update gate</strong>,</p>
<p><span class="math display">\[\begin{align}
\delta z =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{Z}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{Z}_{nxh}^{(t)}}  
= \delta \mathbf{H}_{nxh}^{(t)} \odot \left( \mathbf{H}_{nxh}^{(t-1)} - \mathbf{G}_{nxh}^{(t)} \right)
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{G}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>,</p>
<p><span class="math display">\[\begin{align}
\delta g =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{G}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}  
= \delta \mathbf{H}_{nxh}^{(t)} \odot \left(1 - \mathbf{Z}_{nxh}^{(t)} \right)
\end{align}\]</span></p>
<p>and <strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{R}_{nxh}^{(t)}\right)\)</span> in the <strong>reset gate</strong>,</p>
<p><span class="math display">\[\begin{align}
\delta r =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{R}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  
= \delta g \odot \left(\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  \right)
\end{align}\]</span></p>
<p>where <span class="citation">(Ahlad Kumar <a href="bibliography.html#ref-ref1174a">2021</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
\left(\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}  \right) =
\left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right) \odot \left( 1 - \mathbf{\text{tanh}}^2 \left( \hat{\mathbf{G}}_{nxh}^{(t)} \right)\right)
\end{align}\]</span></p>
<p>and where:</p>
<p><span class="math display">\[\begin{align}
\hat{\mathbf{G}}_{nxh}^{(t)} = \mathbf{W}_{[p,h]xh}^{(g)}  \left[ \mathbf{X}_{nxp}^{(t)},\ \mathbf{R}_{nxh}^{(t)} \odot \mathbf{H}_{nxh}^{(t-1)}\right] + \mathbf{b}_{1xh}^{(g)}.
\end{align}\]</span></p>
<p>For the partial derivative of <strong>tanh</strong>, let us recall calculus:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial\ tanh(ax + by + z)}{\partial x} = sec^2(ax + by + z)(a) = a(1 - tan^2(ax + by + z))
\end{align}\]</span></p>
<p>such that we have:</p>
<p><span class="math display">\[\begin{align}
a = \left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right)
\end{align}\]</span></p>
<p>Also, it is important to note that the input <strong>X</strong> denoted by <span class="math inline">\(\left(\mathbf{X}_{nxp}^{(t)}\right)\)</span> is canceled out from the equation (see calculus); and therefore, its corresponding weight is also canceled out from the concatenation so that we have:</p>
<p><span class="math display">\[\begin{align}
\mathbf{W}_{[p,h]xh}^{(g)} \ \ \ \ \ \rightarrow\ \ \ \ \ \ \mathbf{W}_{hxh}^{(g)} 
\end{align}\]</span></p>
<p>Therefore, our gradient in the <strong>reset gate</strong> becomes:</p>
<p><span class="math display">\[\begin{align}
\delta r = \delta g \odot  \left(\mathbf{H}_{nxh}^{(t-1)} \cdot \mathbf{W}_{hxh}^{(g)} \right) \odot \left( 1 - \mathbf{\text{tanh}}^2 \left( \hat{\mathbf{G}}_{nxh}^{(t)} \right)\right)
\end{align}\]</span></p>
<p>Next, we calculate the gradients with respect to the linear functions, namely:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{Z}}_{nxh}^{(t)}\right)\)</span> in the <strong>update gate</strong>,</p>
<p><span class="math display">\[\begin{align}
\delta \hat{z} =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{Z}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{Z}_{nxh}^{(t)}}   
\frac{\partial \mathbf{Z}_{nxh}^{(t)}}{\partial \mathbf{\hat{Z}}_{nxh}^{(t)}}  
= \delta z \odot \mathbf{Z}_{nxh}^{(t)} \odot \left(1 - \mathbf{Z}_{nxh}^{(t)}\right)
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{G}}_{nxh}^{(t)}\right)\)</span> for the <strong>candidate state</strong>,</p>
<p><span class="math display">\[\begin{align}
\delta \hat{g} =  \frac{\partial \mathcal{L}^{(CE)}} {\partial \mathbf{G}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{H}_{nxh}^{(t)}}
\frac{\partial \mathbf{H}_{nxh}^{(t)}}{\partial \mathbf{G}_{nxh}^{(t)}}   
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{\hat{G}}_{nxh}^{(t)}}  
= \delta g \odot \left(1 - \left(\mathbf{G}_{nxh}^{(t)}\right)^2\right)   
\end{align}\]</span></p>
<p>and <strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{\hat{R}}_{nxh}^{(t)}\right)\)</span> in the <strong>reset gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\delta \hat{r} =  \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{R}_{nxh}^{(t)}} = 
\frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{G}_{nxh}^{(t)}}
\frac{\partial \mathbf{G}_{nxh}^{(t)}}{\partial \mathbf{R}_{nxh}^{(t)}}   
\frac{\partial \mathbf{R}_{nxh}^{(t)}}{\partial \mathbf{\hat{R}}_{nxh}^{(t)}}  
= \delta r \odot  \mathbf{R}_{nxh}^{(t)} \left(1 -  \mathbf{R}_{nxh}^{(t)}\right)
\end{align}\]</span></p>
<p>Lastly, let us get the gradients with respect to the weights and biases. Similarly, only for notation convenience, let us use the concatenation format instead:</p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(z)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(z)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(z)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(z)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{z}
    \ \ \ \ \ \ \ \ \
\nabla b_{1xh}^{(z)} =  \sum_{column-wise}{\delta \hat{z}}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(g)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(g)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(g)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(g)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{g}
    \ \ \ \ \ \ \ \ \ 
\nabla b_{1xh}^{(g)} =  \sum_{column-wise}{\delta \hat{g}}
\end{align}\]</span></p>
<p><strong>Gradient</strong> with respect to <span class="math inline">\(\left(\mathbf{W}_{[p,h]xh}^{(r)}\right)\)</span> and <span class="math inline">\(\left(\mathbf{b}_{1xh}^{(r)}\right)\)</span> in the <strong>output gate</strong>.</p>
<p><span class="math display">\[\begin{align}
\nabla W_{[p,h]xh}^{(r)} = \frac{\partial \mathcal{L}^{(CE)}}{\partial \mathbf{W}_{[p,h]xh}^{(r)}}
 = \left([\mathbf{X,H}]_{nx[p,h]}^{(t)}\right)^{\text{T}}  \cdot \delta \hat{r}
    \ \ \ \ \ \ \ \ \ 
\nabla b_{1xh}^{(r)} =  \sum_{column-wise}{\delta \hat{r}}
\end{align}\]</span></p>
<p>Lastly, we calculate gradients with respect to <span class="math inline">\(\left(\mathbf{H}_{nxh}^{(t-1)}\right)\)</span>, and <span class="math inline">\(\left(\mathbf{X}_{nxh}^{(t)}\right)\)</span>.</p>
<p><span class="math display">\[\begin{align}
\nabla \mathbf{X}_{nxp}^{(t)} &amp;= 
   \delta z \cdot   \left(\mathbf{W_{pxh}^{(z)}}\right)^{\text{T}} 
 + \delta g \cdot   \left(\mathbf{W_{pxh}^{(g)}}\right)^{\text{T}}
 + \delta r \cdot   \left(\mathbf{W_{pxh}^{(r)}}\right)^{\text{T}} \\
\underbrace{\nabla \mathbf{H}_{nxh}^{(t-1)}}_{\text{new dH.next}} &amp;= 
   \delta z \cdot   \left(\mathbf{W_{hxh}^{(z)}}\right)^{\text{T}} 
 + \delta g \cdot   \left(\mathbf{W_{hxh}^{(g)}}\right)^{\text{T}}
 + \delta r \cdot   \left(\mathbf{W_{hxh}^{(r)}}\right)^{\text{T}}  
\end{align}\]</span></p>
<p>The <span class="math inline">\(\left(\nabla \mathbf{H}_{nxh}^{(t-1)}\right)\)</span> becomes our new <strong>dH.next</strong> which we use to propagate back to the next previous time step.</p>
<p><span class="math display">\[\begin{align}
\text{dH.next} = \delta \mathbf{H}_{nxh}^{(t-1)}
\end{align}\]</span></p>
<p>Let us now review our example implementation of <strong>GRU backpropagation</strong> based on the formulations above: </p>

<div class="sourceCode" id="cb2102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2102-1" data-line-number="1">backward.unit.GRU &lt;-<span class="st"> </span><span class="cf">function</span>(dH.next,  X, Y, H, model, params, grad) {</a>
<a class="sourceLine" id="cb2102-2" data-line-number="2">  p       =<span class="st"> </span><span class="kw">ncol</span>(X)</a>
<a class="sourceLine" id="cb2102-3" data-line-number="3">  h       =<span class="st"> </span><span class="kw">ncol</span>(H)</a>
<a class="sourceLine" id="cb2102-4" data-line-number="4">  Ht      =<span class="st"> </span>model<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2102-5" data-line-number="5">  Zt      =<span class="st"> </span>model<span class="op">$</span>Zt;          Rt =<span class="st"> </span>model<span class="op">$</span>Rt;         Gt =<span class="st"> </span>model<span class="op">$</span>Gt</a>
<a class="sourceLine" id="cb2102-6" data-line-number="6">  Wz      =<span class="st"> </span>params<span class="op">$</span>Wz<span class="op">$</span>weight;  Wr =<span class="st"> </span>params<span class="op">$</span>Wr<span class="op">$</span>weight; Wg =<span class="st"> </span>params<span class="op">$</span>Wg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2102-7" data-line-number="7">  bz      =<span class="st"> </span>params<span class="op">$</span>bz<span class="op">$</span>weight;  br =<span class="st"> </span>params<span class="op">$</span>br<span class="op">$</span>weight; bg =<span class="st"> </span>params<span class="op">$</span>bg<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2102-8" data-line-number="8">  G.hat   =<span class="st"> </span>model<span class="op">$</span>G.hat</a>
<a class="sourceLine" id="cb2102-9" data-line-number="9">  dH      =<span class="st"> </span>dH.next</a>
<a class="sourceLine" id="cb2102-10" data-line-number="10">  dZt     =<span class="st"> </span>dH <span class="op">*</span><span class="st"> </span>( Ht <span class="op">-</span><span class="st"> </span>Gt) <span class="op">*</span><span class="st"> </span>Zt <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt)</a>
<a class="sourceLine" id="cb2102-11" data-line-number="11">  dGt     =<span class="st"> </span>dH <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Zt) <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Gt<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2102-12" data-line-number="12">  dRt     =<span class="st"> </span>( dGt <span class="op">*</span><span class="st"> </span>(Ht <span class="op">%*%</span><span class="st"> </span>Wr[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">rnn.tanh</span>(G.hat)<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span></a>
<a class="sourceLine" id="cb2102-13" data-line-number="13"><span class="st">              </span>Rt <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Rt)</a>
<a class="sourceLine" id="cb2102-14" data-line-number="14">  XH      =<span class="st"> </span><span class="kw">cbind</span>(X,H) <span class="co"># concatenate</span></a>
<a class="sourceLine" id="cb2102-15" data-line-number="15">  <span class="co"># Calculate gradient wrt to shared Weights and Biases</span></a>
<a class="sourceLine" id="cb2102-16" data-line-number="16">  dWzt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dZt;       grad<span class="op">$</span>dZ  =<span class="st"> </span>grad<span class="op">$</span>dZ <span class="op">+</span><span class="st"> </span>dWzt</a>
<a class="sourceLine" id="cb2102-17" data-line-number="17">  dWgt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dGt;       grad<span class="op">$</span>dG  =<span class="st"> </span>grad<span class="op">$</span>dG <span class="op">+</span><span class="st"> </span>dWgt</a>
<a class="sourceLine" id="cb2102-18" data-line-number="18">  dWrt    =<span class="st"> </span><span class="kw">t</span>(XH) <span class="op">%*%</span><span class="st"> </span>dRt;       grad<span class="op">$</span>dR  =<span class="st"> </span>grad<span class="op">$</span>dR <span class="op">+</span><span class="st"> </span>dWrt</a>
<a class="sourceLine" id="cb2102-19" data-line-number="19">  dbz     =<span class="st"> </span><span class="kw">apply</span>(dZt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbz =<span class="st"> </span>grad<span class="op">$</span>dbz <span class="op">+</span><span class="st"> </span>dbz    </a>
<a class="sourceLine" id="cb2102-20" data-line-number="20">  dbg     =<span class="st"> </span><span class="kw">apply</span>(dGt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbg =<span class="st"> </span>grad<span class="op">$</span>dbg <span class="op">+</span><span class="st"> </span>dbg</a>
<a class="sourceLine" id="cb2102-21" data-line-number="21">  dbr     =<span class="st"> </span><span class="kw">apply</span>(dRt, <span class="dv">2</span>, sum);  grad<span class="op">$</span>dbr =<span class="st"> </span>grad<span class="op">$</span>dbr <span class="op">+</span><span class="st"> </span>dbr</a>
<a class="sourceLine" id="cb2102-22" data-line-number="22">  dX      =<span class="st"> </span>dZt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wz[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st"> </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[<span class="dv">1</span><span class="op">:</span>p,]) <span class="op">+</span><span class="st">  </span>dRt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wr[<span class="dv">1</span><span class="op">:</span>p,])</a>
<a class="sourceLine" id="cb2102-23" data-line-number="23">  dH      =<span class="st"> </span>dZt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wz[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2102-24" data-line-number="24"><span class="st">            </span>dGt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wg[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) <span class="op">+</span></a>
<a class="sourceLine" id="cb2102-25" data-line-number="25"><span class="st">            </span>dRt <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(Wr[(p<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>(h<span class="op">+</span>p),]) </a>
<a class="sourceLine" id="cb2102-26" data-line-number="26">  grad<span class="op">$</span>dX =<span class="st"> </span>dX</a>
<a class="sourceLine" id="cb2102-27" data-line-number="27">  grad<span class="op">$</span>dH =<span class="st"> </span>dH</a>
<a class="sourceLine" id="cb2102-28" data-line-number="28">  grad</a>
<a class="sourceLine" id="cb2102-29" data-line-number="29">}</a></code></pre></div>

<p>For a simple use, let us concoct a simple dataset and initialize our parameters:</p>

<div class="sourceCode" id="cb2103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2103-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2103-2" data-line-number="2">n    =<span class="st"> </span><span class="dv">5</span>; p =<span class="st"> </span><span class="dv">30</span>; h =<span class="st"> </span><span class="dv">20</span>; o =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2103-3" data-line-number="3">X    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>p, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-4" data-line-number="4">H    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-5" data-line-number="5">W    =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>((p <span class="op">+</span><span class="st"> </span>h) <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>p<span class="op">+</span>h, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-6" data-line-number="6">Wz   =<span class="st"> </span>Wg =<span class="st"> </span>Wr =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span>=W)    <span class="co">#copy same structure for other weights</span></a>
<a class="sourceLine" id="cb2103-7" data-line-number="7">bias =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span><span class="dv">1</span>, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2103-8" data-line-number="8">bz   =<span class="st"> </span>bg =<span class="st"> </span>br =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span>=bias) <span class="co">#copy same structure for other biases</span></a></code></pre></div>

<p>We then run our <strong>forward feed</strong> like so:</p>

<div class="sourceCode" id="cb2104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2104-1" data-line-number="1">params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wz&quot;</span> =<span class="st"> </span>Wz, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wr&quot;</span> =<span class="st"> </span>Wr,</a>
<a class="sourceLine" id="cb2104-2" data-line-number="2">              <span class="st">&quot;bz&quot;</span> =<span class="st"> </span>bz, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;br&quot;</span> =<span class="st"> </span>br)</a>
<a class="sourceLine" id="cb2104-3" data-line-number="3">model  =<span class="st"> </span><span class="kw">forward.unit.GRU</span>(X, H, params)</a>
<a class="sourceLine" id="cb2104-4" data-line-number="4"><span class="kw">str</span>(model)</a></code></pre></div>
<pre><code>## List of 5
##  $ Ht   : num [1:5, 1:20] -0.988 0.474 0.615 0.953 0.513 ...
##  $ G.hat: num [1:5, 1:20] -4.472 0.137 0.606 1.184 1.049 ...
##  $ Zt   : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ...
##  $ Rt   : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ...
##  $ Gt   : num [1:5, 1:20] -1 0.136 0.541 0.829 0.782 ...</code></pre>

<p>Afterwhich, we follow that by running our <strong>backward pass</strong> like so:</p>

<div class="sourceCode" id="cb2106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2106-1" data-line-number="1">gradients =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2106-2" data-line-number="2">                 <span class="st">&quot;dZ&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dR&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2106-3" data-line-number="3">                 <span class="st">&quot;dbz&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbr&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2106-4" data-line-number="4">dH.next   =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rnorm</span>(n <span class="op">*</span><span class="st"> </span>h), <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2106-5" data-line-number="5">gradients =<span class="st"> </span><span class="kw">backward.unit.GRU</span>(dH.next, X, Y, H, model, params, gradients)</a>
<a class="sourceLine" id="cb2106-6" data-line-number="6"><span class="kw">str</span>(gradients)</a></code></pre></div>
<pre><code>## List of 8
##  $ dX : num [1:5, 1:30] 1.0933 -0.8241 0.2523 -0.1001 0.0675 ...
##  $ dH : num [1:5, 1:20] 1.653 -1.46 -0.101 1.061 0.117 ...
##  $ dZ : num [1:50, 1:20] -0.03323 -0.05663 -0.0364 -0.00337 -0.06603 ...
##  $ dG : num [1:50, 1:20] 0.6443 0.0415 0.2329 0.233 0.2817 ...
##  $ dR : num [1:50, 1:20] 0.0561 0.1294 0.0907 0.0223 0.1606 ...
##  $ dbz: num [1:20] -0.073512 0.004256 -0.000381 0.180441 -0.070078 ...
##  $ dbg: num [1:20] 0.52703 -0.00109 -0.00139 0.13647 0.09256 ...
##  $ dbr: num [1:20] 1.79e-01 -4.04e-07 -1.25e-06 -3.29e-02 -3.01e-02 ...</code></pre>

<p>We leave readers to investigate <strong>Minimal Gated Unit (MGU)</strong>, a variant of <strong>GRU</strong>.</p>
</div>
</div>
<div id="deep-stacked-rnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.3</span> Deep Stacked RNN <a href="deeplearning2.html#deep-stacked-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Each cell of an <strong>RNN</strong> forms individual units or components that make up a deep recurrent neural network. This section shows how the cells can stack up on top of each other. Figure <a href="deeplearning2.html#fig:stackedrnn">13.11</a> is a view of a rolled representation of a <strong>deep-stacked RNN</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stackedrnn"></span>
<img src="stackedrnn.png" alt="Deep Stacked  RNN" width="80%" />
<p class="caption">
Figure 13.11: Deep Stacked RNN
</p>
</div>
<p>To fully implement a <strong>deep-stacked RNN</strong>, we first need to implement a few helper functions. We start with the <strong>optimize.update(.)</strong> function that handles optimized updates. This function can be tailored to use <strong>sgd</strong>, <strong>sgd with nesperov momentum</strong>, <strong>adam</strong>, <strong>adagrad</strong>, <strong>adadelta</strong>, etc. similar to our implementation of <strong>CNN</strong>. For our own purposes, we use <strong>adam</strong> as our optimizer.</p>

<div class="sourceCode" id="cb2108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2108-1" data-line-number="1">optimize.adam =<span class="st"> </span>adam &lt;-<span class="st"> </span><span class="cf">function</span>(param, gradient, eta, t) {</a>
<a class="sourceLine" id="cb2108-2" data-line-number="2">    beta1 =<span class="st"> </span><span class="fl">0.90</span>; beta2 =<span class="st"> </span><span class="fl">0.999</span>; eps=<span class="fl">1e-10</span></a>
<a class="sourceLine" id="cb2108-3" data-line-number="3">    param<span class="op">$</span>rho    =<span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>rho <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1) <span class="op">*</span><span class="st"> </span>gradient</a>
<a class="sourceLine" id="cb2108-4" data-line-number="4">    param<span class="op">$</span>nu     =<span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>param<span class="op">$</span>nu <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2) <span class="op">*</span><span class="st"> </span>gradient<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2108-5" data-line-number="5">    rho.hat      =<span class="st"> </span>param<span class="op">$</span>rho <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta1<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2108-6" data-line-number="6">    nu.hat       =<span class="st"> </span>param<span class="op">$</span>nu <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>beta2<span class="op">^</span>t)</a>
<a class="sourceLine" id="cb2108-7" data-line-number="7">    phi          =<span class="st"> </span>eta <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>(nu.hat) <span class="op">+</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb2108-8" data-line-number="8">    param<span class="op">$</span>weight =<span class="st"> </span>param<span class="op">$</span>weight <span class="op">-</span><span class="st"> </span>phi <span class="op">*</span><span class="st"> </span>rho.hat</a>
<a class="sourceLine" id="cb2108-9" data-line-number="9">    param</a>
<a class="sourceLine" id="cb2108-10" data-line-number="10">}</a></code></pre></div>
<div class="sourceCode" id="cb2109"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2109-1" data-line-number="1">optimize.update &lt;-<span class="cf">function</span>(rtype, params, grad,<span class="dt">eta=</span><span class="fl">0.001</span>, t) {</a>
<a class="sourceLine" id="cb2109-2" data-line-number="2">  <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;lstm&quot;</span>) {</a>
<a class="sourceLine" id="cb2109-3" data-line-number="3">    Wf=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wf, grad<span class="op">$</span>dF, eta,t); bf=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bf, grad<span class="op">$</span>dbf, eta,t)</a>
<a class="sourceLine" id="cb2109-4" data-line-number="4">    Wi=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wi, grad<span class="op">$</span>dI, eta,t); bi=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bi, grad<span class="op">$</span>dbi, eta,t)</a>
<a class="sourceLine" id="cb2109-5" data-line-number="5">    Wg=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wg, grad<span class="op">$</span>dG, eta,t); bg=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bg, grad<span class="op">$</span>dbg, eta,t)</a>
<a class="sourceLine" id="cb2109-6" data-line-number="6">    Wo=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wo, grad<span class="op">$</span>dO, eta,t); bo=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bo, grad<span class="op">$</span>dbo, eta,t)</a>
<a class="sourceLine" id="cb2109-7" data-line-number="7">    params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wf&quot;</span> =<span class="st"> </span>Wf, <span class="st">&quot;Wi&quot;</span> =<span class="st"> </span>Wi, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wo&quot;</span> =<span class="st"> </span>Wo,</a>
<a class="sourceLine" id="cb2109-8" data-line-number="8">                  <span class="st">&quot;bf&quot;</span> =<span class="st"> </span>bf, <span class="st">&quot;bi&quot;</span> =<span class="st"> </span>bi, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;bo&quot;</span> =<span class="st"> </span>bo)</a>
<a class="sourceLine" id="cb2109-9" data-line-number="9">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2109-10" data-line-number="10">  <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;gru&quot;</span>) {</a>
<a class="sourceLine" id="cb2109-11" data-line-number="11">    Wz=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wz, grad<span class="op">$</span>dZ, eta,t); bz=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bz, grad<span class="op">$</span>dbz, eta,t)</a>
<a class="sourceLine" id="cb2109-12" data-line-number="12">    Wg=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wg, grad<span class="op">$</span>dG, eta,t); bg=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>bg, grad<span class="op">$</span>dbg, eta,t)</a>
<a class="sourceLine" id="cb2109-13" data-line-number="13">    Wr=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>Wr, grad<span class="op">$</span>dR, eta,t); br=<span class="st"> </span><span class="kw">adam</span>(params<span class="op">$</span>br, grad<span class="op">$</span>dbr, eta,t) </a>
<a class="sourceLine" id="cb2109-14" data-line-number="14">    params =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wz&quot;</span> =<span class="st"> </span>Wz, <span class="st">&quot;Wg&quot;</span> =<span class="st"> </span>Wg, <span class="st">&quot;Wr&quot;</span> =<span class="st"> </span>Wr, </a>
<a class="sourceLine" id="cb2109-15" data-line-number="15">                  <span class="st">&quot;bz&quot;</span> =<span class="st"> </span>bz, <span class="st">&quot;bg&quot;</span> =<span class="st"> </span>bg, <span class="st">&quot;br&quot;</span> =<span class="st"> </span>br)</a>
<a class="sourceLine" id="cb2109-16" data-line-number="16">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2109-17" data-line-number="17">  <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;vanilla.rnn&quot;</span>) {</a>
<a class="sourceLine" id="cb2109-18" data-line-number="18">  <span class="co">############################################</span></a>
<a class="sourceLine" id="cb2109-19" data-line-number="19">  <span class="co"># follow the same code structure as LSTM/GRU</span></a>
<a class="sourceLine" id="cb2109-20" data-line-number="20">  <span class="co">############################################</span></a>
<a class="sourceLine" id="cb2109-21" data-line-number="21">  } </a>
<a class="sourceLine" id="cb2109-22" data-line-number="22">  params</a>
<a class="sourceLine" id="cb2109-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb2109-24" data-line-number="24">optimize.output &lt;-<span class="cf">function</span>(output, <span class="dt">eta=</span><span class="fl">0.001</span>, t) {</a>
<a class="sourceLine" id="cb2109-25" data-line-number="25">    V =<span class="st"> </span><span class="kw">adam</span>(output<span class="op">$</span>V, output<span class="op">$</span>dV, eta,t) </a>
<a class="sourceLine" id="cb2109-26" data-line-number="26">    by =<span class="st"> </span><span class="kw">adam</span>(output<span class="op">$</span>by, output<span class="op">$</span>dby, eta,t)</a>
<a class="sourceLine" id="cb2109-27" data-line-number="27">    <span class="kw">list</span>(<span class="st">&quot;V&quot;</span> =<span class="st"> </span>V, <span class="st">&quot;by&quot;</span> =<span class="st"> </span>by, <span class="st">&quot;dV&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;dby&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2109-28" data-line-number="28">}</a></code></pre></div>

<p>Additionally, we also require a few other helper functions such as <strong>ln(.)</strong> for calculating <strong>log</strong>. We also need <strong>softmax.loss(.)</strong>, and <strong>accuracy</strong> for calculating our <strong>loss</strong> and <strong>accuracy</strong> for metrics. Then the use of <strong>flush.str(.)</strong> for printing status of our training. See our implementation of <strong>CNN</strong> for these functions.</p>


<p>Next, we then write our example implementation of <strong>lstm.fit(.)</strong> function.</p>

<div class="sourceCode" id="cb2110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2110-1" data-line-number="1">lstm.fit &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, layers, eta, epoch) {</a>
<a class="sourceLine" id="cb2110-2" data-line-number="2">  dim.x            =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2110-3" data-line-number="3">  timesteps        =<span class="st"> </span>dim.x[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2110-4" data-line-number="4">  L                =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2110-5" data-line-number="5">  orig.X           =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2110-6" data-line-number="6">  models           =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2110-7" data-line-number="7">  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb2110-8" data-line-number="8">     layer      =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2110-9" data-line-number="9">     output     =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2110-10" data-line-number="10">     params     =<span class="st"> </span>layer<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2110-11" data-line-number="11">     grad       =<span class="st"> </span>layer<span class="op">$</span>gradients  <span class="co"># reinitialized gradient for every epoch</span></a>
<a class="sourceLine" id="cb2110-12" data-line-number="12">     Ht         =<span class="st"> </span>layer<span class="op">$</span>H[,,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb2110-13" data-line-number="13">     Ct         =<span class="st"> </span>layer<span class="op">$</span>C[,,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb2110-14" data-line-number="14">     model      =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2110-15" data-line-number="15">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X  } <span class="cf">else</span> { X =<span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>H }</a>
<a class="sourceLine" id="cb2110-16" data-line-number="16">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2110-17" data-line-number="17">       Xt          =<span class="st"> </span>X[,,t]</a>
<a class="sourceLine" id="cb2110-18" data-line-number="18">       model[[t]]  =<span class="st"> </span><span class="kw">forward.unit.LSTM</span>(Xt, Ht, Ct, params)</a>
<a class="sourceLine" id="cb2110-19" data-line-number="19">       Ht          =<span class="st"> </span>layer<span class="op">$</span>H[,,t] =<span class="st"> </span>model[[t]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2110-20" data-line-number="20">       Ct          =<span class="st"> </span>layer<span class="op">$</span>C[,,t] =<span class="st"> </span>model[[t]]<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2110-21" data-line-number="21">     }</a>
<a class="sourceLine" id="cb2110-22" data-line-number="22">     models[[l]]   =<span class="st"> </span>model; layers[[l]]   =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2110-23" data-line-number="23">  }</a>
<a class="sourceLine" id="cb2110-24" data-line-number="24">  <span class="co"># last model stores the output</span></a>
<a class="sourceLine" id="cb2110-25" data-line-number="25">  outcome          =<span class="st"> </span><span class="kw">get.output</span>(X, Y, model, output, <span class="dt">actfun =</span> <span class="st">&quot;rnn.softmax&quot;</span>)</a>
<a class="sourceLine" id="cb2110-26" data-line-number="26">  <span class="cf">for</span> (l <span class="cf">in</span> L<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2110-27" data-line-number="27">     layer         =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2110-28" data-line-number="28">     params        =<span class="st"> </span>layer<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2110-29" data-line-number="29">     grad          =<span class="st"> </span>layer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2110-30" data-line-number="30">     dnext         =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2110-31" data-line-number="31">     dH.next       =<span class="st"> </span>dC.next =<span class="st"> </span>dnext <span class="co"># copy structure</span></a>
<a class="sourceLine" id="cb2110-32" data-line-number="32">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { X =<span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>H } </a>
<a class="sourceLine" id="cb2110-33" data-line-number="33">     model         =<span class="st"> </span>models[[l]]</a>
<a class="sourceLine" id="cb2110-34" data-line-number="34">     <span class="cf">for</span> (t <span class="cf">in</span> timesteps<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2110-35" data-line-number="35">       Xt          =<span class="st"> </span>X[,,t]</a>
<a class="sourceLine" id="cb2110-36" data-line-number="36">       Ht          =<span class="st"> </span>model[[t]]<span class="op">$</span>Ht; Ct =<span class="st"> </span>model[[t]]<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2110-37" data-line-number="37">       dH.next     =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.next</a>
<a class="sourceLine" id="cb2110-38" data-line-number="38">       grad        =<span class="st"> </span><span class="kw">backward.unit.LSTM</span>(dH.next, dC.next, Xt, Y, Ht, Ct, </a>
<a class="sourceLine" id="cb2110-39" data-line-number="39">                                        model[[t]],  params, grad)</a>
<a class="sourceLine" id="cb2110-40" data-line-number="40">       dH.next     =<span class="st"> </span>grad<span class="op">$</span>dH</a>
<a class="sourceLine" id="cb2110-41" data-line-number="41">       dC.next     =<span class="st"> </span>grad<span class="op">$</span>dC</a>
<a class="sourceLine" id="cb2110-42" data-line-number="42">     }</a>
<a class="sourceLine" id="cb2110-43" data-line-number="43">     outcome<span class="op">$</span>dH   =<span class="st"> </span>grad<span class="op">$</span>dX <span class="co"># pass the next Dout to next previous layer</span></a>
<a class="sourceLine" id="cb2110-44" data-line-number="44">     output<span class="op">$</span>dV     =<span class="st"> </span>outcome<span class="op">$</span>dV; output<span class="op">$</span>dby =<span class="st"> </span>outcome<span class="op">$</span>dby </a>
<a class="sourceLine" id="cb2110-45" data-line-number="45">     layer<span class="op">$</span>params  =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;lstm&quot;</span>, params, grad, eta, epoch)</a>
<a class="sourceLine" id="cb2110-46" data-line-number="46">     layer<span class="op">$</span>output  =<span class="st"> </span><span class="kw">optimize.output</span>(output, eta, epoch) </a>
<a class="sourceLine" id="cb2110-47" data-line-number="47">     layers[[l]]   =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2110-48" data-line-number="48">   }</a>
<a class="sourceLine" id="cb2110-49" data-line-number="49">  <span class="kw">list</span>(<span class="st">&quot;loss&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>loss, <span class="st">&quot;accuracy&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>accuracy,<span class="st">&quot;layers&quot;</span>=<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb2110-50" data-line-number="50">}</a></code></pre></div>

<p>We also implement our <strong>gru.fit(.)</strong> for training <strong>GRU models</strong>.</p>

<div class="sourceCode" id="cb2111"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2111-1" data-line-number="1">gru.fit &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, layers, eta, epoch) {</a>
<a class="sourceLine" id="cb2111-2" data-line-number="2">  dim.x           =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2111-3" data-line-number="3">  timesteps       =<span class="st"> </span>dim.x[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2111-4" data-line-number="4">  L               =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2111-5" data-line-number="5">  orig.X          =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2111-6" data-line-number="6">  models          =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2111-7" data-line-number="7">  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb2111-8" data-line-number="8">     layer        =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2111-9" data-line-number="9">     output       =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2111-10" data-line-number="10">     params       =<span class="st"> </span>layer<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2111-11" data-line-number="11">     grad         =<span class="st"> </span>layer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2111-12" data-line-number="12">     Ht           =<span class="st"> </span>layer<span class="op">$</span>H[,,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb2111-13" data-line-number="13">     model        =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2111-14" data-line-number="14">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X  } <span class="cf">else</span> { X =<span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>H }</a>
<a class="sourceLine" id="cb2111-15" data-line-number="15">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2111-16" data-line-number="16">       Xt         =<span class="st"> </span>X[,,t]</a>
<a class="sourceLine" id="cb2111-17" data-line-number="17">       model[[t]] =<span class="st"> </span><span class="kw">forward.unit.GRU</span>(Xt, Ht,  params)</a>
<a class="sourceLine" id="cb2111-18" data-line-number="18">       Ht         =<span class="st"> </span>layer<span class="op">$</span>H[,,t] =<span class="st"> </span>model[[t]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2111-19" data-line-number="19">     }</a>
<a class="sourceLine" id="cb2111-20" data-line-number="20">     models[[l]]  =<span class="st"> </span>model; layers[[l]]   =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2111-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb2111-22" data-line-number="22">  <span class="co"># last model stores the output</span></a>
<a class="sourceLine" id="cb2111-23" data-line-number="23">  outcome         =<span class="st"> </span><span class="kw">get.output</span>(X, Y, model, output, <span class="dt">actfun =</span> <span class="st">&quot;rnn.softmax&quot;</span>)</a>
<a class="sourceLine" id="cb2111-24" data-line-number="24">  output<span class="op">$</span>dV       =<span class="st"> </span>outcome<span class="op">$</span>dV; output<span class="op">$</span>dby =<span class="st"> </span>outcome<span class="op">$</span>dby </a>
<a class="sourceLine" id="cb2111-25" data-line-number="25">  <span class="cf">for</span> (l <span class="cf">in</span> L<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2111-26" data-line-number="26">     layer         =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2111-27" data-line-number="27">     params        =<span class="st"> </span>layer<span class="op">$</span>params</a>
<a class="sourceLine" id="cb2111-28" data-line-number="28">     grad          =<span class="st"> </span>layer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2111-29" data-line-number="29">     dnext         =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2111-30" data-line-number="30">     dH.next       =<span class="st"> </span>dnext <span class="co"># copy structure</span></a>
<a class="sourceLine" id="cb2111-31" data-line-number="31">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { X =<span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>H } </a>
<a class="sourceLine" id="cb2111-32" data-line-number="32">     model         =<span class="st"> </span>models[[l]]</a>
<a class="sourceLine" id="cb2111-33" data-line-number="33">     <span class="cf">for</span> (t <span class="cf">in</span> timesteps<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2111-34" data-line-number="34">       Xt         =<span class="st"> </span>X[,,t];  Ht =<span class="st"> </span>model[[t]]<span class="op">$</span>Ht </a>
<a class="sourceLine" id="cb2111-35" data-line-number="35">       dH.next    =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.next  </a>
<a class="sourceLine" id="cb2111-36" data-line-number="36">       grad       =<span class="st"> </span><span class="kw">backward.unit.GRU</span>(dH.next,  Xt, Y, Ht, model[[t]], </a>
<a class="sourceLine" id="cb2111-37" data-line-number="37">                                   params, grad)</a>
<a class="sourceLine" id="cb2111-38" data-line-number="38">       dH.next    =<span class="st"> </span>grad<span class="op">$</span>dH</a>
<a class="sourceLine" id="cb2111-39" data-line-number="39">     }</a>
<a class="sourceLine" id="cb2111-40" data-line-number="40">     outcome<span class="op">$</span>dH   =<span class="st"> </span>grad<span class="op">$</span>dX <span class="co"># pass the next Dout to next previous layer</span></a>
<a class="sourceLine" id="cb2111-41" data-line-number="41">     layer<span class="op">$</span>params =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;gru&quot;</span>, params, grad, eta, epoch) </a>
<a class="sourceLine" id="cb2111-42" data-line-number="42">     layer<span class="op">$</span>output =<span class="st"> </span><span class="kw">optimize.output</span>(output, eta, epoch) </a>
<a class="sourceLine" id="cb2111-43" data-line-number="43">     layers[[l]]  =<span class="st"> </span>layer</a>
<a class="sourceLine" id="cb2111-44" data-line-number="44">  }</a>
<a class="sourceLine" id="cb2111-45" data-line-number="45">  <span class="kw">list</span>(<span class="st">&quot;loss&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>loss,<span class="st">&quot;accuracy&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>accuracy,<span class="st">&quot;layers&quot;</span>=<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb2111-46" data-line-number="46">}</a></code></pre></div>

<p>For the output, including calculation of the loss, and accuracy, including gradients for the output weight and bias, we implement a separate function called <strong>get.output(.)</strong>:</p>

<div class="sourceCode" id="cb2112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2112-1" data-line-number="1">get.output &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, model, output, actfunc) {</a>
<a class="sourceLine" id="cb2112-2" data-line-number="2">  actfunc       =<span class="st"> </span><span class="kw">get</span>(actfunc)</a>
<a class="sourceLine" id="cb2112-3" data-line-number="3">  dim.y         =<span class="st"> </span><span class="kw">dim</span>(Y)</a>
<a class="sourceLine" id="cb2112-4" data-line-number="4">  dim.x         =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2112-5" data-line-number="5">  timesteps     =<span class="st"> </span>dim.x[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2112-6" data-line-number="6">  V             =<span class="st"> </span>output<span class="op">$</span>V<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2112-7" data-line-number="7">  by            =<span class="st"> </span>output<span class="op">$</span>by<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2112-8" data-line-number="8">  Y.hat         =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, dim.y)</a>
<a class="sourceLine" id="cb2112-9" data-line-number="9">  <span class="cf">if</span> (<span class="kw">length</span>(dim.y) <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { Y.hat =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dim.y, timesteps)) }</a>
<a class="sourceLine" id="cb2112-10" data-line-number="10">  loss          =<span class="st"> </span>accurate =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, timesteps)</a>
<a class="sourceLine" id="cb2112-11" data-line-number="11">  dV            =<span class="st"> </span>dby =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2112-12" data-line-number="12">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2112-13" data-line-number="13">    <span class="cf">if</span> (<span class="kw">length</span>(dim.y) <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { target =<span class="st"> </span>Y } <span class="cf">else</span> { target =<span class="st"> </span>Y[,,t] }</a>
<a class="sourceLine" id="cb2112-14" data-line-number="14">    Y.hat[,,t]  =<span class="st"> </span><span class="kw">actfunc</span>(<span class="kw">sweep</span>(model[[t]]<span class="op">$</span>Ht <span class="op">%*%</span><span class="st"> </span>V, <span class="dv">2</span>, by, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2112-15" data-line-number="15">    loss[t]     =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">softmax.loss</span>(target, Y.hat[,,t]))</a>
<a class="sourceLine" id="cb2112-16" data-line-number="16">    accurate[t] =<span class="st"> </span><span class="kw">accuracy</span>(target, Y.hat[,,t])</a>
<a class="sourceLine" id="cb2112-17" data-line-number="17">    L           =<span class="st"> </span>(Y.hat[,,t] <span class="op">-</span><span class="st"> </span>target)</a>
<a class="sourceLine" id="cb2112-18" data-line-number="18">    dH          =<span class="st"> </span>L <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V)</a>
<a class="sourceLine" id="cb2112-19" data-line-number="19">    dV          =<span class="st"> </span>dV  <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(model[[t]]<span class="op">$</span>Ht) <span class="op">%*%</span><span class="st"> </span>L</a>
<a class="sourceLine" id="cb2112-20" data-line-number="20">    dby         =<span class="st"> </span>dby <span class="op">+</span><span class="st"> </span><span class="kw">apply</span>(L, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb2112-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb2112-22" data-line-number="22">  <span class="kw">list</span>(<span class="st">&quot;Y.hat&quot;</span> =<span class="st"> </span>Y.hat,  <span class="st">&quot;dH&quot;</span> =<span class="st"> </span>dH, <span class="st">&quot;dV&quot;</span> =<span class="st"> </span>dV, <span class="st">&quot;dby&quot;</span> =<span class="st"> </span>dby,</a>
<a class="sourceLine" id="cb2112-23" data-line-number="23">       <span class="st">&quot;loss&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(loss), <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(accurate))</a>
<a class="sourceLine" id="cb2112-24" data-line-number="24">}</a></code></pre></div>

<p>Finally, our example implementation of <strong>RNN</strong> supports <strong>LSTM</strong> and <strong>GRU</strong>. The <strong>vanilla RNN</strong> is left for readers to try to implement.</p>

<div class="sourceCode" id="cb2113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2113-1" data-line-number="1">my.RNN &lt;-<span class="st"> </span><span class="cf">function</span>(rtype, X, Y, layers, <span class="dt">epoch=</span><span class="dv">100</span>, <span class="dt">eta=</span><span class="fl">0.001</span>) {</a>
<a class="sourceLine" id="cb2113-2" data-line-number="2">  <span class="kw">options</span>(<span class="dt">digits =</span> <span class="dv">16</span>)  <span class="co"># 16 digits precision </span></a>
<a class="sourceLine" id="cb2113-3" data-line-number="3">  cost      =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, epoch)</a>
<a class="sourceLine" id="cb2113-4" data-line-number="4">  accurate  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, epoch)</a>
<a class="sourceLine" id="cb2113-5" data-line-number="5">  my.time   =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2113-6" data-line-number="6">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>epoch) {</a>
<a class="sourceLine" id="cb2113-7" data-line-number="7">     <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;lstm&quot;</span>) {</a>
<a class="sourceLine" id="cb2113-8" data-line-number="8">        model       =<span class="st"> </span><span class="kw">lstm.fit</span>(X, Y, layers, eta, t) </a>
<a class="sourceLine" id="cb2113-9" data-line-number="9">        layers      =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2113-10" data-line-number="10">        cost[t]     =<span class="st"> </span>model<span class="op">$</span>loss</a>
<a class="sourceLine" id="cb2113-11" data-line-number="11">        accurate[t] =<span class="st"> </span>model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2113-12" data-line-number="12">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2113-13" data-line-number="13">     <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;gru&quot;</span>) {</a>
<a class="sourceLine" id="cb2113-14" data-line-number="14">        model       =<span class="st"> </span><span class="kw">gru.fit</span>(X, Y, layers, eta, t) </a>
<a class="sourceLine" id="cb2113-15" data-line-number="15">        layers      =<span class="st"> </span>model<span class="op">$</span>layers</a>
<a class="sourceLine" id="cb2113-16" data-line-number="16">        cost[t]     =<span class="st"> </span>model<span class="op">$</span>loss</a>
<a class="sourceLine" id="cb2113-17" data-line-number="17">        accurate[t] =<span class="st"> </span>model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2113-18" data-line-number="18">     } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2113-19" data-line-number="19">     <span class="cf">if</span> (rtype <span class="op">==</span><span class="st"> &quot;vanilla.rnn&quot;</span>) {</a>
<a class="sourceLine" id="cb2113-20" data-line-number="20">        <span class="co">############################################</span></a>
<a class="sourceLine" id="cb2113-21" data-line-number="21">        <span class="co"># follow the same code structure as LSTM/GRU</span></a>
<a class="sourceLine" id="cb2113-22" data-line-number="22">        <span class="co">############################################</span></a>
<a class="sourceLine" id="cb2113-23" data-line-number="23">     }</a>
<a class="sourceLine" id="cb2113-24" data-line-number="24">     <span class="cf">if</span> (t <span class="op">%%</span><span class="st"> </span>(epoch <span class="op">*</span><span class="st"> </span><span class="fl">0.10</span>) <span class="op">==</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>t <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2113-25" data-line-number="25">       new.time =<span class="st"> </span><span class="kw">Sys.time</span>()</a>
<a class="sourceLine" id="cb2113-26" data-line-number="26">       lag.time =<span class="st"> </span><span class="kw">difftime</span>(new.time, my.time, <span class="dt">units=</span><span class="st">&quot;secs&quot;</span>) </a>
<a class="sourceLine" id="cb2113-27" data-line-number="27">       my.time  =<span class="st"> </span>new.time</a>
<a class="sourceLine" id="cb2113-28" data-line-number="28">       <span class="kw">flush.str</span>( </a>
<a class="sourceLine" id="cb2113-29" data-line-number="29">          <span class="st">&quot;epoch %d - loss: %2.3f, accuracy %2.3f lag time (sec): %5.3f&quot;</span>,</a>
<a class="sourceLine" id="cb2113-30" data-line-number="30">           t, cost[t], accurate[t], lag.time)</a>
<a class="sourceLine" id="cb2113-31" data-line-number="31">     }</a>
<a class="sourceLine" id="cb2113-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb2113-33" data-line-number="33">  <span class="kw">list</span>(<span class="st">&quot;cost&quot;</span>   =<span class="st"> </span>cost,   <span class="st">&quot;accuracy&quot;</span>  =<span class="st"> </span>accurate, <span class="st">&quot;layers&quot;</span> =<span class="st"> </span>layers)</a>
<a class="sourceLine" id="cb2113-34" data-line-number="34">}</a></code></pre></div>

<p>To apply our <strong>RNN</strong>, we need to construct a data structure that keeps track of our parameters and gradients (which also helps to maintain simple metadata and cache for our layers). To illustrate, we implement a network structure for <strong>LSTM</strong>.</p>

<div class="sourceCode" id="cb2114"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2114-1" data-line-number="1">deep.lstm.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, ...) {</a>
<a class="sourceLine" id="cb2114-2" data-line-number="2">  <span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2114-3" data-line-number="3">  rinit     =<span class="st"> </span>rnn.initialize</a>
<a class="sourceLine" id="cb2114-4" data-line-number="4">  layers    =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb2114-5" data-line-number="5">  di        =<span class="st"> </span><span class="kw">dim</span>(X); n =<span class="st"> </span>di[<span class="dv">1</span>]; p  =<span class="st"> </span>di[<span class="dv">2</span>]; t  =<span class="st"> </span>di[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2114-6" data-line-number="6">  di        =<span class="st"> </span><span class="kw">dim</span>(Y); o =<span class="st"> </span>di[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2114-7" data-line-number="7">  structure =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2114-8" data-line-number="8">  l =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2114-9" data-line-number="9">  coef =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2114-10" data-line-number="10">  <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb2114-11" data-line-number="11">    l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2114-12" data-line-number="12">    <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>size))             { layer<span class="op">$</span>size =<span class="st"> </span><span class="dv">30</span> }</a>
<a class="sourceLine" id="cb2114-13" data-line-number="13">    <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>bidirectional))    { layer<span class="op">$</span>bidirectional =<span class="st"> </span><span class="ot">FALSE</span> }</a>
<a class="sourceLine" id="cb2114-14" data-line-number="14">    H         =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(n, h, t))</a>
<a class="sourceLine" id="cb2114-15" data-line-number="15">    C         =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(n, h, t))</a>
<a class="sourceLine" id="cb2114-16" data-line-number="16">    V         =<span class="st"> </span>Wf =<span class="st"> </span>Wi =<span class="st"> </span>Wg =<span class="st"> </span>Wo =<span class="st"> </span>coef <span class="co"># copy the same structure</span></a>
<a class="sourceLine" id="cb2114-17" data-line-number="17">    V<span class="op">$</span>weight  =<span class="st"> </span><span class="kw">rinit</span>(h, o, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2114-18" data-line-number="18">    Wf<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2114-19" data-line-number="19">    Wi<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2114-20" data-line-number="20">    Wg<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2114-21" data-line-number="21">    Wo<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)   </a>
<a class="sourceLine" id="cb2114-22" data-line-number="22">    by        =<span class="st"> </span>bf =<span class="st"> </span>bi =<span class="st"> </span>bg =<span class="st"> </span>bo =<span class="st"> </span>coef <span class="co"># copy the same structure</span></a>
<a class="sourceLine" id="cb2114-23" data-line-number="23">    bf<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2114-24" data-line-number="24">    bi<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2114-25" data-line-number="25">    bg<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2114-26" data-line-number="26">    bo<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2114-27" data-line-number="27">    by<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, o, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2114-28" data-line-number="28">    p        =<span class="st"> </span>h <span class="co"># for next stack</span></a>
<a class="sourceLine" id="cb2114-29" data-line-number="29">    output   =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;V&quot;</span>   =<span class="st"> </span>V,  <span class="st">&quot;by&quot;</span>  =<span class="st"> </span>by, <span class="st">&quot;dV&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dby&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2114-30" data-line-number="30">    parms    =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wf&quot;</span>  =<span class="st"> </span>Wf, <span class="st">&quot;Wi&quot;</span>  =<span class="st"> </span>Wi, <span class="st">&quot;Wg&quot;</span>  =<span class="st"> </span>Wg, <span class="st">&quot;Wo&quot;</span>  =<span class="st"> </span>Wo,</a>
<a class="sourceLine" id="cb2114-31" data-line-number="31">                     <span class="st">&quot;bf&quot;</span>  =<span class="st"> </span>bf, <span class="st">&quot;bi&quot;</span>  =<span class="st"> </span>bi, <span class="st">&quot;bg&quot;</span>  =<span class="st"> </span>bg, <span class="st">&quot;bo&quot;</span>  =<span class="st"> </span>bo)</a>
<a class="sourceLine" id="cb2114-32" data-line-number="32">    grad     =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dC&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,</a>
<a class="sourceLine" id="cb2114-33" data-line-number="33">                     <span class="st">&quot;dF&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dI&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dO&quot;</span>  =<span class="st"> </span><span class="dv">0</span>, </a>
<a class="sourceLine" id="cb2114-34" data-line-number="34">                     <span class="st">&quot;dbf&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbi&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbo&quot;</span> =<span class="st"> </span><span class="dv">0</span> )</a>
<a class="sourceLine" id="cb2114-35" data-line-number="35">    struct   =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H, <span class="st">&quot;C&quot;</span> =<span class="st"> </span>C, <span class="st">&quot;params&quot;</span> =<span class="st"> </span>parms, <span class="st">&quot;gradients&quot;</span> =<span class="st"> </span>grad)</a>
<a class="sourceLine" id="cb2114-36" data-line-number="36">    <span class="cf">if</span> (layer<span class="op">$</span>bidirectional <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2114-37" data-line-number="37">        structure[[l]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;output&quot;</span> =<span class="st"> </span>output, <span class="st">&quot;forward&quot;</span> =<span class="st"> </span>struct, </a>
<a class="sourceLine" id="cb2114-38" data-line-number="38">                               <span class="st">&quot;backward&quot;</span> =<span class="st"> </span>struct )</a>
<a class="sourceLine" id="cb2114-39" data-line-number="39">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2114-40" data-line-number="40">        structure[[l]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H,  <span class="st">&quot;C&quot;</span> =<span class="st"> </span>C, <span class="st">&quot;output&quot;</span> =<span class="st"> </span>output, </a>
<a class="sourceLine" id="cb2114-41" data-line-number="41">                            <span class="st">&quot;params&quot;</span> =<span class="st"> </span>parms, <span class="st">&quot;gradients&quot;</span> =<span class="st"> </span>grad)</a>
<a class="sourceLine" id="cb2114-42" data-line-number="42">    }</a>
<a class="sourceLine" id="cb2114-43" data-line-number="43">  }</a>
<a class="sourceLine" id="cb2114-44" data-line-number="44">  structure</a>
<a class="sourceLine" id="cb2114-45" data-line-number="45">}</a></code></pre></div>

<p>And we also implement another network structure for <strong>GRU</strong>.</p>

<div class="sourceCode" id="cb2115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2115-1" data-line-number="1">deep.gru.layers &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, ...) {</a>
<a class="sourceLine" id="cb2115-2" data-line-number="2">  <span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2115-3" data-line-number="3">  rinit     =<span class="st"> </span>rnn.initialize</a>
<a class="sourceLine" id="cb2115-4" data-line-number="4">  layers    =<span class="st"> </span><span class="kw">list</span>(...)</a>
<a class="sourceLine" id="cb2115-5" data-line-number="5">  di        =<span class="st"> </span><span class="kw">dim</span>(X); n =<span class="st"> </span>di[<span class="dv">1</span>]; p  =<span class="st"> </span>di[<span class="dv">2</span>]; t  =<span class="st"> </span>di[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2115-6" data-line-number="6">  di        =<span class="st"> </span><span class="kw">dim</span>(Y); o =<span class="st"> </span>di[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2115-7" data-line-number="7">  structure =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2115-8" data-line-number="8">  l =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2115-9" data-line-number="9">  coef =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;weight&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;rho&quot;</span> =<span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;nu&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2115-10" data-line-number="10">  <span class="cf">for</span> (layer <span class="cf">in</span> layers) {</a>
<a class="sourceLine" id="cb2115-11" data-line-number="11">    l =<span class="st"> </span>l <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2115-12" data-line-number="12">    <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>size))             { layer<span class="op">$</span>size =<span class="st"> </span><span class="dv">30</span> }</a>
<a class="sourceLine" id="cb2115-13" data-line-number="13">    <span class="cf">if</span> (<span class="kw">is.null</span>(layer<span class="op">$</span>bidirectional))    { layer<span class="op">$</span>bidirectional =<span class="st"> </span><span class="ot">FALSE</span> }</a>
<a class="sourceLine" id="cb2115-14" data-line-number="14">    H         =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(n, h, t))</a>
<a class="sourceLine" id="cb2115-15" data-line-number="15">    V         =<span class="st"> </span>Wz =<span class="st"> </span>Wg =<span class="st"> </span>Wr =<span class="st"> </span>coef <span class="co"># copy the same structure</span></a>
<a class="sourceLine" id="cb2115-16" data-line-number="16">    V<span class="op">$</span>weight  =<span class="st"> </span><span class="kw">rinit</span>(h, o, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2115-17" data-line-number="17">    Wz<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2115-18" data-line-number="18">    Wg<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2115-19" data-line-number="19">    Wr<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(p <span class="op">+</span><span class="st"> </span>h, h, <span class="dt">state=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2115-20" data-line-number="20">    by        =<span class="st"> </span>bz =<span class="st"> </span>bg =<span class="st"> </span>br =<span class="st"> </span>coef <span class="co"># copy the same structure</span></a>
<a class="sourceLine" id="cb2115-21" data-line-number="21">    bz<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2115-22" data-line-number="22">    bg<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2115-23" data-line-number="23">    br<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, h, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2115-24" data-line-number="24">    by<span class="op">$</span>weight =<span class="st"> </span><span class="kw">rinit</span>(<span class="dv">1</span>, o, <span class="dt">state=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb2115-25" data-line-number="25">    p         =<span class="st"> </span>h <span class="co"># for next stack</span></a>
<a class="sourceLine" id="cb2115-26" data-line-number="26">    output    =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;V&quot;</span>   =<span class="st"> </span>V,  <span class="st">&quot;by&quot;</span>  =<span class="st"> </span>by, <span class="st">&quot;dV&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dby&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2115-27" data-line-number="27">    parms     =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;Wz&quot;</span>  =<span class="st"> </span>Wz, <span class="st">&quot;Wg&quot;</span>  =<span class="st"> </span>Wg, <span class="st">&quot;Wr&quot;</span>  =<span class="st"> </span>Wr, </a>
<a class="sourceLine" id="cb2115-28" data-line-number="28">                     <span class="st">&quot;bz&quot;</span>  =<span class="st"> </span>bz, <span class="st">&quot;bg&quot;</span>  =<span class="st"> </span>bg, <span class="st">&quot;br&quot;</span>  =<span class="st"> </span>br )</a>
<a class="sourceLine" id="cb2115-29" data-line-number="29">    grad      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;dX&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dH&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  </a>
<a class="sourceLine" id="cb2115-30" data-line-number="30">                     <span class="st">&quot;dZ&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dG&quot;</span>  =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dR&quot;</span>  =<span class="st"> </span><span class="dv">0</span>, </a>
<a class="sourceLine" id="cb2115-31" data-line-number="31">                     <span class="st">&quot;dbz&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbg&quot;</span> =<span class="st"> </span><span class="dv">0</span>,  <span class="st">&quot;dbr&quot;</span> =<span class="st"> </span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb2115-32" data-line-number="32">    struct    =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H,  <span class="st">&quot;params&quot;</span> =<span class="st"> </span>parms, <span class="st">&quot;gradients&quot;</span> =<span class="st"> </span>grad)</a>
<a class="sourceLine" id="cb2115-33" data-line-number="33">    <span class="cf">if</span> (layer<span class="op">$</span>bidirectional <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb2115-34" data-line-number="34">        structure[[l]] =<span class="st"> </span><span class="kw">list</span>( <span class="st">&quot;output&quot;</span> =<span class="st"> </span>output, <span class="st">&quot;forward&quot;</span> =<span class="st"> </span>struct, </a>
<a class="sourceLine" id="cb2115-35" data-line-number="35">                               <span class="st">&quot;backward&quot;</span> =<span class="st"> </span>struct )</a>
<a class="sourceLine" id="cb2115-36" data-line-number="36">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2115-37" data-line-number="37">        structure[[l]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H,  <span class="st">&quot;output&quot;</span> =<span class="st"> </span>output, </a>
<a class="sourceLine" id="cb2115-38" data-line-number="38">                            <span class="st">&quot;params&quot;</span> =<span class="st"> </span>parms, <span class="st">&quot;gradients&quot;</span> =<span class="st"> </span>grad)</a>
<a class="sourceLine" id="cb2115-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb2115-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb2115-41" data-line-number="41">  structure</a>
<a class="sourceLine" id="cb2115-42" data-line-number="42">}</a></code></pre></div>

<p>An important aspect of <strong>training</strong> is weight initialization. In <strong>CNN</strong>, we identified a few
initialization methods such as <strong>Xavier initialization</strong> and <strong>He initialization</strong>. In <strong>RNN</strong>, we introduce <strong>Orthogonal initialization</strong>.</p>
<p>Here, we use an <strong>rnn.initialize(.)</strong> function which uses <strong>net.initialization(.)</strong> function from our <strong>CNN</strong> implementation. Also, we introduce the use of our <strong>orthogonal.initialization(.)</strong> function based on <strong>Orthogonal initialization</strong> method (Wei Hu <span class="citation">(<a href="bibliography.html#ref-ref1462w">2020</a>)</span>; Eugene Vorontsov <span class="citation">(<a href="bibliography.html#ref-ref1466e">2017</a>)</span>; Stephen Merity <span class="citation">(<a href="bibliography.html#ref-ref1470s">2015</a>)</span>). </p>



<div class="sourceCode" id="cb2116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2116-1" data-line-number="1">orthogonal.initialization &lt;-<span class="st"> </span><span class="cf">function</span>(size, row, col) {</a>
<a class="sourceLine" id="cb2116-2" data-line-number="2">  V =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(size, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>), <span class="dt">nrow=</span>row, <span class="dt">ncol=</span>col, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2116-3" data-line-number="3">  sv =<span class="st"> </span><span class="kw">svd</span>(V)</a>
<a class="sourceLine" id="cb2116-4" data-line-number="4">  di =<span class="st"> </span><span class="kw">dim</span>(sv<span class="op">$</span>u)</a>
<a class="sourceLine" id="cb2116-5" data-line-number="5">  <span class="cf">if</span> (<span class="kw">prod</span>(di) <span class="op">==</span><span class="st"> </span>size) { W =<span class="st"> </span>sv<span class="op">$</span>u } <span class="cf">else</span> { W =<span class="st"> </span>sv<span class="op">$</span>v }</a>
<a class="sourceLine" id="cb2116-6" data-line-number="6">  <span class="kw">array</span>(W, <span class="kw">c</span>(row, col)) <span class="co"># reshape</span></a>
<a class="sourceLine" id="cb2116-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2116-8" data-line-number="8">rnn.initialize &lt;-<span class="st"> </span><span class="cf">function</span>(ni, no, <span class="dt">state=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb2116-9" data-line-number="9">  <span class="cf">if</span> (state <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {  <span class="co"># variable is concatenated btw input and state</span></a>
<a class="sourceLine" id="cb2116-10" data-line-number="10">    p =<span class="st"> </span>ni <span class="op">-</span><span class="st"> </span>no;  h =<span class="st"> </span>no </a>
<a class="sourceLine" id="cb2116-11" data-line-number="11">    I =<span class="st"> </span><span class="kw">net.initialization</span>(p <span class="op">*</span><span class="st"> </span>h, p, h, <span class="dt">itype=</span><span class="st">&quot;glorot&quot;</span>, <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2116-12" data-line-number="12">    I =<span class="st"> </span><span class="kw">array</span>(I, <span class="kw">c</span>(p, h))</a>
<a class="sourceLine" id="cb2116-13" data-line-number="13">    S =<span class="st"> </span><span class="kw">orthogonal.initialization</span>(h <span class="op">*</span><span class="st"> </span>h, h, h)</a>
<a class="sourceLine" id="cb2116-14" data-line-number="14">    W =<span class="st"> </span><span class="kw">rbind</span>(I, S)</a>
<a class="sourceLine" id="cb2116-15" data-line-number="15">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb2116-16" data-line-number="16">    W =<span class="st"> </span><span class="kw">net.initialization</span>(ni <span class="op">*</span><span class="st"> </span>no, ni, no, <span class="dt">itype=</span><span class="st">&quot;glorot&quot;</span>, <span class="dt">dist=</span><span class="st">&quot;uniform&quot;</span>)</a>
<a class="sourceLine" id="cb2116-17" data-line-number="17">    W =<span class="st"> </span><span class="kw">array</span>(W, <span class="kw">c</span>(ni, no))</a>
<a class="sourceLine" id="cb2116-18" data-line-number="18">  }</a>
<a class="sourceLine" id="cb2116-19" data-line-number="19">  W</a>
<a class="sourceLine" id="cb2116-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb2116-21" data-line-number="21"><span class="kw">rnn.initialize</span>(<span class="dv">5</span>, <span class="dv">3</span>, <span class="dt">state=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##             [,1]        [,2]         [,3]
## [1,] -0.31218743  0.91235315 -0.133482087
## [2,] -0.58272098 -1.02850246  1.037803497
## [3,] -0.51916059  0.69726644  0.494258841
## [4,] -0.33635573  0.36494832 -0.868146039
## [5,] -0.78570803 -0.61695400  0.045062776</code></pre>

<p>Let us see what the structure looks like given the below data points (<strong>X</strong>, <strong>Y</strong>). First, our input assumes an <strong>embedding</strong> using a sequence of three randomly generated numbers. Next, we concoct a single batch of 50 samples. Here, we assume a <strong>many-to-one</strong> system.</p>

<div class="sourceCode" id="cb2118"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2118-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2118-2" data-line-number="2">ts    =<span class="st"> </span><span class="dv">3</span>; n =<span class="st"> </span><span class="dv">50</span>; p =<span class="st"> </span><span class="dv">30</span>; h =<span class="st"> </span><span class="dv">20</span>; o =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2118-3" data-line-number="3">X     =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>ts), <span class="kw">c</span>(n, p, ts))</a>
<a class="sourceLine" id="cb2118-4" data-line-number="4">Y     =<span class="st"> </span><span class="kw">array</span>(<span class="kw">sample</span>(<span class="dv">0</span>, <span class="dt">size=</span>n <span class="op">*</span><span class="st"> </span>o , <span class="dt">replace=</span><span class="ot">TRUE</span>), <span class="kw">c</span>(n, o))</a>
<a class="sourceLine" id="cb2118-5" data-line-number="5">Y.idx =<span class="st"> </span><span class="kw">sample.int</span>(o, <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2118-6" data-line-number="6"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) { Y[i, Y.idx[i]] =<span class="st"> </span><span class="dv">1</span> }   <span class="co"># one-hot encoding</span></a></code></pre></div>

<p>For a <strong>many-to-many</strong> system, e.g. <strong>seq2seq</strong>, we may try something like so:</p>

<div class="sourceCode" id="cb2119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2119-1" data-line-number="1">Y     =<span class="st"> </span><span class="kw">array</span>(<span class="kw">sample</span>(<span class="dv">0</span>, <span class="dt">size=</span>n <span class="op">*</span><span class="st"> </span>o <span class="op">*</span><span class="st"> </span>ts, <span class="dt">replace=</span><span class="ot">TRUE</span>), <span class="kw">c</span>(n, o, ts))</a>
<a class="sourceLine" id="cb2119-2" data-line-number="2"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>ts) {</a>
<a class="sourceLine" id="cb2119-3" data-line-number="3">    Y.idx =<span class="st"> </span><span class="kw">sample.int</span>(o, <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2119-4" data-line-number="4">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) { Y[i, Y.idx[i], t] =<span class="st"> </span><span class="dv">1</span> }  <span class="co"># one-hot encoding</span></a>
<a class="sourceLine" id="cb2119-5" data-line-number="5">}</a></code></pre></div>

<p>Now, let us use <strong>deep.lstm.layers(.)</strong> to construct the structure.</p>

<div class="sourceCode" id="cb2120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2120-1" data-line-number="1">layers =<span class="st"> </span><span class="kw">deep.lstm.layers</span>(X, Y, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>))</a>
<a class="sourceLine" id="cb2120-2" data-line-number="2"><span class="kw">str</span>(layers[[<span class="dv">1</span>]]<span class="op">$</span>params<span class="op">$</span>Wf)</a></code></pre></div>
<pre><code>## List of 3
##  $ weight: num [1:50, 1:20] 0.286 -0.143 -0.0284 -0.1161 0.1045 ...
##  $ rho   : num 0
##  $ nu    : num 0</code></pre>

<p>Finally, we use our implementation to train an <strong>LSTM</strong> model.</p>

<div class="sourceCode" id="cb2122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2122-1" data-line-number="1">lstm.model =<span class="st"> </span><span class="kw">my.RNN</span>(<span class="dt">rtype =</span> <span class="st">&quot;lstm&quot;</span>, <span class="dt">X=</span>X, <span class="dt">Y=</span>Y, layers, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">eta=</span><span class="fl">0.01</span>)</a></code></pre></div>
<pre><code>## [1] &quot;epoch 1 - loss: 1.243, accuracy 0.287 lag time (sec): 0.141&quot;
## [1] &quot;epoch 20 - loss: 0.898, accuracy 0.553 lag time (sec): 0.140&quot;
## [1] &quot;epoch 40 - loss: 0.540, accuracy 0.760 lag time (sec): 0.153&quot;
## [1] &quot;epoch 60 - loss: 0.518, accuracy 0.800 lag time (sec): 0.151&quot;
## [1] &quot;epoch 80 - loss: 0.480, accuracy 0.807 lag time (sec): 0.152&quot;
## [1] &quot;epoch 100 - loss: 0.439, accuracy 0.813 lag time (sec): 0.180&quot;
## [1] &quot;epoch 120 - loss: 0.357, accuracy 0.853 lag time (sec): 0.149&quot;
## [1] &quot;epoch 140 - loss: 0.266, accuracy 0.913 lag time (sec): 0.145&quot;
## [1] &quot;epoch 160 - loss: 0.161, accuracy 0.940 lag time (sec): 0.151&quot;
## [1] &quot;epoch 180 - loss: 0.102, accuracy 0.967 lag time (sec): 0.175&quot;
## [1] &quot;epoch 200 - loss: 0.042, accuracy 0.993 lag time (sec): 0.162&quot;</code></pre>

<p>The <strong>Loss</strong> and <strong>Accuracy</strong> are plotted in Figure <a href="deeplearning2.html#fig:lstmplot">13.12</a>. We can see that our model can learn. We see the final loss resulting in 0.042 with a corresponding accuracy of 99.33%. The outcome is a good starting point.</p>

<div class="sourceCode" id="cb2124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2124-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(lstm.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2124-2" data-line-number="2">y =<span class="st"> </span>lstm.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2124-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2124-4" data-line-number="4">y2 =<span class="st"> </span>lstm.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2124-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2124-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2124-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;Deep Stacked LSTM Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2124-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2124-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2124-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lstmplot"></span>
<img src="DS_files/figure-html/lstmplot-1.png" alt="Deep Stacked LSTM Plot" width="70%" />
<p class="caption">
Figure 13.12: Deep Stacked LSTM Plot
</p>
</div>

<p>Next, let us use our implementation to also train a <strong>GRU</strong> model.</p>

<div class="sourceCode" id="cb2125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2125-1" data-line-number="1">layers =<span class="st"> </span><span class="kw">deep.gru.layers</span>(X, Y, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>), <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>))</a>
<a class="sourceLine" id="cb2125-2" data-line-number="2"><span class="kw">str</span>(layers[[<span class="dv">1</span>]]<span class="op">$</span>params<span class="op">$</span>Wz)</a></code></pre></div>
<pre><code>## List of 3
##  $ weight: num [1:50, 1:20] 0.286 -0.143 -0.0284 -0.1161 0.1045 ...
##  $ rho   : num 0
##  $ nu    : num 0</code></pre>
<div class="sourceCode" id="cb2127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2127-1" data-line-number="1">gru.model =<span class="st"> </span><span class="kw">my.RNN</span>(<span class="dt">rtype =</span> <span class="st">&quot;gru&quot;</span>, <span class="dt">X=</span>X, <span class="dt">Y=</span>Y, layers, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">eta=</span><span class="fl">0.01</span>)</a></code></pre></div>
<pre><code>## [1] &quot;epoch 1 - loss: 1.516, accuracy 0.280 lag time (sec): 0.094&quot;
## [1] &quot;epoch 20 - loss: 0.861, accuracy 0.607 lag time (sec): 0.111&quot;
## [1] &quot;epoch 40 - loss: 0.651, accuracy 0.747 lag time (sec): 0.121&quot;
## [1] &quot;epoch 60 - loss: 0.744, accuracy 0.693 lag time (sec): 0.117&quot;
## [1] &quot;epoch 80 - loss: 0.541, accuracy 0.740 lag time (sec): 0.120&quot;
## [1] &quot;epoch 100 - loss: 0.430, accuracy 0.813 lag time (sec): 0.119&quot;
## [1] &quot;epoch 120 - loss: 0.447, accuracy 0.847 lag time (sec): 0.124&quot;
## [1] &quot;epoch 140 - loss: 0.258, accuracy 0.907 lag time (sec): 0.116&quot;
## [1] &quot;epoch 160 - loss: 0.360, accuracy 0.840 lag time (sec): 0.144&quot;
## [1] &quot;epoch 180 - loss: 0.271, accuracy 0.907 lag time (sec): 0.175&quot;
## [1] &quot;epoch 200 - loss: 0.214, accuracy 0.920 lag time (sec): 0.119&quot;</code></pre>

<p>The <strong>Loss</strong> and <strong>Accuracy</strong> are plotted in Figure <a href="deeplearning2.html#fig:gruplot">13.13</a>. Similarly, our model can train. We see the final loss resulting in 0.214 with an accuracy around 92%.</p>

<div class="sourceCode" id="cb2129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2129-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(gru.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2129-2" data-line-number="2">y =<span class="st"> </span>gru.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2129-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2129-4" data-line-number="4">y2 =<span class="st"> </span>gru.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2129-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2129-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2129-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;Deep Stacked GRU Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2129-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2129-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2129-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gruplot"></span>
<img src="DS_files/figure-html/gruplot-1.png" alt="Deep Stacked GRU Plot" width="70%" />
<p class="caption">
Figure 13.13: Deep Stacked GRU Plot
</p>
</div>

</div>
<div id="deep-stacked-bidirectional-rnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.4</span> Deep Stacked Bidirectional RNN <a href="deeplearning2.html#deep-stacked-bidirectional-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Bidirectional RNN</strong> was introduced by Mike Schuster and Kuldip K. Paliwal in <span class="citation">(<a href="bibliography.html#ref-ref1232m">1997</a>)</span>. It describes the design of <strong>RNN</strong> in which we calculate our output based on <strong>forward feed</strong>and use <strong>backward feed</strong>. Note that <strong>backward feed</strong> does not refer to <strong>backward pass</strong> to mean <strong>backpropagation</strong>. Here, a <strong>Bidirectional RNN</strong> allows the use of another <strong>RNN</strong> layer that processes input in the opposite direction. We mean to reverse our input from time step <strong>t+1</strong> to <strong>t</strong> and from <strong>t</strong> to <strong>t-1</strong> and so on. We allow another RNN layer to perform the same <strong>forward feed</strong> against the reversed input. As we should see in Figure <a href="deeplearning2.html#fig:stackedbirnn">13.14</a>, the output <span class="math inline">\(\mathbf{\hat{Y}}\)</span> is dependent on the combined output of the <strong>forward feed</strong> layer and <strong>backward feed</strong> layer in their respective time steps. That allows the output to consider past and future information.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stackedbirnn"></span>
<img src="stackedbirnn.png" alt="Deep Stacked Bidirectional RNN" width="80%" />
<p class="caption">
Figure 13.14: Deep Stacked Bidirectional RNN
</p>
</div>
<p>In certain frameworks, the term <strong>merging</strong> or <strong>combining</strong> the <strong>feedforward output</strong> and <strong>feedbackward output</strong> allows the option to <strong>add</strong>, <strong>average</strong>, or <strong>multiply</strong> them in an element-wise manner. Otherwise, <strong>concatenation</strong> is a more common option.</p>
<p><strong>Bidirectional RNN</strong> applies to the three variants of <strong>RNN</strong>, namely <strong>Vanilla RNN</strong>, <strong>LSTM</strong>, and <strong>GRU</strong>; hereafter, we call this simply <strong>Bidirectional RNN</strong>.</p>
<p>To support a working <strong>Bidirectional RNN</strong>, we should be able to adjust our implementation of <strong>lstm.fit(.)</strong> and <strong>gru.fit(.)</strong>. Here, we use averaging for the <strong>merge</strong>.</p>

<div class="sourceCode" id="cb2130"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2130-1" data-line-number="1">lstm.fit &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, layers, eta, epoch) {</a>
<a class="sourceLine" id="cb2130-2" data-line-number="2">  d             =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2130-3" data-line-number="3">  timesteps     =<span class="st"> </span>d[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2130-4" data-line-number="4">  L             =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2130-5" data-line-number="5">  orig.X        =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2130-6" data-line-number="6">  fmodels       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2130-7" data-line-number="7">  bmodels       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2130-8" data-line-number="8">  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb2130-9" data-line-number="9">     layer      =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2130-10" data-line-number="10">     flayer     =<span class="st"> </span>layer<span class="op">$</span>forward</a>
<a class="sourceLine" id="cb2130-11" data-line-number="11">     blayer     =<span class="st"> </span>layer<span class="op">$</span>backward</a>
<a class="sourceLine" id="cb2130-12" data-line-number="12">     output     =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2130-13" data-line-number="13">     fg         =<span class="st"> </span>flayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2130-14" data-line-number="14">     bg         =<span class="st"> </span>blayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2130-15" data-line-number="15">     Ht         =<span class="st"> </span>flayer<span class="op">$</span>H[,,<span class="dv">1</span>]; Hs =<span class="st"> </span>blayer<span class="op">$</span>H[,,timesteps]</a>
<a class="sourceLine" id="cb2130-16" data-line-number="16">     Ct         =<span class="st"> </span>flayer<span class="op">$</span>C[,,<span class="dv">1</span>]; Cs =<span class="st"> </span>blayer<span class="op">$</span>C[,,timesteps]</a>
<a class="sourceLine" id="cb2130-17" data-line-number="17">     fmodel     =<span class="st"> </span>bmodel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2130-18" data-line-number="18">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb2130-19" data-line-number="19">             X =<span class="st"> </span>(layers[[l<span class="dv">-1</span>]]<span class="op">$</span>forward<span class="op">$</span>H <span class="op">+</span><span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>backward<span class="op">$</span>H )<span class="op">/</span><span class="dv">2</span> } </a>
<a class="sourceLine" id="cb2130-20" data-line-number="20">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2130-21" data-line-number="21">       Xt       =<span class="st"> </span>X[,,t]</a>
<a class="sourceLine" id="cb2130-22" data-line-number="22">       <span class="co"># forward feed</span></a>
<a class="sourceLine" id="cb2130-23" data-line-number="23">       fmodel[[t]]  =<span class="st"> </span><span class="kw">forward.unit.LSTM</span>(Xt, Ht, Ct, flayer<span class="op">$</span>params) </a>
<a class="sourceLine" id="cb2130-24" data-line-number="24">       Ht       =<span class="st"> </span>flayer<span class="op">$</span>H[,,t] =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2130-25" data-line-number="25">       Ct       =<span class="st"> </span>flayer<span class="op">$</span>C[,,t] =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2130-26" data-line-number="26">       s        =<span class="st"> </span>timesteps <span class="op">-</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="co"># reverses input</span></a>
<a class="sourceLine" id="cb2130-27" data-line-number="27">       Xs       =<span class="st"> </span>X[,,s]</a>
<a class="sourceLine" id="cb2130-28" data-line-number="28">       <span class="co"># backward feed</span></a>
<a class="sourceLine" id="cb2130-29" data-line-number="29">       bmodel[[s]]  =<span class="st"> </span><span class="kw">forward.unit.LSTM</span>(Xs, Hs, Cs, blayer<span class="op">$</span>params) </a>
<a class="sourceLine" id="cb2130-30" data-line-number="30">       Hs       =<span class="st"> </span>blayer<span class="op">$</span>H[,,s] =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2130-31" data-line-number="31">       Cs       =<span class="st"> </span>blayer<span class="op">$</span>C[,,s] =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2130-32" data-line-number="32">     }</a>
<a class="sourceLine" id="cb2130-33" data-line-number="33">     fmodels[[l]] =<span class="st"> </span>fmodel; layers[[l]]<span class="op">$</span>forward  =<span class="st"> </span>flayer</a>
<a class="sourceLine" id="cb2130-34" data-line-number="34">     bmodels[[l]] =<span class="st"> </span>bmodel; layers[[l]]<span class="op">$</span>backward =<span class="st"> </span>blayer</a>
<a class="sourceLine" id="cb2130-35" data-line-number="35">  }</a>
<a class="sourceLine" id="cb2130-36" data-line-number="36">  outcome       =<span class="st"> </span><span class="kw">get.output</span>(X, Y, fmodel, bmodel, output,</a>
<a class="sourceLine" id="cb2130-37" data-line-number="37">                             <span class="dt">actfun =</span> <span class="st">&quot;rnn.softmax&quot;</span>)</a>
<a class="sourceLine" id="cb2130-38" data-line-number="38">  output<span class="op">$</span>dV     =<span class="st"> </span>outcome<span class="op">$</span>dV; output<span class="op">$</span>dby =<span class="st"> </span>outcome<span class="op">$</span>dby </a>
<a class="sourceLine" id="cb2130-39" data-line-number="39">  <span class="cf">for</span> (l <span class="cf">in</span> L<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2130-40" data-line-number="40">     layer      =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2130-41" data-line-number="41">     flayer     =<span class="st"> </span>layer<span class="op">$</span>forward;     fmodel     =<span class="st"> </span>fmodels[[l]]</a>
<a class="sourceLine" id="cb2130-42" data-line-number="42">     blayer     =<span class="st"> </span>layer<span class="op">$</span>backward;    bmodel     =<span class="st"> </span>bmodels[[l]]</a>
<a class="sourceLine" id="cb2130-43" data-line-number="43">     fg         =<span class="st"> </span>flayer<span class="op">$</span>gradients;  bg         =<span class="st"> </span>blayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2130-44" data-line-number="44">     dnext      =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2130-45" data-line-number="45">     dH.fnext   =<span class="st"> </span>dH.bnext =<span class="st"> </span>dnext <span class="co"># copy structure</span></a>
<a class="sourceLine" id="cb2130-46" data-line-number="46">     dC.fnext   =<span class="st"> </span>dC.bnext =<span class="st"> </span>dnext <span class="co"># copy structure</span></a>
<a class="sourceLine" id="cb2130-47" data-line-number="47">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb2130-48" data-line-number="48">             X =<span class="st"> </span>(layers[[l<span class="dv">-1</span>]]<span class="op">$</span>forward<span class="op">$</span>H <span class="op">+</span><span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>backward<span class="op">$</span>H )<span class="op">/</span><span class="dv">2</span> } </a>
<a class="sourceLine" id="cb2130-49" data-line-number="49">     <span class="cf">for</span> (t <span class="cf">in</span> timesteps<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2130-50" data-line-number="50">       Xt       =<span class="st"> </span>X[,,t]; Ht =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ht; Ct =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ct  </a>
<a class="sourceLine" id="cb2130-51" data-line-number="51">       dH.fnext =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.fnext</a>
<a class="sourceLine" id="cb2130-52" data-line-number="52">       fg       =<span class="st"> </span><span class="kw">backward.unit.LSTM</span>(dH.fnext, dC.fnext, Xt, Y, Ht, Ct, </a>
<a class="sourceLine" id="cb2130-53" data-line-number="53">                               fmodel[[t]], flayer<span class="op">$</span>params, fg)</a>
<a class="sourceLine" id="cb2130-54" data-line-number="54">       s        =<span class="st"> </span>timesteps <span class="op">-</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2130-55" data-line-number="55">       Xs       =<span class="st"> </span>X[,,s]; Hs =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ht; Cs =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ct</a>
<a class="sourceLine" id="cb2130-56" data-line-number="56">       dH.bnext =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.bnext</a>
<a class="sourceLine" id="cb2130-57" data-line-number="57">       bg       =<span class="st"> </span><span class="kw">backward.unit.LSTM</span>(dH.bnext, dC.bnext, Xs, Y, Hs, Cs, </a>
<a class="sourceLine" id="cb2130-58" data-line-number="58">                               bmodel[[s]], blayer<span class="op">$</span>params, bg)</a>
<a class="sourceLine" id="cb2130-59" data-line-number="59">       dH.fnext =<span class="st"> </span>fg<span class="op">$</span>dH; dC.fnext =<span class="st"> </span>fg<span class="op">$</span>dC</a>
<a class="sourceLine" id="cb2130-60" data-line-number="60">       dH.bnext =<span class="st"> </span>bg<span class="op">$</span>dH; dC.bnext =<span class="st"> </span>bg<span class="op">$</span>dC</a>
<a class="sourceLine" id="cb2130-61" data-line-number="61">     }    </a>
<a class="sourceLine" id="cb2130-62" data-line-number="62">     <span class="co"># pass the next Dout to next previous layer</span></a>
<a class="sourceLine" id="cb2130-63" data-line-number="63">     outcome<span class="op">$</span>dH           =<span class="st"> </span>fg<span class="op">$</span>dX <span class="op">+</span><span class="st"> </span>bg<span class="op">$</span>dX </a>
<a class="sourceLine" id="cb2130-64" data-line-number="64">     flayer<span class="op">$</span>params        =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;lstm&quot;</span>, flayer<span class="op">$</span>params, </a>
<a class="sourceLine" id="cb2130-65" data-line-number="65">                                            fg, eta, epoch) </a>
<a class="sourceLine" id="cb2130-66" data-line-number="66">     blayer<span class="op">$</span>params        =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;lstm&quot;</span>, blayer<span class="op">$</span>params, </a>
<a class="sourceLine" id="cb2130-67" data-line-number="67">                                            bg, eta, epoch) </a>
<a class="sourceLine" id="cb2130-68" data-line-number="68">     layer<span class="op">$</span>output         =<span class="st"> </span><span class="kw">optimize.output</span>(output, eta, epoch) </a>
<a class="sourceLine" id="cb2130-69" data-line-number="69">     layers[[l]]<span class="op">$</span>output   =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2130-70" data-line-number="70">     layers[[l]]<span class="op">$</span>forward  =<span class="st"> </span>flayer</a>
<a class="sourceLine" id="cb2130-71" data-line-number="71">     layers[[l]]<span class="op">$</span>backward =<span class="st"> </span>blayer</a>
<a class="sourceLine" id="cb2130-72" data-line-number="72">  }</a>
<a class="sourceLine" id="cb2130-73" data-line-number="73">  <span class="kw">list</span>(<span class="st">&quot;loss&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>loss, <span class="st">&quot;accuracy&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>accuracy,<span class="st">&quot;layers&quot;</span>=<span class="st"> </span>layers)    </a>
<a class="sourceLine" id="cb2130-74" data-line-number="74">}</a></code></pre></div>

<p>Similarly, we modify <strong>gru.fit(.)</strong>:</p>

<div class="sourceCode" id="cb2131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2131-1" data-line-number="1">gru.fit &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, layers, eta, epoch) {</a>
<a class="sourceLine" id="cb2131-2" data-line-number="2">  d             =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2131-3" data-line-number="3">  timesteps     =<span class="st"> </span>d[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2131-4" data-line-number="4">  L             =<span class="st"> </span><span class="kw">length</span>(layers)</a>
<a class="sourceLine" id="cb2131-5" data-line-number="5">  orig.X        =<span class="st"> </span>X</a>
<a class="sourceLine" id="cb2131-6" data-line-number="6">  fmodels       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2131-7" data-line-number="7">  bmodels       =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2131-8" data-line-number="8">  <span class="cf">for</span> (l <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb2131-9" data-line-number="9">     layer      =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2131-10" data-line-number="10">     flayer     =<span class="st"> </span>layer<span class="op">$</span>forward</a>
<a class="sourceLine" id="cb2131-11" data-line-number="11">     blayer     =<span class="st"> </span>layer<span class="op">$</span>backward</a>
<a class="sourceLine" id="cb2131-12" data-line-number="12">     output     =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2131-13" data-line-number="13">     fg         =<span class="st"> </span>flayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2131-14" data-line-number="14">     bg         =<span class="st"> </span>blayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2131-15" data-line-number="15">     Ht         =<span class="st"> </span>flayer<span class="op">$</span>H[,,<span class="dv">1</span>]; Hs =<span class="st"> </span>blayer<span class="op">$</span>H[,,timesteps]</a>
<a class="sourceLine" id="cb2131-16" data-line-number="16">     fmodel     =<span class="st"> </span>bmodel =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb2131-17" data-line-number="17">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb2131-18" data-line-number="18">             X =<span class="st"> </span>(layers[[l<span class="dv">-1</span>]]<span class="op">$</span>forward<span class="op">$</span>H <span class="op">+</span><span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>backward<span class="op">$</span>H )<span class="op">/</span><span class="dv">2</span> } </a>
<a class="sourceLine" id="cb2131-19" data-line-number="19">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2131-20" data-line-number="20">       Xt       =<span class="st"> </span>X[,,t]</a>
<a class="sourceLine" id="cb2131-21" data-line-number="21">       fmodel[[t]]  =<span class="st"> </span><span class="kw">forward.unit.GRU</span>(Xt, Ht, flayer<span class="op">$</span>params) <span class="co"># forward feed</span></a>
<a class="sourceLine" id="cb2131-22" data-line-number="22">       Ht       =<span class="st"> </span>flayer<span class="op">$</span>H[,,t] =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2131-23" data-line-number="23">       s        =<span class="st"> </span>timesteps <span class="op">-</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="co"># reverses input</span></a>
<a class="sourceLine" id="cb2131-24" data-line-number="24">       Xs       =<span class="st"> </span>X[,,s]</a>
<a class="sourceLine" id="cb2131-25" data-line-number="25">       bmodel[[s]]  =<span class="st"> </span><span class="kw">forward.unit.GRU</span>(Xs, Hs, blayer<span class="op">$</span>params) <span class="co"># backward feed</span></a>
<a class="sourceLine" id="cb2131-26" data-line-number="26">       Hs       =<span class="st"> </span>blayer<span class="op">$</span>H[,,s] =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2131-27" data-line-number="27">     }</a>
<a class="sourceLine" id="cb2131-28" data-line-number="28">     fmodels[[l]] =<span class="st"> </span>fmodel; layers[[l]]<span class="op">$</span>forward  =<span class="st"> </span>flayer</a>
<a class="sourceLine" id="cb2131-29" data-line-number="29">     bmodels[[l]] =<span class="st"> </span>bmodel; layers[[l]]<span class="op">$</span>backward =<span class="st"> </span>blayer</a>
<a class="sourceLine" id="cb2131-30" data-line-number="30">      </a>
<a class="sourceLine" id="cb2131-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb2131-32" data-line-number="32">  outcome       =<span class="st"> </span><span class="kw">get.output</span>(X, Y, fmodel, bmodel, output, </a>
<a class="sourceLine" id="cb2131-33" data-line-number="33">                             <span class="dt">actfun =</span> <span class="st">&quot;rnn.softmax&quot;</span>)</a>
<a class="sourceLine" id="cb2131-34" data-line-number="34">  output<span class="op">$</span>dV     =<span class="st"> </span>outcome<span class="op">$</span>dV; output<span class="op">$</span>dby =<span class="st"> </span>outcome<span class="op">$</span>dby </a>
<a class="sourceLine" id="cb2131-35" data-line-number="35">  <span class="cf">for</span> (l <span class="cf">in</span> L<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2131-36" data-line-number="36">     layer      =<span class="st"> </span>layers[[l]]</a>
<a class="sourceLine" id="cb2131-37" data-line-number="37">     flayer     =<span class="st"> </span>layer<span class="op">$</span>forward;     fmodel     =<span class="st"> </span>fmodels[[l]]</a>
<a class="sourceLine" id="cb2131-38" data-line-number="38">     blayer     =<span class="st"> </span>layer<span class="op">$</span>backward;    bmodel     =<span class="st"> </span>bmodels[[l]]</a>
<a class="sourceLine" id="cb2131-39" data-line-number="39">     fg         =<span class="st"> </span>flayer<span class="op">$</span>gradients;  bg         =<span class="st"> </span>blayer<span class="op">$</span>gradients</a>
<a class="sourceLine" id="cb2131-40" data-line-number="40">     dnext      =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>h, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2131-41" data-line-number="41">     dH.fnext   =<span class="st"> </span>dH.bnext =<span class="st"> </span>dnext <span class="co"># copy structure</span></a>
<a class="sourceLine" id="cb2131-42" data-line-number="42">     <span class="cf">if</span> (l <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { X =<span class="st"> </span>orig.X } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb2131-43" data-line-number="43">             X =<span class="st"> </span>(layers[[l<span class="dv">-1</span>]]<span class="op">$</span>forward<span class="op">$</span>H <span class="op">+</span><span class="st"> </span>layers[[l<span class="dv">-1</span>]]<span class="op">$</span>backward<span class="op">$</span>H )<span class="op">/</span><span class="dv">2</span> } </a>
<a class="sourceLine" id="cb2131-44" data-line-number="44">     <span class="cf">for</span> (t <span class="cf">in</span> timesteps<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb2131-45" data-line-number="45">       Xt       =<span class="st"> </span>X[,,t]; Ht =<span class="st"> </span>fmodel[[t]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2131-46" data-line-number="46">       dH.fnext =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.fnext</a>
<a class="sourceLine" id="cb2131-47" data-line-number="47">       fg       =<span class="st"> </span><span class="kw">backward.unit.GRU</span>(dH.fnext, Xt, Y, Ht,</a>
<a class="sourceLine" id="cb2131-48" data-line-number="48">                               fmodel[[t]], flayer<span class="op">$</span>params, fg)</a>
<a class="sourceLine" id="cb2131-49" data-line-number="49">       s        =<span class="st"> </span>timesteps <span class="op">-</span><span class="st"> </span>t <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2131-50" data-line-number="50">       Xs       =<span class="st"> </span>X[,,s]; Hs =<span class="st"> </span>bmodel[[s]]<span class="op">$</span>Ht</a>
<a class="sourceLine" id="cb2131-51" data-line-number="51">       dH.bnext =<span class="st"> </span>outcome<span class="op">$</span>dH <span class="op">+</span><span class="st"> </span>dH.bnext</a>
<a class="sourceLine" id="cb2131-52" data-line-number="52">       bg       =<span class="st"> </span><span class="kw">backward.unit.GRU</span>(dH.bnext, Xs, Y, Hs, </a>
<a class="sourceLine" id="cb2131-53" data-line-number="53">                               bmodel[[s]], blayer<span class="op">$</span>params, bg)</a>
<a class="sourceLine" id="cb2131-54" data-line-number="54">       dH.fnext =<span class="st"> </span>fg<span class="op">$</span>dH</a>
<a class="sourceLine" id="cb2131-55" data-line-number="55">       dH.bnext =<span class="st"> </span>bg<span class="op">$</span>dH</a>
<a class="sourceLine" id="cb2131-56" data-line-number="56">     }    </a>
<a class="sourceLine" id="cb2131-57" data-line-number="57">     <span class="co"># pass the next Dout to next previous layer</span></a>
<a class="sourceLine" id="cb2131-58" data-line-number="58">     outcome<span class="op">$</span>dH           =<span class="st"> </span>fg<span class="op">$</span>dX <span class="op">+</span><span class="st"> </span>bg<span class="op">$</span>dX </a>
<a class="sourceLine" id="cb2131-59" data-line-number="59">     flayer<span class="op">$</span>params        =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;gru&quot;</span>, flayer<span class="op">$</span>params, </a>
<a class="sourceLine" id="cb2131-60" data-line-number="60">                                            fg, eta, epoch) </a>
<a class="sourceLine" id="cb2131-61" data-line-number="61">     blayer<span class="op">$</span>params        =<span class="st"> </span><span class="kw">optimize.update</span>(<span class="dt">rtype=</span><span class="st">&quot;gru&quot;</span>, blayer<span class="op">$</span>params, </a>
<a class="sourceLine" id="cb2131-62" data-line-number="62">                                            bg, eta, epoch) </a>
<a class="sourceLine" id="cb2131-63" data-line-number="63">     layer<span class="op">$</span>output         =<span class="st"> </span><span class="kw">optimize.output</span>(output, eta, epoch) </a>
<a class="sourceLine" id="cb2131-64" data-line-number="64">     layers[[l]]<span class="op">$</span>output   =<span class="st"> </span>layer<span class="op">$</span>output</a>
<a class="sourceLine" id="cb2131-65" data-line-number="65">     layers[[l]]<span class="op">$</span>forward  =<span class="st"> </span>flayer</a>
<a class="sourceLine" id="cb2131-66" data-line-number="66">     layers[[l]]<span class="op">$</span>backward =<span class="st"> </span>blayer</a>
<a class="sourceLine" id="cb2131-67" data-line-number="67">  }</a>
<a class="sourceLine" id="cb2131-68" data-line-number="68">  <span class="kw">list</span>(<span class="st">&quot;loss&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>loss, <span class="st">&quot;accuracy&quot;</span>=<span class="st"> </span>outcome<span class="op">$</span>accuracy,<span class="st">&quot;layers&quot;</span>=<span class="st"> </span>layers)    </a>
<a class="sourceLine" id="cb2131-69" data-line-number="69">}</a></code></pre></div>

<p>Finally, we modify the <strong>get.output(.)</strong> function to handle the <strong>merge</strong> of outputs from both <strong>forward</strong> and <strong>backward</strong> feed. Here, we use averaging for the merge.</p>

<div class="sourceCode" id="cb2132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2132-1" data-line-number="1">get.output &lt;-<span class="st"> </span><span class="cf">function</span>(X, Y, fmodel, bmodel, output, actfunc) {</a>
<a class="sourceLine" id="cb2132-2" data-line-number="2">  actfunc       =<span class="st"> </span><span class="kw">get</span>(actfunc)</a>
<a class="sourceLine" id="cb2132-3" data-line-number="3">  dim.y         =<span class="st"> </span><span class="kw">dim</span>(Y)</a>
<a class="sourceLine" id="cb2132-4" data-line-number="4">  dim.x         =<span class="st"> </span><span class="kw">dim</span>(X)</a>
<a class="sourceLine" id="cb2132-5" data-line-number="5">  timesteps     =<span class="st"> </span>dim.x[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2132-6" data-line-number="6">  V             =<span class="st"> </span>output<span class="op">$</span>V<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2132-7" data-line-number="7">  by            =<span class="st"> </span>output<span class="op">$</span>by<span class="op">$</span>weight</a>
<a class="sourceLine" id="cb2132-8" data-line-number="8">  Y.hat         =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, dim.y)</a>
<a class="sourceLine" id="cb2132-9" data-line-number="9">  <span class="cf">if</span> (<span class="kw">length</span>(dim.y) <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { Y.hat =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dim.y, timesteps)) }</a>
<a class="sourceLine" id="cb2132-10" data-line-number="10">  loss          =<span class="st"> </span>accurate =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, timesteps)</a>
<a class="sourceLine" id="cb2132-11" data-line-number="11">  dV            =<span class="st"> </span>dby =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb2132-12" data-line-number="12">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>timesteps) {</a>
<a class="sourceLine" id="cb2132-13" data-line-number="13">    <span class="cf">if</span> (<span class="kw">length</span>(dim.y) <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { target =<span class="st"> </span>Y } <span class="cf">else</span> { target =<span class="st"> </span>Y[,,t] }</a>
<a class="sourceLine" id="cb2132-14" data-line-number="14">    Ht          =<span class="st"> </span>(fmodel[[t]]<span class="op">$</span>Ht <span class="op">+</span><span class="st"> </span>bmodel[[t]]<span class="op">$</span>Ht)<span class="op">/</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb2132-15" data-line-number="15">    Y.hat[,,t]  =<span class="st"> </span><span class="kw">actfunc</span>(<span class="kw">sweep</span>(Ht <span class="op">%*%</span><span class="st"> </span>V, <span class="dv">2</span>, by, <span class="st">&#39;+&#39;</span>))</a>
<a class="sourceLine" id="cb2132-16" data-line-number="16">    loss[t]     =<span class="st"> </span><span class="kw">mean</span>(<span class="kw">softmax.loss</span>(target, Y.hat[,,t]))</a>
<a class="sourceLine" id="cb2132-17" data-line-number="17">    accurate[t] =<span class="st"> </span><span class="kw">accuracy</span>(target, Y.hat[,,t])</a>
<a class="sourceLine" id="cb2132-18" data-line-number="18">    L           =<span class="st"> </span>(Y.hat[,,t] <span class="op">-</span><span class="st"> </span>target)</a>
<a class="sourceLine" id="cb2132-19" data-line-number="19">    dH          =<span class="st"> </span>L <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(V)</a>
<a class="sourceLine" id="cb2132-20" data-line-number="20">    dV          =<span class="st"> </span>dV  <span class="op">+</span><span class="st"> </span><span class="kw">t</span>(Ht) <span class="op">%*%</span><span class="st"> </span>L</a>
<a class="sourceLine" id="cb2132-21" data-line-number="21">    dby         =<span class="st"> </span>dby <span class="op">+</span><span class="st"> </span><span class="kw">apply</span>(L, <span class="dv">2</span>, sum)</a>
<a class="sourceLine" id="cb2132-22" data-line-number="22">  }</a>
<a class="sourceLine" id="cb2132-23" data-line-number="23">  <span class="kw">list</span>(<span class="st">&quot;Y.hat&quot;</span> =<span class="st"> </span>Y.hat,  <span class="st">&quot;dH&quot;</span> =<span class="st"> </span>dH, <span class="st">&quot;dV&quot;</span> =<span class="st"> </span>dV, <span class="st">&quot;dby&quot;</span> =<span class="st"> </span>dby,</a>
<a class="sourceLine" id="cb2132-24" data-line-number="24">       <span class="st">&quot;loss&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(loss), <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(accurate))</a>
<a class="sourceLine" id="cb2132-25" data-line-number="25">}</a></code></pre></div>

<p>Lastly, let us create an alias for our <strong>Bidirectional RNN</strong> like so (no modification is required for the <strong>my.RNN(.)</strong> function).</p>

<div class="sourceCode" id="cb2133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2133-1" data-line-number="1">my.BiRNN =<span class="st"> </span>my.RNN</a></code></pre></div>

<p>Let us see how the structure looks like given the following data points (<strong>X</strong>, <strong>Y</strong>):</p>

<div class="sourceCode" id="cb2134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2134-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2134-2" data-line-number="2">t     =<span class="st"> </span><span class="dv">5</span>; n =<span class="st"> </span><span class="dv">50</span>; p =<span class="st"> </span><span class="dv">30</span>; h =<span class="st"> </span><span class="dv">20</span>; o =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb2134-3" data-line-number="3">X     =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(n <span class="op">*</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>t), <span class="kw">c</span>(n, p, t))</a>
<a class="sourceLine" id="cb2134-4" data-line-number="4">Y     =<span class="st"> </span><span class="kw">array</span>(<span class="kw">sample</span>(<span class="dv">0</span>, <span class="dt">size=</span>n <span class="op">*</span><span class="st"> </span>o, <span class="dt">replace=</span><span class="ot">TRUE</span>), <span class="kw">c</span>(n, o))</a>
<a class="sourceLine" id="cb2134-5" data-line-number="5">Y.idx =<span class="st"> </span><span class="kw">sample.int</span>(o, <span class="dt">size=</span>n, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2134-6" data-line-number="6"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) { Y[i, Y.idx[i]] =<span class="st"> </span><span class="dv">1</span> }  <span class="co"># one-hot encoding</span></a></code></pre></div>

<p>Let us now use our implementation to train a <strong>Bidirectional LSTM</strong> model.</p>

<div class="sourceCode" id="cb2135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2135-1" data-line-number="1">layers =<span class="st"> </span><span class="kw">deep.lstm.layers</span>(X, Y, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>, <span class="dt">bidirectional=</span><span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb2135-2" data-line-number="2">                                <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>, <span class="dt">bidirectional=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb2135-3" data-line-number="3">bilstm.model =<span class="st"> </span><span class="kw">my.BiRNN</span>(<span class="dt">rtype =</span> <span class="st">&quot;lstm&quot;</span>, <span class="dt">X=</span>X, <span class="dt">Y=</span>Y, layers, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">eta=</span><span class="fl">0.01</span>)</a></code></pre></div>
<pre><code>## [1] &quot;epoch 1 - loss: 1.162, accuracy 0.380 lag time (sec): 0.135&quot;
## [1] &quot;epoch 20 - loss: 0.928, accuracy 0.612 lag time (sec): 0.418&quot;
## [1] &quot;epoch 40 - loss: 0.879, accuracy 0.620 lag time (sec): 0.477&quot;
## [1] &quot;epoch 60 - loss: 0.640, accuracy 0.764 lag time (sec): 0.479&quot;
## [1] &quot;epoch 80 - loss: 0.327, accuracy 0.876 lag time (sec): 0.621&quot;
## [1] &quot;epoch 100 - loss: 0.225, accuracy 0.912 lag time (sec): 0.442&quot;
## [1] &quot;epoch 120 - loss: 0.126, accuracy 0.960 lag time (sec): 0.470&quot;
## [1] &quot;epoch 140 - loss: 0.049, accuracy 0.980 lag time (sec): 0.565&quot;
## [1] &quot;epoch 160 - loss: 0.060, accuracy 0.980 lag time (sec): 0.439&quot;
## [1] &quot;epoch 180 - loss: 0.033, accuracy 0.980 lag time (sec): 0.469&quot;
## [1] &quot;epoch 200 - loss: 0.012, accuracy 1.000 lag time (sec): 0.539&quot;</code></pre>

<p>The <strong>Loss</strong> and <strong>Accuracy</strong> are plotted in Figure <a href="deeplearning2.html#fig:bilstmplot">13.15</a>. We can see that our model can train. We see the final loss resulting in 0.012 with a corresponding accuracy around 100%.</p>

<div class="sourceCode" id="cb2137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2137-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(bilstm.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2137-2" data-line-number="2">y =<span class="st"> </span>bilstm.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2137-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2137-4" data-line-number="4">y2 =<span class="st"> </span>bilstm.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2137-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2137-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2137-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;Deep Stacked Bidirectional LSTM Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2137-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2137-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2137-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bilstmplot"></span>
<img src="DS_files/figure-html/bilstmplot-1.png" alt="Deep Stacked Bidirectional LSTM Plot" width="70%" />
<p class="caption">
Figure 13.15: Deep Stacked Bidirectional LSTM Plot
</p>
</div>

<p>Let us also use our implementation to train a <strong>Bidirectional GRU</strong> model.</p>

<div class="sourceCode" id="cb2138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2138-1" data-line-number="1">layers =<span class="st"> </span><span class="kw">deep.gru.layers</span>(X, Y, <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>, <span class="dt">bidirectional=</span><span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb2138-2" data-line-number="2">                               <span class="kw">list</span>(<span class="dt">size=</span><span class="dv">20</span>, <span class="dt">bidirectional=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb2138-3" data-line-number="3">bigru.model =<span class="st"> </span><span class="kw">my.BiRNN</span>(<span class="dt">rtype =</span> <span class="st">&quot;gru&quot;</span>, <span class="dt">X=</span>X, <span class="dt">Y=</span>Y, layers, <span class="dt">epoch=</span><span class="dv">200</span>, <span class="dt">eta=</span><span class="fl">0.01</span>)</a></code></pre></div>
<pre><code>## [1] &quot;epoch 1 - loss: 1.406, accuracy 0.380 lag time (sec): 0.090&quot;
## [1] &quot;epoch 20 - loss: 0.973, accuracy 0.424 lag time (sec): 0.328&quot;
## [1] &quot;epoch 40 - loss: 0.972, accuracy 0.472 lag time (sec): 0.404&quot;
## [1] &quot;epoch 60 - loss: 0.788, accuracy 0.616 lag time (sec): 0.341&quot;
## [1] &quot;epoch 80 - loss: 0.664, accuracy 0.704 lag time (sec): 0.348&quot;
## [1] &quot;epoch 100 - loss: 0.387, accuracy 0.856 lag time (sec): 0.410&quot;
## [1] &quot;epoch 120 - loss: 0.431, accuracy 0.796 lag time (sec): 0.336&quot;
## [1] &quot;epoch 140 - loss: 0.253, accuracy 0.896 lag time (sec): 0.344&quot;
## [1] &quot;epoch 160 - loss: 0.222, accuracy 0.928 lag time (sec): 0.354&quot;
## [1] &quot;epoch 180 - loss: 0.139, accuracy 0.956 lag time (sec): 0.423&quot;
## [1] &quot;epoch 200 - loss: 0.098, accuracy 0.964 lag time (sec): 0.344&quot;</code></pre>

<p>The <strong>Loss</strong> and <strong>Accuracy</strong> are plotted in Figure <a href="deeplearning2.html#fig:bigruplot">13.16</a>. Similarly, our model can train. We see the final loss resulting in 0.098 with an accuracy around 96.4%.
</p>
<div class="sourceCode" id="cb2140"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2140-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(bigru.model<span class="op">$</span>cost))</a>
<a class="sourceLine" id="cb2140-2" data-line-number="2">y =<span class="st"> </span>bigru.model<span class="op">$</span>cost</a>
<a class="sourceLine" id="cb2140-3" data-line-number="3">y1 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))<span class="op">/</span>(<span class="kw">max</span>(y) <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(y))</a>
<a class="sourceLine" id="cb2140-4" data-line-number="4">y2 =<span class="st"> </span>bigru.model<span class="op">$</span>accuracy</a>
<a class="sourceLine" id="cb2140-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,y1),   </a>
<a class="sourceLine" id="cb2140-6" data-line-number="6">      <span class="dt">xlab=</span><span class="st">&quot;Epoch&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Cross-Entropy Loss / Accuracy&quot;</span>,   </a>
<a class="sourceLine" id="cb2140-7" data-line-number="7">      <span class="dt">main=</span><span class="st">&quot;Deep Stacked Bidirectional GRU Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2140-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2140-9" data-line-number="9"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2140-10" data-line-number="10"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bigruplot"></span>
<img src="DS_files/figure-html/bigruplot-1.png" alt="Deep Stacked Bidirectional GRU Plot" width="70%" />
<p class="caption">
Figure 13.16: Deep Stacked Bidirectional GRU Plot
</p>
</div>

<p>It is obvious to see that our models perform very well. That is because we iterate through the same dataset. Thus, we may see signs of <strong>overfitting</strong> to the dataset.</p>
<p>While we may not be able to cover many other <strong>RNN</strong> discussions in this section, it helps to recall a few knobs we highlighted in the <strong>CNN</strong> section, which can improve the performance of <strong>RNN</strong>. We mention scheduling, blackout, batch norm, batch size, and many others besides the initialization type and optimization we already covered here.</p>
</div>
<div id="transformer-neural-network-tnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.5</span> Transformer Neural Network (TNN)  <a href="deeplearning2.html#transformer-neural-network-tnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>Attention</strong> mechanism was first introduced in the context of <strong>machine translation</strong> in a paper published by Dzmitry Bahdanau et al. <span class="citation">(<a href="bibliography.html#ref-ref1317d">2015</a>)</span>, followed by another paper published by Minh-Thang Luong et al. <span class="citation">(<a href="bibliography.html#ref-ref1328m">2015</a>)</span> for a variant of the mechanism. Two years later, presented in a paper called <strong>Attention is all you need</strong> <span class="citation">(Vaswani A. et al. <a href="bibliography.html#ref-ref1339a">2017</a>)</span>, <strong>Attention</strong> kick-started the emergence of <strong>Transformer</strong> architectures - also called <strong>Transformer Neural Network</strong> architectures.</p>
<div id="attention" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.1</span> Attention <a href="deeplearning2.html#attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To start this topic, let us first build our fundamental knowledge around <strong>Encoder-Decoder</strong> architecture, using formulas and referencing diagrams from Bahdanau and Luong (with some modifications for presentation purposes only). As we pointed out in the <strong>RNN</strong> sections, an <strong>Encoder-Decoder</strong> model constitutes two separate components: an <strong>Encoder</strong> and a <strong>Decoder</strong>. They correspond to the <strong>Many-to-One</strong> and <strong>One-to-Many</strong> models. The <strong>Encoder</strong> component can use an RNN architecture (using <strong>LSTM</strong> or <strong>GRU</strong> as an alternative and in a bidirectional fashion) with a series of <strong>X</strong> as input and a single <strong>Encoder Vector</strong> as output. The <strong>Decoder</strong> component takes the <strong>Encoder Vector</strong> for its initial state and produces a series of <strong>Y</strong> as output. See Figure <a href="deeplearning2.html#fig:encoderdecoder">13.17</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:encoderdecoder"></span>
<img src="encoderdecoder.png" alt="Encoder-Decoder" width="100%" />
<p class="caption">
Figure 13.17: Encoder-Decoder
</p>
</div>
<p>Mathematically, our interest is to produce the final <strong>hidden state</strong>, e.g. <span class="math inline">\(\mathbf{h_t}\)</span>, from the encoder using the following function:</p>
<p><span class="math display">\[\begin{align}
h_t = f(h_{t-1}, x_t)
\ \ \ \ \ \ \ \rightarrow\ \ \ \ \text{(recall RNN function)}
\end{align}\]</span></p>
<p>The final <strong>hidden state</strong>, e.g., <span class="math inline">\(\mathbf{h_t}\)</span>, is then handed over to serve as the initial state <span class="math inline">\(\mathbf{s_0}\)</span> for the decoder. We use the following function for each decoded output:</p>
<p><span class="math display">\[\begin{align}
y_t = g(s_{t}, y_{t-1}, c)\ \ \ \ \ \ \ \rightarrow\ \ \ \ \text{(also recall RNN function)}
\end{align}\]</span></p>
<p>where <strong>c</strong> is the <strong>encoder vector</strong>, also called <strong>context vector</strong>. The symbol <span class="math inline">\(\mathbf{c}\)</span> should not be confused with the <strong>c</strong> state of <strong>LSTM</strong> in case we decide to use that variant of <strong>RNN</strong>.</p>
<p>One known drawback of such a traditional model lies in the fact that the <strong>Encoder</strong> produces only one single vector representing the entire sentence - sort of a single dot in <span class="math inline">\(\mathbf{d_k}\)</span> dimensional vector space <span class="citation">(Sankar A. <a href="bibliography.html#ref-ref1487a">2019</a>)</span>. One vector representing a context (or multiple contexts) of the entire sentence may not seem fitting, especially with a very long sentence. So we need a different approach. Take the following simple sentence as an example:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{\text{I travel the world in search for the fountain of youth.}}
\end{align*}\]</span></p>
<p>So instead of feeding the decoder with a single vector representing the entire sentence, a proposed idea is to focus on individual target words, e.g., <strong>travel</strong>, and figure out which surrounding words are more relevant to the target word <strong>travel</strong>. For example, the words <strong>I</strong>, <strong>the</strong>, and <strong>world</strong> seem more strongly relevant than the rest. We can then form a vector that may signify some representation about <strong>I travel the world</strong> for the target word <strong>travel</strong>. Similarly, we can also form a vector representation of the target word <strong>fountain</strong> by looking for the closest relevant words such as <strong>in</strong>, <strong>search</strong>, <strong>for</strong>, <strong>the</strong>, and <strong>youth</strong>, then feed the decoder with another vector that signifies another kind of representation, this time, about <strong>search for the fountain of youth</strong>. Thus, herein lies the concept of the <strong>Attention</strong> mechanism, which is used to draw surrounding words that can <strong>pay more attention</strong> to <strong>target words</strong>. We call these surrounding words <strong>contextual words</strong>, which form <strong>contexts</strong> of <strong>target words</strong>.</p>
<p>To illustrate the use of the <strong>Attention</strong> mechanism, we have to modify our diagram in Figure <a href="deeplearning2.html#fig:encoderdecoder">13.17</a>. The modification is shown in Figure <a href="deeplearning2.html#fig:attentionencoder">13.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attentionencoder"></span>
<img src="attentionencoder.png" alt="Encoder-Decoder with Attention" width="100%" />
<p class="caption">
Figure 13.18: Encoder-Decoder with Attention
</p>
</div>
<p>Notice that an <strong>Attention unit</strong> is added to the diagram. The unit forms the core function of the <strong>Attention mechanism</strong>. Here, all the hidden states {<span class="math inline">\(\mathbf{h_1}, \mathbf{h_2}, ..., \mathbf{h_t}\)</span>} are captured by the <strong>Attention unit</strong> which are then used to eventually generate distinct <strong>contextualized</strong> vectors, each denoted by <span class="math inline">\(\mathbf{c_i}\)</span> representing the context of a selected target word (a timestep in the <strong>Encoder</strong>). The individual vectors are then fed into their corresponding <strong>RNN</strong> timesteps in the <strong>Decoder</strong>.</p>
<p>Two popular variants of <strong>Attention</strong> mechanisms were developed. Bahdanau D. et al. <span class="citation">(<a href="bibliography.html#ref-ref1317d">2015</a>)</span> demonstrated the use of the first <strong>Attention</strong> mechanism; hereafter, we call this the <strong>Bahdanau Attention</strong>, regarded as <strong>Additive Attention</strong>. Luong M. et al. <span class="citation">(<a href="bibliography.html#ref-ref1328m">2015</a>)</span> demonstrated the second variant; hereafter, we call this the <strong>Luong Attention</strong>, regarded as <strong>Multiplicative Attention</strong>. We can see the difference between the two mechanisms in Figure <a href="deeplearning2.html#fig:attentionencoder">13.18</a>. In the <strong>Luong Attention</strong>, the individual <strong>context vector</strong> <span class="math inline">\(\mathbf{c_t}\)</span> is also added with the output. We show this in the diagram as dashed arrows which are not found in <strong>Bahdanau Attention</strong>. Additionally, the reason becomes clear why we call the former <strong>Additive Attention</strong> and the latter as <strong>Multiplicative Attention</strong>. Notice that the <strong>Attention</strong> unit takes two inputs, namely, the hidden state <span class="math inline">\(\mathbf{h_t}\)</span> from the encoder and the hidden state <span class="math inline">\(\mathbf{s_t}\)</span> from the decoder. The alignment model uses the two states to calculate the <strong>attention scores</strong>. How we calculate the <strong>attention scores</strong> depends on the two variants.</p>
<p>Let us use Figure <a href="deeplearning2.html#fig:attention">13.19</a> to illustrate the first variant, <strong>Bahdanau attention</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attention"></span>
<img src="attention.png" alt="Attention Unit" width="100%" />
<p class="caption">
Figure 13.19: Attention Unit
</p>
</div>
<p>Mathematically, the use of a single context vector <strong>c</strong> is replaced by individual distinct context vectors <span class="math inline">\(\mathbf{c_t}\)</span> like so:</p>
<p><span class="math display">\[\begin{align}
from: y_t = g(s_{t}, y_{t-1}, c)
\ \ \ \ \ \ \
to: y_t = g(s_{t}, y_{t-1}, c_t)
\end{align}\]</span></p>
<p>It is worth noting that the <strong>g(.)</strong> function yields the probability of every output word conditioned on both previous outputs and the corresponding context vector as written below:</p>
<p><span class="math display">\[\begin{align}
g(s_{t}, y_{t-1}, c_t) = P(y_t |y_1, .... y_{t-1}, c_t)
\end{align}\]</span></p>
<p>whereas originally, we are conditioned on a single context vector:</p>
<p><span class="math display">\[\begin{align}
g(s_{t}, y_{t-1}, c) = P(y_t |y_1, .... y_{t-1}, c)
\end{align}\]</span></p>
<p>We know that the hidden states <span class="math inline">\(\mathbf{s_{t}}\)</span> are generated by <strong>RNN</strong> in the <strong>Decoder</strong> so that each <strong>RNN</strong> unit feeds from previous hidden states. However, on the other hand, it also feeds from corresponding context vectors <span class="math inline">\(\mathbf{c_i}\)</span> generated by the <strong>Attention unit</strong> using the following formula:</p>
<p><span class="math display">\[\begin{align}
c_i = \sum_{j=1}^t \alpha_{ij} h_j
\end{align}\]</span></p>
<p>Here, the symbol <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) represents the <strong>attention weight</strong> which is calculated using <strong>softmax</strong> operation:</p>
<p><span class="math display">\[\begin{align}
\alpha_{ij} = \frac{\mathbf{\text{exp}}(e_{ij})}{\sum_k^t \mathbf{\text{exp}}(e_{ik})}\ \ \ \ \ \ \
\rightarrow\ \ \ \ \text{(softmax)}
\ \ \ \ \ \ \text{where:}\ \ \ \ \ \ \
\sum_{j=1} \alpha_{j} = 1
\end{align}\]</span></p>
<p>Now, in <strong>Bahdanau Attention</strong>, the <strong>attention scores</strong> denoted by <span class="math inline">\(\mathbf{e_{ij}}\)</span> are calculated by an <strong>alignment function</strong>. It is called <strong>Additive Attention</strong> because both states are parameterized for training, then added together, then <strong>tanh</strong> is applied before being multiplied by another parameter.</p>
<p><span class="math display">\[\begin{align}
e_{ij} = a(s_{i-1}, h_j) = v_a^\text{T}\ \mathbf{\text{tanh}}\left(W_a\ s_{i-1} + U_a\ h_j\right)
\end{align}\]</span></p>
<p>In <strong>Luong Attention</strong>, we are given three choices of <strong>alignment function</strong> to calculate the <strong>attention scores</strong>:</p>
<p><span class="math display">\[\begin{align}
e_{ij} = score(h_t, \bar{h}_s) = 
\begin{cases}
h_t^T \bar{h}_s &amp; \text{dot}\\
h_t^T W_a \bar{h}_s &amp; \text{general}\\
v_a^T\ \mathbf{\text{tanh}}\left(W_a [ h_t; \bar{h}_s]\right) &amp; \text{concat}
\end{cases} \label{eqn:eqnnumber803}
\end{align}\]</span></p>
<p>Additionally, <strong>Luong Attention</strong> also proposes <strong>Local Attention</strong> in which only a subset of source input is considered for alignment rather than the entire source input. A good example of such implementation is in <strong>Skip-gram</strong>, which we cover as part of our <strong>Word Embedding</strong> discussion in a later section in which we discuss the idea of using a window that is controlled by window size and that slides across a series of input throughout the iteration. There are good ideas presented in <strong>Luong Attention</strong>, such as using <strong>monotonic alignment</strong>, which considers a few timesteps before and after the target word to construct the window. Alternatively, using <strong>predictive alignment</strong> based on <strong>Gaussian distribution</strong>. The idea of <strong>predictive alignment</strong> is to consider <strong>coverage</strong> of <strong>attentiveness</strong> so that we favor neighboring words in proximity and words that are strongly attentive (closer to the mean) though they may be remote. Consider the following sentence:</p>
<p><span class="math display">\[
\mathbf{\text{I gave browny, my cute little dog, treats.}}
\]</span></p>
<p>Notice that if our target word is <strong>gave</strong>, we can form <strong>contextual words</strong> using <strong>I</strong> and <strong>treats</strong>, both of which happen to be not in proximity.</p>
<p><span class="math display">\[
\mathbf{\text{I gave treats.}}
\]</span></p>
<p>We leave readers to investigate the below <strong>predictive alignment function</strong> with <strong>gaussian</strong> formula:</p>
<p><span class="math display">\[\begin{align}
a_t(s)= \mathbf{align}(h_t, \bar{h}_s)  \mathbf{exp}\left(-\frac{(s - p_t)^2}{2 \sigma^2}\right) 
\end{align}\]</span></p>
<p>We also leave readers to investigate <strong>soft</strong> and <strong>hard</strong> Attention mentioned in the Luong M. paper referring to using <strong>soft</strong> Attention for all patches in an image (see <strong>CNN</strong> for receptive fields); otherwise, using <strong>hard</strong> Attention one patch a time.</p>
<p>In the next section, we shall use <strong>dot product</strong> for our alignment function using <strong>Self-Attention</strong> to demonstrate <strong>Global Attention</strong>.</p>
</div>
<div id="self-attention-and-trainability" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.2</span> Self-Attention and Trainability <a href="deeplearning2.html#self-attention-and-trainability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We saw a small glimpse of how the <strong>Attention</strong> mechanism works, though that is not the complete picture. For example, we are yet to cover how the mechanism can be trained by introducing learnable parameters - an essential property of <strong>Attention</strong>.</p>
<p>In this section, let us briefly take a closer look at <strong>Attention unit</strong> under the hood. Afterwhich, we discuss the trainability of <strong>Attention units</strong>. Let us use Figure <a href="deeplearning2.html#fig:attention">13.19</a> to illustrate:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:selfattention"></span>
<img src="selfattention.png" alt="Attention Unit" width="90%" />
<p class="caption">
Figure 13.20: Attention Unit
</p>
</div>
<p>In the figure, we see a layered RNN stack producing a <strong>sequence</strong> of vectors denoted by <span class="math inline">\(\mathbf{\vec{h}^{(2)}}\)</span> from the second RNN layer. This sequence of vectors is fed through an <strong>Attention unit</strong>. It helps to point out that the diagram reflects a <strong>Self-attention</strong> unit. By <strong>self</strong>, each token in the series is paired with the other. The idea is to derive some contextual relationship amongst each other - although being strongly attentive to oneself becomes implicitly part of the computation.</p>
Now, in a paper titled <strong>Attention is all we need</strong>, published by Ashish Vaswani et al. <span class="citation">(<a href="bibliography.html#ref-ref1339a">2017</a>)</span>, the trainability of an <strong>Attention unit</strong> is made possible with the introduction of attention coefficients - learnable parameters (weights) - constructed in the form of three matrices which the paper refers to as <strong>Q matrix</strong>, <strong>K matrix</strong>, and <strong>V matrix</strong>. To illustrate, let us use Figure <a href="deeplearning2.html#fig:selfattention1">13.21</a>.
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:selfattention1"></span>
<img src="selfattention1.png" alt="Attention with Weights" width="100%" />
<p class="caption">
Figure 13.21: Attention with Weights
</p>
</div>
<p>The <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are instances manifested from the same embedding source that are given roles to play, namely <strong>query</strong>, <strong>keys</strong>, <strong>values</strong>. To give a better picture, let us continue to use our previous example sentence:</p>
<p><span class="math display">\[
\underbrace{\mathbf{\text{I }}}_{\mathbf{h1}}
\underbrace{\mathbf{\text{ travel }}}_{\mathbf{h2}}
\underbrace{\mathbf{\text{ the }}}_{\mathbf{h3}}
\underbrace{\mathbf{\text{ world }}}_{\mathbf{h4}}
\underbrace{\mathbf{\text{ in }}}_{\mathbf{h5}}
\underbrace{\mathbf{\text{ search }}}_{\mathbf{h6}}
\underbrace{\mathbf{\text{ for }}}_{\mathbf{h7}} 
\underbrace{\mathbf{\text{ the }}}_{\mathbf{h8}}
\underbrace{\mathbf{\text{ fountain }}}_{\mathbf{h9}}
\underbrace{\mathbf{\text{ of }}}_{\mathbf{h10}}
\underbrace{\mathbf{\text{ youth }}}_{\mathbf{h11}}
\]</span></p>
<p>Based on <strong>Retrieval Information</strong> theory, particularly in <strong>search engine</strong> systems, we supply a query keyword to the engine and expect the engine to respond with a list of relevant documents. In the context of <strong>Self-Attention</strong>, we supply a query keyword in search for <strong>contextual words</strong> in a list of words called <strong>keys</strong>. The idea is to know which of these keys will pay more <strong>attention</strong> to the query. For a better intuition, let us step through the operations.</p>
<p><strong>First</strong>, for illustration, let us cut the sentence above to only the subject line, <strong>I travel the world</strong>, giving us only about four tokens (or word embeddings). Assume for a moment that our embeddings have five dimensions (dk=5). For now, let us randomly assign some continuous numbers.</p>

<div class="sourceCode" id="cb2141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2141-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2141-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb2141-3" data-line-number="3">t    =<span class="st"> </span><span class="dv">4</span>  <span class="co"># number of tokens (embedding)</span></a>
<a class="sourceLine" id="cb2141-4" data-line-number="4">dk   =<span class="st"> </span><span class="dv">5</span>  <span class="co"># dimension per token</span></a>
<a class="sourceLine" id="cb2141-5" data-line-number="5">random &lt;-<span class="st"> </span><span class="cf">function</span>(dk) { <span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk, <span class="dt">min=</span><span class="fl">0.100</span>, <span class="dt">max=</span><span class="fl">0.900</span>), <span class="dv">3</span>)   }</a>
<a class="sourceLine" id="cb2141-6" data-line-number="6"><span class="co"># assume hidden states produced by RNN for each word</span></a>
<a class="sourceLine" id="cb2141-7" data-line-number="7">h1   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># I       </span></a>
<a class="sourceLine" id="cb2141-8" data-line-number="8">h2   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># travel   </span></a>
<a class="sourceLine" id="cb2141-9" data-line-number="9">h3   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># the</span></a>
<a class="sourceLine" id="cb2141-10" data-line-number="10">h4   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># world </span></a>
<a class="sourceLine" id="cb2141-11" data-line-number="11">(<span class="dt">h    =</span> <span class="kw">rbind</span>(h1, h2, h3, h4))</a></code></pre></div>
<pre><code>##     [,1]  [,2]  [,3]  [,4]  [,5]
## h1 0.155 0.754 0.854 0.316 0.235
## h2 0.127 0.243 0.613 0.118 0.107
## h3 0.414 0.751 0.401 0.405 0.312
## h4 0.451 0.466 0.533 0.633 0.190</code></pre>

<p><strong>Second</strong>, let us query the keys to obtain scores. Mathematically, we have the following expression, including the normalizer:</p>
<p><span class="math display">\[\begin{align}
\text{scaled scores} = \frac{(\text{query})\ (\text{keys}^\text{T})} {\sqrt{d_k}} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2143-1" data-line-number="1">query  =<span class="st"> </span>keys =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2143-2" data-line-number="2">scores =<span class="st"> </span>(query) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(keys)</a>
<a class="sourceLine" id="cb2143-3" data-line-number="3">scaled.scores =<span class="st"> </span><span class="kw">round</span>(scores <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(dk), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2143-4" data-line-number="4"><span class="kw">colnames</span>(scaled.scores) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;q&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2143-5" data-line-number="5"><span class="kw">rownames</span>(scaled.scores) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;s&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2143-6" data-line-number="6">scaled.scores</a></code></pre></div>
<pre><code>##       q1    q2    q3    q4
## s1 0.661 0.353 0.525 0.501
## s2 0.353 0.213 0.251 0.265
## s3 0.525 0.251 0.518 0.477
## s4 0.501 0.265 0.477 0.510</code></pre>

<p><strong>Third</strong>, we perform softmax for each query like so:</p>
<p><span class="math display">\[\begin{align}
\alpha_i = \frac{exp(s_i)} {\sum_j^t(exp(s_{i,j}))}
\end{align}\]</span></p>

<div class="sourceCode" id="cb2145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2145-1" data-line-number="1">attention.weights =<span class="st"> </span><span class="kw">softmax</span>(scaled.scores)</a>
<a class="sourceLine" id="cb2145-2" data-line-number="2"><span class="kw">colnames</span>(attention.weights) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;q&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2145-3" data-line-number="3"><span class="kw">rownames</span>(attention.weights) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;w&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2145-4" data-line-number="4">attention.weights</a></code></pre></div>
<pre><code>##       q1    q2    q3    q4
## w1 0.289 0.212 0.252 0.246
## w2 0.271 0.236 0.245 0.248
## w3 0.270 0.205 0.268 0.257
## w4 0.265 0.209 0.259 0.267</code></pre>

<p>So that if we add the softmax scores, we should get a 1 for all attention weights.</p>
<p><span class="math display">\[\begin{align}
\sum_i^t \left( \alpha_i\right) = 1
\end{align}\]</span></p>

<div class="sourceCode" id="cb2147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2147-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">apply</span>(attention.weights, <span class="dv">1</span>, sum), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## w1 w2 w3 w4 
##  1  1  1  1</code></pre>

<p><strong>Finally</strong>, we obtain the contextual embeddings using the below formula per embedding. Alternatively, we can perform matrix multiplication instead.</p>
<p><span class="math display">\[\begin{align}
c_i = \sum \alpha_i \cdot v_i 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2149-1" data-line-number="1">values =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2149-2" data-line-number="2">context =<span class="st"> </span><span class="kw">round</span>(attention.weights <span class="op">%*%</span><span class="st"> </span>(values), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2149-3" data-line-number="3"><span class="kw">colnames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;f&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,dk))</a>
<a class="sourceLine" id="cb2149-4" data-line-number="4"><span class="kw">rownames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;c&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2149-5" data-line-number="5"><span class="kw">t</span>(context)</a></code></pre></div>
<pre><code>##       c1    c2    c3    c4
## f1 0.287 0.285 0.295 0.295
## f2 0.573 0.561 0.574 0.570
## f3 0.609 0.607 0.601 0.601
## f4 0.374 0.370 0.381 0.382
## f5 0.216 0.212 0.218 0.216</code></pre>

<p>Using <strong>learnable weights</strong>, we first initialize three matrices (<strong>Q</strong>, <strong>K</strong>, <strong>V</strong>) using a random uniform distribution.</p>

<div class="sourceCode" id="cb2151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2151-1" data-line-number="1">Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a>
<a class="sourceLine" id="cb2151-2" data-line-number="2">K =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a>
<a class="sourceLine" id="cb2151-3" data-line-number="3">V =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a></code></pre></div>

<p>We then multiply <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> embeddings with the corresponding matrix like so (noting that <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are sourced from <strong>e</strong>):</p>

<div class="sourceCode" id="cb2152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2152-1" data-line-number="1">q =<span class="st"> </span>k =<span class="st"> </span>v =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2152-2" data-line-number="2">query  =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>Q</a>
<a class="sourceLine" id="cb2152-3" data-line-number="3">keys   =<span class="st"> </span>k <span class="op">%*%</span><span class="st"> </span>K</a>
<a class="sourceLine" id="cb2152-4" data-line-number="4">values =<span class="st"> </span>v <span class="op">%*%</span><span class="st"> </span>V </a></code></pre></div>

<p>Note that we created <strong>q</strong>, <strong>k</strong>, <strong>v</strong> as three separate instances for demonstration. In actual implementation, we may use <strong>e</strong> for the three computations instead to save memory.</p>
<p>We go through the same operations as above to get our context using the <strong>query</strong>, <strong>keys</strong>, and <strong>values</strong>. Here, we try to implement our example of an attention function:</p>

<div class="sourceCode" id="cb2153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2153-1" data-line-number="1">attention.unit &lt;-<span class="st"> </span><span class="cf">function</span>(query, keys, values) {</a>
<a class="sourceLine" id="cb2153-2" data-line-number="2">   t                 =<span class="st"> </span><span class="kw">ncol</span>(query) <span class="co"># t-tokens</span></a>
<a class="sourceLine" id="cb2153-3" data-line-number="3">   dk                =<span class="st"> </span><span class="kw">nrow</span>(query) <span class="co"># k-dimension </span></a>
<a class="sourceLine" id="cb2153-4" data-line-number="4">   scores            =<span class="st"> </span><span class="kw">t</span>(query) <span class="op">%*%</span><span class="st"> </span>keys  </a>
<a class="sourceLine" id="cb2153-5" data-line-number="5">   scaled.scores     =<span class="st"> </span>scores <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(dk)       <span class="co"># scaled</span></a>
<a class="sourceLine" id="cb2153-6" data-line-number="6">   attention.weights =<span class="st"> </span><span class="kw">softmax</span>(scaled.scores) <span class="co"># column-wise </span></a>
<a class="sourceLine" id="cb2153-7" data-line-number="7">   context           =<span class="st"> </span><span class="kw">round</span>(attention.weights <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(values), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2153-8" data-line-number="8">   <span class="kw">colnames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;f&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,dk))</a>
<a class="sourceLine" id="cb2153-9" data-line-number="9">   <span class="kw">rownames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;c&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2153-10" data-line-number="10">   <span class="kw">t</span>(context) <span class="co"># transpose</span></a>
<a class="sourceLine" id="cb2153-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb2153-12" data-line-number="12"><span class="kw">attention.unit</span>(query, keys, values)</a></code></pre></div>
<pre><code>##        c1     c2     c3     c4     c5
## f1 -0.032 -0.026 -0.075 -0.067 -0.063
## f2 -0.010 -0.006 -0.031 -0.027 -0.024
## f3 -0.035 -0.029 -0.079 -0.069 -0.066
## f4 -0.086 -0.081 -0.123 -0.115 -0.111</code></pre>

<p>We should note that the context produced by our attention function is still raw because we relied on the initial values of the <strong>learnable weights</strong>. Similar to our discussion on <strong>MLP</strong> and other types of networks, we need to plug the attention unit into a neural network to allow gradients to flow through the attention layer through <strong>backpropagation</strong>. We will discuss this further once we get to the <strong>Transformer</strong> architecture.</p>
</div>
<div id="multi-head-attention" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.3</span> Multi-Head Attention <a href="deeplearning2.html#multi-head-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Multi-Head Attention</strong> emphasizes on having multiple <strong>heads</strong> of <strong>attention units</strong>, each <strong>head</strong> is trained using a set of <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> matrices containing the <strong>trainable parameters</strong>. See Figure <a href="deeplearning2.html#fig:multiheadattention">13.22</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multiheadattention"></span>
<img src="multiheadattention.png" alt="Multi-Head Attention" width="100%" />
<p class="caption">
Figure 13.22: Multi-Head Attention
</p>
</div>
<p>Having multi-head attention comes from the fact that a single sentence may contain multiple contexts. So, for example, we can break our example sentence in the previous section into three possible contexts like so:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{context 1}} &amp;\rightarrow\ \text{I travel the world}\\
\mathbf{\text{context 2}} &amp;\rightarrow\ \text{search for the fountain}\\
\mathbf{\text{context 3}} &amp;\rightarrow\ \text{fountain of youth}
\end{array}
\]</span></p>
<p>The three contexts may require us to create three <strong>attention</strong> heads. We can choose to use even more heads if we <strong>feel</strong> that there are more hidden contexts (perhaps we can do so heuristically). Each context delivers a set of <strong>Q</strong>, <strong>K</strong>, <strong>V</strong> matrices containing learned parameters.</p>
<p>In terms of computation, notice in the figure that each of the three instances of the embedding, namely <strong>q</strong>, <strong>k</strong>, and <strong>v</strong>, runs through matrix multiplication with each corresponding matrix. Because we are dealing with three attention heads for the three contexts in our example above, it means that <strong>q</strong> gets multiplied with <strong>Q1</strong>, <strong>Q2</strong>, and <strong>Q3</strong> matrices, resulting in three <strong>score matrices</strong>. Similarly, <strong>k</strong> gets multiplied with <strong>K1</strong>, <strong>K2</strong>, and <strong>K3</strong> matrices, resulting in three <strong>score matrices</strong>. The same applies to <strong>v</strong>. Next, the scores are scaled using a normalizer, e.g., <span class="math inline">\(\sqrt{d_k}\)</span>, followed by <strong>softmax</strong> to derive the attention weights - or attention probabilities. The final context is obtained by performing another matrix multiplication between the output of the <strong>softmax</strong> and the output of the score matrices from <strong>v</strong> and the three <strong>V</strong> matrices. For example, in the multi-head attention, the softmax accepts the following input:</p>
<p><span class="math display">\[\begin{align}
\text{attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\text{T}}{\sqrt{d_k}}\right)\cdot V 
\end{align}\]</span></p>
<p>To give an example of how we use <strong>Multi-head Attention</strong>, suppose we have the number of heads equal to 5. Let us randomly initialize the <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> tensors, noting that the tensors have dimension <span class="math inline">\((k \times k \times h)\)</span>:</p>

<div class="sourceCode" id="cb2155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2155-1" data-line-number="1">h =<span class="st"> </span><span class="dv">5</span>;  t =<span class="st"> </span><span class="dv">4</span> <span class="co"># here, h equals no. of heads, t equals no. of tokens</span></a>
<a class="sourceLine" id="cb2155-2" data-line-number="2">Q =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-3" data-line-number="3">K =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-4" data-line-number="4">V =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-5" data-line-number="5">K[,,<span class="dv">1</span>]  <span class="co"># display an initialized K matrix for the first head.</span></a></code></pre></div>
<pre><code>##            [,1]        [,2]        [,3]        [,4]
## [1,] 0.15782952 0.158294656  0.30144395 0.389498414
## [2,] 0.38407710 0.078136669 -0.28551430 0.334255804
## [3,] 0.33999093 0.155190383  0.47465562 0.140585024
## [4,] 0.44216319 0.297951073 -0.32631462 0.373908170
## [5,] 0.30393706 0.069033058 -0.48419310 0.094678544
##             [,5]
## [1,] -0.20034764
## [2,]  0.34011628
## [3,]  0.09883009
## [4,]  0.21031208
## [5,] -0.16618633</code></pre>

<p>We then multiply <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> embeddings with the corresponding matrix like so (noting that <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are sourced from <strong>e</strong>:</p>

<div class="sourceCode" id="cb2157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2157-1" data-line-number="1">query =<span class="st"> </span>keys =<span class="st"> </span>values =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dk, t, h)) <span class="co"># create the q,k,v structures</span></a>
<a class="sourceLine" id="cb2157-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>h) {</a>
<a class="sourceLine" id="cb2157-3" data-line-number="3">  query[,,i]  =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>Q[,,i]</a>
<a class="sourceLine" id="cb2157-4" data-line-number="4">  keys[,,i]   =<span class="st"> </span>k <span class="op">%*%</span><span class="st"> </span>K[,,i]</a>
<a class="sourceLine" id="cb2157-5" data-line-number="5">  values[,,i] =<span class="st"> </span>v <span class="op">%*%</span><span class="st"> </span>V[,,i] </a>
<a class="sourceLine" id="cb2157-6" data-line-number="6">}</a></code></pre></div>

<p>Let us now invoke our <strong>attention.unit</strong> function.</p>

<div class="sourceCode" id="cb2158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2158-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb2158-2" data-line-number="2">contextualized =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dk, t, h)) </a>
<a class="sourceLine" id="cb2158-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>h) {</a>
<a class="sourceLine" id="cb2158-4" data-line-number="4">   contextualized[,,i] =<span class="st"> </span><span class="kw">attention.unit</span>(query[,,i], keys[,,i], values[,,i])</a>
<a class="sourceLine" id="cb2158-5" data-line-number="5">}</a></code></pre></div>
<p>The dimension is [5, 4, 5].</p>

<p>Note that the output of the multi-head attention is a list of <strong>contextualized</strong> matrices. The list has a size equal to the number of heads. To obtain only one <strong>contextualized matrix</strong> with the same dimension as the original embedding, we need to merge the list. The proposed <strong>merge</strong> approach is to concatenate and then run through a dense FC layer, reshaping the output in the output layer, then using softmax past the output layer.</p>
<p><strong>Multihead Attention</strong> is seen in the paper by Ashish Vaswani et al. <span class="citation">(<a href="bibliography.html#ref-ref1339a">2017</a>)</span>. It is used as one of the core components of a novel <strong>Attention-based</strong> architecture called <strong>Transformer</strong>.</p>
<p>Before we jump to <strong>Transformers</strong>, let us discuss a few essential topics, focusing on <strong>sequence-to-sequence</strong> applications. Among many others, we need to build our intuition around the following topics:</p>
<ul>
<li><strong>Word Embedding</strong> - numerical representation of individual sequence elements (e.g., words),</li>
<li><strong>Positional Embedding</strong> - numerical representation of the position of sequence elements, and</li>
<li><strong>Sequence Alignment</strong> - alignment between sequences if we are to translate one sequence to another.</li>
</ul>
</div>
<div id="word-embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.4</span> Word Embedding <a href="deeplearning2.html#word-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A few concepts are introduced in Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), particularly in the context of Natural Language Processing (<strong>NLP</strong>). One of the applications of <strong>NLP</strong> is around <strong>search engines</strong> in which we deal with a given query and a list of documents available to search. Here, we start with <strong>pre-processing</strong> of texts to extract <strong>bag of words (BoW)</strong> using <strong>tokenization</strong>, <strong>case-sensitivity</strong>, <strong>stopwords</strong>, <strong>N-grams</strong>, <strong>stemming</strong>, <strong>lemmatization</strong>, and so on. While such a <strong>bag of words</strong> is merely an extracted list of terms, it does not necessarily carry any numerical representation. Instead, we measure the relevance of each term (or relevance of a query made of terms) by way of ranking techniques. <strong>Okapi BM25</strong> technique is introduced in the Chapter mentioned, which is a rather advanced variant of the generic <strong>TF-IDF</strong> technique covered in the field of <strong>Information Retrieval</strong>. The idea is to be able to rank terms found both in a given query and also in a list of documents. Fundamentally, this is how we score a term, or in other words, how we cast numerical values of terms. Mathematically, we cast words into their numerical representations in the form of <strong>Vector Space Model (SVM)</strong>. From an engineering perspective, we can use <strong>Document Term Matrix (DTM)</strong> or <strong>Term Document Matrix (TDM)</strong> interchangeably to compare documents. Both matrices may contain simple counts of terms or <strong>TF-IDF</strong> results. From there, we use <strong>cosine similarity</strong> to identify the relationship of documents. See Figure <a href="deeplearning2.html#fig:vsmtfidf">13.23</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vsmtfidf"></span>
<img src="vsm_tfidf.png" alt="Vector Space Model (VSM) with TF-IDF scores" width="90%" />
<p class="caption">
Figure 13.23: Vector Space Model (VSM) with TF-IDF scores
</p>
</div>
<p>From the idea of <strong>Vector Space Model (VSM)</strong>, we carry the same intuition for <strong>Word Embedding</strong>. Perhaps, the fundamental definition of <strong>Word Embedding</strong> exemplifies the idea that words can be translated to their numeric representation. To further build the intuition around this, imagine a <strong>matching site</strong> that allows users to enter a list of personality traits and allows its matching engine to pair individual users based on traits. Here, we may compile traits such as <strong>athletic</strong>, <strong>outgoing</strong>, <strong>adventurous</strong>, <strong>likes movies</strong>, and others. See Figure <a href="deeplearning2.html#fig:context">13.24</a>. From this perspective, we can readily say that a person is characterized or represented by a vector of <strong>trait features</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:context"></span>
<img src="context.png" alt="Word Embedding)" width="90%" />
<p class="caption">
Figure 13.24: Word Embedding)
</p>
</div>
<p>Suppose that, by some algorithm, the matching engine can assign numbers to each trait so that a particular user labeled as <strong>You</strong> is 0.85 athletic, 0.61 outgoing, and on. Similarly, a particular unknown individual labeled as <strong>Person</strong> resembles almost the same weights as <strong>You</strong> in terms of traits. Ideally, using a scoring formula such as <strong>cosine similarity</strong>, we can suggest that <strong>You</strong> and <strong>Person</strong> match by proximity. Moreover, imagine by the same token that if there is another user labeled as <strong>Elf</strong> whose traits closely match the combined traits of <strong>You</strong> and <strong>Person</strong>, we now begin to see a cluster of users with similar characteristics (visualizing by reducing dimensionality to two using <strong>PCA</strong>).</p>
<p>In the context of <strong>semantic analysis</strong>, the meaning of a word (its <strong>context</strong>) does not have to be derived only by how the word can be described (based on traits). Instead, it can also be derived from how the word is used in a <strong>sequence of words</strong> based on its surrounding words. This idea comes from the famous quote below used by other literature:</p>
<p><span class="math display">\[
   \text{&quot;You shall know a word by the company it keeps&quot;, }
   \ \ \ \ \ \ \ \mathbf{\text{John Rupert Firth.}}
\]</span></p>
<p>In Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), we introduce two techniques that allow us to capture the semantic essence of texts, namely <strong>Probabilistic Latent Semantic Analysis (pLSA)</strong> and <strong>Latent Dirichlet Allocation (LDA)</strong>. Mostly, the techniques are used for <strong>Topic Modeling</strong> relying on probabilistic language modeling to create a <strong>contextualized distributed representation</strong> of words (in particular using <strong>Dirichlet Distributions</strong> for <strong>LDA</strong>) to highlight probabilities of <strong>topic words</strong>. Both techniques introduce the use of <strong>latent</strong> parameters that follow certain distributions. We can say that <strong>Topic Modeling</strong> creates clustered <strong>contextualized word representation</strong> <span class="citation">(Laure Thompson et al. <a href="bibliography.html#ref-ref1236l">1997</a>)</span>.    </p>
<p>One important criterion that is lacking when dealing with traditional techniques such as <strong>pLSA</strong> and <strong>LDA</strong> is their learnability or trainability. When it comes to that, <strong>Neural Probabilistic Language Modeling (NPLM)</strong> comes into the picture, introduced by Yoshua Bengio et al. <span class="citation">(<a href="bibliography.html#ref-ref1227y">2003</a>)</span>. This model uses <strong>Neural Network</strong> to learn <strong>Word Embeddings</strong>. Figure <a href="deeplearning2.html#fig:neuralprobmodel">13.25</a> shows a neural architecture depicting the original <strong>NPLM</strong> neural network architecture from <strong>Bengioâs</strong> paper.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:neuralprobmodel"></span>
<img src="neuralprobmodel.png" alt="Neural Probabilistic Language Model" width="80%" />
<p class="caption">
Figure 13.25: Neural Probabilistic Language Model
</p>
</div>
<p>The model expects a series of word indexes for which we perform a table lookup against a matrix <strong>C</strong> which can also serve as a <strong>Word Embedding</strong> lookup. We expect the lookup to return a set of feature vectors, each vector corresponding to each word index. We then concatenate the set of feature vectors. This embedding gets forwarded to the hidden layer for matrix multiplication with a matrix of learnable <strong>weight</strong> parameters denoted by (<span class="math inline">\(\omega_h\)</span>). Then a non-linearity (<strong>tanh</strong>) is applied, followed by <strong>softmax</strong>. This architectural process demonstrates a shallow <strong>MLP</strong> with characteristics of a one hidden-layer fully-connected neural network architecture that performs the usual <strong>forward feed</strong> and <strong>backpropagation</strong>.</p>
<p>Many other architectural variants are motivated by the <strong>NPLM</strong> design, laid by Bengioâs paper. Revisiting our <strong>CNN</strong>, variant designs of <strong>NPLM</strong> may utilize different combinations of non-linearity, dropouts, layer normalization, optimization, residual nets, and on <span class="citation">(Sun S., Iyyer M. <a href="bibliography.html#ref-ref1582s">2021</a>)</span>.</p>
<p>One of the more advanced techniques motivated by <strong>NLPM</strong> architecture is called <strong>Word2vec</strong>. It was introduced by Tomas Mikolov et al. <span class="citation">(<a href="bibliography.html#ref-ref1215m">2013</a>)</span>. The grand idea follows <strong>NLPM</strong> in that it associates a word with surrounding words that serve as clues to predict <strong>context</strong>. Now, the type of <strong>Word Embedding</strong> created by <strong>Word2vec</strong> is a set of feature vectors that are learned or trained. The neural architecture of <strong>Word2vec</strong> is designed to utilize two <strong>Word Embedding</strong> approaches, namely <strong>Continuous Bag of Words (CBOW)</strong> and <strong>Skip-Gram</strong>. A <strong>CBOW</strong> model is trained to predict the probability of a target word to occur based on a given <strong>context</strong>. For example, given the context, <strong>I travel the world in search for the fountain of [____]</strong>, the most probable target word is <strong>youth</strong> from a list of word distributions, e.g., <strong>apples, sky, youth</strong>. The words <strong>apples</strong> and <strong>sky</strong> will receive the lowest probability scores. </p>
<p>On the other hand, the <strong>Skip-Gram</strong> model does the reverse in that it is trained to predict the <strong>context</strong> (list of contextual words) from a given target word. In other words, in <strong>Skip-Gram</strong>, it might make sense to predict surrounding words such as <strong>I travel the world in search for the fountain of [____]</strong>, given the target word <strong>youth</strong>.</p>
<p>To illustrate <strong>Skip-Gram</strong>, let us use Figure <a href="deeplearning2.html#fig:skipgram">13.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:skipgram"></span>
<img src="skipgram.png" alt="Skip-Gram Architecture" width="100%" />
<p class="caption">
Figure 13.26: Skip-Gram Architecture
</p>
</div>
<p>The neural architecture of <strong>Skip-Gram</strong> follows a shallow fully-connected <strong>MLP</strong> in which we have <strong>V-dimension</strong> inputs (as one-hot encoding) in the input layer, <strong>H</strong> neurons in the hidden layer, and <strong>O</strong> units for the output. Note that a <strong>window</strong> is used to define the scope of the surrounding words, the target word being at the center. The window is <strong>monotonic</strong> (see <strong>Luong Attention</strong>), and a hyperparameter <strong>window size</strong> can be used to adjust the number of surrounding words. This same number becomes the size of the <strong>O</strong> units which contains the number of predicted <strong>contextual words</strong> (the surrounding words). The window slides iteratively across the series of words until the entire series is accommodated. The size of the <strong>Hidden layer</strong> is defined based on the number of neurons to use for the training. The <strong>output</strong> of the forward feed at every iteration is a matrix that consists of a predicted set of probability vectors. Each vector represents a predicted <strong>contextual word</strong> that is then evaluated against the <strong>context clues</strong>, optimizing the Loss function. The final <strong>output</strong> of the <strong>Skip-Gram</strong> itself is a trained model consisting of the optimized weighted matrices (embedding matrix and context matrix). The matrices contain learnable parameters.  </p>
<p>Additionally, the output layer uses <strong>softmax</strong> to calculate word probabilities. It helps to note, however, that <strong>softmax</strong> is known to be computationally expensive because it involves calculating a matrix that carries the vocabulary (<strong>V</strong>) size. Furthermore, it can get very large. For that reason, <strong>Word2vec Skip-gram</strong> also demonstrates using <strong>negative sampling</strong> with <strong>sigmoid function</strong> as an alternative, switching the classification to binary (second paper published by <span class="citation">(Mikolov T. <a href="bibliography.html#ref-ref1215m">2013</a>)</span>. Other literature emphasizes the same alternative, evaluating the benefits <span class="citation">(Long Chen et al. <a href="bibliography.html#ref-ref1242l">2018</a>)</span>.</p>
<p>We leave readers to investigate other tools such as <strong>Inpatient2vec</strong>, <strong>doc2vec</strong>, and <strong>med2vec</strong> intended for specific <strong>Word Embeddings</strong> while providing a multi-layer medical representation learning for patients, diagnostics, and medical codes, respectively. Such representations provide context to targetted medical terms. Also, it helps to investigate <strong>Global Vectors for Word Representation (GloVe)</strong>.</p>
</div>
<div id="positional-embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.5</span> Positional Embedding <a href="deeplearning2.html#positional-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the emergence of <strong>Attention</strong>, the sequential approach, seen in <strong>RNN</strong> (e.g., <strong>LSTM</strong> and <strong>GRU</strong>), is replaced by an architecture that handles a series of words in parallel. This architecture, however, loses the <strong>order</strong> information. Therefore, that is where <strong>Positional Embedding</strong> is required.</p>
<p><strong>Positional Embedding</strong> is a <strong>learned</strong> numerical representation of the position and order of words in a given <strong>sequence of words</strong>. This information is not captured in vanilla <strong>Word Embeddings</strong>. Thus, we need to add the information to the <strong>Word Embedding</strong>.</p>
<p><span class="math display">\[
\mathbf{\text{Word Embedding}} + \mathbf{\text{Positional Embedding}} =
\mathbf{\text{Position-Aware Word Embedding}}
\]</span></p>
<p>In terms of preserving <strong>position</strong> information, the first thoughts were using <strong>word indices</strong> or reducing the <strong>word indices</strong> to the range 0 and 1 using fractions. However, for the former, a large index at the end of a sequence tends to dominate the value of the <strong>Word Embedding</strong> when added. For the latter, even though the decimal value is the same, e.g., 0.25, a position in the form 1/4 (position one from a sequence of 4 words) does not equal a position in the form 2/8 (position two from a sequence of 8 words). Therefore, a novel approach introduced is to use <strong>Sinusoidal Position Encoding</strong> <span class="citation">(Vaswani A. et al. <a href="bibliography.html#ref-ref1339a">2017</a>)</span>. The big idea is to use frequency produced by <strong>sine</strong> and <strong>cosine</strong> to yield a position representation. Below is the expression used (derivation not included):</p>
<p><span class="math display">\[\begin{align}
PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/dmodel}}\right) 
\ \ \ \ \ \ \ \ \ 
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/dmodel}}\right)
\end{align}\]</span></p>
<p>An example implementation is written below.</p>

<div class="sourceCode" id="cb2159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2159-1" data-line-number="1">sinusoid.encoding &lt;-<span class="st"> </span><span class="cf">function</span>(npos, emb.dim, pos.range) {</a>
<a class="sourceLine" id="cb2159-2" data-line-number="2">    angle &lt;-<span class="st"> </span><span class="cf">function</span>(pos, i) { pos <span class="op">/</span><span class="st"> </span><span class="dv">10000</span><span class="op">^</span>((<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>i) <span class="op">/</span><span class="st">  </span>emb.dim)}</a>
<a class="sourceLine" id="cb2159-3" data-line-number="3">    pos.seq       =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, npos<span class="dv">-1</span>, <span class="dt">length.out=</span>pos.range)</a>
<a class="sourceLine" id="cb2159-4" data-line-number="4">    emb.seq       =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, emb.dim<span class="dv">-1</span>, <span class="dt">length.out=</span>emb.dim)</a>
<a class="sourceLine" id="cb2159-5" data-line-number="5">    pos_embedding =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(pos.range , emb.dim))</a>
<a class="sourceLine" id="cb2159-6" data-line-number="6">    <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>pos.range) {</a>
<a class="sourceLine" id="cb2159-7" data-line-number="7">        pos_embedding[p,] =<span class="st"> </span><span class="kw">angle</span>(pos.seq[p], emb.seq)</a>
<a class="sourceLine" id="cb2159-8" data-line-number="8">    }</a>
<a class="sourceLine" id="cb2159-9" data-line-number="9">    sini =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>, <span class="dt">to=</span>emb.dim<span class="dv">-1</span>, <span class="dt">by=</span><span class="dv">2</span>) <span class="co"># 2i</span></a>
<a class="sourceLine" id="cb2159-10" data-line-number="10">    cosi =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>emb.dim<span class="dv">-1</span>, <span class="dt">by=</span><span class="dv">2</span>) <span class="co"># 2i + 1</span></a>
<a class="sourceLine" id="cb2159-11" data-line-number="11">    pos_embedding[,sini] =<span class="st"> </span><span class="kw">sin</span>(pos_embedding[,sini])</a>
<a class="sourceLine" id="cb2159-12" data-line-number="12">    pos_embedding[,cosi] =<span class="st"> </span><span class="kw">cos</span>(pos_embedding[,cosi])</a>
<a class="sourceLine" id="cb2159-13" data-line-number="13">    pos_embedding</a>
<a class="sourceLine" id="cb2159-14" data-line-number="14">}</a></code></pre></div>

<p>Assume the number of positions is 20 with an embedding size of 40. We should be able to produce a rotary matrix to be then added to a word embedding of the same dimension.</p>
<div class="sourceCode" id="cb2160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2160-1" data-line-number="1">npos     =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb2160-2" data-line-number="2">pos.range =<span class="st"> </span><span class="dv">200</span> <span class="co"># only needed to plot a smoother curve</span></a>
<a class="sourceLine" id="cb2160-3" data-line-number="3">v =<span class="st"> </span><span class="kw">sinusoid.encoding</span>(<span class="dt">npos =</span> npos, <span class="dt">emb.dim=</span><span class="dv">40</span>, <span class="dt">pos.range=</span>pos.range)</a>
<a class="sourceLine" id="cb2160-4" data-line-number="4"><span class="kw">data.frame</span>(v[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="co"># display only 10x3 portion of the matrix</span></a></code></pre></div>
<pre><code>##            X1          X2         X3
## 1  1.00000000 0.000000000 1.00000000
## 2  0.99544550 0.060205727 0.99927770
## 3  0.98182347 0.120193027 0.99711184
## 4  0.95925801 0.179744265 0.99350554
## 5  0.92795465 0.238643386 0.98846403
## 6  0.88819855 0.296676704 0.98199457
## 7  0.84035184 0.353633673 0.97410652
## 8  0.78485036 0.409307652 0.96481128
## 9  0.72219967 0.463496655 0.95412226
## 10 0.65297046 0.516004082 0.94205492</code></pre>
<p>A plot of the matrix is demonstrated in Figure . Here, we choose the first four elements (four dimensions) of an embedding.</p>

<div class="sourceCode" id="cb2162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2162-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, npos<span class="dv">-1</span>, <span class="dt">length.out=</span>pos.range)</a>
<a class="sourceLine" id="cb2162-2" data-line-number="2">y =<span class="st"> </span>v</a>
<a class="sourceLine" id="cb2162-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb2162-4" data-line-number="4">      <span class="dt">ylab=</span><span class="st">&quot;Embedding Dimension (i)&quot;</span>,  <span class="dt">xlab=</span><span class="st">&quot;Position (pos)&quot;</span>,   </a>
<a class="sourceLine" id="cb2162-5" data-line-number="5">      <span class="dt">main=</span><span class="st">&quot;Sinusoidal Position Encoding&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2162-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2162-7" data-line-number="7"><span class="kw">lines</span>(x, v[,<span class="dv">1</span>], <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>,  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)   <span class="co"># 2i   (sine)</span></a>
<a class="sourceLine" id="cb2162-8" data-line-number="8"><span class="kw">lines</span>(x, v[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i+1 (cosine)</span></a>
<a class="sourceLine" id="cb2162-9" data-line-number="9"><span class="kw">lines</span>(x, v[,<span class="dv">3</span>], <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>,        <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i   (sine)</span></a>
<a class="sourceLine" id="cb2162-10" data-line-number="10"><span class="kw">lines</span>(x, v[,<span class="dv">4</span>], <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>,       <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i+1 (cosine)</span></a>
<a class="sourceLine" id="cb2162-11" data-line-number="11"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2162-12" data-line-number="12"><span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="dv">16</span>,   <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb2162-13" data-line-number="13"><span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>), <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.75</span>, <span class="dv">0</span>, <span class="fl">0.95</span>, <span class="fl">-0.58</span>), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb2162-14" data-line-number="14"><span class="kw">text</span>(<span class="dv">14</span>, <span class="fl">-0.75</span>, <span class="st">&quot;i=1&quot;</span>);  <span class="kw">text</span>(<span class="dv">16</span>,  <span class="fl">0.00</span>, <span class="st">&quot;i=2&quot;</span>)</a>
<a class="sourceLine" id="cb2162-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">16</span>,  <span class="fl">0.92</span>, <span class="st">&quot;i=3&quot;</span>);  <span class="kw">text</span>(<span class="dv">14</span>, <span class="fl">-0.58</span>, <span class="st">&quot;i=4&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sinusoidal"></span>
<img src="DS_files/figure-html/sinusoidal-1.png" alt="Sinusoidal Position Encoding" width="70%" />
<p class="caption">
Figure 13.27: Sinusoidal Position Encoding
</p>
</div>

<p>Notice that the absolute position of a word is reduced to a sinusoidal value that ranges between -1 and 1. For example, in the plot for the first dimension (i=1), the position of the first word at zero renders a frequency value of 1, but the same word at position 15 has a frequency value of -0.75.</p>
<p>From here, we can add the derived positional embeddings to the word embeddings, so that assumes we have a word embedding of vector size 40 (40-dimensional features),</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{W1} &amp;= \text{[0.734, 0.123, 0.512, ..., 0.111]}\ \ \ \ (1 \times 40 )\\
\text{W2} &amp;= \text{[0.813, 0.022, 0.762, ..., 0.212]}\ \ \ \ (1 \times 40 ) \\
&amp;... \\
\text{W15} &amp;= \text{[0.544, 0.653, 0.912, ..., 0.433]}\ \ \ \ (1 \times 40 )
\end{array}
\]</span>
we can then, as an example, add the vector of the first dimension like so:</p>
<p><span class="math display">\[\begin{align*}
\underbrace{\text{[0.734, 0.123, 0.512, ..., 0.111]}}_{\text{Word Embedding}} + 
\underbrace{\text{[1.000, 0.653, -0.241, ..., 0.988]}}_{\text{Positional Embedding}} = \\
\underbrace{[0.734, 0.080, ..., 0.110]}_{\text{Position-aware Word Embedding}}
\end{align*}\]</span></p>
<p>Alternatively, instead of dealing with absolute positions such as represented by <strong>sinusoidal embeddings</strong>, we can deal with <strong>relative position embedding</strong>. The idea is to represent the position in calculating the location of a word relevant to the location of other neighboring words. In other words, we measure the distance relationship between words. Below is a sample implementation of deriving distances based on the difference between absolute positions. The resulting distances are shown in the <strong>Toeplitz</strong> matrix form below:</p>

<div class="sourceCode" id="cb2163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2163-1" data-line-number="1">distance &lt;-<span class="st"> </span><span class="cf">function</span>(w1, w2) {  <span class="kw">abs</span>(w2) <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(w1) }</a>
<a class="sourceLine" id="cb2163-2" data-line-number="2">w =<span class="st"> </span>word.indices =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb2163-3" data-line-number="3">n =<span class="st"> </span><span class="kw">length</span>(word.indices)</a>
<a class="sourceLine" id="cb2163-4" data-line-number="4">m =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n <span class="op">*</span><span class="st"> </span>n), <span class="dt">nrow=</span>n)</a>
<a class="sourceLine" id="cb2163-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {  m[i, ] =<span class="st"> </span><span class="kw">distance</span>(w[i], w) }</a>
<a class="sourceLine" id="cb2163-6" data-line-number="6">m</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    0    1    2    3    4    5    6
## [2,]   -1    0    1    2    3    4    5
## [3,]   -2   -1    0    1    2    3    4
## [4,]   -3   -2   -1    0    1    2    3
## [5,]   -4   -3   -2   -1    0    1    2
## [6,]   -5   -4   -3   -2   -1    0    1
## [7,]   -6   -5   -4   -3   -2   -1    0</code></pre>

<p>The matrix above contains scalar distances. In terms of <strong>embeddings</strong> or vectors, the resulting distance is illustrated in Figure <a href="deeplearning2.html#fig:relativepos">13.28</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:relativepos"></span>
<img src="relativepos.png" alt="Relative Position Embedding" width="70%" />
<p class="caption">
Figure 13.28: Relative Position Embedding
</p>
</div>
<p>Each word embedding is paired to itself and every other word in the sequence in a pair-wise manner for which a relative distance representation is formed. See the table in Figure <a href="deeplearning2.html#fig:relativepair">13.29</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:relativepair"></span>
<img src="relativepair.png" alt="Pairwise Relative Relationship" width="100%" />
<p class="caption">
Figure 13.29: Pairwise Relative Relationship
</p>
</div>
<p>Notice that if we pair two embeddings (two vectors), the result is a vector, e.g. (<span class="math inline">\(\vec{\alpha_{1,1}}\)</span>), assuming we pair by element-wise multiplication. Hence, each element in the table in Figure <a href="deeplearning2.html#fig:relativepair">13.29</a> represents a vector. Furthermore, if we are to derive the relative position embedding for <span class="math inline">\(\mathbf{\vec{e_1}}\)</span>, we have to stack the vectors <span class="math inline">\(\vec{\alpha_{1,1}}, \vec{\alpha_{2,1}}, ..., \vec{\alpha_{t,1}}\)</span> and add the vectors element-wise to obtain the vector <span class="math inline">\(\vec{\alpha_1}\)</span> which we then add to the first word embedding, e.g. <span class="math inline">\(\vec{e_1}\)</span>, to get a <strong>position-aware</strong> embedding. Here, we choose to use addition for the stacked vectors to demonstrate how to get the final relative position embedding. However, it may not be a good idea to stack multiple vectors and add them to the vanilla embedding - this may dilute the influence of the <strong>Word Embedding</strong> over the <strong>Position Embedding</strong>.</p>
<p>An alternative approach is to use the <strong>Attention</strong> mechanism to obtain <strong>Relative Position Embedding</strong>. This idea is popularized by Peter Shaw et al. <span class="citation">(<a href="bibliography.html#ref-ref181p">2018</a>)</span> and by Zhiheng Huang et al. <span class="citation">(<a href="bibliography.html#ref-ref191z">2020</a>)</span>. Here, we perform a dot-product between a matrix and its transpose. The matrix, denoted by (<span class="math inline">\(\mathbf{e}\)</span>), contains the embeddings (in column-wise fashion). See Figure <a href="deeplearning2.html#fig:attentionpair">13.30</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attentionpair"></span>
<img src="attentionpair.png" alt="Relative Position using Attention Mechanism" width="100%" />
<p class="caption">
Figure 13.30: Relative Position using Attention Mechanism
</p>
</div>
<p>After performing the dot product, the resulting matrix containing the <strong>re-weighted values</strong> or <strong>score values</strong> denoted by the symbol <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) is then multiplied by the original matrix, e.g. (<span class="math inline">\(\mathbf{e}\)</span>). The operation is also a dot product calculation. Note that our vectors are arranged in a column-wise fashion; thus, the expression below is written as:</p>
<p><span class="math display">\[\begin{align}
\alpha = e^\text{T}e\ \ \ \ \rightarrow \ \ \ \ \  c = \alpha \times e^\text{T}
\end{align}\]</span></p>
<p>To illustrate, let us create an <strong>input</strong> matrix with 3 word embeddings that are arranged in column-wise fashion and in sequence, e.g. { <span class="math inline">\(\mathbf{\vec{e_1}}\)</span>, <span class="math inline">\(\mathbf{\vec{e_2}}\)</span>, <span class="math inline">\(\mathbf{\vec{e_3}}\)</span> }, so that, as an example, the first embedding, e.g. (<span class="math inline">\(\mathbf{\vec{e_1}}\)</span>), contains three contextual features (3 dimensions):</p>

<div class="sourceCode" id="cb2165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2165-1" data-line-number="1">e =<span class="st">  </span>(<span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">9</span>), <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb2165-2" data-line-number="2"><span class="kw">colnames</span>(e) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;e1&quot;</span>, <span class="st">&quot;e2&quot;</span>, <span class="st">&quot;e3&quot;</span>)</a>
<a class="sourceLine" id="cb2165-3" data-line-number="3"><span class="kw">rownames</span>(e) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>)</a>
<a class="sourceLine" id="cb2165-4" data-line-number="4">e</a></code></pre></div>
<pre><code>##          e1 e2 e3
## feature1  1  4  7
## feature2  2  5  8
## feature3  3  6  9</code></pre>

<p>We now perform dot product to get some kind of <strong>score</strong> values like so.</p>
<div class="sourceCode" id="cb2167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2167-1" data-line-number="1">alpha  =<span class="st"> </span><span class="kw">t</span>(e) <span class="op">%*%</span><span class="st"> </span>(e) <span class="co"># dot product</span></a>
<a class="sourceLine" id="cb2167-2" data-line-number="2"><span class="kw">colnames</span>(alpha) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a1&quot;</span>, <span class="st">&quot;a2&quot;</span>, <span class="st">&quot;a3&quot;</span>) <span class="co"># scores</span></a>
<a class="sourceLine" id="cb2167-3" data-line-number="3">alpha</a></code></pre></div>
<pre><code>##    a1  a2  a3
## e1 14  32  50
## e2 32  77 122
## e3 50 122 194</code></pre>
<p>Afterwhich, we then obtain a vector output, e.g., <span class="math inline">\(\mathbf{\vec{c_1}}\)</span>, by multiplying each word embedding by each scalar score using the below expression:</p>
<p><span class="math display">\[\begin{align}
c_i = \sum_j \left(a_{i, j} \times \vec{e_j}\right)
\end{align}\]</span></p>
<p>We can expand this to show the following solution, along with a simple implementation:</p>
<p><span class="math display">\[\begin{align}
\vec{c_1} &amp;= \left(a_{1,1} \times \vec{e_1}\right) + \left(a_{1,2} \times \vec{e_2}\right) + \left(a_{1,3} \times \vec{e_3}\right) \label{eqn:eqnnumber804}\\
&amp;= 14 \times \left[\begin{array}{l}1 \\ 2 \\ 3\end{array}\right] + 
  32 \times \left[\begin{array}{l}4 \\ 5 \\ 6\end{array}\right]  + 
  50 \times \left[\begin{array}{l}7 \\ 8 \\ 9\end{array}\right] \nonumber \\
&amp;= 
  \left[\begin{array}{l}492 \\ 588 \\ 684\end{array}\right] \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb2169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2169-1" data-line-number="1">a1 =<span class="st"> </span>alpha[,<span class="dv">1</span>] <span class="co"># scores</span></a>
<a class="sourceLine" id="cb2169-2" data-line-number="2">(<span class="dt">c1 =</span> a1[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a1[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a1[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>])</a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##      492      588      684</code></pre>

<p>We perform the same operations for <span class="math inline">\(\mathbf{\vec{c_2}}\)</span>,</p>
<p><span class="math display">\[\begin{align}
\vec{c_2} = \left(a_{2,1} \times \vec{e_1}\right) + \left(a_{2,2} \times \vec{e_2}\right) + \left(a_{2,3} \times \vec{e_3}\right) 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2171-1" data-line-number="1">a2 =<span class="st"> </span>alpha[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2171-2" data-line-number="2">(<span class="dt">c2 =</span> a2[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a2[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a2[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>]) </a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##     1194     1425     1656</code></pre>

<p>and the same operations for <span class="math inline">\(\mathbf{\vec{c_3}}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\vec{c_3} = \left(a_{3,1} \times \vec{e_1}\right) + \left(a_{3,2} \times \vec{e_2}\right) + \left(a_{3,3} \times \vec{e_3}\right) 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2173-1" data-line-number="1">a3 =<span class="st"> </span>alpha[,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2173-2" data-line-number="2">(<span class="dt">c3 =</span> a3[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a3[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a3[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>])</a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##     1896     2262     2628</code></pre>

<p>Alternatively, we can instead use a dot product between the two matrices like so:</p>

<div class="sourceCode" id="cb2175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2175-1" data-line-number="1">c =<span class="st"> </span>(alpha) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(e) </a>
<a class="sourceLine" id="cb2175-2" data-line-number="2"><span class="kw">rownames</span>(c) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;c1&quot;</span>, <span class="st">&quot;c2&quot;</span>, <span class="st">&quot;c3&quot;</span>)</a>
<a class="sourceLine" id="cb2175-3" data-line-number="3"><span class="kw">colnames</span>(c) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>)</a>
<a class="sourceLine" id="cb2175-4" data-line-number="4"><span class="kw">t</span>(c)</a></code></pre></div>
<pre><code>##           c1   c2   c3
## feature1 492 1194 1896
## feature2 588 1425 2262
## feature3 684 1656 2628</code></pre>

<p>The vectors {<span class="math inline">\(\mathbf{\vec{c_1}}\)</span>, <span class="math inline">\(\mathbf{\vec{c_2}}\)</span>, <span class="math inline">\(\mathbf{\vec{c_3}}\)</span>} represent the final <strong>Location-aware word Embeddings</strong>.</p>
<p>In the examples above, we focused on the operations only, but we can add other improvements such as normalization for numerical stability.</p>
<p><span class="math display">\[\begin{align}
\alpha = \text{normalize}(e^\text{T}e)\ \ \ \ \rightarrow \ \ \ \ \  c = \alpha \times e^\text{T}
\end{align}\]</span></p>
</div>
<div id="sequence-alignment" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.6</span> Sequence Alignment<a href="deeplearning2.html#sequence-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Alignment</strong> is a consideration that applies mostly to <strong>machine translation</strong>. For example, take the following sequence-to-sequence translation:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{encoder sequence}} &amp;= 
\underbrace{\text{word}_{(1)}, \text{word}_{(2)}}_{\text{context 1}}, 
\underbrace{\text{word}_{(3)}}_{\text{context 2}},
\underbrace{\text{word}_{(4)}, \text{word}_{(5)}, \text{word}_{(6)}}_{\text{context 3}},\underbrace{\text{word}_{(7)}}_{\text{context 4}}\\
\\ 
&amp;\mathbf{\text{encode-decode (translation)}}\\
\\ 
\mathbf{\text{decoder sequence}} &amp;= 
\underbrace{\text{word}_{(1)}}_{\text{context 1}},  
\underbrace{\text{word}_{(2)}, \text{word}_{(3)}}_{\text{context 2}},
\underbrace{\text{word}_{(4)}, \text{word}_{(5)}}_{\text{context 3}},
\underbrace{\text{word}_{(6)}}_{\text{context 4}},
\underbrace{\text{word}_{(7)}}_{\text{context 5}}\\
\end{array}
\]</span></p>
<p>Note in the example above that words are grouped based on <strong>context</strong>. In previous discussions, we used <strong>alignment scores</strong> to identify words that are more aligned to a target word in terms of relevance or attentiveness. Such alignment scores are differently calculated as presented in <strong>Luong</strong> paper.</p>
<p>So far, our discussion around <strong>alignment</strong> only emphasizes the <strong>encoder</strong> side. However, we also have to consider <strong>alignment</strong> on the side of the <strong>decoder</strong>. We have to be able to map both alignments to help with the translation. It, therefore, makes sense to use the <strong>Attention</strong> mechanism on both sides. Thus, one that enforces the idea is shown in the <strong>Transformer</strong> architecture.</p>
</div>
<div id="transformer-architectures" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.7</span> Transformer Architectures <a href="deeplearning2.html#transformer-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like <strong>CNN</strong> architectures, a few <strong>TNN</strong> architectures made a name for themselves, namely <strong>Bert</strong>, <strong>RoBERTa</strong>, and <strong>GPT</strong>. However, the one architecture that pioneered it is the <strong>Transformer</strong>, introduced in the paper titled <strong>Attention is all you need</strong>. See Figure <a href="deeplearning2.html#fig:transformer">13.31</a> <span class="citation">(Vaswani A. et al <a href="bibliography.html#ref-ref1339a">2017</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:transformer"></span>
<img src="transformer.png" alt="Transformer Architecture" width="50%" />
<p class="caption">
Figure 13.31: Transformer Architecture
</p>
</div>
<p>The architecture diagram in the figure exposes an <strong>Encoder-Decoder</strong> model architecture. The <strong>Encoder</strong> and <strong>Decoder</strong> components are enclosed with dashed boxes. The paper treats the two components as layers. So then, we can replicate and stack each layer <strong>N</strong> times, as shown in the diagram. Additionally, the paper breaks the components into sub-layers, consisting of either a <strong>Multi-head Attention</strong> piece with <strong>Add &amp; Norm</strong> piece or a <strong>Feed Forward</strong> piece with <strong>Add &amp; Norm</strong> piece. Therefore, there are two sub-layers in the <strong>Encoding</strong> layer, and there are three sub-layers in the <strong>Decoding</strong> layer, as pointed out in the paper.</p>
<p>A glance at the architecture shows that all the pieces in the architecture are familiar, as we have covered them in previous chapters and previous sections of this chapter. For example, <strong>Normalization</strong> in <strong>Batch</strong> or <strong>Layer</strong> mode is discussed in <strong>CNN</strong>. Then, <strong>Feed Forward</strong> is covered in <strong>MLP</strong>. The discussion extends to <strong>CNN</strong>, <strong>ResNet</strong>, and <strong>RNN</strong> covering <strong>Back Propagation</strong> for training. We also covered <strong>Word Embeddings</strong> and <strong>Positional Embeddings</strong> in the previous sections.</p>
<p>The paper concludes that the need for <strong>RNN</strong> is not required in that <strong>Attention</strong> is all that is needed. The <strong>Transformer</strong> architecture re-introduces old concepts and, at the same time, introduces new concepts. Moreover, like <strong>CNN</strong> and other networks, different pieces can be put together to form all sorts of neural network designs. Therefore, this means that <strong>Attention</strong> and <strong>Positional encoding</strong> can be considered another set of new gadgets that we can keep in our toolbox. After all, there is no limit to how far deep we can go with neural networks in terms of architectural design, which requires perhaps just a little touch of creativity, among many others.</p>
</div>
</div>
<div id="applications-using-tnn-and-rnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.6</span> Applications using TNN (and RNN)<a href="deeplearning2.html#applications-using-tnn-and-rnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We know that <strong>RNN</strong> and <strong>TNN</strong> are the de facto modeling solutions for many sequence-based applications. However, among many other applications, a few typical applications demonstrate the effectiveness of neural networks, namely <strong>Speech recognition</strong>, <strong>Voice recognition</strong>, <strong>Emotion and Excitement recognition</strong>, <strong>Gesture recognition</strong>, and <strong>Handwriting recognition</strong>. This section first covers <strong>Speech Recognition</strong>, emphasizing feature extraction, which introduces <strong>MFCC</strong>. Then, it briefly introduces <strong>CTC</strong> used for alignment as an extension after <strong>RNN</strong> or <strong>TNN</strong> modeling. A good source of information for <strong>CTC</strong> is the <strong>HMM</strong> algorithms used in the <strong>Bayesian model</strong> section in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<div id="speech-recognition" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.6.1</span> Speech Recognition <a href="deeplearning2.html#speech-recognition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Smart Assistant</strong> applications like <strong>Siri</strong> and <strong>Alexa</strong> are known in the consumer world to date as applications that allow consumers to converse with electronic gadgets for assistance. This section describes one feature that is core to <strong>Smart Assistant</strong> applications, namely, <strong>Speech Recognition</strong>. We should note that taking the challenge of developing a <strong>speech recognition</strong> system (or maybe even a <strong>voice recognition</strong> system) is not a straightforward task. At the very least, an understanding of the science of sounds (<strong>Acoustics</strong>) and <strong>Signal Processing</strong> may help.</p>
</div>
<div id="mel-coefficients-feature-extraction" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.6.2</span> Mel Coefficients (Feature Extraction) <a href="deeplearning2.html#mel-coefficients-feature-extraction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Here, we start with the discussion of <strong>Feature Extraction</strong>. See Figure <a href="deeplearning2.html#fig:plpmfcc">13.32</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plpmfcc"></span>
<img src="plpmfcc.png" alt="Feature Extraction Method (Speech Recognition)" width="100%" />
<p class="caption">
Figure 13.32: Feature Extraction Method (Speech Recognition)
</p>
</div>
<p>The figure shows two alternative methods of extracting features given raw data in audio format. The first method is called <strong>Perceptual Linear Prediction (PLP)</strong> introduced by Hynek Hermansky <span class="citation">(<a href="bibliography.html#ref-ref1306h">1990</a>)</span>, and the second method is called <strong>Mel Frequency Cepstral Coefficients (MFCC)</strong>. Per Wikipedia, <strong>MFC</strong> is credited typically to Paul Mermelstein <span class="citation">(<a href="bibliography.html#ref-ref1315s">1980</a>)</span>, whereas he gives credits to Bridle and Brown for the idea. Other literature describes <strong>hybrid</strong> methods as alternatives, such as <strong>PLP-MFCC</strong> and <strong>PLP-RASTA</strong>. We leave readers to investigate those methods.</p>
<p>In whatever case may be, the function of the two methods in the diagram is to produce what we call a <strong>cepstrum</strong> - in the form of <strong>cepstral coefficients</strong>. In simple terms, <strong>MFCC</strong> produces <strong>39</strong> coefficients equivalent to <strong>39</strong> features (in vector format) that we can use as input into our <strong>Encoder-Decoder</strong> system for learning. Figure <a href="deeplearning2.html#fig:spectogram">13.33</a> illustrates that:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:spectogram"></span>
<img src="spectogram.png" alt="Spectogram" width="100%" />
<p class="caption">
Figure 13.33: Spectogram
</p>
</div>
<p>Ideally, when performing <strong>feature extraction</strong>, the raw data (<strong>audio input</strong>) is presented in <strong>sound waves</strong>, continuous in <strong>time-domain</strong>. Hereafter, let us use the term <strong>signal</strong> for <strong>sound waves</strong> in connection to the <strong>signal processing</strong> field. Now, it helps to review Fourier Transform in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>), in which we cover <strong>Time-Domain</strong> and <strong>Frequency-Domain</strong> (See <strong>Time and Frequency Domain</strong> Figure under <strong>Approximating using Fourier Series and Transform</strong> Section). The goal is to discretize the data using <strong>Fast Fourier Transformation (FFT)</strong>, translating the data into the <strong>frequency-domain</strong>. First, we discretize the sound wave into DFT-base chunks measured in <strong>Hertz</strong>, e.g., 20 milliseconds. Then, this discretized <strong>spectrum</strong> gets processed further via filters. The idea is to use filters and other forms of transformation to distinguish <strong>Human voice</strong> from the <strong>spectrum</strong> (e.g., Other literature suggests that the Human voice ranges between 40-60 decibels). Then, once the <strong>spectrum</strong> is filtered, it gets converted into <strong>cepstrum</strong> format.</p>
<p>Fundamentally, we follow the steps below using <strong>MFCC</strong> as our choice of feature extraction <span class="citation">(Muda L. et al., <a href="bibliography.html#ref-ref1435l">2010</a>; D Anggraeni et al. <a href="bibliography.html#ref-ref1446d">2018</a>; Maziar Raissi <a href="bibliography.html#ref-ref1421m">2021</a>)</span>. Note that, for consistency, our implementation compares results from functions such as <strong>powspec(.)</strong>, <strong>spec2cep(.)</strong>, and <strong>lifter(.)</strong> from <strong>tuneR</strong> library used in the steps mentioned by JÃ©rÃ´me Sueur <span class="citation">(<a href="bibliography.html#ref-ref1496j">2018</a>)</span>.</p>
<p><strong>First</strong>, let us use a third-party R library called <strong>tuneR</strong>. Then, we can use one of three popular datasets for our code, namely <strong>Timit</strong>, <strong>Dirha</strong>, and <strong>LibriSpeech</strong>.</p>
<ul>
<li><p><strong>Timit</strong> - a dataset presented as an <strong>Acoustic-Phonetic</strong> speech corpus which includes multiple dialects.</p></li>
<li><p><strong>Dirha</strong> - a dataset presented as a <strong>multi-microphone acoustic</strong> corpus.</p></li>
<li><p><strong>LibriSpeech</strong> - a dataset presented as a collection of <strong>audio book recordings</strong>.</p></li>
</ul>
<p>As suggested from other literature, a suitable dataset for better training is <strong>phonetic-rich</strong>. In other words, our overall system needs to recognize <strong>phonemes</strong> - distinct units of sound (Oxford definition).</p>
<p>For our demonstration, let us use the <strong>Timit</strong> dataset, which comes with sample audio for the train set and a separate set of sample audio for the test set. Each sample comprises a <strong>wav</strong> file (the actual audio recording), <strong>txt</strong> file (the text of the audio), and <strong>phn</strong> file (the corresponding phonemes).</p>
<p>The <strong>Timit</strong> corpus includes time-aligned orthographic, phonetic, and word transcriptions, including a 16-bit, 16kHz speech waveform file for each utterance.</p>
<p>An example of the text file follows:</p>
<div class="sourceCode" id="cb2177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2177-1" data-line-number="1">txt.file =<span class="st"> &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.TXT&quot;</span></a></code></pre></div>
<p><span class="math display">\[\begin{align}
\mathbf{\text{0 47104 She had your dark suit in greasy wash water all year.}}
\end{align}\]</span></p>
<p>That corresponds to the following <strong>phonemes</strong></p>

<div class="sourceCode" id="cb2178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2178-1" data-line-number="1">phn.file =<span class="st"> &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.PHN&quot;</span></a></code></pre></div>

<p><span class="math display">\[
\begin{array}{lll}
0 &amp; 2798 &amp; \text{h}\#\\
2798 &amp; 3960 &amp; \text{sh}\\
3960 &amp; 4649 &amp; \text{iy}\\
4649 &amp; 5200 &amp; \text{hv}\\
5200 &amp; 6477 &amp; \text{eh}\\
6477 &amp; 6959 &amp; \text{dcl}\\
6959 &amp; 7265 &amp; \text{d}\\
7265&amp;  7853 &amp; \text{y}\\
7853 &amp; 8920 &amp; \text{er}\\
\text{...}\\
43655 &amp; 47040 &amp; \text{h}\# \\
\end{array}
\]</span></p>
<p>Here, we use the corresponding audio.</p>

<div class="sourceCode" id="cb2179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2179-1" data-line-number="1"><span class="kw">library</span>(tuneR)</a>
<a class="sourceLine" id="cb2179-2" data-line-number="2">wav.file =<span class="st"> &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.WAV.wav&quot;</span></a>
<a class="sourceLine" id="cb2179-3" data-line-number="3">audio =<span class="st"> </span><span class="kw">readWave</span>(wav.file)</a>
<a class="sourceLine" id="cb2179-4" data-line-number="4">audio</a></code></pre></div>
<pre><code>## 
## Wave Object
##  Number of Samples:      47104
##  Duration (seconds):     2.94
##  Samplingrate (Hertz):   16000
##  Channels (Mono/Stereo): Mono
##  PCM (integer format):   TRUE
##  Bit (8/16/24/32/64):    16</code></pre>

<p>And with a <strong>Mac</strong> computer, we can play the audio like so:</p>

<div class="sourceCode" id="cb2181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2181-1" data-line-number="1"><span class="kw">setWavPlayer</span>(<span class="st">&#39;/usr/bin/afplay&#39;</span>)</a>
<a class="sourceLine" id="cb2181-2" data-line-number="2"><span class="kw">play</span>(audio)</a></code></pre></div>

<p>Let us review the structure of the audio data:</p>

<div class="sourceCode" id="cb2182"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2182-1" data-line-number="1"><span class="kw">str</span>(audio)</a></code></pre></div>
<pre><code>## Formal class &#39;Wave&#39; [package &quot;tuneR&quot;] with 6 slots
##   ..@ left     : int [1:47104] -6 -1 2 12 11 2 -1 6 7 6 ...
##   ..@ right    : num(0) 
##   ..@ stereo   : logi FALSE
##   ..@ samp.rate: int 16000
##   ..@ bit      : int 16
##   ..@ pcm      : logi TRUE</code></pre>

<p>In the wav file, we can see that only the <strong>left</strong> audio channel is recorded, and the <strong>right</strong> audio channel is empty. Also, the sampling rate per second is 16000 (or <strong>16kHz</strong>).</p>

<div class="sourceCode" id="cb2184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2184-1" data-line-number="1">signal        =<span class="st"> </span>audio<span class="op">@</span>left</a>
<a class="sourceLine" id="cb2184-2" data-line-number="2">sampling.rate =<span class="st"> </span>audio<span class="op">@</span>samp.rate         <span class="co"># samp rate in Hz</span></a>
<a class="sourceLine" id="cb2184-3" data-line-number="3">signal.length =<span class="st"> </span><span class="kw">length</span>(signal) <span class="op">/</span><span class="st"> </span>audio<span class="op">@</span>samp.rate</a></code></pre></div>

<p>Based on the structure, the length of our signal is 2.944 seconds based on (47104 <span class="math inline">\(/\)</span> 16000).</p>
<p><strong>Second</strong>, we perform <strong>Pre-emphasis</strong>, a pre-processing step to enhance the high-frequency portion of a signal. The following formula is typically used:</p>
<p><span class="math display">\[\begin{align}
y(n) = x(n) - \alpha x(n-1),\ \ \ \ \ \ 0.90 &lt; \alpha &lt; 1
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{x(n)}\)</span> is the <strong>signal</strong> at time <strong>n</strong> and <span class="math inline">\(\alpha\)</span> is a filtering constant.</p>

<div class="sourceCode" id="cb2185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2185-1" data-line-number="1">preemphasis &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">a =</span> <span class="dv">1</span>, x) {</a>
<a class="sourceLine" id="cb2185-2" data-line-number="2">    N =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb2185-3" data-line-number="3">    y =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb2185-4" data-line-number="4">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N) { y[n] =<span class="st"> </span>x[n] <span class="op">-</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>x[n<span class="dv">-1</span>] }</a>
<a class="sourceLine" id="cb2185-5" data-line-number="5">    y</a>
<a class="sourceLine" id="cb2185-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb2185-7" data-line-number="7">preprocessed.signal  =<span class="st"> </span><span class="kw">preemphasis</span>(<span class="dt">a=</span><span class="fl">0.97</span>, signal)</a>
<a class="sourceLine" id="cb2185-8" data-line-number="8"><span class="kw">head</span>(signal, <span class="dt">n=</span><span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1] -6 -1  2 12 11  2 -1  6  7  6 13 11 13  1  3  1  1
## [18]  5  3 -2</code></pre>
<div class="sourceCode" id="cb2187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2187-1" data-line-number="1"><span class="kw">head</span>(preprocessed.signal, <span class="dt">n=</span><span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1]   0.00   4.82   2.97  10.06  -0.64  -8.67  -2.94
##  [8]   6.97   1.18  -0.79   7.18  -1.61   2.33 -11.61
## [15]   2.03  -1.91   0.03   4.03  -1.85  -4.91</code></pre>

<p>For now, we skip normalization to show the full magnitude of our signal in the plots.</p>

<div class="sourceCode" id="cb2189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2189-1" data-line-number="1">normalized.signal =<span class="st"> </span>preprocessed.signal <span class="op">/</span><span class="st"> </span><span class="kw">max</span>(<span class="kw">abs</span>(preprocessed.signal))</a></code></pre></div>

<p>We also can use the <strong>preemphasis(.)</strong> from <strong>seewave</strong> library to validate consistency.</p>

<div class="sourceCode" id="cb2190"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2190-1" data-line-number="1"><span class="kw">library</span>(seewave)</a>
<a class="sourceLine" id="cb2190-2" data-line-number="2">preprocessed.audio =<span class="st"> </span>seewave<span class="op">::</span><span class="kw">preemphasis</span>(audio, <span class="dt">alpha=</span><span class="fl">0.97</span>, <span class="dt">output=</span><span class="st">&quot;Wave&quot;</span>)</a>
<a class="sourceLine" id="cb2190-3" data-line-number="3"><span class="kw">str</span>(preprocessed.audio)</a></code></pre></div>
<pre><code>## Formal class &#39;Wave&#39; [package &quot;tuneR&quot;] with 6 slots
##   ..@ left     : num [1:47104] 0 4.82 2.97 10.06 -0.64 ...
##   ..@ right    : num(0) 
##   ..@ stereo   : logi FALSE
##   ..@ samp.rate: int 16000
##   ..@ bit      : num 16
##   ..@ pcm      : logi TRUE</code></pre>
<div class="sourceCode" id="cb2192"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2192-1" data-line-number="1"><span class="kw">head</span>(preprocessed.audio<span class="op">@</span>left, <span class="dt">n=</span><span class="dv">20</span>)</a></code></pre></div>
<pre><code>##  [1]   0.00   4.82   2.97  10.06  -0.64  -8.67  -2.94
##  [8]   6.97   1.18  -0.79   7.18  -1.61   2.33 -11.61
## [15]   2.03  -1.91   0.03   4.03  -1.85  -4.91</code></pre>

<p><strong>Third</strong>, we perform <strong>Frame Blocking</strong>, which splits the signal into frames (or frequency bins) with a fixed frame size from a range of 20ms to 40ms. Other literature regards 25ms as standard. Because our signal is <strong>non-stationary</strong>, it helps to capture short-time intervals to highlight unique features within such a short period (e.g., <strong>harmonics</strong> or <strong>frequency tone</strong>). Here, we assume that given short intervals, we will be able to quantize signals into frames, each achieving some level of stationarity. And then, we also apply frame stride. This idea is similar to filters and strides in a convolutional network in that patches overlap due to strides.</p>
<p>Similarly, the stride causes the frames to overlap in the next <strong>Windowing</strong> step. Other literature may consider a 50% overlap as appropriate (or 12.5ms if the frame size is 25ms). Here, we throw the number as a 10ms frame step. Note that <strong>stride size</strong> is also called <strong>window shift</strong> in other literature or <strong>hop length</strong> in other implementations. Therefore, given that, and assume we have a signal that runs for 2.94 seconds, we should now be able to calculate the following:</p>
<p><span class="math display">\[\begin{align}
\text{(num of samples per frame)} = N = \frac{25}{1000 } \times 16000 = 400 \text{ sample points}, s(n) \in \mathbb{R}^{400}
\end{align}\]</span></p>
<p>where <strong>n</strong> is the nth frame in the range <span class="math inline">\(0 &lt; n &lt; N - 1\)</span>.</p>
<p><span class="math display">\[\begin{align}
\text{(sample points per frame step)} = \frac{12.5}{1000 } \times 16000 = 200  \text{ sample points}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{(duration of signal)} = 2.94  \times 16000 = 47040 \text{ sample points} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\text{(num of frames)} = N = \frac{(47040 - 400)}{200} = 234 \text{ frames}   
\ \ \ \text{(ceiling)}
\end{align}\]</span></p>
<p>We can pad the last frame with zeroes as a round-off to align to the frame size.</p>
<p>Therefore, our frame structure should show as <span class="math inline">\(\mathbf{s} \in \mathbb{R}^{234 \times 400}\)</span>, arranged such that if <span class="math inline">\(\mathbf{s}\)</span> is a matrix, it has 234 rows (frames) and 400 columns (dimensions).</p>

<div class="sourceCode" id="cb2194"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2194-1" data-line-number="1">frames &lt;-<span class="st"> </span><span class="cf">function</span>(duration.size, duration.stride) {</a>
<a class="sourceLine" id="cb2194-2" data-line-number="2">  frm.size     =<span class="st"> </span>duration.size <span class="op">*</span><span class="st"> </span>sampling.rate</a>
<a class="sourceLine" id="cb2194-3" data-line-number="3">  frm.stride   =<span class="st"> </span>duration.stride <span class="op">*</span><span class="st"> </span>sampling.rate </a>
<a class="sourceLine" id="cb2194-4" data-line-number="4">  <span class="cf">if</span> (frm.size )</a>
<a class="sourceLine" id="cb2194-5" data-line-number="5">  <span class="kw">list</span>(<span class="st">&quot;size&quot;</span> =<span class="st"> </span>frm.size, <span class="st">&quot;stride&quot;</span> =<span class="st"> </span>frm.stride, </a>
<a class="sourceLine" id="cb2194-6" data-line-number="6">       <span class="st">&quot;duration&quot;</span> =<span class="st"> </span>duration.size, <span class="st">&quot;stride.time&quot;</span> =<span class="st"> </span>duration.stride)</a>
<a class="sourceLine" id="cb2194-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2194-8" data-line-number="8"><span class="co"># 12.5ms = 50% overlap</span></a>
<a class="sourceLine" id="cb2194-9" data-line-number="9">frm =<span class="st"> </span><span class="kw">frames</span>(<span class="dt">duration.size =</span> <span class="fl">0.025</span>, <span class="dt">duration.stride =</span> <span class="fl">0.0125</span>)</a>
<a class="sourceLine" id="cb2194-10" data-line-number="10"><span class="kw">t</span>(<span class="kw">c</span>(frm))</a></code></pre></div>
<pre><code>##      size stride duration stride.time
## [1,] 400  200    0.025    0.0125</code></pre>
<div class="sourceCode" id="cb2196"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2196-1" data-line-number="1">(<span class="dt">N =</span> <span class="dt">num_of_frames =</span> <span class="kw">round</span>((signal.length <span class="op">*</span><span class="st"> </span>sampling.rate <span class="op">-</span><span class="st"> </span>frm<span class="op">$</span>size) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2196-2" data-line-number="2"><span class="st">                             </span>frm<span class="op">$</span>stride))</a></code></pre></div>
<pre><code>## [1] 234</code></pre>

<p>To achieve total efficiency of <strong>FFT</strong> calculation later, it helps to choose a frame with a size in multiples of power of two. So the size closer to that is 512. Being so, we adjust the other hyperparameters, therefore:</p>

<div class="sourceCode" id="cb2198"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2198-1" data-line-number="1">frm<span class="op">$</span>size =<span class="st"> </span><span class="dv">512</span></a>
<a class="sourceLine" id="cb2198-2" data-line-number="2">frm<span class="op">$</span>stride =<span class="st"> </span><span class="dv">256</span></a>
<a class="sourceLine" id="cb2198-3" data-line-number="3">frm<span class="op">$</span>duration =<span class="st"> </span>frm<span class="op">$</span>size <span class="op">/</span><span class="st"> </span>audio<span class="op">@</span>samp.rate</a>
<a class="sourceLine" id="cb2198-4" data-line-number="4">frm<span class="op">$</span>stride.time =<span class="st">  </span>frm<span class="op">$</span>stride <span class="op">/</span><span class="st"> </span>audio<span class="op">@</span>samp.rate</a>
<a class="sourceLine" id="cb2198-5" data-line-number="5"><span class="kw">t</span>(<span class="kw">c</span>(frm))</a></code></pre></div>
<pre><code>##      size stride duration stride.time
## [1,] 512  256    0.032    0.016</code></pre>

<p>The number of frames is then:</p>

<div class="sourceCode" id="cb2200"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2200-1" data-line-number="1">(<span class="dt">N =</span> <span class="dt">num_of_frames =</span> <span class="kw">round</span>((signal.length <span class="op">*</span><span class="st"> </span>sampling.rate <span class="op">-</span><span class="st"> </span>frm<span class="op">$</span>size) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb2200-2" data-line-number="2"><span class="st">                             </span>frm<span class="op">$</span>stride))</a></code></pre></div>
<pre><code>## [1] 182</code></pre>

<p>Actual implementations may choose to pad the edges of the frame with zero frequencies to align with the length of the signal correctly.</p>
<p>Next, we generate a matrix to hold our frames. Each row corresponds to a frame of size 512.</p>

<div class="sourceCode" id="cb2202"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2202-1" data-line-number="1">s =<span class="st"> </span>frm.matrix =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>N, <span class="dt">ncol=</span>frm<span class="op">$</span>size)</a>
<a class="sourceLine" id="cb2202-2" data-line-number="2"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb2202-3" data-line-number="3"> start =<span class="st"> </span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>frm<span class="op">$</span>stride <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2202-4" data-line-number="4"> end   =<span class="st"> </span>start <span class="op">+</span><span class="st"> </span>frm<span class="op">$</span>size <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb2202-5" data-line-number="5"> s[n,] =<span class="st"> </span>preprocessed.signal[start<span class="op">:</span>end]</a>
<a class="sourceLine" id="cb2202-6" data-line-number="6">}</a></code></pre></div>

<p>Let us plot the Frequency-Time (see Figure <a href="deeplearning2.html#fig:freqaudio1">13.34</a>):</p>

<div class="sourceCode" id="cb2203"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2203-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2203-2" data-line-number="2">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(signal)); y =<span class="st"> </span>signal</a>
<a class="sourceLine" id="cb2203-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Time (Entire Signal)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, </a>
<a class="sourceLine" id="cb2203-4" data-line-number="4">     <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2203-5" data-line-number="5">     <span class="dt">main =</span> <span class="st">&quot;Audio Wave (Amplitude-Time)&quot;</span>, <span class="dt">type =</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2203-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2203-7" data-line-number="7">y =<span class="st"> </span><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a>
<a class="sourceLine" id="cb2203-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(s[<span class="dv">1</span>,])); y =<span class="st"> </span>s[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb2203-9" data-line-number="9"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Time (1st Frame)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, </a>
<a class="sourceLine" id="cb2203-10" data-line-number="10">     <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2203-11" data-line-number="11">     <span class="dt">main =</span> <span class="st">&quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2203-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2203-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freqaudio1"></span>
<img src="DS_files/figure-html/freqaudio1-1.png" alt="Audio Wave (Amplitude-Time)" width="100%" />
<p class="caption">
Figure 13.34: Audio Wave (Amplitude-Time)
</p>
</div>

<p><strong>Fourth</strong>, we apply <strong>Windowing</strong> to prevent FFT from hitting the known <strong>spectral leakage</strong> or avoid the <strong>edge effect</strong> issue in which minor frequencies tend to leak to other frames because of the overlap (see strides) <span class="citation">(Lyon D. <a href="bibliography.html#ref-ref1450d">2009</a>)</span>. One of two popular windowing techniques is the <strong>Hamming window</strong>, which is expressed below (see Figure <a href="deeplearning2.html#fig:hamming">13.35</a>): </p>
<p><span class="math display">\[\begin{align}
w[n] = \frac{25}{46} - \frac{21}{46} \ \mathbf{cos} \left( \frac{2\pi n}{N-1}\right)= 0.54 - 0.46\ \mathbf{cos} \left( \frac{2\pi n}{N-1}\right),  0 \le n \le N- 1 
\end{align}\]</span></p>
<p>where N = window size.</p>
<div class="sourceCode" id="cb2204"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2204-1" data-line-number="1">hamming.window &lt;-<span class="cf">function</span>(N) {</a>
<a class="sourceLine" id="cb2204-2" data-line-number="2">   n =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, N<span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb2204-3" data-line-number="3">   <span class="fl">0.54</span> <span class="op">-</span><span class="st"> </span><span class="fl">0.46</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>( ( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>n ) <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) )</a>
<a class="sourceLine" id="cb2204-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb2204-5" data-line-number="5">w =<span class="st"> </span>window =<span class="st"> </span><span class="kw">hamming.window</span>(<span class="dt">N =</span> frm<span class="op">$</span>size)</a></code></pre></div>

<div class="sourceCode" id="cb2205"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2205-1" data-line-number="1">f1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, frm<span class="op">$</span>size<span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb2205-2" data-line-number="2">f2 =<span class="st"> </span>f1 <span class="op">+</span><span class="st"> </span>frm<span class="op">$</span>stride</a>
<a class="sourceLine" id="cb2205-3" data-line-number="3">f3 =<span class="st"> </span>f2 <span class="op">+</span><span class="st"> </span>frm<span class="op">$</span>stride</a>
<a class="sourceLine" id="cb2205-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(f1,f3), <span class="dt">ylim=</span><span class="kw">range</span>(w),   </a>
<a class="sourceLine" id="cb2205-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;Samples&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Amplitude&quot;</span>,   </a>
<a class="sourceLine" id="cb2205-6" data-line-number="6">      <span class="dt">main=</span><span class="st">&quot;Overlapping Hamming Window with Stride&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2205-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2205-8" data-line-number="8"><span class="kw">lines</span>(f1, w, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2205-9" data-line-number="9"><span class="kw">lines</span>(f2, w, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2205-10" data-line-number="10"><span class="kw">lines</span>(f3, w, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hamming"></span>
<img src="DS_files/figure-html/hamming-1.png" alt="Overlapping Hamming Window with Stride" width="70%" />
<p class="caption">
Figure 13.35: Overlapping Hamming Window with Stride
</p>
</div>

<p>Note that each Window shows a bell-shaped curve, a smoothed representation of its main lobe and side lobes of the Window.</p>
<p>The other Window is called <strong>Hanning Window</strong> with the following formula: </p>
<p><span class="math display">\[\begin{align}
w[n] = 0.50 - 0.50\ \mathbf{cos} \left( \frac{2\pi n}{N-1}\right), 0 \le n \le N - 1
\end{align}\]</span></p>
<p>Note that we skip derivations of those formulas - we leave readers to investigate the use of 0.54 and 0.46 constants for <strong>Hamming Window</strong> compared to other windows. We also leave readers to investigate other types of windowing, such as <strong>Blackman window</strong>.</p>
<p>We then multiply our signal with the Window in a row-wise fashion like so (see Figure <a href="deeplearning2.html#fig:freqaudio2">13.36</a>):</p>

<div class="sourceCode" id="cb2206"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2206-1" data-line-number="1">windowed.frm =<span class="st"> </span><span class="kw">sweep</span>(s, <span class="dv">2</span>, w, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2206-2" data-line-number="2"><span class="kw">str</span>(windowed.frm)</a></code></pre></div>
<pre><code>##  num [1:182, 1:512] 0 0.0288 -0.4512 -0.3296 0.4944 ...</code></pre>
<div class="sourceCode" id="cb2208"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2208-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2208-2" data-line-number="2">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(s[<span class="dv">1</span>,])); y =<span class="st"> </span>s[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb2208-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Time (1st Unwindowed Frame)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, </a>
<a class="sourceLine" id="cb2208-4" data-line-number="4">     <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2208-5" data-line-number="5">     <span class="dt">main =</span> <span class="st">&quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2208-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2208-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a>
<a class="sourceLine" id="cb2208-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(windowed.frm[<span class="dv">1</span>,])); y =<span class="st"> </span>windowed.frm[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb2208-9" data-line-number="9"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Time (1st Windowed Frame)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, </a>
<a class="sourceLine" id="cb2208-10" data-line-number="10">     <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2208-11" data-line-number="11">     <span class="dt">main =</span> <span class="st">&quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2208-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2208-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freqaudio2"></span>
<img src="DS_files/figure-html/freqaudio2-1.png" alt="Audio Wave (Amplitude-Time)" width="100%" />
<p class="caption">
Figure 13.36: Audio Wave (Amplitude-Time)
</p>
</div>

<p><strong>Fifth</strong>, we then calculate the <strong>DFT (Discrete Fourier Transform)</strong> for each frame using <strong>FFT</strong>. Recall our example implementation of FFT using the <strong>Cooley-Tukey</strong> algorithm in Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>). It helps to revisit the topic around <strong>Time-Domain</strong> and <strong>Frequency-Domain</strong> in the Chapter mentioned, which covers the equation below to convert our signal to the frequency in the frequency domain:   </p>
<p><span class="math display">\[\begin{align}
\tilde{s}(k) = \sum_{n=0}^{N-1} s_k(n) h(n)e^{-j2\pi nk / N}, \ \ \ \ \ \ 0 \le k &lt; K - 1
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{s_k}(n) \in \mathbb{R}^{400}\)</span> which is the kth frame (row-wise) in our frame matrix and <span class="math inline">\(\mathbf{h}(n) \in \mathbb{R}^{400}\)</span> which is our window. Note that we have already calculated our windowed frames, each denoted as:</p>
<p><span class="math display">\[\begin{align}
w(n) = s_k(n) h(n)\ \ \ \leftarrow\ \ \  \ \text{(windowed frame)}
\end{align}\]</span></p>
<p>Our goal now is to derive a <strong>spectrum of frequencies</strong> per frame denoted by <span class="math inline">\(\tilde{s}(k) \in \mathbb{C}^K\)</span> where <strong>K</strong> is the <strong>N-point FFT</strong>, also called <strong>DFT coefficients</strong>, which is in complex-number format. Other implementations may denote this as <strong>nfft</strong> or <strong>n_fft</strong> which is typically set either as 256 or 512.  </p>
<p>Here, we calculate <strong>DFT</strong> using the <strong>fft(.)</strong> function in R. It helps to note that the first element in the vector produced by the calculation is the <strong>DC component</strong> (refer to signal processing) which is the average of the entire signal (or the samples in a frame). In considering the <strong>DC component</strong> (without disregarding it), other literature may suggest subtracting each sample from the average mean before taking the DFTs.</p>

<div class="sourceCode" id="cb2209"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2209-1" data-line-number="1">NFFT       =<span class="st"> </span>frm<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2209-2" data-line-number="2">frame1     =<span class="st"> </span>windowed.frm[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb2209-3" data-line-number="3">frame1     =<span class="st"> </span>frame1 <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(frame1)   <span class="co"># Zero-out DC Component to center data</span></a></code></pre></div>

<p>However, we skip this extra step to conform with the <strong>powspec(.)</strong> function.</p>
<p>We also calculate the <strong>amplitude or magnitude</strong> of the frequency. Because the result of <strong>fft(.)</strong> is a set of complex numbers, we show four different ways to calculate the magnitude - given the four equations below:</p>
<p><span class="math display">\[\begin{align}
\text{mag}(x) = \sqrt{Real(x)^2 + Imaginary(x)^2}  = |x| = Mod(x) = \sqrt{x} \times \text{Conj}(\sqrt{x})
\end{align}\]</span></p>
<p>Note that the symbol <span class="math inline">\(|.|\)</span> is either interpreted as the absolute value or the modulus of the complex value. For example:</p>

<div class="sourceCode" id="cb2210"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2210-1" data-line-number="1">cplx.num =<span class="st"> </span><span class="dv">24</span> <span class="op">+</span><span class="st"> </span>3i</a>
<a class="sourceLine" id="cb2210-2" data-line-number="2">magnitude &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">Re</span>(x)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">Im</span>(x)<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb2210-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;this&quot;</span> =<span class="st"> </span><span class="kw">magnitude</span>(cplx.num), <span class="st">&quot;this&quot;</span> =<span class="st"> </span><span class="kw">abs</span>(cplx.num), <span class="st">&quot;this&quot;</span> =<span class="st"> </span><span class="kw">Mod</span>(cplx.num),</a>
<a class="sourceLine" id="cb2210-4" data-line-number="4">  <span class="st">&quot;or this&quot;</span> =<span class="st"> </span><span class="kw">sqrt</span>(cplx.num)<span class="op">*</span><span class="kw">Conj</span>(<span class="kw">sqrt</span>(cplx.num)))</a></code></pre></div>
<pre><code>##         this         this         this      or this 
## 24.186773+0i 24.186773+0i 24.186773+0i 24.186773+0i</code></pre>

<p>Note that if we exclude the <strong>DC component</strong>, then the first half of the FFT vector is symmetric to the second half of the vector, just by comparing the raw magnitude. We can show this by comparing the symmetry. See the following:</p>

<div class="sourceCode" id="cb2212"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2212-1" data-line-number="1">NFFT       =<span class="st"> </span>frm<span class="op">$</span>size</a>
<a class="sourceLine" id="cb2212-2" data-line-number="2">frame1     =<span class="st"> </span>windowed.frm[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb2212-3" data-line-number="3">dft.coeff1 =<span class="st"> </span><span class="kw">fft</span>(frame1)</a>
<a class="sourceLine" id="cb2212-4" data-line-number="4">dft.mag1    =<span class="st"> </span><span class="kw">magnitude</span>(dft.coeff1)</a>
<a class="sourceLine" id="cb2212-5" data-line-number="5">half        =<span class="st"> </span><span class="kw">floor</span>(NFFT <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2212-6" data-line-number="6">first.half  =<span class="st"> </span>dft.mag1[<span class="dv">2</span><span class="op">:</span>half]          </a>
<a class="sourceLine" id="cb2212-7" data-line-number="7">second.half =<span class="st"> </span><span class="kw">rev</span>(dft.mag1[half<span class="op">:</span>NFFT]) <span class="co"># reverse the second half</span></a>
<a class="sourceLine" id="cb2212-8" data-line-number="8"><span class="kw">rbind</span>(first.half[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>], second.half[<span class="dv">1</span><span class="op">:</span><span class="dv">5</span>])</a></code></pre></div>
<pre><code>##           [,1]      [,2]      [,3]      [,4]      [,5]
## [1,] 7.7292003 0.8443712 1.7062433 7.6443928 3.8443819
## [2,] 7.7292003 0.8443712 1.7062433 7.6443928 3.8443819</code></pre>
<div class="sourceCode" id="cb2214"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2214-1" data-line-number="1"><span class="kw">setequal</span>(<span class="kw">round</span>(first.half,<span class="dv">8</span>),<span class="kw">round</span>(second.half,<span class="dv">8</span>))</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>See the magnitude plot for the <strong>DFT</strong> in Figure <a href="deeplearning2.html#fig:freqaudio3">13.37</a>.</p>

<div class="sourceCode" id="cb2216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2216-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb2216-2" data-line-number="2">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(dft.mag1)); y =<span class="st"> </span><span class="kw">Re</span>(dft.mag1)</a>
<a class="sourceLine" id="cb2216-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Coefficients (1st Windowed Frame)&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, </a>
<a class="sourceLine" id="cb2216-4" data-line-number="4">     <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2216-5" data-line-number="5">     <span class="dt">main =</span> <span class="st">&quot;DFT (Amplitude-Coefficient)&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2216-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2216-7" data-line-number="7"><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a>
<a class="sourceLine" id="cb2216-8" data-line-number="8">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(first.half)); y =<span class="st"> </span><span class="kw">Re</span>(first.half)</a>
<a class="sourceLine" id="cb2216-9" data-line-number="9"><span class="kw">plot</span>(<span class="ot">NULL</span>,  <span class="dt">xlab =</span> <span class="st">&quot;Coefficients (1st Half of the 1st Windowed Frame)&quot;</span>, </a>
<a class="sourceLine" id="cb2216-10" data-line-number="10">     <span class="dt">ylab =</span> <span class="st">&quot;Amplitude&quot;</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb2216-11" data-line-number="11">     <span class="dt">main =</span> <span class="st">&quot;DFT (Amplitude-Coefficient)&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2216-12" data-line-number="12"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2216-13" data-line-number="13"><span class="kw">lines</span>(x, y, <span class="dt">col =</span> <span class="st">&quot;darkgreen&quot;</span> )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freqaudio3"></span>
<img src="DS_files/figure-html/freqaudio3-1.png" alt="DFT (Amplitude-Coefficient)" width="100%" />
<p class="caption">
Figure 13.37: DFT (Amplitude-Coefficient)
</p>
</div>

<p>Next, we calculate the <strong>Power Spectrum</strong> for each short frame, also called <strong>Short-Time Fourier Transform (STFT)</strong> using the below formula (without the normalizer <strong>N</strong>).   </p>
<p><span class="math display">\[\begin{align}
p_k = |\text{FFT}(\tilde{s}_k)|^2
\end{align}\]</span></p>
<p>Let us calculate the power spectrum of each frame in the windowed frames. Here, we take the first half of the spectrum, including the <strong>DC component</strong>. </p>

<div class="sourceCode" id="cb2217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2217-1" data-line-number="1">half =<span class="st"> </span><span class="kw">floor</span>(NFFT <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2217-2" data-line-number="2">P =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(half, N))</a>
<a class="sourceLine" id="cb2217-3" data-line-number="3"><span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb2217-4" data-line-number="4">  frame =<span class="st"> </span>windowed.frm[n,]</a>
<a class="sourceLine" id="cb2217-5" data-line-number="5">  frame =<span class="st"> </span><span class="kw">fft</span>(frame)</a>
<a class="sourceLine" id="cb2217-6" data-line-number="6">  P[,n] =<span class="st"> </span>frame[<span class="dv">1</span><span class="op">:</span>half]</a>
<a class="sourceLine" id="cb2217-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2217-8" data-line-number="8">my.powspectrum =<span class="st"> </span>P =<span class="st"> </span><span class="kw">magnitude</span>(P)<span class="op">^</span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb2217-9" data-line-number="9"><span class="kw">str</span>(my.powspectrum )</a></code></pre></div>
<pre><code>##  num [1:256, 1:182] 479.088 59.741 0.713 2.911 58.437 ...</code></pre>

<p>We get a matrix with column-vector for the <strong>spectra</strong> and row-vector for the <strong>frequencies</strong> per <strong>spectrum</strong>.</p>
<p>Equivalently, we can also just use a function called <strong>powspec(.)</strong> from <strong>tuneR</strong> library. Note that <strong>powspec(.)</strong> already computes for the window. Thus we use the original pre-emphasized signal instead.</p>

<div class="sourceCode" id="cb2219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2219-1" data-line-number="1">tuneR.powspectrum =<span class="st"> </span><span class="kw">powspec</span>(preprocessed.audio<span class="op">@</span>left, <span class="dt">sr =</span> audio<span class="op">@</span>samp.rate, </a>
<a class="sourceLine" id="cb2219-2" data-line-number="2">                    <span class="dt">wintime =</span> frm<span class="op">$</span>duration, <span class="dt">steptime =</span> frm<span class="op">$</span>stride.time)</a>
<a class="sourceLine" id="cb2219-3" data-line-number="3"><span class="kw">str</span>(tuneR.powspectrum)</a></code></pre></div>
<pre><code>##  num [1:256, 1:182] 479.088 59.741 0.713 2.911 58.437 ...</code></pre>

<p>We can now plot the periodogram for the frame. See Figure <a href="deeplearning2.html#fig:periodogram">13.38</a>. We shifted the second spectrum about one step to the right to show that both spectra have the same exact magnitude.</p>

<div class="sourceCode" id="cb2221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2221-1" data-line-number="1">y1 =<span class="st"> </span>my.powspectrum[,<span class="dv">1</span>]    <span class="co"># 1st DFT</span></a>
<a class="sourceLine" id="cb2221-2" data-line-number="2">y2 =<span class="st"> </span>tuneR.powspectrum[,<span class="dv">1</span>] <span class="co"># 1st DFT</span></a>
<a class="sourceLine" id="cb2221-3" data-line-number="3">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(y1))</a>
<a class="sourceLine" id="cb2221-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y1),   </a>
<a class="sourceLine" id="cb2221-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;(Xm) Frequency (Bin)&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Amplitude (Magnitude)&quot;</span>,   </a>
<a class="sourceLine" id="cb2221-6" data-line-number="6">      <span class="dt">main=</span><span class="st">&quot;Periodogram (Spectrum)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2221-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2221-8" data-line-number="8"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;green&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2221-9" data-line-number="9"><span class="kw">lines</span>(x<span class="op">+</span><span class="dv">1</span>, y2, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">type=</span><span class="st">&quot;l&quot;</span>)</a>
<a class="sourceLine" id="cb2221-10" data-line-number="10"><span class="kw">legend</span>(<span class="dv">170</span>,<span class="dv">80500</span>, <span class="dt">inset=</span>.<span class="dv">02</span>,  <span class="kw">c</span>(<span class="st">&quot;my.powspectrum&quot;</span>, <span class="st">&quot;tuneR.powspectrum&quot;</span>), </a>
<a class="sourceLine" id="cb2221-11" data-line-number="11">       <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;green&quot;</span>, <span class="st">&quot;brown&quot;</span>),  <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:periodogram"></span>
<img src="DS_files/figure-html/periodogram-1.png" alt="Periodogram (Spectrum)" width="70%" />
<p class="caption">
Figure 13.38: Periodogram (Spectrum)
</p>
</div>

<p><strong>Sixth</strong>, we then use a <strong>Filter Bank</strong>, which, in <strong>Signal Processing</strong>, is described as a segmentation of input signals into individual <strong>analysis</strong> signals, each carrying a <strong>band</strong> of frequencies. A good illustration can be seen in Figure <a href="deeplearning2.html#fig:filterbanks">13.39</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:filterbanks"></span>
<img src="filterbanks.png" alt="Filter Bank Structure" width="80%" />
<p class="caption">
Figure 13.39: Filter Bank Structure
</p>
</div>

<p>There are different types of <strong>Filter Banks</strong> in <strong>Signal Processing</strong> namely, <strong>Gabor Filter Banks</strong>, <strong>Polyphase Filter Banks</strong>, <strong>DCT Filter Banks</strong>, <strong>Mel Filter Banks</strong>, etc. A good reference comes from electrical4u.com, providing introductory concepts of each <strong>Filter bank</strong>. However, in this section, we focus on <strong>Mel Filter Bank</strong>, which is tailored more for speech.</p>
<p>The core component of <strong>MFCC</strong> is the <strong>Mel Filter Bank</strong>. Other literature uses <strong>human perception of sound</strong> to explain that the <strong>Mel Filter Bank</strong> mimics how our <strong>inner ears</strong> consist of a set of <strong>hair cells</strong> that filter human sound at lower frequencies (e.g., 40-60 dB). Similarly, <strong>Mel Filter Bank</strong> forms a set of triangular filters for filtering. See Figure <a href="deeplearning2.html#fig:melfilter">13.41</a>. </p>
<p>The idea is to convert the <strong>Power Spectrum</strong> into <strong>Mel scale (Mels)</strong> - named by Stevens, Volkman, and Newman in 1937 (Wikipedia). We use the below Mel scale formulas:</p>
<ul>
<li>From Frequency to Mel scale (where f is the frequency in Hz):</li>
</ul>
<p><span class="math display">\[\begin{align}
m = \mathbf{mels}(f) = 2595 \times \log_{10}\left( 1 + \frac{f}{700}\right) = 1127.01048 \times \log_e \left( 1 + \frac{f}{700}\right) 
\end{align}\]</span></p>
<ul>
<li>From Mel scale to Frequency (where m is the mel scale):</li>
</ul>
<p><span class="math display">\[\begin{align}
f = \mathbf{freq}(m) = 700 \times\left(pow \left(10,\frac{m}{2595}\right) - 1\right) = 700 \times\left(exp\left(\frac{m}{1127.01048}\right) - 1\right)
\end{align}\]</span></p>
<p>As for the Mel constant and logarithm used, we leave readers to see <strong>Fitting the Mel scale</strong> <span class="citation">(Umesh, S., Cohen, L., &amp; Nelson, D. J. <a href="bibliography.html#ref-ref1439s">1999</a>)</span>.</p>
<p>Below is an example implementation of the conversions.</p>

<div class="sourceCode" id="cb2222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2222-1" data-line-number="1">hertz.to.mel &lt;-<span class="st"> </span><span class="cf">function</span>(f) { <span class="dv">2595</span> <span class="op">*</span><span class="st"> </span><span class="kw">log10</span> ( <span class="dv">1</span> <span class="op">+</span><span class="st"> </span>f<span class="op">/</span><span class="dv">700</span>) }</a>
<a class="sourceLine" id="cb2222-2" data-line-number="2">mel.to.hertz &lt;-<span class="st"> </span><span class="cf">function</span>(m) { <span class="dv">700</span> <span class="op">*</span><span class="st"> </span>( <span class="dv">10</span><span class="op">^</span>( m <span class="op">/</span><span class="st"> </span><span class="dv">2595</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb2222-3" data-line-number="3"><span class="co">### Or we can use the alternative formulae</span></a>
<a class="sourceLine" id="cb2222-4" data-line-number="4"><span class="co">#hertz.to.mel &lt;- function(f) {  1127.01048 * log(1 + f / 700, base=exp(1)) }</span></a>
<a class="sourceLine" id="cb2222-5" data-line-number="5"><span class="co">#mel.to.hertz &lt;- function(m) { 700 * (exp(m/1127.01048) - 1) }</span></a>
<a class="sourceLine" id="cb2222-6" data-line-number="6">m =<span class="st"> </span><span class="kw">hertz.to.mel</span>(<span class="dt">f=</span><span class="dv">512</span>)</a>
<a class="sourceLine" id="cb2222-7" data-line-number="7">f =<span class="st"> </span><span class="kw">mel.to.hertz</span>(m)</a>
<a class="sourceLine" id="cb2222-8" data-line-number="8"><span class="kw">c</span>(<span class="st">&quot;m&quot;</span> =<span class="st"> </span>m, <span class="st">&quot;f&quot;</span> =<span class="st"> </span>f)</a></code></pre></div>
<pre><code>##         m         f 
## 618.65988 512.00000</code></pre>

<p>For an illustration, let us review the Mel-to-Frequency plot using Figure <a href="deeplearning2.html#fig:melfreq">13.40</a>.</p>

<div class="sourceCode" id="cb2224"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2224-1" data-line-number="1">f =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10000</span>)</a>
<a class="sourceLine" id="cb2224-2" data-line-number="2">m =<span class="st"> </span><span class="kw">hertz.to.mel</span>(f)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:melfreq"></span>
<img src="DS_files/figure-html/melfreq-1.png" alt="Mel-Frequency Plot" width="70%" />
<p class="caption">
Figure 13.40: Mel-Frequency Plot
</p>
</div>

<p>A way to look at <strong>Mel filter bank</strong> is with the plot Figure <a href="deeplearning2.html#fig:melfilter">13.41</a> using the below function, namely <strong>mel.filterbank(.)</strong>, to generate the center Mel frequencies (motivated by a Python script from scottlawonbc (github)):</p>

<div class="sourceCode" id="cb2225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2225-1" data-line-number="1">mel.filterbank &lt;-<span class="st"> </span><span class="cf">function</span>(nbands, freq.min, freq.max) {</a>
<a class="sourceLine" id="cb2225-2" data-line-number="2">  mel.max    =<span class="st"> </span><span class="kw">hertz.to.mel</span>(freq.max) <span class="co"># or hz2mel(freq.max, htk=TRUE)</span></a>
<a class="sourceLine" id="cb2225-3" data-line-number="3">  mel.min    =<span class="st"> </span><span class="kw">hertz.to.mel</span>(freq.min) <span class="co"># or hz2mel(freq.min, htk=TRUE) </span></a>
<a class="sourceLine" id="cb2225-4" data-line-number="4">  mel.delta  =<span class="st"> </span><span class="kw">abs</span>(mel.max <span class="op">-</span><span class="st"> </span>mel.min) <span class="op">/</span><span class="st"> </span>(nbands <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2225-5" data-line-number="5">  mel.freq   =<span class="st"> </span>mel.min <span class="op">+</span><span class="st"> </span>mel.delta <span class="op">*</span><span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, nbands <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)  </a>
<a class="sourceLine" id="cb2225-6" data-line-number="6">  mel.lower  =<span class="st"> </span><span class="kw">head</span>(mel.freq, <span class="dv">-2</span>)</a>
<a class="sourceLine" id="cb2225-7" data-line-number="7">  mel.upper  =<span class="st"> </span><span class="kw">tail</span>(mel.freq, <span class="dv">-2</span>)</a>
<a class="sourceLine" id="cb2225-8" data-line-number="8">  mel.center =<span class="st"> </span><span class="kw">tail</span>(<span class="kw">head</span>(mel.freq,<span class="op">-</span><span class="dv">1</span>),<span class="op">-</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2225-9" data-line-number="9">  <span class="kw">list</span>( <span class="st">&quot;lower&quot;</span> =<span class="st"> </span>mel.lower, <span class="st">&quot;center&quot;</span> =<span class="st"> </span>mel.center, <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>mel.upper,</a>
<a class="sourceLine" id="cb2225-10" data-line-number="10">        <span class="st">&quot;banks&quot;</span> =<span class="st"> </span>mel.freq)</a>
<a class="sourceLine" id="cb2225-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb2225-12" data-line-number="12">nbands =<span class="st"> </span><span class="dv">7</span>; fmin =<span class="st"> </span><span class="dv">300</span>; fmax =<span class="st"> </span><span class="dv">8000</span></a>
<a class="sourceLine" id="cb2225-13" data-line-number="13">(<span class="dt">melfb =</span> <span class="kw">mel.filterbank</span>(<span class="dt">nbands=</span>nbands, <span class="dt">freq.min=</span>fmin, <span class="dt">freq.max=</span>fmax))</a></code></pre></div>
<pre><code>## $lower
## [1]  401.97059  706.72714 1011.48370 1316.24026
## [5] 1620.99682 1925.75337 2230.50993
## 
## $center
## [1]  706.72714 1011.48370 1316.24026 1620.99682
## [5] 1925.75337 2230.50993 2535.26649
## 
## $upper
## [1] 1011.4837 1316.2403 1620.9968 1925.7534 2230.5099
## [6] 2535.2665 2840.0230
## 
## $banks
## [1]  401.97059  706.72714 1011.48370 1316.24026
## [5] 1620.99682 1925.75337 2230.50993 2535.26649
## [9] 2840.02305</code></pre>

<p>Below are the corresponding Hz frequencies:</p>

<div class="sourceCode" id="cb2227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2227-1" data-line-number="1">fm.prev =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>lower)</a>
<a class="sourceLine" id="cb2227-2" data-line-number="2">fm.ctr  =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>center)</a>
<a class="sourceLine" id="cb2227-3" data-line-number="3">fm.next =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>upper)</a>
<a class="sourceLine" id="cb2227-4" data-line-number="4">fm.bins =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>banks)</a>
<a class="sourceLine" id="cb2227-5" data-line-number="5">(<span class="dt">fm =</span> <span class="kw">list</span>(<span class="st">&quot;lower&quot;</span> =<span class="st"> </span>fm.prev, <span class="st">&quot;center&quot;</span> =<span class="st"> </span>fm.ctr, <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>fm.next, </a>
<a class="sourceLine" id="cb2227-6" data-line-number="6">           <span class="st">&quot;bins&quot;</span> =<span class="st"> </span>fm.bins))</a></code></pre></div>
<pre><code>## $lower
## [1]  300.00000  610.50869 1017.43304 1550.71093
## [5] 2249.57624 3165.44531 4365.69968
## 
## $center
## [1]  610.50869 1017.43304 1550.71093 2249.57624
## [5] 3165.44531 4365.69968 5938.64348
## 
## $upper
## [1] 1017.4330 1550.7109 2249.5762 3165.4453 4365.6997
## [6] 5938.6435 8000.0000
## 
## $bins
## [1]  300.00000  610.50869 1017.43304 1550.71093
## [5] 2249.57624 3165.44531 4365.69968 5938.64348
## [9] 8000.00000</code></pre>

<p>Below is an example implementation, plotting the triangular frequency bands. We use seven bands for demonstration only.</p>

<div class="sourceCode" id="cb2229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2229-1" data-line-number="1">len   =<span class="st"> </span><span class="kw">length</span>(melfb<span class="op">$</span>center)</a>
<a class="sourceLine" id="cb2229-2" data-line-number="2">col   =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, len); ones  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, len); zeros =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, len)</a>
<a class="sourceLine" id="cb2229-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(fmin,fmax), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">1.05</span>),   </a>
<a class="sourceLine" id="cb2229-4" data-line-number="4">      <span class="dt">xlab=</span><span class="st">&quot;(Hz) Frequency&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Weights&quot;</span>, </a>
<a class="sourceLine" id="cb2229-5" data-line-number="5">      <span class="dt">main=</span><span class="st">&quot;Mel Filter Banks&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2229-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2229-7" data-line-number="7"><span class="kw">segments</span>(fm.prev, zeros,  fm.ctr, ones, <span class="dt">col=</span>col, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2229-8" data-line-number="8"><span class="kw">segments</span>(fm.ctr, ones,  fm.next, zeros, <span class="dt">col=</span>col, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2229-9" data-line-number="9"><span class="kw">text</span>(fm.ctr, <span class="fl">1.05</span>, <span class="kw">round</span>(melfb<span class="op">$</span>center,<span class="dv">0</span>), <span class="dt">cex=</span><span class="fl">0.7</span> )</a>
<a class="sourceLine" id="cb2229-10" data-line-number="10"><span class="kw">text</span>(<span class="dv">7500</span>, <span class="fl">1.05</span>, <span class="st">&quot;(mel scale)&quot;</span>, <span class="dt">cex=</span><span class="fl">0.7</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:melfilter"></span>
<img src="DS_files/figure-html/melfilter-1.png" alt="Mel Filter Banks" width="70%" />
<p class="caption">
Figure 13.41: Mel Filter Banks
</p>
</div>

<p>Here, we choose lower and upper frequencies and convert those frequencies into Mels.</p>
<p>To use the <strong>Mel Filter Bank</strong>, we need to construct its matrix form such that <span class="math inline">\(M \in \mathbb{R}^{b \times \left(\frac{K}{2} + 1\right)}\)</span>. Here, we use <span class="math inline">\(\frac{K}{2} + 1\)</span> because if we look carefully at the <strong>Periodogram</strong> in Figure <a href="deeplearning2.html#fig:periodogram">13.38</a>, the frequencies across the spectrum are symmetrical in that the first frequency equals the last frequency in the <strong>K-point</strong> spectrum. Now, in terms of the number of rows denoted by <strong>b</strong>, our choice for the number of bands is 40 for <strong>b</strong>. Below is the formulation to use in constructing our matrix:</p>
<p><span class="math display">\[\begin{align}
H_m(k) = \begin{cases}
0, &amp; k &lt; f(m - 1) \\
\frac{(k-f(m-1))}{f(m) - f(m-1)}, &amp; f(m-1) \le k \le f(m)\\
\frac{(f(m+1)-k)}{f(m+1)-f(m)}, &amp; f(m) &lt; k \le f(m+1)\\ 
0, &amp;k &gt; f(m+1)
\end{cases}
\ \ \ \ \ \ \ \ where: m \in \{0,..., M-1\} \label{eqn:eqnnumber805}
\end{align}\]</span></p>
<p>We then show an example implementation of the formulation above to construct our mel filter bank matrix.</p>

<div class="sourceCode" id="cb2230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2230-1" data-line-number="1">mel.matrix &lt;-<span class="st"> </span><span class="cf">function</span>(freq.bins, nbands, nfft, sr) {</a>
<a class="sourceLine" id="cb2230-2" data-line-number="2">   L=<span class="st"> </span><span class="kw">floor</span>( nfft <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb2230-3" data-line-number="3">   H =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(nbands, L))  <span class="co"># Mel Filter Bank Matrix</span></a>
<a class="sourceLine" id="cb2230-4" data-line-number="4">   f =<span class="st"> </span>freq.bins</a>
<a class="sourceLine" id="cb2230-5" data-line-number="5">   freq =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span><span class="op">:</span>(nfft <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">/</span><span class="st"> </span>nfft <span class="op">*</span><span class="st"> </span>sr</a>
<a class="sourceLine" id="cb2230-6" data-line-number="6">   <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb2230-7" data-line-number="7">     <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(nbands<span class="op">+</span><span class="dv">1</span>)) {</a>
<a class="sourceLine" id="cb2230-8" data-line-number="8">        h =<span class="st"> </span><span class="dv">0</span>; k =<span class="st"> </span>freq[i]</a>
<a class="sourceLine" id="cb2230-9" data-line-number="9">        <span class="cf">if</span> (k <span class="op">&lt;</span><span class="st"> </span>f[m<span class="dv">-1</span>]) {  h =<span class="st"> </span><span class="dv">0</span>  } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb2230-10" data-line-number="10">        <span class="cf">if</span> (f[m<span class="dv">-1</span>] <span class="op">&lt;=</span><span class="st"> </span>k <span class="op">&amp;&amp;</span><span class="st"> </span>k <span class="op">&lt;=</span><span class="st"> </span>f[m]) { </a>
<a class="sourceLine" id="cb2230-11" data-line-number="11">           h =<span class="st"> </span>(k <span class="op">-</span><span class="st"> </span>(f[m<span class="dv">-1</span>]))<span class="op">/</span>(f[m] <span class="op">-</span><span class="st"> </span>(f[m<span class="dv">-1</span>]))  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb2230-12" data-line-number="12">        <span class="cf">if</span> (f[m] <span class="op">&lt;</span><span class="st"> </span>k <span class="op">&amp;&amp;</span><span class="st"> </span>k <span class="op">&lt;=</span><span class="st"> </span>f[m<span class="op">+</span><span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb2230-13" data-line-number="13">            h =<span class="st">  </span>(f[m<span class="op">+</span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>k)<span class="op">/</span>(f[m<span class="op">+</span><span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>f[m])}  <span class="cf">else</span></a>
<a class="sourceLine" id="cb2230-14" data-line-number="14">        <span class="cf">if</span> (k <span class="op">&gt;</span><span class="st"> </span>f[m<span class="op">+</span><span class="dv">1</span>]) {  h =<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb2230-15" data-line-number="15">        H[m <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, i] =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2230-16" data-line-number="16">     }  </a>
<a class="sourceLine" id="cb2230-17" data-line-number="17">    } </a>
<a class="sourceLine" id="cb2230-18" data-line-number="18">    H</a>
<a class="sourceLine" id="cb2230-19" data-line-number="19">}</a></code></pre></div>

<p>The matrix requires us to construct the frequency bins like so:</p>

<div class="sourceCode" id="cb2231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2231-1" data-line-number="1">nbands  =<span class="st"> </span><span class="dv">40</span>; fmin =<span class="st">  </span><span class="dv">300</span>; fmax =<span class="st"> </span><span class="dv">8000</span></a>
<a class="sourceLine" id="cb2231-2" data-line-number="2">melfb   =<span class="st"> </span><span class="kw">mel.filterbank</span>(<span class="dt">nbands=</span>nbands, <span class="dt">freq.min=</span>fmin, <span class="dt">freq.max=</span>fmax)</a>
<a class="sourceLine" id="cb2231-3" data-line-number="3">fm.ctr  =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>center)</a>
<a class="sourceLine" id="cb2231-4" data-line-number="4">fm.bins =<span class="st"> </span><span class="kw">mel.to.hertz</span>(melfb<span class="op">$</span>banks)</a></code></pre></div>

<p>Finally, we call our <strong>mel.matrix(.)</strong> function to construct the matrix Mel weights (wts).</p>

<div class="sourceCode" id="cb2232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2232-1" data-line-number="1">H =<span class="st"> </span>mel.wts =<span class="st"> </span><span class="kw">mel.matrix</span>(<span class="dt">freq.bins =</span> fm.bins, <span class="dt">nbands =</span> nbands, </a>
<a class="sourceLine" id="cb2232-2" data-line-number="2">                         <span class="dt">nfft =</span> NFFT, <span class="dt">sr =</span> audio<span class="op">@</span>samp.rate) </a>
<a class="sourceLine" id="cb2232-3" data-line-number="3"><span class="kw">str</span>(mel.wts)</a></code></pre></div>
<pre><code>##  num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>

<p>Alternatively, we can use <strong>fft2melmx(.)</strong> function from <strong>tuneR</strong> library.</p>

<div class="sourceCode" id="cb2234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2234-1" data-line-number="1">tuneR.mel =<span class="st"> </span>tuneR<span class="op">:::</span><span class="kw">fft2melmx</span>(<span class="dt">nfft =</span> NFFT, <span class="dt">sr =</span> audio<span class="op">@</span>samp.rate, </a>
<a class="sourceLine" id="cb2234-2" data-line-number="2">            <span class="dt">nfilts =</span> nbands,  <span class="dt">width =</span> <span class="dv">1</span>, <span class="dt">minfreq =</span> fmin,  <span class="dt">maxfreq =</span> fmax, </a>
<a class="sourceLine" id="cb2234-3" data-line-number="3">            <span class="dt">htkmel =</span> <span class="ot">TRUE</span>,  <span class="dt">constamp =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2234-4" data-line-number="4">H =<span class="st"> </span>tuneR.wts =<span class="st"> </span>tuneR.mel<span class="op">$</span>wts[,<span class="dv">1</span><span class="op">:</span><span class="dv">256</span>] <span class="co"># if we take only the first half.</span></a>
<a class="sourceLine" id="cb2234-5" data-line-number="5"><span class="kw">str</span>(tuneR.wts)</a></code></pre></div>
<pre><code>##  num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>

<p>Note that the function allows us to use a hyperparameter <strong>htkmel</strong>, which uses the Mel scale formula if we set it to true, as discussed previously.</p>
<p>Let us view the heatmap of our Mel filter bank matrix. See Figure <a href="deeplearning2.html#fig:melheatmap">13.42</a>.</p>

<div class="sourceCode" id="cb2236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2236-1" data-line-number="1"><span class="kw">library</span>(RColorBrewer)</a>
<a class="sourceLine" id="cb2236-2" data-line-number="2">col =<span class="st"> </span><span class="kw">colorRampPalette</span>(<span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Blues&quot;</span>))(<span class="dv">800</span>)</a>
<a class="sourceLine" id="cb2236-3" data-line-number="3"><span class="kw">heatmap</span>(mel.wts, <span class="dt">Colv =</span> <span class="ot">NA</span>, <span class="dt">Rowv =</span> <span class="ot">NA</span>, <span class="dt">col=</span>col)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:melheatmap"></span>
<img src="DS_files/figure-html/melheatmap-1.png" alt="Mel HeatMap" width="60%" />
<p class="caption">
Figure 13.42: Mel HeatMap
</p>
</div>

<p><strong>Seventh</strong>, to get our Mel-filtered power spectrum, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
\tilde{X}(m) = \sum_{k=0}^{N-1} \left(|X(k)|^2 H_m(k)\right),
\ \ \ \ \ 0 \le m \le M - 1  
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{X}(k)\)</span> is our <strong>power spectrum (P)</strong> and <span class="math inline">\(\mathbf{\tilde{X}}(k)\)</span> is our target <strong>Mel power spectrum</strong>. <strong>H</strong> is our Mel weights. The index <strong>m</strong> is the Mel-filter bank number and <strong>k</strong> is the DFT <strong>bin</strong> number.</p>
<p>We know that the two matrices have the following dimension; therefore, we can perform matrix manipulation to construct our <strong>Mel Spectrum</strong> matrix.</p>

<div class="sourceCode" id="cb2237"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2237-1" data-line-number="1">H.dim =<span class="st"> </span><span class="kw">dim</span>(H) <span class="co"># mel filter bank matrix</span></a>
<a class="sourceLine" id="cb2237-2" data-line-number="2">P.dim =<span class="st"> </span><span class="kw">dim</span>(P) <span class="co"># power spectrum</span></a>
<a class="sourceLine" id="cb2237-3" data-line-number="3"><span class="kw">rbind</span>(<span class="st">&quot;H&quot;</span> =<span class="st"> </span>H.dim, <span class="st">&quot;P&quot;</span> =<span class="st"> </span>P.dim)</a></code></pre></div>
<pre><code>##   [,1] [,2]
## H   40  256
## P  256  182</code></pre>
<div class="sourceCode" id="cb2239"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2239-1" data-line-number="1">tilde.X =<span class="st"> </span>my.mel.powspectrum =<span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>P</a>
<a class="sourceLine" id="cb2239-2" data-line-number="2"><span class="kw">str</span>(tilde.X)</a></code></pre></div>
<pre><code>##  num [1:40, 1:182] 238 212 1430 4381 15900 ...</code></pre>

<p>We then take the log of our <strong>Mel Spectrum</strong> which comes from the idea that the human voice is logarithmic, based on notes from other literature:</p>
<p><span class="math display">\[\begin{align}
S(m) = 20 \times \log_{10} \left(\tilde{X}(m)\right)
\end{align}\]</span></p>

<div class="sourceCode" id="cb2241"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2241-1" data-line-number="1">S =<span class="st"> </span>my.log.mel.powspectrum =<span class="st">  </span><span class="dv">20</span> <span class="op">*</span><span class="st"> </span><span class="kw">log10</span>(tilde.X)</a>
<a class="sourceLine" id="cb2241-2" data-line-number="2"><span class="kw">str</span>(S)</a></code></pre></div>
<pre><code>##  num [1:40, 1:182] 47.5 46.5 63.1 72.8 84 ...</code></pre>

<p>Now, we can plot a heatmap. See Figure <a href="deeplearning2.html#fig:cepstrum">13.43</a>:</p>

<div class="sourceCode" id="cb2243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2243-1" data-line-number="1">frames =<span class="st">  </span><span class="kw">ncol</span>(S)</a>
<a class="sourceLine" id="cb2243-2" data-line-number="2">mel.x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, signal.length, <span class="dt">length.out=</span>frames)</a>
<a class="sourceLine" id="cb2243-3" data-line-number="3">mel.y =<span class="st"> </span>fm.ctr</a>
<a class="sourceLine" id="cb2243-4" data-line-number="4">mel.z =<span class="st"> </span><span class="kw">t</span>(S)</a>
<a class="sourceLine" id="cb2243-5" data-line-number="5"><span class="kw">image</span>(<span class="dt">x =</span> mel.x, <span class="dt">y =</span> mel.y, <span class="dt">z =</span> mel.z, <span class="dt">ylab =</span> <span class="st">&#39;Freq [Hz]&#39;</span>, </a>
<a class="sourceLine" id="cb2243-6" data-line-number="6">      <span class="dt">xlab =</span> <span class="st">&#39;Time [s]&#39;</span>, <span class="dt">useRaster=</span><span class="ot">FALSE</span>, <span class="dt">col =</span> <span class="kw">hcl.colors</span>(<span class="dv">12</span>, <span class="st">&quot;YlOrRd&quot;</span>, </a>
<a class="sourceLine" id="cb2243-7" data-line-number="7">      <span class="dt">rev =</span> <span class="ot">FALSE</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cepstrum"></span>
<img src="DS_files/figure-html/cepstrum-1.png" alt="Mel Log Spectrum HeatMap" width="70%" />
<p class="caption">
Figure 13.43: Mel Log Spectrum HeatMap
</p>
</div>

<p><strong>Eight</strong>, note that other literature may stop from here and use the <strong>MFCC spectrogram</strong> as input image to <strong>CNN</strong>. However, for other modeling methods such as <strong>RNN</strong> (e.g., <strong>LSTM</strong>/<strong>GRU</strong>), we can use <strong>PLP</strong> and <strong>LPCC</strong>. The difference is that they vary in the number of coefficients (features) generated. In our case, we may be interested in covering the standard <strong>39 features</strong> of <strong>MFCC</strong> per frame <span class="citation">(Noughreche A. et al. <a href="bibliography.html#ref-ref1512a">2021</a>)</span>:</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{12 MFCC coefficients} &amp; \text{1 Log Energy coefficient  }\\
\text{12 Delta MFCC coefficients} &amp; \text{1 Delta Log Energy coefficient  }\\
\text{12 Delta Delta MFCC coefficients} &amp; \text{1 Delta-Delta Log Energy coefficient}
\end{array}
\]</span></p>
<p>To construct the 39 features, we start with the first 12 <strong>cepstra</strong> coefficients. Here, we take the <strong>Discrete Cosine Transform (DCT)</strong>, which is the inverse <strong>DFT</strong> of the log of the Mel spectrum. It helps to point out as reference only that there are four standard DCT types <span class="citation">(Gilbert Strang <a href="bibliography.html#ref-ref1642g">1999</a>; Shao X., Johnson S.G <a href="bibliography.html#ref-ref1559x">2009</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{lll}
\mathbf{\text{DCT Type}} &amp; \mathbf{\text{Basis Function (BS)}} &amp; \mathbf{\text{Constraint}}\\
------  &amp; ---------- &amp; ---------------\\
1 &amp; \mathbf{\text{cos}} \left[\left(mn\right)\frac{\pi}{M-1}\right]  &amp; \text{divide by } \sqrt{2} \text{ if m or n = 0 or M - 1}\\
2 &amp; \mathbf{\text{cos}}\left[\left(n + \frac{1}{2}\right) m \frac{\pi}{M}\right]  &amp; \text{divide by } \sqrt{2} \text{ if m = 0} \\
3 &amp; \mathbf{\text{cos}}\left[\left(m + \frac{1}{2}\right) n \frac{\pi}{M}\right]  &amp; \text{divide by } \sqrt{2} \text{ if n = 0} \\
4 &amp; \mathbf{\text{cos}}\left[\left(m + \frac{1}{2}\right) \left(n + \frac{1}{2}\right) \frac{\pi}{M}\right] \\
\end{array} \label{eqn:eqnnumber806}
\end{align}\]</span></p>
<p>For example, using the basis function (<strong>BS</strong>) of DCT type 3, we get the following equation for DCT.</p>
<p><span class="math display">\[\begin{align}
C_n = \sum_{m=0}^{M-1} S(m) \text{BS}_3(n, m), \ \ \ \ \ \ \text{C = Mel cepstral coefficients} 
\end{align}\]</span></p>
<p>where <strong>N</strong> is the number of Mel cepstral coefficients, and <strong>M</strong> is the number of frequencies in the Mel power spectrum.</p>
<p>We rely on the <strong>basis function</strong> to produce a matrix of <strong>cosine weights</strong>. Based on our convention, each row-vector forms a <strong>sinusoidal</strong> pattern.</p>
<p>It is essential to point out that other literature also demonstrates the use of the following <strong>DCT III</strong> basis function for <strong>MFCC</strong> (derivation not included).</p>
<p><span class="math display">\[\begin{align}
\text{DCT}_{III}^{(mfcc)}(n)  = \sqrt{\frac{2}{M}} \times \mathbf{\text{cos}}\left[\left(m - \frac{1}{2}\right)\frac{n\pi}{M}\right], \ \ \ \ \  0 \le n \le N-1  
\end{align}\]</span></p>
<p>where <span class="math inline">\(\sqrt{\frac{2}{M}}\)</span> is a scaling factor.</p>
<p>And for <strong>DCT II</strong> to be orthogonal, if <strong>n</strong> = 0, then we divide by <span class="math inline">\(\sqrt{2}\)</span> like so:</p>
<p><span class="math display">\[\begin{align}
\text{DCT}_{II}^{(mfcc)}(0) =  \text{DCT}_{III}^{(mfcc)}(0) \text{ divide by } \sqrt{2}
\end{align}\]</span></p>
<p>For our demonstration purposes, let us use the <strong>DCT III</strong> formulation instead. To do so, let us show in detail how the matrix for <strong>DCT</strong> is constructed, starting with the below naive example implementation of <strong>DCT II and III</strong>:</p>

<div class="sourceCode" id="cb2244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2244-1" data-line-number="1">my.dct &lt;-<span class="st"> </span><span class="cf">function</span>(N, M, <span class="dt">dcttype=</span><span class="dv">3</span>) {</a>
<a class="sourceLine" id="cb2244-2" data-line-number="2">   raw.dct.matrix &lt;-<span class="st"> </span><span class="cf">function</span>(N, M) {</a>
<a class="sourceLine" id="cb2244-3" data-line-number="3">      n=<span class="kw">seq</span>(<span class="dv">0</span>, N<span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb2244-4" data-line-number="4">      bs =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(N, M))</a>
<a class="sourceLine" id="cb2244-5" data-line-number="5">      <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(M)) { bs[,m] =<span class="st"> </span>(n <span class="op">*</span><span class="st"> </span>(m <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span>)) }</a>
<a class="sourceLine" id="cb2244-6" data-line-number="6">      bs</a>
<a class="sourceLine" id="cb2244-7" data-line-number="7">   }</a>
<a class="sourceLine" id="cb2244-8" data-line-number="8">   rmatrix =<span class="st"> </span><span class="kw">raw.dct.matrix</span>(<span class="dt">N=</span>N, <span class="dt">M=</span>M)</a>
<a class="sourceLine" id="cb2244-9" data-line-number="9">   dct.matrix =<span class="st"> </span><span class="kw">cos</span>( pi <span class="op">*</span><span class="st"> </span>rmatrix <span class="op">/</span><span class="st"> </span>M ) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span> <span class="op">/</span><span class="st"> </span>M)</a>
<a class="sourceLine" id="cb2244-10" data-line-number="10">   <span class="cf">if</span> (dcttype <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { dct.matrix[<span class="dv">1</span>,] =<span class="st"> </span>dct.matrix[<span class="dv">1</span>,] <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb2244-11" data-line-number="11">   <span class="kw">list</span>(<span class="st">&quot;rmatrix&quot;</span> =<span class="st"> </span>rmatrix, <span class="st">&quot;matrix&quot;</span> =<span class="st"> </span>dct.matrix)</a>
<a class="sourceLine" id="cb2244-12" data-line-number="12">}</a></code></pre></div>

<p>For a brief example, we show that the raw matrix is constructed as such:</p>

<div class="sourceCode" id="cb2245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2245-1" data-line-number="1">dct =<span class="st"> </span><span class="kw">my.dct</span>(<span class="dt">N=</span><span class="dv">4</span>, <span class="dt">M=</span><span class="dv">5</span>, <span class="dt">dcttype=</span><span class="dv">3</span>) </a>
<a class="sourceLine" id="cb2245-2" data-line-number="2">dct<span class="op">$</span>rmatrix <span class="op">*</span><span class="st"> </span><span class="dv">2</span></a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5]
## [1,]    0    0    0    0    0
## [2,]    1    3    5    7    9
## [3,]    2    6   10   14   18
## [4,]    3    9   15   21   27</code></pre>

<p>And our dct matrix generated by our basis functions is thus shown as</p>

<div class="sourceCode" id="cb2247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2247-1" data-line-number="1">dct<span class="op">$</span>matrix</a></code></pre></div>
<pre><code>##            [,1]        [,2]           [,3]        [,4]
## [1,] 0.63245553  0.63245553  6.3245553e-01  0.63245553
## [2,] 0.60150096  0.37174803  3.8726732e-17 -0.37174803
## [3,] 0.51166727 -0.19543951 -6.3245553e-01 -0.19543951
## [4,] 0.37174803 -0.60150096 -1.1618020e-16  0.60150096
##             [,5]
## [1,]  0.63245553
## [2,] -0.60150096
## [3,]  0.51166727
## [4,] -0.37174803</code></pre>

<p>For our 12 Cepstral coefficients, let us construct a 12 x 40 dct matrix:</p>

<div class="sourceCode" id="cb2249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2249-1" data-line-number="1">ncepstral =<span class="st"> </span><span class="dv">12</span></a>
<a class="sourceLine" id="cb2249-2" data-line-number="2">nfilters  =<span class="st"> </span>nbands</a>
<a class="sourceLine" id="cb2249-3" data-line-number="3">BS =<span class="st"> </span><span class="kw">my.dct</span>(<span class="dt">N=</span>ncepstral, <span class="dt">M=</span>nfilters, <span class="dt">dcttype=</span><span class="dv">3</span>)<span class="op">$</span>matrix</a>
<a class="sourceLine" id="cb2249-4" data-line-number="4"><span class="kw">str</span>(BS)</a></code></pre></div>
<pre><code>##  num [1:12, 1:40] 0.224 0.223 0.223 0.222 0.221 ...</code></pre>

<p>Next, we calculate the <strong>Log Mel spectrum</strong>:</p>

<div class="sourceCode" id="cb2251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2251-1" data-line-number="1">S =<span class="st"> </span><span class="kw">log</span>(my.mel.powspectrum)  </a></code></pre></div>

<p>Finally, we then obtain our <strong>DCT coefficients</strong>, switching back from frequency-domain to time-domain by performing matrix manipulation:</p>

<div class="sourceCode" id="cb2252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2252-1" data-line-number="1"><span class="kw">rbind</span>(<span class="st">&quot;S&quot;</span> =<span class="st"> </span><span class="kw">dim</span>(S), <span class="st">&quot;BS&quot;</span> =<span class="st"> </span><span class="kw">dim</span>(BS))</a></code></pre></div>
<pre><code>##    [,1] [,2]
## S    40  182
## BS   12   40</code></pre>
<div class="sourceCode" id="cb2254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2254-1" data-line-number="1">my.mel.cepstral.coeffs =<span class="st"> </span>BS <span class="op">%*%</span><span class="st"> </span>S  </a>
<a class="sourceLine" id="cb2254-2" data-line-number="2"><span class="kw">str</span>(my.mel.cepstral.coeffs)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] 88.916 -6.612 -4.564 0.239 -1.132 ...</code></pre>

<p><strong>Ninth</strong>, as an alternative to <strong>MFCC</strong>, we leave readers to investigate the 54 features of <strong>PLP</strong> and 39 features of <strong>LPCC</strong>. Other contributions are published for comparative study of different <strong>MFCC</strong> variants <span class="citation">(Ganchev T. et al., <a href="bibliography.html#ref-ref2039g">2005</a>; Elharati H. <a href="bibliography.html#ref-ref1912h">2019</a>; Joshy J., Sambyo K. <a href="bibliography.html#ref-ref1562j">2016</a>)</span>. Specifically, one that may stand out is <strong>MFCC-HTK</strong>, which references <strong>Hidden Markov Chain (HMM) ToolKit</strong>. In terms of <strong>MFCC-HTK</strong> implementation, we reference JÃ©rÃ´me Sueur <span class="citation">(<a href="bibliography.html#ref-ref1496j">2018</a>)</span>. In his book, he proceeds with three steps to get the <strong>Mel coefficients</strong> after obtaining the <strong>Power Spectrum</strong> in which he demonstrates the use of three corresponding functions, namely, <strong>audspec(.)</strong>, <strong>spec2cep(.)</strong>, and <strong>lifter(.)</strong>.</p>
<p>We derive our log Mel power spectrum and the 12 cepstral coefficients in step eight. Alternatively, here we use <strong>audspec(.)</strong> and then <strong>spec2cep(.)</strong> in next step.</p>
<p>Recall that the goal here is to focus on <strong>Critical Band Filtering</strong>, also called <strong>Auditory Filtering</strong> (Fletcher 1940), which is a masking process to reduce the <strong>Power Spectrum</strong> to a set of critical frequency bandwidth relevant to human auditory perceptions. Using the Sueur step, we implement the following to narrow down our spectrum to <strong>Auditory Spectrum</strong>. </p>

<div class="sourceCode" id="cb2256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2256-1" data-line-number="1">tuneR.mel.spectrum =<span class="st"> </span><span class="kw">audspec</span>(tuneR.powspectrum, <span class="dt">sr =</span> audio<span class="op">@</span>samp.rate, </a>
<a class="sourceLine" id="cb2256-2" data-line-number="2">                       <span class="dt">minfreq =</span> fmin, <span class="dt">maxfreq =</span> fmax,</a>
<a class="sourceLine" id="cb2256-3" data-line-number="3">                       <span class="dt">nfilts=</span>nbands, <span class="dt">fbtype=</span><span class="st">&quot;htkmel&quot;</span>, <span class="dt">sumpower=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2256-4" data-line-number="4"><span class="kw">str</span>(tuneR.mel.spectrum)</a></code></pre></div>
<pre><code>## List of 2
##  $ aspectrum: num [1:40, 1:182] 244 199 1339 4066 16012 ...
##  $ wts      : num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ...</code></pre>

<p>The function <strong>audspec(.)</strong> calculates <strong>NFFT</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\text{nfft} = (\text{nfreq} - 1) \times 2
\end{align}\]</span></p>
<p>whereas in our case, we use:</p>
<p><span class="math display">\[\begin{align}
\text{nfft} = \text{nfreq} / 2 \  \ \ \ \ \text{where nfreq = frame size}
\end{align}\]</span></p>
<p>If our route is towards using <strong>PLP</strong> instead of <strong>MFCC</strong>, then to conform with <strong>PLP</strong>, we can use <strong>postaud(.)</strong> function for compression as necessary.</p>

<div class="sourceCode" id="cb2258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2258-1" data-line-number="1">mfcc.post.spectrum =<span class="st"> </span><span class="kw">postaud</span>(<span class="dt">x=</span>tuneR.mel.spectrum<span class="op">$</span>aspectrum, <span class="dt">fbtype=</span><span class="st">&quot;htkmel&quot;</span>, </a>
<a class="sourceLine" id="cb2258-2" data-line-number="2">                             <span class="dt">fmax=</span>fmax)</a>
<a class="sourceLine" id="cb2258-3" data-line-number="3"><span class="kw">str</span>(mfcc.post.spectrum )</a></code></pre></div>
<pre><code>## List of 2
##  $ y  : num [1:40, 1:182] 0.179 0.179 0.85 2.089 4.7 ...
##  $ eql: num [1:40] 0 0.0000272 0.000457 0.0022935 0.0067969 ...</code></pre>

<p><strong>Tenth</strong>, we then use <strong>spec2cep(.)</strong> to convert from Mel Spectrum to Mel Cepstrum. In the process, the function intrinsically performs <strong>Discrete Cosine Transform (DCT)</strong> to decorrelate the spectrum, then it uses <strong>DCT</strong> to obtain the cepstral coefficients <span class="citation">(JÃ©rÃ´me Sueur <a href="bibliography.html#ref-ref1496j">2018</a>)</span>. Here, we use DCT type 3.</p>

<div class="sourceCode" id="cb2260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2260-1" data-line-number="1">cepstra =<span class="st"> </span><span class="kw">spec2cep</span>(tuneR.mel.spectrum<span class="op">$</span>aspectrum, <span class="dt">ncep =</span> ncepstral, <span class="dt">type=</span><span class="st">&quot;t3&quot;</span>)</a>
<a class="sourceLine" id="cb2260-2" data-line-number="2">tuneR.mel.cepstral.coeffs =<span class="st"> </span>cepstra<span class="op">$</span>cep</a>
<a class="sourceLine" id="cb2260-3" data-line-number="3">tuneR.mel.cepstral.dctm   =<span class="st"> </span>cepstra<span class="op">$</span>dctm</a>
<a class="sourceLine" id="cb2260-4" data-line-number="4"><span class="kw">str</span>(cepstra)</a></code></pre></div>
<pre><code>## List of 2
##  $ cep : num [1:12, 1:182] 88.811 -6.682 -4.558 0.272 -1.161 ...
##  $ dctm: num [1:12, 1:40] 0.224 0.223 0.223 0.222 0.221 ...</code></pre>

<p>As an alternative to the overall <strong>MFCC</strong> which uses <strong>audspec(.)</strong> and <strong>spec2cep(.)</strong>, we leave readers to investigate <strong>Linear Predictive Cepstral Coefficient (LPCC)</strong> which uses <strong>dolpc(.)</strong> function and <strong>lpc2cep(.)</strong> correspondingly.</p>
<p><strong>Eleventh</strong>, apply rescaling to normalize magnitude across coefficients. This is done through liftering (or filtering).</p>
<p><span class="math display">\[\begin{align}
\mathbf{\text{liftered.cepstra}} = \left(1 + \frac{L}{2} \mathbf{\text{sin}} \frac{\pi n}{L}\right) \mathbf{\text{cepstra}}
\end{align}\]</span></p>
<p>Below is a simple example implementation of the equation above:</p>

<div class="sourceCode" id="cb2262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2262-1" data-line-number="1">my.htk.lifter &lt;-<span class="st"> </span><span class="cf">function</span>(cepstra, lift) {</a>
<a class="sourceLine" id="cb2262-2" data-line-number="2">  L =<span class="st"> </span>lift</a>
<a class="sourceLine" id="cb2262-3" data-line-number="3">  n =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(cepstra) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb2262-4" data-line-number="4">  lift =<span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>(L<span class="op">/</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi <span class="op">*</span><span class="st"> </span>n<span class="op">/</span>L))</a>
<a class="sourceLine" id="cb2262-5" data-line-number="5">  <span class="kw">sweep</span>(cepstra, <span class="dv">1</span>, lift, <span class="st">&#39;*&#39;</span>)</a>
<a class="sourceLine" id="cb2262-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2262-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb2262-8" data-line-number="8">mels.coefficients=<span class="kw">my.htk.lifter</span>(tuneR.mel.cepstral.coeffs, <span class="dt">lift=</span>ncepstral<span class="dv">-1</span>) </a>
<a class="sourceLine" id="cb2262-9" data-line-number="9"><span class="kw">str</span>(mels.coefficients)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ...</code></pre>

<p>To validate consistency, we use <strong>lifter(.)</strong>.</p>

<div class="sourceCode" id="cb2264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2264-1" data-line-number="1">mels.coefficients =<span class="st"> </span><span class="kw">lifter</span>(tuneR.mel.cepstral.coeffs, <span class="dt">lift=</span>ncepstral<span class="dv">-1</span>,</a>
<a class="sourceLine" id="cb2264-2" data-line-number="2">                           <span class="dt">htk=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2264-3" data-line-number="3"><span class="kw">str</span>(mels.coefficients)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ...</code></pre>

<p><strong>Twelfth</strong>, we compute for the <strong>Log Energy Coefficients</strong>, which captures the change rate (slope approximation) in cepstral features over time <span class="citation">(Jaison Joshy, Koj Sambyo <a href="bibliography.html#ref-ref1562j">2016</a>)</span>. Here, we capture the <strong>12 Delta</strong> MFCC energies as the first-order difference and another <strong>12 Delta Delta</strong> MFCC energies as the second-order difference from the first-order difference. These are approximations of first and second derivatives in terms of speed and acceleration of speech. Deltas are expressed as such <span class="citation">(Noughreche A. et al <a href="bibliography.html#ref-ref1512a">2021</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
\nabla_t = d_t - d_{t-1} \ \ \ \ \ \ \ \  \nabla \nabla_t = \nabla_t  - \nabla_{t-1}
\end{align}\]</span></p>
<p><span class="math display">\[
where: d_t = \frac{\sum_{n=1}^N n\left(C_{n+t} - C_{n-t}\right)}{2 \sum_{n=1}^N n^2},
\ \ \ \ \ \ \ \ \text{C = Mel cepstral coefficients}
\]</span></p>

<div class="sourceCode" id="cb2266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2266-1" data-line-number="1">delta.energies       =<span class="st"> </span><span class="kw">deltas</span>(tuneR.mel.cepstral.coeffs)</a>
<a class="sourceLine" id="cb2266-2" data-line-number="2"><span class="kw">str</span>(delta.energies)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] -39.19 5.299 0.493 -17.704 1.332 ...</code></pre>
<div class="sourceCode" id="cb2268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2268-1" data-line-number="1">delta.delta.energies =<span class="st"> </span><span class="kw">deltas</span>(delta.energies)</a>
<a class="sourceLine" id="cb2268-2" data-line-number="2"><span class="kw">str</span>(delta.delta.energies)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] -1046.3 -246.2 279.8 49.5 96.2 ...</code></pre>

<p><strong>Finally</strong>, to complete the 39 features of <strong>MFCC</strong> per frame, we also need to calculate the <strong>Energy</strong> of the signal. Here, we can either consider the <strong>Raw Energy</strong> directly from the signal or the <strong>Cepstral Energy</strong> from <strong>MFCC</strong> coefficients <span class="citation">(Korzinek D. <a href="bibliography.html#ref-ref1602d">2021</a>)</span>.</p>
<p>As for the <strong>Raw Energy</strong>, we can choose the formulation based on which domain the coefficient happens to be.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\mathbf{\log}\ \text{E} = \log\left(\sum x^2\right)}_{\text{in time-domain}}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{\mathbf{\log}\ \text{E} = \log\left(\frac{|x|^2}{\text{nfft}}\right)}_{\text{in frequency-domain}}
\end{align}\]</span></p>

<div class="sourceCode" id="cb2270"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2270-1" data-line-number="1">get.log.energy  &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  z =<span class="st"> </span><span class="kw">log</span>(<span class="kw">apply</span>(x<span class="op">^</span><span class="dv">2</span>, <span class="dv">2</span>, sum)); z }</a>
<a class="sourceLine" id="cb2270-2" data-line-number="2">get.mfcc.energy &lt;-<span class="st"> </span><span class="cf">function</span>(x, nfft) {  </a>
<a class="sourceLine" id="cb2270-3" data-line-number="3">                          z =<span class="st"> </span><span class="kw">apply</span>(x, <span class="dv">2</span>, sum) <span class="op">*</span><span class="st"> </span>(<span class="kw">sqrt</span>(<span class="dv">2</span><span class="op">/</span>nfft)) ; z }</a></code></pre></div>

<p>To get the energy of the windowed frame, we use the following code:</p>

<div class="sourceCode" id="cb2271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2271-1" data-line-number="1">log.energy =<span class="st"> </span><span class="kw">get.log.energy</span>(<span class="kw">t</span>(windowed.frm))</a>
<a class="sourceLine" id="cb2271-2" data-line-number="2"><span class="kw">str</span>(log.energy)</a></code></pre></div>
<pre><code>##  num [1:182] 8.93 8.86 8.66 8.56 7.97 ...</code></pre>

<p>Otherwise, we also can use the <strong>MFCC</strong> coefficients:</p>

<div class="sourceCode" id="cb2273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2273-1" data-line-number="1">log.mfcc.energy        =<span class="st"> </span><span class="kw">get.mfcc.energy</span>(tuneR.mel.cepstral.coeffs, frm<span class="op">$</span>size)</a>
<a class="sourceLine" id="cb2273-2" data-line-number="2"><span class="kw">str</span>(log.mfcc.energy)</a></code></pre></div>
<pre><code>##  num [1:182] 4.25 4.64 4.35 4.19 4.3 ...</code></pre>

<p>See the close correlation between log raw energy and MFCC energy in Figure <a href="deeplearning2.html#fig:logenergy">13.44</a>.</p>

<div class="sourceCode" id="cb2275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2275-1" data-line-number="1">len   =<span class="st"> </span><span class="kw">length</span>(log.energy)</a>
<a class="sourceLine" id="cb2275-2" data-line-number="2">x     =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, len)</a>
<a class="sourceLine" id="cb2275-3" data-line-number="3">y1    =<span class="st"> </span>log.energy <span class="op">/</span><span class="st"> </span><span class="kw">max</span>(log.energy)</a>
<a class="sourceLine" id="cb2275-4" data-line-number="4">y2    =<span class="st"> </span>log.mfcc.energy <span class="op">/</span><span class="st"> </span><span class="kw">max</span>(log.mfcc.energy) </a>
<a class="sourceLine" id="cb2275-5" data-line-number="5">ylim  =<span class="st"> </span><span class="kw">max</span>(y1, y2)</a>
<a class="sourceLine" id="cb2275-6" data-line-number="6"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,ylim),   </a>
<a class="sourceLine" id="cb2275-7" data-line-number="7">      <span class="dt">xlab=</span><span class="st">&quot;Frames&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Magnitude (Scaled)&quot;</span>, </a>
<a class="sourceLine" id="cb2275-8" data-line-number="8">      <span class="dt">main=</span><span class="st">&quot;Log Energy vs MFCC Energy&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2275-9" data-line-number="9"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb2275-10" data-line-number="10"><span class="kw">lines</span>(x, y1, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb2275-11" data-line-number="11"><span class="kw">lines</span>(x, y2, <span class="dt">col=</span><span class="st">&quot;darkgreen&quot;</span>)</a>
<a class="sourceLine" id="cb2275-12" data-line-number="12"><span class="kw">legend</span>(<span class="dv">100</span>,<span class="fl">0.5</span>, <span class="dt">inset=</span>.<span class="dv">02</span>,  <span class="kw">c</span>(<span class="st">&quot;log raw energy&quot;</span>, <span class="st">&quot;mfcc energy&quot;</span>), </a>
<a class="sourceLine" id="cb2275-13" data-line-number="13">       <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;darkgreen&quot;</span>),  <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logenergy"></span>
<img src="DS_files/figure-html/logenergy-1.png" alt="Log Energy vs MFCC Energy" width="70%" />
<p class="caption">
Figure 13.44: Log Energy vs MFCC Energy
</p>
</div>

<p>Then, we get the log energy of the deltas:</p>

<div class="sourceCode" id="cb2276"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2276-1" data-line-number="1">log.delta.energy       =<span class="st"> </span><span class="kw">get.log.energy</span>(delta.energies)</a>
<a class="sourceLine" id="cb2276-2" data-line-number="2"><span class="kw">str</span>(log.delta.energy)</a></code></pre></div>
<pre><code>##  num [1:182] 7.9 8.62 9.55 10.26 10.4 ...</code></pre>
<div class="sourceCode" id="cb2278"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2278-1" data-line-number="1">log.delta.delta.energy =<span class="st"> </span><span class="kw">get.log.energy</span>(delta.delta.energies)</a>
<a class="sourceLine" id="cb2278-2" data-line-number="2"><span class="kw">str</span>(log.delta.delta.energy)</a></code></pre></div>
<pre><code>##  num [1:182] 14.1 14.3 13.6 14.3 16 ...</code></pre>

<p>We then concatenate all 39 coefficients like so.</p>

<div class="sourceCode" id="cb2280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2280-1" data-line-number="1">mfcc.features =<span class="st"> </span></a>
<a class="sourceLine" id="cb2280-2" data-line-number="2"><span class="st">        </span><span class="kw">array</span>( <span class="kw">rbind</span>(mels.coefficients, delta.energies, delta.delta.energies,</a>
<a class="sourceLine" id="cb2280-3" data-line-number="3">            log.mfcc.energy, log.delta.energy, log.delta.delta.energy),</a>
<a class="sourceLine" id="cb2280-4" data-line-number="4">             <span class="kw">c</span>(<span class="dv">39</span>, <span class="dv">182</span>))</a>
<a class="sourceLine" id="cb2280-5" data-line-number="5"><span class="kw">str</span>(mfcc.features)</a></code></pre></div>
<pre><code>##  num [1:39, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ...</code></pre>

<p>We show the long, intricate steps to extract features from individual frames of an entire sound wave. Existing platforms offer an easier way to extract features through APIs. For example, the <strong>melfcc(.)</strong> function below is enough to extract the 12 Mel coefficients prior to liftering:</p>

<div class="sourceCode" id="cb2282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2282-1" data-line-number="1">mfcc.cepstra  =<span class="st"> </span><span class="kw">melfcc</span>(audio, </a>
<a class="sourceLine" id="cb2282-2" data-line-number="2">     <span class="dt">sr       =</span> audio<span class="op">@</span>samp.rate,  <span class="co"># Signal Rate</span></a>
<a class="sourceLine" id="cb2282-3" data-line-number="3">     <span class="dt">wintime  =</span> frm<span class="op">$</span>duration,     <span class="co"># Window length</span></a>
<a class="sourceLine" id="cb2282-4" data-line-number="4">     <span class="dt">hoptime  =</span> frm<span class="op">$</span>stride.time,  <span class="co"># Successive windown inbetween</span></a>
<a class="sourceLine" id="cb2282-5" data-line-number="5">     <span class="dt">numcep   =</span> <span class="dv">12</span>,               <span class="co"># By default it will be 12 features</span></a>
<a class="sourceLine" id="cb2282-6" data-line-number="6">     <span class="dt">sumpower =</span> <span class="ot">TRUE</span>,             <span class="co"># frequence scale transformation  </span></a>
<a class="sourceLine" id="cb2282-7" data-line-number="7">     <span class="dt">nbands   =</span> <span class="dv">40</span>,               <span class="co"># Number of spectra bands, filter banks</span></a>
<a class="sourceLine" id="cb2282-8" data-line-number="8">     <span class="dt">bwidth   =</span> <span class="dv">1</span>,                <span class="co"># Width of spectral bands</span></a>
<a class="sourceLine" id="cb2282-9" data-line-number="9">     <span class="dt">preemph  =</span> <span class="fl">0.97</span>,             <span class="co"># pre Emphasis</span></a>
<a class="sourceLine" id="cb2282-10" data-line-number="10">     <span class="dt">minfreq  =</span> fmin,</a>
<a class="sourceLine" id="cb2282-11" data-line-number="11">     <span class="dt">maxfreq  =</span> fmax,</a>
<a class="sourceLine" id="cb2282-12" data-line-number="12">     <span class="dt">fbtype   =</span> <span class="st">&quot;htkmel&quot;</span>,</a>
<a class="sourceLine" id="cb2282-13" data-line-number="13">     <span class="dt">dcttype  =</span> <span class="st">&quot;t3&quot;</span>,</a>
<a class="sourceLine" id="cb2282-14" data-line-number="14">     <span class="dt">dither   =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb2282-15" data-line-number="15">     <span class="dt">frames_in_rows =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb2282-16" data-line-number="16">     )</a>
<a class="sourceLine" id="cb2282-17" data-line-number="17"><span class="kw">str</span>(mfcc.cepstra)</a></code></pre></div>
<pre><code>##  num [1:12, 1:182] 88.803 -6.689 -6.914 0.525 -2.67 ...</code></pre>

<p>Other literature may find it helpful to extract other features related to <strong>pitch</strong>, <strong>intensity</strong>, <strong>zero-cross rate</strong>, and on. We leave readers to investigate such features and their necessity.</p>
<p>Having extracted the features, we can now rely on modeling techniques such as <strong>LSTM</strong> and <strong>Transformer Neural Network</strong> (specifically around <strong>Attention</strong>) to encode and decode features. However, one of the challenges of sequence-based applications, especially in <strong>Handwriting recognition</strong>, is <strong>alignment</strong>. This is where <strong>CTC</strong> comes to play.</p>
</div>
<div id="connectionist-temporal-classification-ctc" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.6.3</span> Connectionist Temporal Classification (CTC)  <a href="deeplearning2.html#connectionist-temporal-classification-ctc" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>CTC</strong> was introduced by <strong>Alex Graves et al.</strong> <span class="citation">(<a href="bibliography.html#ref-ref1929a">2006</a>; <a href="bibliography.html#ref-ref1879a">2013</a>)</span>. The former paper showcases a hybrid architecture, combining <strong>HMM</strong> and <strong>Bidirectional LSTM</strong> for sequence-based applications such as <strong>Speech Recognition</strong>. It explains the idea of a dynamic programming algorithm in reference to the approach used by <strong>HMM forward and backward</strong> algorithms and in further reference to the <strong>Viterbi (Decoding) Algorithm</strong>. We, therefore, encourage readers to revisit the three algorithms we discussed under the <strong>Bayesian Model</strong> section in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>).</p>
<p>If we are to describe <strong>CTC</strong>, it is a technique primarily associated with the <strong>alignment</strong> of sequence-based input in that it collapses repeated tokens so that:</p>
<p><span class="math display">\[
A, B, B, C\ \ \ \ \ \rightarrow\ \ \ \ \ \ \ A,B,C
\]</span></p>
<p>Additionally, it also recognizes that certain sequence of tokens, such as the word <strong>too</strong>, should be kept intact.</p>
<p><span class="math display">\[
T, O, O,\ \ \ \ \ \rightarrow\ \ \ \ \ \ T, O, O
\]</span></p>
<p>In <strong>Handwriting recognition</strong>, repeated tokens occur when some handwritten tokens get wide enough to extend beyond a segmentâs boundaries. This is granting we evenly partition or segment a handwritten text into frames. However, a portion of a token becomes visible across frames such that during the segmentation process, tokens may be interpreted as belonging to two frames - thus, there is duplication of tokens. <strong>CTC</strong> removes these duplicated tokens. Moreover, it does it through a modified version of the <strong>HMM</strong> forward and backward algorithms in which we add an extra <strong>blank</strong> token represented by (â<strong>-</strong>â). To visualize the algorithm, we use a graph as shown in Figure <a href="deeplearning2.html#fig:ctc">13.45</a> <span class="citation">(Waseem Gharbieh <a href="bibliography.html#ref-ref1537w">2018</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ctc"></span>
<img src="ctc.png" alt="CTC Forward and Backward Method" width="100%" />
<p class="caption">
Figure 13.45: CTC Forward and Backward Method
</p>
</div>
<p>In the figure, we construct a graph using the classic forward and modified forward methods. The difference between the two graphs is that the latter has extra rows containing <strong>blank</strong> nodes added in between rows containing token nodes. Additionally, <strong>blank</strong> rows are added at the beginning and end rows. Each column represents a timestep. Each row represents the target tokens in their correct order from the ground truth. We also include edges that traverse the graph from the left-most token node at the top to the right-most token node at the bottom. The edges follow a monotonic path such that a token node connects to a token node at the same level or next level in the next timestep. For the modified version, a token node creates an extra edge to another token node across a <strong>blank</strong> node in the next timestep (only if both token nodes do not have the same token symbols).</p>
<p>Now using the former method, to get from token node (a) at <span class="math inline">\((y_0, x_0)\)</span> to token node (d) at <span class="math inline">\((y_3, x_6)\)</span>, we can travel using the path below:</p>
<p><span class="math display">\[\begin{align*}
\text{seq}(aaabccd) = 
a(y_0, x_0)\rightarrow 
a(y_0, x_1)\rightarrow 
a(y_0, x_2)\rightarrow 
b(y_1, x_3)\rightarrow \\
c(y_2, x_4)\rightarrow 
c(y_2, x_5)\rightarrow 
d(y_3, x_6)
\end{align*}\]</span></p>
<p>Here, we can score the path by calculating the product of the probabilities of all nodes along the path such that we have:</p>
<p><span class="math display">\[\begin{align}
\pi = P(path)  = P(\text{seq}(aaabcdd)) = P(a(y_0, x_0))\times P(a(y_0, x_0))\times ... \times  P(d(y_3, x_6))
\end{align}\]</span></p>
<p>The same calculations apply to the modified version with <strong>blank</strong> rows. For example, we have the following path:</p>
<p><span class="math display">\[\begin{align}
\text{seq}(\text{aa-bc-d}) = a(y_0, x_0)\rightarrow a(y_0, x_1)\rightarrow  \text{&quot;-&quot;} \rightarrow   b(y_1, x_3)\rightarrow c(y_2, x_4)\rightarrow \text{&quot;-&quot;} \rightarrow d(y_3, x_6)
\end{align}\]</span></p>
<p>And its probabilities:</p>
<p><span class="math display">\[\begin{align}
\pi &amp;= P(path)  = P(\text{seq}(\text{aa-bc-d})) \\
&amp;= P(a(y_0, x_0))\times P(a(y_0, x_1))\times P(\text{&quot;-&quot;}) \times ... \times  P(d(y_3, x_6))
\end{align}\]</span></p>
<p>We notice that we can construct multiple paths going from (a) to (d). Therefore, we should also consider the paths generated by the <strong>backward</strong> methods (especially the modified version for <strong>CTC</strong>) as means to validate. Ultimately, our goal is to find the best (cost-effective) path, possibly one with the highest probability. This is where we can use the <strong>Beam Search</strong> method with the <strong>Viterbi</strong> Algorithm.</p>
<p>We leave readers to investigate <strong>Beam Search</strong> and how it relates to <strong>Breadth-First Search (BFS)</strong> and <strong>Best First Search (BeFS)</strong> algorithms.</p>
</div>
<div id="model-evaluation" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.6.4</span> Model Evaluation<a href="deeplearning2.html#model-evaluation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To close this section around <strong>Speech Recognition</strong> and its equivalent applications, such as <strong>Text Summarization</strong> and <strong>Machine Translation</strong>, we cover three evaluation metrics that help evaluate our modelsâ effectiveness.</p>
<ul>
<li><strong>WER</strong> stands for <strong>Word Error Rate</strong>, which is based on <strong>Levenstein distance</strong>, and it is simply a measure of the rate of error. The equation is written below:</li>
</ul>
<p><span class="math display">\[\begin{align}
\mathbf{\text{WER}} = \frac{S + D + 1}{N}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>S</strong> - number of subsitutions</li>
<li><strong>D</strong> - number of deletions</li>
<li><strong>I</strong> - number of insertions</li>
<li><strong>N</strong> - number of words in the reference (ground truth)</li>
</ul>
<p>Given the following texts:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{Ground Truth:}}&amp;\text{I travel the world in search for the fountain of youth}\\
\\
\mathbf{\text{Predicted Text:}}&amp;\text{I travel the world in search for the highest mountain}
\end{array}
\]</span></p>
<p>We note that the word <strong>highest</strong> is inserted when comparing the two texts. Furthermore, the word <strong>fountain</strong> is substituted by the word <strong>mountain</strong>, and two words are deleted, namely <strong>of</strong> and <strong>youth</strong>. Therefore, we can calculate <strong>WER</strong> like so:</p>
<p><span class="math display">\[
\mathbf{\text{WER}} = \frac{1 + 2 + 1}{11} = 0.3636364 = 36\%
\]</span></p>
<p>Note that <strong>WER</strong> relies only on word operations; thus, if we pre-process the texts such that we account for lemmatization and stemming, we may be able to reduce the error rate, granting our application necessitates to use those pre-processing methods. Otherwise, we begin to see the limitation of <strong>WER</strong>.  </p>
<ul>
<li><strong>Perplexity</strong> is a measure of randomness and is somewhat related to <strong>entropy</strong> which we covered in the <strong>Information Theory</strong> section under <strong>Bayesian Computation</strong>. Other literature commonly describes this metric as to how well a probability distribution can predict a sample (Wikipedia). We use the following equation. </li>
</ul>
<p><span class="math display">\[\begin{align}
\mathbf{Perplexity}(W) = P(w_1 \times w_2 \times ... \times w_n)^{-\frac{1}{N}} 
\end{align}\]</span></p>
<p>Suppose we have a unique set of only 10 <strong>unigrams</strong> in our vocabulary, and we calculate the probability of each <strong>unigram</strong> to occur (non-replaceable). The assumption is that the sum probability of all the unigrams combined should be 1. If each unigram has an equal probability of occurring in the set, we are looking at a 0.01 probability. Now, if we then calculate the Perplexity, we see the following:</p>
<p><span class="math display">\[\begin{align}
\mathbf{Perplexity}(w_1) = \left(\mathcal{P_{w_1}}\right)^{-\frac{1}{N}} = (0.01)^{-\frac{1}{2}} = 100
\end{align}\]</span></p>
<p>A perplexity of 100 indicates low probability.</p>
<p>The same is true for any combination of the words:</p>

<div class="sourceCode" id="cb2284"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2284-1" data-line-number="1">(<span class="dt">x =</span> <span class="kw">rep</span>(<span class="fl">0.01</span>, <span class="dv">10</span>))</a></code></pre></div>
<pre><code>##  [1] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01</code></pre>
<div class="sourceCode" id="cb2286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2286-1" data-line-number="1">perplexity &lt;-<span class="st"> </span><span class="cf">function</span>(x) { N =<span class="st"> </span><span class="kw">length</span>(x);  <span class="kw">round</span>(<span class="kw">prod</span>(x)<span class="op">^</span>(<span class="op">-</span><span class="dv">1</span><span class="op">/</span>N))}</a>
<a class="sourceLine" id="cb2286-2" data-line-number="2"><span class="kw">perplexity</span>(x[<span class="dv">1</span>])</a></code></pre></div>
<pre><code>## [1] 100</code></pre>
<div class="sourceCode" id="cb2288"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2288-1" data-line-number="1"><span class="kw">perplexity</span>(<span class="kw">c</span>(x[<span class="dv">1</span>], x[<span class="dv">3</span>], x[<span class="dv">5</span>]))</a></code></pre></div>
<pre><code>## [1] 100</code></pre>

<p>Now, assume we change the probability distribution such that the first unigram has a 0.90 probability of occurring while the rest have an equal probability at 0.01.</p>

<div class="sourceCode" id="cb2290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2290-1" data-line-number="1">(<span class="dt">x =</span> <span class="kw">c</span>(<span class="fl">0.90</span>, <span class="kw">rep</span>(<span class="fl">0.01</span>, <span class="dv">9</span>)))</a></code></pre></div>
<pre><code>##  [1] 0.90 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01</code></pre>
<div class="sourceCode" id="cb2292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2292-1" data-line-number="1"><span class="kw">perplexity</span>(x[<span class="dv">1</span>]) <span class="co"># this has the 0.90 probability</span></a></code></pre></div>
<pre><code>## [1] 1</code></pre>
<div class="sourceCode" id="cb2294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2294-1" data-line-number="1"><span class="kw">perplexity</span>(<span class="kw">c</span>(x[<span class="dv">1</span>], x[<span class="dv">3</span>], x[<span class="dv">5</span>])) <span class="co"># this has 0.90, 0.01, 0.1 probabilites (mix)</span></a></code></pre></div>
<pre><code>## [1] 22</code></pre>
<div class="sourceCode" id="cb2296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2296-1" data-line-number="1"><span class="kw">perplexity</span>(<span class="kw">c</span>(x[<span class="dv">2</span>], x[<span class="dv">4</span>], x[<span class="dv">6</span>])) <span class="co"># this has 0.01, 0.01, 0.01 probabilities</span></a></code></pre></div>
<pre><code>## [1] 100</code></pre>

<p>A perplexity of 1 indicates high probability.</p>
<p>Note that N-grams also apply in this case.</p>
<ul>
<li><strong>BLEU</strong> stands for <strong>BiLingual Evaluation Understudy</strong> introduced by Kishore Papineni et al. <span class="citation">(<a href="bibliography.html#ref-ref1556k">2002</a>)</span>. It is a measure between 0 and 1 that evaluates machine translation quality. Below, we reference the formulations from the paper.  </li>
</ul>
<p><span class="math display">\[\begin{align}
P_n = 
\frac{\sum_{C\in \{Candidates\}} \sum_{\text{n-gram}\in C}  \mathbf{Count}(clip(\text{n-gram}))}
     {\sum_{C&#39;\in \{Candidates\}} \sum_{\text{n-gram&#39;}\in C&#39;}  \mathbf{Count}(\text{n-gram&#39;})}
\end{align}\]</span></p>
<p>where <span class="math inline">\(P_n\ \text{is N-gram Precision}\)</span></p>
<p>Note that the equation above is a modified formulation for calculating an <strong>N-gram Precision</strong> because of <strong>clipping</strong>, eliminating redundant N-grams.</p>
<p><span class="math display">\[\begin{align}
BP \times \exp\left[\frac{1}{N}\sum_{n=1}^N \left(\log P_n\right)\right],
\ \ \ \ \ 
BP =\begin{cases}
1 &amp; \text{if c} &gt; r\\
exp(1 - \frac{r}{c}) &amp; \text{if c } \le r
\end{cases} \label{eqn:eqnnumber808}
\end{align}\]</span></p>
<p>where <strong>BP</strong> is <strong>Brevity Penality</strong>, <strong>r</strong> represents reference corpus length, <strong>c</strong> is the length of candidate translation, and <strong>N</strong> is the N-gram to consider. Here, we use N=4 (up to 4-grams). </p>
<p>To use this metric, we need to have a list of <strong>references</strong> and <strong>candidates</strong>. For example:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{Candidate 1}}:\text{I wish to travel the the world}\ \ \ \ (generated)\\
\\
\mathbf{\text{Reference 1}}:\text{I did travel the world}\\
\mathbf{\text{Reference 2}}:\text{I like traveling the world}\\
\mathbf{\text{Reference 3}}:\text{I really wish to travel the world}\\
\end{array}
\]</span></p>
<p>Here, <strong>candidate</strong> refers to the text generated from our model. We start by calculating the <strong>N-gram Precision</strong>, a way to count the number of words that match texts. Let us now reference the table below for the four texts in our example:</p>
<p><span class="math display">\[
\begin{array}{lllll}
\mathbf{\text{N-Grams}} &amp; \mathbf{\text{Unigram}} &amp; \mathbf{\text{Bi-Gram}} &amp; 
     \mathbf{\text{Tri-Gram}} &amp; \mathbf{\text{4-Gram}}\\
\mathbf{\text{Candidate 1}} &amp; 7 &amp; 6 &amp; 5 &amp; 4 \\
\mathbf{\text{Reference 1}} &amp; 5 &amp; 4 &amp; 3 &amp; 2 \\
\mathbf{\text{Reference 2}} &amp; 5 &amp; 4 &amp; 3 &amp; 2\\  
\mathbf{\text{Reference 3}} &amp; 7 &amp; 6 &amp; 5 &amp;  4\\
\end{array}
\]</span></p>
<p>Comparing candidate one and reference one, we see that the words <strong>I</strong>, <strong>travel</strong>, <strong>the</strong>, and <strong>world</strong> match; therefore, using the following formula, we have a <strong>precision score</strong> of:</p>
<p><span class="math display">\[\begin{align}
\text{Unigram Precision Score} (P_1) = \frac{\text{clip(Num of matching 1-gram)}}{\text{Num of 1-gram Generated}}
= \frac{4}{7}  = 0.5714286 
\end{align}\]</span></p>
<p>Notice that, because of <strong>clipping</strong>, the word <strong>the</strong> that occurs twice in our generated text only receives a count of 1.</p>
<p>Let us calculate the precision for the <strong>2-gram</strong>, <strong>3-gram</strong>, and <strong>3-gram</strong>. Assume that our model has not gone through <strong>lemmatization</strong> and <strong>stemming</strong>. We compare the bare N-grams:</p>
<p><span class="math display">\[
\text{2-gram Precision Score} (P_2) = \frac{2}{6} = 0.33 
\]</span>
Here, the <strong>2-grams</strong> that match are {<strong>travel the</strong>} and {<strong>the world</strong>} between <strong>candidate 1</strong> and <strong>reference 1</strong> texts.</p>
<p><span class="math display">\[
\text{3-gram Precision Score} (P_3)  = \frac{0}{5} = 0
\]</span>
Here, there are no <strong>3-grams</strong> that match.</p>
<p><span class="math display">\[
\text{4-gram Precision Score} (P_4) = \frac{0}{4} = 0
\]</span>
Here, there are no 4-gram combinations that match.</p>
<p>Now, to get the <strong>BLUE</strong> score, we take the average of the log of <span class="math inline">\(P_n\)</span>. Notice below that if there are no matching grams, we obtain the log of zero, which renders <span class="math inline">\(-\infty\)</span>. Therefore, it may help mathematically to add <span class="math inline">\(\text{eps}=1e^-100\)</span> (our choice).</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{BLEU} &amp;= BP \times \exp\left(\frac{1}{4}\left[
   \log\left(\frac{4}{7}\right) + \log\left(\frac{2}{6}\right) + 
   \log\left(\frac{0}{5}\right) + \log\left(\frac{0}{4}\right)\right]\right)\\
&amp;=BP \times \text{3.1239399e-51} = 0
\end{array}
\]</span></p>
<div class="sourceCode" id="cb2298"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2298-1" data-line-number="1">eps =<span class="st"> </span><span class="fl">1e-100</span>; N =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb2298-2" data-line-number="2"><span class="kw">exp</span>(<span class="dv">1</span><span class="op">/</span>N <span class="op">*</span><span class="st"> </span>(<span class="kw">log</span>(<span class="dv">4</span><span class="op">/</span><span class="dv">7</span>) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(<span class="dv">2</span><span class="op">/</span><span class="dv">6</span>) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(eps<span class="op">/</span><span class="dv">5</span>) <span class="op">+</span><span class="st"> </span><span class="kw">log</span>(eps<span class="op">/</span><span class="dv">4</span>)))</a></code></pre></div>
<pre><code>## [1] 3.1239399e-51</code></pre>
<p>The <strong>BP</strong> is calculated based on <strong>r</strong> = 5 and <strong>c</strong> = 7. There are 7 words in <strong>candidate 1</strong> and 5 words in <strong>reference 1</strong>. Therefore, obtain the below <strong>BP</strong> for our case:</p>
<div class="sourceCode" id="cb2300"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2300-1" data-line-number="1">r =<span class="st"> </span><span class="dv">5</span>; c =<span class="st"> </span><span class="dv">7</span></a>
<a class="sourceLine" id="cb2300-2" data-line-number="2">(<span class="dt">BP =</span> <span class="kw">ifelse</span>(c <span class="op">&gt;</span><span class="st"> </span>r, <span class="dv">1</span>, <span class="kw">exp</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r<span class="op">/</span>c)))</a></code></pre></div>
<pre><code>## [1] 1</code></pre>
</div>
</div>
<div id="generative-adversarial-network-gan" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.7</span> Generative Adversarial Network (GAN)  <a href="deeplearning2.html#generative-adversarial-network-gan" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>GAN</strong> was introduced by Ian J. Goodfellow et al.Â in <span class="citation">(<a href="bibliography.html#ref-ref1565i">2014</a>)</span>. The basic concept of <strong>GAN</strong> is simple. We model two deep neural networks that pit against each other (thus adversarial). One model is trained by one network to <strong>generate</strong> synthetic data (also known as augmentation), and the other is trained by the other network to <strong>discriminate</strong> against the generated data. Throughout this training process, the ultimate goal is to have both models from the two networks, if trained and optimized well, ironically work together (in an adversarial fashion) to ultimately generate the most plausible data, rendering the data as though it is produced realistically. On the other hand, the <strong>discriminative</strong> nature of the other network makes it possible to allow the model to be powerful enough to discriminate against <strong>spam</strong> as an example. Additionally, ideas around <strong>GAN</strong> extend to the design of games.</p>
<p>Two key architectural components that make <strong>GAN</strong> effective are the <strong>generator</strong> and <strong>discriminator</strong>. In image processing, each component can be architected using <strong>CNN</strong>, e.g., <strong>Deep Convolutional GAN (DCGAN)</strong>.</p>
<p>In terms of a <strong>generator</strong>, there are methods proposed by papers on the best way to generate data. Among many methods, below lists the fundamental methods that can be applied. They are covered in <strong>Bayesian Computation</strong>.</p>
<ul>
<li><strong>Rejection sampling</strong></li>
<li><strong>Metropolis-Hasting algorithm</strong></li>
</ul>
<p>As for <strong>GAN</strong> models, there are evaluation methods and metrics proposed by papers to evaluate the quality of the image produced by these models. However, it seems other literature agrees that there is no consensus as to which evaluation method is accepted as standard. Therefore, we only mention two common evaluation methods for <strong>GAN</strong>, namely <strong>Inception Score (IS)</strong> and <strong>FrÃ©chet Inception Distance (FID)</strong>. The latter is an enhanced variant of the former in terms of coverage. While <strong>IS</strong> focuses on evaluation against the generated images, <strong>FID</strong> also accommodates authentic images. Both evaluation metrics sit upon the fundamental idea of <strong>KL Divergence</strong> in which probability distributions are compared and measured.</p>
<p>Now, in terms of <strong>discriminator</strong>, this component acts as a <strong>critic</strong>. Its role is to distinguish fake data from actual data. Therefore, it becomes beneficial for the discriminator to maximize the distance between the two data. That is where we cover <strong>WGAN</strong>.</p>
<p>A variant of <strong>GAN</strong> called <strong>Wasserstein GAN (WGAN)</strong> was introduced by Martin Arjovsky et al. <span class="citation">(<a href="bibliography.html#ref-ref1565i">2014</a>)</span>. <strong>WGAN</strong> covers the idea of <strong>Wasserstein distance metrics</strong> introduced by <strong>Leonid VaserÅ¡tein</strong> in 1969. Like <strong>KL Divergence</strong>, <strong>Wasserstein (W) metric</strong> compares probability distributions. However, unlike <strong>KL divergence</strong>, <strong>W</strong> is symmetric. For example, <strong>KL divergence</strong> follows the condition below (which is not symmetric):</p>
<p><span class="math display">\[\begin{align}
P(q||p) \ne P(p||q)
\end{align}\]</span></p>
<p>Additionally, <strong>WGAN</strong> is also called <strong>Earth Moverâs distance</strong> because, in a way, it measures distance based on how far to transport pieces of distribution - the distribution mass - from one region to another. In other words, we measure the distance between two regions instead of between specific points of two regions. In doing so, we allow much broader coverage of distance measurement, and it helps the <strong>discriminator</strong> identify fake data, especially if the type of data is an image.</p>
<p>We leave readers to investigate other distance measurements such as <strong>Kolmogorov-Smirnov distance</strong> and <strong>Jensen-Shannon distance</strong>.</p>
</div>
<div id="deep-reinforcement-network-dqn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.8</span> Deep Reinforcement Network (DQN)  <a href="deeplearning2.html#deep-reinforcement-network-dqn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>DQN</strong> was introduced by DeepMind in 2015. What separates <strong>DQN</strong> from vanilla <strong>DNN</strong> architectures using <strong>MLP</strong>, <strong>CNN</strong>, <strong>RNN</strong>, <strong>TNN</strong>, and <strong>GAN</strong> is the use of <strong>reward</strong> and <strong>penalty</strong> concept. If we can design a <strong>DNN</strong> to be <strong>motivated</strong> or <strong>encouraged</strong> to learn (also called <strong>Reinforcement Learning</strong>), that pushes one more step closer to our goal of an <strong>Artificial Intelligence (AI)</strong> system.</p>
<p>In designing <strong>DQN</strong>, other literature introduces an <strong>agent</strong> that gets rewarded for doing a good job and gets penalized for a bad job. Therefore, it is sensible to assume that the agent should at least try to <strong>remember</strong> (having an <strong>associative memory</strong>) for what makes a job rewarding. This <strong>cumulative reward</strong> encourages the <strong>agent</strong> to move in that direction every single time.</p>
<p>Now, how the <strong>agent</strong> is penalized is based on specific rules or policies. An agent should abide by such restrictions to avoid penalties. One of the tricks used is a <strong>trial and error</strong> approach. By attempting different actions, the agent can adapt. It accumulates these long-term actions to be able to <strong>survive</strong> in any situational conditions.</p>
<p>Applications that may benefit from <strong>DQN</strong> are <strong>self-healing</strong> and <strong>self-adaptive</strong> applications. For example, anomaly detection also falls under this category.</p>
</div>
<div id="summary-8" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.9</span> Summary<a href="deeplearning2.html#summary-8" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We end this chapter keeping in mind the different <strong>Deep Neural Network</strong> that we introduced, starting with <strong>MLP</strong> followed by <strong>CNN</strong>, <strong>ResNet</strong>, <strong>RNN</strong> and its variants <strong>LSTM</strong>, <strong>GRU</strong>, <strong>BiLSTM</strong>, <strong>BiGRU</strong>, then <strong>TNN</strong>. Almost all <strong>DNN</strong> architectures rely on a combination of <strong>CNN</strong>, <strong>RNN</strong>, and <strong>TNN</strong> stacked or layered one over the other. It is indeed just a matter of creativity and sound design to produce an architecture with promising results. The problems that such <strong>DNN</strong> architectures try to solve rely on heavy computation so using <strong>GPUs</strong> and cloud-based distributed systems also needs to be considered. If such architecture can be designed more granularly and in a more distributed parallel fashion, then the needed speed of training and inference can be achieved.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="deeplearning1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="distributedcomputation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
