<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 4 Numerical Calculus | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 4 Numerical Calculus | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 4 Numerical Calculus | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="numericallinearalgebra.html"/>
<link rel="next" href="numericalprobability.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="numericalcalculus" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 4</span> Numerical Calculus<a href="numericalcalculus.html#numericalcalculus" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '4.'+n}
      } 
  }
});
</script>
<p>Following the natural order of things, we have shown how we went from Direct to Indirect Methods. Next, as a continuation of the <strong>Numerical Approximation</strong>, we introduce several fundamental numerical methods relevant to <strong>Integration</strong> and <strong>Differentiation</strong>. Additionally, we will cover solutions to problems in Dynamic Systems based on approximation in the latter part of this chapter.</p>
<p>In this chapter, we focus on Numerical Analysis in the context of Calculus as we reference the great works of Atkinson K. E.<span class="citation">(<a href="bibliography.html#ref-ref288k">1989</a>)</span>, Adams R. <span class="citation">(<a href="bibliography.html#ref-ref1322r">1995</a>)</span>, Heath M.T. <span class="citation">(<a href="bibliography.html#ref-ref187m">2002</a>)</span>, Burden R.L. et al. <span class="citation">(<a href="bibliography.html#ref-ref196r">2005</a>)</span>, Larson R. et al. <span class="citation">(<a href="bibliography.html#ref-ref216r">2006</a>)</span>, Press W.H et al. <span class="citation">(<a href="bibliography.html#ref-ref215w">2007</a>)</span>, and Strauss W. A. <span class="citation">(<a href="bibliography.html#ref-ref649w">2008</a>)</span> along with other additional references for consistency.</p>
<p>For a summary, let us review Figure <a href="numericalcalculus.html#fig:odepde">4.1</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:odepde"></span>
<img src="ode_pde.png" alt="ODE and PDE" width="90%" />
<p class="caption">
Figure 4.1: ODE and PDE
</p>
</div>
<p>Let us first have a quick refresh of <strong>Calculus</strong> as a starting point; though, readers with a full grasp of <strong>Calculus</strong> can skip the next few sections.</p>
<div id="introductory-calculus" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.1</span> Introductory Calculus<a href="numericalcalculus.html#introductory-calculus" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the hallmark achievements of the 17th century by <strong>Sir Isaac Newton and Gottfried Leibnitz</strong> was the discovery of <strong>Calculus</strong>. This branch of mathematics deals with <strong>differentiation</strong> and <strong>integration</strong> which we briefly cover in this chapter.</p>
<p>This section is only a review of <strong>Calculus</strong>. There are many textbooks that explain <strong>Calculus</strong> in detail. Our goal here is to recall standard <strong>rules</strong> in derivation and integration such as <strong>chain rules</strong>, <strong>power rules</strong>, <strong>product rules</strong>, <strong>trigonometric rules</strong>, etc. without too much of an explanation of how those rules are formed. The main emphasis in subsequent sections is to present the numerical methods available when specific integrations cannot be achieved by analytical means.</p>
<p>Let us start by explaining about functions.</p>
<div id="function" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.1</span> Function<a href="numericalcalculus.html#function" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In its most general term, a function represents simple to complex expressions that take X number of input variables and produce Y number of output values. This definition becomes more apparent in computer programming, in which program functions can put complex logic into code based on what we intend the computer to accomplish. Mathematically, we can simply say that a function, in its simplest form, takes an input and produces an output.</p>
<p>Here is an example: f(x) = x + 1</p>
<p>The example function f(x) accepts a value and stores it into a variable <strong>x</strong>. Then, the x+1 expression performs an addition. So if the <strong>x</strong> variable takes a value of 200, then the expression adds up to 201, and the f(x) function outputs the result as 201.</p>
<p>If f(x) = y and âyâ is the output variable, then f(x) = y = x + 1; therefore f(200) = 200 + 1 = 201. Therefore, y = 201.</p>
<p>Let us introduce three terms associated with functions.</p>
<ul>
<li><p><strong>Domain</strong> is a set of input values defined for functions. The set of input values can also be termed <strong>arguments</strong> or <strong>argument</strong> values. In computer programming, one will usually encounter such terms, particularly when coding a function.</p></li>
<li><p><strong>Range</strong> is a set of output values for functions.</p></li>
<li><p><strong>Support</strong> is a subset of the <strong>domain</strong> that <strong>supports</strong> the function. We will use such terms when discussing improper integrals and statistical probabilities.</p></li>
</ul>
<p>Figure <a href="numericalcalculus.html#fig:limits1">4.2</a> shows a plot of two functions: A <strong>quadratic function</strong> and a <strong>linear</strong> function.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:limits1"></span>
<img src="DS_files/figure-html/limits1-1.png" alt="Functions" width="70%" />
<p class="caption">
Figure 4.2: Functions
</p>
</div>

<p>The convex curve is drawn by the following quadratic function: <span class="math inline">\(f(x) = x^2 + 50\)</span>.</p>
<p>The slanted line is drawn by the following linear function: <span class="math inline">\(f(x) = x + 50\)</span>.</p>
<p>Each of the two functions accepts a value of X as input and produces an output value - let us call it a value of Y. See Table <a href="numericalcalculus.html#tab:functions2">4.1</a>.</p>

<table>
<caption><span id="tab:functions2">Table 4.1: </span>X and Y Coordinates</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">1</th>
<th align="right">2</th>
<th align="right">3</th>
<th align="right">4</th>
<th align="right">5</th>
<th align="right">6</th>
<th align="right">7</th>
<th align="right">8</th>
<th align="right">9</th>
<th align="right">10</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><strong>X Input</strong></td>
<td align="right">-15</td>
<td align="right">-14</td>
<td align="right">-13</td>
<td align="right">-12</td>
<td align="right">-11</td>
<td align="right">-10</td>
<td align="right">-9</td>
<td align="right">-8</td>
<td align="right">-7</td>
<td align="right">-6</td>
</tr>
<tr class="even">
<td align="left"><strong>Quadratic Ouput</strong></td>
<td align="right">275</td>
<td align="right">246</td>
<td align="right">219</td>
<td align="right">194</td>
<td align="right">171</td>
<td align="right">150</td>
<td align="right">131</td>
<td align="right">114</td>
<td align="right">99</td>
<td align="right">86</td>
</tr>
<tr class="odd">
<td align="left"><strong>Linear Output</strong></td>
<td align="right">35</td>
<td align="right">36</td>
<td align="right">37</td>
<td align="right">38</td>
<td align="right">39</td>
<td align="right">40</td>
<td align="right">41</td>
<td align="right">42</td>
<td align="right">43</td>
<td align="right">44</td>
</tr>
</tbody>
</table>

<p>So, if we choose the first coordinates where <span class="math inline">\(x = -15\)</span>, then we have the following:</p>
<p><span class="math display">\[\begin{align*}
Quadratic\ function &amp; = f(x) = x^2 + 50 = (-15)^2 + 50 = 275 \\
Linear\ function &amp; = f(x) = x + 50 = (-15) + 50 = 35
\end{align*}\]</span></p>
<p>Here, we provide (-15) as an input value of x, and the quadratic function f(x) produces (275) as an output value of y. Similarly, we provide (-15) as an input value of x, and the linear function f(x) produces (35) as an output value of y, given that <span class="math inline">\(y = f(x)\)</span>.</p>
</div>
<div id="slopes" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.2</span> Slopes<a href="numericalcalculus.html#slopes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Slope</strong> is described as the rise over the run. It is expressed as such:</p>
<p><span class="math display">\[\begin{align}
slope = \frac{rise}{run} = \frac{y2 - y1}{x2 - x1}
\end{align}\]</span></p>
<p>We will use a line using a linear function to explain a <strong>slope</strong> as plotted in Figure <a href="numericalcalculus.html#fig:slope">4.3</a>. As the horizontal arrow (the run) increases, so does the vertical arrow (the rise). Mathematically, as illustrated in Figure <a href="numericalcalculus.html#fig:slope">4.3</a>, for every delta change in x - denoted by the symbol (<span class="math inline">\(\delta\)</span>), there is a corresponding delta change in y - denoted by the symbol (<span class="math inline">\(\Delta\)</span>).</p>
<p><span class="math display">\[\exists x\in \mathbb{R} (\delta x \to \Delta y)\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slope"></span>
<img src="DS_files/figure-html/slope-1.png" alt="Slope" width="70%" />
<p class="caption">
Figure 4.3: Slope
</p>
</div>
<p>The two points correspond to the (x1, y1) and the (x2, y2) coordinates. They are represented as:</p>
<p><span class="math display">\[\begin{align}
x1 {} &amp; = x \\
x2 &amp; = x + \delta \\
y1 &amp; = y =  f(x) \\
y2 &amp; = y + \Delta = f(x + \delta) 
\end{align}\]</span></p>
<p>The change from point (x1, y1) to point (x2, y2) is represented as:</p>
<p><span class="math display">\[\begin{align*}
\Delta y  {} &amp; = y2 -y1 = f(x + \delta) - f(x) \\
\delta x  &amp; = x2 - x1 = (x + \delta) - x = \delta
\end{align*}\]</span></p>
<p>Expanding our slope, we get:</p>
<p><span class="math display">\[\begin{align}
slope = \frac{rise}{run} = \frac{y2 - y1}{x2 - x1} = \frac{\Delta y}{\delta x} = \frac{f(x + \delta)-{f(x)}}{\delta}
\end{align}\]</span></p>
<p>For now, let us replace <span class="math inline">\(\delta\)</span> with âhâ:</p>
<p><span class="math display">\[\delta = h\]</span></p>
<p>With that, here is the final equation for the slope:</p>
<p><span class="math display">\[\begin{align}
slope = \frac{f(x + h)-{f(x)}}{h}
\end{align}\]</span></p>
<p>An essential concept about slopes is that it represents the average <strong>rate</strong> of the change of a line <span class="citation">(Larson R. et al. <a href="bibliography.html#ref-ref216r">2006</a>, 105â10)</span>. Just by the use of averages, the following terms are interchangeable:</p>
<ul>
<li><strong>average rate of change</strong> of a line</li>
<li><strong>slope</strong> of a line</li>
</ul>
<p>Another essential fact is that every point in the curve does not have the same average rate; the slope changes at every point. That is explained in the next section.</p>
</div>
<div id="limits" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.3</span> Limits<a href="numericalcalculus.html#limits" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Noting that curves have curvatures - no linearity - one can put an imaginary line at a curvature point, and it balances itself perpendicular to the curve at that point. This line is called a <strong>tangent line</strong> - and is always <strong>perpendicular to the curve at a point</strong>. For example, in Figure <a href="numericalcalculus.html#fig:slopes">4.4</a>, there are three tangent lines (red lines) that intersect with the curve at three different points (points <strong>a</strong>, <strong>b</strong>, and <strong>c</strong>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:slopes"></span>
<img src="slopes.png" alt="Slopes in a Curve" width="70%" />
<p class="caption">
Figure 4.4: Slopes in a Curve
</p>
</div>
<p>If we have to walk along the curve, every point we step into has an imaginary tangent line with a slope that reflects the average range of change at that point in the curve. Therefore, unlike a straight (linear) line; in a curve, slopes may be different each time at a given point as we walk along the curve.</p>
<p>As for limits, the goal here is to find the limit of a function. A limit is the output value of a curve function as a given input approaches a particular point, p.</p>
<p><span class="math display">\[
\lim_{x\to p}f(x) = output\ value = L
\]</span>
Let us use Figure <a href="numericalcalculus.html#fig:limits">4.5</a> to explain further.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:limits"></span>
<img src="embed0026.png" alt="Limits" width="70%" />
<p class="caption">
Figure 4.5: Limits
</p>
</div>
<p>As h approaches 0, the secant line gets closer and closer to the tangent line in the chart. It gets closer that the slope of the secant line is almost equal to the slope of the tangent line at a point.</p>
<p>Using the slope formula, we can now formulate the limit of the function.</p>
<p>The limit of a function - f(x) - is written as fâ(x).</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) =\lim_{h\to 0}\frac{f(x + h) - f(x)}{h} = L
\end{align}\]</span></p>
<p>Example 1: Find the limit of <span class="math inline">\(f(x) = x^2 + 1\)</span> as h approaches 0.</p>

<p><span class="math display">\[\begin{align*}{lll}
f&#39;(x)
   &amp;= \lim_{h\to 0}\frac{f(x + h) - f(x)}{h} \\ 
   &amp;= \lim_{h\to 0}\frac{((x + h)^2 + 1 )  - (x^2+1)}{h} \\
   {} &amp;= \lim_{h\to 0}\frac{(x^2 + 2xh + h^2 + 1)  - (x^2+1)}{h} \\
   &amp;= \lim_{h\to 0}\frac{x^2 + 2xh + h^2 + 1  - x^2 - 1}{h} \\
   {} &amp;= \lim_{h\to 0}\frac{2xh + h^2}{h} \\ &amp;= \lim_{h\to 0}\frac{h(2x + h)}{h} \\
   {} &amp;= \lim_{h\to 0} ( 2x + h ) \\ &amp;= \lim_{h\to 0} ( 2x + 0 )\ ;\ where\ h \to 0\\
   {} &amp;= (2x + 0) &amp; {}\\
   {} &amp;= 2x &amp; {}
\end{align*}\]</span>
</p>
<p>Example 2: Find the limit of <span class="math inline">\(f(x) = x^3 + x^2 + 1\)</span> as h approaches 0.</p>

<p><span class="math display">\[\begin{align*}
f&#39;(x) &amp; = \lim_{h\to 0}\frac{f(x + h) - f(x)}{h} \\
   &amp; = \lim_{h\to 0}\frac{((x + h)^3 + (x + h)^2 + 1 )  - (x^3 + x^2+1)}{h} \\
   &amp; = \lim_{h\to 0}\frac{(x + h)^3 + (x + h)^2 + 1  - x^3 - x^2 - 1}{h} \\
   &amp; = \lim_{h\to 0}\frac{(x + h)^3 + (x + h)^2 - x^3 - x^2}{h} \\
   &amp; \text{(use difference of cube and square)}\\
   &amp; = \lim_{h\to 0}\frac{((x + h)^3 - x^3) + ( (x + h)^2 - x^2) }{h}\\
   &amp; = \lim_{h\to 0}\frac{((x + h)^2 + (x + h)x + x^2)((x + h) - x) + ((x + h) + x)((x + h) - x) }{h} \\
   &amp; \text{(common factor)}\\  
   &amp; = \lim_{h\to 0}\frac{(((x + h)^2 + (x + h)x + x^2) + ((x + h) + x))((x + h) - x) }{h}\\
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
   &amp; = \lim_{h\to 0}\frac{((x + h)^2 + (x + h)x + x^2 + 2x + h)(h)}{h} \\  
   &amp; = \lim_{h\to 0}((x + h)^2 + (x + h)x + x^2 + 2x + h) \\
   &amp; = \lim_{h\to 0}((x^2 + xh + h^2) + x^2 + xh + x^2 + 2x + h) \\
   &amp; = \lim_{h\to 0}(x^2 + xh + h^2 + x^2 + xh + x^2 + 2x + h) \\
   &amp; = \lim_{h\to 0}(3x^2 + 2x + xh + h^2  + xh +  h) \\
   &amp; = \lim_{h\to 0} ( 3x^2 + 2x + x0 + 0^2 + x0 + 0)\ ;\ where\ h \to 0\\
   &amp; = (3x^2 + 2x + 0) \\
   &amp; = 3x^2 + 2x
\end{align*}\]</span>
</p>
<p>Notice that we have to go through a long computation for the limit of <span class="math display">\[f(x) = x^3 + x^2 + 1\]</span></p>
<p>It can get complex as we deal with a higher order of polynomials.</p>
<p>Example: Find the limit of <span class="math inline">\(f(x) = x^6 + x^5 + x^4 + x^3 + x^2 + 1\)</span> as h approaches 0.</p>

<p><span class="math display">\[\begin{align*}
f&#39;(x) {} 
   &amp; = \lim_{h\to 0}\frac{f(x + h) - f(x)}{h} \\
   &amp; = \lim_{h\to 0}\frac{(x + h)^6 + (x + h)^5 + ((x + h)^4 + (x + h)^3 + (x + h)^2 + 1 )  - (x^4 + x^3 + x^2+1)}{h} \\
   &amp; \vdots \\
   &amp; = 6x^5 + 5x^4 + 4x^3 + 3x^2 + 2x
\end{align*}\]</span>
</p>
<p>The answer to the following expression <span class="math inline">\(f(x) = x^6 + x^5 + x^4 + x^3 + x^2 + 1\)</span> for the limit is:</p>
<p><span class="math display">\[6x^5 + 5x^4 + 4x^3 + 3x^2 + 2x\]</span></p>
<p>How did we arrive at the answer? Foremost, there is a short-cut method governed by rules, which is by way of derivatives.</p>
</div>
<div id="derivatives" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.4</span> Derivatives<a href="numericalcalculus.html#derivatives" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us define <strong>Derivatives</strong> in terms of limits the same way we define limits in the previous discussion. Looking for the derivative of a given function is the same as looking for the limit of a function. That is similar to finding the slope of a tangent line at a point.</p>
<p>A slope of a tangent line at a point is defined as the change of y over the change of x. In other words, we are computing the average rate of change.</p>
<p><span class="math display">\[\begin{align}
\text{average rate of change} = \frac{\text{change of a condition (y)}}{\text{change of another condition (x)}}
\end{align}\]</span></p>
<p>In general terms, the average rate of change can be applied to any scenario that has properties of change.</p>
<p><strong>Derivative notation</strong></p>
<p>The derivative of a function - f(x) - is written as:</p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = f&#39;(x) = \lim_{x\to0} \frac{f(x + h) - f(x)}{h}
\end{align}\]</span></p>
<p>where fâ(x) is a Newton notation,</p>
<p>and where <span class="math inline">\(\frac{dy}{dx}\)</span> is a Leibnitz notation and is read as the derivative of y with respect to x.</p>
<p><span class="math display">\[\begin{align}
Since {}&amp; : y = f(x) \nonumber \\
\nonumber \\
\text{It follows that} &amp; : \frac{dy}{dx} = \frac{d f(x)}{dx} = \frac{df}{dx} (x) = \frac{d}{dx} f(x)
\end{align}\]</span></p>
<p>Now, instead of following the way we compute for limits, we will use a different method called <strong>differentiation</strong> using a set of rules.</p>
<p>Let us now differentiate (or find the derivative of a function).</p>
<p><strong>Constant Rule</strong></p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx}(au) = a\frac{d}{dx}(u),\ \ \ where\ a\ \to constant 
\end{align}\]</span></p>
<p>Example: Find the derivative of f(x) = 25x.</p>
<p><span class="math display">\[
\frac{dy}{dx} = \frac{d}{dx}(25x) = 25\frac{d}{dx}{x} = 25
\]</span>
Find the derivative of f(x) = 25.</p>
<p><span class="math display">\[
\frac{dy}{dx} = \frac{d}{dx}(25) = 0
\]</span></p>
<p><strong>Power Rule</strong> </p>

<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx}(x^n) =  nx^{n-1}
\end{align}\]</span>
</p>
<p>Example: Find the derivative of f(x) = <span class="math inline">\(2x^3\cdot3x^2 + 10x^4 + 2x^3 + x^2 + x + 1\)</span>.</p>

<p><span class="math display">\[\begin{align*}
\frac{d}{dx}f(x) {}
&amp; = \frac{d}{dx}( 2x^3\cdot 3x^2 + 10x^4 + 2x^3 + x^2 + x + 1 )\\
&amp; = \frac{d}{dx}( 6x^5 + 10x^4 + 2x^3 + x^2 + x + 1 )\\
&amp; = (5)6x^{5-1} + (4)10x^{4-1} + (3)2x^{3-1} + (2)x^{2-1} + (1)x^{1-1} \\
&amp; = 30x^4 + 40x^3 + 6x^2 + 2x \\
&amp; = 2x( 15x^3 + 20x^2 + 3x + 1)
\end{align*}\]</span>
</p>
<p><strong>Sum and Difference Rule</strong></p>

<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx}(f\pm g) = \frac{d}{dx} f + \frac{d}{dx} g = (f\pm g)&#39; = f&#39;\pm g&#39;
\end{align}\]</span>
</p>
<p>Example:</p>

<p><span class="math display">\[\begin{align*}
\frac{dy}{dx} = \frac{d}{dx}(4x^3 + x^2)  {}&amp; = \frac{d}{dx} (4x^3) + \frac{d}{dx} (x^2)\\
&amp; = (3)4x^{3-1} + (2)x^{2-1}\\
&amp; = 12x^2 + 2x^1\\
&amp;=2x(6x+1)
\end{align*}\]</span>
</p>
<p><strong>Trigonometric Rule</strong> </p>
<table>
<caption><span id="tab:derivatives">Table 4.2: </span>Trigonometric Derivatives</caption>
<colgroup>
<col width="48%" />
<col width="51%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Derivative Functions</th>
<th align="left">Derivatives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}sin(x)\)</span></td>
<td align="left"><span class="math inline">\(cos(x)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}cos(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -sin(x)\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}tan(x)\)</span></td>
<td align="left"><span class="math inline">\(\ sec^2(x)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}cot(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -csc^2(x)\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}sec(x)\)</span></td>
<td align="left"><span class="math inline">\(\ sec(x)tan(x)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}csc(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -csc(x)cot(x)\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}sin^{-1}x\ or\ arcsin(x)\)</span></td>
<td align="left"><span class="math inline">\(\ \frac{1}{\sqrt{1 - x^2}}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}cos^{-1}x\ or\ arccos(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -\frac{1}{\sqrt{1 - x^2}}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}tan^{-1}x\ or\ arctan(x)\)</span></td>
<td align="left"><span class="math inline">\(\ \frac{1}{x^2 + 1}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}cot^{-1}x\ or\ arccot(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -\frac{1}{x^2 + 1}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}sec^{-1}x\ or\ arcsec(x)\)</span></td>
<td align="left"><span class="math inline">\(\ \frac{1}{\mid x \mid \sqrt{x^2 + 1}}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}csc^{-1}x\ or\ arcscs(x)\)</span></td>
<td align="left"><span class="math inline">\(\ -\frac{1}{\mid x \mid \sqrt{x^2 + 1}}\)</span></td>
</tr>
</tbody>
</table>
<p>Example: Find the derivative of f(x) = <span class="math inline">\(2x^3\cdot3x^2 + cos(x)\)</span>.</p>

<p><span class="math display">\[\begin{align*}
f&#39;(x) {}&amp; =  \frac{d}{dx}(2x^3\cdot3x^2 + cos(x) )\\
&amp; =  \frac{d}{dx}(6x^5 + cos(x))\\
&amp; = (5)6x^{5-1} + (-sin(x))\\
&amp; = 30x^4 - sin(x)
\end{align*}\]</span>
</p>
<p><strong>Logarithmic Rule</strong> </p>
<table>
<caption><span id="tab:logderiv">Table 4.3: </span>Logarithmic Derivatives</caption>
<colgroup>
<col width="70%" />
<col width="29%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Derivative Functions</th>
<th align="left">Derivatives</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}(log_{a}\ x) = \frac{d}{dx}(\frac{ln\ x}{ln\ a})\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{x\ ln\ a}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}(log_{a}\ x) =\frac{1}{ln\ a}\frac{d}{dx}(ln\ x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{x\ ln\ a}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}(ln\ x)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{x}\ or\ x^{-1}\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}(ln\ \mid x \mid)\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{x}\ or\ x^{-1}\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}(e^n)\)</span></td>
<td align="left"><span class="math inline">\(e^n\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}(e^2)\)</span></td>
<td align="left"><span class="math inline">\(e^2\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\frac{d}{dx}(x^n)\)</span></td>
<td align="left"><span class="math inline">\(x^nln(x)\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\frac{d}{dx}(log_{a})x\)</span></td>
<td align="left"><span class="math inline">\(\frac{1}{x\cdot ln(a)}\)</span></td>
</tr>
</tbody>
</table>
<p>Example: Find the derivative of f(x) = <span class="math inline">\(2x^3\cdot3x^2 + ln(x)\)</span>.</p>

<p><span class="math display">\[\begin{align*}
f&#39;(x) {}&amp; =  \frac{d}{dx}(2x^3\cdot3x^2 + ln(x))\\
&amp; =  \frac{d}{dx}(6x^5 + ln(x))\\
&amp; = (5)6x^{5-1} + \frac{1}{x}\\
&amp; = 30x^4 + \frac{1}{x}
\end{align*}\]</span>
</p>
<p><strong>Product Rule</strong> </p>

<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx}( u \cdot v ) = \frac{du}{dx} \cdot v + u \cdot \frac{dv}{dx} = (f\cdot g)&#39;(x) = f&#39;g + g&#39;f
\end{align}\]</span>
</p>
<p>Example 1: Find the derivative of f(x) = <span class="math inline">\(5x\cdot 2x\)</span>.</p>

<p><span class="math display">\[\begin{align*}
(f\cdot g)&#39;(x) {} &amp; =  \frac{d}{dx}(5x\cdot 2x) \\
&amp; = \frac{d}{dx}(5x)\cdot2x\ +\ \frac{d}{dx}(2x)\cdot 5x \\
&amp; = 5\frac{d}{dx}(x)\cdot2x\ +\ 2\frac{d}{dx}(x)\cdot 5x \\
&amp; = 5\cdot(x^{1-1})\cdot2x\ +\ 2\cdot(x^{1-1})\cdot 5x \\
&amp; = 5\cdot(1)\cdot2x\ +\ 2\cdot(1)\cdot 5x \\
&amp; = 10x + 10x\\
&amp; = 20x
\end{align*}\]</span>
</p>
<p>Example 2: Find the derivative of f(x) = <span class="math inline">\(x^3\cdot cos(x)\)</span>.</p>

<p><span class="math display">\[\begin{align*}
(f\cdot g)&#39;(x) {} &amp; =  \frac{d}{dx}(x^3\cdot cos(x)) \\
&amp; = (3)x^{3-1}\cdot cos(x) + -sin(x)\cdot x^3\\
&amp; = 3x^2cos(x) - x^3sin(x) \\
&amp; = x^2( 3cos(x) - xsin(x)) 
\end{align*}\]</span>
</p>
<p><strong>Quotient Rule</strong> </p>

<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx}( \frac{u}{v} ) = (\frac{f}{g})&#39;(x) = 
\frac{f&#39;g - g&#39;f}{g^2}
\end{align}\]</span>
</p>
<p>Example: Find the derivative of f(x) = <span class="math inline">\(\frac{3x^2+1}{2x+1}\)</span>.</p>

<p><span class="math display">\[\begin{align*}
(\frac{f}{g})&#39;(x) {} &amp; =  \frac{d}{dx}\left( \frac{3x^2+1}{2x+1} \right)\\
&amp; = \frac{(2)3x^{2-1} (2x+1) - (1)2x^{1-1}(3x^2+1)}{(2x+1)^2}\\
&amp; = \frac{6x (2x+1) - 2(3x^2+1)}{(2x+1)^2}\\
&amp; = \frac{12x^2+6x - 6x^2 - 2}{(2x+1)^2}\\
&amp; = \frac{6x^2 + 6x -2}{(2x+1)^2}
\end{align*}\]</span>
</p>
<p><strong>Chain Rule (General Power Rule)</strong> </p>

<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{dy}{du}\frac{du}{dx} = (f\circ g(x))&#39; = (f(g(x))&#39; = f&#39;(g(x))g&#39;(x)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
(f(g(x))&#39;  = \frac{d}{dx}[\ (f(x))^n\ ] = n\cdot (f(x))^{n-1}\cdot\frac{d}{dx}(f(x))
\end{align}\]</span>
</p>
<p>Example: Find the derivative of f(x) = <span class="math inline">\((2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)^7\)</span> .</p>

<p><span class="math display">\[\begin{align*}
Let\ u {}&amp; = 2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1\\
\\
(f(g(x))&#39; &amp; =  \frac{d}{dx} (u^7) \\
&amp; = (7)u^{7-1}\cdot\frac{d}{dx} u\\  
&amp; = 7u^6 \cdot \frac{d}{dx} u \\
&amp; = 7(2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\cdot \frac{d}{dx}(2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\
&amp; = 7(2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\cdot \frac{d}{dx}(6x^5 + 4x^4 + x^3 + x^2 + 1)\\
&amp; = 7(2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\cdot ((5)6x^{5-1} + (4)4x^{4-1} + (3)x^{3-1} + (2)x^{2-1} )\\
&amp; = 7(2x^3\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\cdot (30x^4 + 16x^3 + 3x^2 + 2x )\\
\end{align*}\]</span>
</p>
<p><strong>Order Rule</strong> </p>
<p>What is the derivative of a derivative of a derivative of a function?</p>

<p><span class="math display">\[\begin{align}
first\ derivative {}&amp; : f&#39;(x) = \frac{dy}{dx}\\
second\ derivative &amp; : f&#39;&#39;(x) = (f&#39;(x))&#39; = \frac{d}{dx} \left(\frac{dy}{dx}\right) = \frac{d^2y}{dx^2} \\
third\ derivative &amp; : f&#39;&#39;&#39;(x) = (f&#39;&#39;(x))&#39; = ((f&#39;(x))&#39;)&#39; = \frac{d}{dx} \left(\frac{d^2y}{dx^2}\right) = \frac{d^3y}{dx^3} 
\end{align}\]</span>
</p>
<p>Example: Find the first derivative, second derivative, third derivative, and fourth derivative of f(x) = <span class="math inline">\(x^4 + x^3 + x^2 + 1\)</span>.</p>

<p><span class="math display">\[\begin{align*}
f(x) &amp; = x^4 + x^3 + x^2 + 1 \\
(speed) f&#39;(x) &amp; = 4x^3 + 3x^2 + 2x\\
(acceleration) f&#39;&#39;(x) &amp; = 12x^2 + 6x + 2\\
(torque/force) f&#39;&#39;&#39;(x) &amp; = 24x + 6 \\
(horsepower/power) f&#39;&#39;&#39;&#39;(x) &amp; = 24 \\
\end{align*}\]</span>
</p>
<p><strong>Multivariate Functions (multivariables)</strong></p>
<p>In univariate (one-variable) calculus, we use the following expression:</p>
<p><span class="math display">\[
y = f(x)
\]</span></p>
<p>Here is an example of a univariat function:</p>
<p><span class="math display">\[
y = f(x) = 4x^2 - 1 = (2x+1)(2x - 1)
\]</span></p>
<p>In multivariate (multivariable) calculus, we use the following expression:</p>
<p><span class="math display">\[\begin{align}
z = f(x,y)
\end{align}\]</span></p>
<p>Here is an example of a multivariate function:</p>
<p><span class="math display">\[
z = f(x,y) = 4x^2 - y^2 = (2x+y)(2x - y)
\]</span></p>
<p>So how do we differentiate a multivariate function? The answer is to use partial derivatives.</p>
<p><strong>Partial Derivatives</strong> </p>
<p>Partial derivative is finding the derivative of functions with respect to individual variables.</p>
<p>Example: Find the derivative of <span class="math inline">\(f(x,y) = (4x^2-y^2)\)</span> with respect to x, where z = f(x,y).</p>

<p><span class="math display">\[\begin{align*}
\frac{\partial z}{\partial x} {}&amp; = \frac{\partial}{\partial x} (4x^2-y^2)\\
&amp; = \frac{\partial}{\partial x} (4x^2) - \frac{\partial}{\partial x}(y^2)\\
&amp; = (2)4x^{2-1} - y^2 \frac{\partial}{\partial x}(1),\ \ \ \  
    \text{where y is a constant and } \frac{\partial}{\partial x}(1) = 0\\
&amp; = (2)4x^1 - y^2\cdot 0  \\
&amp; = 8x
\end{align*}\]</span>
</p>
<p>Now find the derivative of <span class="math inline">\(f(x,y) = (4x^2-y^2)\)</span> with respect to y, where z = f(x,y).</p>

<p><span class="math display">\[\begin{align*}
\frac{\partial z}{\partial y} {}&amp; = \frac{\partial}{\partial y} (4x^2-y^2)\\
&amp; = \frac{\partial}{\partial y} (4x^2) - \frac{\partial}{\partial y}(y^2)\\
&amp; = x^2 \frac{\partial}{\partial y}(4)- (2)y^{2-1} \,\ \ \ \  
    \text{where x is a constant and } \frac{\partial}{\partial x}(4) = 0\\
&amp; = x^2\cdot 0 - 2y^1  \\
&amp; = -2y
\end{align*}\]</span>
</p>
<p>To validate, we use Râs derivative function called <strong>D()</strong>:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" data-line-number="1">f =<span class="st"> </span><span class="kw">expression</span>( <span class="dv">4</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span>y<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb158-2" data-line-number="2">( <span class="dt">x =</span> <span class="kw">D</span>( f, <span class="st">&quot;x&quot;</span>) )</a></code></pre></div>
<pre><code>## 4 * (2 * x)</code></pre>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1">( <span class="dt">y =</span> <span class="kw">D</span>( f, <span class="st">&quot;y&quot;</span>) )</a></code></pre></div>
<pre><code>## -(2 * y)</code></pre>
<p><strong>Gradient of a function</strong> </p>
<p><strong>Gradient</strong> is another word for derivatives (or slope) so that given the same function as above, namely <span class="math inline">\(f(x,y) = (4x^2-y^2)\)</span>, the gradient is written as:</p>
<p><span class="math display">\[
\nabla f(x,y) = \left(\frac{\partial z}{\partial x}, \frac{\partial z}{\partial y}\right) = \left(8x, -2y\right)
\]</span>
Example:</p>
<p><span class="math display">\[
\nabla f(2,3) = \left(8x, -2y\right) = \left(8(2), -2(3)\right) = \left(16, -6\right)
\]</span></p>
</div>
<div id="integrals" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.1.5</span> Integrals <a href="numericalcalculus.html#integrals" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Integration</strong> is the reverse method of <strong>Differentiation</strong> - the antiderivative of a function (or a derivative if in higher orders).</p>
<p><strong>Integral Notations</strong></p>
<p>Note that the antiderivative of a derivative is written as F.</p>

<p><span class="math display">\[\begin{align*}
if{}&amp;: f&#39;(x)\ \text{is the derivative}\\
then&amp;:  F(f&#39;(x))\ \text{ is the antiderivative} \\
therefore&amp;: F(f&#39;(x)) = f(x)
\end{align*}\]</span>
</p>
<p>Similar to Differentiation, we also follow rules when dealing with Integration.</p>
<p>Let us integrate (or find the integral of a derivative).</p>
<p><strong>Constant Rule</strong></p>
<p><span class="math display">\[\begin{align}
\int (k) dx = (k)x + c, \text{where both c and k are constants}
\end{align}\]</span></p>
<p>Example: Find the antiderivative of fâ(x) = 5.</p>
<p><span class="math display">\[\begin{align*}
\int (5) dx {}&amp; = (5)x + c\\
&amp; = 5x + c
\end{align*}\]</span></p>
<p><strong>Trigonometric Rule</strong> </p>
<table>
<caption><span id="tab:integral1">Table 4.4: </span>Trigonometric Integrals</caption>
<thead>
<tr class="header">
<th align="left">Integral Functions</th>
<th align="left">Integrals</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\int sin(x) dx\)</span></td>
<td align="left"><span class="math inline">\(-cos(x)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int cos(x) dx\)</span></td>
<td align="left"><span class="math inline">\(sin(x)\ +\ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int sec^2(x) dx\)</span></td>
<td align="left"><span class="math inline">\(\ tan2(x)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int csc^2(x) dx\)</span></td>
<td align="left"><span class="math inline">\(\ -cot^2(x)\ +\ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int sec(x)tan(x) dx\)</span></td>
<td align="left"><span class="math inline">\(\ sec(x)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int csc(x)cot(x) dx\)</span></td>
<td align="left"><span class="math inline">\(\ -csc(x) + \ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int \frac{1}{\sqrt{1 - x^2}} dx\)</span></td>
<td align="left"><span class="math inline">\(sin^{-1}x\)</span> or arcsin(x)Â +Â c</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int -\frac{1}{\sqrt{1 - x^2}} dx\)</span></td>
<td align="left"><span class="math inline">\(cos^{-1}x\)</span> or arccos(x)Â +Â c</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int \frac{1}{x^2 + 1}dx\)</span></td>
<td align="left"><span class="math inline">\(tan^{-1}x dx\)</span> or arctan(x)Â +Â c</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int -\frac{1}{x^2 + 1}dx\)</span></td>
<td align="left"><span class="math inline">\(cot^{-1}x dx\)</span> or arccot(x)Â +Â c</td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int \frac{1}{\mid x \mid \sqrt{x^2 + 1}}dx\)</span></td>
<td align="left"><span class="math inline">\(sec^{-1}x\)</span> or arcsec(x)Â +Â c</td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int -\frac{1}{\mid x \mid \sqrt{x^2 + 1}}dx\)</span></td>
<td align="left"><span class="math inline">\(csc^{-1}x\)</span> or arcscs(x)Â +Â c</td>
</tr>
</tbody>
</table>
<p>Example: Find the antiderivative of fâ(x) = <span class="math inline">\(2x^3\cdot3x^2 + cos(x)\)</span>.</p>
<p><span class="math display">\[
\int (2x^3\cdot3x^2 + cos(x)) dx = x^6 + sin(x) + c
\]</span>
where:</p>
<p><span class="math display">\[
\int (2x^3\cdot3x^2  ) dx = x^6\ \ \ \ \ \ and \ \ \ \ \
\int cos(x) dx = sin(x)
\]</span></p>
<p><strong>Logarithmic Rule</strong> </p>
<table>
<caption><span id="tab:logintegral">Table 4.5: </span>Logarithmic Derivatives</caption>
<colgroup>
<col width="40%" />
<col width="59%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Functions</th>
<th align="left">Integrals</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left"><span class="math inline">\(\int \frac{1}{x\ ln\ a}dx\)</span></td>
<td align="left"><span class="math inline">\((log_{a}\ x)\ +\ c = (\frac{ln\ x}{ln\ a})\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int \frac{1}{x\ ln\ a}dx\)</span></td>
<td align="left"><span class="math inline">\((log_{a}\ x)\ +\ c =\frac{1}{ln\ a}(ln\ x)\ +\ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int (\frac{1}{x}\ or\ x^{-1})dx\)</span></td>
<td align="left"><span class="math inline">\((ln\ x)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int (\frac{1}{x}\ or\ x^{-1})dx\)</span></td>
<td align="left"><span class="math inline">\((ln\ \mid x \mid)\ +\ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int e^n dx\)</span></td>
<td align="left"><span class="math inline">\((e^n)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int e^2 dx\)</span></td>
<td align="left"><span class="math inline">\((e^2)\ +\ c\)</span></td>
</tr>
<tr class="odd">
<td align="left"><span class="math inline">\(\int x^nln(x) dx\)</span></td>
<td align="left"><span class="math inline">\((x^n)\ +\ c\)</span></td>
</tr>
<tr class="even">
<td align="left"><span class="math inline">\(\int \frac{1}{x\cdot ln(a)} dx\)</span></td>
<td align="left"><span class="math inline">\((log_{a})x\ +\ c\)</span></td>
</tr>
</tbody>
</table>
<p><strong>Power Rule</strong> </p>
<p><span class="math display">\[\begin{align}
\int (k)x^n dx = (k)\frac{x^{n+1}}{n+1} + c, \text{where n }\ne 1
\end{align}\]</span></p>
<p>Example:</p>
<p>Find the antiderivative of <span class="math inline">\(f&#39;(x) = 20x^3\)</span></p>

<p><span class="math display">\[\begin{align*}
\int (20x^3) dx {}&amp; = (20)\frac{x^{3+1}}{3+1} + c \\
&amp; = (20) \frac{x^4}{4} + c\\
&amp; = \frac {20x^4}{4} + c\\
&amp; = 5x^4 + c
\end{align*}\]</span>
</p>
<p><strong>Sum and Difference Rule</strong></p>
<p><span class="math display">\[\begin{align}
\int (f\pm g)\ dx  = \int (f)dx\ \pm \int (g)dx  
\end{align}\]</span></p>
<p>Example: Find the antiderivative of <span class="math inline">\(f&#39;(x) = 12x^2 + 2x\)</span>.</p>

<p><span class="math display">\[\begin{align*}
\int (12x^2 + 2x) dx {}&amp; = \int (12x^2)dx\ + \int (2x)dx \\
&amp; = 12\frac{x^{2+1}}{2+1} + 2\frac{x^{1+1}}{1+1} + c\\
&amp; = 12\frac{x^3}{3} + 2\frac{x^2}{2} + c\\
&amp; = \frac{12x^3}{3} + \frac{2x^2}{2} + c\\
&amp; = 4x^3 + x^2 + c\\
\end{align*}\]</span>
</p>
<p><strong>Product Rule</strong></p>
<p>Unlike Differentiation, there is no product rule for Integration. But we can derive a rule out of it from Differentiation.</p>
<p>Let us start with the following two functions that we intend to perform product calculation.</p>
<p>Given two functions: <span class="math inline">\(u\cdot v\)</span></p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx} (u\cdot v)
\end{align}\]</span></p>
<p>By product rule,</p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = \frac{d}{dx} u\cdot v = \frac{du}{dx}\cdot v + u\cdot \frac{dv}{dx}
\end{align}\]</span></p>
<p>We know that, from our Differentiation discussion, we have the following derivative formula.</p>
<p><span class="math display">\[\begin{align}
(f\cdot g)&#39;(x) = (u\cdot v)&#39;(x) = \frac{du}{dx}\cdot v + u\cdot \frac{dv}{dx}
\end{align}\]</span></p>
<p>We can get the antiderivative therefore:</p>
<p><span class="math display">\[
F((f\cdot g)&#39;(x)) = (u\cdot v)(x) = \int \left( \frac{du}{dx}\cdot v + u\cdot \frac{dv}{dx} \right) dx = uv
\]</span></p>
<p>By sum rule, we get:</p>
<p><span class="math display">\[\begin{align}
uv  {}&amp; = \int \left( \frac{du}{dx}\cdot v \right)dx + \int \left(u\cdot \frac{dv}{dx} \right) dx \\
uv  &amp; = \int du\cdot v  + \int u\cdot dv
\end{align}\]</span></p>
<p>Based on the formula above, let us talk about <em>integration by parts</em>.</p>
<p><strong>Integration by parts</strong> </p>
<p><span class="math display">\[\begin{align}
\int u\cdot dv = uv -\int v\cdot du
\end{align}\]</span></p>
<p>Example 1: Find the antiderivative of <span class="math inline">\(f&#39;(x) = xe^x dx\)</span>.</p>

<p><span class="math display">\[\begin{align*}
let:\\
u {}&amp; = x\\
dv &amp; = e^x\\
therefore:\\
du &amp; = dx\\
v &amp; = e^x\\
\\
now:\\
\int (xe^x)dx &amp; = \int u\ dv\\
&amp; = uv -\int v\ du\\
&amp; = x\cdot e^x - \int e^x dx\\
&amp; = x\cdot e^x - e^x + c
\end{align*}\]</span>
</p>
<p>Example 2: Find the antiderivative of <span class="math inline">\(f&#39;(x) = x\cdot sin(x)\)</span>.</p>

<p><span class="math display">\[\begin{align*}
let:\\
\ u {}&amp; = x\\
dv &amp; = sin(x)dx\\
therefore:\\
du &amp; = dx\\
v &amp; = -cos(x)
\\
now:\\
\int (x\cdot sin(x)) dx &amp; = \int u\ dv\\
&amp; = uv\ -\int v\ du\\
&amp; = -x\cdot cos(x) -\int -cos(x)dx\\
&amp; = -x\cdot cos(x) -(-sin(x) ) + c\\
&amp; = -x\cdot cos(x) + sin(x) + c
\end{align*}\]</span>
</p>
<p><strong>Chain Rule</strong></p>
<p>Unlike Differentiation, there is no chain rule for Integration. What can be available to use is <em>Integration by Substitution</em>.</p>
<p><strong>Integration by Substitution</strong> </p>
<p>This is also known as reverse chain rule.</p>

<p><span class="math display">\[\begin{align}
\int f(g(x))g&#39;(x)dx 
\end{align}\]</span>
</p>
<p>For this to work, we perform substitution using âuâ:</p>

<p><span class="math display">\[\begin{align}
u = g(x)\\
du = g(x)dx
\end{align}\]</span>
</p>
<p>That gives us:</p>

<p><span class="math display">\[\begin{align}
\int f(g(x))g&#39;(x)dx = \int f(u)du
\end{align}\]</span>
</p>
<p>Example: Find the antiderivative of <span class="math inline">\(f&#39;(x) = e^{3x}\)</span></p>

<p><span class="math display">\[\begin{align*}
let:\\
\ u {}&amp; = 3x\\
du &amp; = 3dx\\
where:\\
dx &amp; = \frac{1}{3}du
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
therefore:\\
&amp; = \int (e^{3x}) dx\\
&amp; = \int (e^u)\frac{1}{3} du\\
&amp; = \frac{1}{3}\int {e^u} du\\
&amp; = \frac{1}{3}\cdot e^u + c\\
\text{now substitute back u:}\\
&amp; = \frac{1}{3}\cdot e^{3x} + c
\end{align*}\]</span>
</p>
<p><strong>Definite Integrals</strong></p>
<p>Let us study Figure <a href="numericalcalculus.html#fig:integrals1">4.6</a>. In the figure, we construct a chart using the R code below.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:integrals1"></span>
<img src="DS_files/figure-html/integrals1-1.png" alt="Limits" width="70%" />
<p class="caption">
Figure 4.6: Limits
</p>
</div>
<p>Using Figure <a href="numericalcalculus.html#fig:integrals1">4.6</a>, one of the strengths of Integration is computing for the area bounded by the curve and the line. Areas that have curvatures around the edges are candidates for integral computation.</p>
<p>We highlight the area in the figure by using many small lines of width 1. We use lines to explain the regions filling up the whole area.</p>
<p>The gaps between the lines are the regions of the whole area, and those regions are divided by the lines. As we reduce the width of the region, more lines are added to the area. It means there are more regions (though thinner) to fill the whole area.</p>
<p>Imagine now, if we try to reduce the width of the region to an extremely small value, at that point, the total size of the area spanning all the regions will equal the size of the whole area bounded by the curve and the line.</p>
<p>Mathematically, we know that <span class="math inline">\(\Delta x\)</span> is the size (base) of each region. We also know that each region will have a height that touches the curve:</p>
<p><span class="math display">\[\begin{align}
\Delta y = f(\Delta x) =f(x + \Delta x) - f(x)
\end{align}\]</span></p>
<p>Therefore, the area of each individual rectangular region is:</p>
<p><span class="math display">\[\begin{align}
A_{R} {}&amp; = \Delta x \cdot \Delta y\\
where &amp;: \nonumber \\
 \Delta y &amp; = [ f(x + \Delta x) - f(x) ] \\
 \Delta y &amp; = f(\Delta x)\\
therefore &amp;: \nonumber\\
A_{R} {}&amp; = \Delta x \cdot [ f(x + \Delta x) - f(x) ] \\
&amp; = \Delta x \cdot f(\Delta x)
\end{align}\]</span></p>
<p>Therefore, the whole area bounded by the curve and line where they intersect at points a and b is:</p>
<p><span class="math display">\[\begin{align}
A_{ab} = \sum_{i=1}^{n} A_{R} =  \sum_{i=1}^{n} f(\Delta x) \cdot \Delta x 
\end{align}\]</span></p>
<p>For this to work, we need to make sure we set a limit to the number of regions created. The number of regions is limited to the whole area itself, bounded by points <strong>a</strong> and <strong>b</strong>.</p>
<p><span class="math display">\[\begin{align}
A_{ab} {}&amp; = \lim_{n\to \infty} \sum_{i=1}^{n} A_{R} &amp;on [a,b]\\
&amp; =  \lim_{n\to \infty}  \sum_{i=1}^{n} f(\Delta x) \cdot \Delta x &amp;on\ [a,b]
\end{align}\]</span></p>
<p>To convert this to integral, we write:</p>
<p><span class="math display">\[\begin{align}
A_{ab} {}&amp; =  \lim_{n\to \infty}  \int_{a}^{b} f(\Delta x)\ dx &amp;where\ \Delta x = dx
\end{align}\]</span></p>
<p>That is what we call <strong>definite integral</strong>:</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b} f(x)\ dx
\end{align}\]</span></p>
<p>We will cover this more in statistics and probabilities as we deal with data distribution.</p>
<p><strong>Improper Integrals</strong> </p>
<p>So far we have been dealing with <strong>Indefinite Integral</strong>. One with no limits. On the other hand, <strong>Definite Integral</strong> is one with limits.</p>
<p>We express the equation for <strong>Definite Integrals</strong> this way:</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b}f(x) dx
\end{align}\]</span></p>
<p>Here, we define an integral with an interval between a and b - in other words, having a support or domain [a,b].</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:definiteintegral"></span>
<img src="definite_integral.png" alt="Definite Integral" width="70%" />
<p class="caption">
Figure 4.7: Definite Integral
</p>
</div>
<p>To evaluate the definite integral, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b}f(x) dx = \left[ f(x)\ \right]_{a}^{b} = f(b) - g(a)
\end{align}\]</span></p>
<p>Example: Find the integral of f(x) = x^2 +1 with domain [0,1].</p>

<p><span class="math display">\[\begin{align*}
\int (x^2 +1)dx {}&amp; = \int x^2\ dx + \int (1)dx\\
&amp;= \left[ \frac{1}{3} x^3 + x \right]_{0}^{1} \\
&amp; = \left( \frac{1}{3} (1)^3 + (1) \right) - \left( \frac{1}{3} (0)^3 + (0)  \right) \\
&amp; = \left( \frac{1}{3} + 1 \right) - 0 \\
&amp; =  \frac{1}{3} + 1  \\
&amp; = \frac{4}{3}
\end{align*}\]</span>
</p>
<p>For improper integrals, we have to account for infinite limits of integration. What that is, has to do with the existence of limits.</p>
<p>Let us review each one.</p>
<ul>
<li><p><em>First</em>, we have a closure in which left and right brackets enclose the support: <span class="math inline">\([a,b]\)</span>. The domain is continuous, and both a and b are existing limits.</p></li>
<li><p><em>Second</em>, we have a closure in which a left parenthesis and a right bracket enclose the support: <span class="math inline">\((-\infty,b]\)</span>. The domain is continuous from b but reaches no limit towards the negative direction.</p></li>
<li><p><em>Third</em>, we have a closure in which a left bracket and a right parenthesis enclose the support: <span class="math inline">\([b,\infty)\)</span>. The domain is continuous and b but reaches no limit towards the positive direction.</p></li>
<li><p><em>Fourth</em>, we have a closure in which left and right parentheses enclose the support: <span class="math inline">\((-\infty,\infty)\)</span>. The domain is continuous but reaches no limits towards either end.</p></li>
</ul>
<p>Here, for improper integrals, we deal with enclosures with no limits (the ones with <span class="math inline">\(\infty\)</span>)</p>

<p><span class="math display">\[\begin{align}
\int_{-\infty}^{\infty}f(x) dx
\end{align}\]</span></p>
<p>To solve the equation, we can split the integral into two:</p>
<p><span class="math display">\[\begin{align}
\int_{-\infty}^{\infty}f(x) dx = \int_{-\infty}^{b}f(x) dx + \int_{b}^{\infty}f(x) dx
\end{align}\]</span>
</p>
<p>Notice how we introduce point b to cut the interval into two. The first part has an interval between <span class="math inline">\(-\infty\)</span> and b, and the second part has an interval between b and <span class="math inline">\(\infty\)</span>. We call this method <strong>improper integral</strong>.</p>
<p>So, let us find the improper integral of the following equation:</p>
<p><span class="math display">\[\begin{align}
\int_{-\infty}^{\infty} x^2 dx 
\end{align}\]</span></p>
<p>It can be assumed that between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span> is a zero which we will use to perform improper integrals.</p>
<p><span class="math display">\[\begin{align}
\int_{-\infty}^{\infty} x^2 dx  = \int_{-\infty}^{0} x^2 dx + \int_{0}^{\infty} x^2 dx
\end{align}\]</span></p>
<p>Let us apply limits of definite integrals to the equation:</p>

<p><span class="math display">\[\begin{align}
\int_{-\infty}^{\infty} x^2 dx  {} &amp;= \lim_{a\to-\infty} \int_{-\infty}^{0} x^2 dx
                                      + \lim_{a\to \infty} \int_{0}^{\infty} x^2 dx\\
&amp;= \lim_{a\to-\infty} |\frac{1}{3} x^3 |_{a}^{0} + \lim_{a\to-\infty} |\frac{1}{3} x^3 |_{0}^{a} \\
&amp;= \left( \frac{1}{3}(0)^3 -  \frac{1}{3}(\infty)^3 \right) + \left( \frac{1}{3}(\infty)^3 -  \frac{1}{3}(0)^3 \right) \nonumber \\
&amp;= \left( 0 -  \infty \right) + ( \infty - 0) \nonumber \\
&amp;= -\infty + \infty \nonumber \\
&amp;= (-1+1)\cdot\infty \to \text{factor out common term} \nonumber \\
&amp; = 0\cdot\infty \to \text{indeterminate outcome} \nonumber 
\end{align}\]</span>
</p>
<p><strong>The LâHÃ´pitalâs Rule</strong> </p>
<p>It helps to recall that the following forms are indeterminate:</p>
<p><span class="math display">\[
\frac{\pm\infty}{\pm\infty},\ \ \ \frac{0}{0},\ \ \ \pm\infty \cdot\ 0,\ \ \ 
\infty - \infty,\ \ \ \infty^0,\ \ \ 1^\infty,\ \ \ 0^0 
\]</span></p>
<p>It also helps to recall that the following forms are considered to have finite non-zero limit:</p>
<p><span class="math display">\[
\frac{\mathbb{R}}{\pm0},\ \ \ \ \pm\infty^1\ or \pm\infty
\]</span></p>
<p>If the outcome of a limit ends with an indeterminate form, then we can apply the <span class="math inline">\(\mathbf{L&#39;H\hat{o}pital&#39;s}\)</span> rule; which states that we can differentiate the numerator and denominator separately like so:</p>
<p><span class="math display">\[\begin{align}
\lim_{a\to p}\frac{f(x)}{g(x)} = \lim_{a\to p}\frac{f&#39;(x)}{g&#39;(x)}
\end{align}\]</span></p>
<p>Example: Find the limit of <span class="math inline">\(f(x) = \frac{ln\ x}{x}\)</span> as x approaches <span class="math inline">\(\infty\)</span>.</p>
<p><span class="math display">\[
f&#39;(x) = \lim_{x\to\infty}\frac{ln\ x}{x}
\]</span></p>
<p>Test for indiscriminate form:</p>
<p><span class="math display">\[\begin{align*}
ln\ x {}&amp; = ln\ \infty {}&amp; \to \infty \\
x &amp; = \infty &amp; \to \infty
\end{align*}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
f&#39;(x) = \lim_{x\to\infty}\frac{ln\ x}{x} \to \frac{\infty}{\infty}
\]</span></p>
<p>Let us apply <span class="math inline">\(\mathbf{L&#39;H\hat{o}pital&#39;s}\)</span> rule. Here, we first differentiate the numerator and then differentiate the denominator:</p>

<p><span class="math display">\[\begin{align*}
f&#39;(x) {}&amp; = \lim_{x\to\infty}\frac{ln\ x}{x}\\
&amp; = \lim_{x\to\infty}\left(\frac{\frac{1}{x}}{1}\right) \\
&amp; = \lim_{x\to\infty}\left(\frac{\frac{1}{\infty}}{1}\right)\\
&amp; = \lim_{x\to\infty}\left(\frac{0}{1}\right) \to \frac{1}{\infty} = 0 \\
&amp; = 0 &amp; 
\end{align*}\]</span>
</p>
<p><strong>Order Rule</strong> </p>
<p>What is the antiderivative of an antiderivative of an antiderivative of a function? To answer the question, let us recall the following order-rule formula for differentiation:</p>

<p><span class="math display">\[\begin{align}
first\ derivative {}&amp; : f&#39;(x) = \frac{dy}{dx}\\
second\ derivative &amp; : f&#39;&#39;(x) = (f&#39;(x))&#39; = \frac{d}{dx} \left(\frac{dy}{dx}\right) = \frac{d^2y}{dx^2} \\
third\ derivative &amp; : f&#39;&#39;&#39;(x) = (f&#39;&#39;(x))&#39; = ((f&#39;(x))&#39;)&#39; = \frac{d}{dx} \left(\frac{d^2y}{dx^2}\right) = \frac{d^3y}{dx^3}
\end{align}\]</span>
</p>
<p>Therefore:</p>
<p>The antiderivative of first derivative:</p>

<p><span class="math display">\[\begin{align}
F(f&#39;(x)) = f(x) \to \text{the function itself} \to y = f(x)
\end{align}\]</span>
</p>
<p>The antiderivative of second derivative:</p>

<p><span class="math display">\[\begin{align}
F(f&#39;&#39;(x)) = f&#39;(x)  = \frac{dy}{dx} \to \text{the first derivative}
\end{align}\]</span>
</p>
<p>The antiderivative of third derivative:</p>

<p><span class="math display">\[\begin{align}
F(f&#39;&#39;&#39;(x)) {} = (f&#39;(x))&#39;  = \frac{d}{dx} \left(\frac{dy}{dx}\right) = \frac{d^2y}{dx^2} \to \text{the second derivative}
\end{align}\]</span>
</p>
<p>Example: Find the antiderivatives of the first, second, third, and fourth derivatives of f(x) = <span class="math inline">\(x^4 + x^3 + x^2 + 1\)</span>.</p>
<p>Here are the order of derivatives for the function: f(x) = <span class="math inline">\(x^4 + x^3 + x^2 + 1\)</span>.</p>

<p><span class="math display">\[\begin{align*}
f(x) &amp; = x^4 + x^3 + x^2 + 1 \\
(speed) f&#39;(x) &amp; = 4x^3 + 3x^2 + 2x\\
(acceleration) f&#39;&#39;(x) &amp; = 12x^2 + 6x + 2\\
(torque/force) f&#39;&#39;&#39;(x) &amp; = 24x + 6 \\
(horsepower/power) f&#39;&#39;&#39;&#39;(x) &amp; = 24 \\
\end{align*}\]</span>
</p>
<p>Therefore:</p>
<p>The antiderivative of the first derivative:</p>

<p><span class="math display">\[\begin{align*}
F(f&#39;(x)) {}&amp; = \int (4x^3 + 3x^2 + 2x) dx \\
&amp; = \frac{4x^{3+1}}{3+1} + \frac{3x^{2+1}}{2+1} + \frac{2x^{1+1}}{1+1} + c \\
&amp; = \frac{4x^{4}}{4} + \frac{3x^{3}}{3} + \frac{2x^{2}}{2} + c \\
&amp; = x^4 + x^3 + x^2 + c &amp;\to  f(x)
\end{align*}\]</span>
</p>
<p>The antiderivative of the second derivative:</p>

<p><span class="math display">\[\begin{align*}
F(f&#39;&#39;(x)) {}&amp; = \int (12x^2 + 6x + 2) dx \\
&amp; = \frac{12x^{2+1}}{2+1} + \frac{6x^{1+1}}{1+1} + \frac{2x^{0+1}}{0+1} + c \\
&amp; = \frac{12x^{3}}{3} + \frac{6x^{2}}{2} + \frac{2x^{1}}{1} + c \\
&amp; = 4x^3 + 3x^2 + 2x + c &amp;\to  f&#39;(x)
\end{align*}\]</span>
</p>
<p>The antiderivative of the third derivative:</p>

<p><span class="math display">\[\begin{align*}
F(f&#39;&#39;&#39;(x)) {}&amp; = \int (24x + 6) dx \\
&amp; = \frac{24x^{1+1}}{1+1} + \frac{6x^{0+1}}{0+1} + c \\
&amp; = \frac{24x^{2}}{2} + \frac{6x^{1}}{1} + c \\
&amp; = 12x^2 + 6x  + c &amp;\to  f&#39;&#39;(x)
\end{align*}\]</span>
</p>
<p>The antiderivative of the fourth derivative:</p>

<p><span class="math display">\[\begin{align*}
F(f&#39;&#39;&#39;&#39;(x)) {}&amp; = \int (24) dx \\
&amp; = \frac{24x^{0+1}}{0+1} + c \\
&amp; = \frac{24x^{1}}{1} + c \\
&amp; = 24x + c &amp;\to  f&#39;&#39;&#39;(x)
\end{align*}\]</span>
</p>
<p>Find the antiderivative of the antiderivative of the second derivative:</p>

<p><span class="math display">\[\begin{align}
F(F(f&#39;&#39;(x))) = \iint f&#39;&#39;(x) dxdx
\end{align}\]</span>
</p>
<p>Example:</p>

<p><span class="math display">\[\begin{align*}
F(F(f&#39;&#39;(x))) {}&amp; = \iint (12x^2 + 6x) dx dx \\
&amp; = 4x^3 + 3x^2 + 2x + c
\end{align*}\]</span>
</p>
<p>Find the antiderivative of the antiderivative of the third derivative:</p>

<p><span class="math display">\[\begin{align}
F(F(F(f&#39;&#39;&#39;(x)))) = \iiint f&#39;&#39;&#39;(x) dxdxdx
\end{align}\]</span>
</p>
<p>Example:</p>

<p><span class="math display">\[\begin{align*}
F(F(f&#39;&#39;(x))) {}&amp; = \iiint (24x + 6) dxdxdx \\
&amp; = 4x^3 + 3x^2 + 2x + c
\end{align*}\]</span>
</p>
</div>
</div>
<div id="approximation-by-numerical-integration" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.2</span> Approximation by Numerical Integration <a href="numericalcalculus.html#approximation-by-numerical-integration" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Chapters <strong>2</strong> and <strong>3</strong> (<strong>Numerical Linear Algebra</strong>), we covered Numerical Methods to solve for systems of lines and curves. This section introduces Numerical Methods of Integration to calculate areas of shapes and volumes.</p>
<p>As part of this section, the <strong>Integration</strong> (or <strong>Quadrature</strong>) - or put it, Summation - of smaller squares to fit the area of a circle is what we are now going to discuss as a starting point. Instead of showing how to approximate an area of a full circle, let us perhaps use a quarter of a circle to illustrate a few <strong>quadratic</strong> techniques. Let us first get some basic intuition. Consider the following Figure <a href="numericalcalculus.html#fig:quadrature">4.8</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadrature"></span>
<img src="DS_files/figure-html/quadrature-1.png" alt="Quadrature" width="70%" />
<p class="caption">
Figure 4.8: Quadrature
</p>
</div>
<p>Here, Figure <a href="numericalcalculus.html#fig:quadrature">4.8</a> shows a cosine function bounded by limits [a,b] - the interval - divided into partitions that are spaced equally, denoted as <span class="math inline">\(\Delta x = h\)</span>, between nodes (points), denoted as <strong>n</strong>. That said, we should emphasize a few things:</p>
<p><strong>First</strong>, the polynomial function (cosine in this case) is bounded by an interval [ a=0.0, b=1.0 ].</p>
<p><span class="math display">\[
a = 0.0\ \ \ \ \ \ \ \ b = 1.0
\]</span></p>
<p><strong>Second</strong>, the <strong>distance</strong> (or <strong>Delta</strong>) is equally spaced between nodes (along the x-axis):</p>
<p><span class="math display">\[
\Delta x = h = x_1 - x_0 = x_2 - x_1 =\  ...\ = \frac{b - a}{n-1} = \frac{1.0 - 0.0}{5} = 0.2
\]</span>
We call the equal distance as <strong>stepsize</strong> <span class="math inline">\(\Delta x = h = \frac{b - a}{n-1} = 0.2\)</span>.</p>
<p><strong>Third</strong>, our integration can be computed analytically as such (using cosine function in this case):</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b} f(x)dx = \int_{a=0.0}^{b=1.0} cos(x)dx
\end{align}\]</span></p>
<p>But numerically, we can use approximation:</p>
<p><span class="math display">\[\begin{align}
\int_{a}^{b} f(x)dx \approx \Delta x\sum_{i=0}^{n-1} f(x_i)\ \ \ \ where\ \ \  x_i = a + i \Delta x
\end{align}\]</span></p>
<p>That can be expanded this way:</p>
<p><span class="math display">\[\begin{align}
I(f_x) \approx
\Delta x (\ f(x_0) + f(x_1) + f(x_2) + f(x_3) + f(x_4) \ ) \label{eqn:eqnnumber10}
\end{align}\]</span></p>
<p>Note how we use a simple quadrature approximation by stacking rectangles horizontally and then adding the area (e.g.Â Area = width x height = <span class="math inline">\(\Delta x \times f(x_i)\)</span>) of each rectangles - this is what we call <strong>Reimann sums</strong> method. That can be seen in Figure <a href="numericalcalculus.html#fig:quadrature1">4.9</a>: </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadrature1"></span>
<img src="DS_files/figure-html/quadrature1-1.png" alt="Approximate Integration" width="70%" />
<p class="caption">
Figure 4.9: Approximate Integration
</p>
</div>
<p>Therefore, by adding all the rectangular areas, we get the following approximation (given n-1 = 5 ):</p>
<p><span class="math display">\[\begin{align}
I(f_x) {}&amp;\approx w \times h_1 + w \times h_2  + w \times h_3  + w \times h_4  + w \times h_5   \\
       &amp;\approx \Delta x f(x_0) + \Delta x f(x_1) + \Delta x f(x_2) + \Delta x f(x_3) + \Delta x f(x_4) \\
       &amp;\approx 0.2cos(0.0) +  0.2cos(0.2) + 0.2cos(0.4) + 0.2cos(0.6) + 0.2cos(0.8) \nonumber \\
       &amp;\approx 0.884634 \nonumber
\end{align}\]</span></p>
<p>We then compare the numerical approximation with the analytical result:</p>
<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x)dx  = sin(1) = 0.841471.
\]</span></p>
<p>And we get a large <strong>absolute</strong> error:</p>
<p><span class="math display">\[
absolute\ err = 0.884634 - 0.841471 = 0.043163.
\]</span></p>
<p>One way to reduce the error is to add more partitions. For example, instead of only five partitions, consider a larger number of smaller partitions, e.g., n-1=1000. That ends up with equally distant spacing (width), <span class="math inline">\(\Delta x = 0.01\)</span>. See the following:</p>
<p><span class="math display">\[\begin{align}
I(f) \approx \sum_{i=0}^{n-1=1000} \Delta x f(x_i) = 0.8417008
\end{align}\]</span></p>
<p>We get a much smaller <strong>absolute</strong> error:</p>
<p><span class="math display">\[
absolute\ err = 0.8417008 - 0.841471 = 0.043163 = 0.0002298
\]</span></p>
<p>Now, we can see in Figure <a href="numericalcalculus.html#fig:quadrature1">4.9</a> that each rectangle has an extra area above the cosine curve. Also, we can see the large space above the cosine curve, especially the rectangle starting at point &lt;<span class="math inline">\(x_4, f(x_4)\)</span>&gt; in the upper left corner. Those extra regions are added to the total area below the curve and thus account for the total <strong>absolute</strong> error. One way to solve this is to reduce the height of the rectangle so that it just falls in the middle between the x-points, e.g.</p>
<p><span class="math display">\[\begin{align*}
y_0 {}&amp;= cos ((x_0 + x_1)/2) \\
y_1 &amp;= cos ((x_1 + x_2)/2) \\
&amp;\vdots
\end{align*}\]</span></p>
<p>The new Figure <a href="numericalcalculus.html#fig:quadrature2">4.10</a> now looks like so (compare that with Figure <a href="numericalcalculus.html#fig:quadrature1">4.9</a>):</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:quadrature2"></span>
<img src="DS_files/figure-html/quadrature2-1.png" alt="Midpoint Rule" width="70%" />
<p class="caption">
Figure 4.10: Midpoint Rule
</p>
</div>
<p>By doing so, we get a numerical value <strong>0.841471</strong> equal to the analytical value <strong>0.841471</strong> which renders the absolute error to zero for <strong>cos(x)</strong> Quadrature with n=1000 and <span class="math inline">\(\Delta x = 0.001\)</span>.</p>
<p>The method we used is called <strong>Midpoint rule</strong> - one of the <strong>Newton-Cotes Quadrature</strong> rules. </p>
<p>Let us illustrate other common <strong>Numerical Integration</strong> starting with <strong>Newton-Cotes Quadrature</strong>.</p>
<div id="newton-cotes-quadrature" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.1</span> Newton-Cotes Quadrature <a href="numericalcalculus.html#newton-cotes-quadrature" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Newton-Cotes Quadrature</strong> shares rules governing the numerical integration of interpolating polynomial functions - the integrands <span class="math inline">\(f(x)\)</span> - divided into equal spaces to fit a given order of points. Each rule is characterized as either a <strong>closed</strong> type or an <strong>open</strong> type.</p>
<p>For a <strong>closed</strong> type rule, our formula tackles the quadrature by including both boundary points (x-axis) into the computation where the integrand stretches from <span class="math inline">\(x_0\)</span> and <span class="math inline">\(x_n\)</span> such that <span class="math inline">\(x_0 = a\)</span> and <span class="math inline">\(x_n = b\)</span>. On the other hand, the rule is <strong>open</strong> otherwise.</p>
<p><span class="math display">\[\begin{align}
x_i = a + \delta \Delta x 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
where\ \ \ \ \delta\Delta x =
\begin{cases}
\delta=i,\Delta x = \left(\frac{b-a}{n-1}\right) &amp; if\ closed \\
\\
\delta=(i-1),\Delta x = \left(\frac{b-a}{n+1}\right)  &amp; if\ open
\end{cases} \label{eqn:eqnnumber11}
\end{align}\]</span></p>
<p>Recall the following Quadrature equation we derived previously (See Equation <span class="math inline">\(\ref{eqn:eqnnumber10}\)</span>):</p>
<p><span class="math display">\[\begin{align}
I(f_x) \approx
\Delta x (\ f(x_0) + f(x_1) + f(x_2) + f(x_3) + f(x_4) \ )
\end{align}\]</span></p>
<p>In Table <a href="numericalcalculus.html#tab:newtoncote">4.6</a>, we show <strong>Newton-Cotes</strong> quadrature rules and the formulas used to construct the interpolating polynomials<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> : <span class="math inline">\(f_x = P_n = \Delta x f(x_0) + \Delta x f(x_1) + ... + \Delta x f(x_{n-1})\)</span>.</p>

<table>
<caption><span id="tab:newtoncote">Table 4.6: </span>Newton-Cotes Formula</caption>
<colgroup>
<col width="2%" />
<col width="12%" />
<col width="18%" />
<col width="6%" />
<col width="60%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">n</th>
<th align="left">Rule</th>
<th align="left">stepsize: <span class="math inline">\(\Delta x\)</span></th>
<th align="left">Type</th>
<th align="left">Formula: <span class="math inline">\(I(f_x)\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">1</td>
<td align="left">Midpoint</td>
<td align="left"><span class="math inline">\(\frac{1}{1}(b-a)\)</span></td>
<td align="left">open</td>
<td align="left"><span class="math inline">\(\Delta x f(\frac{a + b}{2})\)</span></td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Midpoint</td>
<td align="left"><span class="math inline">\(\frac{1}{2}(b-a)\)</span></td>
<td align="left">open</td>
<td align="left"><span class="math inline">\(2 \Delta x f(x_1)\)</span></td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Trapezoidal</td>
<td align="left"><span class="math inline">\(\frac{1}{3}(b-a)\)</span></td>
<td align="left">open</td>
<td align="left"><span class="math inline">\(\frac{3}{2} \Delta x \left( f(x_1)+f(x_2) \right)\)</span></td>
</tr>
<tr class="even">
<td align="left">4</td>
<td align="left">Milneâs</td>
<td align="left"><span class="math inline">\(\frac{1}{4}(b-a)\)</span></td>
<td align="left">open</td>
<td align="left"><span class="math inline">\(\frac{4}{3}\Delta x\left( 2f(x_1)-f(x_2)+2f(x_3) \right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">2</td>
<td align="left">Trapezoidal</td>
<td align="left"><span class="math inline">\(\frac{1}{1}(b-a)\)</span></td>
<td align="left">closed</td>
<td align="left"><span class="math inline">\(\frac{1}{2}\Delta x \left( f(x_0)+f(x_1) \right)\)</span></td>
</tr>
<tr class="even">
<td align="left">2</td>
<td align="left">Simpsonâs 1/3</td>
<td align="left"><span class="math inline">\(\frac{1}{2}(b-a)\)</span></td>
<td align="left">closed</td>
<td align="left"><span class="math inline">\(\frac{1}{3}\Delta x\left( f(x_0)+4f(x_1)+f(x_2) \right)\)</span></td>
</tr>
<tr class="odd">
<td align="left">3</td>
<td align="left">Simpsonâs 3/8</td>
<td align="left"><span class="math inline">\(\frac{1}{3}(b-a)\)</span></td>
<td align="left">closed</td>
<td align="left"><span class="math inline">\(\frac{3}{8}\Delta x\left( f(x_0)+3f(x_1)+3f(x_2)+f(x_3) \right)\)</span></td>
</tr>
<tr class="even">
<td align="left">5</td>
<td align="left">Booleâs</td>
<td align="left"><span class="math inline">\(\frac{1}{5}(b-a)\)</span></td>
<td align="left">closed</td>
<td align="left"><span class="math inline">\(\frac{2}{45}\Delta x ( 7f(x_0) +32(x_1)+12f(x_2)+\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(32f(x_3)+7f(x_4) )\)</span></td>
</tr>
</tbody>
</table>
<p>Note in the table that the formula is categorized as <strong>open</strong> or <strong>closed</strong>. It is closed if the interval [a,b] (or endpoints <strong>a</strong> and <strong>b</strong>) are used; otherwise it is open. We leave readers to investigate other combinations of the open or closed types <span class="citation">(Zhao W. et al. <a href="bibliography.html#ref-ref424w">2013</a>; Ramachandran T. et al. <a href="bibliography.html#ref-ref415c">2017</a>; Zafar F. et al. <a href="bibliography.html#ref-ref423m">2014</a>)</span>.</p>

<p>Previously, we approximated the cosine integral, <span class="math inline">\(\int cos(x) dx\)</span>, by introducing the use of <strong>Reimann rule</strong> and <strong>newton-cote midpoint rule</strong>. Let us explore two more rules based on Table <a href="numericalcalculus.html#tab:newtoncote">4.6</a>. </p>
<p>Given the same cosine integral:</p>
<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x) dx
\]</span></p>
<p>we approximate using two of the <strong>newton-cote</strong> rules:</p>
<ul>
<li><strong>Trapezoidal rule</strong> with n=2 and type=closed: </li>
</ul>
<p><span class="math display">\[\begin{align}
I(f)  \approx \Delta x(f(x_0) + f(x_1))\ \ \ where \ \ \ \Delta x = \frac{1}{n-1}(b - a)
\end{align}\]</span></p>
<p>we get:</p>

<p><span class="math display">\[\begin{align*}
\Delta x {}&amp;= (b - a) = 1 (1 - 0) = 1 \\
x_0 &amp;= a + i \Delta x = 0.0 + 0 \times 1 = 0.0 \\
x_1 &amp;= a + i \Delta x = 0.0 + 1 \times 1 = 1.0 \\
I(f)  &amp;\approx \frac{1}{2} \Delta x(f(x_0) + f(x_1)) \\
&amp;\approx  0.5 * ( cos(0.0) + cos(1.0) ) = 0.7701512
\end{align*}\]</span>
</p>
<ul>
<li><strong>Simpsonâs 1/3 rule</strong> with n=2 and type=closed: </li>
</ul>

<p><span class="math display">\[\begin{align}
I(f)  \approx \frac{1}{3} \Delta x(f(x_0) + 4f(x_1) + f(x_2))\ \ \ where \ \ \ \Delta x = \frac{1}{2}(b - a)
\end{align}\]</span>
</p>
<p>we get:</p>

<p><span class="math display">\[\begin{align*}
\Delta x {}&amp;= \frac{1}{2}(b - a) = 0.5 (1 - 0) = 0.5 = 1/2 \\
x_0 &amp;= a + i \Delta x = 0.0 + 0 \times 0.5 = 0.0 \\
x_1 &amp;= a + i \Delta x = 0.0 + 1 \times 0.5 = 0.5 \\
x_2 &amp;= a + i \Delta x = 0.0 + 2 \times 0.5 = 1.0 \\
I(f)  &amp;\approx \frac{1}{3}\Delta x(f(x_0) + 4f(x_1) + f(x_2)) \\
&amp;\approx 
1/3*1/2( cos(0.0) + 4cos(0.5) + cos(1.0) ) = 0.8417721
\end{align*}\]</span>
</p>
<p><strong>Simpsonâs 1/3 rule</strong> renders a numerical integral of <strong>0.8417721</strong>, which is closer to the analytical integral of <strong>0.841471</strong>, having an absolute error of <strong>0.0003011</strong>, which is quite accurate.</p>
</div>
<div id="composite-and-adaptive-quadrature" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.2</span> Composite and Adaptive Quadrature <a href="numericalcalculus.html#composite-and-adaptive-quadrature" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Notice that the <strong>Trapezoidal rule</strong> performs an approximation to the integral of cos(x) but does not quite get there for the expected accuracy.</p>
<p><span class="math display">\[
I(f) \approx 0.7701512\ \ \ \ \ \text{with an absolute error of } \mathbf{0.0713198}
\]</span></p>
<p>That is because we use only <strong>one trapezoid</strong>. We can improve upon that by applying a <strong>Composite Trapezoidal</strong> rule. It means plainly dividing the interval [ a, b ] into more partitions (or sub-intervals) and then using the <strong>Trapezoidal rule</strong> for each partition.</p>
<p>For that, the <strong>Trapezoidal rule</strong> equation changes to:</p>
<p><span class="math display">\[\begin{align}
I(f)  \approx \frac{1}{2}\Delta x(f(a=x_0) + 2f(x_1) + 2f(x_2)+\ ...\ + 2f(x_{n-1}) + f(b=x_n))
\end{align}\]</span></p>
<p>where <span class="math inline">\(\Delta x = \frac{1}{n-1}(b - a)\)</span>.</p>
<p>For example, let us try to partition the interval into <strong>ten trapezoids</strong> (n=11 nodes):</p>
<p><span class="math display">\[
\Delta x = \frac{(b - a)}{n-1} = 1/10 = 0.1
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
x_i = x_0 + i\Delta x\ \ \ \ where\ i = 0,2,\ ...\ ,\ n-1
\]</span>
That gives us:</p>

<p><span class="math display">\[\begin{align*}
x_0 &amp;= a + i \Delta x = 0.0 + 0 \times 0.1 = 0.0 \\
x_1 &amp;= a + i \Delta x = 0.0 + 1 \times 0.1 = 0.1 \\
x_2 &amp;= a + i \Delta x = 0.0 + 2 \times 0.1 = 0.2 \\
&amp;\vdots \\
x_{n-1} &amp;= a + (n-1) \Delta x = 0.0 + (n-1) \times 0.1 = 1.0\\
I(f)  &amp;\approx \frac{1}{2}\Delta x(f(x_0) + 2f(x_1) +\ ...\ + 2f(x_9) + f(x_{10})) \\
&amp;\approx 0.05( cos(0.0) + 2cos(0.1) + 2cos(0.2) + \ ...\ + 2cos(0.9) + cos(1)  ) \\
&amp;\approx 0.840770
\end{align*}\]</span>
</p>
<p>This time the absolute error becomes: 0.841471 - 0.840770 = 0.000701</p>
<p>To illustrate further, let us also tackle <strong>Composite Simpsonâs 1/3 rule</strong> using <strong>ten partitions</strong> (n=11 nodes):</p>
<p><span class="math display">\[\begin{align}
\Delta x = \frac{(b - a)}{n-1} = 1/10 = 0.1
\end{align}\]</span></p>
<p>For that, the <strong>simpsonâs 1/3 rule</strong> equation changes to:</p>

<p><span class="math display">\[\begin{align}
I(f)  \approx \frac{1}{3}\Delta x(f(a=x_0) + 4f(x_1) + 2f(x_2)+\ ...\ + 2f(x_{n-2}) + 4f(x_{n-1}) + f(b=x_n))
\end{align}\]</span>
</p>
<p>Note that the equation works only on <strong>condition</strong> that the <strong>number of partitions is even</strong>.</p>
<p>That gives us:</p>

<p><span class="math display">\[\begin{align*}
x_0 &amp;= a + i \Delta x = 0.0 + 0 \times 0.1 = 0.0 \\
x_1 &amp;= a + i \Delta x = 0.0 + 1 \times 0.1 = 0.1 \\
x_2 &amp;= a + i \Delta x = 0.0 + 2 \times 0.1 = 0.2 \\
&amp;\vdots \\
x_{n-1} &amp;= a + (n-1) \Delta x = 0.0 + (n-1) \times 0.1 = 1.0\\
I(f)  &amp;\approx \frac{1}{3}\Delta x(f(x_0) + 4f(x_1) + 2f(x_2) +\ ...\ + 2f(x_8) + 4f(x_9) + f(x_{10})) \\
&amp;\approx 0.1/3( cos(0.0) + 4cos(0.1) + 2cos(0.2) + \ ...\ + 2cos(0.8) + 4cos(0.9) + cos(1)  ) \\
&amp;\approx 0.841471
\end{align*}\]</span>
</p>
<p>This time the absolute error becomes: 0.841471 - 0.841471 = 0</p>
<p>The use of the <strong>Composite Trapezoidal rule</strong> is illustrated in the <strong>Romberg integration</strong> section later in this chapter.</p>
<p>As for the <strong>Adaptive Quadrature</strong>, the motivation of the <strong>Adaptive Quadrature</strong> is the same as the <strong>Composite Quadrature</strong>. The idea is to adjust the number of intervals to optimize accuracy. In other pieces of literature, the choice of tolerance plays a role in optimizing accuracy.</p>
</div>
<div id="gaussianquadrature" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.3</span> Gaussian Quadrature<a href="numericalcalculus.html#gaussianquadrature" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Note that we can optimize the accuracy of a Numerical Integral based on the correct number of nodes (points), the placements of their abscissas <span class="math inline">\(\mathbf{x_i}\)</span>, and the corresponding optimal value of weights (coefficients) to use. In <strong>newton-cotes</strong> rule, the placement of abscissas <span class="math inline">\(\mathbf{x_i}\)</span> are fixed. In the <strong>Gaussian Quadrature</strong> rule, we can vary the placement of abscissas <span class="math inline">\(\mathbf{x_i}\)</span> apart from the number of points and corresponding weights; and they can be pre-determined. See Figure <a href="numericalcalculus.html#fig:gaussquadrature">4.11</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gaussquadrature"></span>
<img src="gaussquadrature.png" alt="Gaussian Placement of Abscissas" width="70%" />
<p class="caption">
Figure 4.11: Gaussian Placement of Abscissas
</p>
</div>
<p>To perform <strong>Gaussian Quadrature</strong>, we use one of the following common <strong>Orthogonal polynomials</strong> such as:</p>
<ul>
<li>Legendre polynomials</li>
<li>Chebyshev polynomials</li>
<li>Laguerre polynomials</li>
<li>Hermite polynomials</li>
<li>Jacobi polynomials</li>
</ul>
<p>For our purpose, we use <strong>Legendre polynomials</strong> to illustrate <strong>Gaussian Quadrature</strong>. See Figure <a href="numericallinearalgebra.html#fig:legendre">3.13</a>. We may call this <strong>Gauss-Legendre Quadrature</strong> in the same way we can call <strong>Gauss-Chebyshev</strong> if we choose to associate <strong>Chebyshev polynomials</strong> for our <strong>Gaussian Quadrature</strong>.</p>
<p>To proceed with <strong>Gauss-Legendre</strong> quadrature, we first derive (or pre-determine) the <strong>abscissas</strong> <span class="math inline">\(\mathbf{x_i}\)</span>.</p>
<p>Recall the equation for <strong>Legendre</strong> polynomials under <strong>Higher Degree Polynomials</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). To illustrate, we reference three <strong>Legendre</strong> polynomials from the <strong>Numerical Linear Algebra II</strong> chapter. We can validate the results of the abscissas and weights in our calculations below can using Table 25.4 from USC, Engr CE 108 <span class="citation">(<a href="bibliography.html#ref-ref468p">n.d.</a>)</span> in reference to Davis P. et al. <span class="citation">(<a href="bibliography.html#ref-ref458p">1956</a>)</span>. Also, in terms of <strong>Abscissas and Weights</strong>, a good reference to use is Holoborodko P. <span class="citation">(<a href="bibliography.html#ref-ref449p">2012</a>)</span>.</p>
<p>For a quadratic <strong>Legendre polynomial</strong>, we get the abscissas <span class="math inline">\(\mathbf{x_i}\)</span> using the following:</p>
<p><span class="math display">\[\begin{align*}
P_2(x) {}&amp;= \frac{1}{2}(3x^2 -1) \\
0 &amp;= \frac{1}{2}(3x^2 -1)\ \ \ \ \text{apply root-finding} \\
x_0 &amp;= -0.5773503,\ \ \ x_1 = 0.5773503
\end{align*}\]</span></p>
<p>For a Cubic <strong>Legendre</strong> polynomial, we get the abscissas <span class="math inline">\(\mathbf{x_i}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
P_3(x) {}&amp;= \frac{1}{2}(5x^3 -3x)\\
0 &amp;=\frac{1}{2}(5x^3 -3x)\ \ \ \ \text{apply root-finding} \\
x_0 &amp;= 0,\ \ \ x_1 = -0.7745967,\ \ \ x_2 = 0.7745967
\end{align*}\]</span></p>
<p>For a Quartic <strong>legendre</strong> polynomial, we get:</p>
<p><span class="math display">\[\begin{align*}
P_4(x) {}&amp;= \frac{1}{8}(35x^4 -30x^2 + 3)\\
0 &amp;=\frac{1}{8}(35x^4 -30x^2 + 3)\ \ \ \ \text{apply root-finding} \\
x_0 &amp;= -0.339981,\ \ \ x_1 = 0.339981,\ \ \ x_2 = -0.8611363,\ \ \ x_3 = 0.8611363
\end{align*}\]</span></p>
<p>We have a choice to continue computing more roots for other higher-order Legendre polynomials, but for our case, we settle only up to <strong>Quartic Legendre polynomial</strong>. Also, we leave readers to further investigate the other Legendre polynomials along with how to derive them using the following <strong>closed-form</strong> [ -1, 1 ] formula:</p>
<p><span class="math display">\[\begin{align}
P_n(x) = \frac{1}{2\pi i}\oint(1-2tx+t^2)^{-1/2}t^{-n-1}dt
\end{align}\]</span></p>
<p>The <strong>open-form</strong> ( -1, 1 ) can also be investigated using <strong>Gram-Schmidt orthonormalization</strong> which is introduced in <strong>Numerical Linear algebra</strong> under <strong>QR factorization</strong>.</p>
<p><strong>Second</strong>, we compute for the <strong>weights</strong> <span class="math inline">\(\mathbf{w_i}\)</span> using the derived <strong>abscissas</strong> <span class="math inline">\(\mathbf{x_i}\)</span>. We use the formula below:</p>
<p><span class="math display">\[\begin{align}
w_i = \frac{2}{(1-x_i^2)(P&#39;_n(x_i))^2}
\end{align}\]</span></p>
<p>For example, for a <strong>Quadratic Legendre polynomial</strong>, with <span class="math inline">\(x_0 = -0.5773503\)</span> and <span class="math inline">\(x_1 = 0.5773503\)</span>, and a first derivative of the polynomial <span class="math inline">\(P_2(x)\)</span>:</p>
<p><span class="math display">\[
P_2(x) = \frac{1}{2}(3x^2 -1)\ \rightarrow P&#39;_2(x) = 3x
\]</span>
we get the following weights:</p>
<p><span class="math display">\[\begin{align*}
w_0 {}&amp;= \frac{2}{(1-x_0^2)(3x_0)^2} = \frac{2}{(1-(-0.5773503)^2)(3\times-0.5773503)^2} = 1 \\
\\
w_1 &amp;= \frac{2}{(1-x_1^2)(3x_1)^2} = \frac{2}{(1-(0.5773503)^2)(3\times 0.5773503)^2}  = 1
\end{align*}\]</span></p>
<p>For a <strong>Cubic Legendre polynomial</strong>, with <span class="math inline">\(x_0 = 0\)</span>, <span class="math inline">\(x_1 = -0.7745967\)</span> and <span class="math inline">\(x_2 = 0.7745967\)</span>, and a first derivative of the polynomial <span class="math inline">\(P_3(x)\)</span>:</p>
<p><span class="math display">\[
P_3(x) = \frac{1}{2}(5x^3 - 3x)\ \rightarrow P&#39;_3(x) = 7.5x^2 - 1.5
\]</span>
where:</p>
<p><span class="math display">\[
w_0 = \frac{2}{(1-x_0^2)(7.5x_0^2 - 1.5)^2}
\]</span></p>
<p>we get the following weights:</p>
<p><span class="math display">\[
w_0 = 0.8888889,\ \ \ w_1 = 0.5555556,\ \ \ w_2 = 0.5555556
\]</span></p>
<p>For a <strong>Quartic Legendre polynomial</strong>, with <span class="math inline">\(x_0 = -0.339981\)</span>, <span class="math inline">\(x_1 = 0.339981\)</span>, <span class="math inline">\(x_2 = -0.8611363\)</span>, and <span class="math inline">\(x_3 = 0.8611363\)</span>, and a first derivative of the polynomial <span class="math inline">\(P_4(x)\)</span>:</p>
<p><span class="math display">\[
P_4(x) = \frac{1}{8}(35x^4 -30x^2 + 3)\ \rightarrow P&#39;_4(x)  = 17.5x^3 - 7.5x
\]</span>
where:</p>
<p><span class="math display">\[
w_i = \frac{2}{(1-x_i^2)(17.5x_i^3 - 7.5x_i)^2}
\]</span>
we get the following weights:</p>
<p><span class="math display">\[
w_0 = 0.6521452,\ \ \ w_1 = 0.6521452,\ \ \ 
w_2 = 0.3478549,\ \ \ w_3 = 0.3478549
\]</span></p>
<p><strong>Now</strong>, we are ready to perform an <strong>N-point Gauss-Legendre Quadrature</strong>. But let us first use the following integral to get the actual value that we can use to validate against the result of our quadrature approximation later: </p>
<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x) dx
\]</span></p>
<p>We scale the integral to form the following:</p>

<p><span class="math display">\[\begin{align}
\int_a^b f(x) dx = \frac{(b-a)}{2} \int_{-1}^1 f \left( \frac{b-a}{2}x + \frac{b+a}{2} \right) dx
\end{align}\]</span>
</p>
<p>Therefore:</p>

<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x) dx = \frac{(b-a)}{2} \int_{-1}^1 cos \left( \frac{b-a}{2}x + \frac{b+a}{2} \right) dx =
 \frac{1}{2} \int_{-1}^1cos \left( \frac{1}{2}x + \frac{1}{2} \right) dx
\]</span>
</p>
<p>Proof:</p>

<p><span class="math display">\[\begin{align*}
\frac{1}{2} \int_{-1}^1cos\left(\frac{1}{2}x + \frac{1}{2}\right) dx
{}&amp;= \frac{1}{2} \int_{-1}^1cos\left(\frac{1}{2}x\right) cos\left(\frac{1}{2}\right) - sin\left(\frac{1}{2}x\right) sin\left(\frac{1}{2}\right)\ dx \\
&amp;= \frac{1}{2} \left[\int_{-1}^1cos\left(\frac{1}{2}x\right)cos \left(\frac{1}{2}\right)\ dx - \int_{-1}^1sin\left(\frac{1}{2}x\right)sin\left(\frac{1}{2}\right)\ dx\right]  \\
&amp;= \frac{1}{2} \left[cos\left(\frac{1}{2}\right)\int_{-1}^1cos\left(\frac{1}{2}x\right)\ dx - sin\left(\frac{1}{2}\right)\int_{-1}^1 sin\left(\frac{1}{2}x\right)\ dx\right]  \\
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
&amp;\rightarrow \int_{-1}^1 cos\left(\frac{1}{2}x\right)\ dx = 4\ sin\left(\frac{1}{2}\right) \ \ \ \ \text{using u-sub, e.g. cos(u)2du} \\
&amp;\rightarrow\int_{-1}^1 sin\left(\frac{1}{2}x\right)\ dx = 0\ \ \ \ \ \text{odd parity} \\
&amp;= \frac{1}{2} \left[cos\left(\frac{1}{2}\right)4\ sin\left(\frac{1}{2}\right)  - sin\left(\frac{1}{2}\right) \times 0\right]  \\
&amp;=  \frac{1}{2} \left[cos\left(\frac{1}{2}\right)4\ sin\left(\frac{1}{2}\right)\right] \\
&amp;= 0.841471
\end{align*}\]</span>
</p>
<p>And now, to perform numerical integration, let us use the following equation:</p>
<p><span class="math display">\[\begin{align}
\int_{a=0.0}^{b=1.0} f(x) dx \approx \frac{1}{2} I_n(f) = \frac{1}{2}\sum_{i=0}^{n-1}  w_i f(x_i)
\end{align}\]</span></p>
<p>For a <strong>Two-point Gauss-Legendre Quadrature</strong> using the derived abscissas <span class="math inline">\(\mathbf{x_i}\)</span> and weights <span class="math inline">\(\mathbf{w_i}\)</span> from a <strong>Quadratic Legendre polynomial</strong>, we evaluate using the formula below:</p>
<p><span class="math display">\[\begin{align}
I_2(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) \\
I_2(f) &amp;= (1)cos(-0.5773503) + (1)cos(0.5773503) \nonumber \\
I_2(f) &amp;= (1)(-0.5773503) + (1)(0.5403023) = 1.675824 \nonumber
\end{align}\]</span></p>
<p>For a <strong>Three-point Gauss-Legendre Quadrature</strong> using the derived abscissas <span class="math inline">\(\mathbf{x_i}\)</span> and weights <span class="math inline">\(\mathbf{w_i}\)</span> from a <strong>Cubic Legendre polynomial</strong>, we evaluate using the formula below:</p>
<p><span class="math display">\[\begin{align}
I_3(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) + w_2 f(x_2)\\
I_3(f) &amp;= (0.8888889)cos(0) + (0.5555556)cos(-0.7745967) + (0.5555556)cos(0.7745967) \nonumber \\
I_3(f) &amp;= 1.683004 \nonumber
\end{align}\]</span></p>
<p>For a <strong>Four-point Gauss-Legendre Quadrature</strong> using the derived abscissas <span class="math inline">\(\mathbf{x_i}\)</span> and weights <span class="math inline">\(\mathbf{w_i}\)</span> from a <strong>Quartic Legendre polynomial</strong>, we evaluate using the formula below:</p>
<p><span class="math display">\[\begin{align}
I_4(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) + w_2 f(x_2) + w_3 f(x_3)\\
I_4(f) &amp;= (0.6521452)cos(-0.339981) + (0.6521452)cos(0.339981) \nonumber \\ 
&amp;+ (0.3478549)cos(-0.8611363) + (0.3478549)cos(0.8611363) \nonumber \\
I_4(f) &amp;= 1.682942 \nonumber 
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x) dx \approx \frac{1}{2} I_4(f) = \frac{1}{2}(1.682942) = 0.841471
\]</span></p>
<p>It turns out that the <strong>Four-point Gauss-Legendre Quadrature</strong> has the most precise result compared to the other two in our sample case of integrating cosine(x).</p>
<p>We have used <strong>Gauss-Legendre</strong> to demonstrate <strong>Gaussian Quadrature</strong>. It helps to read about other <strong>Orthogonal polynomials</strong> used. Also, we leave readers to investigate the <strong>Gauss-Kronrod Quadrature</strong>.</p>
</div>
<div id="romberg-integration" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.2.4</span> Romberg integration <a href="numericalcalculus.html#romberg-integration" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Romberg Integration</strong> combines <strong>Composite Trapezoidal rule</strong> and <strong>Richardson Extrapolation</strong> to approximate the integral of a function. The idea is to construct the following lower-triangular matrix based on two algorithms <span class="citation">(Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>.</p>
<p><span class="math display">\[
\begin{array}{lllllllll}
R_{0,0} \\
&amp; \searrow \\
R_{1,0} &amp; \rightarrow &amp; R_{1,1}\\
&amp; \searrow &amp; &amp; \searrow \\
R_{2,0} &amp; \rightarrow &amp; R_{2,1} &amp; \rightarrow &amp; R_{2,2}\\
&amp; \searrow &amp; &amp; \searrow  &amp; &amp; \searrow \\
R_{3,0} &amp;  \rightarrow &amp; R_{3,1} &amp;  \rightarrow &amp; R_{3,2} &amp;  \rightarrow &amp; R_{3,3}\\
\vdots  &amp;  &amp; \vdots &amp; &amp;  \vdots &amp; &amp;  \vdots &amp; \ddots \\
R_{n,0} &amp;   \rightarrow &amp;  R_{n,1} &amp;   \rightarrow &amp;  R_{n,2} &amp;   \rightarrow &amp;  R_{n,3} &amp; ... &amp; R_{n,n}\\
\end{array}
\]</span></p>
<p>The first algorithm uses <strong>Composite Trapezoidal rule CTR)</strong> to construct the first column of the triangle, yielding a column whose components are approximate integral based on <strong>trapezoidal rule</strong>. The first column is built using a series of intervals (or trapezoids) based on <span class="math inline">\(2^n\)</span> given that <span class="math inline">\(k = 1\)</span>. It will be apparent once we illustrate the <strong>Romberg integration</strong> steps.</p>
<p>The second algorithm uses the <strong>General Romberg</strong> formula where (<span class="math inline">\(k \neq 1\)</span>):</p>
<p><span class="math display">\[\begin{align}
R_{j,k} = \frac{4^{k-1}R_{j,k-1} - R_{j-1,k-1}}{4^{k-1}-1}
\end{align}\]</span></p>
<p>where (for example):</p>
<p><span class="math display">\[\begin{align}
R_{1,1} = \frac{4^1R_{1,0} - R_{0,0}}{4^1 - 1} = \frac{4R_{1,0} - R_{0,0}}{3} \\
R_{3,2} = \frac{4^2R_{3,1} - R_{2,1}}{4^2 - 1} = \frac{16R_{3,1} - R_{2,1}}{15} 
\end{align}\]</span></p>
<p>To illustrate <strong>Romberg Integration</strong>, as usual, we start with the sample cosine(x) integral:</p>
<p><span class="math display">\[
\int_{a=0.0}^{b=1.0} cos(x) dx
\]</span></p>
<p><strong>First</strong>, let us apply <strong>Composite Trapezoidal rule</strong> to the above integral by constructing a list of approximate integrals in the order of <span class="math inline">\(N=2^n\)</span>, e.g. (n=2, n=4, n=8):</p>
<ul>
<li>n = 2 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\Delta x = 0.5\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align*}
1/2 I_2(f) {}&amp;= 1/2( (0.5)cos(0) + 2(0.5)cos(0.5) + (0.5)cos(1) ) = 0.8238669 \\
\\
R_{0,0} &amp;= 0.8238669
\end{align*}\]</span></p>
<ul>
<li>n = 4 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\Delta x = 0.25\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align*}
1/2 I_4(f) {}&amp;= 1/2( (0.25)cos(0) + 2(0.25)cos(0.25) + 2(0.25)cos(0.50) \\
&amp;+ 2(0.25)cos(0.75)  + (0.25)cos(1) ) = 0.8370838 \\
\\
R_{1,0} &amp;= 0.8370838
\end{align*}\]</span></p>
<ul>
<li>n = 8 <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(\Delta x = 0.125\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align*}
1/2 I_8(f) {}&amp;= 1/2( 0.125cos(0) + 2(0.125)cos(0.125) + ... \\
&amp;+ 2(0.125)cos(0.875)+(0.125)cos(1) ) = 0.8403750 \\
\\
R_{2,0} &amp;= 0.8403750
\end{align*}\]</span></p>
<p><strong>Second</strong>, let us apply <strong>Richardson Extrapolation</strong> using the derived approximation: </p>
<p><span class="math display">\[\begin{align*}
R_{0,0} = 1/2 I_2(f) = 0.8238669\\
R_{1,0} = 1/2 I_3(f) = 0.8370838\\
R_{2,0} = 1/2 I_4(f) = 0.8403750\\
\end{align*}\]</span></p>
<p>Solve for <span class="math inline">\(R_{1,1}\)</span>:</p>
<p><span class="math display">\[
R_{1,1} = \frac{ 4^1 R_{1,0} - R_{0,0}}{4^1-1} = \frac{4(0.8370838) - 0.8238669}{3} = 0.8414894
\]</span>
Solve for <span class="math inline">\(R_{2,1}\)</span>:</p>
<p><span class="math display">\[
R_{2,1} = \frac{ 4^1 R_{2,0} - R_{1,0}}{4^1-1} = \frac{4(0.8403750) - 0.8370838}{3} = 0.8414721
\]</span></p>
<p>Solve for <span class="math inline">\(R_{2,2}\)</span>:</p>
<p><span class="math display">\[
R_{2,2} = \frac{ 4^2 R_{2,1} - R_{1,1}}{4^2-1} = \frac{16(0.8414721) - 0.8414894}{15} = 0.841471
\]</span></p>
<p>We get a lower triangular matrix with the following <strong>Romberg values</strong>:</p>
<p><span class="math display">\[
\begin{array}{lll}
R_{0,0} = 0.8238669\\
R_{1,0} = 0.8370838 &amp; R_{1,1} = 0.8414894\\
R_{2,0} = 0.8403750 &amp; R_{2,1} = 0.8414721 &amp; R_{2,2} = 0.841471\\
\end{array}
\]</span></p>
<p>Here, <strong>Romberg Iteration</strong> uses n=2, n=4, n=8 in <strong>composite trapezoidal rule</strong> to yield three initial approximate values for the cosine(x) integral. Then those approximates are further processed using the <strong>General Romberg Integration</strong> formula. From there, we arrive at an approximate value <span class="math inline">\(\mathbf{R_{2,2}=0.841471}\)</span> which is remarkably accurate.</p>
<p>Here is the <strong>Romberg integration</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
R = matrix(n,n) \\
loop\ j\ in\ 1:n \\
\ \ \ \ loop\ k\ in\ 1:j \\
\ \ \ \ \ \ \ \ if\ k==1:\\
\ \ \ \ \ \ \ \ \ \ \ \ R_{j,1} = trapezoidal\_rule(2^j, lower\_bound, upper\_bound, func) \\
\ \ \ \ \ \ \ \ else: \\
\ \ \ \ \ \ \ \ \ \ \ \ R_{j,k} = \frac{4^{k-1}R_{j,k-1} - R_{j-1,k-1}}{4^{k-1}-1} \leftarrow \mathbf{general \ Romberg\  formula} \\
\ \ \ \ end\ loop \\
end\ loop
\end{array}
\]</span></p>
<p>Let us look at our naive implementation of <strong>Romberg integration</strong> in R code:</p>

<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb162-1" data-line-number="1">f &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">cos</span>(x) }</a>
<a class="sourceLine" id="cb162-2" data-line-number="2"><span class="co"># where t = number of trapezoid</span></a>
<a class="sourceLine" id="cb162-3" data-line-number="3"><span class="co"># where t + 1 = number of points (including end points)</span></a>
<a class="sourceLine" id="cb162-4" data-line-number="4">trapezoidal_rule &lt;-<span class="st"> </span><span class="cf">function</span>(t, a, b, f) {</a>
<a class="sourceLine" id="cb162-5" data-line-number="5">    h =<span class="st"> </span>(b<span class="op">-</span>a) <span class="op">/</span><span class="st"> </span>t</a>
<a class="sourceLine" id="cb162-6" data-line-number="6">    x =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, (t<span class="op">+</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb162-7" data-line-number="7">    s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb162-8" data-line-number="8">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(t<span class="op">+</span><span class="dv">1</span>)) {</a>
<a class="sourceLine" id="cb162-9" data-line-number="9">      x[i] =<span class="st"> </span>a <span class="op">+</span><span class="st"> </span>(i<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>h  </a>
<a class="sourceLine" id="cb162-10" data-line-number="10">      <span class="cf">if</span> (x[i] <span class="op">!=</span><span class="st"> </span>a <span class="op">&amp;&amp;</span><span class="st"> </span>x[i] <span class="op">!=</span><span class="st"> </span>b) {</a>
<a class="sourceLine" id="cb162-11" data-line-number="11">          s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">f</span>( x[i] )</a>
<a class="sourceLine" id="cb162-12" data-line-number="12">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb162-13" data-line-number="13">          s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="kw">f</span>( x[i] ) </a>
<a class="sourceLine" id="cb162-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb162-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb162-16" data-line-number="16">    <span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>( h <span class="op">*</span><span class="st"> </span>s )</a>
<a class="sourceLine" id="cb162-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb162-18" data-line-number="18">romberg_integration &lt;-<span class="st"> </span><span class="cf">function</span>(n, lower, upper) {</a>
<a class="sourceLine" id="cb162-19" data-line-number="19">    R =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb162-20" data-line-number="20">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb162-21" data-line-number="21">    actual =<span class="st"> </span><span class="kw">integrate</span>(<span class="dt">f=</span>f, <span class="dt">lower=</span>lower, <span class="dt">upper=</span>upper)[<span class="dv">1</span>]<span class="op">$</span>value</a>
<a class="sourceLine" id="cb162-22" data-line-number="22">    approx =<span class="st"> </span><span class="dv">0</span>;  err =<span class="st"> </span><span class="dv">0</span>; N_ =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb162-23" data-line-number="23">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb162-24" data-line-number="24">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb162-25" data-line-number="25">            <span class="cf">if</span> (k <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb162-26" data-line-number="26">                <span class="co"># Trapezoidal rule (1st column)</span></a>
<a class="sourceLine" id="cb162-27" data-line-number="27">                R[j, <span class="dv">1</span>] =<span class="st"> </span><span class="kw">trapezoidal_rule</span>(<span class="dv">2</span><span class="op">^</span>j, <span class="dv">0</span>, <span class="dv">1</span>, f)</a>
<a class="sourceLine" id="cb162-28" data-line-number="28">            } <span class="cf">else</span> {            </a>
<a class="sourceLine" id="cb162-29" data-line-number="29">                <span class="co"># General Richardson Extrapolation</span></a>
<a class="sourceLine" id="cb162-30" data-line-number="30">                R[j,k] =<span class="st">  </span>( <span class="dv">4</span><span class="op">^</span>(k<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>R[j,k<span class="dv">-1</span>] <span class="op">-</span></a>
<a class="sourceLine" id="cb162-31" data-line-number="31"><span class="st">                               </span>R[j<span class="dv">-1</span>, k<span class="dv">-1</span>] ) <span class="op">/</span><span class="st"> </span>( <span class="dv">4</span><span class="op">^</span>(k<span class="dv">-1</span>) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb162-32" data-line-number="32">            }</a>
<a class="sourceLine" id="cb162-33" data-line-number="33">        }</a>
<a class="sourceLine" id="cb162-34" data-line-number="34">        <span class="co"># Check error</span></a>
<a class="sourceLine" id="cb162-35" data-line-number="35">        N_ =<span class="st"> </span>j</a>
<a class="sourceLine" id="cb162-36" data-line-number="36">        approx=R[N_,N_]</a>
<a class="sourceLine" id="cb162-37" data-line-number="37">        err =<span class="st"> </span><span class="kw">abs</span>( actual <span class="op">-</span><span class="st"> </span>approx )</a>
<a class="sourceLine" id="cb162-38" data-line-number="38">        <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb162-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb162-40" data-line-number="40">    A =<span class="st"> </span>R[<span class="dv">1</span><span class="op">:</span>N_, <span class="dv">1</span><span class="op">:</span>N_]</a>
<a class="sourceLine" id="cb162-41" data-line-number="41">    <span class="kw">colnames</span>(A) =<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;k&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,N_,<span class="dv">1</span>), <span class="dt">sep=</span><span class="st">&quot;=&quot;</span>)</a>
<a class="sourceLine" id="cb162-42" data-line-number="42">    <span class="kw">rownames</span>(A) =<span class="st"> </span><span class="kw">paste</span>(<span class="st">&quot;j&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,N_,<span class="dv">1</span>), <span class="dt">sep=</span><span class="st">&quot;=&quot;</span>)</a>
<a class="sourceLine" id="cb162-43" data-line-number="43">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;actual_value&quot;</span>=actual, </a>
<a class="sourceLine" id="cb162-44" data-line-number="44">         <span class="st">&quot;approx_value&quot;</span>=A[N_,N_], <span class="st">&quot;absolute_error&quot;</span>=err)</a>
<a class="sourceLine" id="cb162-45" data-line-number="45">}</a>
<a class="sourceLine" id="cb162-46" data-line-number="46"><span class="kw">romberg_integration</span>(<span class="dt">n=</span><span class="dv">5</span>, <span class="dt">lower=</span><span class="dv">0</span>, <span class="dt">upper=</span><span class="dv">1</span>)</a></code></pre></div>
<pre><code>## $matrix
##           k=1       k=2      k=3
## j=1 0.8238669 0.0000000 0.000000
## j=2 0.8370838 0.8414894 0.000000
## j=3 0.8403750 0.8414721 0.841471
## 
## $actual_value
## [1] 0.841471
## 
## $approx_value
## [1] 0.841471
## 
## $absolute_error
## [1] 6.849664e-09</code></pre>

</div>
</div>
<div id="approximation-by-numerical-differentiation" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.3</span> Approximation by Numerical Differentiation <a href="numericalcalculus.html#approximation-by-numerical-differentiation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Differentiation</strong> is the reverse of the <strong>Integration</strong> process. Similar to <strong>Numerical Integration</strong>, the idea here is to approximate the value of a derivative (or slope): <span class="math inline">\(f&#39;(x)\)</span>. And we use the <strong>Finite Difference</strong> method to do just that.</p>
<div id="order-of-accuracy" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.1</span> Order of Accuracy<a href="numericalcalculus.html#order-of-accuracy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It may help to explain <strong>order of error and accuracy</strong> by using <strong>Taylor Series</strong>. We know that if <span class="math inline">\(\mathbf{\hat{x}}\)</span> is an approximate value we are aiming to solve, then <span class="math inline">\(h\)</span> must be the difference between the actual value <span class="math inline">\(\mathbf{x}\)</span> and the approximate value:</p>
<p><span class="math display">\[\begin{align}
\mathbf{x} = \mathbf{\hat{x}} + h\ \ \ \ \ \ where\ h =  \mathbf{x} - \mathbf{\hat{x}}
\end{align}\]</span></p>
<p>The question is to know if we can quantify the <strong>error</strong>. After all, an approximation can come in many forms. To quantify the <strong>order of error</strong> we use <strong>Taylor series</strong> to expand a function <span class="math inline">\(f(\hat{x} + h)\)</span>.</p>
<p><span class="math display">\[\begin{align}
f(x) = f(\hat{x} + h) = f(\hat{x}) + f&#39;(\hat{x})h + \frac{f&#39;&#39;(\hat{x})h^2}{2!} + \frac{f&#39;&#39;&#39;(\hat{x})h^3}{3!}\ +\ ...
\end{align}\]</span></p>
<p>An approximation of a derivative is in the <strong>first-order</strong> of accuracy if the formula includes only up to the first derivative of the <strong>Taylor series</strong> expansion; thus, <span class="math inline">\(\mathbf{O(h^2)}\)</span> gets truncated (sort of trimming the error):</p>
<p><span class="math display">\[\begin{align}
f(\hat{x} + h) {}&amp;= f(\hat{x}) + f&#39;(\hat{x})h +  \mathbf{O(h^2)}\ \ \ \ \ \ \ \ \ \ where\ \mathbf{O(h^2)} =  \frac{f&#39;&#39;(\xi)h^2}{2!}\ +\ ... \\
\nonumber \\
f(\hat{x} + h) &amp;\approx f(\hat{x}) + f&#39;(\hat{x})h 
\end{align}\]</span></p>
<p>An approximation of a derivative is in the <strong>second-order</strong> of accuracy if the formula includes up to the second derivative of the <strong>Taylor series</strong> expansion; thus, <span class="math inline">\(\mathbf{O(h^3)}\)</span> gets truncated:</p>
<p><span class="math display">\[\begin{align}
f(\hat{x} + h) {}&amp;= f(\hat{x}) + f&#39;(\hat{x})h + \frac{f&#39;&#39;(\hat{x})h^2}{2!} + \mathbf{O(h^3)}\ \ \ where\ \mathbf{O(h^3)} =  \frac{f&#39;&#39;(\xi)h^3}{3!}\ +\ ... \\
\nonumber \\
f(\hat{x} + h) &amp;\approx f(\hat{x}) + f&#39;(\hat{x})h + \frac{f&#39;&#39;(\hat{x})h^2}{2!}
\end{align}\]</span></p>
<p>Higher-order of accuracy for approximation follows the same. The minute we <strong>truncate</strong> the <strong>Taylor series</strong> expansion of a function, the result becomes just an approximation, and therefore there is a <strong>Finite Difference</strong>.</p>
</div>
<div id="finite-difference" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.3.2</span> Finite Difference <a href="numericalcalculus.html#finite-difference" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Recall the following formula in <strong>Calculus</strong> section under <strong>Limits</strong> subsection:</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) = \lim_{h\rightarrow 0}\frac{f(x+h) - f(x)}{h}
\end{align}\]</span></p>
<p>Recall also that we introduced the concept of <strong>slope</strong> in <strong>Calculus</strong> section under <strong>Slopes</strong> subsection. We describe slope as the <strong>average rate of change</strong>. In other words, the derivative <strong>fâ(fx)</strong> represents the slope.</p>
<p>Here, with both the terms <strong>Limits</strong> and <strong>Slopes</strong>, let us introduce another common phrase, <strong>Difference Quotient</strong>, which is the formula derived from above: </p>
<p><span class="math display">\[\begin{align}
\frac{f(x+h) - f(x)}{h}
\end{align}\]</span></p>
<p>The derivation of the formula is explained - geometrically - in the <strong>Calculus</strong> section under the <strong>Slopes</strong> subsection. Here, we derive <span class="math inline">\(\mathbf{f(x+h)}\)</span> using <strong>Taylor Series</strong> expansion <span class="citation">(LeVeque R. J. <a href="bibliography.html#ref-ref475r">2007</a>, ch.1)</span>. </p>
<p>As long as we have a finite h &gt; 0, then the <strong>Taylor series</strong> expansion below becomes useful in deriving a formula for <span class="math inline">\(\mathbf{f(x+h)}\)</span>.</p>
<p><span class="math display">\[\begin{align}
f(x + h) = f(x) + \frac{f&#39;(x)h^1}{1!} + \frac{f&#39;&#39;(x)h^2}{2!}\  +\  \frac{f&#39;&#39;&#39;(x)h^3}{3!}\ + \ ... 
\end{align}\]</span></p>
<p>If we limit the order of accuracy to <strong>first-order</strong> such that:</p>
<p><span class="math display">\[\begin{align}
f(x + h) \approx  f(x) + \frac{f&#39;(x)h^1}{1!}
\end{align}\]</span></p>
<p>then we get the <strong>finite forward difference formula</strong>:</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) \approx \frac{f(x+h) - f(x)}{h} \leftarrow \mathbf{\text{finite forward difference}}
\end{align}\]</span></p>
<p>Another formula to consider is the <strong>Finite Backward Difference</strong>: </p>
<p><span class="math display">\[\begin{align}
f&#39;(x) \approx \frac{f(x) - f(x -h)}{h} \leftarrow \mathbf{\text{finite backward difference}}
\end{align}\]</span></p>
<p>which is derived out of the same <strong>first-order</strong> accuracy:</p>
<p><span class="math display">\[\begin{align}
f(x - h) \approx f(x) - \frac{f&#39;(x)h^1}{1!}
\end{align}\]</span></p>
<p>using the following <strong>Taylor series</strong> expansion to derive <span class="math inline">\(\mathbf{f(x - h)}\)</span>:</p>
<p><span class="math display">\[\begin{align}
f(x - h) = f(x) + \frac{f&#39;(x)(-h)^1}{1!} + \frac{f&#39;&#39;(x)(-h)^2}{2!}\  +\  \frac{f&#39;&#39;&#39;(x)(-h)^3}{3!}\ + \ ... 
\end{align}\]</span></p>
<p>Combining the <strong>Finite Forward Difference</strong> and <strong>Finite Backward Difference</strong> formulae gives us the <strong>Finite Centered Difference</strong> formula of <strong>first-order</strong> accuracy:</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) {}&amp;\approx \frac{f(x+h) - f(x)}{h} + \frac{f(x) - f(x -h)}{h} \\
\nonumber \\
f&#39;(x) &amp;\approx \frac{f(x+h) -f(x - h)}{2h} \leftarrow \mathbf{\text{finite centered difference}}
\end{align}\]</span></p>
<p>To illustrate, suppose we have the following problem statement and we are looking for the differential (derivative) of <span class="math inline">\(\mathbf{cos(6x)}\)</span>:</p>
<p><span class="math display">\[
f(x) = cos(6x), \ \ \ \ \ \ where\ \ \ \ \ x = 0.5,\ \ \ \ \ h=0.1
\]</span>
where exact solution is:</p>
<p><span class="math display">\[
f&#39;(x) = -6sin(6x) = -6sin(6 \times 0.5) = -0.84672
\]</span></p>
<p>To approximate the value <span class="math inline">\(f&#39;(x)\)</span>, let us use the three finite difference formulas:</p>
<p><span class="math display">\[\begin{align*}
FD \rightarrow f&#39;(x) {}&amp;\approx \frac{f(x+h) - f(x)}{h} = \frac{cos(0.5+ 0.1) - cos(0.5)}{0.1} \\
&amp;\approx \frac{-0.896758 - -0.989992}{0.1} = 0.932341 \\
\\
BD \rightarrow f&#39;(x) &amp;\approx  \frac{f(x) - f(x -h)}{h} = \frac{cos(0.5) - cos(0.5-0.1)}{0.1} \\
&amp;\approx \frac{-0.989992 - -0.737394}{0.1} = -2.525988 \\
\\
CD \rightarrow f&#39;(x) &amp;\approx  \frac{f(x+h) -f(x - h)}{2h} = \frac{cos(0.5+0.1) - cos(0.5-0.1)}{0.2} \\
&amp;\approx \frac{-0.896758 - -0.737394}{0.2} = -0.796824
\end{align*}\]</span></p>
<p>The <strong>Centered Difference</strong> formula yields an absolute error of -0.049896 compared to the other two. </p>
<p>Here is an implementation of the <strong>Finite Difference</strong> methods in R code: </p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:finitedifference"></span>
<img src="embed0031.png" alt="Finite Difference" width="70%" />
<p class="caption">
Figure 4.12: Finite Difference
</p>
</div>

<p>For a higher-order Numerical Differentiation, we can use the same <strong>Taylor Series</strong>. For example, to approximate the second derivative of a function, we first can combine the following:</p>

<p><span class="math display">\[\begin{align}
f(x + h) + f(x -h) 
{}&amp;= \left[ f(x) + \frac{f&#39;(x)(h)^1}{1!} + \frac{f&#39;&#39;(x)(h)^2}{2!}\  +\  \frac{f&#39;&#39;&#39;(x)(h)^3}{3!} + O(h^4) \right] \nonumber \\
&amp;+ \left[ f(x) + \frac{f&#39;(x)(-h)^1}{1!} + \frac{f&#39;&#39;(x)(-h)^2}{2!}\  +\  \frac{f&#39;&#39;&#39;(x)(-h)^3}{3!} + O(h^4) \right] \nonumber \\
&amp;= \left[ 2f(x) + 2\frac{f&#39;&#39;(x)(h)^2}{2!}\ + 2\frac{f^{(4)}(x)(h)^4}{4!}\ + O(h^6) \right]  \\
&amp;=  2f(x) + f&#39;&#39;(x)(h)^2\ + O(h^4) + ...
\end{align}\]</span>
</p>
<p>Note that terms with <strong>odd stepsize</strong> such as <span class="math inline">\(\pm h^1, \pm h^3, \pm h^5, ...\)</span> are cancelled when added, e.g.:</p>
<p><span class="math display">\[\begin{align}
f&#39;(x)(h)^1 + f&#39;(x)(-h)^1 = f&#39;(x)(h)^1 - f&#39;(x)(h)^1 = 0
\end{align}\]</span></p>
<p>Now, re-arrange the terms to get the approximate second derivative of a function:</p>
<p><span class="math display">\[\begin{align}
f(x + h) + f(x -h) {}&amp;= 2f(x) + f&#39;&#39;(x)(h)^2\ +  O(h^4) \\
f&#39;&#39;(x)(h)^2\ + O(h^4)  &amp;= f(x + h) - 2f(x)  + f(x -h) \\
f&#39;&#39;(x)\  &amp;= \frac{f(x + h) - 2f(x) + f(x -h) }{h^2} + O(h^2)\ \ \leftarrow \frac{O(h^4)}{h^2} = O(h^2) \\
f&#39;&#39;(x)\ &amp;\approx \frac{f(x + h) - 2f(x) + f(x -h) }{h^2}
\end{align}\]</span></p>
<p>We drop the order of accuracy <span class="math inline">\(\mathbf{O(h^2)}\)</span> since we are approximating. From there, we get the <strong>second-order Finite Centered Difference</strong>.</p>
<p>We then compare that with the exact solution based on the second derivative:</p>
<p><span class="math display">\[
f&#39;(x) = -6sin(6x)\ \ \ \ \rightarrow\ \ \ \ \ \ f&#39;&#39;(x) = -36cos(6x)
\]</span></p>
</div>
</div>
<div id="approximation-using-ordinary-differential-equations" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.4</span> Approximation using Ordinary Differential Equations  <a href="numericalcalculus.html#approximation-using-ordinary-differential-equations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Change</strong> happens from moment to moment and place to place. It happens from the movement of celestial bodies down to the motion of a pendulum. There is <strong>Change</strong> where there is growing, aging, decaying, cooling, and heating. There is also <strong>Change</strong> as water flows downstream or as the wind blows in all directions.</p>
<p>In this section, we deal with the rate of <strong>Change</strong> with respect to time - we introduce <strong>Differential Equations</strong>. </p>
<p><strong>Differential Equations</strong>, in one of its most basic examples, is expressed in one of the following general forms:</p>
<p><span class="math display">\[\begin{align}
y&#39; = y\ \ \ \ \ \ or\ \ \ \ \ \ \ y&#39;&#39; = y\ \ \ \ \ \ or\ \ \ \ \ \ \ y&#39;&#39; + y&#39; = y\ \ \ \ \  or\ \ \ \ \ \ \ y&#39;&#39;&#39; + y&#39;&#39; + y&#39; = y
\end{align}\]</span></p>
<p>It simply means that the equation comes with a function <strong>y</strong> and its corresponding one or more derivatives <strong>yâ</strong>, <strong>yââ</strong>, etc. Note that what we refer to as function <strong>y</strong> can also be expressed in this form:</p>
<p><span class="math display">\[\begin{align}
y&#39; = y\ \ \ \rightarrow\ \ \  f&#39;(x) = f(x)\ \ \ \ \ \ where\ y = f(x)\ \ and\ \ y&#39; = f&#39;(x)
\end{align}\]</span></p>
<p>For most of the <strong>methods of differential equations</strong> covered in this chapter, we use the simplest form: <span class="math inline">\(y&#39; = y\ or\ y&#39;&#39;=y\)</span> to explain a point.</p>
<p>The idea of <strong>differential equations</strong> is to <strong>solve for function</strong> <strong>y</strong>. For that, we use a method called <strong>separation of variables</strong>. As long as a <strong>differential equation</strong> can be separable, then a <strong>separable differential equation</strong> can yield us a solution to a function. For example, we know that:</p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dx} = y\ \ \ \ \leftarrow\ \ \ \ \ y&#39; = y 
\end{align}\]</span></p>
<p>Here we separate <strong>x</strong> and <strong>y</strong> like so, then we integrate:</p>
<p><span class="math display">\[
\frac{dy}{dx} = y\ \ \ \ \rightarrow\ \ \ \frac{dy}{y} = dx\ \ \rightarrow\ \ \ \int\frac{dy}{y} = \int dx\ \ \ \rightarrow\ \ \ \ln|y|+c = x + c\ \ \ \ \rightarrow\ \ \ y = e^{x}
\]</span></p>
<p>Here, we illustrate how to solve a function which can be expressed this way:</p>
<p><span class="math display">\[
y = e^{x}\ \  \ \ \rightarrow\ \ \ \ \ f(x) = e^{x}
\]</span></p>
<p>Plotting <span class="math inline">\(\mathbf{f(x) = e^{x}}\)</span> gives us the following Figure <a href="numericalcalculus.html#fig:differential1">4.13</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:differential1"></span>
<img src="embed0032.png" alt="Separable Differential Equations" width="70%" />
<p class="caption">
Figure 4.13: Separable Differential Equations
</p>
</div>
<p>One pointer to note is that not all <strong>Differential Equations</strong> are <strong>separable</strong> or that solving the function <strong>y</strong> may post an extreme challenge. For that, we need <strong>to approximate</strong>. We investigate a few <strong>approximation</strong> methods to solve function <strong>y</strong> in <strong>ODE</strong> following the rest of the sections. Note that other literature denotes the approximate solution <strong>y</strong> as <strong>u</strong> to distinguish exact solution vs.Â approximate solution, e.g.:</p>
<p><span class="math display">\[
\mathbf{y_{(exact)}\ vs\ y_{(approx)}}\ \ \ \rightarrow
\ \ \ \ \ \ \ \ y=e^x\ \ \ \ \ \ vs\ \ \ \ u\approx c_0 + c_1x + c_2 x^2
\]</span></p>
<p>In most of our discussions, we use the notation <strong>y</strong> to denote the approximate solution. And we express the exact solution more explicitly.</p>
<p>First, we introduce two types of <strong>Differential equations</strong>: the first type is the <strong>Ordinary Differential Equations (ODE)</strong> with one input. The second type is the <strong>Partial Differential Equations (PDE)</strong> with multiple inputs.</p>
<p>With <strong>ODE</strong>, there are two problems we try to solve: the <strong>initial value problem (IVP)</strong> and the <strong>boundary value problem (BVP)</strong>.</p>
<p>In many cases, one of the challenges we deal with when it comes to <strong>Change</strong> is determining the initial state of the problem before the first <strong>Change</strong> - what was the <strong>initial value</strong> of the problem? If the <strong>initial value</strong> is unknown, we face multiple solutions. But suppose a problem has a given initial value, e.g., <span class="math inline">\(y(0) = 1\)</span>, and has a derivative of an unknown function. In that case, this is an <strong>initial value problem (IVP)</strong>, so then we can approximate a target value.</p>
<p>We introduce a few methods that deal with solving <strong>IVPs</strong> in the following sections, using an <strong>initial value</strong> as a starting point for an <strong>iterative method</strong>. We start with one of the classic methods called <strong>Eulerâs method</strong>.</p>
<div id="eulers-method-explicit" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.1</span> Eulerâs Method (Explicit) <a href="numericalcalculus.html#eulers-method-explicit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All Change starts from an <strong>initial value</strong>, then <strong>Change</strong> happens one step at a time over the course of a period. If we are to express the statement into an <strong>Euler</strong> equation, we get:</p>
<p><span class="math display">\[\begin{align}
y_{1} = y_0 + \Delta t y&#39;_0\ \ \ \leftarrow y_1 = y_0 + \Delta t\ f(t_0, y_0) \ \ \ \ \ \leftarrow \text{recall}\ y = mx + b 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{array}{lll}
y_0 &amp;\leftarrow &amp; \text{initial value} \\
y_1 &amp;\leftarrow &amp;\text{next value} \\
t &amp;\leftarrow &amp; \text{time}\ \ \ \text{t is the abscissa x in x-axis }\\
\Delta t  &amp;\leftarrow &amp;\text{one step at a time (stepsize of time)}  \\
y_0&#39; &amp;\leftarrow &amp;\text{course of time (change with respect to time)}\ \ \leftarrow \frac{dy_0}{dt_0}
\end{array}
\]</span></p>
<p>Here, we are after approximating an <strong>unknown target point</strong> given an <strong>initial point</strong> and a <strong>first-order derivative</strong> of a function. In general, the formula is as follows:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} = y_{k} + \Delta t y&#39;_k \ \ \ \leftarrow y_{k+1} = y_k + \Delta t\ f(t_k, y_k)
\end{align}\]</span></p>
<p>and where:</p>
<p><span class="math display">\[\begin{align}
y_k&#39; = f(t_k, y_k)
\end{align}\]</span></p>
<p>To illustrate, we use basic <strong>iterative method</strong> called <strong>Eulerâs method</strong>.</p>
<p>For example, use <strong>Eulerâs method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[\begin{align}
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y 
\end{align}\]</span></p>
<p>Here, we have:</p>
<p><span class="math display">\[\begin{align}
f(t_k, y_k) = y_k&#39; = y\ \ \ \ \ \ \ \ \text{where our exact solution is }\ \mathbf{y = e^t}
\end{align}\]</span></p>
<p>See Table <a href="numericalcalculus.html#tab:eulertable1">4.7</a>.</p>

<table>
<caption><span id="tab:eulertable1">Table 4.7: </span>Euler Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(y_k + \Delta t y&#39;_k\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right"><span class="math inline">\(1.000 + (0.2)(1.000)\)</span></td>
<td align="right">1.200</td>
<td align="right">1.221</td>
<td align="right">0.021</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.200</td>
<td align="right"><span class="math inline">\(1.200 + (0.2)(1.200)\)</span></td>
<td align="right">1.440</td>
<td align="right">1.492</td>
<td align="right">0.052</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.440</td>
<td align="right"><span class="math inline">\(1.440 + (0.2)(1.440)\)</span></td>
<td align="right">1.728</td>
<td align="right">1.822</td>
<td align="right">0.094</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.728</td>
<td align="right"><span class="math inline">\(1.728 + (0.2)(1.728)\)</span></td>
<td align="right">2.074</td>
<td align="right">2.226</td>
<td align="right">0.152</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.074</td>
<td align="right"><span class="math inline">\(2.074 + (0.2)(2.074)\)</span></td>
<td align="right">2.488</td>
<td align="right">2.718</td>
<td align="right">0.230</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.488}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.230}\)</span></td>
</tr>
</tbody>
</table>

<p>Here is a naive implementation of <strong>Eulerâs method</strong> in R code using a more general <strong>IVP method function</strong> (see Figure <a href="numericalcalculus.html#fig:ivp">4.14</a>):</p>

<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>( x) }</a>
<a class="sourceLine" id="cb164-2" data-line-number="2">f_forward &lt;-<span class="st"> </span><span class="cf">function</span>(tk, yk) { yk } </a>
<a class="sourceLine" id="cb164-3" data-line-number="3">f_backward &lt;-<span class="st"> </span><span class="cf">function</span>(delta_tk, yk) { yk <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>delta_tk )} </a>
<a class="sourceLine" id="cb164-4" data-line-number="4">ivp_method &lt;-<span class="st"> </span><span class="cf">function</span>(init_val, fun, iteration, target, </a>
<a class="sourceLine" id="cb164-5" data-line-number="5">                       <span class="dt">type=</span><span class="st">&quot;rungekutta&quot;</span>) {</a>
<a class="sourceLine" id="cb164-6" data-line-number="6">   x0 =<span class="st"> </span>init_val[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb164-7" data-line-number="7">   y0 =<span class="st"> </span>yk =<span class="st"> </span>init_val[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb164-8" data-line-number="8">   k =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, target, <span class="dt">length.out=</span>iteration )</a>
<a class="sourceLine" id="cb164-9" data-line-number="9">   stepsize =<span class="st"> </span>k[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>k[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb164-10" data-line-number="10">   sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb164-11" data-line-number="11">   cnt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb164-12" data-line-number="12">   <span class="cf">for</span> (tk <span class="cf">in</span> k) {</a>
<a class="sourceLine" id="cb164-13" data-line-number="13">       <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;explicit&quot;</span>) {  <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb164-14" data-line-number="14">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">f_forward</span>(tk, yk)</a>
<a class="sourceLine" id="cb164-15" data-line-number="15">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;implicit&quot;</span>) { <span class="co"># euler backward</span></a>
<a class="sourceLine" id="cb164-16" data-line-number="16">           yk =<span class="st">  </span><span class="kw">f_backward</span>( stepsize, yk)</a>
<a class="sourceLine" id="cb164-17" data-line-number="17">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;heun&quot;</span>) {</a>
<a class="sourceLine" id="cb164-18" data-line-number="18">           k1 =<span class="st">  </span><span class="kw">f_forward</span>(tk, yk) </a>
<a class="sourceLine" id="cb164-19" data-line-number="19">           k2 =<span class="st">  </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k1 )</a>
<a class="sourceLine" id="cb164-20" data-line-number="20">           yk =<span class="st"> </span>heun =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(k1 <span class="op">+</span><span class="st"> </span>k2)</a>
<a class="sourceLine" id="cb164-21" data-line-number="21">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;rungekutta&quot;</span>) {</a>
<a class="sourceLine" id="cb164-22" data-line-number="22">           k1 =<span class="st"> </span><span class="kw">f_forward</span>(tk, yk)  </a>
<a class="sourceLine" id="cb164-23" data-line-number="23">           k2 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1 )  </a>
<a class="sourceLine" id="cb164-24" data-line-number="24">           k3 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2 )  </a>
<a class="sourceLine" id="cb164-25" data-line-number="25">           k4 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3)  </a>
<a class="sourceLine" id="cb164-26" data-line-number="26">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>( k1 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k3 <span class="op">+</span><span class="st"> </span>k4)</a>
<a class="sourceLine" id="cb164-27" data-line-number="27">       }</a>
<a class="sourceLine" id="cb164-28" data-line-number="28">       true_y =<span class="st"> </span><span class="kw">exp</span>(tk )  </a>
<a class="sourceLine" id="cb164-29" data-line-number="29">       err =<span class="st"> </span><span class="kw">abs</span>( true_y <span class="op">-</span><span class="st"> </span>y0 )</a>
<a class="sourceLine" id="cb164-30" data-line-number="30">       cnt =<span class="st"> </span>cnt <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb164-31" data-line-number="31">       sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(cnt, stepsize, tk,  y0, </a>
<a class="sourceLine" id="cb164-32" data-line-number="32">                                    true_y, err))</a>
<a class="sourceLine" id="cb164-33" data-line-number="33">       y0 =<span class="st"> </span>yk</a>
<a class="sourceLine" id="cb164-34" data-line-number="34">   }</a>
<a class="sourceLine" id="cb164-35" data-line-number="35">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;iteration&quot;</span>, <span class="st">&quot;stepsize&quot;</span>, <span class="st">&quot;tk&quot;</span>,</a>
<a class="sourceLine" id="cb164-36" data-line-number="36">                          <span class="st">&quot;y-approx&quot;</span>, <span class="st">&quot;y-exact&quot;</span>, <span class="st">&quot;absolute error&quot;</span>)</a>
<a class="sourceLine" id="cb164-37" data-line-number="37">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=<span class="st"> </span>sequence, <span class="st">&quot;count&quot;</span>=<span class="kw">nrow</span>(sequence), <span class="st">&quot;type&quot;</span>=type )</a>
<a class="sourceLine" id="cb164-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb164-39" data-line-number="39">ivp_plot &lt;-<span class="st"> </span><span class="cf">function</span>(ivp, base_delta, target, <span class="dt">label=</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb164-40" data-line-number="40">  iteration =<span class="st"> </span>ivp<span class="op">$</span>Iteration</a>
<a class="sourceLine" id="cb164-41" data-line-number="41">  iteration =<span class="st"> </span>iteration[ <span class="kw">which</span>( <span class="kw">round</span>(iteration[,<span class="dv">3</span>],<span class="dv">3</span>) <span class="op">%in%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb164-42" data-line-number="42"><span class="st">                         </span><span class="kw">round</span>(<span class="kw">seq</span>(<span class="dv">0</span>, target, base_delta),<span class="dv">3</span>) ), ]</a>
<a class="sourceLine" id="cb164-43" data-line-number="43">  n =<span class="st"> </span><span class="kw">nrow</span>(iteration)</a>
<a class="sourceLine" id="cb164-44" data-line-number="44">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb164-45" data-line-number="45">     h =<span class="st"> </span>iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span>iteration[k,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb164-46" data-line-number="46">     x1 =<span class="st"> </span>iteration[k,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb164-47" data-line-number="47">     x2 =<span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>h</a>
<a class="sourceLine" id="cb164-48" data-line-number="48">     y1_true =<span class="st"> </span><span class="kw">f_true</span>(x1)</a>
<a class="sourceLine" id="cb164-49" data-line-number="49">     y2_true =<span class="st"> </span><span class="kw">f_true</span>(x2)</a>
<a class="sourceLine" id="cb164-50" data-line-number="50">     y1 =<span class="st"> </span>iteration[k,<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb164-51" data-line-number="51">     y2 =<span class="st"> </span>iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb164-52" data-line-number="52">     <span class="cf">if</span> (k <span class="op">==</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb164-53" data-line-number="53">       a =<span class="st"> </span><span class="kw">round</span>( iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">4</span>],<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb164-54" data-line-number="54">       stepsize =<span class="st"> </span>iteration[k, <span class="dv">2</span>]</a>
<a class="sourceLine" id="cb164-55" data-line-number="55">       <span class="kw">text</span>(<span class="fl">1.2</span>, y2, <span class="dt">cex=</span><span class="fl">0.7</span>, <span class="dt">label=</span><span class="kw">substitute</span>(</a>
<a class="sourceLine" id="cb164-56" data-line-number="56">           <span class="kw">paste</span>(Delta,<span class="st">&quot;t=&quot;</span>, stepsize, <span class="st">&quot;, y(1)=&quot;</span>, a, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>))) </a>
<a class="sourceLine" id="cb164-57" data-line-number="57">     }</a>
<a class="sourceLine" id="cb164-58" data-line-number="58">     <span class="kw">lines</span>(<span class="kw">c</span>(x1,x2), <span class="kw">c</span>(y1,y2), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb164-59" data-line-number="59">     <span class="kw">points</span>(<span class="kw">c</span>(x1, x2), <span class="kw">c</span>(y1, y2), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb164-60" data-line-number="60">     <span class="kw">points</span>(<span class="kw">c</span>(x1, x2), <span class="kw">c</span>(y1_true, y2_true), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb164-61" data-line-number="61">  }</a>
<a class="sourceLine" id="cb164-62" data-line-number="62">  <span class="cf">if</span> (ivp<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;explicit&quot;</span> <span class="op">&amp;&amp;</span><span class="st"> </span>label<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb164-63" data-line-number="63">    <span class="kw">text</span>(<span class="dv">1</span>,<span class="fl">2.3</span>, <span class="dt">cex=</span><span class="fl">0.7</span>, <span class="dt">label=</span><span class="st">&quot;tangent&quot;</span>)</a>
<a class="sourceLine" id="cb164-64" data-line-number="64">  }</a>
<a class="sourceLine" id="cb164-65" data-line-number="65">  <span class="cf">if</span> (label<span class="op">==</span><span class="ot">TRUE</span>) { iteration }</a>
<a class="sourceLine" id="cb164-66" data-line-number="66">}</a>
<a class="sourceLine" id="cb164-67" data-line-number="67">n =<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb164-68" data-line-number="68">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a></code></pre></div>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1      0.2 0.0  1.00000 1.000000     0.00000000
## [2,]         2      0.2 0.2  1.20000 1.221403     0.02140276
## [3,]         3      0.2 0.4  1.44000 1.491825     0.05182470
## [4,]         4      0.2 0.6  1.72800 1.822119     0.09411880
## [5,]         5      0.2 0.8  2.07360 2.225541     0.15194093
## [6,]         6      0.2 1.0  2.48832 2.718282     0.22996183</code></pre>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1     0.05 0.0 1.000000 1.000000    0.000000000
## [2,]         5     0.05 0.2 1.215506 1.221403    0.005896508
## [3,]         9     0.05 0.4 1.477455 1.491825    0.014369254
## [4,]        13     0.05 0.6 1.795856 1.822119    0.026262474
## [5,]        17     0.05 0.8 2.182875 2.225541    0.042666340
## [6,]        21     0.05 1.0 2.653298 2.718282    0.064984123</code></pre>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1    0.002 0.0 1.000000 1.000000   0.0000000000
## [2,]       101    0.002 0.2 1.221159 1.221403   0.0002439310
## [3,]       201    0.002 0.4 1.491229 1.491825   0.0005958164
## [4,]       301    0.002 0.6 1.821027 1.822119   0.0010914887
## [5,]       401    0.002 0.8 2.223764 2.225541   0.0017773523
## [6,]       501    0.002 1.0 2.715569 2.718282   0.0027133078</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ivp"></span>
<img src="DS_files/figure-html/ivp-1.png" alt="IVP Methods" width="70%" />
<p class="caption">
Figure 4.14: IVP Methods
</p>
</div>

<p>Note that the dotted lines in the figure represent <strong>tangent lines</strong> for which the corresponding slope is based on the approximated point (<span class="math inline">\(\mathbf{t_{k+1}, y_{k+1}}\)</span>). We sample three iterations in the code towards an approximate value. The one with the smallest step size (0.002) results in better accuracy. It yields an approximate value of <span class="math inline">\(\mathbf{2.71557}\)</span> with an absolute error of <span class="math inline">\(\mathbf{0.0027}\)</span> compared to the other two. As the plot shows in Figure <a href="numericalcalculus.html#fig:ivp">4.14</a>, we can observe that the smaller the step size becomes, the tangent line gets more aligned to the actual function curve. Also, we excluded all other nodes (or tangent lines) in the plot to show a fixed set of select but fixed nodes.</p>
<p>Also, recall that if we do not specify an <strong>initial value</strong>, we end up with an infinite number of solutions. Figure <a href="numericalcalculus.html#fig:euler2">4.15</a> shows an example of multiple solutions. In the figure, we show three possible initial points, <span class="math inline">\(y(0) \in (0.5,1,5)\)</span>, arriving at three separate unique solutions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:euler2"></span>
<img src="DS_files/figure-html/euler2-1.png" alt="Initial Value Problem - Euler Method" width="70%" />
<p class="caption">
Figure 4.15: Initial Value Problem - Euler Method
</p>
</div>

<p>Indeed, our goal in solving <strong>IVPs</strong> is to set a unique solution by having an <strong>initial value</strong> and a corresponding derivative of a function.</p>
</div>
<div id="eulers-method-implicit" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.2</span> Eulerâs Method (Implicit)<a href="numericalcalculus.html#eulers-method-implicit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The previous <strong>Eulerâs method</strong> is an <strong>explicit method</strong> which uses conditions under the current state to advance to the next state. This method is called <strong>Forward Eulerâs method</strong> with the following general formula:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} = y_{k} + \Delta t\ y&#39;_{k} \ \ \ \leftarrow y_{k+1} = y_k + \Delta t\ f(t_{k}, y_{k})
\ \ \leftarrow\ \ \ \frac{y_{k+1} - y_{k-1}}{\Delta t} = f(t_{k}, y_{k})
\end{align}\]</span></p>
<p>which can also be derived from a truncated <strong>Taylor series</strong> expansion:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} \approx f(t_k + \Delta t) = f(t_k) + \frac{f&#39;(t_k)\Delta t^1}{1!} + O(\Delta t^2)
\end{align}\]</span></p>
<p>Here, we explore <strong>Eulerâs method</strong> further by illustrating <strong>implicit method</strong> called <strong>Backward Eulerâs method</strong>. The method uses conditions of the next state, which may appear counter-intuitive, but the method becomes more apparent using the following general formula and an illustration.</p>
<p><span class="math display">\[\begin{align}
y_{k+1} &amp;= y_{k} + \Delta t y&#39;_{k+1} \\
&amp;= y_k + \Delta t\ f(t_{k+1}, y_{k+1})
\ \ \leftarrow\ \ \ \frac{y_{k+1} - y_{k}}{\Delta t} = f(t_{k+1}, y_{k+1})
\end{align}\]</span></p>
<p>which can also be derived from a truncated <strong>Taylor series</strong> expansion:</p>
<p><span class="math display">\[\begin{align}
y_{k} \approx f(t_{k+1} - \Delta t) = f(t_{k+1}) - \frac{f&#39;(t_{k+1})\Delta t^1}{1!} + O(\Delta t^2)
\end{align}\]</span></p>
<p>The <strong>implicit method</strong> improves upon the stability that otherwise, in certain cases, is unattainable using the <strong>explicit method</strong>.</p>
<p>To illustrate, we use <strong>implicit method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Let us re-arrange our <strong>implicit method</strong> equation to adapt to our specific problem:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} {}&amp;= y_k + \Delta t\ f(t_{k+1}, y_{k+1}) \\
y_{k+1} &amp;= y_k + \Delta t\ y_{k+1}\ \ \ \ \leftarrow\ \ \ \ \ \ f(t_{k+1}, y_{k+1}) = y&#39;_{k+1} = y_{k+1} \\
y_{k+1} - \Delta t\ y_{k+1} &amp;= y_k \\
y_{k+1}( 1 - \Delta t) &amp;= y_k \\
y_{k+1} &amp;= \frac{y_k}{(1 - \Delta t)}
\end{align}\]</span></p>
<p>See Table <a href="numericalcalculus.html#tab:eulertable2">4.8</a>.</p>
<table>
<caption><span id="tab:eulertable2">Table 4.8: </span>Backward Euler Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(y_k / (1 - \Delta t )\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right"><span class="math inline">\(1.000 / (0.8)\)</span></td>
<td align="right">1.250</td>
<td align="right">1.221</td>
<td align="right">0.029</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.250</td>
<td align="right"><span class="math inline">\(1.250 / (0.8)\)</span></td>
<td align="right">1.562</td>
<td align="right">1.492</td>
<td align="right">0.070</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.562</td>
<td align="right"><span class="math inline">\(1.562 / (0.8)\)</span></td>
<td align="right">1.953</td>
<td align="right">1.822</td>
<td align="right">0.131</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.953</td>
<td align="right"><span class="math inline">\(1.953 / (0.8)\)</span></td>
<td align="right">2.441</td>
<td align="right">2.226</td>
<td align="right">0.215</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.441</td>
<td align="right"><span class="math inline">\(2.441 / (0.8)\)</span></td>
<td align="right">3.052</td>
<td align="right">2.718</td>
<td align="right">0.334</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{3.052}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.334}\)</span></td>
</tr>
</tbody>
</table>
<p>As we can see, <strong>Backward Eulerâs method</strong> requires extra algebraic preparation to arrive at a new equation such as: <span class="math inline">\(y_{k+1} = y_k / (1 - \Delta t)\)</span>. Such preparation is only possible because we use a simple <strong>differential equation</strong>: <span class="math inline">\(y&#39; = y\)</span>. It could have been a lot more complicated. But even after simplifying the equation, <strong>Backward Eulerâs method</strong> does not always yield better accuracy than <strong>the explicit method</strong> though perhaps more stable.</p>
<p>As for applying a naive implementation of the <strong>implicit method</strong> in R code, just set the type to <strong>implicit</strong>.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb168-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb168-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb168-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, <span class="dt">type=</span><span class="st">&quot;implicit&quot;</span>)</a>
<a class="sourceLine" id="cb168-5" data-line-number="5"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
</div>
<div id="heuns-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.3</span> Heunâs Method <a href="numericalcalculus.html#heuns-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Heunâs method</strong> is called an <strong>improved Eulerâs method</strong>. It is also a second-order <strong>implicit method</strong> that combines both the <strong>Eulerâs explicit and implicit</strong> equations to arrive at the following:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} = y_k + \frac{\Delta t}{2}\left( \mathbf{k_1} + \mathbf{k_2} \right)
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{k_1} {}&amp;= f(t_{k}, y_{k}) \\
\mathbf{k_2} &amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \Delta t, y_k + \Delta t k_1)
\end{align*}\]</span></p>
<p>To illustrate, we use <strong>Heun method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Note that:</p>
<p><span class="math display">\[\begin{align}
y_k&#39; = f(t_k, y_k)
\end{align}\]</span></p>
<p><strong>First</strong>, we solve for <span class="math inline">\(\mathbf{k_1}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\mathbf{k_1} = f(t_{k}, y_{k}) = y&#39;_k = y_k  = 1 
\end{align}\]</span></p>
<p><strong>Second</strong>, we solve for <span class="math inline">\(\mathbf{k_2}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\mathbf{k_2} {}&amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \Delta t, y_k + \Delta t k_1) \\
&amp;= f( 0.0 + 0.2, 1 + 0.2(1)) = f(0.2, 1.2) \nonumber \\
&amp;= 1.2\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k   \nonumber
\end{align}\]</span></p>
<p><strong>Third</strong>, we solve for <span class="math inline">\(\mathbf{y_{k+1}}\)</span>:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} {}&amp;= y_k + \frac{\Delta t}{2}\left( \mathbf{k_1} + \mathbf{k_2} \right) \\
&amp;= 1 + \frac{0.2}{2}(1 + 1.2) \nonumber \\
&amp;= 1 + 0.1 (2.2) = 1.22 \nonumber
\end{align}\]</span></p>
<p><strong>Finally</strong>, we repeat the process until we hit the target, substituting <span class="math inline">\(\mathbf{y_{k}}\)</span> with <span class="math inline">\(\mathbf{y_{k+1}}\)</span>.</p>
<p>See Table <a href="numericalcalculus.html#tab:heuntable">4.9</a>.</p>
<table>
<caption><span id="tab:heuntable">Table 4.9: </span>Heun Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(f(t_k, y_k)\)</span></th>
<th align="right"><span class="math inline">\(f(t_{k+1}, y_{k+1})\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.000</td>
<td align="right">1.220</td>
<td align="right">1.220</td>
<td align="right">1.221</td>
<td align="right">0.001</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.220</td>
<td align="right">1.220</td>
<td align="right">1.464</td>
<td align="right">1.488</td>
<td align="right">1.492</td>
<td align="right">0.003</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.488</td>
<td align="right">1.488</td>
<td align="right">1.786</td>
<td align="right">1.816</td>
<td align="right">1.822</td>
<td align="right">0.006</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.816</td>
<td align="right">1.816</td>
<td align="right">2.179</td>
<td align="right">2.215</td>
<td align="right">2.223</td>
<td align="right">0.010</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.215</td>
<td align="right">2.215</td>
<td align="right">2.658</td>
<td align="right">2.702</td>
<td align="right">2.718</td>
<td align="right">0.015</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.702}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.015}\)</span></td>
</tr>
</tbody>
</table>
<p>As for applying a naive implementation of the <strong>Heunâs</strong> method in R code, just set type to <strong>heun</strong>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb169-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb169-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb169-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, <span class="dt">type=</span><span class="st">&quot;heun&quot;</span>)</a>
<a class="sourceLine" id="cb169-5" data-line-number="5"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
</div>
<div id="runge-kutta-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.4</span> Runge-Kutta Method <a href="numericalcalculus.html#runge-kutta-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Runge-Kutta method</strong> is an <strong>implicit method</strong> and comes with different formulas of N-order <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>. Below are two examples of the <strong>Runge-Kutta</strong> method of the 3rd-order and 4th-order.</p>
<p><span class="math display">\[
\begin{array}{llll}
\begin{array}{l}
3rd-Order \\
========\\
\mathbf{k_1} =  f(t_k, y_k)   \\
\mathbf{k_2} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_1} \right)  \\ 
\mathbf{k_3} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + 2 \mathbf{k_2} - \mathbf{k_1} \right) \\
\\
\\
\mathbf{y_{k+1}} = y_k + \frac{\Delta t}{6}\ ( \mathbf{k_1} + 4\mathbf{k_2} + \mathbf{k_3} )
\end{array} &amp; &amp; &amp;
\begin{array}{l}
4th-Order \\
========\\
\mathbf{k_1} =  f(t_k, y_k)   \\
\mathbf{k_2} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_1} \right)  \\ 
\mathbf{k_3} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_2} \right) \\
\mathbf{k_4} =  f(t_k + \Delta t,\ y_k + \Delta t  \mathbf{k_3})\\
\\
\mathbf{y_{k+1}} = y_k + \frac{\Delta t}{6}\ ( \mathbf{k_1} + 2\mathbf{k_2} + 2\mathbf{k_3} + \mathbf{k_4})
\end{array}
\end{array}
\]</span></p>
<p>In fact, <strong>Heunâs</strong> method is a 2nd-order <strong>Runge-Kutta</strong> method:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} = y_k + \frac{\Delta t}{2}(\mathbf{k_1} + \mathbf{k_2})
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\mathbf{k_1} {}&amp;= f(t_{k}, y_{k})  \\
\mathbf{k_2} &amp;=  f\left(t_k + \Delta t,\ y_k + \Delta t \mathbf{k_1} \right)
\end{align}\]</span></p>
<p>We leave the readers to investigate other orders such as 5th-order and 6th-order.</p>
<p>Here, we illustrate <strong>Runge-Kutta</strong> method using the 4th-order. Note that, with the 4th order, the parameters <span class="math inline">\(\mathbf{k_1, k_2, k_3, k_4}\)</span> are all <strong>slopes</strong> of tangent line to an estimated curve function. Those four slopes are averaged to arrive at an optimal slope, e.g. <span class="math inline">\(y_{k+1} = y&#39;_1\)</span>. See Figure <a href="numericalcalculus.html#fig:rungekutta">4.16</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rungekutta"></span>
<img src="rungekutta.png" alt="Runge-Kutta Method" width="70%" />
<p class="caption">
Figure 4.16: Runge-Kutta Method
</p>
</div>
<p>To illustrate, we use <strong>Runge-Kutta method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span>
and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p><strong>First</strong>, solve for <span class="math inline">\(\mathbf{k_1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
k_1 =  f(t_k, y_k) =  y_k&#39; = y_k = 1
\end{align}\]</span></p>
<p>Notice <span class="math inline">\(\mathbf{k_1}\)</span> is similar to <strong>forward Eulerâs method</strong></p>
<p><strong>Second</strong>, solve for <span class="math inline">\(\mathbf{k_2}\)</span>:</p>
<p><span class="math display">\[\begin{align}
k_2 {}&amp;=  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t k_1}{2} \right)  \\
&amp;= f \left(0.0 + \frac{0.2}{2},\ 1.0 + \frac{0.2 (1)}{2} \right) \nonumber \\
&amp;= f \left(0.1,\ 1.1 \right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k \nonumber \\
&amp;=1.1 \nonumber
\end{align}\]</span></p>
<p><strong>Third</strong>, solve for <span class="math inline">\(\mathbf{k_3}\)</span>:</p>
<p><span class="math display">\[\begin{align}
k_3 {}&amp;=  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t k_2}{2}\right) \\
&amp;=  f\left(0.0 + 0.1,\ 1.0 + 0.1(1.1)\right) \nonumber \\
&amp;= f\left(0.1, 1.11\right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k  \nonumber \\
&amp;=1.11 \nonumber
\end{align}\]</span></p>
<p><strong>Fourth</strong>, solve for <span class="math inline">\(\mathbf{k_4}\)</span>:</p>
<p><span class="math display">\[\begin{align}
k_4 {}&amp;=  f\left(t_k + \Delta t,\ y_k + \Delta t  k_3\right) \\
&amp;=  f\left(0.0 + 0.2,\ 1.0 + 0.2 (1.11)\right) \nonumber \\
&amp;= f\left(0.2, 1.222\right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k  \nonumber \\
&amp;= 1.222 \nonumber
\end{align}\]</span></p>
<p><strong>Fifth</strong>, solve for <span class="math inline">\(\mathbf{y_{k+1}}\)</span>:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} {}&amp;= y_k + \frac{\Delta t}{6}\ ( k_1 + 2k_2 + 2k_3 + k_4) \\
&amp;= 1.0 + \frac{0.2}{6}\ ( 1.0 + 2(1.1) + 2(1.11) + 1.222) \nonumber \\ 
&amp;= 1.0 + \frac{0.2}{6}\ (6.642) \nonumber \\
&amp;= 1.2214 \nonumber
\end{align}\]</span></p>
<p><strong>Finally</strong>, repeat the process until we hit the target, substituting <span class="math inline">\(\mathbf{y_{k}}\)</span> with <span class="math inline">\(\mathbf{y_{k+1}}\)</span>. See Table <a href="numericalcalculus.html#tab:rungetable">4.10</a>. In the table, <span class="math inline">\(\mathbf{y_{k+1}}\)</span> represents the approximate solution and <span class="math inline">\(\mathbf{e^{t+1}}\)</span> represents the exact target.</p>
<table>
<caption><span id="tab:rungetable">Table 4.10: </span>Runge-Kutta Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(k_1\)</span></th>
<th align="right"><span class="math inline">\(k_2\)</span></th>
<th align="right"><span class="math inline">\(k_=3\)</span></th>
<th align="right"><span class="math inline">\(k_4\)</span></th>
<th align="right"><span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right"><span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.000</td>
<td align="right">1.100</td>
<td align="right">1.110</td>
<td align="right">1.222</td>
<td align="right">1.221</td>
<td align="right">1.221</td>
<td align="right">2.8e-6</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.221</td>
<td align="right">1.221</td>
<td align="right">1.344</td>
<td align="right">1.356</td>
<td align="right">1.493</td>
<td align="right">1.492</td>
<td align="right">1.492</td>
<td align="right">6.7e-6</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.492</td>
<td align="right">1.492</td>
<td align="right">1.641</td>
<td align="right">1.660</td>
<td align="right">1.823</td>
<td align="right">1.822</td>
<td align="right">1.822</td>
<td align="right">1.2e-5</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.822</td>
<td align="right">1.822</td>
<td align="right">2.004</td>
<td align="right">2.023</td>
<td align="right">2.227</td>
<td align="right">2.226</td>
<td align="right">2.226</td>
<td align="right">2.0e-5</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.226</td>
<td align="right">2.226</td>
<td align="right">2.448</td>
<td align="right">2.470</td>
<td align="right">2.720</td>
<td align="right">2.718</td>
<td align="right">2.718</td>
<td align="right">3.1e-5</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{3.1e-5}\)</span></td>
</tr>
</tbody>
</table>
<p>As for applying a naive implementation of the <strong>Runge-Kutta method</strong> in R code, just set the type to <strong>rungekutta</strong>.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb170-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb170-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb170-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb170-5" data-line-number="5">                 <span class="dt">type=</span><span class="st">&quot;rungekutta&quot;</span>)</a>
<a class="sourceLine" id="cb170-6" data-line-number="6"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<p>Before we move on to <strong>Boundary Value Problems</strong>, we leave readers to investigate two enhancements to Runge-Kutta methods that use <strong>Adaptive Stepsize</strong>, which can yield errors in the order of <span class="math inline">\(O(h^9)\)</span> or even <span class="math inline">\(O(h^{10})\)</span>.</p>
<ul>
<li>Dormand-Prince Method (1980)</li>
<li>Bogacki-Shampine Method (1989)</li>
</ul>
<p>We also leave readers to investigate <strong>Multi-Step Methods</strong> such as:</p>
<ul>
<li>Adams-Bashforth Method</li>
<li>Adams-Moulton Method</li>
</ul>
</div>
<div id="shooting-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.5</span> Shooting Method <a href="numericalcalculus.html#shooting-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have methods that deal with <strong>Initial Value Problems (IVP)</strong> in the previous sections. This section covers the discussion around <strong>Boundary Value Problems (BVP)</strong>. Here, we shall see that we can reduce <strong>BVP</strong> into <strong>IVP</strong> to simplify the process. As a start, we use the <strong>Shooting</strong> method. To explain the method, we use Figure <a href="numericalcalculus.html#fig:shootingmethod">4.17</a>.     </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:shootingmethod"></span>
<img src="shootingmethod.png" alt="Shooting Method" width="70%" />
<p class="caption">
Figure 4.17: Shooting Method
</p>
</div>
<p>As observed in the figure, our target is represented as a slope given by <span class="math inline">\(y&#39;(b) = \beta\)</span> at a horizontal distance <span class="math inline">\(b\)</span>. We first try to aim at the target by adjusting our angle in the form of a slope given by <span class="math inline">\(y&#39;(a) = \alpha_1\)</span>. This first attempt to <strong>shoot an arrow</strong> forms a trajectory that eventually lands at <span class="math inline">\(b\)</span> with an angle represented as a slope given by <span class="math inline">\(y&#39;(b) = \sigma_1\)</span>. Similarly, we make a second attempt, <strong>shooting</strong> at an angle, <span class="math inline">\(y&#39;(a) = \alpha_2\)</span>, and hits a slope, <span class="math inline">\(y&#39;(b) = \sigma_2\)</span>. It does show that we miss the target twice.</p>
<p>Ideally, if we shoot multiple attempts, we can use <strong>interpolation</strong> using the approximate targets to find one unique solution closer to the actual target. We explain that later in this section.</p>
<p>Now, when it comes to boundaries, notice that there seems to be a boundary condition in place for both the starting point and ending point ( a Dirichlet type of boundary condition):</p>
<p><span class="math display">\[
a &lt; x &lt; b 
\]</span></p>
<p>There are a few common boundary conditions we can use depending on the situation. See Table <a href="numericalcalculus.html#tab:bvpboundary">4.11</a>.</p>

<table>
<caption><span id="tab:bvpboundary">Table 4.11: </span>Shooting Method Boundaries</caption>
<thead>
<tr class="header">
<th align="left">Boundary Conditions</th>
<th align="left">Starting Point (a)</th>
<th align="left">Ending Point (b)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Neumann</td>
<td align="left"><span class="math inline">\(y&#39;(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(b) = \beta\)</span></td>
</tr>
<tr class="even">
<td align="left">Dirichlet</td>
<td align="left"><span class="math inline">\(y(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y(b) = \beta\)</span></td>
</tr>
<tr class="odd">
<td align="left">Cauchy</td>
<td align="left"><span class="math inline">\(y(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(a) = \beta\)</span></td>
</tr>
<tr class="even">
<td align="left">Robin</td>
<td align="left"><span class="math inline">\(c_1 y(a) + c_2 y&#39;(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(c_1 y(b) + c_2 y&#39;(b) = \beta\)</span></td>
</tr>
<tr class="odd">
<td align="left">Periodic</td>
<td align="left"><span class="math inline">\(y(a) = y(b)\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(b) = y&#39;(b)\)</span></td>
</tr>
</tbody>
</table>

<p>For our illustration, we use the <strong>Dirichlet</strong> boundary condition against a nonlinear second-order <strong>BVP</strong> equation:</p>
<p><span class="math display">\[\begin{align}
y&#39;&#39; = f(t, y, y&#39;)\ \ \ \ \ \ \ \ \ where\ \ \ \ a \leq t \leq b
\end{align}\]</span></p>
<p>with boundary conditions,</p>
<p><span class="math display">\[\begin{align}
y(a) = \alpha,\ \ \ \ \ \ \ \ \ y(b) = \beta 
\end{align}\]</span></p>
<p>One way to solve for two-point <strong>BVP</strong> is to resort back to any <strong>IVP</strong> methods; in particular, we arbitrarily choose the simple first-order <strong>forward Euler method</strong>. We know that <strong>IVP</strong> is satisfied with the following conditions (which is a <strong>Cauchy</strong> type of boundary condition):</p>
<p><span class="math display">\[
y(a) = \alpha,\ \ \ \ \ \ y&#39;(a) = \sigma,\ \ \ \ where\ \sigma =\ &lt;guess&gt;
\]</span></p>
<p>To use <strong>IVP</strong> method, we perform some mathematical manipulation against the following equation:</p>
<p><span class="math display">\[\begin{align}
 y&#39;&#39;(t) = \frac{d^2y}{dt^2}\ \ \ \rightarrow (y&#39;(t))&#39; = \frac{d}{dt}\left(\frac{dy}{dt}\right)
\end{align}\]</span></p>
<p>by extracting such first-order derivative from the second-order derivative of the below <strong>Leibnitz</strong> notation:</p>
<p><span class="math display">\[\begin{align}
\frac{d^2y}{dt^2} = \frac{d}{dt}\left(\frac{dy}{dt}\right) \ \ \ \rightarrow\ \ \ \ (1)\ \ \ \
\frac{dy}{dt} = x = f(x,y,t),\ \ \ \ (2)\ \ \ \ \frac{dx}{dt} = g(x,y,t)
\end{align}\]</span></p>
<p>and then use <strong>Eulerâs forward method</strong> formula (e.g. <span class="math inline">\(y_{k+1} = y_k + \Delta t\ f\)</span>):</p>
<p><span class="math display">\[\begin{align}
(1)\ \ \ \
\frac{dy}{dt} &amp;= f(x,y,t)\ \ \rightarrow y_{k+1} = y_k + f(x_k, y_k, t_k) \Delta t \\
(2)\ \ \ \ 
\frac{dx}{dt} &amp;= g(x,y,t)\ \ \rightarrow x_{k+1} = x_k + g(x_k, y_k, t_k) \Delta t  
\end{align}\]</span></p>
<p>So in general, solving <strong>BVP</strong> using <strong>shooting method</strong> given:</p>
<p><span class="math display">\[\begin{align}
\frac{d^2y}{dt^2} = \frac{dx}{dt} =g(x, y, t) \ \ \ \ \rightarrow\ \ \ \ where\ \ \ y(a) = \alpha,\ \ \ y(b) = \beta
\end{align}\]</span></p>
<p>can be solved using <strong>IVP</strong> following the below equation and condition:</p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dt} = f(x,y,t) \ \ \ \ \rightarrow\ \ \ \ where\ \ y(a) = \alpha,\ \ \ y&#39;(a) = \sigma
\end{align}\]</span></p>
<p>Our goal then is to guess for:</p>
<p><span class="math display">\[\begin{align}
\frac{dy}{dt}(0) = y&#39;(0) = x(0) = &lt;unknown&gt;
\end{align}\]</span></p>
<p>To illustrate, suppose we have the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p><strong>First</strong>, let us aim for our <strong>target</strong>. We know that <strong>aiming</strong> for the <strong>target</strong> can be a <strong>miss</strong> or a <strong>hit</strong>. We assume that our first attempt can be a <strong>miss</strong> (a close one but still a miss).</p>
<p>Our approximate target (guess) is:</p>
<p><span class="math display">\[
y_0&#39;(0) = x_0 = 1.10
\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{y_1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \Delta t \\
y_1 &amp;= y_0 + f(x_0, y_0, t_0) \Delta t \\
&amp;= 1 + f(1.100, 1.00, 0.00) 0.2 \nonumber \\
&amp;= 1 + 1.100 * 0.2  \ \ \leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \nonumber  \\
&amp;= 1.220 \nonumber 
\end{align}\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{x_1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \Delta t \\
x_1 &amp;= x_0 + g(x_0, y_0, t_0) \Delta t \\
&amp;= 1.100 + g(1.100, 1.00, 0.00) 0.2 \nonumber \\
&amp;= 1.100 + 1.00 * 0.2  \ \ \leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \nonumber \\
&amp;= 1.300 \nonumber 
\end{align}\]</span></p>
<p><strong>Second</strong>, let us iterate one more time:</p>
<p>Solve for <span class="math inline">\(\mathbf{y_2}\)</span>:</p>
<p><span class="math display">\[\begin{align}
y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \Delta t \\
y_2 &amp;= y_1 + f(x_1, y_1, t_1) \Delta t \\
&amp;= 1.220 + f(1.300, 1.220, 0.20) 0.2 \nonumber \\
&amp;= 1.220 + 1.300 * 0.2  \ \ \leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \nonumber  \\
&amp;= 1.480 \nonumber 
\end{align}\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{x_2}\)</span>:</p>
<p><span class="math display">\[\begin{align}
x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \Delta t \\
x_2 &amp;= x_1 + g(x_1, y_1, t_1) \Delta t \\
&amp;= 1.300 + g(1.300, 1.220, 0.20) 0.2 \nonumber \\
&amp;= 1.300 + 1.220 * 0.2  \ \ \leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \nonumber \\
&amp;= 1.544 \nonumber 
\end{align}\]</span></p>
<p>We repeat a few times until we reach <span class="math inline">\(y_k(1)\)</span>.</p>
<p>See Table <a href="numericalcalculus.html#tab:shooting">4.12</a>.</p>
<table>
<caption><span id="tab:shooting">Table 4.12: </span>Shooting Method Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.100</td>
<td align="right">1.100</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.220</td>
<td align="right">1.300</td>
<td align="right">1.221</td>
<td align="right">0.001</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.480</td>
<td align="right">1.544</td>
<td align="right">1.492</td>
<td align="right">0.012</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.789</td>
<td align="right">1.840</td>
<td align="right">1.822</td>
<td align="right">0.033</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.157</td>
<td align="right">2.198</td>
<td align="right">2.226</td>
<td align="right">0.069</td>
</tr>
<tr class="even">
<td align="right">1.0</td>
<td align="right">2.596</td>
<td align="right">2.630</td>
<td align="right">2.718</td>
<td align="right">0.122</td>
</tr>
</tbody>
</table>
<p>Here is a naive implementation of <strong>Shooting method</strong> in R code using a more general <strong>BVP method function</strong> and also, this time, using the <strong>Runge-Kutta</strong> method for <strong>IVP</strong>:</p>

<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>( x) }</a>
<a class="sourceLine" id="cb171-2" data-line-number="2">f_forward &lt;-<span class="st"> </span><span class="cf">function</span>(xk, yk, tk) { xk }</a>
<a class="sourceLine" id="cb171-3" data-line-number="3">g_forward &lt;-<span class="st"> </span><span class="cf">function</span>(xk, yk, tk) { yk }</a>
<a class="sourceLine" id="cb171-4" data-line-number="4">f_rungekutta &lt;-<span class="st"> </span><span class="cf">function</span>(f, xk, yk, tk, stepsize) {</a>
<a class="sourceLine" id="cb171-5" data-line-number="5">    k1 =<span class="st"> </span><span class="kw">f</span>(xk, yk, tk)  </a>
<a class="sourceLine" id="cb171-6" data-line-number="6">    k2 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1, </a>
<a class="sourceLine" id="cb171-7" data-line-number="7">           tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )  </a>
<a class="sourceLine" id="cb171-8" data-line-number="8">    k3 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2, </a>
<a class="sourceLine" id="cb171-9" data-line-number="9">           tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb171-10" data-line-number="10">    k4 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3, tk <span class="op">+</span><span class="st"> </span>stepsize)  </a>
<a class="sourceLine" id="cb171-11" data-line-number="11">    yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>( k1 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k3 <span class="op">+</span><span class="st"> </span>k4)</a>
<a class="sourceLine" id="cb171-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb171-13" data-line-number="13">bvp_method &lt;-<span class="st"> </span><span class="cf">function</span>(init_val, fun, iteration, target, </a>
<a class="sourceLine" id="cb171-14" data-line-number="14">                       <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>) {</a>
<a class="sourceLine" id="cb171-15" data-line-number="15">   t0 =<span class="st"> </span>init_val[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb171-16" data-line-number="16">   y0 =<span class="st"> </span>yk =<span class="st"> </span>init_val[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb171-17" data-line-number="17">   x0 =<span class="st"> </span>xk =<span class="st"> </span>guess =<span class="st"> </span>init_val[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb171-18" data-line-number="18">   k =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, target, <span class="dt">length.out=</span>iteration )</a>
<a class="sourceLine" id="cb171-19" data-line-number="19">   stepsize =<span class="st"> </span>k[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>k[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb171-20" data-line-number="20">   sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>)</a>
<a class="sourceLine" id="cb171-21" data-line-number="21">   cnt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb171-22" data-line-number="22">   <span class="cf">for</span> (tk <span class="cf">in</span> k) {</a>
<a class="sourceLine" id="cb171-23" data-line-number="23">       <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;shooting:euler&quot;</span>) {</a>
<a class="sourceLine" id="cb171-24" data-line-number="24">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">f_forward</span>(x0, y0, tk) <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb171-25" data-line-number="25">           xk =<span class="st"> </span>xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">g_forward</span>(x0, y0, tk) <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb171-26" data-line-number="26">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;shooting:rkutta&quot;</span>) {</a>
<a class="sourceLine" id="cb171-27" data-line-number="27">           yk =<span class="st"> </span><span class="kw">f_rungekutta</span>(f_forward, x0, y0, tk, stepsize)  </a>
<a class="sourceLine" id="cb171-28" data-line-number="28">           xk =<span class="st"> </span><span class="kw">f_rungekutta</span>(g_forward, x0, y0, tk, stepsize)</a>
<a class="sourceLine" id="cb171-29" data-line-number="29">       }  </a>
<a class="sourceLine" id="cb171-30" data-line-number="30">       true_y =<span class="st"> </span><span class="kw">exp</span>(tk )  </a>
<a class="sourceLine" id="cb171-31" data-line-number="31">       err =<span class="st"> </span><span class="kw">abs</span>( true_y <span class="op">-</span><span class="st"> </span>y0 )</a>
<a class="sourceLine" id="cb171-32" data-line-number="32">       cnt =<span class="st"> </span>cnt <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb171-33" data-line-number="33">       sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(cnt, stepsize, tk,  y0, x0, </a>
<a class="sourceLine" id="cb171-34" data-line-number="34">                                    true_y, err))</a>
<a class="sourceLine" id="cb171-35" data-line-number="35">       y0 =<span class="st"> </span>yk</a>
<a class="sourceLine" id="cb171-36" data-line-number="36">       x0 =<span class="st"> </span>xk</a>
<a class="sourceLine" id="cb171-37" data-line-number="37">   }</a>
<a class="sourceLine" id="cb171-38" data-line-number="38">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;iteration&quot;</span>, <span class="st">&quot;stepsize&quot;</span>, <span class="st">&quot;tk&quot;</span>,</a>
<a class="sourceLine" id="cb171-39" data-line-number="39">                          <span class="st">&quot;yk&quot;</span>, <span class="st">&quot;xk&quot;</span>, <span class="st">&quot;y-exact&quot;</span>, <span class="st">&quot;absolute error&quot;</span>)</a>
<a class="sourceLine" id="cb171-40" data-line-number="40">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=<span class="st"> </span>sequence, <span class="st">&quot;count&quot;</span>=<span class="kw">nrow</span>(sequence), </a>
<a class="sourceLine" id="cb171-41" data-line-number="41">        <span class="st">&quot;type&quot;</span>=type, <span class="st">&quot;guess&quot;</span>=guess )</a>
<a class="sourceLine" id="cb171-42" data-line-number="42">}</a>
<a class="sourceLine" id="cb171-43" data-line-number="43">bvp =<span class="st"> </span><span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb171-44" data-line-number="44">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>)</a>
<a class="sourceLine" id="cb171-45" data-line-number="45">bvp</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.100000 1.000000     0.00000000
## [2,]         2      0.2 0.2 1.243540 1.221400 1.221403     0.02213724
## [3,]         3      0.2 0.4 1.513958 1.518860 1.491825     0.02213326
## [4,]         4      0.2 0.6 1.850234 1.849148 1.822119     0.02811471
## [5,]         5      0.2 0.8 2.259635 2.259875 2.225541     0.03409400
## [6,]         6      0.2 1.0 2.759971 2.759918 2.718282     0.04168948
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1.1</code></pre>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1">bvp =<span class="st"> </span><span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.3</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb173-2" data-line-number="2">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>)</a>
<a class="sourceLine" id="cb173-3" data-line-number="3">bvp</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.300000 1.000000     0.00000000
## [2,]         2      0.2 0.2 1.287820 1.221400 1.221403     0.06641724
## [3,]         3      0.2 0.4 1.558238 1.572943 1.491825     0.06641326
## [4,]         4      0.2 0.6 1.906488 1.903232 1.822119     0.08436882
## [5,]         5      0.2 0.8 2.327863 2.328584 2.225541     0.10232222
## [6,]         6      0.2 1.0 2.843412 2.843252 2.718282     0.12512981
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1.3</code></pre>

<p>Instead of going for a third shot, let us perform <strong>linear interpolation</strong> using the results of the two attempts of <strong>shooting</strong> for the target (where superscripts indicate 1st and 2nd attempts) :</p>
<p><span class="math display">\[\begin{align}
y_0&#39;(0) &amp;= x_0^{(1)} +  \frac { x_0^{(2)} - x_0^{(1)}}{y_0^{(2)} - y_0^{(1)}}( y - y_0^{(1)}) \\
y_0&#39;(0) &amp;= 1.10 + \frac{1.30 - 1.10}{2.843412 - 2.759971} (2.718282 - 2.759971) \nonumber \\
y_0&#39;(0) &amp;= 1.00 \nonumber
\end{align}\]</span></p>
<p>where <span class="math inline">\(y = f(1) = 2.718282\)</span> which is our exact solution.</p>
<p>We now go back and attempt shooting another arrow given a guess of <span class="math inline">\(y&#39;(0) = 1.00\)</span>. We use the interpolation result to become our next 3rd attempt to guess the target.</p>
<p><span class="math display">\[
y_0&#39;(0) = x_0^{(3)} = 1.00
\]</span></p>

<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1">(<span class="dt">bvp =</span> <span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.0</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb175-2" data-line-number="2">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.000000 1.000000   0.000000e+00
## [2,]         2      0.2 0.2 1.221400 1.221400 1.221403   2.758160e-06
## [3,]         3      0.2 0.4 1.491818 1.491818 1.491825   6.737641e-06
## [4,]         4      0.2 0.6 1.822106 1.822106 1.822119   1.234405e-05
## [5,]         5      0.2 0.8 2.225521 2.225521 2.225541   2.010271e-05
## [6,]         6      0.2 1.0 2.718251 2.718251 2.718282   3.069185e-05
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1</code></pre>

<p>The solution we get out of our <strong>interpolated</strong> guess, <span class="math inline">\(x_0^{(3)} = 1.0\)</span>, ends up with the following:</p>
<p><span class="math display">\[
y_0 = 1.000,\ \ \ \
y_1 = 1.221,\ \ \ \
y_2 = 1.492,\ \ \ \
y_3 = 1.822,\ \ \ \
y_4 = 2.226,\ \ \ \
y_5 = 2.718
\]</span></p>
<p>If we graph the trajectory, we get the Figure <a href="numericalcalculus.html#fig:bvptrajectory">4.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bvptrajectory"></span>
<img src="embed0035.png" alt="BVP Trajectory For ODE" width="70%" />
<p class="caption">
Figure 4.18: BVP Trajectory For ODE
</p>
</div>
<p>Note that the unique solution we seek is a set of <span class="math inline">\(\mathbf{y_k} = f(x_k)\)</span> with their corresponding <strong>slopes</strong>, <span class="math inline">\(\mathbf{x_k}\)</span>, forming a trajectory that lands close to the target, <span class="math inline">\(y(1) = 2.718\)</span>. The next method we cover, <strong>Finite Difference</strong>, illustrates how we solve for Systems of Linear Equations to derive the solution for our BVP.</p>
</div>
<div id="finite-difference-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.6</span> Finite Difference Method  <a href="numericalcalculus.html#finite-difference-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we solve <strong>BVPs</strong> using <strong>Finite Difference</strong>. Recall <strong>second-order Centered Finite Difference</strong> formula:</p>
<p><span class="math display">\[\begin{align}
f&#39;&#39;(x)\ \approx \frac{f(x + h) - 2f(x) + f(x -h) }{h^2}\ \ \ \rightarrow\ \ \ \
y&#39;&#39; \approx \frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2}
\end{align}\]</span></p>
<p>Recall also <strong>first-order Forward Finite Difference</strong> formula:</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) \approx \frac{f(x+h) - f(x)}{h} \ \ \ \rightarrow\ \ \ \
y&#39; \approx \frac{ y_{k+1}  -y_k}{\Delta t}
\end{align}\]</span></p>
<p>To illustrate the use of those formulas, suppose we have the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_0(1) = 2.718, \ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span>
and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Here, because <span class="math inline">\(y&#39;&#39; = f&#39;&#39;(x)\)</span>, we simply substitute:</p>
<p><span class="math display">\[\begin{align}
y&#39;&#39; = y\ \ \ \rightarrow\ \ \ \ \ \frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2} = y_k
\end{align}\]</span></p>
<p>Note that we are not substituting <span class="math inline">\(y&#39;\)</span> for anything because our problem statement does not include the first derivative (e.g., <span class="math inline">\(y&#39;&#39; = y\)</span> ).</p>
<p>Now, let us simplify:</p>

<p><span class="math display">\[\begin{align}
\frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2} {}&amp;= y_k\\
\nonumber \\
y_{k+1} - 2y_k + y_{k-1} &amp;= y_k(\Delta t)^2 \\
y_{k+1} - 2y_k - y_k(\Delta t)^2  + y_{k-1} &amp;= 0\\
y_{k+1} - (2 + (\Delta t)^2)y_k  + y_{k-1} &amp;= 0\\
y_{k-1} - (2 + (\Delta t)^2)y_k  + y_{k+1} &amp;= 0
\end{align}\]</span>
</p>
<p>We now have an equation we can use to solve our <strong>BVP</strong> (note that this is a discretization of the <strong>domain</strong> of our general function, <span class="math inline">\(\mathbf{f(x)}\)</span>):</p>
<p><span class="math display">\[\begin{align}
y_{k-1} - (2 + (\Delta t)^2)y_k  + y_{k+1} = 0
\end{align}\]</span></p>
<p>To be able to use the equation, let us review Figure <a href="numericalcalculus.html#fig:finitediffode">4.19</a> which illustrates our problem statement; where our <strong>Dirichlet</strong> boundary is <span class="math inline">\(t \in [a,b]\)</span>. Notice that our general function, <span class="math inline">\(f(x)\)</span>, is discretized into nodes of individual functions, <span class="math inline">\(f(x) \in \{f(0), f(0.2), ...,f(0.8), f(1)\}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:finitediffode"></span>
<img src="embed0036.png" alt="Discretized Nodes For ODE" width="70%" />
<p class="caption">
Figure 4.19: Discretized Nodes For ODE
</p>
</div>
<p>In the figure, we see six nodes of functions that we now can use to form our system of equations:</p>

<p><span class="math display">\[\begin{align}
f(0.0)\ \ \ &amp;\rightarrow\ \ \ \ y_0 = 1.0\ \ \leftarrow boundary\ a\\ 
f(0.2)\ \ \ &amp;\rightarrow\ \ \ \ y_0 - (2 + (\Delta t)^2)y_1  + y_2 = 0 \\ 
f(0.4)\ \ \ &amp;\rightarrow\ \ \ \ y_1 - (2 + (\Delta t)^2)y_2  + y_3 = 0 \\ 
f(0.6)\ \ \ &amp;\rightarrow\ \ \ \ y_2 - (2 + (\Delta t)^2)y_3  + y_4 = 0 \\ 
f(0.8)\ \ \ &amp;\rightarrow\ \ \ \ y_3 - (2 + (\Delta t)^2)y_4  + y_5 = 0 \\ 
f(1.0)\ \ \ &amp;\rightarrow\ \ \ \ y_5 = 2.718\ \ \leftarrow boundary\ b 
\end{align}\]</span>
</p>
<p>and then to translate into <strong>tridiagonal</strong> matrix form where:</p>
<p><span class="math display">\[
-(2 + (\Delta t)^2) = -2.04.
\]</span>
We have:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrrr}
1 &amp; . &amp; . &amp; . &amp; . &amp; . \\
1 &amp; -2.04 &amp; 1 &amp; . &amp; . &amp; . \\
. &amp; 1 &amp; -2.04  &amp; 1 &amp; . &amp; . \\
. &amp; . &amp; 1 &amp; -2.04  &amp; 1 &amp; . \\
. &amp; . &amp; . &amp; 1 &amp; -2.04  &amp; 1  \\
. &amp; . &amp; . &amp; . &amp; . &amp; 1 \\
\end{array}
\right]
\left[\begin{array}{r}y_0 \\ y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5  \end{array}\right] =
\left[\begin{array}{r}1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 2.718  \end{array}\right]
\]</span>
</p>
<p>We can then use any numerical methods (e.g., Newton, Broyden, etc.) discussed in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) to solve for Systems of Linear Equations, forming a domain with the following set of <strong>y</strong> values.</p>
<p><span class="math display">\[
y_0 = 1.000,\ \ \ \
y_1 = 1.222,\ \ \ \
y_2 = 1.492,\ \ \ \
y_3 = 1.823,\ \ \ \
y_4 = 2.226,\ \ \ \
y_5 = 2.718
\]</span>
And if we graph the trajectory of the approximate solution, it will show a geometric match for the <strong>Shooting method</strong> shown in Figure <a href="numericalcalculus.html#fig:bvptrajectory">4.18</a>. Note that the figure shows the trajectory of our exact solution itself, e.g. <span class="math inline">\(\mathbf{y = e^x}\)</span>.</p>
</div>
<div id="finite-element-method-based-on-wrm-and-vm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.7</span> Finite Element Method (based on WRM and VM) <a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Finite Element</strong> method has been used across many fields, from mechanical and aerospace engineering to material science. It is also used in structural (solid) mechanics, fluid (liquid and gas) mechanics, quantum mechanics, and other fields of physics. Computer simulations assist in <strong>Finite Element analysis</strong> to understand such dynamic systems <span class="citation">(Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>. We leave readers to investigate a few simulation software starting with Ansys, OpenFOAM, and SimScale.</p>
<p>In structural engineering, construction materials are gauged based on strain when stress is applied or displacement when force is applied. Similar to <strong>piecewise</strong> polynomial interpolation, a well-known approach to determine the overall state of the entire material is to first partition the material into pieces (or elements) and compute the local state of each element; hence, finite element. And then aggregate the individual (local) states to form the overall (global) state. There are two common ways to partition the material: rectangular or triangular. See Figure <a href="numericalcalculus.html#fig:finiteelement">4.20</a>. Note that the figure shows the most rudimentary way of generating the mesh. <strong>Mesh generation</strong> can be done precisely by computers today using graphics software following <strong>Delaunay Refinement algorithm</strong> for <strong>Triangular Mesh</strong> and adjusting vertices following <strong>Ruppertâs algorithm</strong>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:finiteelement"></span>
<img src="finiteelement.png" alt="Finite Element Mesh" width="70%" />
<p class="caption">
Figure 4.20: Finite Element Mesh
</p>
</div>
<p>In this section, instead of discussing complex structures involving constructing a complex mesh of elements, let us first get the intuition of FEM using a simple one-dimension two-point <strong>BVP</strong>. Then, we use a <strong>stencil</strong> to show that. Discussion about complex structures is left to readers pursuing structural or mechanical engineering or material science.</p>
<p>Moreover, in other literature, <strong>FEM</strong> offers two approaches, namely <strong>Weighted Residual Method (WRM)</strong> and <strong>Variational Method (VM)</strong>. In this section, we only focus on <strong>WRM</strong>.</p>
<p>Similar to <strong>Finite Difference Methods (FDM)</strong>, to solve <strong>BVPs</strong> using <strong>Finite Element methods (FEM)</strong>, we discretize the <strong>domain</strong> of the function into equally-spaced intervals forming a set of discretized nodes - each node representing a function. The difference, however, is that in <strong>FEM</strong>, we build a linear combination (as an approximate solution) constructed from a standard series or a polynomial of choice associated with a set of discretized nodes. These nodes are considered <strong>basis functions</strong>. These <strong>basis functions</strong> denoted as <span class="math inline">\(\phi(t)\)</span> form an approximation to the solution. The <strong>approximate solution</strong> is a <strong>weighted solution</strong> using the following choice of polynomials or series for its <strong>basis functions</strong>: </p>
<ul>
<li>Polynomial Interpolation (e.g.Â Lagrange, Newton, B-spline, Legendre, Chebyshev, â¦)</li>
<li>Fourier Series (e.g.Â Sinusoidal, â¦)</li>
<li>Wavelets (e.g.Â Harmonic, â¦)</li>
</ul>
<p>In here, we use the generalized <strong>Taylor series</strong> for our trial function, <span class="math inline">\(y(x)\)</span>. </p>
<p><span class="math display">\[\begin{align}
y(x) = \sum_{n=0}^{\infty} c_n x^n =  c_0 + c_1x + c_2 x^2 + c_3 x^3 +\ . . . \ \ \ \ where\ c_n = \frac{f(n)(a)}{n!}
\end{align}\]</span></p>
<p>One may prefer to use <strong>Sinusoidal series</strong> instead (depending on study or use):</p>
<p><span class="math display">\[\begin{align}
y(x) = \sum_{n=1}^{\infty} c_n sin\frac{n\pi x}{L} = c_1 sin \frac{\pi x}{L} + c_2 sin \frac{2\pi x}{L} + c_3 sin \frac{3\pi x}{L} +\ . . .
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(c_i\)</span> are the <strong>unknown coefficients</strong> that we need to compute. From physical connotation, the number of unknown coefficients is based on the characteristic of the material. In other words, the number of properties of each element determines the order of the polynomial we use. For example, we may account for stress, strain, and displacement in solid dynamics. In this case, we may need a <strong>cubic polynomial</strong> for each element of the elastic material. And in fluid dynamics, we may account for viscosity, temperature, density, pressure, and velocity.</p>
<p>As for simple illustrations, we narrow down our case to a one-dimension two-point <strong>BVP</strong> covering a few <strong>Weighted Residual</strong> approaches under <strong>Finite Element</strong> methods and using the usual problem statement below:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_0(1) = 2.718, \ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span></p>
<p>and just as the same, we use the below exact solution as a baseline to validate our choice of trial function - this is our <strong>governing equation</strong>:</p>
<p><span class="math display">\[
y = e^t\ \ \ \ \ \leftarrow \text{exact function as our baseline}
\]</span>
Note that <strong>governing equations</strong> are used for the more practical cases for <strong>Finite Element analysis</strong>. One <strong>governing equation</strong> is the <strong>Navier-Stokes equation</strong> for <strong>Fluid Dynamics</strong>. We base this equation on the law of conservation (e.g., mass, momentum, energy) around the physical properties of fluids such as viscosity, density, pressure, temperature, and velocity. </p>
<p>In our case, we use the most simple <strong>governing equation</strong> to illustrate the fundamental idea of the <strong>Finite Element method</strong> and may not necessarily reflect any physical phenomenon other than to show our approximate geometric representation of the actual equation:</p>
<p><span class="math display">\[
y = e^t
\]</span></p>
<p>Our first step is to generate a set of <strong>trial functions</strong> using the Taylor series as our choice. Given the <strong>Dirichlet</strong> boundary condition above, it shows that we have six nodes based on <span class="math inline">\(\Delta t = 0.2\)</span>, which is just our way of illustrating the partition of a <strong>global</strong> domain (the geometric curve) into five sub-domains (or elements). Here is the <strong>stencil</strong> for that which is shown in Figure <a href="numericalcalculus.html#fig:trialfunction">4.21</a>: </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trialfunction"></span>
<img src="trialfunction.png" alt="Stencil for Trial Function" width="70%" />
<p class="caption">
Figure 4.21: Stencil for Trial Function
</p>
</div>
<p>For our <strong>trial function</strong>, we use a simple <strong>cubic polynomial</strong> - a truncated taylor series - for each element:</p>
<p><span class="math display">\[\begin{align}
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\end{align}\]</span></p>
<p>One may choose other <strong>polynomials</strong> as <strong>basis (trial) function</strong> as long as it satisfies three conditions:</p>
<ul>
<li>it satisfies the essential boundary conditions</li>
<li>it is continuous</li>
<li>and as such, if it is continuous, it is differentiable in which the square of its derivative is integrable and bounded.</li>
</ul>
<p>One point to make is that the <strong>basis function</strong> is regarded as a <strong>weighted function</strong>. In variational formulation, where we require to solve for the <strong>roots</strong> of the equation (the properties of the elements), we relax the third condition above, which means that we do not require a second-order derivation of continuity - which makes it, therefore, a <strong>weak formulation</strong>. Otherwise, the <strong>weighted function</strong> requires it if we do not reduce (meaning, we weaken the requirement of) the second-order of continuity (of the approximate solution) using integration (but not by parts) and thus is considered a <strong>strong formulation</strong>. </p>
<p>In the next few sections, we focus on <strong>Weighted Residual methods (WRM)</strong> to solve for <strong>Bounded Value Problems</strong>. We start with the <strong>Least-Square Method</strong>.</p>
</div>
<div id="least-square-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.8</span> Least-Square Method (using WRM)<a href="numericalcalculus.html#least-square-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us introduce a <strong>WRM</strong> under <strong>FEM</strong> called the <strong>Least-Square</strong> method. Foremost, note that <strong>WRM</strong> is an integral approach that integrates <strong>weighted residuals</strong> of <strong>trial functions</strong> to form an approximation function which is a weaker formulation of the actual function <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>.</p>
<p>To illustrate, let us form a system of equations starting with the <strong>basis function</strong>, which we derived from <strong>Taylor Series</strong> as pointed out previously:</p>
<p><span class="math display">\[\begin{align}
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\end{align}\]</span></p>
<p>Our first goal is to find the values of the <strong>unknown coefficients</strong> that minimize <span class="math inline">\(y(x)\)</span>, which we require to fulfill our ultimate goal of formulating the approximation function that we can use to determine the values of each element in the <strong>domain</strong> - the curve.</p>
<p><strong>First</strong>, for the first boundary, <strong>a</strong>, where <span class="math inline">\(\mathbf{y(0) = 1}\)</span>, we compute for <span class="math inline">\(\mathbf{c_0}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2 + c_3 x^3 \\
y(0) &amp;= c_0 + c_1 (0) + c_2 (0)^2 + c_3 (0)^3 \\
y(0) &amp;= c_0  \\
\therefore c_0 &amp;= 1
\end{align*}\]</span></p>
<p><strong>Second</strong>, for the second boundary, <strong>b</strong>, where <span class="math inline">\(\mathbf{y(1) = 2.718}\)</span>, we compute for <span class="math inline">\(\mathbf{c_1}\)</span> (note that we also substitute <span class="math inline">\(c_0\)</span> in the equation):</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2  + c_3 x^3\\
y(1) &amp;= 1 + c_1 (1) + c_2 (1)^2 + c_3 (1)^3\\
y(1) &amp;= 1 + c_1  + c_2  + c_3 \\
c_1 &amp;= y(1) - 1 - c_2 -  c_3  \\
c_1 &amp;= 2.718 - 1 - c_2 - c_3   \\
\therefore c_1 &amp;= 1.718 - c_2 - c_3
\end{align*}\]</span></p>
<p><strong>Third</strong>, let us simplify our <strong>trial function</strong>:</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2 + c_3 x^3 \\
y(x) &amp;= 1 + (1.718 - c_2 - c_3)x + c_2 x^2 + c_3 x^3  \\
y(x) &amp;= 1 + 1.718x - c_2x - c_3x + c_2 x^2 + c_3 x^3 \\
y(x) &amp;= 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) \\
\end{align*}\]</span></p>
<p><strong>Fourth</strong>, let us get the first-order and-second order of the simplified <strong>trial function</strong>:</p>
<p><span class="math display">\[\begin{align*}
y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\
y&#39;&#39;(x) &amp;= 2c_2 + 6c_3x  \\
\end{align*}\]</span></p>
<p><strong>Fifth</strong>, let us now plug our derivatives and trial functions into our problem statement to get the <strong>Residual</strong> equation:</p>
<p><span class="math display">\[\begin{align*}
y&#39;&#39; = y\ \ \ \rightarrow\ \ \ \ \ \ \ R = y&#39;&#39; - y {}&amp;= 0\\
\\
2c_2 + 6c_3x  - ( 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x)) &amp;= 0 \\
2c_2 + 6c_3x   - 1 - 1.718x - c_2(x^2 - x) - c_3(x^3 - x)  &amp;= 0 \\
2c_2 + 6c_3x   - 1 -  1.718x - c_2 x^2 + c_2 x  - c_3x^3 + c_3 x &amp;=  0 \\
c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1 &amp; = 0 \\
\\
\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1  &amp;= 0
\end{align*}\]</span></p>
<p><strong>Sixth</strong>, given the following integral form of our weighted residual, let us compute for other equations (this is the core of <strong>WRM</strong>):</p>
<p><span class="math display">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = 2 + x - x^2 \\
W_2 = 6x - x^3 + x
\end{cases} \label{eqn:eqnnumber12}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (2 + x - x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
4.7c_2+7.05c_3-4.0278 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (6x - x^3 + x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
7.050c_2+13.676 c_3-6.9150 = 0
\]</span></p>
<p>Based on the four equations we generated, we form our <strong>matrix</strong> of system of equations:</p>

<p><span class="math display">\[
\begin{array}{r}
c_0 = 1.0000\\
4.7c_2+7.05c_3 = 4.0278 \\
7.05c_2+13.676 c_3 = 6.9150 \\
c_1 + c_2 + c_3 = 1.7182
\end{array}\ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; . &amp; 4.700 &amp; 7.050 \\
. &amp; . &amp; 7.050 &amp; 13.676 \\
. &amp; 1 &amp; 1 &amp; 1 
\end{array}
\right]
\left[\begin{array}{r} c_0 \\ c_1 \\ c_2 \\ c_3 \end{array}\right] = 
\left[\begin{array}{r} 1.0000 \\ 4.0278 \\ 6.9150 \\ 1.7182 \end{array}\right]
\]</span>
</p>
<p>Therefore, our unknown coefficients are:</p>
<p><span class="math display">\[
c_0 = 1\ \ \ \ \ c_1 = 1.002031 \ \ \ \ \ c_2 = 0.4345505  \ \ \ \ \ c_3 = 0.2816188 
\]</span></p>
<p>Giving us the <strong>weak formulation</strong> of our <strong>approximate solution</strong> ( note that we chose <strong>cubic polynomial</strong>):</p>
<p><span class="math display">\[
y(x) = 1 + 1.002031 x + 0.4345505  x^2 + 0.2816188 x^3
\]</span></p>
<p>With that, we get a geometric match of the exact solution, <span class="math inline">\(y=e^x\)</span>, in Figure <a href="numericalcalculus.html#fig:fem0">4.22</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fem0"></span>
<img src="embed0037.png" alt="Finite Element Method (WRM)" width="70%" />
<p class="caption">
Figure 4.22: Finite Element Method (WRM)
</p>
</div>
<p>Also, in terms of error, we get the following reasonable result:</p>

<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1">f_exact &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>(x) }</a>
<a class="sourceLine" id="cb177-2" data-line-number="2">f_approx &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.002031</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.4345505</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb177-3" data-line-number="3"><span class="st">                           </span><span class="fl">0.2816188</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> }</a>
<a class="sourceLine" id="cb177-4" data-line-number="4">n =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb177-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb177-6" data-line-number="6">y_exact =<span class="st"> </span><span class="kw">f_exact</span>(x)</a>
<a class="sourceLine" id="cb177-7" data-line-number="7">y_approx =<span class="st"> </span><span class="kw">f_approx</span>(x)</a>
<a class="sourceLine" id="cb177-8" data-line-number="8">err =<span class="st"> </span>y_exact <span class="op">-</span><span class="st"> </span>y_approx</a>
<a class="sourceLine" id="cb177-9" data-line-number="9">m =<span class="st"> </span><span class="kw">cbind</span>( y_exact, <span class="kw">cbind</span>( y_approx, err))</a>
<a class="sourceLine" id="cb177-10" data-line-number="10"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Exact&quot;</span>, <span class="st">&quot;Approximate&quot;</span>, <span class="st">&quot;Error&quot;</span> )</a>
<a class="sourceLine" id="cb177-11" data-line-number="11">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Least-Square Method&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb177-12" data-line-number="12">              <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:femleastsqr">Table 4.13: </span>Least-Square Method</caption>
<thead>
<tr class="header">
<th align="right">Exact</th>
<th align="right">Approximate</th>
<th align="right">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.000000</td>
<td align="right">1.000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="right">1.221403</td>
<td align="right">1.220041</td>
<td align="right">0.0013616</td>
</tr>
<tr class="odd">
<td align="right">1.491825</td>
<td align="right">1.488364</td>
<td align="right">0.0034606</td>
</tr>
<tr class="even">
<td align="right">1.822119</td>
<td align="right">1.818486</td>
<td align="right">0.0036324</td>
</tr>
<tr class="odd">
<td align="right">2.225541</td>
<td align="right">2.223926</td>
<td align="right">0.0016150</td>
</tr>
<tr class="even">
<td align="right">2.718282</td>
<td align="right">2.718200</td>
<td align="right">0.0000815</td>
</tr>
</tbody>
</table>

</div>
<div id="galerkin-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.9</span> Galerkin Method (using WRM)<a href="numericalcalculus.html#galerkin-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us introduce another <strong>WRM</strong> under <strong>FEM</strong> called the <strong>Galerkin</strong> method <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>. Similar to the <strong>Least Square</strong> method, we use the same <strong>trial function</strong> as below (note that we can use other polynomials instead):</p>
<p><span class="math display">\[
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\]</span></p>
<p>with its <strong>simplified form</strong> as illustrated in the <strong>Least-Square</strong> method:</p>
<p><span class="math display">\[
y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) 
\]</span></p>
<p>Now, the <strong>Weighing function</strong> is different in the <strong>Sixth</strong> step in the <strong>Least Square</strong> method for the <strong>Galerkin</strong> method. The difference this time is that we use the <strong>simplified form</strong> to extract our <strong>weighing functions</strong>:</p>
<p>So given the following integral form of our weighted residual, let us compute for other equations:</p>
<p><span class="math display">\[
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x^2 - x \\
W_2 = x^3 - x
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{R}\)</span> remains to be (the same as Least Squares method):</p>
<p><span class="math display">\[\begin{align}
\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1  \label{eqn:eqnnumber13}
\end{align}\]</span></p>
<p>So that, for <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^2 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.3098-0.3667c_2-0.5500c_3 = 0
\]</span></p>
<p>And for <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^3 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.4791-0.5500c_2-0.8762c_3 = 0
\]</span></p>
<p>Based on the four equations we generated, we form our <strong>matrix</strong> of system of equations:</p>

<p><span class="math display">\[
\begin{array}{r}
c_0 = 1.0000\\
0.3667c_2+0.5500c_3 = 0.3098 \\
0.5500c_2+0.8762 c_3 = 0.4791 \\
c_1 + c_2 + c_3 = 1.7182
\end{array}\ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; . &amp; 0.3667 &amp; 0.5500 \\
. &amp; . &amp; 0.5500 &amp; 0.8762 \\
. &amp; 1 &amp; 1 &amp; 1 
\end{array}
\right]
\left[\begin{array}{r} c_0 \\ c_1 \\ c_2 \\ c_3 \end{array}\right] = 
\left[\begin{array}{r} 1.0000 \\ 0.3098 \\ 0.4791 \\ 1.7182 \end{array}\right]
\]</span>
</p>
<p>Therefore, our unknown coefficients are:</p>
<p><span class="math display">\[
c_0 = 1\ \ \ \ \ c_1 = 1.014161 \ \ \ \ \ c_2 = 0.422377 \ \ \ \ \ c_3 = 0.2816625 
\]</span></p>
<p>and our complete approximate solution becomes:</p>
<p><span class="math display">\[
y(x) = 1 + 1.014161x + 0.422377 x^2 + 0.2816625 x^3
\]</span></p>
<p>Also, in terms of error, we get the following reasonable result:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1">f_exact &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>(x) }</a>
<a class="sourceLine" id="cb178-2" data-line-number="2">f_approx &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.014161</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.422377</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb178-3" data-line-number="3"><span class="st">                           </span><span class="fl">0.2816625</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> }</a>
<a class="sourceLine" id="cb178-4" data-line-number="4">n =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb178-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb178-6" data-line-number="6">y_exact =<span class="st"> </span><span class="kw">f_exact</span>(x)</a>
<a class="sourceLine" id="cb178-7" data-line-number="7">y_approx =<span class="st"> </span><span class="kw">f_approx</span>(x)</a>
<a class="sourceLine" id="cb178-8" data-line-number="8">err =<span class="st"> </span>y_exact <span class="op">-</span><span class="st"> </span>y_approx</a>
<a class="sourceLine" id="cb178-9" data-line-number="9">m =<span class="st"> </span><span class="kw">cbind</span>( y_exact, <span class="kw">cbind</span>( y_approx, err))</a>
<a class="sourceLine" id="cb178-10" data-line-number="10"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Exact&quot;</span>, <span class="st">&quot;Approximate&quot;</span>, <span class="st">&quot;Error&quot;</span> )</a>
<a class="sourceLine" id="cb178-11" data-line-number="11">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Galerkin Method&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb178-12" data-line-number="12">              <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:femgalerkin">Table 4.14: </span>Galerkin Method</caption>
<thead>
<tr class="header">
<th align="right">Exact</th>
<th align="right">Approximate</th>
<th align="right">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.000000</td>
<td align="right">1.000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="right">1.221403</td>
<td align="right">1.221981</td>
<td align="right">-0.0005778</td>
</tr>
<tr class="odd">
<td align="right">1.491825</td>
<td align="right">1.491271</td>
<td align="right">0.0005536</td>
</tr>
<tr class="even">
<td align="right">1.822119</td>
<td align="right">1.821391</td>
<td align="right">0.0007274</td>
</tr>
<tr class="odd">
<td align="right">2.225541</td>
<td align="right">2.225861</td>
<td align="right">-0.0003204</td>
</tr>
<tr class="even">
<td align="right">2.718282</td>
<td align="right">2.718201</td>
<td align="right">0.0000813</td>
</tr>
</tbody>
</table>
</div>
<div id="petrov-galerkin-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.10</span> Petrov-Galerkin Method (using WRM)<a href="numericalcalculus.html#petrov-galerkin-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Petrov-Galerkin</strong> method, our <strong>Weighing functions</strong> are such that given the following integral form of our weighted residual, we just compute for the equations using <strong>x</strong> and raise the order until we meet the system of equations we require:</p>
<p><span class="math display">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x \\
W_2 = x^2
\end{cases}\ or
\begin{cases}
W_1 = x^2 \\
W_2 = x^3
\end{cases} \label{eqn:eqnnumber14}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
3.25c_2+6.4c_3-3.218 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.15c_2+4.75c_3-2.2885 = 0
\]</span></p>
<p>Our complete approximate solution becomes:</p>
<p><span class="math display">\[
y(x) = 1 + 1.027872 x + 0.3093443 x^2 + 0.3809836 x^3
\]</span></p>
</div>
<div id="rayleigh-ritz-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.11</span> Rayleigh-Ritz Method (using WRM)<a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, we introduce the <strong>Rayleigh-Ritz</strong> method using <strong>WRM under FEM</strong> <span class="citation">(Papadopoulos P. <a href="bibliography.html#ref-ref491p">2015</a>)</span>. Using the same approach as <strong>Galerkin</strong>, we use the same <strong>trial function</strong> as before:</p>
<p><span class="math display">\[
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\]</span></p>
<p>with its <strong>simplified form</strong> as illustrated in the <strong>Least Square</strong>:</p>
<p><span class="math display">\[
y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) 
\]</span></p>
<p>and with its first-order derivative:</p>
<p><span class="math display">\[\begin{align*}
y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\
\end{align*}\]</span></p>
<p>Now, similar to <strong>Galerkin</strong> method, we use the <strong>simplified form</strong> to extract our <strong>weighing functions</strong>:</p>

<p><span class="math display">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x^2 - x,\ \ \  W&#39;_1 = 2x - 1 \\
W_2 = x^3 - x,\ \ \  W&#39;_2 = 3x^2 - 1 \\
\end{cases} \label{eqn:eqnnumber8}
\end{align}\]</span>
</p>
<p>where this time, we use integration by parts starting with <span class="math inline">\(\mathbf{R}\)</span>:</p>

<p><span class="math display">\[\begin{align}
R = y&#39;&#39; - y = 0\ \ \ \ \rightarrow\ \ \ \ \ R = \frac{d^2y}{dx^2} -  \frac{dy}{dx} = 0
\end{align}\]</span>
</p>
<p>So then, that brings our <strong>integral</strong> equation into the following form:</p>

<p><span class="math display">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0
\ \ \ \ \ \rightarrow \ \ \ \ \ \ 
\int_{i=0}^{n} W_i \left(\frac{d^2y}{dx^2} -  \frac{dy}{dx}\right)\ dx = 0
\end{align}\]</span>
</p>
<p>And by integration by parts:</p>

<p><span class="math display">\[\begin{align}
\int_{i=0}^{n} W_i \left(\frac{d^2y}{dx^2}\right) dx -  \int_{i=0}^{n} W_i \left(\frac{dy}{dx}\right)\ dx = 0
\end{align}\]</span>
</p>
<p>We continue solving for each part:</p>

<p><span class="math display">\[\begin{align}
\left[w\left(\frac{dy}{dx}\right)\right]_0^1  - \int_{0}^{1}  \left(\frac{dw}{dx}\right)\left(\frac{dy}{dx}\right) dx -  \int_{0}^{1} W_i \left(\frac{dy}{dx}\right)\ dx = 0
\end{align}\]</span>
</p>
<p>giving us the final integral form:</p>

<p><span class="math display">\[
- \int_{0}^{1}  \left(\frac{dw}{dx}\right)\left(\frac{dy}{dx}\right) dx -  \int_{0}^{1} W_i \left(\frac{dy}{dx}\right)\ dx = 0, \ \ \ \ \ \ \ \ where\ \ \ \left[w\left(\frac{dy}{dx}\right)\right]_0^1 is\ cancelled
\]</span>
</p>
<p>So that, for <span class="math inline">\(\mathbf{W_1}\)</span> and <span class="math inline">\(\mathbf{W_1&#39;}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
{}&amp;- \int_{0}^{1} (2x-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\
&amp;\ \ \ - \int_{0}^{1} (x^2-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0
\end{align*}\]</span>
</p>
<p>we get the following equation:</p>
<p><span class="math display">\[
-0.3333 c_2-0.5167 c_3+0.2863 = 0
\]</span></p>
<p>And for <span class="math inline">\(\mathbf{W_2}\)</span> and <span class="math inline">\(\mathbf{W_2&#39;}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
{}&amp;- \int_{0}^{1} (3x^2-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\
&amp;\ \ \ - \int_{0}^{1} (x^3-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0
\end{align*}\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.4295-0.48333 c_2-0.8c_3 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 0.835589 x + 0.8733474 x^2 + 0.0092640 x^3
\]</span></p>
</div>
<div id="subdomain-method-using-subdomains" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.12</span> Subdomain Method (using subdomains)<a href="numericalcalculus.html#subdomain-method-using-subdomains" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Subdomain Method</strong> does not require the <strong>Weighing function</strong>; however, it uses integration to subdivide the <strong>Residual</strong> equation (See Equation <span class="math inline">\(\ref{eqn:eqnnumber13}\)</span>) into subdomains, generating the required equations <span class="citation">(Papadopoulos P. <a href="bibliography.html#ref-ref491p">2015</a>; Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>)</span>. One may use as many subdomains as there are to fit a domain; however, because we only need a number of equations sufficient enough to form a system of equations, we only need two more equations in our case.</p>
<p><span class="math display">\[\begin{align}
\int_a^b  R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
S_1 \rightarrow a = 0.0, b = 0.5 \\
S_2 \rightarrow a = 0.5, b = 1.0
\end{cases} \label{eqn:eqnnumber9}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{S_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^{0.5}  ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
1.08334c_2+0.859375c_3-0.71475 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{S_2}\)</span>:</p>
<p><span class="math display">\[
\int_{0.5}^{1}  ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
1.08333c_2+2.390625c_3-1.14425 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 1.000445 x + 0.4372653 x^2 + 0.2804898 x^3
\]</span></p>
</div>
<div id="collocation-method-using-direct-location-points" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.13</span> Collocation Method (using direct location points) <a href="numericalcalculus.html#collocation-method-using-direct-location-points" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Collocation Method</strong> does not require the <strong>Weighing function</strong> and the integration <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>. Instead, we plug the location directly into the <strong>Residual equation</strong>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - x_i^2 + x_i) + c_3 ( 6x_i - x_i^3 + x_i) - 1.718x_i - 1  = 0\ \ \ \ \ where\ \ \
x \in \{ 0.2, 0.4, 0.6, 0.8 \}
\]</span></p>
<p>Note that we need only two equations to complete the <strong>system of equations</strong> for the matrix and so we can just randomly choose two from the set of x. Here, we choose <span class="math inline">\(x \in \{0.2, .06\}\)</span>:</p>
<p>For <span class="math inline">\(\mathbf{x_i = 0.2}\)</span>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - (0.2)^2 + (0.2)) + c_3 ( 6(0.2) - (0.2)^3 + (0.2)) - 1.718(0.2) - 1 = 0 
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.16c_2+1.392c_3-1.3436 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{x_i = 0.6}\)</span>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - (0.6)^2 + (0.6)) + c_3 ( 6(0.6) - (0.6)^3 + (0.6)) - 1.718(0.6) - 1 = 0 
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.24c_2+3.984c_3-2.0308 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 1.006949 x + 0.4603359 x^2 + 0.2509156 x^3
\]</span></p>
</div>
<div id="weighted-residual-summary" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.14</span> Weighted Residual Summary <a href="numericalcalculus.html#weighted-residual-summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In summary, comparing the different <strong>Weighted Residual methods</strong> vs. <strong>Subdomain</strong> and <strong>Collocation</strong>, we get the following:</p>
<table>
<caption><span id="tab:femtable">Table 4.15: </span>Weighted Residual</caption>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="left"><span class="math inline">\(c_0\)</span></th>
<th align="left"><span class="math inline">\(c_1\)</span></th>
<th align="left"><span class="math inline">\(c_2\)</span></th>
<th align="left"><span class="math inline">\(c_3\)</span></th>
<th align="left">y</th>
<th align="left">abs error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Least Square</td>
<td align="left">1</td>
<td align="left">1.002031</td>
<td align="left">0.4345505</td>
<td align="left">0.2816188</td>
<td align="left">2.7182</td>
<td align="left">3e-07</td>
</tr>
<tr class="even">
<td align="left">Galerkin</td>
<td align="left">1</td>
<td align="left">1.014161</td>
<td align="left">0.4223770</td>
<td align="left">0.2816625</td>
<td align="left">2.7182</td>
<td align="left">5e-07</td>
</tr>
<tr class="odd">
<td align="left">Petrov-Galerkin</td>
<td align="left">1</td>
<td align="left">1.027872</td>
<td align="left">0.3093443</td>
<td align="left">0.3809836</td>
<td align="left">2.7182</td>
<td align="left">1e-07</td>
</tr>
<tr class="even">
<td align="left">Rayleigh-Ritz</td>
<td align="left">1</td>
<td align="left">0.835589</td>
<td align="left">0.8733474</td>
<td align="left">0.0092640</td>
<td align="left">2.7182</td>
<td align="left">4e-07</td>
</tr>
<tr class="odd">
<td align="left">Subdomain</td>
<td align="left">1</td>
<td align="left">1.000445</td>
<td align="left">0.4372653</td>
<td align="left">0.2804898</td>
<td align="left">2.7182</td>
<td align="left">1e-07</td>
</tr>
<tr class="even">
<td align="left">Collocation</td>
<td align="left">1</td>
<td align="left">1.006949</td>
<td align="left">0.4603359</td>
<td align="left">0.2509156</td>
<td align="left">2.7182</td>
<td align="left">5e-07</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="approximation-using-functional-differential-equations" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.5</span> Approximation using Functional Differential Equations <a href="numericalcalculus.html#approximation-using-functional-differential-equations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we cover <strong>Partial Differential Equations</strong>, it helps introduce the concept of <strong>Calculus of Variations</strong>. We start with a discussion around <strong>Variational functions</strong>.</p>
<div id="variational-functions" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.5.1</span> Variational Functions <a href="numericalcalculus.html#variational-functions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to operate on functions instead of variables. Suppose we have the following function that accepts two variables: <strong>x = 5</strong> and <strong>y = 5</strong>.</p>
<p><span class="math display">\[\begin{align}
f(x, y) = x + y = 10
\end{align}\]</span></p>
<p>The function gives us a fixed outcome of 10.</p>
<p>Depending on the problem statement, we can use a <strong>Functional</strong> denoted by <span class="math inline">\(\mathbf{F}[f(.)] \equiv \mathbf{F}[u(.)]\)</span> that accepts an arbitrary function (or a set of arbitrary functions), namely <strong>u</strong>, instead of a variable (or set of variables). See the example below:</p>

<p><span class="math display">\[
F\left[u(x,y)\right] =
\begin{cases}
10 &amp; u(x,y) = x + y \\
20 &amp; u(x,y) = 2(x + y) \\
50 &amp; u(x,y) = x^2 + y^2 \\
100 &amp; u(x,y) = (x + y)^2 \\
\end{cases}\ \ \ \
F\left[u&#39;(x,y)\right]  =
\begin{cases}
1 &amp; u&#39;(x,y) = 1  \\
2 &amp; u&#39;(x,y) = 2 \\
10 &amp; u&#39;(x,y) = 2x  \\
20 &amp; u&#39;(x,y) = 2(x + y) \\
\end{cases}
\]</span>
</p>
<p>where:</p>
<p><span class="math display">\[\begin{align*}
{}&amp;F(u)\ \rightarrow \text{(passing function u)}\\
&amp;F(u&#39;)\ \rightarrow \text{(passing 1st derivative of function u)}\\
&amp;F(u&#39;&#39;)\ \rightarrow \text{(passing 2nd derivative of function u)}
\end{align*}\]</span></p>
<p>One popular equation that calculates for the length of an arc uses a functional like so:</p>
<p><span class="math display">\[
S[u(x)] = \int_a^b \sqrt{1+(u&#39;)^2} dx
\]</span>
For example, find the length of an arc given <span class="math inline">\(u = u(x) = x^{\frac{3}{2}}\)</span> and <span class="math inline">\(u&#39; = u&#39;(x) = \frac{3x^{\frac{1}{2}}}{2}\)</span>. Our equation becomes:</p>
<p><span class="math display">\[
S[u(x)] = \int_0^1 \sqrt{1+\left(\frac{3x^{\frac{1}{2}}}{2}\right)^2} dx 
\]</span>
Here is a sample R code:</p>
<div class="sourceCode" id="cb179"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb179-1" data-line-number="1">u.deriv =<span class="st"> </span><span class="kw">deriv</span>(<span class="kw">expression</span>(x<span class="op">^</span>(<span class="dv">3</span><span class="op">/</span><span class="dv">2</span>)), <span class="st">&quot;x&quot;</span>, <span class="dt">func=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb179-2" data-line-number="2">u &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">u.deriv</span>(x)<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb179-3" data-line-number="3">S.functional &lt;-<span class="st"> </span><span class="cf">function</span>(u) {  <span class="kw">integrate</span>(u, <span class="dv">0</span>, <span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb179-4" data-line-number="4"><span class="kw">S.functional</span>(u)</a></code></pre></div>
<pre><code>## 1.111448 with absolute error &lt; 1.2e-14</code></pre>
<p><strong>Euler-Lagrange Equation</strong> is a popular functional derivative equation. We leave readers to investigate the derivation: </p>
<p><span class="math display">\[\begin{align}
\frac{\partial F}{\partial y} = \frac{d}{dx}\left(\frac{\partial F}{\partial y&#39;}\right)
\end{align}\]</span></p>
</div>
<div id="variational-methods" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.5.2</span> Variational Methods <a href="numericalcalculus.html#variational-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the previous sub-section, we summarize the different <strong>Weighted Residual methods</strong> along with the choice of polynomials as basis functions to solve for <strong>ODE</strong>.</p>
<p><strong>Variational method</strong> is an alternative <strong>FEM</strong> method to solve for <strong>ODE</strong>. In our case, we briefly introduce the concept of <strong>configuration space</strong> using a string such as illustrated in Figure <a href="numericalcalculus.html#fig:fem2">4.23</a>. In the figure, the string is displaced in the shape of half of a <strong>sine</strong> function. We focus on six points in the displaced string. Each point corresponds to functions {f(0), f(0.2), f(0.4), f(0.6), f(0.8), f(1)}. There are three <strong>configurations (or unique paths)</strong> formed by the string. We can assume that the three different displacements or configurations are consecutive at three-time intervals caused by an initial perturbation into the system.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fem2"></span>
<img src="embed0038.png" alt="Finite Element Method (Variational)" width="70%" />
<p class="caption">
Figure 4.23: Finite Element Method (Variational)
</p>
</div>

<p>This book does not cover classical mechanics or even quantum mechanics. However, it also helps investigate different principles and variational equations governing stress levels of bendable or stretchable materials. Tall building structures, cranes, pulleys, and bridges exemplify how those principles and equations are applied.</p>
<p>As shown in Figure <a href="numericalcalculus.html#fig:fem2">4.23</a>, a simple bendable (or stretchable) string already follows three principles:</p>
<ul>
<li><strong>Hamiltonian Principle</strong> is expressed as the sum of kinetic energy (T) and potential energy (V). </li>
</ul>
<p><span class="math display">\[\begin{align}
H = T + V
\end{align}\]</span></p>
<p>The Hamiltonian principle also covers functional and <strong>extremitizing</strong> functions in which we minimize or maximize a function. For example, in <strong>FEM</strong>, we look for the extremum of each function (minimum or maximum points) to determine the subsequent <strong>displacement</strong> of a string at each marching time.</p>
<ul>
<li><strong>Lagrangeâs equation</strong> is expressed as the difference between the kinetic energy (T) and potential energy (V). </li>
</ul>
<p><span class="math display">\[\begin{align}
L = T - V
\end{align}\]</span></p>
<ul>
<li><strong>DâAlembertâs Principle</strong> expresses an alternate form of Newtonâs second law in which force (F) minus mass (m) and acceleration (a) equals zero. </li>
</ul>
<p><span class="math display">\[\begin{align}
F - ma = 0
\end{align}\]</span></p>
</div>
</div>
<div id="approximation-using-partial-differential-equations" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.6</span> Approximation using Partial Differential Equations <a href="numericalcalculus.html#approximation-using-partial-differential-equations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Partial Differential Equations (PDE)</strong> deal with more than one input (independent variables). The intuition becomes more apparent as we go through three common <strong>PDE</strong> equations: Heat, Wave, and Laplace.</p>
<p>In the previous sections, we introduce <strong>Finite Element Methods (FEM)</strong> using several <strong>Weighted Residual methods (WRM)</strong> to solve for <strong>ODE</strong> and briefly cover <strong>Variational methods</strong>.</p>
<p>In this section, we use <strong>Finite Difference Methods (FDM)</strong> to solve for <strong>PDE</strong>, although we can use <strong>FEM</strong> also.
### The Poisson Equation </p>
<p>Many dynamic systems are <strong>governed</strong> by <strong>PDEs</strong>. For example, an equation used to <strong>describe</strong> the dynamics of newtonâs gravity is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\nabla \cdot \mathbf{g} = -4 \pi G \rho
\end{align}\]</span></p>
<p>A general equation called <strong>Poissonâs general equation</strong> governs such well-studied dynamic systems.</p>
<p><span class="math display">\[\begin{align}
\Delta u = f
\end{align}\]</span></p>
<p>where <strong>u</strong> can be numerically replaced with an approximating function (rather than an analytical function such as <span class="math inline">\(\mathbf{g} = -\nabla \phi\)</span>).</p>
<p>The <span class="math inline">\(\Delta\)</span> is the <strong>Laplace operator</strong> and the notation <span class="math inline">\(\Delta u\)</span> is represented in different forms:</p>
<p><span class="math display">\[\begin{align}
\Delta u = \nabla^2 u = \nabla \cdot \nabla u\ \ \ \ \ \ \ \ 
where\ \ \ \ \ \ \ \nabla = \left(\frac{\partial}{\partial_{x1}}, \frac{\partial}{\partial_{x2}},\ ...\ \frac{\partial}{\partial_{xn}}   \right)
\end{align}\]</span></p>
<p>In a one-dimension system, we see notations of the equation such as:</p>
<p><span class="math display">\[\begin{align}
\Delta u(x) = f(x) \ \ \ \ {}&amp;\rightarrow \ \ \ \ 
\left( \frac{\partial^2}{\partial x^2}  \right)u(x) = f(x)\\
\ \ \ \ &amp;\rightarrow \ \ \ \ 
 \frac{\partial^2u}{\partial x^2}  = f\\
\ \ \ \ &amp;\rightarrow \ \ \ \ u_{xx} = f 
\end{align}\]</span></p>
<p>In a multi-dimension system, we see notations of the equation such as:</p>
<p><span class="math display">\[\begin{align}
\Delta u(x, y,\ ...) = f(x,y,\ ...) \ \ \ \ {}&amp;\rightarrow \ \ \ \ 
\left( \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} +\ ...\right)u(x,y,\ ...) = f(x,y,\ ...)\\
\ \ \ \ &amp;\rightarrow \ \ \ \ 
 \frac{\partial^2u}{\partial x^2} + \frac{\partial^2u}{\partial y^2} +\ ... = f \\
\ \ \ \ &amp;\rightarrow \ \ \ \ u_{xx}\ +\ u_{yy} + ... = f 
\end{align}\]</span></p>
<p>Notice the change of notation for simplicity:</p>
<p><span class="math display">\[\begin{align}
u_x \rightarrow \frac{\partial u}{\partial x}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
u_{xx} \rightarrow \frac{\partial^2 u}{\partial x^2}
\end{align}\]</span></p>
<p>A two-dimensional system governed by a general <strong>Poisson equation</strong> becomes:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = f
\ \ \ \ \rightarrow\ \ \ \ \ u_{xx}\ +\ u_{yy} = f
\end{align}\]</span></p>
<p>There are dynamic systems that have unknown functions <span class="math inline">\(\mathbf{u(x,...)}\)</span>. Others have known (analytical) functions but are computationally impractical. Our goal is to use numerical methods that provide approximate solutions. In subsequent sections, we use the <strong>Finite Difference</strong> methods to perform <strong>PDE</strong> approximation to the solution of the three equations we mentioned. Note that in PDE, our solution definition is a function solution rather than a value solution, which means that we are looking for a function that can best describe a domain (e.g., See Figure <a href="numericalcalculus.html#fig:pde">4.24</a>).</p>
<div id="the-laplace-equation-elliptic-pde" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.6.1</span> The Laplace Equation (Elliptic PDE)  <a href="numericalcalculus.html#the-laplace-equation-elliptic-pde" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If a particular dynamic system reaches the steady-state or equilibrium, e.g.Â no heat, then we have the following <strong>Laplace</strong> equation:</p>
<p><span class="math display">\[\begin{align}
\Delta u = 0
\end{align}\]</span></p>
<p>The intuition behind the <strong>Laplace equation</strong> can be explained with <strong>heat diffusion</strong>. In other words, we can interpret the notation <span class="math inline">\(\Delta u\)</span> in the equation as the gradient divergence where the gradient (heat) vector field diverges outward or inward.</p>
<p>Consider the lower domain (heat segment) in Figure <a href="numericalcalculus.html#fig:pde">4.24</a> which closely characterizes the <strong>heat diffusion</strong> of the <strong>metallic rod</strong>. If we are to apply heat at the center of the <strong>metallic rod</strong>, the <strong>heat</strong> starts to resemble a tall narrower <strong>gaussian</strong> shape. As we stop the source of <strong>heat</strong>, the <strong>gaussian</strong> shape starts to get shorter and wider, indicating that the <strong>heat</strong> is diffusing towards the left and right ends of the metallic rod until such that the temperature reaches equilibrium - or in laplacian terms, until the Laplace equation is zero. See Figure <a href="numericalcalculus.html#fig:pde">4.24</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pde"></span>
<img src="pde.png" alt="1D heat and wave graph" width="80%" />
<p class="caption">
Figure 4.24: 1D heat and wave graph
</p>
</div>
<p>The <strong>Laplace equation</strong> for a one-dimensional dynamic system is expressed as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 u}{\partial x^2} = 0
\ \ \ \ \rightarrow\ \ \ \ \ u_{xx} = 0
\end{align}\]</span></p>
<p>The <strong>Laplace equation</strong> for a two-dimensional dynamic system is expressed as:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0
\ \ \ \ \rightarrow\ \ \ \ \ u_{xx} + u_{yy} = 0
\end{align}\]</span></p>
<p>and it can serve as means for us to determine <strong>equilibrium</strong>.</p>
<p>To illustrate the use of <strong>Laplace equation</strong> for a two-dimensional dynamic system, we <strong>use discretization</strong> using <strong>second-order centered finite difference</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} = 0
\ \ \ \ \rightarrow\ \ \ \ \
\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\Delta x)^2} + 
\frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\Delta y)^2} = 0
\end{align}\]</span></p>
<p>Let the deltas be equal distant, <span class="math inline">\(\Delta x = \Delta y\)</span>. That gives us the following nodal equation:</p>
<p><span class="math display">\[\begin{align}
0 {}&amp;= u_{k+1}^t  + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1} - 4u_k^t \\
u_k^t &amp;= \frac{1}{4} \left( u_{k+1}^t  + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1}\right)
\end{align}\]</span></p>
<p>Figure <a href="numericalcalculus.html#fig:laplaceplate">4.25</a> shows a grid that allows us to construct our system of linear equations. Each circle in the grid represents the temperature of a node, <span class="math inline">\(\mathbf{u_{ij}}\)</span>, which is expressed and can be solved using the nodal equation above. In our case, we assume that the temperature of the external nodes is already known, and the temperature of the interior nodes is not known. If we think that there are no practical analytical means of solving for the internal temperatures, we use approximation.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:laplaceplate"></span>
<img src="laplaceplate.png" alt="Laplace Equation using Gauss-Seidel Method" width="80%" />
<p class="caption">
Figure 4.25: Laplace Equation using Gauss-Seidel Method
</p>
</div>
<p>We start with the given boundary conditions (e.g.Â temperatures at the exterior nodes are known):</p>
<p><span class="math display">\[
u_k^0 = 20^\circ C,\ \ \ \ \ \
u_0^t = 40^\circ C,\ \ \ \ \ \
u_5^t = 60^\circ C,\ \ \ \ \ \
u_k^5 = 80^\circ C
\]</span></p>
<p>where:</p>
<p><span class="math display">\[
t=\{0\ ..\ 5\},\ \ \ \ \ \ \ k=\{1\ ..\ 4\},\ \ \ \ \ \ \mathbf{u_k^t} = 0^\circ C 
\]</span>
Our goal is to approximate the temperature of the interior nodes. We use the <strong>Gauss-Seidel method</strong> to perform an iteration using an <strong>initial temperature of zero for the interior nodes</strong>; we can use other methods to solve a system of linear equations. </p>
<p><strong>First</strong>, let us start by choosing an interior node in the matrix, say <span class="math inline">\(\mathbf{u_1^1}\)</span>, from which to construct our first equation. The nodal equation for the node is easier shown using a stencil. See Figure <a href="numericalcalculus.html#fig:2dstencil">4.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:2dstencil"></span>
<img src="2dstencil.png" alt="2D Stencil" width="60%" />
<p class="caption">
Figure 4.26: 2D Stencil
</p>
</div>
<p>We use the nodal equation to solve for <span class="math inline">\(\mathbf{u_1^1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
u_k^t {}&amp;= \frac{1}{4} \left( u_{k+1}^t  + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1}\right)\\
u_1^1  &amp;= \frac{1}{4} \left( u_2^1  + u_0^1 + u_1^2 + u_1^0 \right) \\
&amp;= \frac{1}{4} \left( 0  + 40.00 + 0 + 20.00 \right), \ \ \ \ where\ interior\ nodes\ are\ zero \nonumber \\
&amp;= \frac{1}{4} (60) = 15.00 \nonumber
\end{align}\]</span></p>
<p><strong>Second</strong>, let us choose the next interior node in the matrix, say <span class="math inline">\(\mathbf{u_1^2}\)</span>:</p>
<p><span class="math display">\[\begin{align}
u_1^2 {}&amp;= \frac{1}{4} \left( u_2^2  + u_0^2 + u_1^3 + u_1^1 \right)\\
u_1^2 &amp;= \frac{1}{4} \left( 0  + 40.00 + 0 + 15.00 \right)  \nonumber\\
&amp;= \frac{1}{4} (55) = 13.75  \nonumber
\end{align}\]</span></p>
<p><strong>Third</strong>, we proceed with <span class="math inline">\(\mathbf{u_1^3}\)</span> and <span class="math inline">\(\mathbf{u_1^4}\)</span>:</p>
<p><span class="math display">\[\begin{align}
u_3^1 {}&amp;= \frac{1}{4} \left( u_3^2  + u_0^3 + u_1^4 + u_1^2 \right)\\
u_3^1 &amp;= \frac{1}{4} \left( 0  + 40.00 + 0 + 13.75 \right)  \nonumber\\
&amp;= \frac{1}{4} (28.75) = 13.44  \nonumber\\
 \nonumber\\
u_4^1 {}&amp;= \frac{1}{4} \left( u_4^2  + u_0^4 + u_1^5 + u_1^3 \right)\\
u_4^1 &amp;= \frac{1}{4} \left( 0  + 40.00 + 80.00 + 13.44 \right)  \nonumber\\
&amp;= \frac{1}{4} (87.188) = 33.36  \nonumber
\end{align}\]</span></p>
<p><strong>Fourth</strong>, let us proceed with the next column, <span class="math inline">\(\mathbf{u_2^1}\)</span>:</p>
<p><span class="math display">\[\begin{align}
u_2^1 {}&amp;= \frac{1}{4} \left( u_3^1  + u_1^1 + u_2^2 + u_2^0 \right)\\
u_2^1 &amp;= \frac{1}{4} \left( 0  + 15.00 + 0 + 20.00 \right)  \nonumber\\
&amp;= \frac{1}{4} (45) = 8.75  \nonumber
\end{align}\]</span></p>
<p><strong>Fifth</strong>, we proceed with the rest of the interior nodes. After computing the temperature of the interior nodes, we see the following result of the first iteration:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrrrrr}
u_0^5 &amp; u_1^5 &amp; u_2^5 &amp; u_3^5 &amp; u_4^5 &amp; u_5^5\\
u_0^4 &amp; u_1^4 &amp; u_2^4 &amp; u_3^4 &amp; u_4^4 &amp; u_5^4 \\
u_0^3 &amp; u_1^3 &amp; u_2^3 &amp; u_3^3 &amp; u_4^3 &amp; u_5^3 \\
u_0^2 &amp; u_1^2 &amp; u_2^2 &amp; u_3^2 &amp; u_4^2 &amp; u_5^2 \\
u_0^1 &amp; u_1^1 &amp; u_2^1 &amp; u_3^1 &amp; u_4^1 &amp; u_5^1 \\
u_0^0 &amp; u_1^0 &amp; u_2^0 &amp; u_3^0 &amp; u_4^0 &amp; u_5^0 \\
\end{array}
\right]\rightarrow
\left[
\begin{array}{rrrrrr}
40.00 &amp; 80.00 &amp; 80.00 &amp; 80.00 &amp;  80.00 &amp; 60.00\\
40.00 &amp; 33.36 &amp; 29.53 &amp; 27.88 &amp; 47.17 &amp; 60.00 \\
40.00 &amp; 13.44 &amp; 4.77 &amp; 1.99 &amp; 20.81 &amp; 60.00 \\
40.00 &amp; 13.75 &amp; 5.62 &amp; 3.20 &amp; 21.25 &amp; 60.00  \\
40.00 &amp; 15.00 &amp; 8.75 &amp; 7.19 &amp; 21.80 &amp; 60.00  \\
40.00 &amp; 20.00 &amp; 20.00 &amp; 20.00 &amp; 20.00 &amp; 60.00
\end{array}
\right]
\]</span></p>
<p>We repeat the process starting with <span class="math inline">\(\mathbf{u_1^1}\)</span> again and completing all interior nodes for the next iteration.</p>
<p>We iterate until convergence. Convergence happens when the temperature of all of the individual interior nodes (the nodal temperature) reaches a tolerable relevant error.</p>
<p><span class="math display">\[
\left|\frac{ u_{ij}^{current} - u_{ij}^{previous}} {u_{ij}^{current}}\right| &lt; tol
\]</span>
Here is a naive R implementation of solving for the initial temperature distribution across the plate using the <strong>Laplacian equation</strong> and <strong>Gauss-Seidel</strong> method:</p>

<div class="sourceCode" id="cb181"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb181-1" data-line-number="1"><span class="co">#laplatian plate with initial temperature of zero for interior nodes</span></a>
<a class="sourceLine" id="cb181-2" data-line-number="2"><span class="co"># 36 nodes</span></a>
<a class="sourceLine" id="cb181-3" data-line-number="3">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb181-4" data-line-number="4"><span class="co">#boundary conditions for exterior nodes</span></a>
<a class="sourceLine" id="cb181-5" data-line-number="5">A[<span class="dv">1</span>,] =<span class="st"> </span><span class="dv">80</span>; A[<span class="dv">6</span>,] =<span class="st"> </span><span class="dv">20</span>; A[,<span class="dv">1</span>]=<span class="dv">40</span>; A[,<span class="dv">6</span>]=<span class="dv">60</span></a>
<a class="sourceLine" id="cb181-6" data-line-number="6">laplace &lt;-<span class="st"> </span><span class="cf">function</span>(A, iter ) {</a>
<a class="sourceLine" id="cb181-7" data-line-number="7">    err =<span class="st"> </span>A</a>
<a class="sourceLine" id="cb181-8" data-line-number="8">    converge =<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb181-9" data-line-number="9">    tol =<span class="st"> </span><span class="fl">1e-1</span></a>
<a class="sourceLine" id="cb181-10" data-line-number="10">    M =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb181-11" data-line-number="11">    N =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb181-12" data-line-number="12">    <span class="co"># Gauss-Seidel</span></a>
<a class="sourceLine" id="cb181-13" data-line-number="13">    <span class="cf">for</span> (K <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>,(N<span class="dv">-2</span>),<span class="dv">1</span>)) {</a>
<a class="sourceLine" id="cb181-14" data-line-number="14">        <span class="cf">for</span> (T <span class="cf">in</span> <span class="kw">seq</span>(<span class="dv">1</span>,(M<span class="dv">-2</span>),<span class="dv">1</span>)) {</a>
<a class="sourceLine" id="cb181-15" data-line-number="15">            k=K<span class="op">+</span><span class="dv">1</span>; t=M<span class="op">-</span>T</a>
<a class="sourceLine" id="cb181-16" data-line-number="16">            A[t,k] =<span class="st"> </span>(A[t,k<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>A[t,k<span class="dv">-1</span>] <span class="op">+</span><span class="st"> </span>A[t<span class="dv">-1</span>,k] <span class="op">+</span><span class="st"> </span>A[t<span class="op">+</span><span class="dv">1</span>,k] ) <span class="op">/</span><span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb181-17" data-line-number="17">            err[t,k] =<span class="st"> </span><span class="kw">abs</span>( ( A[t,k] <span class="op">-</span><span class="st"> </span>err[t,k] ) <span class="op">/</span><span class="st"> </span>A[t,k] ) <span class="op">*</span><span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb181-18" data-line-number="18">            <span class="cf">if</span>  (err[t,k] <span class="op">&gt;</span><span class="st"> </span>tol) converge =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb181-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb181-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb181-21" data-line-number="21">    <span class="kw">list</span>(<span class="st">&quot;converge&quot;</span>=converge, <span class="st">&quot;rel_err&quot;</span>=err, </a>
<a class="sourceLine" id="cb181-22" data-line-number="22">         <span class="st">&quot;laplace_plate&quot;</span>=<span class="kw">round</span>(A,<span class="dv">2</span>),  <span class="st">&quot;iteration&quot;</span>=iter )</a>
<a class="sourceLine" id="cb181-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb181-24" data-line-number="24">limit=<span class="dv">50</span>; sol =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb181-25" data-line-number="25"><span class="cf">for</span> (iterate <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb181-26" data-line-number="26">     sol =<span class="st"> </span><span class="kw">laplace</span>(A, iterate)</a>
<a class="sourceLine" id="cb181-27" data-line-number="27">     <span class="cf">if</span> (sol<span class="op">$</span>converge <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) <span class="cf">break</span></a>
<a class="sourceLine" id="cb181-28" data-line-number="28">     A =<span class="st"> </span>sol<span class="op">$</span>laplace_plate</a>
<a class="sourceLine" id="cb181-29" data-line-number="29">}</a>
<a class="sourceLine" id="cb181-30" data-line-number="30">sol</a></code></pre></div>
<pre><code>## $converge
## [1] TRUE
## 
## $rel_err
##      [,1]        [,2]        [,3]        [,4]         [,5] [,6]
## [1,]   40 80.00000000 80.00000000 80.00000000 80.000000000   60
## [2,]   40  0.02377444  0.01943362  0.01486774  0.006828848   60
## [3,]   40  0.03180476  0.04954826  0.03051368  0.013977502   60
## [4,]   40  0.05142632  0.06883892  0.04793824  0.030091247   60
## [5,]   40  0.04463621  0.05658029  0.05558349  0.023736587   60
## [6,]   40 20.00000000 20.00000000 20.00000000 20.000000000   60
## 
## $laplace_plate
##      [,1]  [,2]  [,3]  [,4]  [,5] [,6]
## [1,]   40 80.00 80.00 80.00 80.00   60
## [2,]   40 58.16 64.52 66.80 66.35   60
## [3,]   40 48.15 53.14 56.33 58.62   60
## [4,]   40 41.32 43.58 46.77 51.80   60
## [5,]   40 33.61 33.14 35.42 41.80   60
## [6,]   40 20.00 20.00 20.00 20.00   60
## 
## $iteration
## [1] 18</code></pre>

<p>After convergence, we see the <strong>final solution</strong> of the <strong>initial heat distribution</strong> across the plate. Note that this does not imply that the temperature across the plate has diffused in equilibrium. The solution is only for one specific point in time. Suppose we account for a time-dependent dynamic system. In that case, we have to capture each solution for every step over time until heat diffuses in equilibrium - meaning that the nodal temperature across the plate is the same.</p>
</div>
<div id="the-heat-equation-parabolic-pde" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.6.2</span> The Heat equation (Parabolic PDE)  <a href="numericalcalculus.html#the-heat-equation-parabolic-pde" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It helps to understand <strong>PDE</strong> by introducing the <strong>dynamics of heat and wave</strong> on a simple one-dimension manifold such as a line - in our case, we use <strong>rod</strong> for our physical material. To illustrate <strong>heat</strong>, we assume a thin <strong>metallic rod</strong> tightly bolted onto two heat sink materials or ice blocks. See Figure <a href="numericalcalculus.html#fig:pde">4.24</a>.</p>
<p>We may need to assume a few things also. First, the heat in the cross-section of the rod is constant as we heat the metallic rod in the middle. Second, we assume that the rod is insulated to disallow heat flux from escaping the rod. Then we assume that the metallic rod has already heated to an initial temperature. Our goal is to demonstrate that as we apply heat in the middle of the rod, the heat diffuses towards the left and right edges of the rod until the temperature across the rod meets equilibrium.</p>
<p>For illustration, we use a one-dimension <strong>homogeneous heat equation</strong>, which is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial u}{\partial t} = \alpha\frac{\partial^2 u}{\partial x^2}
\ \ \ \ \rightarrow\ \ \ \ \ u_{t} = \alpha\ u_{xx}
\ \ \ \ \rightarrow\ \ \ \ \ u_{t}(x,t) = \alpha\ u_{xx}(x,t)
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
\alpha = \frac{k}{c \rho}\ \ \ \ \rightarrow\ \ \ \  k=conductivity,\ c=capacity,\ \rho=density
\end{align}\]</span></p>
<p>and where <span class="math inline">\(\mathbf{\alpha}\)</span> is a constant (e.g.Â diffusivity) and our <strong>dirichlet boundary condition</strong> applies to space:</p>
<p><span class="math display">\[\begin{align}
u(0, t) = 0,\ \ \ \ \ \ \ \ u(1, t) = 0
\end{align}\]</span></p>
<p>Our initial condition (initial temperature) is as follows:</p>
<p><span class="math display">\[\begin{align}
u(x, 0) = f(x) = \frac{1}{\sqrt{0.4\pi}} e^{-\frac{x^2}{0.4^2}}
\end{align}\]</span></p>
<p>We use <strong>Finite Difference method</strong> to approximate the solution to the heat equation. Our approximation approach uses a <strong>fully discretized method</strong> called <strong>Forward Time, Centered Space (FTCS)</strong>.</p>
<p>The idea is to use the <strong>first-order Forward Finite Difference</strong> for time ( the <strong>t</strong> independent variable):</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) \approx \frac{f(x + h) - f(x)}{h}\ \rightarrow\ u_t \approx \frac{u_k^{t+1} - u_k^t}{\Delta t}
\end{align}\]</span></p>
<p>where our <strong>h</strong> is <strong>change in time</strong> denoted as <span class="math inline">\(\mathbf{\Delta t}\)</span>.</p>
<p>Additionally, the idea is also to use the <strong>second-order Centered Finite Difference</strong> for space (the <strong>x</strong> independent variable):</p>
<p><span class="math display">\[\begin{align}
f&#39;&#39;(x) \approx \frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\ \rightarrow\ u_{xx} \approx \frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\Delta x)^2}
\end{align}\]</span></p>
<p>We substitute with the approximate finite difference equations:</p>
<p><span class="math display">\[\begin{align}
u_t = \alpha u_{xx}\ \ \ \rightarrow \frac{u_k^{t+1} - u_k^t}{\Delta t} {}&amp;= \alpha \left(\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\Delta x)^2}\right) \\
u_k^{t+1} - u_k^t &amp;= \left(\frac{\alpha  \Delta t}{(\Delta x)^2}\right)  \left( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\right) 
\end{align}\]</span></p>
<p>and then perform simplification to arrive at the following equation:</p>
<p><span class="math display">\[\begin{align}
u_k^{t+1} - u_k^t  {}&amp;= \lambda  ( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}) \\
u_k^{t+1}   &amp;= u_k^t + \lambda  ( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}) \\
u_k^{t+1}   &amp;=  \lambda u_{k+1}^t  + ( 1 - 2 \lambda )u_k^t + \lambda  u_{k-1}^{t} 
\end{align}\]</span></p>
<p>where we have the <strong>CFL condition</strong> (Courant, Friedrichs, Lewy 1928) as:</p>
<p><span class="math display">\[\begin{align}
\lambda  = \left(\frac{\alpha  \Delta t}{(\Delta x)^2}\right) &lt;= 0.5
\end{align}\]</span></p>
<p>In matrix form, we get:</p>
<p><span class="math display">\[
\left[\begin{array}{r} 
u_{0}^{t+1} \\ 
u_{1}^{t+1}  \\ 
u_{2}^{t+1}  \\ 
\vdots \\ 
u_{n}^{t+1} \end{array}\right] =
\left[
\begin{array}{rrrrrr}
(1-2\lambda ) &amp; \lambda &amp; 0 &amp; ... &amp; 0 \\
\lambda&amp; (1-2\lambda) &amp; \lambda &amp; ... &amp; 0 \\
0 &amp; \lambda &amp; (1-2\lambda) &amp; ... &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \lambda \\
0 &amp; ... &amp; 0 &amp; \lambda&amp; (1-2\lambda) \\
\end{array}
\right] 
\left[\begin{array}{r} u_{0}^t \\ u_{1}^t \\ u_{2}^t \\ \vdots \\ u_n^t \end{array}\right]
\]</span></p>
<p>Here is a naive implementation of <strong>Heat Diffusion (non-smooth)</strong> in R code without using matrix or methods such as <strong>Gauss-Seidel or SOR</strong> to solve the matrix. See Figure <a href="numericalcalculus.html#fig:heatequation1">4.27</a>. </p>

<div class="sourceCode" id="cb183"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb183-1" data-line-number="1">f &lt;-<span class="st"> </span><span class="cf">function</span>(x, t) {  </a>
<a class="sourceLine" id="cb183-2" data-line-number="2">        <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>( <span class="fl">0.4</span> <span class="op">*</span><span class="st"> </span>pi ) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">/</span>(<span class="fl">0.4</span>)<span class="op">^</span><span class="dv">2</span> )  </a>
<a class="sourceLine" id="cb183-3" data-line-number="3">    } </a>
<a class="sourceLine" id="cb183-4" data-line-number="4">initial &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb183-5" data-line-number="5">    x_ =<span class="st"> </span><span class="kw">f</span>(x, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb183-6" data-line-number="6">    x_[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span>; x_[N] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb183-7" data-line-number="7">    x_</a>
<a class="sourceLine" id="cb183-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb183-9" data-line-number="9"><span class="co"># finite difference (ftcs)</span></a>
<a class="sourceLine" id="cb183-10" data-line-number="10">heat_ftcs &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda) { </a>
<a class="sourceLine" id="cb183-11" data-line-number="11">    u =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol=</span>N, <span class="dt">nrow=</span>M)</a>
<a class="sourceLine" id="cb183-12" data-line-number="12">    u[<span class="dv">1</span>,] =<span class="st"> </span><span class="kw">initial</span>(x)</a>
<a class="sourceLine" id="cb183-13" data-line-number="13">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(M<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb183-14" data-line-number="14">        u_t =<span class="st">  </span>u[t,] </a>
<a class="sourceLine" id="cb183-15" data-line-number="15">        x_  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb183-16" data-line-number="16">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb183-17" data-line-number="17">             x_[k] =<span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>u_t[k<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>u_t[k] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb183-18" data-line-number="18"><span class="st">                     </span>lambda <span class="op">*</span><span class="st"> </span>u_t[k<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb183-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb183-20" data-line-number="20">        u[t<span class="op">+</span><span class="dv">1</span>,] =<span class="st"> </span>x_</a>
<a class="sourceLine" id="cb183-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb183-22" data-line-number="22">    u</a>
<a class="sourceLine" id="cb183-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb183-24" data-line-number="24"><span class="co"># finite difference (crank-nicolson)</span></a>
<a class="sourceLine" id="cb183-25" data-line-number="25">heat_crank_nicolson &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda) { </a>
<a class="sourceLine" id="cb183-26" data-line-number="26">    u =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol=</span>N, <span class="dt">nrow=</span>M)</a>
<a class="sourceLine" id="cb183-27" data-line-number="27">    u[<span class="dv">1</span>,] =<span class="st"> </span><span class="kw">initial</span>(x)</a>
<a class="sourceLine" id="cb183-28" data-line-number="28">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(M<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb183-29" data-line-number="29">        u_t =<span class="st">  </span>u[t,] </a>
<a class="sourceLine" id="cb183-30" data-line-number="30">        x_forward =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb183-31" data-line-number="31">        x_backward =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb183-32" data-line-number="32">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb183-33" data-line-number="33">             x_forward[k] =<span class="st"> </span></a>
<a class="sourceLine" id="cb183-34" data-line-number="34"><span class="st">                  </span>lambda <span class="op">*</span><span class="st"> </span>u_t[k<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>u_t[k] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb183-35" data-line-number="35"><span class="st">                  </span>lambda <span class="op">*</span><span class="st"> </span>u_t[k<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb183-36" data-line-number="36">        }</a>
<a class="sourceLine" id="cb183-37" data-line-number="37">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb183-38" data-line-number="38">             x_backward[k] =<span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span>x_forward[k<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb183-39" data-line-number="39"><span class="st">                  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st">  </span>x_forward[k] <span class="op">+</span><span class="st"> </span>lambda <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb183-40" data-line-number="40"><span class="st">                                      </span>x_forward[k<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb183-41" data-line-number="41">        }</a>
<a class="sourceLine" id="cb183-42" data-line-number="42">        u[t<span class="op">+</span><span class="dv">1</span>,] =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(x_forward <span class="op">+</span><span class="st"> </span>x_backward )</a>
<a class="sourceLine" id="cb183-43" data-line-number="43">    }</a>
<a class="sourceLine" id="cb183-44" data-line-number="44">    u</a>
<a class="sourceLine" id="cb183-45" data-line-number="45">}</a>
<a class="sourceLine" id="cb183-46" data-line-number="46">space =<span class="st"> </span>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">31</span>)</a>
<a class="sourceLine" id="cb183-47" data-line-number="47">time =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb183-48" data-line-number="48">N =<span class="st"> </span><span class="kw">length</span>(space); M =<span class="st"> </span><span class="kw">length</span>(time)</a>
<a class="sourceLine" id="cb183-49" data-line-number="49">u =<span class="st"> </span><span class="kw">heat_ftcs</span>(space, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb183-50" data-line-number="50"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb183-51" data-line-number="51">     <span class="dt">xlab=</span><span class="st">&quot;metalic rod&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;temperature&quot;</span>,</a>
<a class="sourceLine" id="cb183-52" data-line-number="52">     <span class="dt">main=</span><span class="st">&quot;2D view of heat diffusion&quot;</span>)</a>
<a class="sourceLine" id="cb183-53" data-line-number="53"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb183-54" data-line-number="54"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb183-55" data-line-number="55"></a>
<a class="sourceLine" id="cb183-56" data-line-number="56"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb183-57" data-line-number="57">    <span class="kw">lines</span>(x, u[t,], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb183-58" data-line-number="58">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heatequation1"></span>
<img src="embed0039.png" alt="Heat Diffusion" width="80%" />
<p class="caption">
Figure 4.27: Heat Diffusion
</p>
</div>
<div class="sourceCode" id="cb184"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb184-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&#39;plot3D&#39;</span>)</a>
<a class="sourceLine" id="cb184-2" data-line-number="2">x=<span class="dv">1</span><span class="op">:</span>N;y=<span class="dv">1</span><span class="op">:</span>M;z=u</a>
<a class="sourceLine" id="cb184-3" data-line-number="3"><span class="kw">persp3D</span>(y, x, z, <span class="dt">xlab=</span><span class="st">&quot;time&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;rod length&quot;</span>, </a>
<a class="sourceLine" id="cb184-4" data-line-number="4">        <span class="dt">zlab=</span><span class="st">&quot;temperature&quot;</span>, </a>
<a class="sourceLine" id="cb184-5" data-line-number="5">        <span class="dt">main=</span><span class="st">&quot;3D view of heat diffusion&quot;</span>,</a>
<a class="sourceLine" id="cb184-6" data-line-number="6">        <span class="dt">axes=</span><span class="ot">TRUE</span>, <span class="dt">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heatequation2"></span>
<img src="embed0040.png" alt="3D view of heat diffusion" width="80%" />
<p class="caption">
Figure 4.28: 3D view of heat diffusion
</p>
</div>

<p>Notice in the FTCS implementation of the heat equation that the <span class="math inline">\(\lambda\)</span> affects the stability of the system. A value less than 0.5 renders stability.</p>
</div>
<div id="the-wave-equation-hyperbolic-pde" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.6.3</span> The Wave equation (Hyperbolic PDE)  <a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hyperbolic systems</strong> deal with vibrations (oscillations) and waves. Unlike <strong>Heat diffusion</strong> where temperature settles down to equilibrium immediately, the behavior of vibration in particular starts from an initial state (say high altitude). But instead of slowing down to equilibrium, it tends to overshoot to the other side, eventually reaching a pulling resistance, causing the system to oscillate back and forth like a pendulum or a vibrating string. Each oscillation, however, exponentially manifests a slow-down decay until such that it eventually reaches equilibrium. The resistance - restoring force - encountered by the oscillation is the inertia that conserves energy.</p>
<p>The <strong>Wave equation</strong> we use is slightly different from the <strong>Heat equation</strong> by raising the left side of the equation to the power of 2.</p>
<p>To illustrate the use of <strong>Wave equation</strong>, we likewise use the <strong>Finite Difference</strong> method to approximate the one-dimension <strong>homogeneous Wave equation</strong> below for a vibrating string:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial^2 u}{\partial t^2} = \alpha \frac{\partial^2 u}{\partial x^2}
\ \ \ \ \rightarrow\ \ \ \ \ u_{tt} = \alpha u_{xx} 
\ \ \ \ \rightarrow\ \ \ \ \ u_{tt}(x,t) = \alpha\ u_{xx}(x,t)
\end{align}\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align}
\alpha = \frac{\tau}{\rho}\ \ \ \ \rightarrow\ \ \ \  \tau=tension,\ \rho=density
\end{align}\]</span></p>
<p>and where <span class="math inline">\(\mathbf{\alpha}\)</span> is a constant (e.g.Â speed of wave propagation) and our <strong>Dirichlet boundary condition</strong> applies to space:</p>

<p><span class="math display">\[\begin{align}
u(0, t) = 0,\ \ \ \ \ \ \ \ u(1, t) = 0
\end{align}\]</span>
</p>
<p>Our <strong>initial conditions</strong> for displacement and velocity are:</p>

<p><span class="math display">\[\begin{align}
u(x, 0) = f(x)\ \leftarrow displacement,\ \ \ \ u_t(x, 0) = \frac{\partial u}{\partial t}(x,0) = g(x)\ \leftarrow velocity,\ \ \ \ 0 \leq x \leq 1
\end{align}\]</span>
</p>
<p>Here, the idea is to compute the trajectory of the waves by knowing the displacement and velocity. In the case of our illustration, we have a vibrating string.</p>
<p>Our approximation approach is to use the <strong>second-order Centered Finite Difference</strong>:</p>

<p><span class="math display">\[\begin{align}
f&#39;&#39;(x) \approx \frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\ \rightarrow\ u_{tt} \approx \frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\Delta t)^2}
\end{align}\]</span>
</p>
<p>where our <strong>h</strong> is <strong>change in time</strong> denoted as <span class="math inline">\(\mathbf{\Delta t}\)</span>,</p>
<p>Additionally, we also use the <strong>second-order Centered Finite Difference</strong> for <strong>x</strong>:</p>

<p><span class="math display">\[\begin{align}
f&#39;&#39;(x) \approx \frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\ \rightarrow\ u_{xx} \approx \frac{u_{k+1}^{t} - 2u_k^t + u_{k-1}^t}{(\Delta x)^2}
\end{align}\]</span>
</p>
<p>We substitute with the approximate <strong>Finite Difference equations</strong>:</p>

<p><span class="math display">\[\begin{align}
u_{tt} = \alpha u_{xx}\ \ \ \rightarrow {}&amp;\frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\Delta t)^2} = \alpha\left(\frac{u_{k+1}^{t} - 2u_k^t + u_{k-1}^t}{(\Delta x)^2}\right) \\
&amp;u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1} =  \alpha \left( \frac{\Delta t}{\Delta x}\right)^2(u_{k+1}^{t} - 2u_k^t + u_{k-1}^t)
\end{align}\]</span>
</p>
<p>and then perform simplification to arrive at the following equation:</p>

<p><span class="math display">\[\begin{align}
{}&amp;u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1} =  \lambda (u_{k+1}^{t} - 2u_k^t + u_{k-1}^t)\\
&amp;u_{k}^{t+1} =  2u_k^t - u_{k}^{t-1} + \lambda (u_{k+1}^{t} - 2u_k^t + u_{k-1}^t)\\
&amp;u_{k}^{t+1} =  \lambda(u_{k+1}^t + u_{k-1}^t) + 2(1 - \lambda)u_k^t - u_k^{t-1}
\end{align}\]</span>
</p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\lambda = \alpha \left( \frac{\Delta t}{\Delta x}\right)^2 &lt;= 0.5
\end{align}\]</span></p>
<p>Here is a naive way of implementing the <strong>Wave equation</strong> in R code:</p>

<div class="sourceCode" id="cb185"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb185-1" data-line-number="1">f &lt;-<span class="st"> </span><span class="cf">function</span>(x,t) {  </a>
<a class="sourceLine" id="cb185-2" data-line-number="2">    <span class="dv">1</span><span class="op">/</span><span class="kw">sqrt</span>( <span class="fl">0.4</span> <span class="op">*</span><span class="st"> </span>pi ) <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">/</span>(<span class="fl">0.4</span>)<span class="op">^</span><span class="dv">2</span> )  </a>
<a class="sourceLine" id="cb185-3" data-line-number="3">} </a>
<a class="sourceLine" id="cb185-4" data-line-number="4">initial &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb185-5" data-line-number="5">    x_ =<span class="st"> </span><span class="kw">f</span>(x, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb185-6" data-line-number="6">    x_[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span>; x_[N] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb185-7" data-line-number="7">    x_</a>
<a class="sourceLine" id="cb185-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb185-9" data-line-number="9">wave &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda) { <span class="co"># finite difference</span></a>
<a class="sourceLine" id="cb185-10" data-line-number="10">    u =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">ncol=</span>N, <span class="dt">nrow=</span>M)</a>
<a class="sourceLine" id="cb185-11" data-line-number="11">    u[<span class="dv">1</span>,] =<span class="st"> </span>u_t_old =<span class="st"> </span><span class="kw">initial</span>(x)</a>
<a class="sourceLine" id="cb185-12" data-line-number="12">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(M<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb185-13" data-line-number="13">        u_t =<span class="st">  </span>u[t,] </a>
<a class="sourceLine" id="cb185-14" data-line-number="14">        u_t_new =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb185-15" data-line-number="15">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb185-16" data-line-number="16">             u_t_new[k] =<span class="st"> </span>lambda<span class="op">*</span>(u_t[k<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>u_t[k<span class="dv">-1</span>]) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb185-17" data-line-number="17"><span class="st">                          </span><span class="dv">2</span> <span class="op">*</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>lambda)<span class="op">*</span>u_t[k] <span class="op">-</span><span class="st"> </span>u_t_old[k]</a>
<a class="sourceLine" id="cb185-18" data-line-number="18">        }</a>
<a class="sourceLine" id="cb185-19" data-line-number="19">        u_t_old =<span class="st"> </span>u_t</a>
<a class="sourceLine" id="cb185-20" data-line-number="20">        u[t<span class="op">+</span><span class="dv">1</span>,] =<span class="st"> </span>u_t_new</a>
<a class="sourceLine" id="cb185-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb185-22" data-line-number="22">    u</a>
<a class="sourceLine" id="cb185-23" data-line-number="23">}</a>
<a class="sourceLine" id="cb185-24" data-line-number="24">space =<span class="st"> </span>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">41</span>)</a>
<a class="sourceLine" id="cb185-25" data-line-number="25">time =<span class="st"> </span><span class="kw">seq</span>(<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb185-26" data-line-number="26">N =<span class="st"> </span><span class="kw">length</span>(space); M =<span class="st"> </span><span class="kw">length</span>(time)</a>
<a class="sourceLine" id="cb185-27" data-line-number="27">u =<span class="st"> </span><span class="kw">wave</span>(space, <span class="fl">0.4</span>)</a>
<a class="sourceLine" id="cb185-28" data-line-number="28"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb185-29" data-line-number="29">     <span class="dt">xlab=</span><span class="st">&quot;vibrating string&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;oscillation&quot;</span>,</a>
<a class="sourceLine" id="cb185-30" data-line-number="30">     <span class="dt">main=</span><span class="st">&quot;2D view of Oscillation&quot;</span>)</a>
<a class="sourceLine" id="cb185-31" data-line-number="31"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb185-32" data-line-number="32"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb185-33" data-line-number="33"><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb185-34" data-line-number="34">    <span class="kw">lines</span>(x, u[t,], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb185-35" data-line-number="35">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:waveequation1"></span>
<img src="embed0041.png" alt="Wave Propagation" width="70%" />
<p class="caption">
Figure 4.29: Wave Propagation
</p>
</div>
<div class="sourceCode" id="cb186"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb186-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&#39;plot3D&#39;</span>)</a>
<a class="sourceLine" id="cb186-2" data-line-number="2">x=<span class="dv">1</span><span class="op">:</span>N;y=<span class="dv">1</span><span class="op">:</span>M;z=u</a>
<a class="sourceLine" id="cb186-3" data-line-number="3"><span class="kw">persp3D</span>(y, x, z, <span class="dt">xlab=</span><span class="st">&quot;time&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;string length&quot;</span>, </a>
<a class="sourceLine" id="cb186-4" data-line-number="4">        <span class="dt">zlab=</span><span class="st">&quot;oscillation&quot;</span>, </a>
<a class="sourceLine" id="cb186-5" data-line-number="5">        <span class="dt">main=</span><span class="st">&quot;3D view of Oscillation&quot;</span>,</a>
<a class="sourceLine" id="cb186-6" data-line-number="6">        <span class="dt">axes=</span><span class="ot">TRUE</span>, <span class="dt">ticktype=</span><span class="st">&quot;detailed&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:waveequation2"></span>
<img src="embed0042.png" alt="3D view of Wave Propagation" width="70%" />
<p class="caption">
Figure 4.30: 3D view of Wave Propagation
</p>
</div>

<p>Note that Figure <a href="numericalcalculus.html#fig:waveequation1">4.29</a> shows a sample of an oscillating string. We can also simulate a wave that propagates, such as a lightwave, by adjusting the boundary conditions and possibly the initial condition while retaining the computed Finite Difference equation:</p>
<p><span class="math display">\[\begin{align}
u_{k}^{t+1} =  \lambda(u_{k+1}^t + u_{k-1}^t) + 2(1 - \lambda)u_k^t - u_k^{t-1}
\end{align}\]</span></p>
<p>Taking into account stability and accuracy, we leave readers to investigate other <strong>explicit finite difference</strong> schemes such as <strong>Forward Euler</strong>, <strong>Leapfrog</strong>, <strong>Lax-Wendroff</strong> and also explore other <strong>implicit finite difference</strong> schemes such as <strong>Backward Euler</strong>, and <strong>Crank-Nicolson</strong>.</p>
<p>We also leave readers to investigate solving <strong>heat equation</strong> and <strong>wave equations</strong> using <strong>Finite Element Methods</strong>.</p>
<p>Lastly, for a more accurate (or precise) result, we leave readers to also investigate <strong>Modern Tailor Series Method (MTSM)</strong>.</p>
</div>
<div id="the-crank-nicolson-equation" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.6.4</span> The Crank-Nicolson Equation <a href="numericalcalculus.html#the-crank-nicolson-equation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>An alternate approximation for the <strong>heat equation</strong> is the <strong>Crank-Nicolson Equation</strong> which is unconditionally stable.</p>
<p>Our approximation approach account for the average between two time-intervals using <strong>explicit and implicit finite difference</strong>, a.l.a, forward Euler and backward Euler).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cranknicolson"></span>
<img src="cranknicolson.png" alt="Crank Nicolson Stencil" width="60%" />
<p class="caption">
Figure 4.31: Crank Nicolson Stencil
</p>
</div>
<p>The idea is still to use the <strong>first-order Forward Finite Difference</strong> for time ( the <strong>t</strong> independent variable):</p>
<p><span class="math display">\[\begin{align}
f&#39;(x) \approx \frac{f(x + h) - f(x)}{h}\ \rightarrow\ u_t \approx \frac{u_k^{t+1} - u_k^t}{\Delta t}
\end{align}\]</span></p>
<p>where our <strong>h</strong> is <strong>change in time</strong> denoted as <span class="math inline">\(\mathbf{\Delta t}\)</span>.</p>
<p>Additionally, the idea is to also use the average of the explicit and implicit <strong>second-order Centered Finite Difference</strong> for space (the <strong>x</strong> independent variable):</p>

<p><span class="math display">\[\begin{align}
f&#39;&#39;(x) {}&amp;\approx \frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\\
u_{xx} &amp;\approx 
\frac{1}{2} \left(
\frac{u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1}}{(\Delta x)^2} + \frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\Delta x)^2}
\right) \\
&amp;\approx
\frac{1}{2(\Delta x)^2} \left(
u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}
\right)
\end{align}\]</span>
</p>
<p>We substitute with the approximate finite difference equations:</p>

<p><span class="math display">\[\begin{align}
u_t = \alpha u_{xx}\ \ \ \rightarrow \frac{u_k^{t+1} - u_k^t}{\Delta t} {}&amp;= 
\alpha \left[ \frac{1}{2(\Delta x)^2} 
\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\right) 
\right]\\
u_k^{t+1} - u_k^t &amp;= \frac{\lambda}{2}
\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\right)  
\end{align}\]</span>
</p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\lambda  = \left(\frac{\alpha  \Delta t}{(\Delta x)^2}\right) &lt;= 0.7
\end{align}\]</span></p>
<p>Simplifying further, we then get the following:</p>

<p><span class="math display">\[\begin{align}
{}&amp;u_k^{t+1} - u_k^t = \frac{\lambda}{2}
\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\right) \\
&amp;- \lambda u_{k+1}^{t+1}  + 2(1 + \lambda)u_k^{t+1} - \lambda u_{k-1}^{t+1} = \lambda u_{k+1}^t + 2(1 - \lambda)u_k^t  + \lambda u_{k-1}^{t} 
\end{align}\]</span>
</p>
<p>The equation above can be used to form a matrix of system of equations.</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrr}
2(1+\lambda) &amp; -\lambda &amp; 0 &amp; ... &amp; 0 \\
-\lambda&amp; 2(1+\lambda) &amp; -\lambda &amp; ... &amp; 0 \\
0 &amp; -\lambda &amp; 2(1+\lambda) &amp; ... &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; -\lambda \\
0 &amp; ... &amp; 0 &amp; -\lambda&amp; 2(1+\lambda) \\
\end{array}
\right] 
\left[\begin{array}{r} u_{0}^{t+1} \\ u_{1}^{t+1}
 \\ u_{2}^{t+1} \\ \vdots \\ u_n^{t+1} \end{array}\right]
 =
 \left[\begin{array}{r} 
2(1-\lambda)u_{0}^t + \lambda u_{1}^t \\ 
\lambda u_{0}^t +  2(1-\lambda)u_{1}^t + \lambda u_{2}^t  \\ 
\lambda u_{1}^t +  2(1-\lambda)u_{2}^t + \lambda u_{3}^t  \\ 
\vdots \\ 
\lambda u_{n-1}^t + 2(1-\lambda)u_{n}^t \end{array}\right]
\]</span>
</p>
<p>Approximation methods such as <strong>Gauss-Seidel</strong>, <strong>SOR</strong>, etc., can then be used to solve the unknown functions for each time interval. Also, our naive implementation in R for the <strong>heat equation</strong> shows the <strong>Crank-Nicolson</strong> method of solving the solution using a greedy loop rather than matrix manipulation methods.</p>
</div>
<div id="the-burgers-equation" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.6.5</span> The Burgerâs Equation <a href="numericalcalculus.html#the-burgers-equation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It helps to introduce <strong>Burgerâs equation</strong> because this <strong>PDE</strong> has essential applications in areas such as <strong>fluid dynamics</strong> and <strong>traffic flow</strong>.</p>
<p><span class="math display">\[\begin{align}
\frac{\partial u}{\partial t}  + u\frac{\partial u}{\partial x} = \alpha\frac{\partial^2 u}{\partial x^2}
\ \ \ \ \rightarrow\ \ \ \ \ u_{t} + uu_x= \alpha u_{xx}
\end{align}\]</span></p>
<p>We leave readers to further investigate this equation.</p>
<p>In subsequent sections, we focus on <strong>harmonic or periodic</strong> systems used for <strong>signal processing</strong>. Along with the discussions, we illustrate how to identify and work on individual components of complex systems.</p>
</div>
</div>
<div id="approximation-using-fourier-series-and-transform" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.7</span> Approximation using Fourier Series And Transform <a href="numericalcalculus.html#approximation-using-fourier-series-and-transform" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>), we introduced <strong>Polynomial Approximation</strong> and <strong>Polynomial Interpolation</strong> to deal with irregular curves. We also introduced methods based on the <strong>Taylor Series</strong>, representing a *Polynomial Expansion** - a linear combination of polynomials <span class="citation">(Press W.H. et al. <a href="bibliography.html#ref-ref215w">2007</a>)</span>.</p>
<p>Here, we introduce <strong>Fourier Series</strong> which represents a sinusoidal expansion - a linear combination of sine and cosine functions. We present curves that demonstrate periodic or harmonic patterns such that each period (T) repeats:</p>
<p><span class="math display">\[\begin{align}
f(x) = f(x + T)
\end{align}\]</span></p>
<p><strong>Sine</strong> and <strong>Cosine</strong> functions give us an example of periodic wave patterns. Apart from such patterns, we introduce composite periodic patterns generated by a combination of periodic functions instead of just being individual <strong>Sine</strong> or <strong>Cosine</strong> patterns.</p>
<p>Figure <a href="numericalcalculus.html#fig:fourier">4.32</a> shows a <strong>signal</strong> generated by a simple summation of <strong>sine</strong> and <strong>cosine</strong>, e.g.:</p>
<p><span class="math display">\[\begin{align}
f(x) = sin(x) + cos(x)
\end{align}\]</span></p>
<p>The period used in the graph is of length <span class="math inline">\(\mathbf{2\pi}\)</span>. In the graph, we see three periods (or three waves) overlapping with an extra square wave having the following properties:</p>
<p><span class="math display">\[
f(x) \rightarrow
\begin{cases}
-1 &amp; -\pi &lt; x &lt; 0 \\
\ \ \ 0 &amp; x = 0,\pm \pi, \pm 2 \pi \\
\ \ \ 1 &amp; 0 &lt; x &lt; \pi
\end{cases}
\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fourier"></span>
<img src="embed0043.png" alt="Periodic Signal" width="70%" />
<p class="caption">
Figure 4.32: Periodic Signal
</p>
</div>
<p>This section introduces <strong>Fourier Series</strong>, expressed as a function that handles curves with harmonic or periodic patterns. In this particular case, we intend to derive a <strong>Fourier Series</strong> function that can closely represent the square wave in Figure <a href="numericalcalculus.html#fig:fourier">4.32</a>. We can express such periodic series using the following equation:</p>
<p><span class="math display">\[\begin{align}
F(x) = \sum_{k=1}^{\infty} e^{kx} = a_0 + \sum_{k=1}^{\infty} (a_k cos(kx) + b_k sin(kx))
\end{align}\]</span></p>
<p>The three coefficients in the formula expand into the following (note that we do not cover the derivation of the coefficients here):</p>
<p><span class="math display">\[\begin{align}
a_0 {}&amp;= \frac{1}{2L} \int_{-L}^L f(x) dx \\
\nonumber \\
a_k &amp;= \frac{1}{L}  \int_{-L}^L f(x) cos(kx) dx \\
\nonumber \\
b_k &amp;= \frac{1}{L}  \int_{-L}^L f(x) sin(kx) dx 
\end{align}\]</span></p>
<p>where <span class="math inline">\(L = (period)/2\)</span>.</p>
<p>To illustrate the use of the <strong>Fourier series</strong> equation, let us solve for the coefficients; given a period of <span class="math inline">\(\mathbf{2\pi}\)</span>, along with the graph in Figure <a href="numericalcalculus.html#fig:fourier">4.32</a>, and including the properties of the square wave.</p>
<p>Note that the period, <span class="math inline">\(\mathbf{2\pi}\)</span>, results in <span class="math inline">\(L=(2\pi)/2 = \pi\)</span>.</p>
<p><strong>First</strong>, let us solve for <span class="math inline">\(\mathbf{a_0}\)</span> by splitting the improper integral into two integrals:</p>
<p><span class="math display">\[\begin{align}
a_0 {}&amp;= \frac{1}{2\pi} \int_{-\pi}^\pi f(x) dx \\
&amp;= \frac{1}{2\pi} \left[ \int_{-\pi}^0 (-1)dx + \int_0^{\pi} (1)dx \right] \\
&amp;= 0 \nonumber
\end{align}\]</span></p>
<p><strong>Second</strong>, we solve for <span class="math inline">\(\mathbf{a_k}\)</span>.</p>
<p>Here, it helps to be aware of the concept of <strong>odd</strong> or <strong>even</strong> functions. It allows us to immediately determine the ending result of the function without having to go through the integration.</p>
<p>A function is <strong>even</strong> if <span class="math inline">\(\mathbf{f(x) = f(-x)}\)</span> for all <span class="math inline">\(\mathbf{x}\)</span>. And a function is <strong>odd</strong> if <span class="math inline">\(\mathbf{f(x) = -f(-x)}\)</span> for all <span class="math inline">\(\mathbf{x}\)</span>. This gives us the following formulas:</p>
<p><span class="math display">\[\begin{align}
f_{odd}(x) = \int_{-L}^{L} f(x) dx = 0,\ \ \ \ \ \ \ \ \ \ \
f_{even}(x) =  \int_{-L}^{L} f(x) dx = 2 \int_{0}^{L} f(x) dx
\end{align}\]</span></p>
<p>Additionally, an odd function multiplied by an even function results in an odd function. Otherwise, it is even function. For example, below is an odd function because it is a multiplication of even and odd. Therefore, the result is zero. To illustrate:</p>
<p><span class="math display">\[
f(x) = 1 \rightarrow odd,\ \ \ \ \ cos(kx) \rightarrow even,\ \ \ \ \ \therefore\ \text{we get odd}.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{align*}
a_k &amp;= \frac{1}{\pi}  \int_{-\pi}^\pi f(x) cos(kx) dx,\ \ \leftarrow \text{odd function} \\
&amp;= 0.
\end{align*}\]</span></p>
<p><strong>Third</strong>, we solve for <span class="math inline">\(\mathbf{b_k}\)</span>.</p>
<p>Here, we have the following</p>
<p><span class="math display">\[
f(x) = 1 \rightarrow odd,\ \ \ \ \ sin(kx) \rightarrow odd,\ \ \ \ \ \therefore\ \text{we get even}.
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[\begin{align*}
b_k &amp;= \frac{1}{\pi}  \int_{-\pi}^\pi f(x) sin(kx) dx \\
&amp;= \frac{2}{\pi}  \int_{0}^\pi f(x) sin(kx) dx,\ \ \leftarrow \text{even function}   \\
&amp;= \frac{2}{\pi}  \int_{0}^\pi (1) sin(kx) dx\\
&amp;= \frac{2}{\pi}  \left[\ -cos(kx)\ \right]_0^{\pi}\\
&amp;= 
\begin{cases}
0 &amp; \text{k is even} \\
\frac{4}{k\pi} &amp; \text{k is odd}
\end{cases}
\end{align*}\]</span></p>
<p><strong>Finally</strong>, we compose our <strong>Fourier Series</strong> function for the square wave function in Figure <a href="numericalcalculus.html#fig:fourier">4.32</a> using the <strong>coefficients</strong>, and we end up with the following square-wave equation:</p>
<p><span class="math display">\[
F(x) = \frac{4}{\pi}\left(sin(x) + \frac{1}{3}sin(3x) + \frac{1}{5}sin(5x)\ +\ ... \right)
\]</span></p>
<p>One of the widely used examples of <strong>Fourier series</strong> is shown in Figure <a href="numericalcalculus.html#fig:signal">4.33</a> which shows a simple wave function transforming into a square-wave periodic function. The figure shows how a simple sinusoidal curve can evolve into a square-wave periodic pattern by extending the terms of a sine-wave <strong>Fourier series</strong> function.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:signal"></span>
<img src="embed0044.png" alt="Sinusoids in Time (Periodic) Domain" width="80%" />
<p class="caption">
Figure 4.33: Sinusoids in Time (Periodic) Domain
</p>
</div>
<p>The phenomena can be explained by observing two waves, which, when combined, form a convolution of a larger wave. The total amplitude is the sum of the individual amplitude of the individual waves. The larger wave and its total amplitude is the physical manifestation of the <strong>superposition</strong> of two waves (two wave functions). If we review the individual signals in Figure <a href="numericalcalculus.html#fig:signal">4.33</a> with the fixed-frequency waves in Figure <a href="numericalcalculus.html#fig:sinosoidal1">4.34</a>, we can see that each signal is a <strong>linear combination</strong> of their corresponding set of frequencies.</p>
<p>For example, <strong>signal 6</strong> renders a periodic pattern that composes of six <strong>terms</strong> (or six <strong>frequencies</strong>) based on the following <strong>linear combination</strong>:</p>
<p><span class="math display">\[
F(x) = \frac{4}{\pi} \left( sin(x) + \frac{sin(3x)}{3} + \frac{sin(5x)}{5}  + \frac{sin(7x)}{7}  + \frac{sin(9x)}{9}  + \frac{sin(11x)}{11} \right)
\]</span></p>
<p>If we are to graph each of those six <strong>terms</strong>, we see the following Figure <a href="numericalcalculus.html#fig:sinosoidal1">4.34</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sinosoidal1"></span>
<img src="embed0045.png" alt="Sinusoidal Components in Frequency Domain" width="80%" />
<p class="caption">
Figure 4.34: Sinusoidal Components in Frequency Domain
</p>
</div>
<p>The sixth sinusoid - <strong>signal 6</strong> - is decomposed into six unique frequencies (or sinusoidal components). We see that the sinusoid has a <strong>frequency spectrum</strong> of six unique frequencies.</p>
<p>Notice also in Figure <a href="numericalcalculus.html#fig:signal">4.33</a> that each of the signals flows within its own <strong>time-domain</strong> in that each signal is broken into individual frequencies spread across its <strong>frequency-domain</strong> as shown in Figure <a href="numericalcalculus.html#fig:sinosoidal1">4.34</a>. Each of the <strong>unique frequencies</strong> is held into a <strong>frequency bin</strong>.</p>
<p>Let us further illustrate this by using Figure <a href="numericalcalculus.html#fig:freqspectrum1">4.35</a> which shows a 2D view of a periodic signal and Figure <a href="numericalcalculus.html#fig:freqspectrum2">4.36</a> which shows its 3D view.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freqspectrum1"></span>
<img src="embed0046.png" alt="Time and Frequency Domain" width="80%" />
<p class="caption">
Figure 4.35: Time and Frequency Domain
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freqspectrum2"></span>
<img src="embed0047.png" alt="Time and Frequency Domain" width="80%" />
<p class="caption">
Figure 4.36: Time and Frequency Domain
</p>
</div>
<p>In Figure <a href="numericalcalculus.html#fig:freqspectrum2">4.36</a>, the first <strong>ribbon</strong> (left in the figure) represents the composition of the next three <strong>ribbons</strong> which respectively represent the three frequencies in the <strong>frequency-domain</strong> . The first <strong>ribbon</strong> is expressed as:</p>
<p><span class="math display">\[
F(x) = sin(x) + \frac{sin(3x)}{3} + \frac{sin(5x)}{5}
\]</span></p>
<p>whereas the other three frequencies are as follows:</p>
<p><span class="math display">\[
F1(x) = sin(x)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
F2(x) = \frac{sin(3x)}{3} \ \ \ \ \ \ \ \  \ \ \ \ \ \ \ \
F3(x) = \frac{sin(5x)}{5}
\]</span></p>
<p>So far, we have shown one property of the <strong>Fourier Series</strong>, e.g., <strong>Linearity</strong>. However, other properties carry relatively advanced concepts around the <strong>Fourier Series and Transform</strong>. We leave readers to further investigate the other properties (including Linearity):</p>
<ul>
<li>linearity</li>
<li>time-shifting (in time-domain)</li>
<li>frequency shifting (in frequency-domain)</li>
<li>conjugation</li>
<li>convolution</li>
<li>multiplication</li>
</ul>
<p>It is notable to mention that <strong>Fourier series</strong> is a <strong>periodic</strong> function that deals with <strong>continuous</strong> signals in <strong>continuous-time</strong> <span class="citation">(Collings, I. <a href="bibliography.html#ref-ref2976c">2021</a>)</span>. Literatures may sometimes refer to this as <strong>Continuous-Time Fourier Series (CTFS)</strong>.  </p>
<p>Another function that deals with <strong>continuous</strong> signal is <strong>Fourier transform</strong>; however, this function is <strong>aperiodic</strong> - the signal does not repeat <span class="citation">(Collings, I. <a href="bibliography.html#ref-ref2976c">2021</a>)</span>. Other pieces of literature may sometimes refer to this as <strong>Continuous-Time Fourier Transform (CTFT)</strong>. This type of transform applies around signal compression or decompression.  </p>
<p>Now, similar to periodic <strong>CTFS</strong> and aperiodic <strong>CTFT</strong> in continuous-time, we have the counter-part signals in discrete-time in the form of periodic <strong>Discrete-Time Fourier Series (DTFS)</strong> and aperiodic <strong>Discrete-Time Fourier Transform (DTFT)</strong> <span class="citation">(Collings, I. <a href="bibliography.html#ref-ref2976c">2021</a>)</span>.    </p>
<p>We leave readers to investigate the differences between the signals mentioned above.</p>
<p>Now, in terms of signals in discrete time, our focus in the next section is on computing discrete signals in the form of <strong>Discrete Fourier Transform (DFT)</strong>. Instead of dealing with <strong>analog signal</strong>, we sample signal in <strong>finite</strong> <strong>discrete-time</strong>. By <strong>discretizing</strong> this way, we approximate the function that may closely represent the actual <strong>analog signal</strong>. We perform <strong>transformation</strong>during the process, which deals with transforming signals from their <strong>time-domain</strong> representation into their <strong>frequency-domain</strong> representation and vice-versa.</p>
<div id="discrete-fourier-transform-dft" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.7.1</span> Discrete Fourier Transform (DFT)  <a href="numericalcalculus.html#discrete-fourier-transform-dft" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We showed that complex signals may come as an aggregation (superposition) of individual simple frequencies. If we decompose signals into their frequencies, we require some function or operation to do so - or the mathematical equation it represents. <strong>DFT</strong> operation can help here. It describes the correlation between the signal and some <strong>oscillating sinusoidal</strong> frequency.</p>
<p>A signal-processing equation can represent a <strong>DFT</strong> operation which approximates frequencies given a signal. The function decomposes complex signals into simpler finite individual frequencies. Take Figure <a href="numericalcalculus.html#fig:sinosoidal2">4.37</a> as an example with a given unknown digital signal.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sinosoidal2"></span>
<img src="embed0048.png" alt="Unknown Digital Signal" width="80%" />
<p class="caption">
Figure 4.37: Unknown Digital Signal
</p>
</div>
<p>The <strong>unknown digital signal</strong> is generated from the below equation:</p>
<p><span class="math display">\[
F(x) = sin(x) + \frac{sin(3x)}{3} + \frac{sin(5x)}{5} 
\]</span></p>
<p>But if we only receive the signal as a dataset of samples, we need to approximate the solution that can closely match the signal. We do need to decipher the signal and approximate its frequency contents. To do that, we need to be familiar with the following <strong>DFT</strong> equation (<span class="math inline">\(\ref{eqn:eqnnumber6111}\)</span>):</p>
<p>From infinite continuous-time form (e.g., an analog signal) - in the time-domain:</p>
<p><span class="math display">\[\begin{align}
X(f) = \int_{-\infty}^{\infty}  f(t) e^{-j2\pi ft} dt
\end{align}\]</span></p>
<p>To finite discrete-time form (e.g.Â digital signal) - in the frequency-domain (see Figure <a href="numericalcalculus.html#fig:frequencydomain">4.39</a>):</p>
<p><span class="math display">\[\begin{align}
X_m = \sum_{k=0}^{N-1}  x_k e^{\frac{1}{N}(-j 2\pi km)} = \sum_{k=0}^{N-1}  x_k \omega_N^{km} \label{eqn:eqnnumber6111}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\omega_N\)</span> signifies the fundamental frequency.</p>
<p><span class="math display">\[\begin{align}
\omega_N = e^{\frac{1}{N}(-j2\pi)}\ \ \ \ or\ \ \ \rightarrow \omega_N^{km} = e^{\frac{1}{N}(-j2\pi km)}
\end{align}\]</span></p>
<p>and the frequency interval (space between samples) is <span class="math inline">\(\frac{2\pi}{N}\)</span>.</p>
<p>Note that <span class="math inline">\(2\pi\)</span> represents one <strong>period</strong>, which is one cycle (assume sine wave) before the value repeats, and <strong>N</strong> represents the number of sample points to capture within that period.</p>
<p>We recognize that if we do not sample the signal correctly in terms of spacing, the correlation between our signal and frequency may not correctly align. We encourage readers to investigate the sampling rate in terms of spacing using <strong>Nyquist rate and Nyquist frequency</strong>.  </p>
<p>Note that <span class="math inline">\(\omega_N\)</span> is an <strong>analyzing function</strong> or <strong>basis function</strong> that handles the <strong>sinusoid</strong> pattern, denoted by the below <strong>Eulerâs identity</strong>: </p>
<p><span class="math display">\[\begin{align}
e^{j\theta } = (cos(\theta) + j\ sin(\theta))\ \ \ \ \rightarrow\ \ \ \ \ \ \ 
where:  \theta = \frac{-2\pi k m}{N}
\end{align}\]</span></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:euleridentity"></span>
<img src="complexexponential.png" alt="Euler's Identity" width="60%" />
<p class="caption">
Figure 4.38: Eulerâs Identity
</p>
</div>
<p>Figure <a href="numericalcalculus.html#fig:euleridentity">4.38</a> shows a geometric view of Eulerâs identity (image reference: Rick Lyons <span class="citation">(<a href="bibliography.html#ref-ref1407r">2013</a>)</span>).</p>
<p>Also, if we use <strong>Sine</strong> and <strong>Cosine</strong> functions, it removes us from dealing with complex exponentials. No matter what the case, however, <strong>DFT</strong> translates (or transforms) the <strong>signal</strong> into a spectrum of frequencies - this is for us to perform further <strong>spectral analysis of the frequencies</strong>. Figure <a href="numericalcalculus.html#fig:frequencydomain">4.39</a> shows the <strong>frequency-domain</strong> that contains frequencies of our unknown signals in Figure <a href="numericalcalculus.html#fig:sinosoidal2">4.37</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:frequencydomain"></span>
<img src="embed0049.png" alt="Frequency Domain" width="80%" />
<p class="caption">
Figure 4.39: Frequency Domain
</p>
</div>
<p>To illustrate, let us work on a sampling of four real numbers from a digital signal:</p>
<p><span class="math display">\[
x \in \{\ 0.00,\ 0.10,\ 0.40,\ 0.10\ \}\ \ \ \ \ where\ k = \{0,1,2,3\}
\]</span></p>
<p>Using the <strong>DFT</strong> equation (<span class="math inline">\(\ref{eqn:eqnnumber6111}\)</span>) where <strong>N</strong> is the number of samples in the <strong>time-domain</strong>, e.g.Â N=4, our goal here is to solve for all the <strong>DFT coefficients</strong>, <span class="math inline">\(\mathbf{\{ X_1\ ... \ X_m \}}\)</span>. Here, we start by first solving <span class="math inline">\(\mathbf{X_1}\)</span>. Then iteratively, solving for the other coefficients.</p>
<p>For that, it does help to show the equation one more time:</p>
<p><span class="math display">\[\begin{align}
X_m = \sum_{k=0}^{N-1}  x_k e^{\frac{1}{N}(-j 2\pi km)},
\end{align}\]</span></p>
<p><strong>First</strong>, we solve for <span class="math inline">\({X_1}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
X_1 {}&amp;= 
0.00 \cdotp e^{\frac{1}{4}(-j2\pi (0) (0))} +
0.10 \cdotp e^{\frac{1}{4}(-j2\pi (1) (0))} + \\
&amp;\ 0.40 \cdotp e^{\frac{1}{4}(-j2\pi (2) (0))} +
0.10 \cdotp e^{\frac{1}{4}(-j2\pi (3) (0))} \\
\end{align*}\]</span>
</p>
<p>Next, we use <strong>Eulerâs identity</strong> to further solve <span class="math inline">\(\mathbf{X_1}\)</span>. Perform substitution for <span class="math inline">\(e^{j\theta}\)</span>.</p>

<p><span class="math display">\[\begin{align*}
X_1 {}&amp;= 
0.00 \left[cos\left(-\frac{\pi(0)(0)}{2}\right) + j\ sin\left(-\frac{\pi(0)(0)}{2}\right) \right] \\
&amp;+ 0.10 \left[cos\left(-\frac{\pi(1)(0)}{2}\right) + j\ sin\left(-\frac{\pi(1)(0)}{2}\right) \right] \\
&amp;+ 0.40 \left[cos\left(-\frac{\pi(2)(0)}{2}\right) + j\ sin\left(-\frac{\pi(2)(0)}{2}\right) \right] \\
&amp;+ 0.10 \left[cos\left(-\frac{\pi(3)(0)}{2}\right) + j\ sin\left(-\frac{\pi(3)(0)}{2}\right) \right] \\
\\
&amp;=0.00(1+0i) + 0.10(1+0i) + 0.40(1+0i) + 0.10(1+0i) 
\\
&amp;= 0.60 + 0j
\end{align*}\]</span>
</p>
<p><strong>Second</strong>, we repeat the process for the rest of the <strong>frequency bins</strong>. In doing so, we get the following <strong>DFT coefficients</strong>:</p>
<p><span class="math display">\[
X_1 =  0.60+0j,\ \ \ \ X_2 = -0.40-0j,\ \ \ \ \ X_3 =  0.20+0j,\ \ \ \ X_4 = -0.40-0j
\]</span></p>
<p>To save time from the long process, as illustrated in the first step, we can rely on a <strong>DFT matrix</strong> form ( using the equation with exponential terms rather than the cosine-sine terms):</p>
<p><span class="math display">\[
\left[
\begin{array}{llllll}
\omega_n^0 &amp; \omega_n^0 &amp; \omega_n^0 &amp; \omega_n^0 &amp; \cdots &amp; \omega_n^0\\
\omega_n^0 &amp; \omega_n^1 &amp; \omega_n^2 &amp; \omega_n^3 &amp; \cdots &amp; \omega_n^{N-1}  \\
\omega_n^0 &amp; \omega_n^2 &amp; \omega_n^4 &amp; \omega_n^6 &amp; \cdots &amp; \omega_n^{N-2}  \\
\omega_n^0 &amp; \omega_n^3 &amp; \omega_n^6 &amp; \omega_n^9  &amp; \cdots &amp; \omega_n^{N-3}  \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\omega_n^0 &amp; \omega_n^{N-1} &amp; \omega_n^{N-2} &amp; \omega_n^{N-3}  &amp; \cdots &amp; \omega_n^1  \\
\end{array}
\right] 
\left[\begin{array}{c} x_0 \\ x_1 \\ x_2 \\ x_3 \\ \vdots \\ x_{N-1} \end{array}\right]
= \left[ \begin{array}{c} X_1 \\ X_2 \\ X_3 \\ X_4 \\ \vdots \\ X_m \end{array}\right]
\]</span></p>
<p>Using our sample dataset, we fill up our matrix equation with the following:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrrr}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; \omega_4^1 &amp; \omega_4^2 &amp; \omega_4^3 \\
1 &amp; \omega_4^2 &amp; \omega_4^4 &amp; \omega_4^6 \\
1 &amp; \omega_4^3 &amp; \omega_4^6 &amp; \omega_4^9 \\
\end{array}
\right] 
\left[ \begin{array}{r} 0.00 \\ 0.10 \\ 0.40 \\ 0.10  \end{array}\right]
= \left[ \begin{array}{c} X_1 \\ X_2 \\ X_3 \\ X_4  \end{array}\right]
\]</span></p>
<p><span class="math display">\[
\downarrow
\]</span></p>
<p><span class="math display">\[
\left[
\begin{array}{rrrr}
1 &amp; 1 &amp; 1 &amp; 1 \\
1 &amp; -j &amp; -1 &amp; j \\
1 &amp; -1 &amp; 1 &amp; -1 \\
1 &amp; j &amp; -1 &amp; -j \\
\end{array}
\right] 
\left[ \begin{array}{r} 0.00 \\ 0.10 \\ 0.40 \\ 0.10  \end{array}\right]
= \left[ \begin{array}{r} 0.60+0j \\ -0.40-0j \\ 0.20+0j \\ -0.40-0j  \end{array}\right]
\]</span></p>
<p>Note that the <strong>imaginary part in complex numbers</strong> is represented as either <strong>j</strong> or <strong>i</strong>. Other pieces of Engineering literature may use <strong>j</strong>. However, we may interchangeably use <strong>j</strong> or <strong>i</strong>. For example, in R code, we may have to use <strong>i</strong>, but in this book, we use <strong>j</strong>.</p>
<p>Here is a naive implementation of <strong>DFT</strong> in R code:</p>

<div class="sourceCode" id="cb187"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb187-1" data-line-number="1">dft_exponential &lt;-<span class="cf">function</span>(x, M) {</a>
<a class="sourceLine" id="cb187-2" data-line-number="2">    N =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb187-3" data-line-number="3">    X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, M))</a>
<a class="sourceLine" id="cb187-4" data-line-number="4">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>(M<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb187-5" data-line-number="5">        X[m<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb187-6" data-line-number="6">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb187-7" data-line-number="7">            w =<span class="st"> </span>( <span class="dv">-2</span><span class="op">*</span>pi<span class="op">*</span>k<span class="op">*</span>m ) <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb187-8" data-line-number="8">            X[m<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>X[m<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x[k<span class="op">+</span><span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>( <span class="kw">exp</span>(1i<span class="op">*</span>w) )</a>
<a class="sourceLine" id="cb187-9" data-line-number="9">        }</a>
<a class="sourceLine" id="cb187-10" data-line-number="10">    }</a>
<a class="sourceLine" id="cb187-11" data-line-number="11">    <span class="kw">list</span>(<span class="st">&quot;DFT_Exponential&quot;</span>=X)</a>
<a class="sourceLine" id="cb187-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb187-13" data-line-number="13">dft_cosine_sine &lt;-<span class="cf">function</span>(x, M) {</a>
<a class="sourceLine" id="cb187-14" data-line-number="14">    N =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb187-15" data-line-number="15">    X =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, M))</a>
<a class="sourceLine" id="cb187-16" data-line-number="16">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>(M<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb187-17" data-line-number="17">        X[m<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb187-18" data-line-number="18">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>(N<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb187-19" data-line-number="19">            w =<span class="st"> </span>( <span class="dv">-2</span><span class="op">*</span>pi<span class="op">*</span>k<span class="op">*</span>m ) <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb187-20" data-line-number="20">            X[m<span class="op">+</span><span class="dv">1</span>] =<span class="st"> </span>X[m<span class="op">+</span><span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>x[k<span class="op">+</span><span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>( <span class="kw">cos</span>(w) <span class="op">+</span><span class="st"> </span>1i<span class="op">*</span><span class="kw">sin</span>(w))</a>
<a class="sourceLine" id="cb187-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb187-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb187-23" data-line-number="23">    <span class="kw">list</span>(<span class="st">&quot;DFT_cosine_sine&quot;</span>=X)</a>
<a class="sourceLine" id="cb187-24" data-line-number="24">}</a>
<a class="sourceLine" id="cb187-25" data-line-number="25">x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="fl">0.10</span>,<span class="fl">0.40</span>,<span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb187-26" data-line-number="26"><span class="kw">dft_exponential</span>(x, <span class="kw">length</span>(x))</a></code></pre></div>
<pre><code>## $DFT_Exponential
##         [,1]
## [1,]  0.6+0i
## [2,] -0.4-0i
## [3,]  0.2+0i
## [4,] -0.4-0i</code></pre>
<div class="sourceCode" id="cb189"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb189-1" data-line-number="1">(<span class="dt">X=</span><span class="kw">dft_cosine_sine</span>(x, <span class="kw">length</span>(x)))</a></code></pre></div>
<pre><code>## $DFT_cosine_sine
##         [,1]
## [1,]  0.6+0i
## [2,] -0.4-0i
## [3,]  0.2+0i
## [4,] -0.4-0i</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dftcode"></span>
<img src="embed0050.png" alt="DFT" width="80%" />
<p class="caption">
Figure 4.40: DFT
</p>
</div>

<p><strong>Finally</strong>, given the <strong>DFT coefficients</strong>, let us approximate a sinusoidal equation that can match the original signal. Here, we use the following inverse equation (<strong>Inverse Discrete Fourier Transform (IDFT)</strong>):</p>
<p><span class="math display">\[\begin{align}
x_k = \frac{1}{L} \sum_{m=1}^{L} X_m e^{\frac{1}{L}(j 2\pi km)}
\end{align}\]</span></p>
<p>where <strong>L</strong> is the number of frequencies in the <strong>frequency-domain</strong>. Assume, we have <span class="math inline">\(\mathbf{L=4}\)</span>.</p>

<p><span class="math display">\[\begin{align*}
x_n {}&amp;= \frac{1}{4} \left[]
      X_1\cdotp e^{\frac{1}{4}j2\pi (1)(n)} +
      X_2\cdotp e^{\frac{1}{4}j2\pi (2)(n)} +
      X_3\cdotp e^{\frac{1}{4}j2\pi (3)(n)} +
      X_4\cdotp e^{\frac{1}{4}j2\pi (4)(n)} \right] \\
 &amp;=  \frac{1}{4} \left[
      X_1\cdotp e^{\frac{1}{2}j\pi n}  +
      X_2\cdotp e^{j\pi n} +
      X_3\cdotp e^{\frac{3}{2}j\pi n} +
      X_4\cdotp e^{j2\pi n} \right] \\
 &amp;=  \frac{1}{4} \left[
      0.60 \cdotp e^{\frac{1}{2}j\pi n}  +
     -0.40 \cdotp e^{j\pi n} +
      0.20 \cdotp e^{\frac{3}{2}j\pi n} +
     -0.40 \cdotp e^{j2\pi n} \right] \\
 &amp;=   0.15 \cdotp e^{\frac{1}{2}j\pi n} +
     -0.10 \cdotp e^{j\pi n} +
      0.05 \cdotp e^{\frac{3}{2}j\pi n} +
     -0.10 \cdotp e^{j2\pi n}  \\
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
x_n &amp;=  0.15 \left(cos(\frac{1}{2}\pi n) + j sin(\frac{1}{2}\pi n)\right)  
     + -0.10 \left(cos(\pi n) + j sin(\pi n)\right)  \\
    &amp;+  0.05 \left(cos(\frac{3}{2}\pi n) + j sin(\frac{3}{2}\pi n)\right) 
     + -0.10 \left(cos(2\pi n) + j sin(2\pi n)\right) 
\end{align*}\]</span>
</p>
<p>There are cases when the <strong>DFT coefficients</strong> may not correctly approximate the <strong>frequencies</strong> of the <strong>signal</strong> if we do not choose the correct <strong>Nyquist frequency and rate</strong>. Here, both parameters can determine the <strong>critical points</strong>. We can then take a signal sampling to represent the frequencies better given these critical points; otherwise, the <strong>basis function</strong> may render a different sinusoidal wave (oscillation) not representative of the frequency that correlates to the signal. We leave the topic on <strong>Nyquist</strong> for the readers to explore further.</p>
</div>
<div id="inverse-discrete-fourier-transformation-idft" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.7.2</span> Inverse Discrete Fourier Transformation (IDFT)  <a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>As a reference, this section is a quick outline of the equations for <strong>DFT</strong> and its inverse which we have already covered in the previous section. We use the following equations for the inverse of DFT.</p>
<p>From continuous form (e.g.Â analog signal) - in time-domain:</p>

<p><span class="math display">\[\begin{align}
X(f) = \int_{-\infty}^{\infty}  f(t) e^{-j2\pi ft} dt\ \ \rightarrow inverse\ \ \rightarrow
f(t) = \int_{-\infty}^{\infty} X(f) e^{-j2\pi ft} df
\end{align}\]</span>
</p>
<p>To discrete form (e.g.Â digital signal) - in the frequency-domain:</p>

<p><span class="math display">\[\begin{align}
X_m = \sum_{k=0}^{N-1}  x_k e^{\frac{1}{N}(-j 2\pi km)} 
\ \ \rightarrow inverse\ \ \rightarrow
x_k = \frac{1}{N} \sum_{m=0}^{N-1} X_m e^{\frac{1}{N}(j 2\pi km)}
\end{align}\]</span>
</p>
</div>
<div id="fast-fourier-transform-fft" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.7.3</span> Fast Fourier Transform (FFT)  <a href="numericalcalculus.html#fast-fourier-transform-fft" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>FFT</strong> is a numerically faster and more efficient way of calculating <strong>DFT</strong>, rendering <span class="math inline">\(O(N log_2 N)\)</span> operations compared to <span class="math inline">\(O(N^2)\)</span> operations for the <strong>DFT</strong> function. While there are <strong>FFT</strong> algorithms introduced in other pieces of literature such as <strong>Prime-factor</strong>, <strong>Rader</strong>, and <strong>Bruun</strong>, here in this section, we use the <strong>Cooley-Tukey</strong> algorithm (James Cooley and John Tukey [1965]) which implements a divide-and-conquer approach.</p>
<p>Recall the following equation:</p>
<p><span class="math display">\[\begin{align}
X_m = \sum_{k=0}^{N-1}  x_k e^{\frac{1}{N}(-j 2\pi km)} = \sum_{k=0}^{N-1}  x_k \omega_N^{km}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\omega_N\)</span> signifies the fundamental frequency (the twiddle factor).</p>
<p><span class="math display">\[\begin{align}
\omega_N = e^{\frac{1}{N}(-j2\pi)}\ \ \ \ or\ \ \ \rightarrow \omega_N^{km} = e^{\frac{1}{N}(-j2\pi km)}
\end{align}\]</span></p>
<p>The idea in <strong>FFT</strong> is to divide the signal data into two groups: the even group and the odd group. For example, given the following signal data:</p>
<p><span class="math display">\[
x = \{0.00, 0.10, 0.40, 0.10 \},
\]</span></p>
<p>we separate the data that fall under odd indices from the data that fall under the even indices. Therefore, we get:</p>
<p><span class="math display">\[
odd = \{0.00, 0.40\},\ \ \ \ \ \ \ \ \ even = \{ 0.10, 0.10\}
\]</span></p>
<p>We first perform the first recursive step. We split into even groups and odd groups, then compute.</p>

<p><span class="math display">\[\begin{align*}
\hat{X_0} {}&amp;= x_1 + x_3 \cdot\omega^0 = \ \ 0.2+0j\ \leftarrow \text{odd} \\
\hat{X_1} &amp;= x_1 - x_3 \cdot\omega^1 = \ \ 0.0+0j\ \leftarrow \text{odd} \\
\hat{X_2} &amp;= x_0 + x_2 \cdot\omega^0 = \ \ 0.4+0j\ \leftarrow \text{even} \\
\hat{X_3} &amp;= x_0 - x_2 \cdot\omega^1 = -0.4+0j\ \leftarrow \text{even} \\
\end{align*}\]</span>
</p>
<p>Then we perform the second recursive step:</p>

<p><span class="math display">\[\begin{align*}
\hat{X_0} + \hat{X_2} {}&amp;= X_0 \\
\hat{X_1} + \hat{X_3} &amp;= X_1\\
\hat{X_0} - \hat{X_2} &amp;= X_2\\
\hat{X_1} - \hat{X_3} &amp;= X_3
\end{align*}\]</span>
</p>
<p>If we simplify, we get the following equations:</p>
<p><span class="math display">\[\begin{align*}
X_0 {}&amp;= (x_1 + x_3 \cdot\omega^0) + (x_0 + x_2 \cdot\omega^0) = (x_0 + x_1) + \omega^0(x_2 + x_3) \\
X_1 &amp;= (x_1 - x_3 \cdot\omega^1) + (x_0 - x_2 \cdot\omega^1) = (x_0 + x_1) - \omega^1(x_2 + x_3)   \\
X_2 &amp;= (x_1 + x_3 \cdot\omega^0) - (x_0 + x_2 \cdot\omega^0) = (x_1 - x_0) + \omega^0(x_3 - x_2)   \\
X_3 &amp;= (x_1 - x_3 \cdot\omega^1) - (x_0 - x_2 \cdot\omega^1) = (x_1 - x_0) - \omega^1(x_3 - x_2)   \\
\end{align*}\]</span></p>
<p>Here is a naive implementation of <strong>FFT</strong> in R code using <strong>Cooley-Tukey</strong> algorithm: </p>

<div class="sourceCode" id="cb191"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb191-1" data-line-number="1">fft &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="co"># Signal Vector</span></a>
<a class="sourceLine" id="cb191-2" data-line-number="2">    N =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb191-3" data-line-number="3">    <span class="cf">if</span> (N <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) <span class="kw">return</span>(x)    </a>
<a class="sourceLine" id="cb191-4" data-line-number="4">    <span class="cf">if</span> (N <span class="op">%%</span><span class="st"> </span><span class="dv">2</span>) { x =<span class="st"> </span><span class="kw">append</span>(x, <span class="dv">0</span>); N =<span class="st"> </span>N <span class="op">+</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb191-5" data-line-number="5">    X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, N, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb191-6" data-line-number="6">    odd =<span class="st"> </span>x[<span class="kw">c</span>(<span class="ot">FALSE</span>, <span class="ot">TRUE</span>)]</a>
<a class="sourceLine" id="cb191-7" data-line-number="7">    even =<span class="st"> </span>x[<span class="kw">c</span>(<span class="ot">TRUE</span>,<span class="ot">FALSE</span>)]    </a>
<a class="sourceLine" id="cb191-8" data-line-number="8">    X_e =<span class="st"> </span><span class="kw">fft</span>(even)</a>
<a class="sourceLine" id="cb191-9" data-line-number="9">    X_o =<span class="st"> </span><span class="kw">fft</span>(odd)</a>
<a class="sourceLine" id="cb191-10" data-line-number="10">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(N<span class="op">/</span><span class="dv">2</span>)) {</a>
<a class="sourceLine" id="cb191-11" data-line-number="11">        omega =<span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>2i<span class="op">*</span>pi<span class="op">*</span>(k<span class="dv">-1</span>)<span class="op">/</span>N)  </a>
<a class="sourceLine" id="cb191-12" data-line-number="12">        X[k] =<span class="st"> </span>X_e[k] <span class="op">+</span><span class="st">  </span>X_o[k] <span class="op">*</span><span class="st"> </span>omega</a>
<a class="sourceLine" id="cb191-13" data-line-number="13">        X[k <span class="op">+</span><span class="st"> </span>(N<span class="op">/</span><span class="dv">2</span>)] =<span class="st"> </span>X_e[k] <span class="op">-</span><span class="st">  </span>X_o[k] <span class="op">*</span><span class="st"> </span>omega</a>
<a class="sourceLine" id="cb191-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb191-15" data-line-number="15">    <span class="kw">return</span>( X )</a>
<a class="sourceLine" id="cb191-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb191-17" data-line-number="17">x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.00</span>, <span class="fl">0.10</span>, <span class="fl">0.40</span>, <span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb191-18" data-line-number="18"><span class="kw">fft</span>(x)</a></code></pre></div>
<pre><code>## [1]  0.6+0i -0.4+0i  0.2+0i -0.4+0i</code></pre>

<p>The R code implementation of the <strong>Cooley-Tukey</strong> algorithm shows a recursive way of dividing the signal data and computing for the frequency. We can also represent the algorithm using a <strong>Butterfly Diagram</strong>. See Figure <a href="numericalcalculus.html#fig:cooleytukey">4.41</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cooleytukey"></span>
<img src="cooleytukey.png" alt="Cooley-Tukey (FFT) Algorithm" width="60%" />
<p class="caption">
Figure 4.41: Cooley-Tukey (FFT) Algorithm
</p>
</div>
<p>There are other <strong>Fourier Transforms</strong> published in other pieces of literature that are notable for investigation. We leave three of them for readers to investigate:</p>
<ul>
<li>Sparse Fourier Transform (SFT)</li>
<li>Discrete Tchebichef (Chebyshev) Transform (DTT)</li>
<li>Fractional Fourier Transform (FrFT)</li>
</ul>
</div>
</div>
<div id="summary-2" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.8</span> Summary<a href="numericalcalculus.html#summary-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter discussed Integration, Ordinary Differential Equations, Partial Differential Equations, and Fourier Series. These essential topics have become a classic standard part of most, if not all, Calculus syllabi. As the next step, we leave readers to take a further reading of such topics in detail. Other great references include Olsen-Kettle L. <span class="citation">(<a href="bibliography.html#ref-ref667l">2011</a>)</span>, Lewis A.D <span class="citation">(<a href="bibliography.html#ref-ref658a">2017</a>)</span>, Jakobsen P.K. <span class="citation">(<a href="bibliography.html#ref-ref638p">2019</a>)</span>, and Ivrii V. <span class="citation">(<a href="bibliography.html#ref-ref628v">2021</a>)</span>.</p>
<p>As shown, <strong>Numerical methods</strong> deal with approximations. We illustrated some of the classic approaches to approximate the behavior of dynamic systems. We also demonstrated how to arrive at a steady state of systems from an initial dynamic state and vice-versa.</p>
<p>In volume II of this book, we begin to discuss <strong>approximations</strong> in the context of <strong>Stochastic methods</strong>, covering three major topics, namely <strong>Probability</strong>, <strong>Statistical Computation</strong>, and <strong>Bayesian Computation</strong>.</p>


</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>wikipedia: <a href="https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas" class="uri">https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas</a><a href="numericalcalculus.html#fnref1" class="footnote-back">â©</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="numericallinearalgebra.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="numericalprobability.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
