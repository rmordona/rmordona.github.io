<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>9.5 Exploratory Data Analysis | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="9.5 Exploratory Data Analysis | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="9.5 Exploratory Data Analysis | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-19" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="9.4-distance-metrics.html"/>
<link rel="next" href="9.6-featureengineering.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exploratory-data-analysis" class="section level2 hasAnchor">
<h2><span class="header-section-number">9.5</span> Exploratory Data Analysis<a href="9.5-exploratory-data-analysis.html#exploratory-data-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Exploratory Data Analysis (EDA)</strong> aims to study the underlying structure of data and to discover patterns and associations, including effects and influences. We have covered the nature of data distribution in previous chapters. We also covered statistical and bayesian analysis around data.</p>
<p>This section introduces a few concepts in <strong>Data Mining</strong> to supplement our understanding of <strong>Exploratory Analysis</strong>. <strong>Data Mining</strong> is a superset course that includes <strong>Exploratory Data Analysis (EDA)</strong>.</p>
<p>Though there are many more topics in <strong>Data Mining</strong> that are not possible to cover in this book, let us frame the idea of <strong>Data Mining</strong> in the context of only the following topics:</p>
<ul>
<li>Data Cleaning</li>
<li>Association</li>
<li>Pattern Discovery</li>
<li>Null Invariance</li>
<li>Correlation and Collinearity</li>
<li>Covariance</li>
<li>Missingness and Imputation</li>
<li>Others (e.g., Outliers, Leverage, Influence, Data Leakage, and Confounding Variables)</li>
</ul>
<div id="data-cleaning-wrangling" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.1</span> Data Cleaning (Wrangling)  <a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Data Cleaning</strong> is essential before handling Exploratory Data Analysis methods. Specialists in Data Analysis offer best practices to address this area <span class="citation">(e.g., Osborne, J. W. <a href="bibliography.html#ref-ref1502j">2013</a>)</span>. Notwithstanding a thorough coverage, let us instead briefly list eight dimensions of Data Quality that can be used as a guide to know if our Data is <strong>Clean</strong>. Other literature may list only six dimensions of different combinations out of the eight dimensions. We reference articles that cover them <span class="citation">(Elgabry O. <a href="bibliography.html#ref-ref1502o">2019</a>; Gupta A. <a href="bibliography.html#ref-ref1509a">2021</a>)</span>:</p>
<ul>
<li>Accuracy - Data is as close to its true value.</li>
<li>Completeness - Data is as comprehensive.</li>
<li>Consistency - Values of the same Data are the same everywhere.</li>
<li>Validity - Data is within the prescribed constraints.</li>
<li>Timeliness - Data is available as required.</li>
<li>Uniformity - Data is uniform everywhere in terms of presentation.</li>
<li>Uniqueness - Data is not duplicated anywhere.</li>
<li>Integrity - Data references are intact (See Referential Integrity in Structured Data)</li>
</ul>
<p>The quality of data can also be improved during the gathering, filtering, and transforming (ETL) of data.</p>
</div>
<div id="association" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.2</span> Association<a href="9.5-exploratory-data-analysis.html#association" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To explain the intuition behind <strong>Association</strong>, suppose we shop online and make a few orders (corresponding to a few transactions recorded into our database). Each order has a list of ordered items. Our goal is to find the most frequent set of ordered items called <strong>itemset</strong> across a set of transactions. We can use any of three <strong>algorithms</strong> starting with <strong>Apriori Algorithm</strong> (note here that we refer to a set of transactions as <strong>transactional database</strong>).</p>
<p><strong>Apriori Algorithm</strong> </p>
<p>To explain <strong>Apriori Algorithm</strong>, let us use Figure <a href="9.5-exploratory-data-analysis.html#fig:apriori">9.23</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:apriori"></span>
<img src="apriori.png" alt="Apriori Algorithm" width="85%" />
<p class="caption">
Figure 9.23: Apriori Algorithm
</p>
</div>
<p>Figure <a href="9.5-exploratory-data-analysis.html#fig:apriori">9.23</a> illustrates the classic technique called <strong>Apriori algorithm</strong> used to find the most frequent itemsets <span class="citation">(Srikant, R., &amp; Agrawal, R. <a href="bibliography.html#ref-ref378">1996</a>)</span>. In the figure, we notice multiple scans. Each scan results in a table of itemsets with corresponding frequency. For example, in table C1, we see an itemset (group) that contains only item A. It shows that this item in the first itemset counts 3. As we go to the next scan, we only consider itemsets with a frequency equal to or greater than the minimum support. We then perform the same count, but this time, notice that table C2 has itemsets (groups) that contain two items.</p>
<p>The <strong>Apriori Algorithm</strong> is described below:</p>
<p><span class="math display">\[
\begin{array}{l}
L_1 = \text{\{frequent 1-itemset\}}\\
\text{For k in 2,...   repeat the following}\\
\ \ \ \ \ \ \ \ C_k\ \text{= candidate k-itemsets from }L_{k-1}\\
\ \ \ \ \ \ \ \ \text{For t in 1,... repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{tx = transaction[t]}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{For each k-itemset in tx, repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{count frequency}\\
\ \ \ \ \ \ \ \ L_k\ \text{= All candidate k-itemsets from } C_k \text{ with minimum support/frequency}\\
\end{array}
\]</span></p>
<p>Grouping items into itemsets based on frequency is one type of <strong>association</strong>, and it follows specific <strong>association rules</strong> as a measure for the effectiveness of the <strong>association</strong> (note that we cover more measures in the <strong>Null-Invariant</strong> section).</p>
<p><strong>Association rules</strong> </p>
<ul>
<li><strong>Support</strong> measures the frequency or number of occurrences of an itemset. An <strong>absolute support</strong> measures the number of occurrences. A <strong>relative support</strong> measures the fraction of the number of transactions in which the itemset occurs out of all transactions. For example, item <strong>A</strong> in Figure <a href="9.5-exploratory-data-analysis.html#fig:apriori">9.23</a> is a 1-itemset with absolute Support of 3 and relative Support of 60% because it shows occurrences in three transactions out of five transactions.</li>
</ul>
<p><span class="math display">\[ 
Relative\ Support = P(A) = S(A) = 
\frac{\text{transaction in which A occurs}}{\text{total transactions}}=  \frac{3}{5} = 0.60 
\]</span></p>
<ul>
<li><strong>Confidence</strong> measures the probability that an itemset, say a 2-itemset containing <strong>A</strong> and <strong>B</strong>, will occur conditioned on the occurrence of an itemset, say <strong>A</strong>. It measures the fraction of the number of transactions in which <strong>A</strong> and <strong>B</strong> occur out of all transactions in which <strong>A</strong> occurs. For example, in Figure <a href="9.5-exploratory-data-analysis.html#fig:apriori">9.23</a>, there are three transactions in which <strong>B</strong> occurs. There is one transaction in which both <strong>A</strong> and <strong>B</strong> occur. Therefore, calculating the <strong>confidence</strong> of <strong>A</strong> and <strong>B</strong> to occur together in the same transaction, conditioned on <strong>B</strong>, we have:</li>
</ul>
<p><span class="math display">\[\begin{align*}
Confidence &amp;= \frac{P(A,B)}{P(A)} = C(A \rightarrow B) = \frac{\text{transactions in which A and B occur}}{\text{transactions in which A occurs}} \\
&amp;= \frac{1}{3} = 0.33 
\end{align*}\]</span></p>
<ul>
<li><strong>Lift</strong> measures the ratio of confidence over expected confidence. For example, if <strong>A</strong> and <strong>B</strong> will occur conditioned on the occurrence of an item, say <strong>A</strong>; then the expected confidence is the relative support of <strong>B</strong>.</li>
</ul>
<p><span class="math display">\[
Lift = L(A,B) = \frac{P(A,B)}{P(A)P(B)} = 
\frac{C(A \rightarrow B)}{S(B)} = 
\frac{
\frac{\text{tx in which A and B occur}}{\text{tx in which B occurs}} 
}{
\frac{\text{tx in which B occurs}}{\text{total transactions}}
} =  \frac{ \frac{1}{3}}{\frac{3}{5}} = 0.56
\]</span></p>
<p>The candidate itemsets are selected based on the rules above. For example, in Figure <a href="9.5-exploratory-data-analysis.html#fig:apriori">9.23</a>, we have chosen a <strong>minimum support</strong> of 2 to limit the itemsets based on that frequency limit. The end result shows that our 3-itemset with the most number of occurrences is <span class="math inline">\(\{A, C, D\}\)</span>.</p>
<p>Note that <strong>Apriori algorithm</strong> requires a repeated scan of the transactional database to generate candidate itemsets. In the next portion of this section, we cover two other alternatives. One of the two algorithms eliminates the need to scan the database repeatedly.</p>
<p><strong>FP-growth Algorithm</strong> </p>
<p>The <strong>Frequent Pattern (FP) growth algorithm</strong>, also called <strong>FP-Growth algorithm</strong>, is proposed by Han J. et al. <span class="citation">(<a href="bibliography.html#ref-ref368">2000</a>)</span> in that it avoids candidate generation found in <strong>Apriori algorithm</strong> when generating frequent itemsets. It scans transactions in a database to arrange the candidate itemsets in descending order and then makes one final scan to construct a compact FP-tree structure based on the ordered itemsets. See Figure <a href="9.5-exploratory-data-analysis.html#fig:fpgrowth">9.24</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fpgrowth"></span>
<img src="fpgrowth.png" alt="FP-Growth Data" width="85%" />
<p class="caption">
Figure 9.24: FP-Growth Data
</p>
</div>
<p>The algorithm proceeds as follows (granting our minimum support equals 2):</p>
<p><strong>First</strong>, generate a frequent 1-itemset by scanning the transaction database.</p>
<p><span class="math display">\[
\text{\{O:5\},\{A:4\},\{C:4\},\{R:4\},\{D:2\},\{E:2\},\{P:2\},\{B:2\},\{F:2\}}
\]</span></p>
<p><strong>Second</strong>, generate an <strong>ordered</strong> list of k-itemsets by re-scanning the transaction database for each transaction based on the 1-itemset generated. See the first table in Figure <a href="9.5-exploratory-data-analysis.html#fig:fpgrowth">9.24</a> for the <strong>(Ordered) itemsets</strong>.</p>
<p><strong>Third</strong> with the <strong>ordered</strong> k-itemsets, construct the <strong>FP-tree</strong>. For example, starting with the first transaction <strong>TID=101</strong>, we create a tree with the <strong>Null {}</strong> node being the root. </p>
<p><span class="math display">\[
\text{\{\}}\rightarrow \text{\{O:1\}}\rightarrow \text{\{A:1\}}
\rightarrow \text{\{C:1\}}\rightarrow \text{\{R:1\}}\rightarrow \text{\{B:1\}}
\]</span></p>
<p>Then, we read the second transaction <strong>TID=102</strong> and continue to build the tree.</p>
<p><span class="math display">\[
\text{\{\}}\rightarrow \text{\{O:2\}}\rightarrow \text{\{A:2\}}
\underset{\text{\{D:1\}}\rightarrow \text{\{E:1\}}\rightarrow \text{\{P:1\}}\rightarrow \text{\{B:1\}}}{ \underset{\downarrow}{ \rightarrow \text{\{C:2\}}\rightarrow \text{\{R:2\}}\rightarrow \text{\{B:1\}}}}
\]</span></p>
<p>Notice that while we traverse the tree, we increment the node corresponding to each occurring item in the current transaction. We also branch out to a new node if an occurring item reaches a node with no other neighboring node corresponding to the item. In our case, for the second transaction, {R:2} is the last matching node. So we branch out by creating a new node, namely {D:1}, and proceed with building the branch.</p>
<p>Then, we read the third transaction <strong>TID=103</strong> and continue to build the tree:</p>
<p><span class="math display">\[
\underset{\{D:1\}\rightarrow \{E:1\} \rightarrow \{P:1\} }{ \underset{\downarrow}{ \text{\{\}} \rightarrow \text{\{O:3\}}\rightarrow }}
 \text{\{A:2\}}
\underset{\text{\{D:1\}}\rightarrow \text{\{E:1\}}\rightarrow \text{\{P:1\}}\rightarrow \text{\{B:1\}}}{ \underset{\downarrow}{ \rightarrow \text{\{C:2\}}\rightarrow \text{\{R:2\}}\rightarrow \text{\{B:1\}}}}
\]</span></p>
<p>Here, we create a new branch from {O:3}.</p>
<p>We continue processing the next transactions. The <strong>FP-tree</strong> is shown in Figure <a href="9.5-exploratory-data-analysis.html#fig:fpgrowth">9.24</a>. </p>
<p><strong>Fourth</strong>, we create a <strong>conditional pattern base</strong> table column. We use the <strong>FP-tree</strong> to populate the conditional pattern base. For each of the 1-itemset, we traverse the tree to the root and build the k-itemset based on the traversed path. For example, the 1-itemset <strong>{F}</strong> forms a path to the root, namely <strong>{O, A, C, R:2}</strong>. That is a new 4-itemset. The number <strong>2</strong> in the new 4-itemset corresponds to the number in the node of the 1-itemset <strong>{F}</strong>, namely <strong>{F:2}</strong>. In another example, there are two paths for 1-itemset <strong>{E}</strong>. The first one forms a path to the root, namely <strong>{O, A, C, R, D:1}</strong>, and the second one forms another path to the root, namely <strong>{O, D:1}</strong>.</p>
<p><strong>Finally</strong>, we use the <strong>conditional pattern base</strong> column to generate our <strong>frequent patterns</strong> by intersecting the ordered itemsets and its 1-itemset for each unique item:</p>

<p><span class="math display">\[\begin{align*}
\text{A-conditional = }&amp;\text{\{A,O:4\}} \\
\text{C-conditional = }&amp;\text{\{O,C:4\},\{A,C:4\},\{O,A,C:4\}}\\
\text{R-conditional = }&amp;\text{\{O,R:4\},\{A,R:4\},\{C,R:4\},\{O,A,R:4\},\{O,C,R:4\},\{O,A,C,R:4\}}\\
\text{D-conditional = }&amp;\text{\{O,D:2\}}\\
\text{E-conditional = }&amp;\text{\{O,E:2\},\{D,E:2\}, \{O,D,E:2\}}\\
\text{P-conditional = }&amp;\text{\{O,P:2\},\{D,P:2\},\{E,P:2\},\{O,D,P:2\},\{O,E,P:2\},,\{O,D,E,P:2\}}\\
\text{B-conditional = }&amp;\text{\{O,B:2\},\{A,B:2\},\{C,B:2\},\{R,B:2\},\{O,A,B:2\},\{O,C,B:2\},\{O,R,B:2\}}\\
&amp; \text{\{A,R,B:2\},\{O,A,C,B:2\},\{O,A,R,B:2\},\{A,C,R,B:2\},\{O,C,R,B:2\}}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
&amp;\text{\{O,A,C,R,B:2\}}\\
\text{F-conditional = }&amp;\text{\{O,F:2\},\{A,F:2\},\{C,F:2\},\{R,F:2\},\{O,A,F:2\},\{O,C,F:2\},\{O,R,F:2\}}\\
&amp;\text{\{A,R,F:2\},\{O,A,C,F:2\},\{O,A,R,F:2\},\{A,C,R,F:2\},\{O,C,R,F:2\}}\\
&amp;\text{\{O,A,C,R,F:2\}}
\end{align*}\]</span>
</p>
<p>For <strong>D-conditional</strong>, <strong>E-conditional</strong>, and <strong>P-conditional</strong>, we discard any pattern having any of the following items <strong>&lt;A, B, C&gt;</strong> because the patterns will generate a frequency below the minimum.</p>
<p>Similarly, for <strong>B-conditional</strong>, we discard any pattern having <strong>&lt;D, E, P&gt;</strong>.</p>
<p><strong>Eclat Algorithm</strong> </p>
<p>The <strong>Equivalence Class Transformation (Eclat) algorithm</strong> is a Depth First Search algorithm that uses a vertically transposed data format.</p>
<p>The algorithm proceeds as follows:</p>
<p><strong>First</strong>, get the transaction list for each item. For example, for item <strong>A</strong>, we have the following list of transactions: <strong>101, 103, 104</strong>. See Figure <a href="9.5-exploratory-data-analysis.html#fig:eclat">9.25</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:eclat"></span>
<img src="eclat.png" alt="Eclat Data" width="60%" />
<p class="caption">
Figure 9.25: Eclat Data
</p>
</div>
<p><strong>Second</strong>, list the frequently occurring patterns by intersecting the transaction list of each item with the transaction list of the other items. For example, suppose our minimum support equals 2. We then have the following intersection:</p>
<p><span class="math display">\[
\begin{array}{llllll}
A \cap B &amp;= \text{\{104\}} &amp; A \cap C \cap D &amp;= \text{\{104,103\}} &amp; A \cap C \cap D \cap E &amp;= \text{\{103\}}\\
A \cap C &amp;= \text{\{101,103\}} &amp; A \cap C \cap E &amp;= \text{\{103\}} \\
A \cap D &amp;= \text{\{101,103\}} &amp; A \cap D \cap E &amp;= \text{\{103\}} \\
A \cap E &amp;= \text{\{103\}}
\end{array}
\]</span></p>
<p><strong>Third</strong>, with minimum support of 2, we reduce the list to the following:</p>
<p><span class="math display">\[
\begin{array}{llll}
A \cap C &amp;= \text{\{101,103\}} &amp; A \cap C \cap D &amp;= \text{\{104,103\}}\\
A \cap D &amp;= \text{\{101,103\}}
\end{array} 
\]</span></p>
<p>Therefore the below itemsets are more likely to occur than other associations:</p>
<p><span class="math display">\[
\text{\{A,C\}, \{A,D\}, \{A,C,D\}}
\]</span>
We leave readers to investigate variants and enhancements made to the algorithms mentioned above, particularly in computational costs such as space savings by compression.</p>
</div>
<div id="pattern-discovery" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.3</span> Pattern Discovery<a href="9.5-exploratory-data-analysis.html#pattern-discovery" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we introduce algorithms that deal with items not only for their association but also to discover patterns that have sequential characteristics. Unlike a transaction database in which we have a list of transactions identified by transaction IDs, each containing an unordered list of items, we operate on a database with sequences identified by sequence IDs, each containing a sequentially-ordered list of items. We refer to a set of sequences as <strong>sequential database</strong>). See Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sequential"></span>
<img src="sequential.png" alt="Sequential Data" width="60%" />
<p class="caption">
Figure 9.26: Sequential Data
</p>
</div>
<p>For example, the sequence with ID equal to 102 has 7 elements.</p>

<p><span class="math display">\[
\text{&lt;(ABC)DEF(GH)(IJ)K&gt;}\ \ \rightarrow
\]</span>
<span class="math display">\[
\underbrace{
\underbrace{\text{&lt;(ABC)}}_{\begin{array}{c}\text{1st}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{
\underbrace{\text{D}}_{\begin{array}{c}\text{2nd}\\ \text{element}\end{array}}
\underbrace{\text{E}}_{\begin{array}{c}\text{3rd}\\ \text{element}\end{array}}
\underbrace{\text{F}}_{\begin{array}{c}\text{4th}\\ \text{element}\end{array}}
}_\text{(ordered)}
\underbrace{\text{(GH)}}_{\begin{array}{c}\text{5th}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{\text{(IJ)}}_{\begin{array}{c}\text{6th}\\ \text{element}\\ \text{(unordered)}\end{array}}
\underbrace{\text{K&gt;}}_{\begin{array}{c}\text{7th}\\ \text{element}\\ \text{(ordered)}\end{array}}
}_\text{(ordered)} 
\]</span>
</p>
<p>Note the below alternative longer notation:</p>
<p><span class="math display">\[
\text{&lt;(ABC)DEF(GH)(IJ)K&gt;} \equiv
\text{&lt;(ABC)}\rightarrow
\text{D}\rightarrow\text{E}\rightarrow\text{F}\rightarrow\text{(GH)}\rightarrow\text{(IJ)}\rightarrow\text{K&gt;}
\]</span></p>
<p>Also, note that an unordered element produces a list of possible unordered combinations (vs ordered permutation). For example, any of the below combinations can represent the unordered element (ABC).</p>
<p><span class="math display">\[
(ABC) = \text{ (A), (B), (C), (AB), (AC), (CB), ()}
\]</span>
We also can interpret the element (ABC) to mean that a customer ordered items A, B, and C together. On the other hand, a second customer may order a subset of the pattern, e.g., items A and B together, items B and C together, or items A and C together. Then a third customer may order only item A or nothing at all.</p>
<p>So that if the element is part of a sequence like so, then the element can have any of the following sub-sequences:</p>
<p><span class="math display">\[
\underbrace{\text{&lt;D(A)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(AB)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(AC)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;D(CB)E&gt;}}_\text{sub-sequence},
\underbrace{\text{&lt;DE&gt;}}_\text{sub-sequence},...
\ \ \ \ \
\in \underbrace{\text{&lt;D(ABC)E&gt;}}_\text{sequence} 
\]</span>
The sub-sequence <strong>&lt; D(CB)E &gt;</strong> means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with items C and B together, then followed by a third-order with item E.</p>
<p>The sub-sequence <strong>&lt; DE &gt;</strong> means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with item E. there are no other orders made with any combination containing items A, B, and C together.</p>
<p>Let us now introduce a few algorithms that allow us to discover (or mine) sequential patterns:</p>
<p><strong>Generalized Sequential Patterns (GSP)</strong>  </p>
<p><strong>GSP</strong> uses <strong>Apriori algorithm</strong> to discover sequential patterns <span class="citation">(Srikant, R., &amp; Agrawal, R. <a href="bibliography.html#ref-ref378">1996</a>)</span>. The algorithm proceeds as such (assume the maximum length of a sequence of interest is N):</p>
<p><span class="math display">\[
\begin{array}{l}
F_1 = \text{\{frequent 1-sequence\}}\\
\text{For k in 2,...,N   repeat the following}\\
\ \ \ \ \ \ \ \ C_k\ \text{= candidate k-sequence from }F_{k-1}\\
\ \ \ \ \ \ \ \ \text{For i in 1,... repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{sq = sequence[i]}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{For each k-sequence in sq, repeat the following}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{count frequency}\\
\ \ \ \ \ \ \ \ F_k\ \text{= All candidate k-sequence from } C_k \text{ with minimum support/frequency}\\
\end{array}
\]</span></p>
<p><strong>First</strong>, using Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a>, let us generate the following <strong>length-1</strong> candidate subsequences:</p>
<p><span class="math display">\[\begin{align*}
\text{&lt;A&gt;:5},\ \text{&lt;B&gt;:5},\ \text{&lt;C&gt;:4},\ \text{&lt;D&gt;:3},\ \text{&lt;E&gt;:3},\ \text{&lt;F&gt;:3},\\ \text{&lt;G&gt;:1},\ \text{&lt;H&gt;:1},\ \text{&lt;I&gt;:1},\ \text{&lt;J&gt;:1},\ \text{&lt;K&gt;:1}
\end{align*}\]</span></p>
<p>From there, we keep subsequences that meet the minimum support (e.g.Â minsup = 2).</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p><strong>Second</strong>, let us mine for <strong>length-2</strong> sequential pattern using the list of items with minsup=2. Here, we construct a <strong>frequent item matrix</strong>. See Figure <a href="9.5-exploratory-data-analysis.html#fig:gsp">9.27</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsp"></span>
<img src="gsp.png" alt="GSP (51 2-length sequence)" width="100%" />
<p class="caption">
Figure 9.27: GSP (51 2-length sequence)
</p>
</div>
<p>Based on the matrix, we see 17 <strong>length-2</strong> sequential patterns that meet the minsup = 2.</p>
<p><strong>Third</strong>, we proceed mining for <strong>length-3</strong> sequential patterns.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gsp3"></span>
<img src="gsp3.png" alt="GSP (6 3-length sequence)" width="45%" />
<p class="caption">
Figure 9.28: GSP (6 3-length sequence)
</p>
</div>
<p>That gives us the following <strong>length-3</strong> sequential patterns that meet the minsup = 2. A few of the patterns are listed below.</p>
<p><span class="math display">\[
\text{&lt;ABF&gt;,&lt;ACF&gt;,&lt;ADE&gt;,&lt;BCF&gt;,&lt;CDE&gt;},\ \ \cdots
\]</span>
We labeled the third column as <strong>3-item sequence</strong>, which is different from the <strong>length-3</strong> sequential pattern. For example, <strong>(AB)C</strong> is a <strong>3-item sequence</strong> but falls under <strong>length-2</strong> sequential patterns because it has only two elements, namely <strong>(AB)</strong> being an unordered element and <strong>C</strong>.</p>
<p><strong>Fourth</strong>, we continue to form all other <strong>length-k</strong> sequences until we reach a point where we do not have any other sequence that meets our minimum support of 2.</p>
<p><strong>Sequential Pattern Discovery using Equivalence classes (SPADE)</strong> </p>
<p><strong>SPADE</strong> is an alternative algorithm that uses <strong>ECLAT algorithm</strong> for its association <span class="citation">(Zaki M.J. <a href="bibliography.html#ref-ref372">2001</a>)</span>. The data is vertically-formatted for Depth First Search pattern discovery.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:spade"></span>
<img src="spade.png" alt="SPADE" width="90%" />
<p class="caption">
Figure 9.29: SPADE
</p>
</div>
<p>To illustrate the algorithm, we use Figure <a href="9.5-exploratory-data-analysis.html#fig:spade">9.29</a>.</p>
<p><strong>First</strong>, we scan the sequential database for all sequences (see table <strong>T1</strong>) to construct an element table (see table <strong>T2</strong>). For example, the first element of the first sequence corresponds to the following:</p>
<p><span class="math display">\[
\text{SID}^\text{(sequence id)} = 101,\ \ \ \ \ \ \ \
\text{EID}^\text{(element id)} = 1,\ \ \ \ \ \ \ \ \ \ 
\text{Item} = \text{A}
\]</span></p>
<p>The second element of the first sequence corresponds to the following:</p>
<p><span class="math display">\[
\text{SID}^\text{(sequence id)} = 101,\ \ \ \ \ \ \ \
\text{EID}^\text{(element id)} = 2,\ \ \ \ \ \ \ \ \ \ 
\text{Item} = \text{AB}
\]</span></p>
<p><strong>Second</strong>, we use table <strong>T2</strong> to construct our vertically-formatted <strong>length-2</strong> sequence table (see table <strong>T3</strong>). For example, item <strong>A</strong> is a <strong>length-1</strong> sequential pattern that has the following sequence ids and element ids:</p>
<p><span class="math display">\[
\text{length-1 sequence for A = }\left[
\begin{array}{ll}
101 &amp; 1\\101 &amp; 2\\102 &amp; 1\\103 &amp; 1\\ \vdots &amp; \vdots
\end{array}
\right]
\ \ \ \ \ \ \ \ \ \ \ \ 
\text{length-1 sequence for B = }\left[
\begin{array}{ll}
101 &amp; 2\\102 &amp; 1\\103 &amp; 2\\104 &amp; 1\\ \vdots &amp; \vdots
\end{array}
\right]
\]</span></p>
<p><strong>Third</strong>, we continue to use table <strong>T2</strong> to construct our next vertically-formatted <strong>length-2</strong> sequence table (see table <strong>T4</strong>). For example, item <strong>AB</strong> is a <strong>length-2</strong> sequential pattern. The first entry in the table corresponds to the first sequence with SID=101 containing item <strong>A</strong> being the first element (EID=1) in the sequence and item <strong>B</strong> being one of the combinations of the second element (<strong>AB</strong>) with EID=2 in the sequence.</p>
<p>Note that table <strong>T5</strong> is just an extension of table <strong>T4</strong> in which item <strong>BA</strong> is also a <strong>length-2</strong> sequential pattern. The entry in the table corresponds to the fifth sequential pattern with SID=105 containing item <strong>B</strong> being one of the combinations of the second element (<strong>AB</strong>) in the sequence and item <strong>A</strong> being one of the combinations of the fourth element (<strong>FA</strong>) in the sequence.</p>
<p><strong>Fourth</strong>, we construct our next vertically-formatted <strong>length-3</strong> sequence table using <strong>T2</strong>.
The pattern corresponds to the following entry:</p>
<p><span class="math display">\[
\text{ABC = }
\left[
\begin{array}{llll}
101 &amp; 1 &amp; 2 &amp; 3\\
105 &amp; 1 &amp; 2 &amp; 3
\end{array}
\right]
\ \ \ \ \ \ \ \
\text{CDE = }
\left[
\begin{array}{llll}
102 &amp; 1 &amp; 2 &amp; 3\\
104 &amp; 1 &amp; 2 &amp; 3
\end{array}
\right]
\ \ \ \ \ \ \ \ \cdots
\]</span></p>
<p>To interpret the <strong>length-3</strong> sequential pattern, we see a pattern in which a customer made a sequence of orders in which the first order has item A followed by a second order with item B and another order with item C. We see a frequency of two (one from SID=101 and the other from SID=105).</p>
<p><strong>Fifth</strong>, we continue to form all other <strong>length-k</strong> sequences until we reach a point where we do not have any other sequences that meet our minimum support of 2.</p>
<p><strong>Frequent Pattern-Projected Sequential Pattern (FreeSpan)</strong> </p>
<p><strong>FreeSpan</strong> uses a divide-and-conquer approach operating on projected subsequences based on <strong>length-k</strong> sequential patterns. These subsequences are the frequent pattern <strong>length-k projections</strong> <span class="citation">(Han J. et al. <a href="bibliography.html#ref-ref380">2000</a>)</span>.</p>
<p>To illustrate the <strong>FreeSpan</strong> algorithm, we proceed as follows:</p>
<p><strong>First</strong>, using the sequence table in Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a>, we generate a frequent item list (or <strong>f_list</strong>) of frequent <strong>length-1</strong> sequential patterns that meet a minimum support of 2 (sorted in descending order):</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p><strong>Second</strong>, based on the generated <strong>length-1</strong> sequential pattern above, we construct our <strong>frequent item matrix</strong> <span class="citation">(Han J. et al. <a href="bibliography.html#ref-ref380">2000</a>)</span>. The rows and columns are ordered in descending order of support. See Figure <a href="9.5-exploratory-data-analysis.html#fig:freespan">9.30</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:freespan"></span>
<img src="freespan.png" alt="FreeSpan (1-projection)" width="65%" />
<p class="caption">
Figure 9.30: FreeSpan (1-projection)
</p>
</div>
<p>To explain, each cell in the matrix is represented by the intersection of two items and has the format (X, Y, Z) so that if the two items are <strong>A</strong> and <strong>B</strong>, then X is the number of occurrences for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;AB&gt;}}\)</span>, Y is the number of occurrences for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;BA&gt;}}\)</span>, and Z is the number of occurrence for the sequential pattern <span class="math inline">\(\mathbf{\text{&lt;(AB)&gt;}}\)</span>. For example, in Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a>, for items <strong>A</strong> and <strong>B</strong>, the intersecting cell has (3,1,5) which comes from the following:</p>
<p>For X corresponding to <span class="math inline">\(\mathbf{\text{&lt;AB&gt;}}\)</span>:</p>
<p><span class="math display">\[
X:3\ \ \ \rightarrow 101:&lt;\underline{A}(A\underline{B})CF&gt;, 103:&lt;\underline{A}(A\underline{B})(ED)&gt;,
105:&lt;\underline{A}(A\underline{B})C(AF)&gt;
\]</span></p>
<p>For Y corresponding to <span class="math inline">\(\mathbf{\text{&lt;BA&gt;}}\)</span>:</p>
<p><span class="math display">\[
Y:1\ \ \ \rightarrow 105:&lt;A(A\underline{B})C(\underline{A}F)&gt;
\]</span></p>
<p>For Z corresponding to <span class="math inline">\(\mathbf{\text{&lt;(AB)&gt;}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
Z:5\ \ \ {}&amp;\rightarrow 101:&lt;A(\underline{A}\underline{B})CF&gt;, 
102:&lt;(\underline{A}\underline{B}C)DEF(GH)(IJ)K&gt;, \\
\ \ \ &amp;\rightarrow 103:&lt;A(\underline{A}\underline{B})(ED)&gt;, 
104:&lt;(\underline{A}\underline{B}C)(CD)E&gt;,
105:&lt;A(\underline{A}\underline{B})C(AF)&gt;
\end{align*}\]</span></p>
<p>Finally, for the cell intersecting <strong>A</strong> (column-wise) and <strong>A</strong> (row-wise), we see the following combination:</p>
<p><span class="math display">\[
&lt;AA&gt;:3\ \ \ \rightarrow 101:&lt;\underline{A}(\underline{A}B)CF&gt;, 103:&lt;\underline{A}(\underline{A}B)(ED)&gt;,
105:&lt;\underline{A}(\underline{A}B)C(AF)&gt;
\]</span></p>
<p><strong>Third</strong>, let us find <strong>length-2</strong> sequential patterns based on the <strong>frequent item matrix</strong> (given a minsup = 2).</p>
<p><span class="math display">\[\begin{align*}
A{}&amp;:\ \ \ \ \ \text{&lt;AA&gt;:3}\\
B&amp;:\ \ \ \ \ \text{&lt;AB&gt;:3},\text{&lt;(AB)&gt;:5}\\
C&amp;:\ \ \ \ \ \text{&lt;AC&gt;:3},\text{&lt;(AC)&gt;:2},\text{&lt;BC&gt;:3},\text{&lt;(BC)&gt;:2}\\
D&amp;:\ \ \ \ \ \text{&lt;AD&gt;:3},\text{&lt;BD&gt;:3},\text{&lt;CD&gt;:2}\\
E&amp;:\ \ \ \ \ \text{&lt;AE&gt;:3},\text{&lt;BE&gt;:3},\text{&lt;CE&gt;:2}\\
F&amp;:\ \ \ \ \ \text{&lt;AF&gt;:3},\text{&lt;BF&gt;:3},\text{&lt;CF&gt;:3}\\
\end{align*}\]</span></p>
<p><strong>Fourth</strong>, we can continue with this approach for every smaller subsequences or projection with <strong>length-k</strong> sequential patterns. For example, for a <strong>length-3</strong> sequential patterns, we have:</p>
<p><span class="math display">\[
\text{&lt;ABC&gt;:2}\ \ \ \ \ \ \ \text{&lt;CDE&gt;:2}\ \ \ \ \ \cdots
\]</span></p>
<p><strong>Prefix-projected Sequential Pattern (PrefixSpan)</strong> </p>
<p><strong>PrefixSpan</strong> extends and enhances <strong>FreeSpan</strong> by reducing projections <span class="citation">(Pei J. et al. <a href="bibliography.html#ref-ref368_1">2004</a>)</span>.</p>
<p>Similar to <strong>Freespan</strong>, we use the sequence table in Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a> to generate a list of <strong>length-1</strong> sequential patterns that meet minimum support of 2:</p>
<p><span class="math display">\[
\text{&lt;A&gt;:5},\ \ \text{&lt;B&gt;:5},\ \ \text{&lt;C&gt;:4},\ \ \text{&lt;D&gt;:3},\ \ \text{&lt;E&gt;:3},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p>From here, we construct our <strong>projection databases</strong> based on the <strong>length-1</strong> sequential patterns above. Then, we work our way from items that meet minimum support using a divide-and-conquer approach (by partitioning).</p>
<p><strong>First</strong>, we start with the initial <strong>length-1</strong> sequential patterns. For example, let us show the <strong>length-1</strong> prefix projections. We exclude G, H, I, J, and K as they do not meet the minimum support.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prefixspan"></span>
<img src="prefixspan.png" alt="PrefixSpan (1-sequence projection)" width="90%" />
<p class="caption">
Figure 9.31: PrefixSpan (1-sequence projection)
</p>
</div>
<p>Note that sequences get pruned starting from the position of the first occurrence of the prefix to the beginning of the sequence. If the prefix is in an unordered element, the prefixed is replaced with an underscore &quot;_&quot; as a placeholder. For example, suppose our prefix is <strong>A</strong>, then we have the following three examples of an A-projection (A-suffix).</p>

<p><span class="math display">\[
\underbrace{\text{&lt;B(AC)D&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;(}\underline{}\text{C)D&gt;}}_\text{A-projection}
\ \ \ \ \ \ \ \
\underbrace{\text{&lt;B(CA)DF&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;DF&gt;}}_\text{A-projection}
\ \ \ \ \ \ \ \
\underbrace{\text{&lt;A(BC)EF&gt;}}_\text{sequence} \rightarrow 
\underbrace{\text{&lt;(BC)EF&gt;}}_\text{A-projection}
\]</span>
</p>
<p>Also, note that we highlighted the âC-projectionâ column in Figure <a href="9.5-exploratory-data-analysis.html#fig:prefixspan">9.31</a> for our next operation.</p>
<p><strong>Second</strong>, we now operate on <strong>2-sequential patterns</strong>. For example, based on the result above for the <strong>length-1</strong> sequence prefix patterns, let us use âC-prefix columnâ to generate a list of <strong>length-1</strong> sequential patterns that meet the minimum support of 2:</p>
<p><span class="math display">\[
\text{&lt;A&gt;:1},\ \ \text{&lt;B&gt;:0},\ \ \text{&lt;C&gt;:1},\ \ \text{&lt;D&gt;:2},\ \ \text{&lt;E&gt;:2},\ \ \text{&lt;F&gt;:3}
\]</span>
Therefore, we get the following minimum <strong>length-1</strong> sequential pattern from the <strong>C-projected</strong> column:</p>
<p><span class="math display">\[
\text{&lt;D&gt;:2},\ \ \text{&lt;E&gt;:2},\ \ \text{&lt;F&gt;:3}
\]</span></p>
<p>That corresponds to the following <strong>length-2</strong> sequential patterns:</p>
<p><span class="math display">\[
\text{&lt;CD&gt;:2}\ \ \ \ \ \
\text{&lt;CE&gt;:2}\ \ \ \ \ \
\text{&lt;CF&gt;:3}\ \ \ \ \ \
\]</span>
We can do the same for the other projections, e.g., <strong>A-projection</strong> and <strong>B-projection</strong>, to get the rest o the <strong>length-2</strong> sequential patterns.</p>
<p>To generate a <strong>length-2</strong> sequence projection from the <strong>C-projected</strong> column using the <strong>length-1</strong> sequential patterns, we get the following:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prefixspan1"></span>
<img src="prefixspan1.png" alt="PrefixSpan (2-sequence projection)" width="50%" />
<p class="caption">
Figure 9.32: PrefixSpan (2-sequence projection)
</p>
</div>
<p><strong>Third</strong>, for a <strong>length-3</strong> sequential pattern, we can see in Figure <a href="9.5-exploratory-data-analysis.html#fig:prefixspan1">9.32</a> that given the <strong>CD-Projection</strong>, we have a sequential pattern that meets the minimum support because they both occur in the second and fourth sequences:</p>
<p><span class="math display">\[
\text{&lt;CDE&gt;:2}
\]</span></p>
<p>We can do the same for the other projections, e.g., <strong>AB-projection</strong> and <strong>AC-projection</strong>, to get the rest of the <strong>length-3</strong> sequential patterns. For example:</p>
<p><span class="math display">\[
\text{&lt;ABC&gt;:2}
\]</span></p>
<p><strong>Lastly</strong>, we can continue with the other <strong>length-k</strong> sequential patterns to get the complete list of sequential patterns.</p>
<p>Let us now show an implementation of <strong>Association and Pattern Discovery</strong> using three libraries. The first library is called <strong>arules</strong> and it provides functions to work on <strong>association</strong> and <strong>association rules</strong>. The second library is called <strong>arulesSequences</strong>, which allows operating on <strong>sequences</strong>.</p>
<p>To demonstrate the <strong>association</strong> and <strong>apriori</strong> algorithm, let us define our dataset and convert it into a transaction format. First, let us form a matrix with rows corresponding to a list of transaction orders and columns corresponding to a list of items.</p>

<div class="sourceCode" id="cb991"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb991-1" data-line-number="1"><span class="kw">library</span>(<span class="st">&quot;arules&quot;</span>)</a>
<a class="sourceLine" id="cb991-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb991-3" data-line-number="3">TX =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb991-4" data-line-number="4">trans =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, TX)</a>
<a class="sourceLine" id="cb991-5" data-line-number="5">alpha =<span class="st">  </span><span class="kw">toupper</span>(letters) </a>
<a class="sourceLine" id="cb991-6" data-line-number="6">orders =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span>TX, <span class="dt">ncol=</span><span class="kw">length</span>(alpha), <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb991-7" data-line-number="7"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>TX) {</a>
<a class="sourceLine" id="cb991-8" data-line-number="8">  orders[i,] =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">26</span>, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="fl">0.15</span>)</a>
<a class="sourceLine" id="cb991-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb991-10" data-line-number="10"><span class="kw">colnames</span>(orders) =<span class="st"> </span>alpha</a>
<a class="sourceLine" id="cb991-11" data-line-number="11">orders_trans =<span class="st"> </span><span class="kw">as</span>(orders, <span class="st">&quot;transactions&quot;</span>)</a></code></pre></div>

<p>We use the <strong>inspect(.)</strong> function to inspect the content of our transaction orders.</p>

<div class="sourceCode" id="cb992"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb992-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(orders_trans))</a></code></pre></div>
<pre><code>##     items          
## [1] {Q,X,Y}        
## [2] {E,G,H,L,O,Q,R}
## [3] {A,M,O,T}      
## [4] {Z}            
## [5] {J,Q,V,W}      
## [6] {M,N,X}</code></pre>

<p>We use the <strong>summary(.)</strong> function for additional information about our transactional database. The function provides the most frequent 1-itemset, including quartile statistics.</p>

<div class="sourceCode" id="cb994"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb994-1" data-line-number="1"><span class="kw">summary</span>(orders_trans)</a></code></pre></div>
<pre><code>## transactions as itemMatrix in sparse format with
##  100 rows (elements/itemsets/transactions) and
##  26 columns (items) and a density of 0.1438 
## 
## most frequent items:
##       S       Q       R       U       M (Other) 
##      21      19      19      19      18     278 
## 
## element (itemset/transaction) length distribution:
## sizes
##  1  2  3  4  5  6  7  8 
##  8 18 21 23 11 14  4  1 
## 
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    1.00    2.00    4.00    3.74    5.00    8.00 
## 
## includes extended item information - examples:
##   labels
## 1      A
## 2      B
## 3      C</code></pre>

<p>Finally, if we need to view our data structure, we can use the <strong>str(.)</strong> function.</p>

<div class="sourceCode" id="cb996"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb996-1" data-line-number="1"><span class="kw">str</span>(orders_trans, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## Formal class &#39;transactions&#39; [package &quot;arules&quot;] with 3 slots
## ..@ data :Formal class &#39;ngCMatrix&#39; [package &quot;Matrix&quot;] with 5 slots
## .. .. ..@ i : int [1:374] 16 23 24 4 6 7 11 14 16 17 ...
## .. .. ..@ p : int [1:101] 0 3 10 14 15 19 22 29 31 36 ...
## .. .. ..@ Dim : int [1:2] 26 100
## .. .. ..@ Dimnames:List of 2
## .. .. .. ..$ : NULL
## .. .. .. ..$ : NULL
## .. .. ..@ factors : list()
## ..@ itemInfo :&#39;data.frame&#39;: 26 obs. of 1 variable:
## .. ..$ labels: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ...
## ..@ itemsetInfo:&#39;data.frame&#39;: 0 obs. of 0 variables</code></pre>

<p>To implement the <strong>Apriori algorithm</strong> and limit patterns, we use the <strong>apriori(.)</strong> function.</p>

<div class="sourceCode" id="cb998"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb998-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb998-2" data-line-number="2">apriori.output &lt;-<span class="st"> </span><span class="kw">apriori</span>(orders_trans, </a>
<a class="sourceLine" id="cb998-3" data-line-number="3">                    <span class="dt">parameter =</span> <span class="kw">list</span>(<span class="dt">supp=</span><span class="fl">0.001</span>, <span class="dt">conf=</span><span class="fl">0.8</span>,<span class="dt">maxlen=</span><span class="dv">10</span>))</a></code></pre></div>
<pre><code>## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport
##         0.8    0.1    1 none FALSE            TRUE
##  maxtime support minlen maxlen target  ext
##        5   0.001      1     10  rules TRUE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 0 
## 
## set item appearances ...[0 item(s)] done [0.00s].
## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s].
## sorting and recoding items ... [26 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3 4 5 6 7 8 done [0.00s].
## writing ... [3442 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].</code></pre>

<p>We use the <strong>inspect(.)</strong> function one more time to inspect the result of apriori, displaying four measurements (e.g., support, confidence, coverage, and lift).</p>

<div class="sourceCode" id="cb1000"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1000-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(apriori.output))</a></code></pre></div>
<pre><code>##     lhs      rhs support confidence coverage lift  
## [1] {C,G} =&gt; {J} 0.01    1          0.01      7.692
## [2] {G,J} =&gt; {C} 0.01    1          0.01     12.500
## [3] {C,D} =&gt; {N} 0.01    1          0.01      6.667
## [4] {C,D} =&gt; {S} 0.01    1          0.01      4.762
## [5] {C,I} =&gt; {X} 0.01    1          0.01      7.692
## [6] {I,X} =&gt; {C} 0.01    1          0.01     12.500
##     count
## [1] 1    
## [2] 1    
## [3] 1    
## [4] 1    
## [5] 1    
## [6] 1</code></pre>

<p>If we want to limit the result only to a few items, we can use the following:</p>

<div class="sourceCode" id="cb1002"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1002-1" data-line-number="1">apriori.output &lt;-<span class="st"> </span><span class="kw">apriori</span>(orders_trans, <span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1002-2" data-line-number="2">                          <span class="kw">list</span>(<span class="dt">supp=</span><span class="fl">0.001</span>, <span class="dt">conf=</span><span class="fl">0.8</span>,<span class="dt">maxlen=</span><span class="dv">10</span>),</a>
<a class="sourceLine" id="cb1002-3" data-line-number="3">                          <span class="dt">appearance=</span><span class="kw">list</span>(<span class="dt">lhs=</span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>,<span class="st">&quot;C&quot;</span>)))</a></code></pre></div>
<pre><code>## Apriori
## 
## Parameter specification:
##  confidence minval smax arem  aval originalSupport
##         0.8    0.1    1 none FALSE            TRUE
##  maxtime support minlen maxlen target  ext
##        5   0.001      1     10  rules TRUE
## 
## Algorithmic control:
##  filter tree heap memopt load sort verbose
##     0.1 TRUE TRUE  FALSE TRUE    2    TRUE
## 
## Absolute minimum support count: 0 
## 
## set item appearances ...[3 item(s)] done [0.00s].
## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s].
## sorting and recoding items ... [26 item(s)] done [0.00s].
## creating transaction tree ... done [0.00s].
## checking subsets of size 1 2 3 done [0.00s].
## writing ... [3 rule(s)] done [0.00s].
## creating S4 object  ... done [0.00s].</code></pre>
<div class="sourceCode" id="cb1004"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1004-1" data-line-number="1"><span class="kw">inspect</span>(<span class="kw">head</span>(apriori.output))</a></code></pre></div>
<pre><code>##     lhs      rhs support confidence coverage lift 
## [1] {B,C} =&gt; {X} 0.01    1          0.01     7.692
## [2] {B,C} =&gt; {M} 0.01    1          0.01     5.556
## [3] {A,B} =&gt; {P} 0.02    1          0.02     7.143
##     count
## [1] 1    
## [2] 1    
## [3] 2</code></pre>

<p>For sequences, let us demonstrate the use of the <strong>SPADE</strong> algorithm. First, let us construct our sequence database using Figure <a href="9.5-exploratory-data-analysis.html#fig:sequential">9.26</a>.</p>

<div class="sourceCode" id="cb1006"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1006-1" data-line-number="1"><span class="kw">library</span>(arulesSequences)</a>
<a class="sourceLine" id="cb1006-2" data-line-number="2">orders =<span class="st"> </span><span class="kw">list</span>(<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>),<span class="kw">c</span>(<span class="st">&quot;A&quot;</span>,<span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;F&quot;</span>), </a>
<a class="sourceLine" id="cb1006-3" data-line-number="3">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;D&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;E&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;F&quot;</span>), </a>
<a class="sourceLine" id="cb1006-4" data-line-number="4">               <span class="kw">c</span>(<span class="st">&quot;G&quot;</span>,<span class="st">&quot;H&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;I&quot;</span>, <span class="st">&quot;J&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;K&quot;</span>),</a>
<a class="sourceLine" id="cb1006-5" data-line-number="5">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;D&quot;</span>, <span class="st">&quot;E&quot;</span>),</a>
<a class="sourceLine" id="cb1006-6" data-line-number="6">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>, <span class="st">&quot;D&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;E&quot;</span>),</a>
<a class="sourceLine" id="cb1006-7" data-line-number="7">               <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;C&quot;</span>), <span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;F&quot;</span>))</a>
<a class="sourceLine" id="cb1006-8" data-line-number="8"><span class="kw">names</span>(orders) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">21</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1006-9" data-line-number="9">trans.seq =<span class="st"> </span><span class="kw">as</span>(orders, <span class="st">&quot;transactions&quot;</span>)</a>
<a class="sourceLine" id="cb1006-10" data-line-number="10"><span class="kw">transactionInfo</span>(trans.seq)<span class="op">$</span>sequenceID =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,<span class="dv">2</span>,</a>
<a class="sourceLine" id="cb1006-11" data-line-number="11">                                          <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1006-12" data-line-number="12"><span class="kw">transactionInfo</span>(trans.seq)<span class="op">$</span>eventID =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>,<span class="dv">7</span>,<span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1006-13" data-line-number="13">                                       <span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1006-14" data-line-number="14">trans.seq</a></code></pre></div>
<pre><code>## transactions in sparse format with
##  21 transactions (rows) and
##  11 items (columns)</code></pre>

<p>Let us then inspect our sequential data. This should match table <strong>T2</strong> in Figure <a href="9.5-exploratory-data-analysis.html#fig:spade">9.29</a>.</p>

<div class="sourceCode" id="cb1008"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1008-1" data-line-number="1"><span class="kw">inspect</span>(trans.seq)</a></code></pre></div>
<pre><code>##      items   transactionID sequenceID eventID
## [1]  {A}     T1            1          1      
## [2]  {A,B}   T2            1          2      
## [3]  {C}     T3            1          3      
## [4]  {F}     T4            1          4      
## [5]  {A,B,C} T5            2          1      
## [6]  {D}     T6            2          2      
## [7]  {E}     T7            2          3      
## [8]  {F}     T8            2          4      
## [9]  {G,H}   T9            2          5      
## [10] {I,J}   T10           2          6      
## [11] {K}     T11           2          7      
## [12] {A}     T12           3          1      
## [13] {A,B}   T13           3          2      
## [14] {D,E}   T14           3          3      
## [15] {A,B,C} T15           4          1      
## [16] {C,D}   T16           4          2      
## [17] {E}     T17           4          3      
## [18] {A}     T18           5          1      
## [19] {A,B}   T19           5          2      
## [20] {C}     T20           5          3      
## [21] {A,F}   T21           5          4</code></pre>

<p>Finally, we use <strong>cspade(.)</strong> function to demonstrate <strong>SPADE</strong> algorithm. We mine sequential patterns that meet our minimum support (in this case, we are using the relative support of 0.40).</p>
<p>For a <strong>length-2</strong> sequential pattern, we obtain the following:</p>

<div class="sourceCode" id="cb1010"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1010-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1010-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>),</a>
<a class="sourceLine" id="cb1010-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a></code></pre></div>

<p>Let us review how many sequential patterns have the support of 1,2, and 3:</p>

<div class="sourceCode" id="cb1011"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1011-1" data-line-number="1">supports  =<span class="st"> </span><span class="kw">size</span>(spade.output)</a>
<a class="sourceLine" id="cb1011-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;length-1&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1011-3" data-line-number="3">  <span class="st">&quot;length-2&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb1011-4" data-line-number="4">  <span class="st">&quot;length-3&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(supports <span class="op">==</span><span class="st"> </span><span class="dv">3</span>))</a></code></pre></div>
<pre><code>## length-1 length-2 length-3 
##       10       25       16</code></pre>

<p>For a <strong>length-1</strong> sequential pattern, we can obtain by running the following:</p>

<div class="sourceCode" id="cb1013"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1013-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1013-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>, <span class="dt">maxsize=</span><span class="dv">1</span>, <span class="dt">maxlen=</span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1013-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1013-4" data-line-number="4"><span class="kw">as</span>(spade.output, <span class="st">&quot;data.frame&quot;</span>)</a></code></pre></div>
<pre><code>##   sequence support
## 1    &lt;{A}&gt;     1.0
## 2    &lt;{B}&gt;     1.0
## 3    &lt;{C}&gt;     0.8
## 4    &lt;{D}&gt;     0.6
## 5    &lt;{E}&gt;     0.6
## 6    &lt;{F}&gt;     0.6</code></pre>

<p>For a <strong>length-2</strong> sequential pattern, we obtain the following list using the support:</p>

<div class="sourceCode" id="cb1015"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1015-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1015-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>),</a>
<a class="sourceLine" id="cb1015-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1015-4" data-line-number="4">sub.spade =<span class="st"> </span><span class="kw">subset</span>(spade.output, supports <span class="op">==</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1015-5" data-line-number="5"><span class="kw">head</span>(<span class="kw">as</span>(sub.spade , <span class="st">&quot;data.frame&quot;</span>), <span class="dt">n =</span> <span class="dv">15</span>) <span class="co"># display only the first 15</span></a></code></pre></div>
<pre><code>##         sequence support
## 7      &lt;{A},{F}&gt;     0.6
## 8      &lt;{B},{F}&gt;     0.6
## 9      &lt;{C},{F}&gt;     0.6
## 16   &lt;{A,B},{F}&gt;     0.6
## 20     &lt;{A},{E}&gt;     0.6
## 21     &lt;{B},{E}&gt;     0.6
## 22     &lt;{C},{E}&gt;     0.4
## 23     &lt;{D},{E}&gt;     0.4
## 31   &lt;{B,C},{E}&gt;     0.4
## 32   &lt;{A,C},{E}&gt;     0.4
## 33 &lt;{A,B,C},{E}&gt;     0.4
## 34   &lt;{A,B},{E}&gt;     0.6
## 35     &lt;{A},{D}&gt;     0.6
## 36     &lt;{B},{D}&gt;     0.6
## 37     &lt;{C},{D}&gt;     0.4</code></pre>

<p>For a <strong>length-3</strong> sequential pattern, we obtain the following:</p>

<div class="sourceCode" id="cb1017"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1017-1" data-line-number="1">spade.output =<span class="st"> </span><span class="kw">cspade</span>(trans.seq,<span class="dt">parameter =</span> </a>
<a class="sourceLine" id="cb1017-2" data-line-number="2">                    <span class="kw">list</span>(<span class="dt">support =</span> <span class="fl">0.40</span>, <span class="dt">maxsize =</span> <span class="dv">3</span>, <span class="dt">maxlen =</span> <span class="dv">3</span>),</a>
<a class="sourceLine" id="cb1017-3" data-line-number="3">                    <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">verbose =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb1017-4" data-line-number="4">sub.spade =<span class="st"> </span><span class="kw">subset</span>(spade.output, <span class="kw">size</span>(spade.output) <span class="op">==</span><span class="st"> </span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1017-5" data-line-number="5"><span class="kw">as</span>(sub.spade , <span class="st">&quot;data.frame&quot;</span>)</a></code></pre></div>
<pre><code>##             sequence support
## 10     &lt;{B},{C},{F}&gt;     0.4
## 11     &lt;{A},{C},{F}&gt;     0.4
## 12   &lt;{A,B},{C},{F}&gt;     0.4
## 14     &lt;{A},{B},{F}&gt;     0.4
## 15   &lt;{A},{A,B},{F}&gt;     0.4
## 16     &lt;{A},{A},{F}&gt;     0.4
## 21     &lt;{C},{D},{E}&gt;     0.4
## 22     &lt;{B},{D},{E}&gt;     0.4
## 23     &lt;{A},{D},{E}&gt;     0.4
## 24   &lt;{B,C},{D},{E}&gt;     0.4
## 25   &lt;{A,C},{D},{E}&gt;     0.4
## 26 &lt;{A,B,C},{D},{E}&gt;     0.4
## 27   &lt;{A,B},{D},{E}&gt;     0.4
## 44     &lt;{A},{B},{C}&gt;     0.4
## 45   &lt;{A},{A,B},{C}&gt;     0.4
## 46     &lt;{A},{A},{C}&gt;     0.4</code></pre>

<p>Apart from <strong>sequential patterns</strong>, part of mining data is to mine <strong>graph patterns</strong>. Here, we leave readers to investigate <strong>Graph pattern mining</strong>.</p>
</div>
<div id="null-invariance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.4</span> Null Invariance <a href="9.5-exploratory-data-analysis.html#null-invariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, let us extend the concept of <strong>Association rules</strong> as discussed in a previous section and introduce <strong>Interestingness measures</strong>, which provide the strength of association of items. </p>
<p>Suppose we sell items <strong>A</strong>, <strong>B</strong>, and <strong>C</strong>. While we know that customers may order any combination of the three items, we are interested in the association only between item <strong>A</strong> and item <strong>B</strong>. For example, it may help to know the possibility that when customers order item <strong>A</strong>, they also order item <strong>B</strong>. Alternatively, perhaps, customers are interested only in item <strong>C</strong> but not both items <strong>A</strong> and <strong>B</strong> - these orders without items <strong>A</strong> and <strong>B</strong> are called the <strong>null transactions</strong> with respect to <strong>A</strong> and <strong>B</strong>. To illustrate further, let us use Figure <a href="9.5-exploratory-data-analysis.html#fig:nulltrans">9.33</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:nulltrans"></span>
<img src="nulltrans.png" alt="Datasets with Null Transactions" width="95%" />
<p class="caption">
Figure 9.33: Datasets with Null Transactions
</p>
</div>
<p>Analyzing the orders in table <strong>T1</strong>, we see that there are orders containing item <strong>A</strong> and item <strong>B</strong>, making 850 orders. There are orders containing only item <strong>A</strong> but not <strong>B</strong> making 965 orders. Finally, there are orders containing only item <strong>B</strong> but not <strong>A</strong>, making 750. Moreover, some orders do not contain both items <strong>A</strong> and <strong>B</strong> - the <strong>null transactions</strong> - making a total of 80.</p>
<p>Let us compute for the <strong>Support</strong> and <strong>Confidence</strong> using <strong>T1</strong>.</p>

<p><span class="math display">\[
\begin{array}{lll}
S(B) = \frac{1600}{2645} = 0.6049  &amp;
S(A \rightarrow B) = \frac{850}{2645} = 0.3214 &amp;
C(A \rightarrow B) = \frac{850}{1815} = 0.4683 \\
&amp; S(\overline{A} \rightarrow B) = \frac{750}{2645} = 0.2836 &amp;
C(\overline{A} \rightarrow B) = \frac{750}{830} = 0.9036
\end{array}
\]</span>
</p>
<p>The above measures show that there is a confidence level of 46.83% for item <strong>B</strong> being ordered along with item <strong>A</strong>, with the association having the support of 32.14%. However, we see a higher confidence level of 90.36% for item <strong>B</strong> being on its own without item <strong>A</strong>.</p>
<p>The strength of association can also be measured using the <strong>Lift</strong> measure in which a value less than one reflects a negative correlation; otherwise, we see a positive correlation. For example, below, we see a <strong>Lift</strong> value less than one; thus, we see a negative correlation.</p>

<p><span class="math display">\[
L(A,B) = \frac{C(A \rightarrow B)}{S(B)} = \frac{850/1815}{1600/2645} = 0.7742
\]</span>
</p>
<p>Another measure of association is the <strong>Chi-squared </strong> <span class="math inline">\((X^2)\)</span>. Given the corresponding expected values, a <strong>Chi-squared</strong> value equal to zero means no correlations between items; otherwise, there is a positive or negative correlation depending on the sign of the value. </p>
<p>Using the <strong>expected values</strong>:</p>

<p><span class="math display">\[\begin{align*}
\mathbb{E}(A,B) {}&amp;= \frac{A \times B}{\text{total tx}} = \frac{1815 \times 1600}{2645} = 1097.921\\
\mathbb{E}(\overline{A},B) &amp;= \frac{ \overline{A} \times B}{\text{total tx}} = \frac{830 \times 1600}{2645} = 502.0794\\
\mathbb{E}(A,\overline{B}) &amp;= \frac{ A \times \overline{B}}{\text{total tx}} = \frac{1815 \times 1045}{2645} = 717.0794\\
\mathbb{E}(\overline{A},\overline{B}) &amp;= \frac{\overline{A} \times \overline{B}}{\text{total tx}} = \frac{830 \times 1045}{2645} = 327.9206\\
\end{align*}\]</span>
</p>
<p>we compute for the <span class="math inline">\(X^2\)</span> like so:</p>

<p><span class="math display">\[\begin{align*}
X^2 {}&amp;= \sum_{i,j} \frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\
&amp;= 
\frac{(850 - 1097.921)^2}{1097.921} +
\frac{(750 - 502.0794)^2}{502.0794} +
\frac{(965 - 717.0794)^2}{717.0794} +
\frac{(80 - 327.9206)^2}{327.9206}\\
&amp;= 451.5558
\end{align*}\]</span>
</p>
<p>In terms of measuring <strong>similarity</strong> and <strong>distance</strong>, <strong>Interestingness</strong> of association can be measured using <strong>Cosine Similarity</strong>, <strong>Jaccard Coefficient</strong>, <strong>Jaccard Distance</strong>, <strong>Gini Index</strong>, and many others. For example, Figure <a href="9.5-exploratory-data-analysis.html#fig:interestingness">9.34</a> shows a partial list of <strong>Interestingness</strong> measures citing Tan P. et al. <span class="citation">(<a href="bibliography.html#ref-ref368_2">2002</a>)</span> and Han J. et al. <span class="citation">(<a href="bibliography.html#ref-ref370_2">2002</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:interestingness"></span>
<img src="interestingness.png" alt="Interestingness Measures" width="100%" />
<p class="caption">
Figure 9.34: Interestingness Measures
</p>
</div>
<p>Notice that there are 10000 <strong>null transactions</strong> in table <strong>T2</strong> compared to <strong>T1</strong> which has only 80 <strong>null transactions</strong>; yet, the computed values do not change (hence invariant) for <strong>Confidence</strong>, <strong>Kulczynski</strong>, <strong>Jaccard Coefficient/Distance</strong>, <strong>Cosine Similarity</strong>, and <strong>Imbalance Ratio</strong> measures.</p>
<p><strong>Null Invariance (NI)</strong> is a binary property of a given <strong>Interestingness</strong> measure that describes the effect of a change in the number of null transactions. For example, it helps to know if the value of our measure changes as the number of customer orders not containing items <strong>A</strong> and <strong>B</strong> increases. Here, we introduce the term <strong>Interestingness</strong> measure, a ranking criterion to determine the strength of association of items. Many <strong>Interestingness</strong> measures are used to rank the strength of association while also classified based on being <strong>Null-Invariant or not</strong>. Three of which are discussed in the previous section, namely <strong>Support</strong>, <strong>Confidence</strong>, and <strong>Lift</strong>. Additionally, the table in Figure <a href="9.5-exploratory-data-analysis.html#fig:interestingness">9.34</a> lists a few additional measures (J. Han et al.Â 2004; Pang-Ning Tan et al.Â 2002).</p>
<p>We only show three cases (tables <strong>T1</strong>, <strong>T2</strong>, <strong>T3</strong>); however, there are situations in which the number of transactions with respect to the combinations <span class="math inline">\((A,B), (\overline{A},B), (A,\overline{B}),(\overline{A},\overline{B})\)</span> vary across many cases. For that reason, we see disagreements in measures.</p>
<p>One measure that may help is the <strong>Imbalance Ratio (IR)</strong>. Along with <strong>Kulczynski</strong>, the <strong>Imbalance ratio (IR)</strong> determines both neutrality and imbalance of certain different datasets (J. Han 2004). An <strong>IR</strong> value closer to one indicates a strong imbalance. Moreover, a <strong>Kulczynski</strong> value of 0.5 consistent across multiple datasets indicates neutrality. Comparing tables <strong>T1</strong>, <strong>T2</strong>, <strong>T3</strong>, we see that they are all neutral based on the <strong>Kulczynski</strong> measure; however, only table <strong>T3</strong> is balanced based on the <strong>IR</strong> measure.  </p>
<p>For illustration, let us show a crude implementation of the measures listed in Figure <a href="9.5-exploratory-data-analysis.html#fig:interestingness">9.34</a>.</p>
<p>For reference, we provide the following probabilities for table <strong>T1</strong>:</p>

<p><span class="math display">\[
\begin{array}{l}
P(A,B) = \frac{850}{2645} = 0.32136 \\
P(\overline{A},B) = \frac{750}{2645} = 0.28355 \\
P(A,\overline{B}) = \frac{965}{2645} = 0.36484 \\
P(\overline{A},\overline{B}) = \frac{80}{2645} = 0.03025 
\end{array}
\ \ \ \ \ \ \ \ \ \ 
\begin{array}{l}
P(A) = \frac{1815}{2645} = 0.68620 \\
P(\overline{A}) = \frac{830}{2645} = 0.31380 \\
P(B) = \frac{1600}{2645}= 0.60491 \\
P(\overline{B}) = \frac{1045}{2645} = 0.39509
\end{array}
\]</span>
</p>
<p>Let us also compute for conditional probabilities (based on chain rule):</p>

<p><span class="math display">\[
\begin{array}{l}
P(A|B) = \frac{P(A,B)} {P(B)} = \frac{0.32136}{0.60491} = 0.53125\\
P(\overline{A}|B) = \frac{P(\overline{A},B)} {P(B)} = \frac{0.28355}{0.60491} = 0.46875\\
P(A|\overline{B}) = \frac{P(A,\overline{B})} {P(\overline{B})} = \frac{0.36484}{0.39509} = 0.92344\\
P(\overline{A}|\overline{B}) = \frac{P(\overline{A},\overline{B})} {P(\overline{B})} = \frac{0.03025}{0.39509} = 0.07656\\
\end{array}
\ \ \ \ \ \ \
\begin{array}{l}
P(B|A) = \frac{P(B,A)} {P(A)} = \frac{0.32136}{0.68620} = 0.46832\\
P(\overline{B}|A)  = \frac{P(\overline{B},A)} {P(A)} =  \frac{0.36484}{0.68620} = 0.53168\\
P(B|\overline{A})  = \frac{P(B,\overline{A})} {P(\overline{A})} =  \frac{0.28355}{0.31380} = 0.90360\\
P(\overline{B}|\overline{A})  = \frac{P(\overline{B},\overline{A})} {P(\overline{A})} =  \frac{0.03025}{0.31380} = 0.09640\\
\end{array}
\]</span>
</p>
<p>Here, the sum of the following probabilities should equal one:</p>

<p><span class="math display">\[\begin{align*}
1 {}&amp;= P(A,B) + P(\overline{A}, B) + P(A, \overline{B}) + P(\overline{A},\overline{B})\\
1 &amp;= P(A) + P(\overline{A})\\
1 &amp;= P(B) + P(\overline{B})\\
1 &amp;= P(A|B) + P(\overline{A}|B)\\
1 &amp;= P(A|\overline{B}) + P(\overline{A}|\overline{B})\\
1 &amp;= P(B|A) + P(\overline{B}|A)\\
1 &amp;= P(B|\overline{A}) + P(\overline{B}|\overline{A})\\
\end{align*}\]</span>
</p>
<p><strong>First</strong>, we construct the three matrices and define a few helper functions:</p>

<div class="sourceCode" id="cb1019"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1019-1" data-line-number="1">T1 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">750</span>, <span class="dv">965</span>, <span class="dv">80</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-2" data-line-number="2">T2 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">750</span>, <span class="dv">965</span>, <span class="dv">10000</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-3" data-line-number="3">T3 =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">850</span>, <span class="dv">850</span>, <span class="dv">850</span>, <span class="dv">10000</span>), <span class="dt">nrow=</span><span class="dv">2</span>, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1019-4" data-line-number="4">A.B   &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">1</span>,<span class="dv">1</span>] }              <span class="co"># (A,B)</span></a>
<a class="sourceLine" id="cb1019-5" data-line-number="5">A_.B  &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">1</span>,<span class="dv">2</span>] }              <span class="co"># (not A, B)</span></a>
<a class="sourceLine" id="cb1019-6" data-line-number="6">A.B_  &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">2</span>,<span class="dv">1</span>] }              <span class="co"># (A, not B)</span></a>
<a class="sourceLine" id="cb1019-7" data-line-number="7">A_.B_ &lt;-<span class="st"> </span><span class="cf">function</span>(D) { D[<span class="dv">2</span>,<span class="dv">2</span>] }              <span class="co"># (not A, not B)</span></a>
<a class="sourceLine" id="cb1019-8" data-line-number="8">A     &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B_</span>(D) }    <span class="co"># (A)</span></a>
<a class="sourceLine" id="cb1019-9" data-line-number="9">B     &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B</span>(D) }    <span class="co"># (B)</span></a>
<a class="sourceLine" id="cb1019-10" data-line-number="10">A_    &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }  <span class="co"># (not A)</span></a>
<a class="sourceLine" id="cb1019-11" data-line-number="11">B_    &lt;-<span class="st"> </span><span class="cf">function</span>(D) { <span class="kw">A.B_</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }  <span class="co"># (not B)</span></a>
<a class="sourceLine" id="cb1019-12" data-line-number="12">E &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">X</span>(D) <span class="op">*</span><span class="st"> </span><span class="kw">Y</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) } <span class="co"># expectation</span></a>
<a class="sourceLine" id="cb1019-13" data-line-number="13">P &lt;-<span class="st"> </span><span class="cf">function</span>(D, X) { <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) } <span class="co"># probability</span></a>
<a class="sourceLine" id="cb1019-14" data-line-number="14"> <span class="co"># conditional probability</span></a>
<a class="sourceLine" id="cb1019-15" data-line-number="15">P.cond &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">P</span>(D, X) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D, Y) }</a>
<a class="sourceLine" id="cb1019-16" data-line-number="16">Tot.Tx &lt;-<span class="st"> </span><span class="cf">function</span>(D) {  <span class="kw">A.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A.B_</span>(D) <span class="op">+</span><span class="st"> </span><span class="kw">A_.B_</span>(D) }</a></code></pre></div>

<p><strong>Second</strong>, we define functions for the <strong>Interestingness</strong> measures:</p>

<div class="sourceCode" id="cb1020"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1020-1" data-line-number="1">support &lt;-<span class="st"> </span><span class="cf">function</span>(D, X) {  <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Tot.Tx</span>(D) }</a>
<a class="sourceLine" id="cb1020-2" data-line-number="2">confidence &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { <span class="kw">X</span>(D) <span class="op">/</span><span class="st"> </span><span class="kw">Y</span>(D) }</a>
<a class="sourceLine" id="cb1020-3" data-line-number="3">lift &lt;-<span class="st"> </span><span class="cf">function</span>(D, X, Y) { X <span class="op">/</span><span class="st"> </span>Y }</a>
<a class="sourceLine" id="cb1020-4" data-line-number="4">cosine.similarity &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-5" data-line-number="5">  <span class="kw">P</span>(D, A.B) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">P</span>(D, A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B))</a>
<a class="sourceLine" id="cb1020-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1020-7" data-line-number="7">jaccard.coefficient &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-8" data-line-number="8">  <span class="kw">P</span>(D, A.B) <span class="op">/</span><span class="st"> </span>(<span class="kw">P</span>(D,A) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D, A.B))</a>
<a class="sourceLine" id="cb1020-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1020-10" data-line-number="10">jaccard.distance &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-11" data-line-number="11">  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">jaccard.coefficient</span>(D)</a>
<a class="sourceLine" id="cb1020-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb1020-13" data-line-number="13">leverage &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-14" data-line-number="14">  <span class="kw">P</span>(D,A.B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B)</a>
<a class="sourceLine" id="cb1020-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb1020-16" data-line-number="16">conviction &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-17" data-line-number="17">  <span class="kw">max</span>(</a>
<a class="sourceLine" id="cb1020-18" data-line-number="18">    (<span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B_)) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A.B_),</a>
<a class="sourceLine" id="cb1020-19" data-line-number="19">    (<span class="kw">P</span>(D,B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_)) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A_.B)</a>
<a class="sourceLine" id="cb1020-20" data-line-number="20">  )</a>
<a class="sourceLine" id="cb1020-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb1020-22" data-line-number="22">chi.squared &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-23" data-line-number="23">  ( <span class="kw">A.B</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A,B) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A,B) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-24" data-line-number="24"><span class="st">    </span>( <span class="kw">A_.B</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A_,B) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A_,B) <span class="op">+</span></a>
<a class="sourceLine" id="cb1020-25" data-line-number="25"><span class="st">  </span>( <span class="kw">A.B_</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A,B_) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A,B_)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-26" data-line-number="26"><span class="st">    </span>( <span class="kw">A_.B_</span>(D) <span class="op">-</span><span class="st"> </span><span class="kw">E</span>(D,A_,B_) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">E</span>(D,A_,B_)</a>
<a class="sourceLine" id="cb1020-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb1020-28" data-line-number="28">kappa &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-29" data-line-number="29">  ( <span class="kw">P</span>(D, A.B) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D, A_.B_) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B_) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-30" data-line-number="30"><span class="st">    </span>( <span class="dv">1</span><span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D, B_))</a>
<a class="sourceLine" id="cb1020-31" data-line-number="31">}</a>
<a class="sourceLine" id="cb1020-32" data-line-number="32">gini.index &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-33" data-line-number="33">  <span class="kw">P</span>(D, A) <span class="op">*</span><span class="st"> </span>( <span class="kw">P.cond</span>(D, A.B, A)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D, A.B_,A)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-34" data-line-number="34"><span class="st">  </span><span class="kw">P</span>(D, A_) <span class="op">*</span><span class="st"> </span>(<span class="kw">P.cond</span>(D, A_.B, A_)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D, A_.B_, A_)<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-35" data-line-number="35"><span class="st">  </span><span class="kw">P</span>(D, B)<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D, B_)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1020-36" data-line-number="36">}</a>
<a class="sourceLine" id="cb1020-37" data-line-number="37">yule.Q &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-38" data-line-number="38">  ( <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_) <span class="op">-</span><span class="st"> </span><span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-39" data-line-number="39"><span class="st">  </span>( <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_) <span class="op">+</span><span class="st"> </span><span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_) )</a>
<a class="sourceLine" id="cb1020-40" data-line-number="40">}</a>
<a class="sourceLine" id="cb1020-41" data-line-number="41">yule.Y &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-42" data-line-number="42">  ( <span class="kw">sqrt</span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">-</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_)) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1020-43" data-line-number="43"><span class="st">  </span>( <span class="kw">sqrt</span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">+</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">P</span>(D,A_.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A.B_)) )</a>
<a class="sourceLine" id="cb1020-44" data-line-number="44">}</a>
<a class="sourceLine" id="cb1020-45" data-line-number="45">kulczynski &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-46" data-line-number="46">  <span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span>(<span class="kw">P.cond</span>(D,A.B,B) <span class="op">+</span><span class="st"> </span><span class="kw">P.cond</span>(D,A.B, A))</a>
<a class="sourceLine" id="cb1020-47" data-line-number="47">}</a>
<a class="sourceLine" id="cb1020-48" data-line-number="48">odds.ratio &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-49" data-line-number="49">  (<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B_)) <span class="op">/</span><span class="st"> </span>(<span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">P</span>(D,A_.B))</a>
<a class="sourceLine" id="cb1020-50" data-line-number="50">}</a>
<a class="sourceLine" id="cb1020-51" data-line-number="51">j.measure &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-52" data-line-number="52">  <span class="kw">max</span>(</a>
<a class="sourceLine" id="cb1020-53" data-line-number="53">  <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B,A) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B), <span class="dv">2</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb1020-54" data-line-number="54"><span class="st">  </span><span class="kw">P</span>(D,A.B_) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B_,A) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B_), <span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1020-55" data-line-number="55">  <span class="kw">P</span>(D,A.B) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A.B,B) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,B), <span class="dv">2</span>) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-56" data-line-number="56"><span class="st">  </span><span class="kw">P</span>(D,A.B_) <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(<span class="kw">P.cond</span>(D,A_.B,B) <span class="op">/</span><span class="st"> </span><span class="kw">P</span>(D,A_), <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1020-57" data-line-number="57">}</a>
<a class="sourceLine" id="cb1020-58" data-line-number="58">imbalance.ratio &lt;-<span class="st"> </span><span class="cf">function</span>(D) {</a>
<a class="sourceLine" id="cb1020-59" data-line-number="59">  <span class="kw">abs</span>(<span class="kw">support</span>(D,A) <span class="op">-</span><span class="st"> </span><span class="kw">support</span>(D,B)) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1020-60" data-line-number="60"><span class="st">  </span>( <span class="kw">support</span> (D,A) <span class="op">+</span><span class="st"> </span><span class="kw">support</span>(D,B) <span class="op">-</span><span class="st"> </span><span class="kw">support</span>(D,A.B))</a>
<a class="sourceLine" id="cb1020-61" data-line-number="61">}</a></code></pre></div>

<p><strong>Third</strong>, we then compute for the measures of the three matrices:</p>

<div class="sourceCode" id="cb1021"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1021-1" data-line-number="1">m =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="dv">17</span>, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1021-2" data-line-number="2"><span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">3</span>) {</a>
<a class="sourceLine" id="cb1021-3" data-line-number="3">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { D =<span class="st"> </span>T1 } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1021-4" data-line-number="4">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) { D =<span class="st"> </span>T2 } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1021-5" data-line-number="5">  <span class="cf">if</span> (j <span class="op">==</span><span class="st"> </span><span class="dv">3</span>) { D =<span class="st"> </span>T3 }</a>
<a class="sourceLine" id="cb1021-6" data-line-number="6">  m[, j] =<span class="st"> </span><span class="kw">c</span>( <span class="kw">support</span>(D, A.B), <span class="kw">confidence</span>(D, A.B, A), </a>
<a class="sourceLine" id="cb1021-7" data-line-number="7">              <span class="kw">lift</span>(D, <span class="kw">confidence</span>(D, A.B, A), <span class="kw">support</span>(D,B) ),</a>
<a class="sourceLine" id="cb1021-8" data-line-number="8">              <span class="kw">chi.squared</span>(D), <span class="kw">kulczynski</span>(D), <span class="kw">leverage</span>(D),</a>
<a class="sourceLine" id="cb1021-9" data-line-number="9">              <span class="kw">jaccard.coefficient</span>(D), <span class="kw">jaccard.distance</span>(D),</a>
<a class="sourceLine" id="cb1021-10" data-line-number="10">              <span class="kw">cosine.similarity</span>(D), <span class="kw">conviction</span>(D),</a>
<a class="sourceLine" id="cb1021-11" data-line-number="11">              <span class="kw">kappa</span>(D), <span class="kw">gini.index</span>(D),  <span class="kw">yule.Q</span>(D),</a>
<a class="sourceLine" id="cb1021-12" data-line-number="12">              <span class="kw">yule.Y</span>(D), <span class="kw">odds.ratio</span>(D), <span class="kw">j.measure</span>(D),</a>
<a class="sourceLine" id="cb1021-13" data-line-number="13">              <span class="kw">imbalance.ratio</span>(D))</a>
<a class="sourceLine" id="cb1021-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb1021-15" data-line-number="15"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;T1&quot;</span>, <span class="st">&quot;T2&quot;</span>, <span class="st">&quot;T3&quot;</span>)</a>
<a class="sourceLine" id="cb1021-16" data-line-number="16"><span class="kw">rownames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Support&quot;</span>, <span class="st">&quot;Confidence&quot;</span>, <span class="st">&quot;Lift&quot;</span>, <span class="st">&quot;Chi-Squared&quot;</span>,</a>
<a class="sourceLine" id="cb1021-17" data-line-number="17">                <span class="st">&quot;Kulczynski&quot;</span>, <span class="st">&quot;Leverage (Piatetsky-Shapiro)&quot;</span>, </a>
<a class="sourceLine" id="cb1021-18" data-line-number="18">                <span class="st">&quot;Jaccard Coefficient&quot;</span>,  <span class="st">&quot;Jaccard Distance&quot;</span>, </a>
<a class="sourceLine" id="cb1021-19" data-line-number="19">                <span class="st">&quot;Cosine Similarity&quot;</span>, <span class="st">&quot;Conviction&quot;</span>,</a>
<a class="sourceLine" id="cb1021-20" data-line-number="20">                <span class="st">&quot;Kappa&quot;</span>, <span class="st">&quot;Gini Index&quot;</span>, <span class="st">&quot;Yule Q&quot;</span>, <span class="st">&quot;Yule Y&quot;</span>,</a>
<a class="sourceLine" id="cb1021-21" data-line-number="21">                <span class="st">&quot;Odds Ratio&quot;</span>, <span class="st">&quot;J-Measure&quot;</span>, <span class="st">&quot;Imbalance Ratio&quot;</span>)</a>
<a class="sourceLine" id="cb1021-22" data-line-number="22"><span class="kw">round</span>(m, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>##                                   T1       T2       T3
## Support                        0.321    0.068    0.068
## Confidence                     0.468    0.468    0.500
## Lift                           0.774    3.678    3.691
## Chi-Squared                  451.556 2219.674 2231.344
## Kulczynski                     0.500    0.500    0.500
## Leverage (Piatetsky-Shapiro)  -0.094    0.049    0.049
## Jaccard Coefficient            0.331    0.331    0.333
## Jaccard Distance               0.669    0.669    0.667
## Cosine Similarity              0.499    0.499    0.500
## Conviction                     0.743    1.825    1.729
## Kappa                         -0.407    0.419    0.422
## Gini Index                     0.082    0.039    0.042
## Yule Q                        -0.828    0.843    0.843
## Yule Y                        -0.531    0.548    0.549
## Odds Ratio                     0.107   13.333   11.765
## J-Measure                      0.151    0.073    0.074
## Imbalance Ratio                0.084    0.084    0.000</code></pre>

</div>
<div id="correlation-and-collinearity" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.5</span> Correlation and Collinearity  <a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Correlation</strong> measures the strength of relation amongst variables characterized by the linear movement in a positive (ascending) or negative (descending) direction. Thus, <strong>Correlation</strong> is also termed <strong>collinearity</strong> in its linear Correlation. <strong>Multicollinearity</strong> is a more common term for Correlation amongst multiple variables.</p>
<p>In this section, we cover Correlation (or collinearity) between random variables - starting with two variables - by showing the plots that they form. It helps to visualize the Correlation between distributions as this becomes our baseline and foundation in comparing with other unusual, skewed, or out-of-the-ordinary distributions.</p>
<p>Here, we introduce <strong>Correlation Coefficient</strong> to measure the correlation strength between two variables. A measure of zero means there is no correlation between the variables. A measure of one means a solidly positive correlation; otherwise, it is a solidly negative correlation. Below are the two <strong>Pearson r correlation</strong> formulae that render the same result.</p>

<p><span class="math display" id="eq:equate1110032">\[\begin{align}
Cor = \frac{n\sum(AB) - \sum{A} \sum{B}}
      {\sqrt{
        \left(n\sum A^2 - (\sum A)^2\right)
        \left(n\sum B^2 - (\sum B)^2\right)
       }}\ \ \ \ \ \ \ \ \text{range}: [-1,1] \tag{9.33} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1110033">\[\begin{align}
Cor = \frac{\sum (A - \overline{A})(B - \overline{B})}
{ \sqrt{\sum (A - \overline{A})^2} \sqrt{\sum(B - \overline{B})^2}}\ \ \ \ \ \ \ \ \text{range}: [-1,1] \tag{9.34} 
\end{align}\]</span>
</p>
<p>A simple implementation of the <strong>Correlation Coefficient</strong> is provided below:</p>

<div class="sourceCode" id="cb1023"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1023-1" data-line-number="1">correlation1 &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1023-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1023-3" data-line-number="3">  ( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>( A <span class="op">*</span><span class="st"> </span>B) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(A) <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(B) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1023-4" data-line-number="4"><span class="st">  </span><span class="kw">sqrt</span>( ( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(A<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(A)<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st">  </span>( n <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>(B<span class="op">^</span><span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>(B)<span class="op">^</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1023-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1023-6" data-line-number="6">correlation2 &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1023-7" data-line-number="7">  <span class="kw">sum</span> ( (A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A)) <span class="op">*</span><span class="st"> </span>(B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B)) ) <span class="op">/</span></a>
<a class="sourceLine" id="cb1023-8" data-line-number="8"><span class="st">  </span>( <span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A))<span class="op">^</span><span class="dv">2</span>)) <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B))<span class="op">^</span><span class="dv">2</span>))   )</a>
<a class="sourceLine" id="cb1023-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1023-10" data-line-number="10">correlation =<span class="st"> </span>correlation1</a></code></pre></div>

<p>Using Figure <a href="9.5-exploratory-data-analysis.html#fig:c1">9.35</a>, we show a correlation chart between two random variables. It illustrates a solid increasing diagonal line. The correlation is 100% - given that we are using two duplicated random variables to simulate correlation. The two variables are dependent. As one increases, so does the other. The correlation coefficient is 1, suggesting a solidly positive correlation. The correlation coefficient for the descending line is -1, suggesting a solidly negative correlation. This case of having two variables with a correlation coefficient of -1 or 1 also means that one of the two variables can be representative of the other - and as each one duplicates the other, the chances are that we choose one of them for our purpose, whatever it may be.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c1"></span>
<img src="DS_files/figure-html/c1-1.png" alt="Very Strong Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.35: Very Strong Correlation (Uniform vs Uniform)
</p>
</div>
<p>In reality, most datasets with some degree of correlation come with noise (perturbation or error). In a later section, we discuss data leakage, confounding variables, outliers, and missing values. As for the plot in Figure <a href="9.5-exploratory-data-analysis.html#fig:cc1">9.36</a>, we try to determine any <strong>linear</strong> correlation. The correlation coefficient is 0.9244, suggesting a strong positive correlation (due to the added noise).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cc1"></span>
<img src="DS_files/figure-html/cc1-1.png" alt="Very Strong Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.36: Very Strong Correlation (Uniform vs Uniform)
</p>
</div>
<p>Using Figure <a href="9.5-exploratory-data-analysis.html#fig:c5">9.37</a>, we show a correlation between a <strong>uniform</strong> distribution and a <strong>normal</strong> distribution. The plot follows a vertical convergence of data to the center for the <strong>normal</strong> distribution and an even (or uniform) horizontal distribution of data. That illustrates very low - to no - correlation, given that we are using two different random variables of two different types of distributions. Any pair of random variables will have a low correlation if a low percentage of the data closely matches. The correlation coefficient is 0.0252, suggesting a very weak correlation. This dataset type represents a constant timeline in which every moment captures some random event with uncertainty. Our goal is to estimate the point of convergence (the center or mean) of such a random event in a time series.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c5"></span>
<img src="DS_files/figure-html/c5-1.png" alt="Very Weak Correlation (Uniform vs Normal)" width="70%" />
<p class="caption">
Figure 9.37: Very Weak Correlation (Uniform vs Normal)
</p>
</div>
<p>Using Figure <a href="9.5-exploratory-data-analysis.html#fig:c3">9.38</a>, we show a correlation chart between two random variables of <strong>uniform</strong> distribution. That illustrates no correlation, given that we are using two independent random variables. That also indicates that the correlation between two different âuniformâ distributions shows data points covering a rectangular region. The correlation coefficient is 0.0054.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c3"></span>
<img src="DS_files/figure-html/c3-1.png" alt="No Correlation (Uniform vs Uniform)" width="70%" />
<p class="caption">
Figure 9.38: No Correlation (Uniform vs Uniform)
</p>
</div>
<p>In the next figure, let us use <strong>RMSE</strong> to measure the relationship and compare it with <strong>correlation coefficient</strong>. A measure of zero indicates no perturbation.  </p>
<p><span class="math display" id="eq:equate1110034">\[\begin{align}
RMSE = \sqrt{\frac{1}{n}\sum(A-B)^2}\ \ \ \ \ \ \ \ \text{range}: [0,\infty] \tag{9.35} 
\end{align}\]</span></p>
<p>A simple implementation of <strong>RMSE</strong> is provided below:</p>

<div class="sourceCode" id="cb1024"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1024-1" data-line-number="1">rmse &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1024-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1024-3" data-line-number="3">  <span class="kw">sqrt</span>(<span class="kw">mean</span>((A <span class="op">-</span><span class="st"> </span>B)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1024-4" data-line-number="4">}</a></code></pre></div>

<p>Using Figure <a href="9.5-exploratory-data-analysis.html#fig:c1">9.35</a>, we show a correlation chart between two random variables of <strong>normal</strong> distribution. It illustrates a more dispersed behavior (variance) converging to the center (mean). The correlation coefficient is 0.01 which suggests non-linear correlation. However, the <strong>RMSE</strong> is 1.2868. A zero <strong>RMSE</strong> indicates that the two variables have zero perturbation. An RMSE of around 1.2 indicates a standard deviation close to 1. In other words, RMSE increases as variance (error/perturbation) increases. Let us cover <strong>covariance</strong> later in the next section.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c2"></span>
<img src="DS_files/figure-html/c2-1.png" alt="Weak Correlation (Normal vs Normal)" width="70%" />
<p class="caption">
Figure 9.39: Weak Correlation (Normal vs Normal)
</p>
</div>

<p>Categorically, Figure <a href="9.5-exploratory-data-analysis.html#fig:c6">9.40</a> shows a correlation chart between a <strong>uniform</strong> distribution and a <strong>categorical</strong> distribution stratified from the same <strong>uniform</strong> distribution. Because we are using the same uniform distribution to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numeric categories (1,2,3). The distribution of categories is not spread evenly - because of the uniform distribution. Category 1 is spread across data points from -1.5 to 0.5. Category 2 is spread across data points from -0.5 to 0.5. Category 3 is spread across data points from 0.5 to 1.5.</p>

<div class="sourceCode" id="cb1025"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1025-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1025-2" data-line-number="2">n=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1025-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb1025-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">stratify</span>(x1, <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), n) </a>
<a class="sourceLine" id="cb1025-5" data-line-number="5"><span class="kw">plot</span>(x2, x1, <span class="dt">main=</span><span class="st">&quot;Strong Category (Uniform vs Stratified Uniform)&quot;</span>, </a>
<a class="sourceLine" id="cb1025-6" data-line-number="6">     <span class="dt">ylab=</span><span class="st">&quot;X1 (Uniform)&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;X2 (Stratified Uniform)&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c6"></span>
<img src="DS_files/figure-html/c6-1.png" alt="Strong Category (Uniform vs Stratified Uniform)" width="70%" />
<p class="caption">
Figure 9.40: Strong Category (Uniform vs Stratified Uniform)
</p>
</div>

<p>Categorically, Figure <a href="9.5-exploratory-data-analysis.html#fig:c7">9.41</a> shows a correlation chart between a <strong>uniform</strong> distribution and a <strong>categorical</strong> distribution stratified from another <strong>uniform</strong> distribution. Because we are using different uniform distributions to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numerical categories (1,2,3). The distribution of the categories is spread evenly across data points -1.5 to 1.5.</p>

<div class="sourceCode" id="cb1026"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1026-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1026-2" data-line-number="2">n=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1026-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>)</a>
<a class="sourceLine" id="cb1026-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">stratify</span>(<span class="kw">runif</span>(n, <span class="dt">min=</span><span class="op">-</span><span class="fl">1.5</span>, <span class="dt">max=</span><span class="fl">1.5</span>), <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>), n) </a>
<a class="sourceLine" id="cb1026-5" data-line-number="5"><span class="kw">plot</span>(x2, x1, <span class="dt">main=</span><span class="st">&quot;Uniform vs Stratified Uniform Non-Collinearity&quot;</span>, </a>
<a class="sourceLine" id="cb1026-6" data-line-number="6">     <span class="dt">ylab=</span><span class="st">&quot;Uniform&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Different Stratified Uniform&quot;</span>, <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:c7"></span>
<img src="DS_files/figure-html/c7-1.png" alt="Uniform vs Stratified Uniform Non-Collinearity" width="70%" />
<p class="caption">
Figure 9.41: Uniform vs Stratified Uniform Non-Collinearity
</p>
</div>

<p>With all that said, let us use a function called <strong>pairs(.)</strong> to generate a single plot for all pair-wise combinations of multiple variables. We use the <strong>mtcars</strong> dataset with a select number of variables (mpg, cyl, disp, hp, drat, carb). See Figure <a href="9.5-exploratory-data-analysis.html#fig:col1">9.42</a>. Notice that the mpg-disp pair seems to show a negative correlation. The mpg-drat pair seems to show a positive correlation. The cyl-disp pair seems to have a categorical correlation, however.</p>

<div class="sourceCode" id="cb1027"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1027-1" data-line-number="1"><span class="kw">pairs</span>(mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>, <span class="st">&quot;carb&quot;</span>)], <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:col1"></span>
<img src="DS_files/figure-html/col1-1.png" alt="Correlation pairs" width="100%" />
<p class="caption">
Figure 9.42: Correlation pairs
</p>
</div>

<p>When our dataset comes with multiple variables, we then use <strong>multicollinearity</strong> to measure correlation across multiple variables. To illustrate, let us introduce a third-party package in R called <strong>corrplot</strong>. We use <strong>corrplot(.)</strong> function to analyze <strong>collinearity</strong> in one plot similar to the <strong>pairs(.)</strong> function. See Figure <a href="9.5-exploratory-data-analysis.html#fig:cl1">9.43</a>. The color spectrum maps to the range [-1,1]. Colors closer to 1 indicate positive collinearity. Colors closer to -1 indicate negative collinearity. Otherwise, there is no collinearity. Here, we use the <strong>mtcars</strong> for our dataset, which comes with 11 variables.</p>

<div class="sourceCode" id="cb1028"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1028-1" data-line-number="1"><span class="kw">library</span>(corrplot)</a>
<a class="sourceLine" id="cb1028-2" data-line-number="2">mt &lt;-<span class="st"> </span><span class="kw">cor</span>(mtcars)</a>
<a class="sourceLine" id="cb1028-3" data-line-number="3"><span class="kw">corrplot</span>(mt, <span class="dt">method =</span> <span class="st">&quot;circle&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cl1"></span>
<img src="DS_files/figure-html/cl1-1.png" alt="Collinearity Matrix" width="50%" />
<p class="caption">
Figure 9.43: Collinearity Matrix
</p>
</div>

<p>Below, we select a few variables to compare collinearity.</p>

<div class="sourceCode" id="cb1029"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1029-1" data-line-number="1">mt =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>)]</a>
<a class="sourceLine" id="cb1029-2" data-line-number="2"><span class="kw">cor</span>(mt)</a></code></pre></div>
<pre><code>##          mpg     cyl    disp      hp    drat
## mpg   1.0000 -0.8522 -0.8476 -0.7762  0.6812
## cyl  -0.8522  1.0000  0.9020  0.8324 -0.6999
## disp -0.8476  0.9020  1.0000  0.7909 -0.7102
## hp   -0.7762  0.8324  0.7909  1.0000 -0.4488
## drat  0.6812 -0.6999 -0.7102 -0.4488  1.0000</code></pre>

<p>Notice that the cyl-hp pair has a collinearity coefficient of 0.8324475.</p>

<div class="sourceCode" id="cb1031"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1031-1" data-line-number="1">cyl =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;cyl&quot;</span>)]</a>
<a class="sourceLine" id="cb1031-2" data-line-number="2">hp =<span class="st"> </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;hp&quot;</span>)]</a>
<a class="sourceLine" id="cb1031-3" data-line-number="3"><span class="kw">correlation</span>(cyl,hp)</a></code></pre></div>
<pre><code>## [1] 0.8324</code></pre>

<p>Recall in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) about <strong>Dot Product</strong> and <strong>Determinant</strong>. To measure <strong>Collinearity</strong>, we can also use <strong>Dot Product</strong> and <strong>Determinant</strong>. If two vectors have a zero determinant, then the two vectors are not only <strong>Collinear</strong>, they are <strong>100% collinear</strong>, and thus one is enough to represent the other. If the <strong>Dot Product</strong>, however, is zero, then the two vectors are orthogonal, and therefore there is no <strong>Collinearity</strong>; otherwise, there is some degree of <strong>Collinearity</strong>.</p>
</div>
<div id="covariance" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.6</span> Covariance <a href="9.5-exploratory-data-analysis.html#covariance" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Datasets with <strong>gaussian distribution</strong> may be measured using <strong>covariance</strong> instead of <strong>collinearity</strong>. Below is our covariance formula:</p>

<p><span class="math display" id="eq:equate1110035">\[\begin{align}
\underbrace{Cov = \frac{\sum (A - \overline{A})(B - \overline{B})}{N}}_{\text{population covariance}}
\ \ \ \ \ \ \ \ 
\text{range}: [0,\infty] \tag{9.36} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1110036">\[\begin{align}
\underbrace{Cov = \frac{\sum (A - \overline{A})(B - \overline{B})}{(N-1)}}_{\text{sample covariance}}
\ \ \ \ \ \ \ \ 
\text{range}: [0,\infty] \tag{9.37} 
\end{align}\]</span>
</p>
<p>A simple implementation of the sample <strong>Covariance</strong> is provided below:</p>

<div class="sourceCode" id="cb1033"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1033-1" data-line-number="1">covariance &lt;-<span class="st"> </span><span class="cf">function</span>(A,B) {</a>
<a class="sourceLine" id="cb1033-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(A)</a>
<a class="sourceLine" id="cb1033-3" data-line-number="3">  <span class="kw">sum</span> ( (A <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(A)) <span class="op">*</span><span class="st"> </span>(B <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(B)) ) <span class="op">/</span><span class="st"> </span>(N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1033-4" data-line-number="4">}</a></code></pre></div>

<p>Let us use <strong>mtcars</strong> dataset and compare <strong>mpg</strong> and <strong>drat</strong> variables. We use three measures: Correlation, RMSE, and Covariance. Similar to RMSE, a high covariance indicates a high degree of deviation of the variables from the mean. A zero covariance means that all data points converge to the center.</p>

<div class="sourceCode" id="cb1034"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1034-1" data-line-number="1">mpg =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)]</a>
<a class="sourceLine" id="cb1034-2" data-line-number="2">drat =<span class="st"> </span>mtcars[,<span class="kw">c</span>(<span class="st">&quot;drat&quot;</span>)]</a>
<a class="sourceLine" id="cb1034-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;Correlation&quot;</span> =<span class="st"> </span><span class="kw">correlation</span>(mpg, drat),</a>
<a class="sourceLine" id="cb1034-4" data-line-number="4">  <span class="st">&quot;RMSE&quot;</span> =<span class="st"> </span><span class="kw">rmse</span>(mpg, drat),</a>
<a class="sourceLine" id="cb1034-5" data-line-number="5">  <span class="st">&quot;Covariance&quot;</span> =<span class="st"> </span><span class="kw">covariance</span>(mpg,drat))</a></code></pre></div>
<pre><code>## Correlation        RMSE  Covariance 
##      0.6812     17.4146      2.1951</code></pre>

</div>
<div id="outliers-leverage-influence" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.7</span> Outliers, Leverage, Influence   <a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>6</strong> (<strong>Statistical Computation</strong>), we covered the topic of <strong>Outliers, Leverage, and Influence</strong>.</p>
<p><strong>Outliers</strong> are data points with y values (response) that are outside the usual trend of the data.</p>
<p><strong>Leverage</strong> are data points with x values (predictors) outside the normal range of the data.</p>
<p><strong>Influence</strong> measures the effect (or Influence) of data points on the rest of the data.</p>

<div class="sourceCode" id="cb1036"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1036-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1036-2" data-line-number="2">beta0 =<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1036-3" data-line-number="3">beta1 =<span class="st"> </span><span class="fl">1.2</span></a>
<a class="sourceLine" id="cb1036-4" data-line-number="4">x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">30</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1036-5" data-line-number="5">random_noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">30</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1036-6" data-line-number="6">expected_y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>random_noise</a>
<a class="sourceLine" id="cb1036-7" data-line-number="7">model =<span class="st"> </span><span class="kw">lm</span>(expected_y <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb1036-8" data-line-number="8"><span class="kw">plot</span>(expected_y <span class="op">~</span><span class="st"> </span>x,   <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="dv">1</span>, <span class="dt">main=</span><span class="st">&quot;Goodness of Fit&quot;</span>, </a>
<a class="sourceLine" id="cb1036-9" data-line-number="9">      <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;Response&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Predictor&quot;</span>, </a>
<a class="sourceLine" id="cb1036-10" data-line-number="10">      <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">6</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">6</span>))</a>
<a class="sourceLine" id="cb1036-11" data-line-number="11"><span class="kw">abline</span>(model, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1036-12" data-line-number="12">new_x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>, <span class="fl">4.5</span>)</a>
<a class="sourceLine" id="cb1036-13" data-line-number="13">new_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> new_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>, <span class="dv">0</span>, <span class="fl">-.5</span>)</a>
<a class="sourceLine" id="cb1036-14" data-line-number="14"><span class="kw">points</span>(new_x, new_y, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1036-15" data-line-number="15">       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>))</a>
<a class="sourceLine" id="cb1036-16" data-line-number="16"><span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">inset=</span>.<span class="dv">02</span>, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,</a>
<a class="sourceLine" id="cb1036-17" data-line-number="17">       <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Outlier (Influential)&quot;</span>, <span class="st">&quot;Leverage&quot;</span>, </a>
<a class="sourceLine" id="cb1036-18" data-line-number="18">                <span class="st">&quot;Leverage (Influential)&quot;</span>), </a>
<a class="sourceLine" id="cb1036-19" data-line-number="19">       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>) )</a>
<a class="sourceLine" id="cb1036-20" data-line-number="20"></a>
<a class="sourceLine" id="cb1036-21" data-line-number="21">influence_x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1036-22" data-line-number="22">influence_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, </a>
<a class="sourceLine" id="cb1036-23" data-line-number="23">                      <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> influence_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1036-24" data-line-number="24">x1 =<span class="st"> </span><span class="kw">c</span>(x, influence_x)</a>
<a class="sourceLine" id="cb1036-25" data-line-number="25">y1 =<span class="st"> </span><span class="kw">c</span>(expected_y, influence_y)</a>
<a class="sourceLine" id="cb1036-26" data-line-number="26">influenced_model =<span class="st"> </span><span class="kw">lm</span> ( y1 <span class="op">~</span><span class="st"> </span>x1 )</a>
<a class="sourceLine" id="cb1036-27" data-line-number="27"><span class="kw">abline</span>(influenced_model, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1036-28" data-line-number="28"></a>
<a class="sourceLine" id="cb1036-29" data-line-number="29">influence_x =<span class="st"> </span><span class="kw">c</span>(<span class="fl">4.5</span>)</a>
<a class="sourceLine" id="cb1036-30" data-line-number="30">influence_y =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(model, </a>
<a class="sourceLine" id="cb1036-31" data-line-number="31">                      <span class="dt">newdata =</span>  <span class="kw">data.frame</span>(<span class="dt">x =</span> influence_x)) <span class="op">+</span><span class="st"> </span><span class="kw">c</span>(<span class="op">-</span>.<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1036-32" data-line-number="32">x1 =<span class="st"> </span><span class="kw">c</span>(x, influence_x)</a>
<a class="sourceLine" id="cb1036-33" data-line-number="33">y1 =<span class="st"> </span><span class="kw">c</span>(expected_y, influence_y)</a>
<a class="sourceLine" id="cb1036-34" data-line-number="34">influenced_model =<span class="st"> </span><span class="kw">lm</span> ( y1 <span class="op">~</span><span class="st"> </span>x1 )</a>
<a class="sourceLine" id="cb1036-35" data-line-number="35"><span class="kw">abline</span>(influenced_model, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:influence"></span>
<img src="DS_files/figure-html/influence-1.png" alt="Outliers, Leverages, Influentials" width="70%" />
<p class="caption">
Figure 9.44: Outliers, Leverages, Influentials
</p>
</div>

<p>For example, Figure <a href="9.5-exploratory-data-analysis.html#fig:influence">9.44</a> illustrates three data points (red dot, black dot, blue dot). The red dot is an outlier with a significant influence as its slope is steeper, pulling the fitted line further away from the usual trend - the red dotted line.</p>
<p>The data point with blue dot color is not an outlier as its y value sticks to the trend; nonetheless, it is considered leverage since its x value is extremely away from the normal range of the other data points. Notice also that it slightly affects the slope, as can be seen by the blue dotted line.</p>
<p>On the other hand, the data point with the black dot color does not influence since it can keep the slope unchanged - the orange line - even though it is considered leverage as its x value is extremely away from the normal range of the other data.</p>
</div>
<div id="dominating-factors" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.8</span> Dominating Factors <a href="9.5-exploratory-data-analysis.html#dominating-factors" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There may be situations in which we see categorical variables whose values are extremely uneven in that given two levels (categories), one level extremely dominates the other levels. See Figure <a href="9.5-exploratory-data-analysis.html#fig:dominating">9.45</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dominating"></span>
<img src="DS_files/figure-html/dominating-1.png" alt="Extreme Values" width="60%" />
<p class="caption">
Figure 9.45: Extreme Values
</p>
</div>
<p>Out of 1000 observations, 990 observations register a dominating value of 3.141593, whereas ten observations register a value of 2.718282. It would be good to trim (or remove) such variables if such extreme levels may not contribute.</p>
</div>
<div id="missingness-and-imputation" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.9</span> Missingness and Imputation  <a href="9.5-exploratory-data-analysis.html#missingness-and-imputation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we introduce the concept of missing data. One negative effect of missing data is that it reduces the statistical power and analyzability of observations. It also reduces the inference power of <strong>ML</strong> algorithms. For that reason, we need to be able to manage missing data.</p>
<p>There are three mechanisms of missingness that we need to be familiar with to manage missing data.</p>
<p><strong>MCAR</strong> (Missing completely at random) refers to missing data for no reason other than it being completely random. There is no dependency on the response or any observed or unobserved data. Therefore, this eliminates systematic bias. For example, given a list of observations, we simulate missing data by flipping a coin and removing values of observations if the coin lands on tails. In a real-world scenario, hardware storage may fail mechanically due to wear and tear, ,causing data loss. Without proper backup, some records may become corrupted and unrecoverable due to media failures or errors.</p>
<p><strong>MAR</strong> (Missing at random) refers to missing data for no reason other than it can be explained by other information (other factors). That is dependent on the observed data. For example, male participants may skip answering some questions in a survey. If we analyze the entire survey, we may find that 90% of females answered some specific questions that about 50% of male participants failed to answer. The missing data is not relevant to or dependent on the question itself; rather, it is dependent on the fact that the participantâs gender is male. Therefore, the missing data is dependent on other information in the observed data - in this case, gender is part of the survey (one of the questions in the survey).</p>
<p><strong>MNAR</strong> (Missing not at random) refers to missing data due to dependence on unobserved data or missing predictors. In the survey example above, the value of the missing data influences the probability of participants from willing to provide the value of the missing data due to some other reasons (that are not part of the survey - the unobserved data or missing predictor). One reason is if our survey includes personal or private information. Some questions may be skipped with no provided information. In such cases, sampled data may not become a true representative of the entire population because of bias.</p>
<p>To manage missing data, we introduce <strong>Imputation</strong>, a method to replace the missing data with estimates.</p>
<p>Indeed, the most simple solution for missing data is to ignore the records or perform <strong>row deletion</strong>. However, the consequence of deleting records is that it may create imbalances. For example, in the survey for <strong>MAR</strong>, 50% of male participants skipped answering some questions. Therefore, removing 50% of the male records creates <strong>BIAS</strong> towards female participants.</p>
<p>One form of <strong>imputation</strong> is to compute the average (the mean) across the observations. However, we need to be wary of the size of the missing data. To replace a large number of missing data using the average over a lesser number of available data creates <strong>invariability</strong>. We reduce variance because then data becomes closely similar to each other.</p>
<p>To solve this <strong>invariance</strong> problem, we can use the <strong>Hot Deck Method</strong>method. That is achieved by averaging a group of relevant predictors together and replacing the missing data only for those observations belonging to the respective group.</p>
<p>A better method but computationally costly is the <strong>Multiple Imputation</strong> method. That is achieved by performing <strong>regression (e.g., least-square) and prediction</strong> and then performing <strong>bayesian analysis</strong>. The idea is to generate <strong>M</strong> sub-samples from the overall observations - <strong>M</strong> being the number of imputations. For each sub-sample, we perform regression - fitting the data based on Least-Squares (as an example). We then use the regression model to predict all our missing data within the sub-sample. Next, we calculate the average or mean of the predicted values within each sub-sample. We repeat the process for all sub-samples. Afterwhich, we calculate the overall mean of all the sub-sample means. This overall mean becomes our replacement value. We also calculate the variance within each sub-sample to complement our value for discrepancies.</p>
<p>To illustrate, let us use a third-party R package called <strong>mice</strong> and use a dataset called <strong>airquality</strong>. Unfortunately, our dataset has missing data for the <strong>Ozone</strong> and <strong>Solar.R</strong> predictors:</p>

<div class="sourceCode" id="cb1037"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1037-1" data-line-number="1"><span class="co"># display only the first 10 observations.  </span></a>
<a class="sourceLine" id="cb1037-2" data-line-number="2"><span class="kw">head</span>(airquality, <span class="dt">n=</span> <span class="dv">10</span>)</a></code></pre></div>
<pre><code>##    Ozone Solar.R Wind Temp Month Day
## 1     41     190  7.4   67     5   1
## 2     36     118  8.0   72     5   2
## 3     12     149 12.6   74     5   3
## 4     18     313 11.5   62     5   4
## 5     NA      NA 14.3   56     5   5
## 6     28      NA 14.9   66     5   6
## 7     23     299  8.6   65     5   7
## 8     19      99 13.8   59     5   8
## 9      8      19 20.1   61     5   9
## 10    NA     194  8.6   69     5  10</code></pre>

<p>In the list, we see our <strong>Ozone</strong> predictor having two missing data (from the 1st displayed records) and two missing data for <strong>Solar.R</strong>. Assuming the <strong>missingness</strong> is <strong>MCAR</strong>. Using <strong>mice(.)</strong>, let us review some missing data patterns. Here, we exclude <strong>Month</strong> and <strong>Day</strong> columns. See Figure <a href="9.5-exploratory-data-analysis.html#fig:imputation">9.46</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:imputation"></span>
<img src="DS_files/figure-html/imputation-1.png" alt="Imputation" width="50%" />
<p class="caption">
Figure 9.46: Imputation
</p>
</div>
<pre><code>##     Wind Temp Solar.R Ozone   
## 111    1    1       1     1  0
## 35     1    1       1     0  1
## 5      1    1       0     1  1
## 2      1    1       0     0  2
##        0    0       7    37 44</code></pre>

<p>There are 153 observations in which the <strong>Zone</strong> predictor has 37 missing values and the <strong>Solar.R</strong> predictor has seven missing values.</p>
<p>Let us perform <strong>imputation</strong> using <strong>mice(.)</strong>. The function will run through 5 iterations for 2 imputations (m=2 sub-samples). We use <strong>Predictive mean matching (PMM)</strong> as our imputation method <span class="citation">(Morris T.P. et al. <a href="bibliography.html#ref-ref463">2014</a>)</span>. </p>

<div class="sourceCode" id="cb1040"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1040-1" data-line-number="1">imputed.datasets =<span class="st"> </span><span class="kw">mice</span>(missing.data, <span class="dt">m=</span><span class="dv">2</span>, <span class="dt">maxit=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb1040-2" data-line-number="2">                        <span class="dt">meth=</span><span class="st">&#39;pmm&#39;</span>, <span class="dt">seed=</span><span class="dv">2020</span>)</a></code></pre></div>
<pre><code>## 
##  iter imp variable
##   1   1  Ozone  Solar.R
##   1   2  Ozone  Solar.R
##   2   1  Ozone  Solar.R
##   2   2  Ozone  Solar.R
##   3   1  Ozone  Solar.R
##   3   2  Ozone  Solar.R
##   4   1  Ozone  Solar.R
##   4   2  Ozone  Solar.R
##   5   1  Ozone  Solar.R
##   5   2  Ozone  Solar.R</code></pre>
<div class="sourceCode" id="cb1042"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1042-1" data-line-number="1"><span class="kw">summary</span>(imputed.datasets)</a></code></pre></div>
<pre><code>## Class: mids
## Number of multiple imputations:  2 
## Imputation methods:
##   Ozone Solar.R    Wind    Temp 
##   &quot;pmm&quot;   &quot;pmm&quot;      &quot;&quot;      &quot;&quot; 
## PredictorMatrix:
##         Ozone Solar.R Wind Temp
## Ozone       0       1    1    1
## Solar.R     1       0    1    1
## Wind        1       1    0    1
## Temp        1       1    1    0</code></pre>

<p>To determine the imputed values calculated by <strong>mice(.)</strong>, we use the following format below. We use two imputations (m=2 sub-samples) and five iterations with seed=2020. The result below shows two dataset options from which to choose an imputed dataset. For example, the fifth observation with missing <strong>Solar.R</strong> value has an imputed value of 322 for the first dataset and an imputed value of 260 for the second dataset:</p>

<div class="sourceCode" id="cb1044"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1044-1" data-line-number="1"><span class="co"># display only the 1st 10 records</span></a>
<a class="sourceLine" id="cb1044-2" data-line-number="2"><span class="kw">head</span>( imputed.datasets<span class="op">$</span>imp<span class="op">$</span>Solar.R, <span class="dt">n =</span> <span class="dv">6</span>)</a></code></pre></div>
<pre><code>##      1   2
## 5   44 220
## 6  322 260
## 11   8   7
## 27   8  14
## 96 187 139
## 97  36 193</code></pre>

<p>Let us now complete the imputation by using <strong>complete(.)</strong> function. Below is the original dataset with missing data.</p>

<div class="sourceCode" id="cb1046"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1046-1" data-line-number="1"><span class="kw">head</span>(airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp Month Day
## 1    41     190  7.4   67     5   1
## 2    36     118  8.0   72     5   2
## 3    12     149 12.6   74     5   3
## 4    18     313 11.5   62     5   4
## 5    NA      NA 14.3   56     5   5
## 6    28      NA 14.9   66     5   6</code></pre>

<p>Let us choose the first dataset option for completing the imputation.</p>

<div class="sourceCode" id="cb1048"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1048-1" data-line-number="1">imputed.airquality =<span class="st"> </span><span class="kw">complete</span>(imputed.datasets,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1048-2" data-line-number="2"><span class="kw">head</span>(imputed.airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp
## 1    41     190  7.4   67
## 2    36     118  8.0   72
## 3    12     149 12.6   74
## 4    18     313 11.5   62
## 5    14      44 14.3   56
## 6    28     322 14.9   66</code></pre>

<p>Alternatively, we can choose the second dataset option, which produces different values for the missing data.</p>

<div class="sourceCode" id="cb1050"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1050-1" data-line-number="1">imputed.airquality =<span class="st"> </span><span class="kw">complete</span>(imputed.datasets,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1050-2" data-line-number="2"><span class="kw">head</span>(imputed.airquality, <span class="dt">n=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>##   Ozone Solar.R Wind Temp
## 1    41     190  7.4   67
## 2    36     118  8.0   72
## 3    12     149 12.6   74
## 4    18     313 11.5   62
## 5    14     220 14.3   56
## 6    28     260 14.9   66</code></pre>

<p>We leave readers to also investigate <strong>Local Residual Draws (LRD)</strong>. </p>
</div>
<div id="confounding-variable" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.10</span> Confounding Variable <a href="9.5-exploratory-data-analysis.html#confounding-variable" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To understand the concept of <strong>Confounding Variable</strong>, let us first review the linear formula below:</p>
<p><span class="math display" id="eq:equate1110037">\[\begin{align}
y = \beta_0 + \beta_1 \cdot X + err \tag{9.38} 
\end{align}\]</span></p>
<p>The <span class="math inline">\(\beta_0\)</span> represents an intercept, and <span class="math inline">\(\beta_1\)</span> is a coefficient (or a weight). The third term in the equation can be interpreted as noise (or maybe error) contributed by some unknown variable that is not included in the model. Because this variable is unknown, we treat it as noise. It has many interpretations, namely error, residual, perturbation, variation, or effect. This <strong>effect</strong> or <strong>residual</strong> is yet to be known. In such cases, this unknown variable (or a set of unknown variables) may be called a <strong>confounded</strong> variable if it contributes to noise (creating a non-zero covariance).</p>
<p>In exploring and analyzing our data, there are cases in which our data does not just come with one predictor variable but with multiple predictor variables. Take the Table <a href="9.5-exploratory-data-analysis.html#tab:climatechange1">9.33</a> listing contributing factors to climate change. There are three predictor variables: Humidity, Temperature, and Wind Velocity.</p>
<table>
<caption><span id="tab:climatechange1">Table 9.33: </span>Climate Change</caption>
<thead>
<tr class="header">
<th align="left">Date</th>
<th align="left">Humidity (%)</th>
<th align="left">Temperature (F)</th>
<th align="left">Wind Velocity (mph)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">January 01, 2018</td>
<td align="left">20</td>
<td align="left">70</td>
<td align="left">30</td>
</tr>
<tr class="even">
<td align="left">January 02, 2018</td>
<td align="left">13</td>
<td align="left">100</td>
<td align="left">10</td>
</tr>
</tbody>
</table>
<p>We use the three predictor variables to predict climate change (response variable). However, we know that the three predictors do not just influence climate change. Any or all of the following variables may contribute to climate change - deforestation, coal mining, greenhouse gases, and smog emissions. Notice that we just listed four more variables in the mix. Our task now is to study the effect of those additional predictor variables on climate change. That is where <strong>confounding variables</strong> may come into play. Mixing the right predictor variables is a challenge. However, if we mix unwanted input variables that distort our analysis, it is not in our desired position. A confounding variable tends to be described in terms of its bad significance. It means that its use in the model may cause chaos or distortion to the outcome. If so, in essence, a confounding variable is a bias variable - or an error variable, to put it simply. We commit an error in adding or excluding the variable if the variable distorts the outcome.</p>
</div>
<div id="data-leakage" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.11</span> Data Leakage <a href="9.5-exploratory-data-analysis.html#data-leakage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Data Leakage</strong> has a similar concept as <strong>Confounding Variable</strong> in terms of the effect of distorting outcomes. However, in this section, we deal with observations instead of predictors. <strong>Data leakage</strong> is a condition that happens when unseen data (e.g., from our testing set) gets mixed into our training set (and vice versa). Contaminating our training set distorts our outcome. Moreover, if we use the outcome as proof in announcing bold claims, it certainly creates an unwanted position.</p>
<p>One way to determine <strong>data leakage</strong> is to analyze the correlation between our training and test set. Moreover, one way to avoid <strong>data leakage</strong> is to have a separate validation set to evaluate our final model.</p>
<p>We also cover <strong>data leakage</strong> under <strong>General Modeling</strong> section.</p>
</div>
<div id="one-hot-encoding" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.12</span> One Hot Encoding <a href="9.5-exploratory-data-analysis.html#one-hot-encoding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>One Hot Encoding</strong> is a method used in predictions when the dataset contains both categorical and numerical data. To fit our data well into our prediction model, we transform our categorical data into one with a vertical format. See Figure <a href="9.5-exploratory-data-analysis.html#fig:onehot">9.47</a>. The <strong>weather</strong> feature is translated into a set of bit vectors. Each bit vector represents a unique category in the <strong>weather</strong> feature. For example, we create a new <strong>Sunny</strong> feature, representing a bit vector with 1 for all weather patterns that fall under sunny weather; otherwise, the bit is set to zero. Similarly, we create a new feature called <strong>Rainy</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onehot"></span>
<img src="onehot.png" alt="One Hot Encoding" width="70%" />
<p class="caption">
Figure 9.47: One Hot Encoding
</p>
</div>
<p>To illustrate in R code, let us first create a simple dataset. We include a categorical feature called <strong>Weather</strong> like so:</p>

<div class="sourceCode" id="cb1052"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1052-1" data-line-number="1">dataset =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1052-2" data-line-number="2">    <span class="dt">Weather =</span> <span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Sunny&quot;</span>),</a>
<a class="sourceLine" id="cb1052-3" data-line-number="3">    <span class="dt">Items =</span> <span class="kw">c</span>(<span class="st">&quot;A,C,D&quot;</span>, <span class="st">&quot;B,E&quot;</span>, <span class="st">&quot;A,C,D,E&quot;</span>, <span class="st">&quot;A,B&quot;</span>, <span class="st">&quot;B,F&quot;</span>),</a>
<a class="sourceLine" id="cb1052-4" data-line-number="4">    <span class="dt">Score =</span> <span class="kw">c</span>(<span class="fl">6.5</span>, <span class="fl">9.8</span>, <span class="fl">3.2</span>, <span class="fl">4.4</span>, <span class="fl">7.8</span>) </a>
<a class="sourceLine" id="cb1052-5" data-line-number="5">)</a>
<a class="sourceLine" id="cb1052-6" data-line-number="6">dataset</a></code></pre></div>
<pre><code>##   Weather   Items Score
## 1   Sunny   A,C,D   6.5
## 2   Rainy     B,E   9.8
## 3   Rainy A,C,D,E   3.2
## 4   Sunny     A,B   4.4
## 5   Sunny     B,F   7.8</code></pre>

<p>We then perform <strong>One Hot encoding</strong> on the <strong>weather</strong> feature using <strong>model.matrix(.)</strong> function. That creates a dataset with a list of bit vectors.</p>

<div class="sourceCode" id="cb1054"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1054-1" data-line-number="1">onehot =<span class="st"> </span><span class="kw">model.matrix</span>( <span class="op">~</span><span class="st"> </span><span class="dv">0</span> <span class="op">+</span><span class="st"> </span>Weather , <span class="dt">data =</span> dataset )</a>
<a class="sourceLine" id="cb1054-2" data-line-number="2"><span class="kw">colnames</span>(onehot) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Sunny&quot;</span>)</a>
<a class="sourceLine" id="cb1054-3" data-line-number="3">onehot[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span>]</a></code></pre></div>
<pre><code>##   Rainy Sunny
## 1     0     1
## 2     1     0
## 3     1     0
## 4     0     1
## 5     0     1</code></pre>

<p>Our next step is to incorporate the bit vectors into the dataset using <strong>mutate(.)</strong> function from <strong>dplyr</strong> package:</p>

<div class="sourceCode" id="cb1056"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1056-1" data-line-number="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1056-2" data-line-number="2">mutated.dataset =<span class="st"> </span><span class="kw">mutate</span>(dataset, <span class="dt">Weather=</span><span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1056-3" data-line-number="3">                         <span class="dt">Rainy =</span> onehot[,<span class="dv">1</span>], <span class="dt">Sunny =</span> onehot[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb1056-4" data-line-number="4">mutated.dataset [, <span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>, <span class="st">&quot;Items&quot;</span>, <span class="st">&quot;Score&quot;</span>)]</a></code></pre></div>
<pre><code>##   Sunny Rainy   Items Score
## 1     1     0   A,C,D   6.5
## 2     0     1     B,E   9.8
## 3     0     1 A,C,D,E   3.2
## 4     1     0     A,B   4.4
## 5     1     0     B,F   7.8</code></pre>

</div>
<div id="winsorization-and-trimming" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.13</span> Winsorization and Trimming  <a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The concept of <strong>Winsorization</strong> and <strong>Trimming</strong> is best explained with a dataset that follows a <strong>Gaussian Normal distribution</strong>. We see the right-most tail and the left-most tail representing extreme outliers in a Normal distribution. Instead of stretching our dataset to cover both extreme outliers, it may be necessary at times to <strong>trim off</strong> those outliers - hence, the term <strong>trimming</strong>. For example, any data point below the 5% quartile or above the 95% quartile is excluded from the dataset. Our dataset covers only the range between the 5% quartile and 95% quartile.</p>
<p>Another way to handle outliers is by <strong>Winsorization</strong>. The idea is to take the values right at the exact 5% quartile and 95% quartile. We then use the two values to replace any outlier below the 5% or above the 95% quartile. This concept is almost similar to <strong>Missingness and Imputation</strong>, in which we replace missing values with the average values of our observation (granting we choose the simplest method). In the case of <strong>Winsorization</strong>, we replace extreme values with a cut-off value taken at specified boundaries.</p>
<p>To illustrate, let us apply <strong>winsorization</strong> in R code using a third-party package called <strong>DescTools</strong> along with its function <strong>Winsorize(.)</strong>. To achieve <strong>Winsorization</strong>, let us simulate a dataset evenly spread between -4 and 4. We then winsorize at -3 and 3.</p>

<div class="sourceCode" id="cb1058"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1058-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1058-2" data-line-number="2"><span class="kw">library</span>(DescTools)</a>
<a class="sourceLine" id="cb1058-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1058-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>, <span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1058-5" data-line-number="5">dataset</a></code></pre></div>
<pre><code>##  [1] -4.0000 -3.1111 -2.2222 -1.3333 -0.4444  0.4444
##  [7]  1.3333  2.2222  3.1111  4.0000</code></pre>
<div class="sourceCode" id="cb1060"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1060-1" data-line-number="1"><span class="kw">Winsorize</span>(dataset, <span class="dt">minval=</span><span class="op">-</span><span class="fl">3.0</span>, <span class="dt">maxval =</span> <span class="fl">3.0</span> ) </a></code></pre></div>
<pre><code>##  [1] -3.0000 -3.0000 -2.2222 -1.3333 -0.4444  0.4444
##  [7]  1.3333  2.2222  3.0000  3.0000</code></pre>

</div>
<div id="discretization" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.14</span> Discretization <a href="9.5-exploratory-data-analysis.html#discretization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Discretization</strong>, also called <strong>Binning</strong>, is the method of replacing a continuous number with its discretized counterpart. We divide the continuous space into discrete intervals called <strong>bins</strong> and then assign continuous numbers to its corresponding <strong>bin</strong>. To illustrate, let us use <strong>discretize(.)</strong> function found in <strong>arules</strong> package.</p>

<div class="sourceCode" id="cb1062"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1062-1" data-line-number="1"><span class="kw">library</span>(arules)</a>
<a class="sourceLine" id="cb1062-2" data-line-number="2">data =<span class="st"> </span><span class="kw">round</span>( <span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">12</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb1062-3" data-line-number="3">discrete =<span class="st"> </span><span class="kw">discretize</span>(data, <span class="dt">method=</span><span class="st">&quot;frequency&quot;</span>, <span class="dt">breaks=</span><span class="dv">4</span>, </a>
<a class="sourceLine" id="cb1062-4" data-line-number="4">                      <span class="dt">labels=</span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>, <span class="st">&quot;D&quot;</span>))</a>
<a class="sourceLine" id="cb1062-5" data-line-number="5">transformed.data =<span class="st"> </span><span class="kw">data.frame</span>(</a>
<a class="sourceLine" id="cb1062-6" data-line-number="6">  <span class="dt">continuous.form =</span> data,</a>
<a class="sourceLine" id="cb1062-7" data-line-number="7">  <span class="dt">discrete.form =</span> <span class="kw">c</span>(discrete),</a>
<a class="sourceLine" id="cb1062-8" data-line-number="8">  <span class="dt">label =</span> <span class="kw">factor</span>(discrete)</a>
<a class="sourceLine" id="cb1062-9" data-line-number="9">)</a>
<a class="sourceLine" id="cb1062-10" data-line-number="10">transformed.data </a></code></pre></div>
<pre><code>##    continuous.form discrete.form label
## 1             0.00             1     A
## 2             0.09             1     A
## 3             0.18             1     A
## 4             0.27             2     B
## 5             0.36             2     B
## 6             0.45             2     B
## 7             0.55             3     C
## 8             0.64             3     C
## 9             0.73             3     C
## 10            0.82             4     D
## 11            0.91             4     D
## 12            1.00             4     D</code></pre>

<p>In our application of discretization, we use four intervals corresponding to four bins which are given labels (A, B, C, D). Another term for <strong>bin</strong> is <strong>category</strong> or <strong>level</strong>. Below, we use the function <strong>levels(.)</strong> to show the category used:</p>
<div class="sourceCode" id="cb1064"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1064-1" data-line-number="1"><span class="kw">levels</span>(discrete)</a></code></pre></div>
<pre><code>## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot;</code></pre>
<p>If we omit the label, then we see the intervals used for the discrete counterparts:</p>

<div class="sourceCode" id="cb1066"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1066-1" data-line-number="1">discrete =<span class="st"> </span><span class="kw">discretize</span>(data, <span class="dt">method=</span><span class="st">&quot;frequency&quot;</span>, <span class="dt">breaks=</span><span class="dv">4</span>, <span class="dt">labels  =</span> <span class="ot">NULL</span>)</a>
<a class="sourceLine" id="cb1066-2" data-line-number="2"><span class="kw">levels</span>(discrete)</a></code></pre></div>
<pre><code>## [1] &quot;[0,0.247)&quot;   &quot;[0.247,0.5)&quot; &quot;[0.5,0.752)&quot;
## [4] &quot;[0.752,1]&quot;</code></pre>

</div>
<div id="stratification" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.15</span> Stratification <a href="9.5-exploratory-data-analysis.html#stratification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Stratification</strong>, also called <strong>Segmenting</strong> or <strong>Stratifying</strong>, is a method to sort observations into groups (<strong>strata</strong>). An individual group is called the <strong>stratum</strong>.</p>
<p>To illustrate, we use a 3rd-party package called <strong>splitstackshape</strong>. A function to use for stratification is <strong>stratified(.)</strong>. Now, suppose we have the following dataset.</p>

<div class="sourceCode" id="cb1068"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1068-1" data-line-number="1"><span class="kw">library</span>(splitstackshape)</a>
<a class="sourceLine" id="cb1068-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1068-3" data-line-number="3">N=<span class="dv">10</span></a>
<a class="sourceLine" id="cb1068-4" data-line-number="4">dataset =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">Weather =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&quot;Sunny&quot;</span>, <span class="st">&quot;Rainy&quot;</span>), <span class="dt">size=</span>N, </a>
<a class="sourceLine" id="cb1068-5" data-line-number="5">                                       <span class="dt">replace=</span><span class="ot">TRUE</span>),  </a>
<a class="sourceLine" id="cb1068-6" data-line-number="6">                <span class="dt">Participation =</span> <span class="kw">rbinom</span>(<span class="dt">n =</span> N, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">prob=</span><span class="fl">0.50</span>) ) </a>
<a class="sourceLine" id="cb1068-7" data-line-number="7">dataset</a></code></pre></div>
<pre><code>##    Weather Participation
## 1    Rainy            57
## 2    Sunny            53
## 3    Rainy            60
## 4    Sunny            46
## 5    Sunny            53
## 6    Sunny            50
## 7    Sunny            54
## 8    Sunny            59
## 9    Sunny            47
## 10   Rainy            42</code></pre>

<p>Let us stratify the dataset using <strong>stratified(.)</strong>. Our goal is to sort the activities into sample groups of rainy and sunny weather. After grouping the observations, we then take a sample per group, given a specific sample size - it can be a fraction of the total size of the sample (here, we use 50%).</p>

<div class="sourceCode" id="cb1070"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1070-1" data-line-number="1">(<span class="dt">strata =</span> <span class="kw">stratified</span>(<span class="dt">indt=</span>dataset, <span class="dt">group=</span><span class="kw">c</span>(<span class="st">&quot;Weather&quot;</span>), <span class="dt">size=</span><span class="fl">0.50</span>))</a></code></pre></div>
<pre><code>##    Weather Participation
## 1:   Rainy            60
## 2:   Rainy            57
## 3:   Sunny            47
## 4:   Sunny            46
## 5:   Sunny            54
## 6:   Sunny            50</code></pre>

<p>To validate, let us startify the dataset by using a combination of functions from <strong>dplyr</strong> package, namely <strong>group_by(.)</strong> and <strong>sample_frac(.)</strong>.</p>

<div class="sourceCode" id="cb1072"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1072-1" data-line-number="1">strata =<span class="st"> </span>dataset <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">group_by</span>(Weather) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sample_frac</span>(<span class="fl">0.50</span>)</a>
<a class="sourceLine" id="cb1072-2" data-line-number="2"><span class="kw">as.data.frame</span>(strata)</a></code></pre></div>
<pre><code>##   Weather Participation
## 1   Rainy            60
## 2   Rainy            42
## 3   Sunny            50
## 4   Sunny            54
## 5   Sunny            47
## 6   Sunny            53</code></pre>

<p>Our dataset is grouped by Weather. We can summarize each group using two functions.</p>
<p>using <strong>aggregate</strong>:</p>

<div class="sourceCode" id="cb1074"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1074-1" data-line-number="1"><span class="kw">aggregate</span>(dataset<span class="op">$</span>Participation, <span class="dt">by=</span><span class="kw">list</span>(<span class="dt">Weather=</span>dataset<span class="op">$</span>Weather), </a>
<a class="sourceLine" id="cb1074-2" data-line-number="2">          <span class="dt">FUN=</span>length)</a></code></pre></div>
<pre><code>##   Weather x
## 1   Rainy 3
## 2   Sunny 7</code></pre>

<p>using <strong>tapply</strong>:</p>
<div class="sourceCode" id="cb1076"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1076-1" data-line-number="1">(<span class="dt">group.size =</span> <span class="kw">tapply</span>(dataset<span class="op">$</span>Participation, dataset<span class="op">$</span>Weather, </a>
<a class="sourceLine" id="cb1076-2" data-line-number="2">                     <span class="dt">FUN=</span>length))</a></code></pre></div>
<pre><code>## Rainy Sunny 
##     3     7</code></pre>
<p>The <strong>Rainy</strong> group has a size of 3 and the <strong>Sunny</strong> group has a size 7. If we apply a sample fraction of 50%, then we have the following sample size per group:</p>
<p>Sample Size for Rainy group: 3 <span class="math inline">\(\times\)</span> 0.50 = 2.</p>
<p>Sample Size for Sunny group: 7 <span class="math inline">\(\times\)</span> 0.50 = 4.</p>
</div>
<div id="fine-and-coarse-classing" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.16</span> Fine and Coarse Classing<a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Dataset may come with continuous independent variables. Therefore, there may be cases when we need to transform continuous independent variables into a discrete form before classifying them. One method we can use for the transformation is <strong>Binning</strong>, which we covered in <strong>Computational Learning I</strong> under the <strong>EDA</strong> section. Here, we call such <strong>Binning</strong> either <strong>Fine Classing</strong> or <strong>Coarse Classing</strong>, which groups the independent variable into K groups. <strong>Fine Classing</strong> is binning features with manageable cardinality, e.g., 20 and less.</p>
<p>On the other hand, <strong>Coarse Classing</strong> tries to combine smaller <strong>Fine Classes</strong> into larger classes. A common method is called <strong>Weight of Evidence (WoE)</strong> and <strong>Information Value (IV)</strong> for interaction which <strong>flattens</strong> a distribution. For example, we can use log-odds to transform the dataset into a dichotomous dataset given a continuous distribution. Such methods try to address concerns about losing information during the transformation. </p>
<p><span class="math display" id="eq:equate1110038">\[\begin{align}
\text{WoE}_j = \ \log_e(\text{Odds)} = \ \log_e \left[\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\right] \tag{9.39} 
\end{align}\]</span></p>
<p><span class="math display">\[
\text{where Odds} = \frac{\% \text{ Positive}}{\% \text{ Negative}}
\]</span></p>
<p>Such formula is related to <strong>Naive Bayes</strong> in which we have the following:</p>
<p><span class="math display" id="eq:equate1110039">\[\begin{align}
\underbrace{\log_e \left[\frac{P(Y=1|X = x_j)}{P(Y = 0|X = x_j)}\right]}_{\text{posterior}} = 
\underbrace{\log_e \left[\frac{P(Y=1)}{P(Y = 0)}\right]}_{\text{prior}} +
\underbrace{\log_e \left[\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\right]}_{\text{likelihood}} \tag{9.40} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(x_j \in B_i\)</span> such that <strong>B</strong> is a set of bins.</p>
<p>Because <strong>prior</strong> excludes the <strong>X</strong> feature, we can ignore the <strong>Prior</strong> and thus end with the <strong>WeO</strong> formula.</p>
<p>To then measure the predictive power of a feature, we can use <strong>IV</strong>:</p>
<p><span class="math display" id="eq:eqnnumber400">\[\begin{align}
\begin{array}{lll}
\text{IV}_j &amp;= \text{WoE}_j \times (\% \text{Positive } - \% \text{Negative}) \\
&amp;= \text{WoE}_j \times \left(\sum_i^k \left[P(x_j \in B_i|Y=1) - P(x_j \in B_i|Y=0)\right]  \right)
\end{array} \tag{9.41}
\end{align}\]</span></p>
<p>Based on <strong>IV</strong>, we can then interpret the predictive power like so (as an example):</p>

<p><span class="math display">\[
\begin{array}{ll}
0\% \le \text{IV} &lt; 20\% &amp; \text{Not Predictive (Exclude)} \\
20\% \le \text{IV} &lt; 40\% &amp; \text{Weak} \\
40\% \le \text{IV} &lt; 70\% &amp; \text{Medium} \\
70\% \le \text{IV} \le 100\% &amp; \text{Highly Predictive (Strong)} \\
\end{array}
\]</span>
</p>
<p>We leave readers to investigate further the use of <strong>WoE</strong> and <strong>IV</strong> in terms of feature transformation and selection, excluding the need to handle missing values, dummy variables, and outliers.</p>
</div>
<div id="embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">9.5.17</span> Embedding <a href="9.5-exploratory-data-analysis.html#embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Other types of data that we also have to explore deal with pictures (images), videos, music, and natural languages (words and sentences). To process these types of data computationally, somehow, we need to transform or map them into their numerical representation, structured in the form of matrices or vectors). Such an <strong>interpretable</strong> representation of a particular data type is called <strong>Embedding</strong>.</p>
<p><strong>Embedding</strong> has many definitions on the net (a few of which are):</p>
<ul>
<li>a mapping from high dimensional space to lower-dimensional space.</li>
<li>a transformation of discrete (categorical) data into its continuous numerical representation.</li>
</ul>
<p>A few examples of embeddings are:</p>
<ul>
<li>images are mapped to a 3-D matrix (e.g., convnet).</li>
<li>words are mapped into a vector (e.g., word2vec).</li>
<li>medical codes are mapped into multi-layered representation (e.g., med2vec).</li>
</ul>
<p>In this section, let us use a simple embedding from a bag of words to a vector.</p>
<p><strong>First</strong>, let us define a few terms:</p>
<ul>
<li><strong>Corpus</strong> - a collection of documents or texts (written or spoken).</li>
<li><strong>Documents</strong> - a collection of thoughts (that are informational).</li>
<li><strong>Dictionary</strong> - a collection of unique words (with lexical and semantic meaning).</li>
<li><strong>Term</strong> - is another synonym for word.</li>
</ul>
<p><strong>Second</strong>, let us collect a corpus of documents like so:</p>
<ul>
<li>Document 1 - Tomorrow will be another day, but today is a great day.</li>
<li>Document 2 - I have a great day today.</li>
<li>Document 3 - It is a wonderful day.</li>
</ul>
<p><strong>Third</strong>, let us construct our dictionary of words (terms) and count the occurrence per word. See Figure <a href="9.5-exploratory-data-analysis.html#fig:corpus">9.48</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:corpus"></span>
<img src="corpus.png" alt="Embedding (Vector Space Model)" width="85%" />
<p class="caption">
Figure 9.48: Embedding (Vector Space Model)
</p>
</div>
<p>We introduce <strong>stop words</strong> (table <strong>T1</strong>) in the example to note that certain words are common enough to be filtered out as they may not add value (instead of add distortion or skewness) to our analysis. Depending on necessity, our list of <strong>stop words</strong> may vary.</p>
<p>Table <strong>T2</strong> represents our dictionary - a list of vocabulary.</p>
<p><strong>Fourth</strong>, let us map the three documents based on our generated vocabulary. Three documents are mapped to a <strong>term-document matrix</strong> (see table <strong>T3</strong>) - this simple embedding is called <strong>vector space model (VSM)</strong>.</p>
<p><strong>Finally</strong>, the use of embedding becomes apparent when we search for a document given a query. For example, suppose we are searching for a document, and we have the following search query:</p>
<p><span class="math display">\[
\text{Query = &quot;great day today&quot;}.
\]</span></p>
<p>Let us discuss two simple solutions to help us rank our documents and retrieve the document with the highest rank.</p>
<p>The first solution is called <strong>simple VSM</strong>, which counts for the existence of unique terms in each document. So, for example, there are three unique terms in our query that exist in D1 and D2. Moreover, only one unique term in our query exists in D3.</p>
<p><span class="math display">\[
\text{search(Query, D1) = 3}\ \ \ \ \ \ \ \
\text{search(Query, D2) = 3}\ \ \ \ \ \ \ \ 
\text{search(Query, D3) = 1}
\]</span>
Therefore, the documents we need to retrieve are D1 and D2.</p>
<p>A better approach is called <strong>Term-frequency (TF) weighing</strong>, which accounts for duplicate terms. For example:</p>
<p><span class="math display">\[
\text{search(Query, D1)} = \underbrace{1}_{\text{great}} + \underbrace{2}_{\text{day}} + \underbrace{1}_{\text{today}} = 4
\]</span></p>
<p><span class="math display">\[
\text{search(Query, D2)} = \underbrace{1}_{\text{great}} + \underbrace{1}_{\text{day}} + \underbrace{1}_{\text{today}} = 3
\]</span></p>
<p>Therefore, the document we need to retrieve is D1.</p>
<p>In Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), we introduce <strong>TF-IDF</strong> and <strong>Cosine Similarity</strong>, which are covered in <strong>Text Mining</strong>, complementing and enhancing document ranking.</p>
<p>We leave readers to investigate <strong>word2vec</strong>, <strong>med2vec</strong>, and <strong>convnet</strong> for some well-known <strong>embeddings</strong>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="9.4-distance-metrics.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="9.6-featureengineering.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
