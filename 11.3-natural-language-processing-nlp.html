<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.3 Natural Language Processing (NLP)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="11.3 Natural Language Processing (NLP)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.3 Natural Language Processing (NLP)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="11.2-meta-learning.html"/>
<link rel="next" href="11.4-time-series-forecasting.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#AdaBoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoostandSAMME"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost and SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-learning-deep-rl.html"><a href="13.8-deep-reinforcement-learning-deep-rl.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Learning (Deep RL)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="natural-language-processing-nlp" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.3</span> Natural Language Processing (NLP)  <a href="11.3-natural-language-processing-nlp.html#natural-language-processing-nlp" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Natural Language Processing (NLP)</strong> is covered in <strong>Information Retrieval and Text Mining</strong> Theory <span class="citation">(Zhai, C., &amp; Massung, C. <a href="bibliography.html#ref-ref1947c">2016</a>)</span>. However, other literature covers <strong>NLP</strong> in <strong>Artificial Intelligence</strong> that deals with the computational processing of human languages based on <strong>Word Embeddings</strong> - a topic worth covering in the last chapter.</p>
<p>A better way to explain <strong>NLP</strong> is to start with a simple list of sample documents - consider this as our <strong>corpus</strong> for now:</p>

<div class="sourceCode" id="cb1589"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1589-1" data-line-number="1">D1 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;There is a big animal in my house.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-2" data-line-number="2">D2 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;The animal has great big ears, great big eyes, great big teeth.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-3" data-line-number="3">D3 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A wolf is an animal.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-4" data-line-number="4">D4 =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Little Red Riding Hood met a Wolf.&quot;</span>)</a>
<a class="sourceLine" id="cb1589-5" data-line-number="5">(<span class="dt">documents =</span> <span class="kw">c</span>(D1, D2, D3, D4))</a></code></pre></div>
<pre><code>## [1] &quot;There is a big animal in my house.&quot;                             
## [2] &quot;The animal has great big ears, great big eyes, great big teeth.&quot;
## [3] &quot;A wolf is an animal.&quot;                                           
## [4] &quot;Little Red Riding Hood met a Wolf.&quot;</code></pre>

<p>In our sample list of documents, we extract words and convert the list into a vector of words - we call this vector as <strong>bag of words</strong>. It is raw and needs cleanup. Our goal is to perform pre-processing which includes dealing with <strong>tokenization</strong>, <strong>case-sensitivity</strong>, <strong>stopwords</strong>, <strong>N-grams</strong>, <strong>stemming</strong>, and <strong>lemmatization</strong> to name a few.</p>
<div id="pre-processing-texts" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.1</span> Pre-Processing Texts<a href="11.3-natural-language-processing-nlp.html#pre-processing-texts" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Part of the pre-processing text is to tokenize terms in documents. <strong>Tokenization</strong> is extracting units of text information called <strong>tokens</strong>. A token is represented as a <strong>word</strong> (or a <strong>term</strong>) in a document. Below, we extract words and map them into a vector.</p>

<div class="sourceCode" id="cb1591"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1591-1" data-line-number="1">(<span class="dt">words2vector =</span> <span class="kw">unlist</span>( <span class="kw">strsplit</span>(documents, <span class="dt">split=</span><span class="st">&#39; &#39;</span>)))</a></code></pre></div>
<pre><code>##  [1] &quot;There&quot;   &quot;is&quot;      &quot;a&quot;       &quot;big&quot;     &quot;animal&quot;  &quot;in&quot;     
##  [7] &quot;my&quot;      &quot;house.&quot;  &quot;The&quot;     &quot;animal&quot;  &quot;has&quot;     &quot;great&quot;  
## [13] &quot;big&quot;     &quot;ears,&quot;   &quot;great&quot;   &quot;big&quot;     &quot;eyes,&quot;   &quot;great&quot;  
## [19] &quot;big&quot;     &quot;teeth.&quot;  &quot;A&quot;       &quot;wolf&quot;    &quot;is&quot;      &quot;an&quot;     
## [25] &quot;animal.&quot; &quot;Little&quot;  &quot;Red&quot;     &quot;Riding&quot;  &quot;Hood&quot;    &quot;met&quot;    
## [31] &quot;a&quot;       &quot;Wolf.&quot;</code></pre>

<p>Now, there is pre-processing to be made when tokenizing documents. Let us cover a few of them:</p>
<p><strong>Case-Sensitivity</strong></p>
<p>If we do not care much about <strong>case sensitive characters</strong>, we may choose to set all words in small cases:</p>

<div class="sourceCode" id="cb1593"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1593-1" data-line-number="1">(<span class="dt">documents =</span> <span class="kw">tolower</span>(documents))</a></code></pre></div>
<pre><code>## [1] &quot;there is a big animal in my house.&quot;                             
## [2] &quot;the animal has great big ears, great big eyes, great big teeth.&quot;
## [3] &quot;a wolf is an animal.&quot;                                           
## [4] &quot;little red riding hood met a wolf.&quot;</code></pre>

<p><strong>Punctuations</strong></p>
<p>Note that documents have <strong>punctuations</strong> or special whitespaces that we need to remove.</p>

<div class="sourceCode" id="cb1595"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1595-1" data-line-number="1">(<span class="dt">documents =</span> <span class="kw">gsub</span>(<span class="st">&quot;[[:punct:]]&quot;</span>, <span class="st">&quot;&quot;</span>, documents))</a></code></pre></div>
<pre><code>## [1] &quot;there is a big animal in my house&quot;                           
## [2] &quot;the animal has great big ears great big eyes great big teeth&quot;
## [3] &quot;a wolf is an animal&quot;                                         
## [4] &quot;little red riding hood met a wolf&quot;</code></pre>

<p><strong>StopWords</strong></p>
<p>Apparently, we also notice that certain terms such as (<strong>is</strong>, <strong>a</strong>, <strong>an</strong>, <strong>in</strong>, <strong>the</strong>) are common in documents that may give little to no <strong>information</strong>. We can treat them as <strong>stopwords</strong> and can be removed. For <strong>stopwords</strong>, let us use <strong>anti_join(.)</strong> function from <strong>dplyr</strong> library. Also, let us use <strong>tibble(.)</strong> to convert the documents to <strong>tibble</strong> data frame. Here, we use <strong>unnest_tokens(.)</strong> from <strong>tidytext</strong> library to tokenize.</p>

<div class="sourceCode" id="cb1597"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1597-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1597-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1597-3" data-line-number="3">stopwords =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;is&quot;</span>, <span class="st">&quot;a&quot;</span>, <span class="st">&quot;an&quot;</span>, <span class="st">&quot;has&quot;</span>, <span class="st">&quot;in&quot;</span>, <span class="st">&quot;the&quot;</span>, <span class="st">&quot;there&quot;</span>)</a>
<a class="sourceLine" id="cb1597-4" data-line-number="4">stopwords.frame  =<span class="st"> </span><span class="kw">tibble</span>( <span class="dt">term =</span> stopwords)</a>
<a class="sourceLine" id="cb1597-5" data-line-number="5">doc.frame =<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">docid =</span> <span class="kw">c</span>(<span class="st">&#39;D1&#39;</span>,<span class="st">&#39;D2&#39;</span>,<span class="st">&#39;D3&#39;</span>,<span class="st">&#39;D4&#39;</span>), <span class="dt">term =</span> documents)</a>
<a class="sourceLine" id="cb1597-6" data-line-number="6">tokenized.table =<span class="st">  </span>doc.frame  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1597-7" data-line-number="7"><span class="st">        </span><span class="kw">unnest_tokens</span>(term, term) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1597-8" data-line-number="8"><span class="st">        </span><span class="kw">anti_join</span>(stopwords.frame,  <span class="dt">by =</span> <span class="kw">c</span>(<span class="st">&quot;term&quot;</span> =<span class="st"> &quot;term&quot;</span> ))</a></code></pre></div>

<p>We now have the final pre-processed list of words above.</p>

<div class="sourceCode" id="cb1598"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1598-1" data-line-number="1">preprocessed.doc &lt;-<span class="st"> </span><span class="cf">function</span>(t) {</a>
<a class="sourceLine" id="cb1598-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(tokenized.table)</a>
<a class="sourceLine" id="cb1598-3" data-line-number="3">  ids =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;D1&quot;</span>, <span class="st">&quot;D2&quot;</span>, <span class="st">&quot;D3&quot;</span>, <span class="st">&quot;D4&quot;</span>)</a>
<a class="sourceLine" id="cb1598-4" data-line-number="4">  docs =<span class="st"> </span><span class="kw">rep</span>(<span class="st">&quot;&quot;</span>, n)</a>
<a class="sourceLine" id="cb1598-5" data-line-number="5">  <span class="cf">for</span> (i  <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(ids)) {</a>
<a class="sourceLine" id="cb1598-6" data-line-number="6">    idx =<span class="st"> </span><span class="kw">which</span>(tokenized.table<span class="op">$</span>docid <span class="op">==</span><span class="st"> </span>ids[i])</a>
<a class="sourceLine" id="cb1598-7" data-line-number="7">    docs[i] =<span class="st"> </span><span class="kw">paste0</span>(tokenized.table<span class="op">$</span>term[idx], <span class="dt">sep=</span><span class="st">&quot; &quot;</span>, <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1598-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1598-9" data-line-number="9">  docs</a>
<a class="sourceLine" id="cb1598-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1598-11" data-line-number="11">(<span class="dt">documents =</span> <span class="kw">preprocessed.doc</span>(tokenized.table))</a></code></pre></div>
<pre><code>## [1] &quot;big animal my house &quot;                                 
## [2] &quot;animal great big ears great big eyes great big teeth &quot;
## [3] &quot;wolf animal &quot;                                         
## [4] &quot;little red riding hood met wolf &quot;</code></pre>

<p><strong>N-Gram</strong> </p>
<p>At times, in <strong>Text Mining</strong>, it helps to consider also a group of words and not just individual words. For example, a sequence of one, two, and three words is called a <strong>unigram</strong>, <strong>bigram</strong>, and <strong>trigram</strong>, respectively. For example, below, we generate a sequence of <strong>bigram</strong> tokens.</p>

<div class="sourceCode" id="cb1600"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1600-1" data-line-number="1">tokens =<span class="st"> </span>doc.frame  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1600-2" data-line-number="2"><span class="st">        </span><span class="kw">unnest_tokens</span>(term, term, <span class="dt">token =</span> <span class="st">&quot;ngrams&quot;</span>, <span class="dt">n =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1600-3" data-line-number="3"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">count</span>(term, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1600-4" data-line-number="4"><span class="kw">c</span>(tokens)</a></code></pre></div>
<pre><code>## $term
##  [1] &quot;great big&quot;   &quot;a wolf&quot;      &quot;a big&quot;       &quot;an animal&quot;  
##  [5] &quot;animal has&quot;  &quot;animal in&quot;   &quot;big animal&quot;  &quot;big ears&quot;   
##  [9] &quot;big eyes&quot;    &quot;big teeth&quot;   &quot;ears great&quot;  &quot;eyes great&quot; 
## [13] &quot;has great&quot;   &quot;hood met&quot;    &quot;in my&quot;       &quot;is a&quot;       
## [17] &quot;is an&quot;       &quot;little red&quot;  &quot;met a&quot;       &quot;my house&quot;   
## [21] &quot;red riding&quot;  &quot;riding hood&quot; &quot;the animal&quot;  &quot;there is&quot;   
## [25] &quot;wolf is&quot;    
## 
## $n
##  [1] 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1</code></pre>

<p><strong>Stemming</strong> and <strong>Lemmatization</strong>  </p>
<p>In the <strong>English</strong> language, we use <strong>inflection forms</strong> for <strong>parts of speech</strong>. For example, the <strong>adjective</strong> form of <strong>beauty</strong> is <strong>beautiful</strong>. The past participle of <strong>run</strong> is <strong>running</strong>. The plural form of <strong>chocolate</strong> is <strong>chocolates</strong>. Now, it may be necessary to reduce such terms into their <strong>stem</strong> (or <strong>root</strong>) form, e.g. <strong>beauty</strong>, <strong>run</strong>, and <strong>chocolate</strong>. This method is called <strong>Stemming</strong> and <strong>Lemmatization</strong>.</p>
<p><strong>Stemming</strong> cuts the <strong>suffix</strong> and <strong>prefix</strong> of words. For example, <strong>stemming</strong> cuts the suffix <strong>ing</strong> from the term <strong>cooking</strong>. A good choice of stemmer is important to avoid <strong>over stemming</strong> or <strong>under stemming</strong>. For example, a poor <strong>stemmer</strong> may cut <strong>driving</strong> into <strong>driv</strong>.</p>
<p><strong>Lemmatization</strong> takes into account the morphological structure of words. For example, if we simply cut the prefix <strong>ing</strong> for <strong>running</strong>, we may end up with <strong>runn</strong>. <strong>Lemmatization</strong> allows us to transform <strong>running</strong> into <strong>run</strong>, <strong>swimming</strong> into <strong>swim</strong>, <strong>driving</strong> into <strong>drive</strong>. Also, words such as <strong>am</strong>, <strong>is</strong>, <strong>are</strong> get converted into <strong>be</strong>.</p>
<p>To illustrate, let us use the <strong>quanteda</strong> library to perform all the <strong>pre-processing</strong> methods we discussed so far, including stemming and lemmatization. Here, we use <strong>dfm(.)</strong> function.</p>
<p>First, we generate a document corpus using <strong>corpus(.)</strong> function from <strong>quanteda</strong> library:</p>

<div class="sourceCode" id="cb1602"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1602-1" data-line-number="1"><span class="kw">library</span>(quanteda)</a>
<a class="sourceLine" id="cb1602-2" data-line-number="2">doc.frame =<span class="st">  </span><span class="kw">tibble</span>(<span class="dt">docid =</span> <span class="kw">c</span>(<span class="st">&#39;D1&#39;</span>,<span class="st">&#39;D2&#39;</span>,<span class="st">&#39;D3&#39;</span>,<span class="st">&#39;D4&#39;</span>), <span class="dt">term =</span> <span class="kw">c</span>(D1, D2, D3, D4))</a>
<a class="sourceLine" id="cb1602-3" data-line-number="3">doc.corpus =<span class="st"> </span><span class="kw">corpus</span>(doc.frame, <span class="dt">docid_field =</span> <span class="st">&quot;docid&quot;</span>, <span class="dt">text_field =</span> <span class="st">&quot;term&quot;</span>)</a>
<a class="sourceLine" id="cb1602-4" data-line-number="4"><span class="kw">summary</span>(doc.corpus)</a></code></pre></div>
<pre><code>## Corpus consisting of 4 documents, showing 4 documents:
## 
##  Text Types Tokens Sentences
##    D1     9      9         1
##    D2    10     15         1
##    D3     6      6         1
##    D4     8      8         1</code></pre>

<p>Now, we can perform the pre-processing:</p>

<div class="sourceCode" id="cb1604"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1604-1" data-line-number="1">doc.tokens =<span class="st"> </span><span class="kw">tokens</span>(doc.corpus, <span class="dt">remove_punct =</span> <span class="ot">TRUE</span>,  <span class="dt">remove_numbers =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1604-2" data-line-number="2">doc.tokens =<span class="st"> </span><span class="kw">tokens_select</span>(doc.tokens, stopwords, <span class="dt">case_insensitive =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1604-3" data-line-number="3">                           <span class="dt">selection=</span><span class="st">&#39;remove&#39;</span>)</a>
<a class="sourceLine" id="cb1604-4" data-line-number="4">doc.tokens =<span class="st"> </span><span class="kw">tokens_wordstem</span>(doc.tokens)</a>
<a class="sourceLine" id="cb1604-5" data-line-number="5">doc.tokens</a></code></pre></div>
<pre><code>## Tokens consisting of 4 documents.
## D1 :
## [1] &quot;big&quot;  &quot;anim&quot; &quot;my&quot;   &quot;hous&quot;
## 
## D2 :
##  [1] &quot;anim&quot;  &quot;great&quot; &quot;big&quot;   &quot;ear&quot;   &quot;great&quot; &quot;big&quot;   &quot;eye&quot;   &quot;great&quot;
##  [9] &quot;big&quot;   &quot;teeth&quot;
## 
## D3 :
## [1] &quot;wolf&quot; &quot;anim&quot;
## 
## D4 :
## [1] &quot;Littl&quot; &quot;Red&quot;   &quot;Ride&quot;  &quot;Hood&quot;  &quot;met&quot;   &quot;Wolf&quot;</code></pre>

<p>One point to make is that <strong>stemming</strong> results in the term <strong>hous</strong> for <strong>house</strong> and <strong>anim</strong> for <strong>animal</strong>, which may be correct but not as expected.</p>
<p>Another point to make is that <strong>quanteda</strong> has its list of <strong>stopwords</strong>:</p>

<div class="sourceCode" id="cb1606"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1606-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">stopwords</span>(<span class="dt">source =</span> <span class="st">&quot;smart&quot;</span>), <span class="dt">n=</span><span class="dv">25</span>)  <span class="co"># listing only 25 stopwords</span></a></code></pre></div>
<pre><code>##  [1] &quot;a&quot;           &quot;a&#39;s&quot;         &quot;able&quot;        &quot;about&quot;      
##  [5] &quot;above&quot;       &quot;according&quot;   &quot;accordingly&quot; &quot;across&quot;     
##  [9] &quot;actually&quot;    &quot;after&quot;       &quot;afterwards&quot;  &quot;again&quot;      
## [13] &quot;against&quot;     &quot;ain&#39;t&quot;       &quot;all&quot;         &quot;allow&quot;      
## [17] &quot;allows&quot;      &quot;almost&quot;      &quot;alone&quot;       &quot;along&quot;      
## [21] &quot;already&quot;     &quot;also&quot;        &quot;although&quot;    &quot;always&quot;     
## [25] &quot;am&quot;</code></pre>

</div>
<div id="ranking-and-scoring" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.2</span> Ranking and Scoring <a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>One of the simplest pieces of information we can gather from a sequence of pre-processed words is the corresponding ranks based on frequency. For example, we show that the terms <strong>big</strong> and <strong>great</strong> are ranked at the top below.</p>

<div class="sourceCode" id="cb1608"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1608-1" data-line-number="1">words =<span class="st">  </span><span class="kw">unlist</span>( <span class="kw">strsplit</span>(documents, <span class="dt">split=</span><span class="st">&#39; &#39;</span>))</a>
<a class="sourceLine" id="cb1608-2" data-line-number="2">(<span class="dt">ranked.words =</span> <span class="kw">sort</span>(<span class="kw">table</span>(words), <span class="dt">decreasing=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## words
##    big animal  great   wolf   ears   eyes   hood  house little    met 
##      4      3      3      2      1      1      1      1      1      1 
##     my    red riding  teeth 
##      1      1      1      1</code></pre>

<p>We can also just quickly validate the result by using <strong>count(.)</strong>.</p>

<div class="sourceCode" id="cb1610"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1610-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1610-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1610-3" data-line-number="3">stopwords.frame =<span class="st"> </span><span class="kw">tibble</span>( <span class="dt">term =</span> stopwords)</a>
<a class="sourceLine" id="cb1610-4" data-line-number="4">tokens =<span class="st"> </span>tokenized.table  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1610-5" data-line-number="5"><span class="st">        </span>dplyr<span class="op">::</span><span class="kw">count</span>(term, <span class="dt">sort=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1610-6" data-line-number="6"><span class="kw">c</span>(tokens)</a></code></pre></div>
<pre><code>## $term
##  [1] &quot;big&quot;    &quot;animal&quot; &quot;great&quot;  &quot;wolf&quot;   &quot;ears&quot;   &quot;eyes&quot;   &quot;hood&quot;  
##  [8] &quot;house&quot;  &quot;little&quot; &quot;met&quot;    &quot;my&quot;     &quot;red&quot;    &quot;riding&quot; &quot;teeth&quot; 
## 
## $n
##  [1] 4 3 3 2 1 1 1 1 1 1 1 1 1 1</code></pre>

<p><strong>Ranking based on TF-IDF</strong> </p>
<p>Ranking can be achieved using <strong>TF-IDF</strong>, short for <strong>Term Frequency - Inverse Document Frequency</strong>. It is a method to rank <strong>relevant</strong> <strong>words</strong> (or <strong>terms</strong>) found in documents. A simple ranking method is to count for the occurrence of terms - the frequency - which we demonstrated in the previous section. The result only provides the <strong>frequency</strong> as the basis for our ranking. However, it does not necessarily provide accurate relevance of the terms to the four documents from where they come. <strong>TF-IDF</strong> uses a formula to compute for <strong>relevance</strong>:   </p>

<p><span class="math display" id="eq:equate1130003">\[\begin{align}
\text{score}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t,D) \tag{11.4} 
\end{align}\]</span>
</p>
<p>where:</p>

<p><span class="math display" id="eq:equate1130006" id="eq:equate1130005" id="eq:equate1130004">\[\begin{align}
\text{TF}(t, d) &amp;= \frac{\text{count}(t\ \in\ d)}{\text{count}(d)} = \frac{\text{Freq of the term }\mathbf{t}\ \text{ in d}}{\text{No of words in d}}   \tag{11.5} \\
\text{IDF}(t,D) &amp;= \log_e \left(\frac{N}{df_t \text{ = count}(d\ \in\ D:\ t\ \in\ d)}\right) \tag{11.6} \\
&amp;= \log_e \left( \frac{\text{Total No of Documents}}{\text{No of Documents containing the term }\mathbf{t}} \right)  \tag{11.7} 
\end{align}\]</span>
</p>
<p>and where:</p>
<ul>
<li><span class="math inline">\(\mathbf{\text{TF}(t,d)}\)</span> is the frequency of word (t) found in document (d).</li>
<li><span class="math inline">\(\mathbf{\text{IDF}(t,D)}\)</span> is the number of documents containing word (t). Its intent is to give high score (relevance) to rare words.</li>
</ul>
<p>Note that stopwords such as <strong>the</strong>, <strong>a</strong>, and <strong>an</strong> are high in frequency and are not rare. In <strong>TF-IDF</strong>, the inclusion of the <strong>IDF</strong> formula in the equation allows us to give low scores (or low relevance) to stopwords; thus, we choose not to remove the <strong>stopwords</strong> in our corpus as they get pushed to lower relevance.</p>
<p>To illustrate <strong>TF-IDF</strong>, let us calculate the <strong>TF-IDF</strong> weight for the terms <strong>big</strong> in D1 versus in D2 using the <strong>pre-processed</strong> documents.</p>

<p><span class="math display">\[
\begin{array}{ll}
\text{score}(big, D1) &amp;= TF(big, D1) \times IDF(big,D) \\
&amp;= \frac{\text{Freq of the term }\mathbf{big}\ \text{ in D1}}{\text{No of words in D1}} \times 
    \log_e \left( \frac{\text{Total No of Documents}}{\text{No of Docs containing the term }\mathbf{big}} \right) \\
&amp;= \frac{1}{4} \times \log_e \left( \frac{4}{2} \right) \\ 
&amp;= 0.25 \times \log_e(2)\\
&amp;= 0.25 \times 0.6931472 = 0.1732868\\
\\
\text{score}(big, D2) &amp;= \frac{3}{10} \times \log_e \left( \frac{4}{2} \right) \\ 
&amp;= 0.30 \times \log_e(2)\\
&amp;= 0.30 \times 0.6931472 = 0.2079442 
\end{array}
\]</span>
</p>
<p>The calculations show that the term <strong>big</strong> has a 17.3% relevance in D2 and has an 8.66% relevance in D1, indicating that the term is more relevant in D2 than in D1.</p>
<p>An enhanced variance of <strong>TF-IDF</strong> called <strong>Okapi BM25</strong> <span class="citation">(S.E. Robertson et al. <a href="bibliography.html#ref-ref984se">1994</a>)</span> has the following formula below that accounts for other considerations when ranking terms:  </p>

<p><span class="math display" id="eq:equate1130007">\[\begin{align}
score(D,Q)_{(BM25)} = \underbrace{ \sum_{t\ \in\ Q} \frac{TF_{t,d} \times \left(k + 1\right)}{TF_{t,d} + k \times \left(1-b+b \times \frac{dl}{adl}\right)}}_{\text{TF}} \times 
\underbrace{\log_e \left(\frac{N - df_t + 0.5}{df_t + 0.5}\right)}_{\text{IDF}}  \tag{11.8} 
\end{align}\]</span>
</p>
<p>The first consideration is to account for <strong>Term Saturation</strong>, which modifies the <strong>TF</strong> into the following formula with the idea that as the frequency of a given term fully reaches some maximum level, any further increase in frequency may not provide any further benefit. Therefore the <strong>k</strong> parameter controls the effect of <strong>TF</strong>.</p>
<p><span class="math display" id="eq:equate1130008">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1)}{TF(t,d) + k} \tag{11.9} 
\end{align}\]</span></p>
<p>Figure <a href="11.3-natural-language-processing-nlp.html#fig:tfidf1">11.8</a> illustrates the effect of different values of <strong>k</strong>:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tfidf1"></span>
<img src="DS_files/figure-html/tfidf1-1.png" alt="TF (Term Saturation" width="70%" />
<p class="caption">
Figure 11.8: TF (Term Saturation
</p>
</div>

<p>The second consideration accounts for <strong>Document Length</strong>. The <strong>TF</strong> term gets modified further like so:</p>

<p><span class="math display" id="eq:equate1130009">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1 ) }{TF(t,d) + k \times \frac{dl}{adl}} \tag{11.10} 
\end{align}\]</span>
</p>
<p>where <strong>dl</strong> is the document length and <strong>adl</strong> is the average document length. The relevance of a term in a document is much higher if the length is shorter than if it is longer; thus, a higher weight is given to terms in shorter documents. On the other hand, if <strong>document length</strong> is not as important, we can disable or reduce the weight using the <strong>b</strong> parameter, giving a value between [0,1].</p>

<p><span class="math display" id="eq:equate1130010">\[\begin{align}
TF(t, d)\ \text{modified into} \rightarrow \frac{TF(t,d) \times (k + 1)}{TF(t,d) + k \times \left(1 - b + b \times\frac{dl}{adl}\right)} \tag{11.11} 
\end{align}\]</span>
</p>
<p>The last consideration is the adjustment of <strong>IDF</strong>. This adjustment can be regarded as an additional tune-up for <strong>term relevance</strong> from <strong>Lucene</strong> variance.</p>

<p><span class="math display" id="eq:equate1130011">\[\begin{align}
IDF = \log_e\left(\frac{N}{df_t}\right) \propto \log_e \left( \frac{1+ N - df_t + 0.5}{df_t + 0.5} \right)   \tag{11.12} 
\end{align}\]</span>
</p>
<p>Let us apply <strong>Okapi BM25</strong> to our previous example (assume k=2 and b=0.70):</p>

<p><span class="math display">\[
\begin{array}{ll}
\text{score}(big, D1) &amp;= TF(big, D1) \times IDF(big,D) \\
&amp;= \frac{\frac{1}{4} \times (2 + 1)}{ \frac{1}{4}  + 2 \times \left(1 - 0.70 + 0.70 \times \frac{4}{5.5}\right)} \times \log_e \left( \frac{1 + 4 - 2 + 0.5}{2 + 0.5} \right) \\
&amp;= \frac{0.75}{1.868182} \times \log_e(\frac{3.5}{2.5})\\
&amp;= 0.1350801 
\\
\text{score}(big, D2) &amp;= \frac{\frac{3}{10} \times (2 + 1)}{ \frac{3}{10}  + 2 \times \left(1 - 0.70 + 0.70 \times \frac{10}{5.5}\right)} \times \log_e \left( \frac{1 + 4 - 2 + 0.5}{2 + 0.5} \right) \\
&amp;= \frac{0.9}{3.445455} \times \log_e(\frac{3.5}{2.5})\\
&amp;= 0.08789115
\end{array}
\]</span>
</p>
<p>The document lengths for <strong>d1</strong> and <strong>d2</strong> are 4 and 10 respectively. The average length is 5.5 based on the computation below:</p>

<div class="sourceCode" id="cb1612"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1612-1" data-line-number="1">D    =<span class="st"> </span>documents</a>
<a class="sourceLine" id="cb1612-2" data-line-number="2">dl   =<span class="st"> </span><span class="cf">function</span> (d) { <span class="kw">length</span>( <span class="kw">unlist</span>(<span class="kw">strsplit</span>(d, <span class="st">&quot; &quot;</span>)) ) }</a>
<a class="sourceLine" id="cb1612-3" data-line-number="3">(<span class="dt">adl =</span> ( <span class="kw">dl</span>(D[<span class="dv">1</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">2</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">3</span>]) <span class="op">+</span><span class="st"> </span><span class="kw">dl</span>(D[<span class="dv">4</span>]) ) <span class="op">/</span><span class="st"> </span><span class="dv">4</span>)</a></code></pre></div>
<pre><code>## [1] 5.5</code></pre>

<p>Let us now re-evaluate our original ranked words and rank them using additional functions from <strong>tidytext</strong> library. Here, we use <strong>bind_tf_idf(.)</strong> function to calculate <strong>TF-IDF</strong> score.</p>

<div class="sourceCode" id="cb1614"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1614-1" data-line-number="1"><span class="kw">library</span>(tidytext)</a>
<a class="sourceLine" id="cb1614-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb1614-3" data-line-number="3">tokens =<span class="st"> </span>tokenized.table  <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1614-4" data-line-number="4"><span class="st">        </span><span class="kw">group_by</span>(docid, term) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1614-5" data-line-number="5"><span class="st">        </span><span class="kw">tally</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb1614-6" data-line-number="6"><span class="st">        </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))</a>
<a class="sourceLine" id="cb1614-7" data-line-number="7"><span class="kw">head</span>( tokens <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">bind_tf_idf</span>(term, docid, n) ) <span class="co"># display top 5</span></a></code></pre></div>
<pre><code>## # A tibble: 6 x 6
## # Groups:   docid [2]
##   docid term       n    tf   idf tf_idf
##   &lt;chr&gt; &lt;chr&gt;  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 D2    big        3  0.3  0.693 0.208 
## 2 D2    great      3  0.3  1.39  0.416 
## 3 D1    animal     1  0.25 0.288 0.0719
## 4 D1    big        1  0.25 0.693 0.173 
## 5 D1    house      1  0.25 1.39  0.347 
## 6 D1    my         1  0.25 1.39  0.347</code></pre>

<p>We leave readers to evaluate other variants of <strong>BM25</strong> and its other counterparts.</p>
<p><strong>Scoring Based on Cumulative Gain</strong></p>
<p>While <strong>TF-IDF</strong> provides us the ability to rank words, we use two popular <strong>ranking</strong> measures to determine the <strong>quality of ranking</strong>, namely <strong>mean Average Precision (mAP)</strong> and <strong>Normalized Discounted Cumulative Gain (nDCG)</strong>. In this section, let us cover <strong>nDCG</strong> and defer the discussion of <strong>mAP</strong> in the <strong>Recommender System (Image Classification)</strong> section.   </p>
<p>We begin with the idea that a <strong>search engine</strong> retrieves a list of documents based on a given user query. In <strong>search engines</strong>, we rank retrieved articles based on relevance to our <strong>query</strong>. The <strong>relevance</strong> of each retrieved documents can be categorically labeled as: <strong>1 - not relevant</strong>, <strong>2 - fairly relevant</strong>, <strong>3 - quite relevant</strong>. The value given to a document is called the <strong>Gain</strong>. However, we are after the <strong>cumulative gain</strong> to score the effectiveness of the retrieval, for which we use the following <strong>nDCG</strong> Scoring:</p>
<p><span class="math display" id="eq:equate1130012">\[\begin{align}
nDCG = \frac{DCG_{(n)}}{IDCG_{(n)}}\ \ \ \ \ \ \text{where n is number of retrieved documents} \tag{11.13} 
\end{align}\]</span></p>
<p>The score depends on the <strong>Cumulative Gain</strong> and <strong>Discounted Cumulative Gain</strong> and <strong>Ideal Discounted Cumulative Gain (IDC)</strong> expressed respectively below:    </p>

<p><span class="math display" id="eq:equate1130014" id="eq:equate1130013">\[\begin{align}
\mathbf{CG} = \sum_{i=1}^n G_i\ \ \ \ \  \mathbf{DCG} = \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)} \tag{11.14} \\
\mathbf{IDCG} =  \sum \left[\text{sort}\_\text{terms} \left\{\frac{G_i}{\log_e(i + 1)}\right\}_{i=1}^n\right] \tag{11.15} 
\end{align}\]</span>
</p>
<p>To illustrate, suppose we have the following documents and their relevance (the <strong>Gain</strong>):</p>
<p><span class="math display">\[
D_{1} = 3\ \ \ \ \ \ 
D_{2} = 1\ \ \ \ \ \ 
D_{3} = 2\ \ \ \ \ \ 
D_{4} = 0\ \ \ \ \ \ 
D_{5} = 2\ \ \ \ \ \ 
\]</span></p>
<p>The <strong>Cumulative Gain (CG)</strong> result is:</p>

<p><span class="math display" id="eq:equate1130015">\[\begin{align}
\mathbf{CG} = \sum_{i=1}^n G_i = 3 + 1 + 2 + 0 + 3 = 9 \tag{11.16} 
\end{align}\]</span>
</p>
<p>The <strong>Discounted Cumulative Gain (DCG)</strong> is:</p>

<p><span class="math display" id="eq:equate1130016">\[\begin{align}
\mathbf{DCG}_{(5)} &amp;= \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)}  \tag{11.17} \\ 
&amp;=\frac{3}{\log_e(2)} + \frac{1}{\log_e(3)} +  \frac{2}{\log_e(4)}  
\frac{0}{\log_e(5)} +  \frac{3}{\log_e(6)} = 8.355351 \nonumber
\end{align}\]</span>
</p>
<p>The <strong>Ideal (sorted) Discounted Cumulative Gain (IDCG)</strong> is:</p>

<p><span class="math display" id="eq:equate1130017">\[\begin{align}
\mathbf{IDCG}_{(5)} &amp;= \sum_{i=1}^n \frac{G_i}{\log_e(i + 1)} \tag{11.18} \\ 
&amp;= \frac{3}{\log_e(2)} + \frac{3}{\log_e(3)} +  \frac{2}{\log_e(4)} + 
\frac{1}{\log_e(5)} +  \frac{0}{\log_e(6)} = 9.059608 \nonumber
\end{align}\]</span>
</p>
<p><strong>Finally</strong>, the <strong>nDCG</strong> score is:</p>

<p><span class="math display" id="eq:equate1130018">\[\begin{align}
\mathbf{nDCG}_{(5)} = \frac{DCG_{(5)}}{IDCG_{(5)}} = \frac{8.355351}{9.059608} = 0.9222641 \tag{11.19} 
\end{align}\]</span>
</p>
<p>Based on the score, we can say that the <strong>search engine</strong> demonstrates 92.23% effectiveness in retrieving the relevant documents.</p>
</div>
<div id="document-similarity" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.3</span> Document Similarity <a href="11.3-natural-language-processing-nlp.html#document-similarity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <strong>Information Retrieval</strong>, a classic representation of texts is in the form of a <strong>Vector Space Model (VSM)</strong>. The model is achieved by casting each term in the texts into its corresponding numerical equivalence that becomes helpful in measuring similarity at some point. Our goal is to use <strong>VSM</strong> to determine term similarities or word relationships.  </p>
<p>We start with a generic similarity function like so:</p>
<p><span class="math display" id="eq:equate1130019">\[\begin{align}
\mathbf{\text{similarity}}\left(d^{(1)}, d^{(2)}\right)\ \ \ \ \ \ \ \ \text{where }\mathbf{d^{(1)}}\ \text{is 1st document and }\mathbf{d^{(2)}}\ \text{is 2nd document.} \tag{11.20} 
\end{align}\]</span></p>
<p>Note that we can also compare a given query relevant to a retrieved document like so:</p>
<p><span class="math display" id="eq:equate1130020">\[\begin{align}
\mathbf{\text{similarity}}(q, d)\ \ \ \ \ \ \ \ \text{where }\mathbf{q}\ \text{is query and }\mathbf{d}\ \text{is document.} \tag{11.21} 
\end{align}\]</span></p>
<p>The goal is to determine if the given query is similar (or <strong>relevant</strong>) to a <strong>retrieved</strong> document. Here, a good measure of similarity is based on <strong>cosine similarity</strong>; albeit, we also can use <strong>euclidean distance</strong> and so on for comparison.</p>
<p><span class="math display" id="eq:equate1130021">\[\begin{align}
\mathbf{\text{similarity}}(q, d) = \mathbf{\text{cosine}}(\vec{q}, \vec{d}) = \frac{\vec{q} \cdot \vec{d}}{\|\vec{q}\|\|\vec{d}\|}  =
\frac{\sum_{i=1}^{|v|} q_i d_i}{\sqrt{\sum_{i=1}^{|v|}q_i^2} \sqrt{\sum_{i=1}^{|v|} d_i^2}} \tag{11.22} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(|v|\)</span> represents the number of unique features (unique terms).</li>
<li><span class="math inline">\(\mathbf{q_i}\)</span> is a calculated numerical value, e.g. <strong>tf-idf</strong> score for ith term in the query (<strong>q</strong>).</li>
<li><span class="math inline">\(\mathbf{d_i}\)</span> is a calculated numerical value, e.g. <strong>tf-idf</strong> score for ith term in the document (<strong>d</strong>).</li>
</ul>
<p>To illustrate, let us cast our original document corpus into <strong>Document Feature Matrix (DFM)</strong>, also called <strong>Document Term Matrix (DTM)</strong> using <strong>dfm(.)</strong> function from <strong>quanteda</strong> library. <strong>Pre-processing</strong> can also be applied through the function (below, we disable stemming):    </p>

<div class="sourceCode" id="cb1616"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1616-1" data-line-number="1">doc.dfm =<span class="st">  </span><span class="kw">dfm</span>(doc.corpus, <span class="dt">tolower =</span> <span class="ot">TRUE</span>,  <span class="dt">remove_punct =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb1616-2" data-line-number="2">                    <span class="dt">remove_numbers =</span> <span class="ot">TRUE</span>,  <span class="dt">stem         =</span> <span class="ot">FALSE</span>,</a>
<a class="sourceLine" id="cb1616-3" data-line-number="3">                    <span class="dt">remove         =</span> stopwords</a>
<a class="sourceLine" id="cb1616-4" data-line-number="4">                )</a>
<a class="sourceLine" id="cb1616-5" data-line-number="5">(<span class="dt">dtm.doc =</span> <span class="kw">data.frame</span>(<span class="kw">as.matrix</span>(doc.dfm)))[,<span class="dv">1</span><span class="op">:</span><span class="dv">8</span>] <span class="co"># display in data frame </span></a></code></pre></div>
<pre><code>##    big animal my house great ears eyes teeth
## D1   1      1  1     1     0    0    0     0
## D2   3      1  0     0     3    1    1     1
## D3   0      1  0     0     0    0    0     0
## D4   0      0  0     0     0    0    0     0</code></pre>

<p>We can convert the data frame into a <strong>Term Document Matrix (TDM)</strong> using transpose (we can come back to this form in a later section for <strong>LSA</strong>):</p>

<div class="sourceCode" id="cb1618"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1618-1" data-line-number="1">(<span class="dt">tdm.doc =</span> <span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(dtm.doc))))</a></code></pre></div>
<pre><code>##        D1 D2 D3 D4
## big     1  3  0  0
## animal  1  1  1  0
## my      1  0  0  0
## house   1  0  0  0
## great   0  3  0  0
## ears    0  1  0  0
## eyes    0  1  0  0
## teeth   0  1  0  0
## wolf    0  0  1  1
## little  0  0  0  1
## red     0  0  0  1
## riding  0  0  0  1
## hood    0  0  0  1
## met     0  0  0  1</code></pre>

<p>We can use the <strong>topfeatures(.)</strong> function with <strong>DFM</strong> to rank terms. It shows the same result as before.</p>

<div class="sourceCode" id="cb1620"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1620-1" data-line-number="1"><span class="kw">topfeatures</span>(doc.dfm) </a></code></pre></div>
<pre><code>##    big animal  great   wolf     my  house   ears   eyes  teeth little 
##      4      3      3      2      1      1      1      1      1      1</code></pre>

<p>For <strong>cosine similarity</strong>, we can use the <strong>dfm_weight(.)</strong> function to obtain the same result as <strong>dfm(.)</strong></p>

<div class="sourceCode" id="cb1622"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1622-1" data-line-number="1"><span class="kw">as.matrix</span>( <span class="kw">dfm_weight</span>(doc.dfm) )</a></code></pre></div>
<pre><code>##     features
## docs big animal my house great ears eyes teeth wolf little red riding
##   D1   1      1  1     1     0    0    0     0    0      0   0      0
##   D2   3      1  0     0     3    1    1     1    0      0   0      0
##   D3   0      1  0     0     0    0    0     0    1      0   0      0
##   D4   0      0  0     0     0    0    0     0    1      1   1      1
##     features
## docs hood met
##   D1    0   0
##   D2    0   0
##   D3    0   0
##   D4    1   1</code></pre>

<p>Using the <strong>DTM</strong> above, the corresponding <strong>Cosine Similarity</strong> between the document D1 and D2 is: </p>
<p><span class="math display" id="eq:equate1130025" id="eq:equate1130024" id="eq:equate1130023" id="eq:equate1130022">\[\begin{align}
\vec{d^{(1)}} \cdot \vec{d^{(2)}}  &amp;= \sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)} \tag{11.23} \\
&amp;=(1\times 3) + (1 \times 1) + (1\times 0) + (1\times 0) + (0 \times 3) + \nonumber \\
&amp;\ ( 0 \times 1 ) +  ( 0 \times 1 ) + ( 0 \times 1 ) \nonumber \\
&amp;= 4 \nonumber \\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \sqrt{1^2 + 1^2 + 1^2 + 1^2}  = 2 \tag{11.24} \\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \sqrt{3^2 + 1^2 +  3^2 + 1^2 + 1^2 + 1^2}  = 4.690416 \tag{11.25} \\
\nonumber \\
\mathbf{\text{cosine}}(\vec{q}, \vec{d}) &amp;= 4\ /\ (\ 2 \times 4.690416\ ) = 0.4264014 \tag{11.26} 
\end{align}\]</span></p>
<p>A value of one means that the two documents are similar.</p>
<p>Alternatively, we can also generate a <strong>Document Term Matrix</strong> with <strong>TF-IDF</strong> score:</p>

<div class="sourceCode" id="cb1624"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1624-1" data-line-number="1">tfidf =<span class="st"> </span><span class="kw">dfm_tfidf</span>(doc.dfm, <span class="dt">scheme_tf =</span> <span class="st">&quot;prop&quot;</span>, <span class="dt">base=</span><span class="kw">exp</span>(<span class="dv">1</span>), <span class="dt">force=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1624-2" data-line-number="2"><span class="kw">round</span>( <span class="kw">as.matrix</span>( tfidf ), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##     features
## docs  big animal   my house great ears eyes teeth wolf little  red
##   D1 0.17   0.07 0.35  0.35  0.00 0.00 0.00  0.00 0.00   0.00 0.00
##   D2 0.21   0.03 0.00  0.00  0.42 0.14 0.14  0.14 0.00   0.00 0.00
##   D3 0.00   0.14 0.00  0.00  0.00 0.00 0.00  0.00 0.35   0.00 0.00
##   D4 0.00   0.00 0.00  0.00  0.00 0.00 0.00  0.00 0.12   0.23 0.23
##     features
## docs riding hood  met
##   D1   0.00 0.00 0.00
##   D2   0.00 0.00 0.00
##   D3   0.00 0.00 0.00
##   D4   0.23 0.23 0.23</code></pre>

<p>Using the <strong>DTM</strong> above, the corresponding <strong>cosine similarity</strong> between the document D1 and D2 is:</p>
<p><span class="math display" id="eq:equate1130029" id="eq:equate1130028" id="eq:equate1130027" id="eq:equate1130026">\[\begin{align}
\vec{d^{(1)}} \cdot \vec{d^{(2)}}  &amp;= \sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)} \tag{11.27} \\
&amp;=(0.17\times 0.21) + (0.07 \times 0.03) + (0.35\times 0) + (0.35\times 0)  \nonumber \\
&amp;\ \ \  + (0 \times 0.42)  + ( 0 \times 0.14 ) +  ( 0 \times 0.14 ) + ( 0 \times 0.14 ) \nonumber \\
&amp;= 0.0378 \nonumber\\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \sqrt{0.17^2 + 0.07^2 + 0.35^2 + 0.35^2 }  = 0.5280152 \tag{11.28} \\
\nonumber \\
\sqrt{\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \sqrt{0.03^2 + 0.42^2 + 0.14^2 + 0.14^2 + 0.14^2}  = 0.4859012 \tag{11.29} \\
\nonumber \\ 
\mathbf{\text{cosine}}(\vec{q}, \vec{d}) &amp;= 0.0378\ /\ (\ 0.5280152\times 0.4859012\ ) = 0.1473321 \tag{11.30} 
\end{align}\]</span></p>
<p>The <strong>cosine similarity</strong> of the two documents is adjusted using <strong>TF-IDF</strong>. Notice that with <strong>TF-IDF</strong>, the <strong>similarity</strong> measurements drop even lower, indicating dissimilarity between the two documents, though human intuition seems there seems to be some relation between the two documents. We can revisit this in <strong>Latent Semantic Analysis (LSA)</strong>.</p>
</div>
<div id="linguistic-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.4</span> Linguistic Analysis <a href="11.3-natural-language-processing-nlp.html#linguistic-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>When it comes to <strong>Speech Recognition</strong>, we emphasize <strong>Linguistics</strong>, defined by <strong>Oxford language</strong> and <strong>Google Dictionary</strong> to be the scientific study of language and structure, including the study of <strong>morphology</strong> (language form), <strong>Syntax</strong>, <strong>Phonetics</strong>, and <strong>Semantics</strong> which includes both <strong>Lexical</strong> structure (language vocabulary for meaning and relationship) and <strong>Conceptual</strong> structure (language vocabulary for meaning and cognition). <strong>Wikipedia</strong> also includes <strong>Contextual</strong> structure, including social, historical, cultural, and other properties that influence the language.</p>
<p>We may not be able to cover them all in this book as they deserve their own merits. Instead, the next few sections will touch only on <strong>Lexical</strong> and <strong>Semantic</strong> analysis basics. This, in particular, covers <strong>Parts of Speech (POS)</strong> and <strong>Phrases</strong> (also called <strong>Chunks</strong>).</p>
<p>Also, we cover additional insight about <strong>Speech Recognition</strong> in Chapter <strong>13</strong> (<strong>Computational Deep Learning II</strong>).</p>
</div>
<div id="lexical-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.5</span> Lexical Analysis <a href="11.3-natural-language-processing-nlp.html#lexical-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For <strong>NLP</strong>, getting the closest insight about the content (or possibly common key themes) of a set of documents is essential. A straightforward way to determine the closest topic is to count the frequency of all terms found in the documents to rank them accordingly.</p>
<p>This section extends our discussion on <strong>pre-processing</strong> of words and <strong>TF-IDF</strong> starting with <strong>POS tagging</strong> - the processes of labeling or annotating words with <strong>Parts of Speech</strong>.</p>
<p><strong>POS Tagging</strong> </p>
<p>There should be more than 15 to 20 different tags, each corresponding to a particular <strong>Part of Speech</strong>. The following <strong>(POS) tags</strong> are common (albeit we only show a few tags):</p>
<ul>
<li><strong>NN</strong> - Noun</li>
<li><strong>NNS</strong> - Noun Plural</li>
<li><strong>VBD</strong> - Verb</li>
<li><strong>JJ</strong> - Adjective</li>
<li><strong>RB</strong> - Adverb</li>
<li><strong>PRP</strong> - Possessive Pronoun</li>
</ul>
<p>To illustrate, we use <strong>treetag(.)</strong> function from the <strong>koRpus</strong> library for POS tagging:</p>

<div class="sourceCode" id="cb1626"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1626-1" data-line-number="1"><span class="kw">library</span>(koRpus)</a>
<a class="sourceLine" id="cb1626-2" data-line-number="2"><span class="kw">library</span>(koRpus.lang.en)</a>
<a class="sourceLine" id="cb1626-3" data-line-number="3"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb1626-4" data-line-number="4"><span class="kw">library</span>(SnowballC)</a>
<a class="sourceLine" id="cb1626-5" data-line-number="5"><span class="kw">set.kRp.env</span>(<span class="dt">TT.cmd=</span><span class="st">&quot;~/nlp/cmd/tree-tagger-english&quot;</span> , <span class="dt">lang=</span><span class="st">&quot;en&quot;</span>, </a>
<a class="sourceLine" id="cb1626-6" data-line-number="6">             <span class="dt">preset=</span><span class="st">&quot;en&quot;</span>, <span class="dt">treetagger=</span><span class="st">&quot;kRp.env&quot;</span>, <span class="dt">format=</span><span class="st">&quot;file&quot;</span>, <span class="dt">TT.tknz=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1626-7" data-line-number="7">             <span class="dt">add.desc =</span> <span class="ot">FALSE</span>, <span class="dt">encoding=</span><span class="st">&quot;UTF-8&quot;</span>)</a>
<a class="sourceLine" id="cb1626-8" data-line-number="8">tagged.text =<span class="st"> </span><span class="kw">treetag</span>( <span class="st">&quot;~/nlp/documents.txt&quot;</span> , </a>
<a class="sourceLine" id="cb1626-9" data-line-number="9">                       <span class="dt">stopwords =</span> tm<span class="op">::</span><span class="kw">stopwords</span>(<span class="st">&quot;en&quot;</span>),</a>
<a class="sourceLine" id="cb1626-10" data-line-number="10">                       <span class="dt">stemmer   =</span> SnowballC<span class="op">::</span>wordStem) </a>
<a class="sourceLine" id="cb1626-11" data-line-number="11">tagged.text[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>, <span class="dv">2</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># Display the 1st 10 lines</span></a></code></pre></div>
<pre><code>##     token  tag  lemma lttr      wclass desc  stop  stem idx
## 1   There  EX0  there    5 existential   NA  TRUE There   1
## 2      is  VBZ     be    2        verb   NA  TRUE     i   2
## 3       a  AT0      a    1     article   NA  TRUE     a   3
## 4     big  AJ0    big    3   adjective   NA FALSE   big   4
## 5  animal  NN1 animal    6        noun   NA FALSE  anim   5
## 6      in  PRP     in    2 preposition   NA  TRUE    in   6
## 7      my  DPS      i    2  determiner   NA  TRUE    my   7
## 8   house  NN1  house    5        noun   NA FALSE  hous   8
## 9       . SENT      .    1    fullstop   NA FALSE     .   9
## 10    The  AT0    the    3     article   NA  TRUE   The  10</code></pre>

<p>We can then show the mapping between terms (tokens) and tags like so:</p>

<div class="sourceCode" id="cb1628"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1628-1" data-line-number="1">pos_tag &lt;-<span class="st"> </span><span class="cf">function</span>(tokens, tags) { <span class="kw">paste0</span>(tokens, <span class="st">&quot;/&quot;</span>, tags) }</a>
<a class="sourceLine" id="cb1628-2" data-line-number="2">tokens =<span class="st"> </span>tagged.text<span class="op">@</span>tokens<span class="op">$</span>token</a>
<a class="sourceLine" id="cb1628-3" data-line-number="3">tags   =<span class="st"> </span>tagged.text<span class="op">@</span>tokens<span class="op">$</span>tag</a>
<a class="sourceLine" id="cb1628-4" data-line-number="4"><span class="kw">pos_tag</span>(tokens, tags)</a></code></pre></div>
<pre><code>##  [1] &quot;There/EX0&quot;  &quot;is/VBZ&quot;     &quot;a/AT0&quot;      &quot;big/AJ0&quot;    &quot;animal/NN1&quot;
##  [6] &quot;in/PRP&quot;     &quot;my/DPS&quot;     &quot;house/NN1&quot;  &quot;./SENT&quot;     &quot;The/AT0&quot;   
## [11] &quot;animal/NN1&quot; &quot;has/VBZ&quot;    &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;ears/NN2&quot;  
## [16] &quot;,/PUN&quot;      &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;eyes/NN2&quot;   &quot;,/PUN&quot;     
## [21] &quot;great/AJ0&quot;  &quot;big/AJ0&quot;    &quot;teeth/NN2&quot;  &quot;./SENT&quot;     &quot;A/AT0&quot;     
## [26] &quot;wolf/NN1&quot;   &quot;is/VBZ&quot;     &quot;an/AT0&quot;     &quot;animal/NN1&quot; &quot;./SENT&quot;    
## [31] &quot;Little/DT0&quot; &quot;Red/AJ0&quot;    &quot;Riding/NP0&quot; &quot;Hood/NP0&quot;   &quot;met/VBD&quot;   
## [36] &quot;a/AT0&quot;      &quot;Wolf/NP0&quot;   &quot;./SENT&quot;</code></pre>

<p>There are a few notes to make about the output above. First, the document.txt file contains four sentences, as in the case we have previously. Each term in the output above maps to its corresponding sentence.</p>
<p>Secondly, note that we used 3rd-party stopwords and stemmers from <strong>tm</strong> library and <strong>SnowballC</strong> library, respectively. We also can use our <strong>stopwords</strong> as an option.</p>
<p>Lastly, note that <strong>treetagger</strong> is a 3rd-party software externally used by <strong>koRpus</strong>.</p>
<p><strong>Annotation</strong></p>
<p>Different structures of a text can be annotated. We can annotate <strong>Documents</strong>, <strong>Paragraphs</strong>, <strong>Sentences</strong>, <strong>Phrases</strong>, <strong>Clauses</strong>, <strong>Words</strong>. Only a few will be explained. Moreover, we use a common library called <strong>NLP</strong> and <strong>openNLP</strong>.</p>
<p>To illustrate, let us use <strong>Annotator</strong> functions from <strong>openNLP</strong> and <strong>NLP</strong> libraries for annotation. At the same time, we show an alternative way to perform <strong>POS tagging</strong>, and in addition, to handle <strong>Chunking</strong>. Below, we annotate texts based on sentence structure, word structure, and POS forms.</p>

<div class="sourceCode" id="cb1630"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1630-1" data-line-number="1"><span class="kw">library</span>(NLP)</a>
<a class="sourceLine" id="cb1630-2" data-line-number="2"><span class="kw">library</span>(openNLP)</a>
<a class="sourceLine" id="cb1630-3" data-line-number="3"><span class="kw">library</span>(openNLPmodels.en)</a>
<a class="sourceLine" id="cb1630-4" data-line-number="4">text =<span class="st">  </span><span class="kw">as.String</span>( doc.corpus ) <span class="co"># as.String(doc.corpus)</span></a>
<a class="sourceLine" id="cb1630-5" data-line-number="5">sentence.annotator =<span class="st"> </span><span class="kw">Maxent_Sent_Token_Annotator</span>() </a>
<a class="sourceLine" id="cb1630-6" data-line-number="6">word.annotator     =<span class="st"> </span><span class="kw">Maxent_Word_Token_Annotator</span>() </a>
<a class="sourceLine" id="cb1630-7" data-line-number="7">pos.annotator      =<span class="st"> </span><span class="kw">Maxent_POS_Tag_Annotator</span>(<span class="dt">probs=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1630-8" data-line-number="8">annotated.doc  =<span class="st"> </span><span class="kw">annotate</span>(<span class="dt">s =</span> text, <span class="kw">list</span>(sentence.annotator, word.annotator))</a>
<a class="sourceLine" id="cb1630-9" data-line-number="9">pos_tagged.doc =<span class="st"> </span><span class="kw">annotate</span>(<span class="dt">s =</span> text, pos.annotator, annotated.doc)</a></code></pre></div>

<p>Now, let us show a summary of the annotations for the part of speech - the <strong>POS tags</strong>:</p>

<div class="sourceCode" id="cb1631"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1631-1" data-line-number="1"><span class="kw">head</span>(pos_tagged.doc, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># display only first 10 lines</span></a></code></pre></div>
<pre><code>##  id type     start end features
##   1 sentence     1  34 constituents=&lt;&lt;integer,9&gt;&gt;
##   2 sentence    36  98 constituents=&lt;&lt;integer,15&gt;&gt;
##   3 sentence   100 119 constituents=&lt;&lt;integer,6&gt;&gt;
##   4 sentence   121 154 constituents=&lt;&lt;integer,8&gt;&gt;
##   5 word         1   5 POS=EX, POS_prob=0.9517
##   6 word         7   8 POS=VBZ, POS_prob=0.9993
##   7 word        10  10 POS=DT, POS_prob=0.9909
##   8 word        12  14 POS=JJ, POS_prob=0.9979
##   9 word        16  21 POS=NN, POS_prob=0.9799
##  10 word        23  24 POS=IN, POS_prob=0.9801</code></pre>

<p>To map tokens (terms) to tags, we code the following:</p>

<div class="sourceCode" id="cb1633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1633-1" data-line-number="1">text.string =<span class="st"> </span><span class="kw">as.String</span>(doc.corpus)</a>
<a class="sourceLine" id="cb1633-2" data-line-number="2">words       =<span class="st"> </span><span class="kw">subset</span>(pos_tagged.doc, type<span class="op">==</span><span class="st">&quot;word&quot;</span>)</a>
<a class="sourceLine" id="cb1633-3" data-line-number="3">tags        =<span class="st"> </span><span class="kw">sapply</span>(words<span class="op">$</span>features, <span class="st">&#39;[[&#39;</span>,<span class="st">&quot;POS&quot;</span> )</a>
<a class="sourceLine" id="cb1633-4" data-line-number="4">tokens      =<span class="st"> </span>text.string[<span class="kw">subset</span>(pos_tagged.doc, type<span class="op">==</span><span class="st">&quot;word&quot;</span>)]</a>
<a class="sourceLine" id="cb1633-5" data-line-number="5"><span class="kw">pos_tag</span>(tokens, tags)</a></code></pre></div>
<pre><code>##  [1] &quot;There/EX&quot;   &quot;is/VBZ&quot;     &quot;a/DT&quot;       &quot;big/JJ&quot;     &quot;animal/NN&quot; 
##  [6] &quot;in/IN&quot;      &quot;my/PRP$&quot;    &quot;house/NN&quot;   &quot;./.&quot;        &quot;The/DT&quot;    
## [11] &quot;animal/NN&quot;  &quot;has/VBZ&quot;    &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;ears/NNS&quot;  
## [16] &quot;,/,&quot;        &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;eyes/NNS&quot;   &quot;,/,&quot;       
## [21] &quot;great/JJ&quot;   &quot;big/JJ&quot;     &quot;teeth/NNS&quot;  &quot;./.&quot;        &quot;A/DT&quot;      
## [26] &quot;wolf/NN&quot;    &quot;is/VBZ&quot;     &quot;an/DT&quot;      &quot;animal/NN&quot;  &quot;./.&quot;       
## [31] &quot;Little/NNP&quot; &quot;Red/NNP&quot;    &quot;Riding/VBG&quot; &quot;Hood/NNP&quot;   &quot;met/VBD&quot;   
## [36] &quot;a/DT&quot;       &quot;Wolf/NNP&quot;   &quot;./.&quot;</code></pre>

<p>The output shows that the token <strong>big</strong> maps to the <strong>JJ</strong> tag, and the token <strong>animal</strong> maps to <strong>NN</strong>.</p>
<p>Note that the tag used by <strong>openNLP</strong>, such as <strong>JJ</strong> for adjectives, is different
from the tag used by <strong>treetag</strong>, e.g., <strong>AJ0</strong>. There are a few differences to observe likewise.</p>
<p><strong>Chunking</strong> and <strong>Syntactic Parsing</strong>  </p>
<p><strong>Chunking</strong> is extracting and parsing texts to group terms into phrases and is often guided by the corresponding language syntax and structure. For example, a <strong>clause</strong> may represent a combination of adjective and noun tagged as âJJ+NNâ in that order. A <strong>phrase</strong>, on the other hand, is made up of <strong>clauses</strong>.</p>
<p>Let us show a summary of the <strong>Chunks</strong>:</p>

<div class="sourceCode" id="cb1635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1635-1" data-line-number="1">chunk.annotator    =<span class="st"> </span><span class="kw">Maxent_Chunk_Annotator</span>(<span class="dt">probs =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb1635-2" data-line-number="2">chunked.doc    =<span class="st"> </span><span class="kw">annotate</span>(text, chunk.annotator, pos_tagged.doc)</a>
<a class="sourceLine" id="cb1635-3" data-line-number="3"><span class="kw">head</span>(chunked.doc, <span class="dt">n=</span><span class="dv">10</span>) <span class="co"># display only first 10 lines</span></a></code></pre></div>
<pre><code>##  id type     start end features
##   1 sentence     1  34 constituents=&lt;&lt;integer,9&gt;&gt;
##   2 sentence    36  98 constituents=&lt;&lt;integer,15&gt;&gt;
##   3 sentence   100 119 constituents=&lt;&lt;integer,6&gt;&gt;
##   4 sentence   121 154 constituents=&lt;&lt;integer,8&gt;&gt;
##   5 word         1   5 POS=EX, POS_prob=0.9517, chunk_tag=B-NP,
##                        chunk_prob=0.9736
##   6 word         7   8 POS=VBZ, POS_prob=0.9993, chunk_tag=B-VP,
##                        chunk_prob=0.9909
##   7 word        10  10 POS=DT, POS_prob=0.9909, chunk_tag=B-NP,
##                        chunk_prob=0.9962
##   8 word        12  14 POS=JJ, POS_prob=0.9979, chunk_tag=I-NP,
##                        chunk_prob=0.9939
##   9 word        16  21 POS=NN, POS_prob=0.9799, chunk_tag=I-NP,
##                        chunk_prob=0.9898
##  10 word        23  24 POS=IN, POS_prob=0.9801, chunk_tag=B-PP,
##                        chunk_prob=0.992</code></pre>

<p>Note that <strong>Maxent_Chunk_Annotator(.)</strong> requires a model file for language <strong>en</strong>. This function can be installed from <strong>datacube site</strong> like so:</p>
<div class="sourceCode" id="cb1637"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="machinelearning3.html#cb72-1" aria-hidden="true" tabindex="-1"></a>datacube <span class="ot">=</span> <span class="st">&quot;http://datacube.wu.ac.at/&quot;</span></span>
<span id="cb72-2"><a href="machinelearning3.html#cb72-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;openNLPmodels.en&quot;</span>, <span class="at">repos =</span> datacube,  <span class="at">type =</span> <span class="st">&quot;source&quot;</span>) </span></code></div>
<p>We can begin parsing the text using the annotated chunks and create a parsed tree. However, it is worth mentioning that parsing is an expensive operation and may require CPU and memory resources. For that reason, specifically for the parser used in <strong>openNLP</strong>, we adjust the memory requirement - being so, the parser uses java virtual machine (JVM). From there, we can invoke the annotator.</p>

<div class="sourceCode" id="cb1638"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1638-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">java.parameters =</span> <span class="st">&quot;-Xmx4096m&quot;</span>)</a>
<a class="sourceLine" id="cb1638-2" data-line-number="2">parse.annotator =<span class="st"> </span><span class="kw">Parse_Annotator</span>()</a></code></pre></div>

<p>Then we can annotate. Here, we used the annotated chunk texts.</p>

<div class="sourceCode" id="cb1639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1639-1" data-line-number="1">parsed.doc   =<span class="st"> </span><span class="kw">parse.annotator</span>(text, chunked.doc)</a>
<a class="sourceLine" id="cb1639-2" data-line-number="2">parsed.texts =<span class="st"> </span><span class="kw">sapply</span>(parsed.doc<span class="op">$</span>features, <span class="st">`</span><span class="dt">[[</span><span class="st">`</span>, <span class="st">&quot;parse&quot;</span>)</a>
<a class="sourceLine" id="cb1639-3" data-line-number="3">parsed.tree  =<span class="st"> </span><span class="kw">lapply</span>(parsed.texts , Tree_parse)</a></code></pre></div>

<p>The parsed tree of the 1st document has the text format below:</p>

<div class="sourceCode" id="cb1640"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1640-1" data-line-number="1">parsed.tree[[<span class="dv">1</span>]]</a></code></pre></div>
<pre><code>## (TOP
##   (S
##     (NP (EX There))
##     (VP
##       (VBZ is)
##       (NP
##         (NP (DT a) (JJ big) (NN animal))
##         (PP (IN in) (NP (PRP$ my) (NN house)))))
##     (. .)))</code></pre>

<p>The equivalent graph form is shown in Figure <a href="11.3-natural-language-processing-nlp.html#fig:parsedtree1">11.9</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parsedtree1"></span>
<img src="parsedtree1.png" alt="Parsed Tree 1" width="60%" />
<p class="caption">
Figure 11.9: Parsed Tree 1
</p>
</div>
<p>Similarly, the parsed tree of the 4th document has the text format below:</p>

<div class="sourceCode" id="cb1642"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1642-1" data-line-number="1">parsed.tree[[<span class="dv">4</span>]]</a></code></pre></div>
<pre><code>## (TOP
##   (S
##     (NP (NNP Little) (NNP Red) (VBG Riding) (NNP Hood))
##     (VP (VBD met) (NP (DT a) (NNP Wolf)))
##     (. .)))</code></pre>

<p>The equivalent graph form is shown in Figure <a href="11.3-natural-language-processing-nlp.html#fig:parsedtree4">11.10</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:parsedtree4"></span>
<img src="parsedtree4.png" alt="Parsed Tree 4" width="60%" />
<p class="caption">
Figure 11.10: Parsed Tree 4
</p>
</div>
</div>
<div id="semantic-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.6</span> Semantic Analysis <a href="11.3-natural-language-processing-nlp.html#semantic-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In a sense, <strong>Semantics Analysis</strong> reminds us of <strong>Clustering</strong>, which depends on the similarities of members in a cluster and the grouping of members in their respective clusters. In <strong>Semantic Analysis</strong>, we are also interested in determining the relationship of members within and between clusters. In the context of <strong>NLP</strong>, clusters may represent groups of words (possibly the syntactic structures - e.g., parsed tree - as we have shown in the previous section) and their relationship to the documents. Let us start with one of the first popular classic methods called <strong>Latent Semantic Analysis (LSA)</strong>.</p>
<p>Note that we do not cover <strong>Semantic Relationships</strong> in this book. We leave readers to investigate the types of <strong>Semantic Relationships</strong>, such as <strong>Synonymy</strong>, <strong>Antonymy</strong>, <strong>Homonymy</strong>, and <strong>Metonymy</strong>, which may be covered in the study of <strong>Linguistics</strong>. Such <strong>relationships</strong> may require knowledge-based inputs as references for training our models. We may present here basic relationship measures in comparing similarities based on weighted words (or chunks) in documents.</p>
<p><strong>Latent Semantic Analysis (LSA)</strong>  </p>
<p><strong>LSA</strong> is an NLP technique formulated by Susan T. Dumais et al. <span class="citation">(<a href="bibliography.html#ref-ref867d">1988</a>)</span> for semantic analysis. It is dependent upon terms found in documents. With <strong>LSA</strong>, our goal is to determine the relationship between documents. To achieve our goal, let us recall a <strong>Matrix Decomposition</strong> method in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>) called <strong>Singular Value Decomposition (SVD)</strong>.  </p>
<p><span class="math display" id="eq:equate1130030">\[\begin{align}
\mathbf{M}_{(\text{n x m})} = \mathbf{U}_{(\text{n x p})}\mathbf{\Sigma}_{(\text{p x p})}\mathbf{V}^{\text{T}}_{(\text{p x m})}
\ \ \ \ \ \
where\ \mathbf{\Sigma}\ \text{is diagonal matrix} \tag{11.31} 
\end{align}\]</span></p>
<p>The formula can be interpreted based on Figure <a href="11.3-natural-language-processing-nlp.html#fig:lsamatrix">11.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lsamatrix"></span>
<img src="lsa_matrix.png" alt="Latent Semantic Analysis" width="100%" />
<p class="caption">
Figure 11.11: Latent Semantic Analysis
</p>
</div>
<p>Previously, we are able to derive the <strong>DTM</strong> and <strong>TDM</strong> representations of our relevant terms. Here, both our <strong>DTM</strong> and <strong>TDM</strong> contain <strong>TF-IDF</strong> weights instead of basic <strong>Term Frequency</strong>.</p>

<div class="sourceCode" id="cb1644"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1644-1" data-line-number="1">dtm.doc =<span class="st"> </span>tfidf =<span class="st"> </span><span class="kw">dfm_tfidf</span>(doc.dfm, <span class="dt">scheme_tf =</span> <span class="st">&quot;prop&quot;</span>, <span class="dt">base=</span><span class="kw">exp</span>(<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1644-2" data-line-number="2">                            <span class="dt">force=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1644-3" data-line-number="3">tdm.doc =<span class="st"> </span><span class="kw">data.frame</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(dtm.doc)))</a>
<a class="sourceLine" id="cb1644-4" data-line-number="4">tdm.doc</a></code></pre></div>
<pre><code>##             D1      D2     D3     D4
## big    0.17329 0.20794 0.0000 0.0000
## animal 0.07192 0.02877 0.1438 0.0000
## my     0.34657 0.00000 0.0000 0.0000
## house  0.34657 0.00000 0.0000 0.0000
## great  0.00000 0.41589 0.0000 0.0000
## ears   0.00000 0.13863 0.0000 0.0000
## eyes   0.00000 0.13863 0.0000 0.0000
## teeth  0.00000 0.13863 0.0000 0.0000
## wolf   0.00000 0.00000 0.3466 0.1155
## little 0.00000 0.00000 0.0000 0.2310
## red    0.00000 0.00000 0.0000 0.2310
## riding 0.00000 0.00000 0.0000 0.2310
## hood   0.00000 0.00000 0.0000 0.2310
## met    0.00000 0.00000 0.0000 0.2310</code></pre>

<p>Following <strong>SVD</strong>, we should be able to decompose <strong>TDM</strong> and obtain the <strong>Topic-To-Document</strong> matrix.</p>

<div class="sourceCode" id="cb1646"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1646-1" data-line-number="1">(<span class="dt">M =</span> <span class="kw">svd</span>(tdm.doc))</a></code></pre></div>
<pre><code>## $d
## [1] 0.5604 0.5391 0.4869 0.3598
## 
## $u
##          [,1]     [,2]     [,3]     [,4]
##  [1,] 0.47681 -0.05787  0.05537 -0.03755
##  [2,] 0.14739  0.05267 -0.07076  0.37164
##  [3,] 0.43794 -0.04288 -0.49816 -0.06325
##  [4,] 0.43794 -0.04288 -0.49816 -0.06325
##  [5,] 0.51567 -0.07286  0.60890 -0.01185
##  [6,] 0.17189 -0.02429  0.20297 -0.00395
##  [7,] 0.17189 -0.02429  0.20297 -0.00395
##  [8,] 0.17189 -0.02429  0.20297 -0.00395
##  [9,] 0.07005  0.36649 -0.01581  0.84684
## [10,] 0.03971  0.41202  0.01415 -0.16440
## [11,] 0.03971  0.41202  0.01415 -0.16440
## [12,] 0.03971  0.41202  0.01415 -0.16440
## [13,] 0.03971  0.41202  0.01415 -0.16440
## [14,] 0.03971  0.41202  0.01415 -0.16440
## 
## $v
##         [,1]     [,2]     [,3]     [,4]
## [1,] 0.70809 -0.06671 -0.69989 -0.06566
## [2,] 0.69480 -0.09445  0.71290 -0.01025
## [3,] 0.08116  0.24965 -0.03216  0.96439
## [4,] 0.09632  0.96141  0.02982 -0.25599</code></pre>

<p>We reduce the decomposed matrices such that we only consider the first two columns:</p>

<div class="sourceCode" id="cb1648"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1648-1" data-line-number="1">reduced.dtm =<span class="st"> </span>M<span class="op">$</span>u[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ] <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d[<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ]) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v[,<span class="dv">1</span><span class="op">:</span><span class="dv">2</span> ])</a>
<a class="sourceLine" id="cb1648-2" data-line-number="2"><span class="kw">rownames</span>(reduced.dtm)  =<span class="st"> </span><span class="kw">rownames</span>(tdm.doc)</a>
<a class="sourceLine" id="cb1648-3" data-line-number="3"><span class="kw">colnames</span>(reduced.dtm)  =<span class="st"> </span><span class="kw">colnames</span>(tdm.doc)</a>
<a class="sourceLine" id="cb1648-4" data-line-number="4">reduced.dtm</a></code></pre></div>
<pre><code>##               D1        D2       D3        D4
## big    0.1912700  0.188585 0.013895 -0.004262
## animal 0.0565857  0.054700 0.013792  0.035253
## my     0.1753109  0.172691 0.014145  0.001409
## house  0.1753109  0.172691 0.014145  0.001409
## great  0.2072291  0.204479 0.013645 -0.009933
## ears   0.0690764  0.068160 0.004548 -0.003311
## eyes   0.0690764  0.068160 0.004548 -0.003311
## teeth  0.0690764  0.068160 0.004548 -0.003311
## wolf   0.0146147  0.008612 0.052513  0.193743
## little 0.0009392 -0.005518 0.057261  0.215703
## red    0.0009392 -0.005518 0.057261  0.215703
## riding 0.0009392 -0.005518 0.057261  0.215703
## hood   0.0009392 -0.005518 0.057261  0.215703
## met    0.0009392 -0.005518 0.057261  0.215703</code></pre>

<p>With the new coefficients reflected in our <strong>TDM</strong> obtained through <strong>SVD</strong>, let us compute for <strong>cosine similarity</strong> to see any effect. Here, we use a 3rd-party library called <strong>lsa</strong> for our similarity calculation:</p>

<div class="sourceCode" id="cb1650"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1650-1" data-line-number="1"><span class="kw">library</span>(lsa)</a>
<a class="sourceLine" id="cb1650-2" data-line-number="2">original.dtm =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">dfm_weight</span>(doc.dfm) )</a>
<a class="sourceLine" id="cb1650-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;original&quot;</span> =<span class="st"> </span>lsa<span class="op">::</span><span class="kw">cosine</span>(original.dtm [<span class="dv">1</span>,], original.dtm [<span class="dv">2</span>,]),</a>
<a class="sourceLine" id="cb1650-4" data-line-number="4">  <span class="st">&quot;new&quot;</span> =<span class="st"> </span>lsa<span class="op">::</span><span class="kw">cosine</span>(reduced.dtm[<span class="dv">1</span>,], reduced.dtm[<span class="dv">2</span>,]))</a></code></pre></div>
<pre><code>## original      new 
##   0.4264   0.9016</code></pre>

<p>Notice that with <strong>LSA</strong>, our two documents (D1 and D2) are similar. Hence, there is a 90.16% relationship between the two documents. However, this latent relationship is presented only as a percentage - the interpretation of such a relationship is unknown. Therefore, let us review <strong>pLSA</strong> to see how the interpretation is considered.</p>
<p><strong>Probabilistic Latent Semantic Analysis (pLSA)</strong>  </p>
<p><strong>pLSA</strong> is formulated by Thomas Hofmann <span class="citation">(<a href="bibliography.html#ref-ref220t">1999</a>)</span>, and it introduces the idea of an <strong>Aspect model</strong> (See Figure <a href="11.3-natural-language-processing-nlp.html#fig:aspectmodel">11.12</a> adapted from David M. Blei <span class="citation">(<a href="bibliography.html#ref-ref866b">2012</a>)</span>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aspectmodel"></span>
<img src="aspectmodel.png" alt="pLSA (Aspect Model)" width="100%" />
<p class="caption">
Figure 11.12: pLSA (Aspect Model)
</p>
</div>
<p>The model can also be represented in the form of a <strong>Probabilistic Graphical Model (PGM)</strong> (see Figure <a href="11.3-natural-language-processing-nlp.html#fig:plsapgm">11.13</a>), particularly formulation 1.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plsapgm"></span>
<img src="plsapgm.png" alt="pLSA (PGM)" width="80%" />
<p class="caption">
Figure 11.13: pLSA (PGM)
</p>
</div>
<p>If we are to follow the <strong>PGM</strong> figure, we have two random variables denoted by <strong>Z</strong> and <strong>W</strong>. The former follows a multinomial distribution denoted by <span class="math inline">\(\theta_m\)</span>, and the latter follows a second multinomial distribution denoted by <span class="math inline">\(\phi_k\)</span>. Hence, we can express them as such:</p>
<p><span class="math display">\[
Z_{mn} \sim Multinomial(\theta_m)\ \ \ \ \ \ \ \ \ \ \ W_{mn} \sim Multinomial(\phi_k)
\]</span></p>
<p>Our goal is to be able to generate the topic proportionality (<span class="math inline">\(\theta_m\)</span> ) per-document and word distribution (<span class="math inline">\(\phi_k\)</span>) per-topic (see Figure <a href="11.3-natural-language-processing-nlp.html#fig:plsadisrib">11.14</a>) based on our given <strong>Corpus</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plsadisrib"></span>
<img src="plsadistrib.png" alt="pLSA (Multinomial Distribution)" width="80%" />
<p class="caption">
Figure 11.14: pLSA (Multinomial Distribution)
</p>
</div>
<p>We first determine the probability of seeing a document and a word appearing together to achieve our goal. We can fashion this using a joint distribution but with two similar formulations:</p>
<p>The first formulation allows us to determine the likelihood of seeing a document in which we can also find a word based on the distribution of topics.</p>
<p><span class="math display" id="eq:equate1130031">\[\begin{align}
P(d, w) = P(d)\sum_{z \in Z} P(w|z)P(z|d) \tag{11.32} 
\end{align}\]</span></p>
<p>The second formulation follows the <strong>LSA</strong> SVD-based matrix model such that we have the following:</p>
<p><span class="math display" id="eq:equate1130032">\[\begin{align}
P(d, w) = \sum_z \underbrace{P(z)}_{\mathbf{\text{U}}}
    \underbrace{P(d|z)}_{\mathbf{\text{S}}}\underbrace{P(w|z)}_{\mathbf{\text{V}}^T} \tag{11.33} 
\end{align}\]</span></p>
<p>Determining the probabilities or their corresponding parameters needs to be done heuristically. The most common method uses <strong>Expectation-Maximization (EM)</strong>; albeit, for <strong>LDA</strong> in the next section, we can use <strong>Gibbs Sampling</strong> due to the <strong>bayesian a-priori</strong> hyperparameters introduced.</p>
<p>Let us consider <strong>EM</strong> based on the two formulations for <strong>pLSA</strong>. See below <span class="citation">(Hong L. <a href="bibliography.html#ref-ref929l">2012</a>)</span> - derivation is not included:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Formulation 1:}}\\
\ \ \ \ P(d, w) = P(d)\sum_z P(w|z)P(z|d)\\
\mathbf{\text{E-Step:}}\\
P(z|w,d) = \frac{P(w|z)P(z|d)}
              {\sum_z P(w|z)P(z|d)}\\
\mathbf{\text{M-Step:}}\\
P(d) = \frac{n(d)}{\sum_d n(d)}\\
P(w|z) = \frac{\sum_d n(d,w) P(z|w,d)}
             {\sum_w \sum_d n(d,w) P(z|w,d)} \\
P(z|d) = \frac{\sum_w n(d,w)P(z|w,d)}{n(d)}
\\  \\ \\
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Formulation 2:}}\\
\ \ \ \ P(d, w) = \sum_zP(z) P(d|z)P(w|z)\\
\mathbf{\text{E-Step:}}\\
P(z|w,d) = \frac{P(w,z,d)}{P(w,d)} \\
\ \ \ \ \ \ \ \ \ \ \ \  =\frac{P(w|z)P(d|z)P(z)}
            {\sum_z P(w|z)P(d|z)P(z)}
\\
\mathbf{\text{M-Step:}}\\
P(z) = \frac{\sum_d \sum_w n(d,w)P(z|w,d)}
          {\sum_d \sum_w n(d,w)}\\
P(w|z) = \frac{\sum_d  n(d,w)P(z|w,d)}
          {\sum_w \sum_d \sum_w n(d,w)}\\
P(d|z) = \frac{\sum_w  n(d,w)P(z|w,d)}
          {\sum_d \sum_w \sum_w n(d,w)}\\
\end{array}
\right.
\]</span></p>
<p>We perform iterative calculations and updates to maximize the log-likelihood. For the first formulation, we use the following log-likelihood:</p>
<p><span class="math display" id="eq:equate1130033">\[\begin{align}
\mathcal{L}_1 = \sum_d \sum_w n(d, w) \log_e \left[P(d) \sum_z P(w|z) P(z|d)\right] \tag{11.34} 
\end{align}\]</span></p>
<p>For the second formulation, we use the following log-likelihood:</p>
<p><span class="math display" id="eq:equate1130034">\[\begin{align}
\mathcal{L}_2 = \sum_d \sum_w n(d, w) \log_e \left[ \sum_zP(z)  P(d|z)P(w|z)\right] \tag{11.35} 
\end{align}\]</span></p>
<p>Instead of giving an example to illustrate <strong>pLSA</strong>, let us rather extend our discussion by introducing <strong>Latent Dirichlet Allocation (LDA)</strong> which is an enhanced variant of <strong>pLSA</strong> in that it uses <strong>Naive Bayes</strong> for which both <span class="math inline">\(\theta_m\)</span> and <span class="math inline">\(\phi_k\)</span> follow a <strong>Dirichlet</strong> distribution using two corresponding <strong>Prior</strong> hyperparameters, namely <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) and <strong>beta</strong> (<span class="math inline">\(\beta\)</span>).</p>
<p><strong>Latent Dirichlet Allocation (LDA)</strong>  </p>
<p><strong>LDA</strong> is a popular algorithm commonly used in <strong>topic modeling</strong> formulated by David M. Blei, Andrew Y. Ng, and Michael I. Jordan <span class="citation">(<a href="bibliography.html#ref-ref209d">2003</a>)</span>. As with <strong>pLSA</strong>, the grand idea is to <strong>classify</strong> a bag of words gathered from a collection of documents called <strong>corpus</strong> into a set of arbitrarily chosen topics. </p>
<p>Let us first review the <strong>PGM</strong> for <strong>LDA</strong> (see Figure <a href="11.3-natural-language-processing-nlp.html#fig:ldapgm">11.15</a>).</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ldapgm"></span>
<img src="ldapgm.png" alt="LDA (PGM)" width="90%" />
<p class="caption">
Figure 11.15: LDA (PGM)
</p>
</div>
<p>The two <strong>Dirichlet</strong> parameters are used to provide the initial multinomial distributions, e.g.Â sampling a <strong>Dirichlet distribution</strong> as <strong>priors</strong> for <span class="math inline">\(\theta_m\)</span> and <span class="math inline">\(\phi_k\)</span>.</p>
<p><span class="math display" id="eq:equate1130035">\[\begin{align}
\theta_m \sim Dir(\alpha)\ \ \ \ \ \ \ \ \ \ \
\phi_k \sim Dir(\beta) \tag{11.36} 
\end{align}\]</span></p>
<p>To illustrate, let us use <strong>icml-nips-iclr-dataset</strong> dataset sourced from <strong>github/cqql</strong> - a note that all contents of the datasets comes from ICML, NIPS, and ICLR 2016-2018. For our dataset, we are only interested in the <strong>title</strong> for papers published in 2018. Assume for a moment that each title represents one document.</p>

<div class="sourceCode" id="cb1652"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1652-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1652-2" data-line-number="2">icml.nips.iclr =<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">file =</span> <span class="st">&quot;~/Documents/nlp/papers.csv&quot;</span>, </a>
<a class="sourceLine" id="cb1652-3" data-line-number="3">                          <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1652-4" data-line-number="4">yr.idx       =<span class="st"> </span><span class="kw">which</span>(icml.nips.iclr[,<span class="kw">c</span>(<span class="st">&quot;Year&quot;</span>)] <span class="op">==</span><span class="st"> &quot;2018&quot;</span>)</a>
<a class="sourceLine" id="cb1652-5" data-line-number="5">doc.titles  =<span class="st"> </span><span class="kw">unique</span>(icml.nips.iclr[yr.idx,<span class="kw">c</span>(<span class="st">&quot;Title&quot;</span>)])</a>
<a class="sourceLine" id="cb1652-6" data-line-number="6"><span class="kw">strwrap</span>(<span class="kw">head</span>(doc.titles), <span class="dt">width=</span><span class="dv">70</span>, <span class="dt">exdent=</span><span class="dv">5</span>)</a></code></pre></div>
<pre><code>## [1] &quot;Spline Filters For End-to-End Deep Learning&quot;                          
## [2] &quot;Non-linear motor control by local learning in spiking neural networks&quot;
## [3] &quot;Implicit Quantile Networks for Distributional Reinforcement Learning&quot; 
## [4] &quot;An Inference-Based Policy Gradient Method for Learning Options&quot;       
## [5] &quot;Predict and Constrain: Modeling Cardinality in Deep Structured&quot;       
## [6] &quot;     Prediction&quot;                                                      
## [7] &quot;Differentially Private Matrix Completion Revisited&quot;</code></pre>

<p>In this illustration, let us use <strong>tm</strong> to perform the pre-processing, ignoring stemming for a moment.</p>

<div class="sourceCode" id="cb1654"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1654-1" data-line-number="1"><span class="kw">library</span>(tm)</a>
<a class="sourceLine" id="cb1654-2" data-line-number="2">no.docs          =<span class="st"> </span><span class="kw">length</span>(doc.titles)</a>
<a class="sourceLine" id="cb1654-3" data-line-number="3">doc_ids          =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;D&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>, no.docs))</a>
<a class="sourceLine" id="cb1654-4" data-line-number="4">doc.dfm          =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">doc_id =</span> doc_ids, <span class="dt">text =</span> doc.titles)</a>
<a class="sourceLine" id="cb1654-5" data-line-number="5">doc.dfm.source   =<span class="st"> </span><span class="kw">DataframeSource</span>(doc.dfm)</a>
<a class="sourceLine" id="cb1654-6" data-line-number="6">doc.corpus       =<span class="st"> </span><span class="kw">Corpus</span>(doc.dfm.source)</a>
<a class="sourceLine" id="cb1654-7" data-line-number="7">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(doc.corpus, <span class="kw">content_transformer</span>(tolower))</a>
<a class="sourceLine" id="cb1654-8" data-line-number="8">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removePunctuation, </a>
<a class="sourceLine" id="cb1654-9" data-line-number="9">                          <span class="dt">preserve_intra_word_dashes =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1654-10" data-line-number="10">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removeNumbers)</a>
<a class="sourceLine" id="cb1654-11" data-line-number="11">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, removeWords, <span class="kw">stopwords</span>(<span class="st">&quot;english&quot;</span>))</a>
<a class="sourceLine" id="cb1654-12" data-line-number="12"><span class="co">#preprocessed.doc = tm_map(preprocessed.doc, stemDocument, language = &quot;en&quot;)</span></a>
<a class="sourceLine" id="cb1654-13" data-line-number="13">preprocessed.doc =<span class="st"> </span><span class="kw">tm_map</span>(preprocessed.doc, stripWhitespace)</a>
<a class="sourceLine" id="cb1654-14" data-line-number="14">preprocessed.doc</a></code></pre></div>
<pre><code>## &lt;&lt;SimpleCorpus&gt;&gt;
## Metadata:  corpus specific: 1, document level (indexed): 0
## Content:  documents: 1966</code></pre>

<p>Now, let us introduce the use of the <strong>topicmodels</strong> package to help us with <strong>LDA</strong>. In this exercise, we limit our solution to only four topics over words with a minimum frequency of 1. From there, we cast the pre-processed document into <strong>DTM</strong> format.</p>

<div class="sourceCode" id="cb1656"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1656-1" data-line-number="1"><span class="kw">library</span>(topicmodels)</a>
<a class="sourceLine" id="cb1656-2" data-line-number="2">min.Freq =<span class="st"> </span><span class="dv">1</span>; K =<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb1656-3" data-line-number="3">DTM =<span class="st"> </span><span class="kw">DocumentTermMatrix</span>(preprocessed.doc, </a>
<a class="sourceLine" id="cb1656-4" data-line-number="4">      <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">bounds =</span> <span class="kw">list</span>(<span class="dt">global =</span> <span class="kw">c</span>(min.Freq, <span class="ot">Inf</span>))))</a></code></pre></div>

<p>We can use a couple of methods to meet our goals, as pointed out in the <strong>pLSA</strong> section. We can use <strong>Variational EM (VEM)</strong> or <strong>Gibbs Sampling</strong>.  </p>
<p>For <strong>VEM</strong>, recall <strong>Variational Inference</strong> in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under <strong>Bayesian Inference</strong> Section. Using <strong>LDA(.)</strong> function, we issue a set of controls, including tolerance levels for convergence and maximum iteration, with the seed being set.</p>

<div class="sourceCode" id="cb1657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1657-1" data-line-number="1">tol =<span class="st"> </span><span class="fl">1e-3</span></a>
<a class="sourceLine" id="cb1657-2" data-line-number="2">em.control =<span class="st"> </span>var.control =<span class="st"> </span><span class="kw">list</span>(<span class="dt">iter.max =</span> <span class="dv">500</span>,  <span class="dt">tol =</span> tol)</a>
<a class="sourceLine" id="cb1657-3" data-line-number="3">lda.vem.control =<span class="st"> </span><span class="kw">list</span>(  <span class="dt">nstart =</span> <span class="dv">1</span>, <span class="dt">seed =</span> <span class="dv">2018</span>,  <span class="dt">verbose=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1657-4" data-line-number="4">             <span class="dt">var  =</span> var.control,  <span class="dt">em   =</span> em.control, <span class="dt">initialize =</span> <span class="st">&quot;random&quot;</span>)</a>
<a class="sourceLine" id="cb1657-5" data-line-number="5">topic.model =<span class="st"> </span><span class="kw">LDA</span>(DTM, K, <span class="dt">method=</span><span class="st">&quot;VEM&quot;</span>, <span class="dt">control=</span>lda.vem.control)</a></code></pre></div>
<pre><code>## **** em iteration 1 ****
## document 1966
## new alpha = 13.80105
## **** em iteration 2 ****
## document 1966
## new alpha = 15.10332
## **** em iteration 3 ****
## document 1966
## new alpha = 16.39318
## final e step document 1966</code></pre>

<p>Let us output the list of latent topics generated using the <strong>VEM</strong> method, restricting to only the top 15 words per topic.</p>

<div class="sourceCode" id="cb1659"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1659-1" data-line-number="1">(<span class="dt">topic.matrix =</span> <span class="kw">terms</span>(topic.model, <span class="dv">15</span>))</a></code></pre></div>
<pre><code>##       Topic 1        Topic 2         Topic 3        
##  [1,] &quot;learning&quot;     &quot;learning&quot;      &quot;learning&quot;     
##  [2,] &quot;neural&quot;       &quot;deep&quot;          &quot;neural&quot;       
##  [3,] &quot;optimization&quot; &quot;networks&quot;      &quot;deep&quot;         
##  [4,] &quot;data&quot;         &quot;optimization&quot;  &quot;variational&quot;  
##  [5,] &quot;stochastic&quot;   &quot;generative&quot;    &quot;adversarial&quot;  
##  [6,] &quot;efficient&quot;    &quot;reinforcement&quot; &quot;networks&quot;     
##  [7,] &quot;networks&quot;     &quot;via&quot;           &quot;models&quot;       
##  [8,] &quot;inference&quot;    &quot;adversarial&quot;   &quot;recurrent&quot;    
##  [9,] &quot;network&quot;      &quot;bayesian&quot;      &quot;via&quot;          
## [10,] &quot;via&quot;          &quot;training&quot;      &quot;reinforcement&quot;
## [11,] &quot;descent&quot;      &quot;models&quot;        &quot;training&quot;     
## [12,] &quot;prediction&quot;   &quot;stochastic&quot;    &quot;sparse&quot;       
## [13,] &quot;unsupervised&quot; &quot;inference&quot;     &quot;generative&quot;   
## [14,] &quot;models&quot;       &quot;model&quot;         &quot;linear&quot;       
## [15,] &quot;machine&quot;      &quot;gradient&quot;      &quot;using&quot;        
##       Topic 4        
##  [1,] &quot;networks&quot;     
##  [2,] &quot;neural&quot;       
##  [3,] &quot;using&quot;        
##  [4,] &quot;gradient&quot;     
##  [5,] &quot;models&quot;       
##  [6,] &quot;via&quot;          
##  [7,] &quot;learning&quot;     
##  [8,] &quot;reinforcement&quot;
##  [9,] &quot;adversarial&quot;  
## [10,] &quot;data&quot;         
## [11,] &quot;model&quot;        
## [12,] &quot;estimation&quot;   
## [13,] &quot;bayesian&quot;     
## [14,] &quot;deep&quot;         
## [15,] &quot;fast&quot;</code></pre>

<p>For the <strong>Gibbs Sampling</strong> method, recall the <strong>Simulation and Sampling</strong> methods in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>). Using <strong>LDA(.)</strong>, we introduce <strong>Burn-In</strong> and <strong>Thinning</strong>. Note that we use <strong>Gibbs sampling</strong> with 600 sampling iterations - including <strong>Burn-in</strong> - but for simplicity, we omit <strong>Thinning</strong> for now.</p>

<div class="sourceCode" id="cb1661"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1661-1" data-line-number="1"><span class="kw">library</span>(topicmodels)</a>
<a class="sourceLine" id="cb1661-2" data-line-number="2">lda.gibbs.control =<span class="st"> </span><span class="kw">list</span>( <span class="dt">burnin =</span> <span class="dv">100</span>,  <span class="dt">iter=</span><span class="dv">500</span>, <span class="dt">nstart=</span><span class="dv">1</span>,  </a>
<a class="sourceLine" id="cb1661-3" data-line-number="3">                          <span class="dt">verbose=</span><span class="dv">100</span>,  <span class="dt">seed=</span><span class="dv">2018</span>)</a>
<a class="sourceLine" id="cb1661-4" data-line-number="4">topic.model =<span class="st"> </span><span class="kw">LDA</span>(DTM, K, <span class="dt">method=</span><span class="st">&quot;Gibbs&quot;</span>, <span class="dt">control=</span>lda.gibbs.control)</a></code></pre></div>
<pre><code>## K = 4; V = 3152; M = 1966
## Sampling 600 iterations!
## Iteration 100 ...
## Iteration 200 ...
## Iteration 300 ...
## Iteration 400 ...
## Iteration 500 ...
## Iteration 600 ...
## Gibbs sampling completed!</code></pre>

<p>Let us output the list of latent topics generated using the <strong>Gibbs Sampling</strong> method, restricting to only the top 15 words per topic.</p>

<div class="sourceCode" id="cb1663"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1663-1" data-line-number="1">(<span class="dt">topic.matrix =</span> <span class="kw">terms</span>(topic.model, <span class="dv">15</span>))</a></code></pre></div>
<pre><code>##       Topic 1          Topic 2       Topic 3       
##  [1,] &quot;networks&quot;       &quot;models&quot;      &quot;stochastic&quot;  
##  [2,] &quot;neural&quot;         &quot;adversarial&quot; &quot;via&quot;         
##  [3,] &quot;optimization&quot;   &quot;inference&quot;   &quot;using&quot;       
##  [4,] &quot;bayesian&quot;       &quot;generative&quot;  &quot;data&quot;        
##  [5,] &quot;training&quot;       &quot;variational&quot; &quot;gradient&quot;    
##  [6,] &quot;network&quot;        &quot;descent&quot;     &quot;linear&quot;      
##  [7,] &quot;convolutional&quot;  &quot;gaussian&quot;    &quot;distributed&quot; 
##  [8,] &quot;recurrent&quot;      &quot;processes&quot;   &quot;sparse&quot;      
##  [9,] &quot;gradient&quot;       &quot;machine&quot;     &quot;graph&quot;       
## [10,] &quot;policy&quot;         &quot;learning&quot;    &quot;algorithms&quot;  
## [11,] &quot;adaptive&quot;       &quot;synthesis&quot;   &quot;efficient&quot;   
## [12,] &quot;matrix&quot;         &quot;information&quot; &quot;estimation&quot;  
## [13,] &quot;efficient&quot;      &quot;clustering&quot;  &quot;unsupervised&quot;
## [14,] &quot;representation&quot; &quot;local&quot;       &quot;image&quot;       
## [15,] &quot;classification&quot; &quot;search&quot;      &quot;approach&quot;    
##       Topic 4          
##  [1,] &quot;learning&quot;       
##  [2,] &quot;deep&quot;           
##  [3,] &quot;reinforcement&quot;  
##  [4,] &quot;model&quot;          
##  [5,] &quot;structured&quot;     
##  [6,] &quot;via&quot;            
##  [7,] &quot;sampling&quot;       
##  [8,] &quot;transfer&quot;       
##  [9,] &quot;representations&quot;
## [10,] &quot;graphs&quot;         
## [11,] &quot;knowledge&quot;      
## [12,] &quot;language&quot;       
## [13,] &quot;method&quot;         
## [14,] &quot;selection&quot;      
## [15,] &quot;active&quot;</code></pre>

<p>Note that we leave the choice of methods to the readers. The success of topic modeling comes with proper pre-processing of terms and proper tune-up of each method, among many other considerations.</p>
<p>For the computed <strong>per-document topic proportion</strong>, let us display only the first ten documents and their corresponding topic distribution. Notice that the summation of topic proportion per document is always 1.</p>

<div class="sourceCode" id="cb1665"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1665-1" data-line-number="1">doc.topic =<span class="st"> </span><span class="kw">posterior</span>(topic.model)<span class="op">$</span>topics[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,]</a>
<a class="sourceLine" id="cb1665-2" data-line-number="2"><span class="kw">colnames</span>(doc.topic) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>, K))</a>
<a class="sourceLine" id="cb1665-3" data-line-number="3">doc.topic</a></code></pre></div>
<pre><code>##         T1     T2     T3     T4
## D1  0.2273 0.2818 0.2273 0.2636
## D2  0.2500 0.2500 0.2500 0.2500
## D3  0.2768 0.2232 0.2232 0.2768
## D4  0.2232 0.2232 0.2411 0.3125
## D5  0.2368 0.2193 0.2368 0.3070
## D6  0.2455 0.2818 0.2455 0.2273
## D7  0.3070 0.2368 0.2368 0.2193
## D8  0.2547 0.2547 0.2358 0.2547
## D9  0.2368 0.2368 0.2368 0.2895
## D10 0.2845 0.2328 0.2328 0.2500</code></pre>

<p>For the computed <strong>per-topic word distribution</strong>, let us display only three topics and their corresponding word distribution.</p>

<div class="sourceCode" id="cb1667"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1667-1" data-line-number="1">topic.terms =<span class="st"> </span><span class="kw">posterior</span>(topic.model)<span class="op">$</span>terms</a>
<a class="sourceLine" id="cb1667-2" data-line-number="2">ttd =<span class="st"> </span><span class="ot">NULL</span>; most =<span class="st"> </span><span class="dv">250</span>; topic.term.distribution =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1667-3" data-line-number="3"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(K<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb1667-4" data-line-number="4">  topic.term.distribution[[k]] =<span class="st">  </span><span class="kw">sort</span>(topic.terms[k, ], <span class="dt">decreasing=</span><span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb1667-5" data-line-number="5">  topics =<span class="st"> </span><span class="kw">as.data.frame</span>(topic.term.distribution[[k]][<span class="dv">1</span><span class="op">:</span>most], </a>
<a class="sourceLine" id="cb1667-6" data-line-number="6">                         <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1667-7" data-line-number="7">  t.names =<span class="st"> </span><span class="kw">rownames</span>(topics)</a>
<a class="sourceLine" id="cb1667-8" data-line-number="8">  p =<span class="st"> </span><span class="kw">cbind</span>(   t.names,  <span class="kw">round</span>(topics,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb1667-9" data-line-number="9">  <span class="kw">colnames</span>(p) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;T&quot;</span>, k), <span class="kw">paste0</span>(<span class="st">&quot;Probs&quot;</span>, k))</a>
<a class="sourceLine" id="cb1667-10" data-line-number="10">  ttd =<span class="st"> </span><span class="kw">cbind</span>(ttd, <span class="kw">as.matrix</span>(p))</a>
<a class="sourceLine" id="cb1667-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1667-12" data-line-number="12"><span class="kw">rownames</span>(ttd) =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1667-13" data-line-number="13"><span class="co"># topic.term.distribution, display 1st 15 rows</span></a>
<a class="sourceLine" id="cb1667-14" data-line-number="14">(<span class="dt">ttd =</span> <span class="kw">as.data.frame</span>(ttd, <span class="dt">stringsAsFactors=</span><span class="ot">FALSE</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">15</span>,] </a></code></pre></div>
<pre><code>##                T1 Probs1          T2 Probs2
## 1        networks  0.073      models  0.035
## 2          neural  0.067 adversarial  0.027
## 3    optimization  0.032   inference  0.020
## 4        bayesian  0.020  generative  0.020
## 5        training  0.018 variational  0.018
## 6         network  0.016     descent  0.013
## 7   convolutional  0.013    gaussian  0.010
## 8       recurrent  0.012   processes  0.009
## 9        gradient  0.012     machine  0.008
## 10         policy  0.010    learning  0.008
## 11       adaptive  0.009   synthesis  0.007
## 12         matrix  0.009 information  0.007
## 13      efficient  0.009  clustering  0.007
## 14 representation  0.007       local  0.007
## 15 classification  0.007      search  0.007
##              T3 Probs3
## 1    stochastic  0.025
## 2           via  0.024
## 3         using  0.024
## 4          data  0.021
## 5      gradient  0.014
## 6        linear  0.013
## 7   distributed  0.013
## 8        sparse  0.012
## 9         graph  0.012
## 10   algorithms  0.012
## 11    efficient  0.010
## 12   estimation  0.010
## 13 unsupervised  0.009
## 14        image  0.009
## 15     approach  0.009</code></pre>

<p>Let us display <strong>wordcloud</strong> for the top terms in the first topic (See Figure <a href="11.3-natural-language-processing-nlp.html#fig:wordcloudtopic">11.16</a>).</p>

<div class="sourceCode" id="cb1669"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1669-1" data-line-number="1"><span class="kw">library</span>(wordcloud)    <span class="co"># generates wordcloud.</span></a>
<a class="sourceLine" id="cb1669-2" data-line-number="2"><span class="kw">library</span>(RColorBrewer) <span class="co"># renders colored texts.</span></a>
<a class="sourceLine" id="cb1669-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1669-4" data-line-number="4">terms =<span class="st"> </span>ttd[,<span class="kw">c</span>(<span class="st">&quot;T1&quot;</span>)]; probabilities =<span class="st"> </span><span class="kw">as.numeric</span>(ttd[,<span class="kw">c</span>(<span class="st">&quot;Probs1&quot;</span>)])</a>
<a class="sourceLine" id="cb1669-5" data-line-number="5"><span class="kw">wordcloud</span>(<span class="dt">words =</span> terms, <span class="dt">freq =</span> probabilities,  <span class="dt">max.words=</span><span class="dv">150</span> , </a>
<a class="sourceLine" id="cb1669-6" data-line-number="6">          <span class="dt">random.order=</span><span class="ot">FALSE</span>,  <span class="dt">rot.per =</span> <span class="fl">0.35</span>, <span class="dt">min.freq =</span> <span class="dv">1</span>,</a>
<a class="sourceLine" id="cb1669-7" data-line-number="7">          <span class="dt">colors=</span><span class="kw">brewer.pal</span>(<span class="dv">8</span>, <span class="st">&quot;Dark2&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:wordcloudtopic"></span>
<img src="DS_files/figure-html/wordcloudtopic-1.png" alt="Topic Model (WordCloud)" width="90%" />
<p class="caption">
Figure 11.16: Topic Model (WordCloud)
</p>
</div>

<p>We can readily assume that the first topic suggests a context around neural networks.</p>
<p>There are many other considerations when dealing with <strong>Topic Modeling</strong>. Such considerations cannot all be discussed in one book. However, one such important consideration is choosing a suitable number (<strong>K</strong>) of topics. There are methods proposed, and the <strong>LDA(.)</strong> function offers a select number of metrics. We leave readers to experiment on the <strong>FindtopicsNumber(.)</strong> function from the <strong>ldatuning</strong> R package for the metrics named after Griffiths 2004, CaoJuan 2009, Arun 2010, and Deveaud 2014. We also encourage exploring the <strong>Rate of Perplexity Change (RPC)</strong>, which helps to measure and cross-validate a suitable number (<strong>K</strong>) of topics to use.</p>
<p><strong>Explicit Semantic Analysis (ESA)</strong>  </p>
<p>Another <strong>Semantic Analysis</strong> method proposed is <strong>Explicit Semantic Analysis (ESA)</strong>. This method may be regarded as an enhancement to the concept of <strong>LSA</strong> and <strong>LDA</strong> in that, instead of topics being <strong>latent</strong>, it introduces the use of <strong>topics</strong> that are explicitly pre-determined. In particular, <strong>topics</strong> are generated from <strong>Wikipedia</strong> or other sources, e.g.Â from <strong>Encyclopedia</strong>. We leave readers to investigate <strong>ESA</strong>.</p>
</div>
<div id="named-entity-recognition-ner" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.7</span> Named Entity Recognition (NER)  <a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Entity identification or extraction</strong> is an essential component of <strong>NLP</strong> and is a sub-part of text extraction. It helps to recognize <strong>Named Entities</strong> to add more context to topic modeling. A <strong>Named Entity</strong> is a real-world proper name given to objects, people, places, dates, events, organizations, and currency. For example, there is only one <strong>Mount Everest</strong> in the world. Someoneâs street address is a <strong>named entity</strong>. <strong>Neil Armstrong</strong> is the first man on the moon. <strong>Bitcoin</strong> is a digital currency. Lastly, <strong>Mcdonaldâs and Coca-Cola</strong> are two world-famous names.</p>
<p>Additionally, <strong>Named Entities</strong> can be categorized accordingly. For example, <strong>January 1st</strong> is categorized as <strong>Date</strong>. Likewise, <strong>New Yearâs Eve</strong> is categorized as one of the dates/times.</p>
<p><strong>NER</strong> becomes apparent for applications around <strong>Customer Support</strong>, <strong>Human Resource</strong>, <strong>Health Care</strong>, and many others.</p>
</div>
<div id="sentiment-and-opinion-analysis" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.3.8</span> Sentiment and Opinion Analysis  <a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>NER</strong>, reading and recognizing sentiments and not just <strong>named entities</strong> is just as important. It helps to map text to sentiments to interpret and predict sentiments. For this, one of the common <strong>Lexicon</strong> resources that maps terms to sentiments is called <strong>SentiWordNet</strong>. There are other resources such as <strong>WordNet-Affect</strong>, <strong>MPQA</strong>, and <strong>SenticNet</strong> to compare <span class="citation">(Musto C., Semeraro G., Polignano M., <a href="bibliography.html#ref-ref940c">n.d.</a>)</span>.</p>
<p>Each of the different resources addresses the following sentimental representations:</p>
<ul>
<li><p><strong>Common Polarity</strong> - texts are represented as <strong>Positive</strong>, <strong>Negative</strong>, or <strong>Neutral</strong>. For example, the word <strong>good</strong> is positive, <strong>bad</strong> is negative, and <strong>ok</strong> may be interpreted as <strong>Neutral</strong>.</p></li>
<li><p><strong>Intensity</strong> - this adds granularity in the form of <strong>Strong</strong>, <strong>Medium</strong>, <strong>Weak</strong> or in the form <strong>positive</strong> and <strong>negative</strong> scores. For example <strong>very good</strong> instead of just <strong>good</strong>.</p></li>
<li><p><strong>Subjectivity</strong> - texts are represented based on human senses, emotions, or opinions. For example, the word <strong>happy</strong> has a higher positive score, and <strong>sad</strong> has a lower negative score. A text that says <strong>this art looks good</strong> has a higher positive score than one that says <strong>this art looks terrible</strong>.</p></li>
<li><p><strong>Objectivity</strong> - texts are represented based on facts. For example, <strong>Earth is round</strong>.</p></li>
</ul>
<p>To illustrate, let us use a 3rd-party library called <strong>sentimentr</strong> developed by <strong>Tyler Rinker</strong> that uses <strong>SentiWordNet</strong>.</p>
<p>We feed a sentence to the function <strong>sentiment_by(.)</strong> that suggests positive sentiment, and we get the following positive score:</p>

<div class="sourceCode" id="cb1670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1670-1" data-line-number="1"><span class="kw">library</span>(sentimentr)</a>
<a class="sourceLine" id="cb1670-2" data-line-number="2"><span class="kw">sentiment</span>(<span class="st">&#39;I am very good&#39;</span>)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          4     0.675</code></pre>

<p>We then feed a sentence that suggests negative sentiment, and we get a positive negative score.</p>

<div class="sourceCode" id="cb1672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1672-1" data-line-number="1"><span class="kw">sentiment</span>(<span class="st">&#39;I am not very good&#39;</span>)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          5  -0.06708</code></pre>

<p>Let us feed random positive words and see how the score looks like:</p>

<div class="sourceCode" id="cb1674"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1674-1" data-line-number="1">terms =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;happy&quot;</span>, <span class="st">&quot;care&quot;</span>, <span class="st">&quot;great&quot;</span>, <span class="st">&quot;light&quot;</span>, <span class="st">&quot;cheerful&quot;</span> )</a>
<a class="sourceLine" id="cb1674-2" data-line-number="2"><span class="kw">sentiment</span>(terms)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          1      0.75
## 2:          2           1          1      1.00
## 3:          3           1          1      0.50
## 4:          4           1          1      0.00
## 5:          5           1          1      0.75</code></pre>

<p>Now, let us try with random negative words:</p>

<div class="sourceCode" id="cb1676"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1676-1" data-line-number="1">terms =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;sad&quot;</span>, <span class="st">&quot;careless&quot;</span>, <span class="st">&quot;worse&quot;</span>, <span class="st">&quot;dark&quot;</span>)</a>
<a class="sourceLine" id="cb1676-2" data-line-number="2"><span class="kw">sentiment</span>(terms)</a></code></pre></div>
<pre><code>##    element_id sentence_id word_count sentiment
## 1:          1           1          1     -0.50
## 2:          2           1          1     -0.50
## 3:          3           1          1     -0.75
## 4:          4           1          1     -0.60</code></pre>

<p>In terms of application, we may find <strong>sentiment analysis</strong> to be useful as part of a <strong>Recommender System</strong>, which we discuss further in a section.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11.2-meta-learning.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="11.4-time-series-forecasting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
