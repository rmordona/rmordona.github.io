<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8.2 Simulation and Sampling | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="8.2 Simulation and Sampling | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8.2 Simulation and Sampling | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="8.1-bayesian-models.html"/>
<link rel="next" href="8.3-bayesian-analysis.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Learning (Deep RL)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="simulation-and-sampling" class="section level2 hasAnchor">
<h2><span class="header-section-number">8.2</span> Simulation and Sampling<a href="8.2-simulation-and-sampling.html#simulation-and-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section discusses a few <strong>simulation</strong> and <strong>sampling</strong> techniques. Such need for simulation and sampling becomes apparent in situations where there are no practical means to obtain samples of an observation. Especially in models with very high dimensionality, such as <strong>lattice models</strong> in which we deal with thousands or even millions of variables (or features in machine learning), numerical computation may not be possible. Sampling data points from large datasets are ideal in this case.</p>
<div id="monte-carlo-estimation" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.1</span> Monte Carlo Estimation <a href="8.2-simulation-and-sampling.html#monte-carlo-estimation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A concept behind <strong>Monte Carlo estimation</strong> is the <strong>Law of the Unconscious Statistician (LOTUS)</strong>, which states that the expected value of a function can be calculated without depending on its distribution. That can be written as such:</p>
<p><span class="math display" id="eq:equate1100148">\[\begin{align}
\mathbb{E}\left[f(X)\right] = \underbrace{\int f(X) P_{(pdf)}(X) dX}_\text{continuous} \approx \underbrace{\sum_{i=1}^N f(X_i)  P_{(pmf)}(X)}_\text{discrete} \tag{8.158} 
\end{align}\]</span></p>
<p>Here, we define the expected value as the sum of the product of the function (the transformed value) and the probability distribution (of the original value). The idea is to use this calculation to estimate the complex integration of a function; thus, this is also called <strong>Monte Carlo integration</strong>.</p>
<p><span class="math display" id="eq:equate1100149">\[\begin{align}
\mathcal{I}(f(x)) = \mathbb{E}\left[f(X)\right] \approx \frac{1}{N} \sum^N_{i=1} f(x_i) \tag{8.159} 
\end{align}\]</span></p>
<p>To illustrate, suppose we have the following simple function: <span class="math inline">\(f(X) = 3X\)</span>. We then generate ten random outcomes between 1 and 5 for X that give us the calculated (transformed) values using <span class="math inline">\(f(X)\)</span>:</p>

<div class="sourceCode" id="cb909"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb909-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb909-2" data-line-number="2">f &lt;-<span class="st"> </span><span class="cf">function</span>(X) { <span class="dv">3</span><span class="op">*</span>X }</a>
<a class="sourceLine" id="cb909-3" data-line-number="3">N =<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb909-4" data-line-number="4">possible.outcome =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">5</span>)</a>
<a class="sourceLine" id="cb909-5" data-line-number="5">prob =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.20</span>, <span class="fl">0.30</span>, <span class="fl">0.05</span>, <span class="fl">0.35</span>)</a>
<a class="sourceLine" id="cb909-6" data-line-number="6">X =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> possible.outcome, <span class="dt">size =</span> N, <span class="dt">replace=</span><span class="ot">TRUE</span>, <span class="dt">prob=</span>prob)</a>
<a class="sourceLine" id="cb909-7" data-line-number="7">Y =<span class="st"> </span><span class="kw">f</span>(X)</a>
<a class="sourceLine" id="cb909-8" data-line-number="8">D =<span class="st"> </span><span class="kw">rbind</span>(X[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>], Y[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]) <span class="co"># show only the 10 values</span></a>
<a class="sourceLine" id="cb909-9" data-line-number="9">Df =<span class="st"> </span><span class="kw">data.frame</span>(D)</a>
<a class="sourceLine" id="cb909-10" data-line-number="10"><span class="kw">rownames</span>(Df) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;original (x)&quot;</span>, <span class="st">&quot;transformed f(x)&quot;</span>)</a>
<a class="sourceLine" id="cb909-11" data-line-number="11">Df</a></code></pre></div>
<pre><code>##                  X1 X2 X3 X4 X5 X6 X7 X8 X9 X10
## original (x)      3  3  3  3  5  5  5  3  5   3
## transformed f(x)  9  9  9  9 15 15 15  9 15   9</code></pre>
<div class="sourceCode" id="cb911"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb911-1" data-line-number="1"><span class="kw">hist</span>(Y,   <span class="dt">breaks =</span> <span class="dv">5</span>, <span class="dt">las=</span><span class="dv">1</span>, <span class="dt">xlab=</span><span class="st">&quot;f(X)&quot;</span>,</a>
<a class="sourceLine" id="cb911-2" data-line-number="2">      <span class="dt">cex.main =</span> <span class="fl">1.55</span>, <span class="dt">cex.lab =</span> <span class="dv">1</span>, <span class="dt">freq =</span> <span class="ot">TRUE</span>,</a>
<a class="sourceLine" id="cb911-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Monte Carlo Estimation&quot;</span>,</a>
<a class="sourceLine" id="cb911-4" data-line-number="4">           <span class="dt">border=</span><span class="st">&quot;black&quot;</span>,  <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb911-5" data-line-number="5"><span class="kw">axis</span>(<span class="dv">1</span>, <span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">15</span>,<span class="dv">1</span>) )</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mcestimation"></span>
<img src="DS_files/figure-html/mcestimation-1.png" alt="Monte Carlo Estimation" width="70%" />
<p class="caption">
Figure 8.25: Monte Carlo Estimation
</p>
</div>

<p>Note that <span class="math inline">\(f(X)\)</span> generates a new distribution different from the distribution of <strong>X</strong>.</p>
<p>Now, suppose we know the probability distribution of our function:</p>
<p><span class="math display">\[
f(x) = (3,6,9,12,15)\ \ maps\ to\ \ \ \ P\left[f(x)\right] = \left( 0.10, 0.20, 0.30, 0.05, 0.35\right).
\]</span></p>
<p>In that case, we can calculate the expected value based on the known probabilities of all possible outcomes (of the function):</p>
<p><span class="math display" id="eq:equate1100150">\[\begin{align}
I_q {}&amp;= f(x = 1)P\left[f(x=1)\right] + 
f(x = 2)P\left[f(x=2)\right] +
f(x = 3)P\left[f(x=3)\right] + \nonumber \\
&amp;f(x = 4)P\left[f(x=4)\right] + 
f(x = 5)P\left[f(x=5)\right] \tag{8.160} \\
&amp;= 3 \times 0.10 + 6 \times 0.20 + 9 \times 0.30 + 12 \times 0.05 + 15 \times 0.35 \nonumber \\
&amp;= 10.05 \nonumber
\end{align}\]</span></p>
<p>Otherwise, if the probability distribution of the function is not known, then we use the probability distribution of the original values like so (here, we use uniform probabilities):</p>
<p><span class="math display">\[
X = (1,2,3,4,5)\ \ maps\ to\ \ \ \ P\left(X\right) = \left( 
\frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5}, \frac{1}{5}
\right).
\]</span></p>
<p>so that, we then get the following expected value:</p>
<p><span class="math display" id="eq:equate1100151">\[\begin{align}
I_q {}&amp;= f(x = 1)P\left(x = 1\right) + 
f(x = 2)P\left(x = 2\right) +
f(x = 3)P\left(x = 3\right) + \nonumber \\
&amp;f(x = 4)P\left(x = 4\right) + 
f(x = 5)P\left(x = 5\right) \tag{8.161} \\
&amp;= 3 \times 0.20 + 6 \times 0.20 + 9 \times 0.20  + 12 \times 0.20  + 15 \times 0.20 \nonumber \\
&amp;= 9.0 \nonumber
\end{align}\]</span></p>
<p>Below is a simple implementation of <strong>Monte Carlo estimation</strong> in R code:</p>

<div class="sourceCode" id="cb912"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb912-1" data-line-number="1">Pf =<span class="st"> </span>prob <span class="co"># known probabilities of function</span></a>
<a class="sourceLine" id="cb912-2" data-line-number="2">E =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb912-3" data-line-number="3">n =<span class="st"> </span><span class="kw">length</span>(possible.outcome)</a>
<a class="sourceLine" id="cb912-4" data-line-number="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb912-5" data-line-number="5">  E =<span class="st"> </span>E <span class="op">+</span><span class="st">  </span><span class="kw">f</span>(possible.outcome[i]) <span class="op">*</span><span class="st"> </span>Pf[possible.outcome[i]]</a>
<a class="sourceLine" id="cb912-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb912-7" data-line-number="7"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Expected Value = &quot;</span>, <span class="kw">round</span>(E,<span class="dv">5</span>)))</a></code></pre></div>
<pre><code>## [1] &quot;Expected Value = 10.05&quot;</code></pre>

<p>We further discuss using <strong>Monte Carlo estimation</strong> in the <strong>Importance sampling</strong> section.</p>
</div>
<div id="monte-carlo-simulation" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.2</span> Monte Carlo Simulation <a href="8.2-simulation-and-sampling.html#monte-carlo-simulation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Monte Carlo simulation</strong>, also called <strong>Monte Carlo method</strong> and <strong>Monte Carlo sampling</strong>, offers the ability to draw random samples from a posterior distribution by simulating probabilities. The objective is to measure risk and uncertainty.</p>
<p>To demonstrate <strong>Monte Carlo</strong>, we can use an R function called <strong>sample(.)</strong> to simulate multiple random tosses of a coin:</p>

<div class="sourceCode" id="cb914"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb914-1" data-line-number="1">data =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;H&quot;</span>, <span class="st">&quot;T&quot;</span>)</a>
<a class="sourceLine" id="cb914-2" data-line-number="2"><span class="kw">sample</span>(data, <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##  [1] &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot;</code></pre>

<p>Another popular function used in R to implement <strong>MC</strong> is the <strong>rnorm(.)</strong> which simulates generating random numbers, given a mean <span class="math inline">\(\mu = 0\)</span> and variance <span class="math inline">\(\sigma^2=1\)</span>, like so:</p>

<div class="sourceCode" id="cb916"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb916-1" data-line-number="1">mu =<span class="st"> </span><span class="dv">0</span>;  sigma =<span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb916-2" data-line-number="2"><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">5</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sigma)</a></code></pre></div>
<pre><code>## [1] -0.8781 -0.4207 -2.6625 -1.6323  1.0070</code></pre>

<p>A simple example of <strong>MC</strong> is a simulated random walk using the function <strong>rbinom(.)</strong>. See Figure <a href="8.2-simulation-and-sampling.html#fig:randomwalk">8.26</a>):</p>

<div class="sourceCode" id="cb918"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb918-1" data-line-number="1">random.walk &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">no_of_walks =</span> <span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb918-2" data-line-number="2">  <span class="kw">set.seed</span>(<span class="dv">2020</span> )</a>
<a class="sourceLine" id="cb918-3" data-line-number="3">  direction =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;north&quot;</span>, <span class="st">&quot;south&quot;</span>, <span class="st">&quot;east&quot;</span>, <span class="st">&quot;west&quot;</span>)</a>
<a class="sourceLine" id="cb918-4" data-line-number="4">  walk =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, no_of_walks)</a>
<a class="sourceLine" id="cb918-5" data-line-number="5">  rnd =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> no_of_walks, <span class="dt">size =</span> <span class="dv">3</span>, <span class="dt">prob=</span><span class="fl">0.50</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb918-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>no_of_walks) {</a>
<a class="sourceLine" id="cb918-7" data-line-number="7">    walk[i] =<span class="st"> </span>direction[rnd[i]]</a>
<a class="sourceLine" id="cb918-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb918-9" data-line-number="9">  walk</a>
<a class="sourceLine" id="cb918-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb918-11" data-line-number="11">walk =<span class="st"> </span><span class="kw">random.walk</span>(<span class="dv">100</span>)</a></code></pre></div>


<div class="figure" style="text-align: center"><span style="display:block;" id="fig:randomwalk"></span>
<img src="DS_files/figure-html/randomwalk-1.png" alt="Random Walk (Monte Carlo)" width="70%" />
<p class="caption">
Figure 8.26: Random Walk (Monte Carlo)
</p>
</div>

<p>A more complex example of using <strong>Monte Carlo</strong> is studying autonomous vehicles, e.g., self-driving cars, especially around tracking multiple targets (Kim, D., Hong, S. 2013). One property of an autonomous moving vehicle is to detect surrounding objects and thus be able to avoid a collision ultimately. Therefore, the vehicle needs to monitor and measure the distance and velocity between the moving vehicle and multiple targets. The problem statement is to distinguish and map detected measurements with corresponding targets.</p>
<p>There are six independent measurements generated from six sensors attached to an autonomous vehicle, namely Infrared Sensor, Ultrasonic Sensor, VideoCam, GPS, Lidar Sensor using light waves, and Radar Sensor using radio waves. Such sensor measurements are fed into the central computer for analysis.</p>
<p>In particular, let us focus on linear Doppler shifts of light waves detected from an FMCW Lidar sensor. See Figure <a href="8.2-simulation-and-sampling.html#fig:fmcw">8.27</a> showing two frequencies, namely a transmitted frequency from a moving vehicle and a reflected frequency from a stationary object straight up ahead.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fmcw"></span>
<img src="fmcw.png" alt="Triangular Chirp Modulation for an FMCW system" width="70%" />
<p class="caption">
Figure 8.27: Triangular Chirp Modulation for an FMCW system
</p>
</div>

<p>The diagram demonstrates a triangular chirp (or beat) modulation for the two frequencies allowing a calculation of the distance and velocity of a moving source to a stationary target. To a moving object (being the source) and a stationary observer (being the target), classic radar sensors detect changes in wave frequency called Doppler shifts (denoted <span class="math inline">\(\mathbf{f_D}\)</span>). As for light-based sensors, Doppler frequency has the following equation:</p>
<p><span class="math display" id="eq:equate1100152">\[\begin{align}
f_D = \frac{2V_r f_o}{C}  \tag{8.162} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{V_r}\)</span> is relative Velocity,</li>
<li><span class="math inline">\(\mathbf{f_o}\)</span> is operating frequency of transmitted signal,</li>
<li><strong>C</strong> is a constant for speed of light, e.g. <span class="math inline">\(3\times 10^8\)</span> m/s.</li>
</ul>
<p>For example, suppose a moving vehicle has a speed of 30 m/s with a lidar signal transmitted at a frequency of 77 GHz. Therefore, the Doppler frequency is calculated to be:</p>
<p><span class="math display">\[
f_D = \frac{2 (30) (77\times 10^9)} {(3\times 10^8)} = 15.4\ \text{kHz}
\]</span></p>
<p>Now, suppose our goal is to prevent a collision by allowing a moving vehicle to autonomously slow down to a stop at a safe stopping distance from multiple stationary targets detected straight ahead. For that, we need to measure the distance and velocity of the moving object using the following series of equations and their derivations based on Figure <a href="8.2-simulation-and-sampling.html#fig:fmcw">8.27</a>:</p>
<p><span class="math display" id="eq:eqnnumber334">\[\begin{align}
\begin{array}{lllll}
\tau &amp;= \frac{2R}{C}&amp;\ \ \ \ &amp;f_b &amp;= (\tau)\frac{B}{T_s} (slope = \frac{rise}{run})\\
f_{bu} &amp;= f_b - f_D&amp;\ \ \ \ &amp;f_{bd} &amp;= f_b + f_D\\
R &amp;= \frac{CT_s}{2B} \times\frac{(f_{bd} + f_{bu})}{2}&amp;\ \ \ \ 
&amp;V_r &amp;= \frac{C}{2 f_o}\times\frac{(f_{bd} - f_{bu})}{2}
\end{array} \tag{8.163}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>C</strong> is a constant for speed of light, e.g. <span class="math inline">\(3\times 10^8\)</span> m/s,</li>
<li><strong>B</strong> is chirp (triangular modulation) bandwidth,</li>
<li><span class="math inline">\(\mathbf{T_s}\)</span> is one chirp sweep period (wave period),</li>
<li><span class="math inline">\(\tau\)</span> is time delay,</li>
<li><span class="math inline">\(\mathbf{f_o}\)</span> is operating frequency of transmitted signal,</li>
<li><span class="math inline">\(\mathbf{f_b}\)</span> is frequency of beat (chirp rate),</li>
<li><span class="math inline">\(\mathbf{f_{bu}}\)</span> is frequency of beat (up=ramp rate of chirp),</li>
<li><span class="math inline">\(\mathbf{f_{bd}}\)</span> is frequency of beat (down-ramp rate of chirp),</li>
<li><span class="math inline">\(\mathbf{f_D}\)</span> is Doppler frequency shift,</li>
<li><strong>R</strong> is range (distance) from source to target,</li>
<li><span class="math inline">\(\mathbf{V_r}\)</span> is velocity of moving source.</li>
</ul>
<p>Note that measurements of Doppler shifts may come with distortion, leakage, biases, and ambiguity. Thus, with such uncertainty, our goal is to improve the accuracy of the sensors by calibration. One approach is to simulate sampling of multiple measurements across a wide range of targets positioned in different locations and distances and possibly inducing a wide range of distortions, leakage, biases, and ambiguity. That is where <strong>Monte Carlo</strong> comes into play.</p>
<p>While we may not illustrate the case in this book, we leave readers to investigate this simulation.</p>
<p>Notice that our discussion of <strong>Monte Carlo</strong> illustrates <strong>random sampling</strong>; however, in <strong>bayesian sense</strong>, we draw random samples from a posterior distribution. In the case of a <strong>Normal distribution</strong>, we use its <strong>sufficient statistics</strong>, namely <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>, to help generate the random sample.</p>
</div>
<div id="markov-chain-monte-carlo" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.3</span> Markov Chain Monte Carlo  <a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Markov Chain Monte Carlo (MCMC)</strong> is a <strong>Monte Carlo</strong> technique that draws random sampling in which each drawn sample is conditionally dependent upon the recent previous drawn sample. This dependency property is one of <strong>Markov Chain</strong> properties <span class="citation">(suppl refs: Robert C., George Casella G. <a href="bibliography.html#ref-ref1002c">2008</a>; Speagle J. S. <a href="bibliography.html#ref-ref993j">2020</a>)</span>.</p>
<p>Unlike the previous random walk example, let us modify our random walk to illustrate <strong>MCMC</strong>. Here, let us introduce a table of transition probabilities so that if our next random step makes a transition to a greater probability than that of our recent previous step, then we take that step; otherwise, we use the direction of the previous step for our next step. Here, we only use three movements (F = walk forward, L = walk left, R = walk right). See Figure <a href="8.2-simulation-and-sampling.html#fig:mcmc">8.28</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mcmc"></span>
<img src="mcmc.png" alt="MCMC Model" width="70%" />
<p class="caption">
Figure 8.28: MCMC Model
</p>
</div>

<p>Here is a sample R code for the <strong>MCMC</strong> random walk:</p>

<div class="sourceCode" id="cb919"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb919-1" data-line-number="1">random.walk &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">no_of_walks =</span> <span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb919-2" data-line-number="2">  <span class="kw">set.seed</span>(<span class="dv">123</span>)</a>
<a class="sourceLine" id="cb919-3" data-line-number="3">  transition =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="fl">0.10</span>, <span class="fl">0.45</span>, <span class="fl">0.45</span>,</a>
<a class="sourceLine" id="cb919-4" data-line-number="4">                        <span class="fl">0.45</span>, <span class="fl">0.10</span>, <span class="fl">0.45</span>, </a>
<a class="sourceLine" id="cb919-5" data-line-number="5">                        <span class="fl">0.45</span>, <span class="fl">0.45</span>, <span class="fl">0.10</span>), <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb919-6" data-line-number="6">  direction =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;F&quot;</span>, <span class="st">&quot;L&quot;</span>, <span class="st">&quot;R&quot;</span>)</a>
<a class="sourceLine" id="cb919-7" data-line-number="7">  walk =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, no_of_walks)</a>
<a class="sourceLine" id="cb919-8" data-line-number="8">  seq.of.steps  =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> no_of_walks, <span class="dt">size =</span> <span class="dv">2</span>, <span class="dt">prob=</span><span class="fl">0.50</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb919-9" data-line-number="9">  prev.prob =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb919-10" data-line-number="10">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>no_of_walks) {</a>
<a class="sourceLine" id="cb919-11" data-line-number="11">    step =<span class="st"> </span>direction[seq.of.steps [i]]</a>
<a class="sourceLine" id="cb919-12" data-line-number="12">    <span class="cf">if</span> (i <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb919-13" data-line-number="13">      previous.step =<span class="st"> </span>walk[i<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb919-14" data-line-number="14">      previous.idx =<span class="st"> </span><span class="kw">which</span>(direction<span class="op">==</span>previous.step)</a>
<a class="sourceLine" id="cb919-15" data-line-number="15">      probability =<span class="st"> </span>transition[previous.idx, seq.of.steps[i]]</a>
<a class="sourceLine" id="cb919-16" data-line-number="16">      <span class="cf">if</span> (previous.step <span class="op">!=</span><span class="st"> </span>step) {</a>
<a class="sourceLine" id="cb919-17" data-line-number="17">        <span class="cf">if</span> (prev.prob <span class="op">&lt;=</span><span class="st"> </span>probability) {</a>
<a class="sourceLine" id="cb919-18" data-line-number="18">          prev.prob =<span class="st"> </span>probability</a>
<a class="sourceLine" id="cb919-19" data-line-number="19">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb919-20" data-line-number="20">          step =<span class="st"> </span>previous.step</a>
<a class="sourceLine" id="cb919-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb919-22" data-line-number="22">      }</a>
<a class="sourceLine" id="cb919-23" data-line-number="23">    } </a>
<a class="sourceLine" id="cb919-24" data-line-number="24">    walk[i] =<span class="st"> </span>step</a>
<a class="sourceLine" id="cb919-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb919-26" data-line-number="26">  walk</a>
<a class="sourceLine" id="cb919-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb919-28" data-line-number="28">walk =<span class="st"> </span><span class="kw">random.walk</span>(<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb919-29" data-line-number="29"><span class="kw">head</span>(walk)</a></code></pre></div>
<pre><code>## [1] &quot;L&quot; &quot;R&quot; &quot;L&quot; &quot;R&quot; &quot;R&quot; &quot;F&quot;</code></pre>

<p>Figure <a href="8.2-simulation-and-sampling.html#fig:randomwalk1">8.29</a> shows the revised random walk.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:randomwalk1"></span>
<img src="DS_files/figure-html/randomwalk1-1.png" alt="Random Walk (MCMC)" width="70%" />
<p class="caption">
Figure 8.29: Random Walk (MCMC)
</p>
</div>

</div>
<div id="metropolis-hastings-monte-carlo" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.4</span> Metropolis-Hastings Monte Carlo  <a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Metropolis-Hastings Monte Carlo (MHMC)</strong> is an <strong>MCMC</strong> technique so that given an approximation (proposal) distribution, namely <span class="math inline">\(\mathcal{Q}(x^*|x)\)</span>, the goal is to refine the distribution until such that it settles into a target (equilibrium) distribution, namely <span class="math inline">\(\pi(x)\)</span>. While <strong>MHMC</strong> may best suit distributions that do not rather fall under the common list of distributions, let us use <strong>Gaussian</strong> distribution as a case in our discussion for simplicity and illustration, but only up to the constant (meaning, we ignore the normalizer). This section includes supplemental references: <span class="citation">(C.P. Robert <a href="bibliography.html#ref-ref523c">2016</a>; Burke N. <a href="bibliography.html#ref-ref1011n">2018</a>)</span>.</p>
<p><span class="math display" id="eq:equate1100153">\[\begin{align}
\mathcal{Q}(x^*|x)  
 \propto \left\{ \pi(x; \mu, \sigma^2)= exp \left(-\frac{(x - \mu)^2}{2 \sigma^2}\right) \right\} \tag{8.164} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(x^*\)</span> is a proposed candidate data point.</p>
<p><strong>MHMC</strong> has the following algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
\text{Initialize the first data point, e.g. }x_0 = \text{&lt;an arbitrary value&gt;}\\
\text{For i in 1,...,n,    repeat the following:}\\
\ \ \ \ \ \ \ \text{Propose a candidate by drawing a sample from }\mathcal{Q}(x^*|x)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
x^* \sim \mathcal{Q}(x^*|x_{i-1})\\
\ \ \ \ \ \ \ \text{Calculate the Acceptance Probability (A)}:\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\text{A = min}\left(\frac{\mathcal{Q}(x_{i-1}|x^*)\pi(x^*)}{\mathcal{Q}(x^*|x_{i-1})\pi(x_{i-1})}, 1\right)}_\text{asymmetric}
\ \ \ \ \ \ or \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\text{A = min}\left(\frac{\pi(x^*)}{\pi(x_{i-1})}, 1\right)}_\text{symmetric}\\
\ \ \ \ \ \ \ \text{Draw a random number u and campare with A}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ u \sim \text{Uniform}(0,1)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \text{If u &lt; A, we accept the proposal}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
x_i = x^*\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\text{Otherwise, we reject:}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
x_i = x_{i-1}\\
\text{return x  where  x = }(x_1, x_2,...)
\end{array}
\]</span></p>
<p>Note that <span class="math inline">\(\frac{\mathcal{Q}(x_{i-1}|x^*)}{\mathcal{Q}(x^*|x_{i-1})}\)</span> is called the <strong>Hasting ratio</strong>. If the <strong>ratio</strong> is 1, it means that the distribution is symmetric; thus, we can use the <strong>symmetric</strong> version of the <strong>acceptance probability</strong> equation.</p>
<p>The <strong>proposed candidate</strong> uses the previous sample value with added noise (perturbation) so that, in our case, we have the following:</p>
<p><span class="math display" id="eq:equate1100154">\[\begin{align}
x^* = x_{i-1} + \mathcal{N}(0, 1) \tag{8.165} 
\end{align}\]</span></p>
<p>Or in the case of simulating a random walk for <strong>MHMC</strong>, we also can use:</p>
<p><span class="math display" id="eq:equate1100155">\[\begin{align}
x^* = x_{i-1} + \mathcal{U}(x_{i-1} - 1, x_{i-1} + 1) \tag{8.166} 
\end{align}\]</span></p>
<p>Below is a simple example of <strong>MHMC</strong> implementation in R code (see Figure <a href="8.2-simulation-and-sampling.html#fig:mhmc">8.30</a>).</p>

<div class="sourceCode" id="cb921"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb921-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb921-2" data-line-number="2">kernel &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="co"># Gaussian Kernel</span></a>
<a class="sourceLine" id="cb921-3" data-line-number="3">  mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb921-4" data-line-number="4">  <span class="kw">exp</span>(<span class="op">-</span>(x <span class="op">-</span><span class="st"> </span>mu)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sd))</a>
<a class="sourceLine" id="cb921-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb921-6" data-line-number="6">pi.func &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">kernel</span>(x) }</a>
<a class="sourceLine" id="cb921-7" data-line-number="7">metropolis.hasting &lt;-<span class="st"> </span><span class="cf">function</span>(n) { <span class="co"># symmetric</span></a>
<a class="sourceLine" id="cb921-8" data-line-number="8">  x =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>,n)</a>
<a class="sourceLine" id="cb921-9" data-line-number="9">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb921-10" data-line-number="10">      previous.x =<span class="st"> </span>x[i<span class="dv">-1</span>]</a>
<a class="sourceLine" id="cb921-11" data-line-number="11">      proposed.x =<span class="st">  </span>previous.x <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb921-12" data-line-number="12">      A =<span class="st"> </span><span class="kw">min</span>( <span class="kw">pi.func</span>(proposed.x)<span class="op">/</span><span class="kw">pi.func</span>(previous.x), <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb921-13" data-line-number="13">      u =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb921-14" data-line-number="14">      <span class="cf">if</span> ( u <span class="op">&gt;</span><span class="st"> </span>A) {</a>
<a class="sourceLine" id="cb921-15" data-line-number="15">        proposed.x =<span class="st"> </span>previous.x</a>
<a class="sourceLine" id="cb921-16" data-line-number="16">      }</a>
<a class="sourceLine" id="cb921-17" data-line-number="17">      x[i] =<span class="st"> </span>proposed.x</a>
<a class="sourceLine" id="cb921-18" data-line-number="18">  }</a>
<a class="sourceLine" id="cb921-19" data-line-number="19">  x</a>
<a class="sourceLine" id="cb921-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb921-21" data-line-number="21">plot.metropolis &lt;-<span class="st"> </span><span class="cf">function</span>(n) {</a>
<a class="sourceLine" id="cb921-22" data-line-number="22">  <span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">0.8</span>), </a>
<a class="sourceLine" id="cb921-23" data-line-number="23">     <span class="dt">xlab=</span><span class="st">&quot;x&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;density&quot;</span>,</a>
<a class="sourceLine" id="cb921-24" data-line-number="24">     <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;MHCM (n=&quot;</span>, n, <span class="st">&quot;)&quot;</span>),  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb921-25" data-line-number="25">  <span class="kw">grid</span>()</a>
<a class="sourceLine" id="cb921-26" data-line-number="26">  x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb921-27" data-line-number="27">  <span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb921-28" data-line-number="28">  pt =<span class="st"> </span><span class="kw">density</span>(<span class="kw">metropolis.hasting</span>(n))</a>
<a class="sourceLine" id="cb921-29" data-line-number="29">  <span class="kw">lines</span>(pt<span class="op">$</span>x, pt<span class="op">$</span>y, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb921-30" data-line-number="30">}</a>
<a class="sourceLine" id="cb921-31" data-line-number="31"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb921-32" data-line-number="32"><span class="kw">plot.metropolis</span>(<span class="dt">n=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb921-33" data-line-number="33"><span class="kw">plot.metropolis</span>(<span class="dt">n=</span><span class="dv">50</span>)</a>
<a class="sourceLine" id="cb921-34" data-line-number="34"><span class="kw">plot.metropolis</span>(<span class="dt">n=</span><span class="dv">500</span>)</a>
<a class="sourceLine" id="cb921-35" data-line-number="35"><span class="kw">plot.metropolis</span>(<span class="dt">n=</span><span class="dv">10000</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mhmc"></span>
<img src="DS_files/figure-html/mhmc-1.png" alt="Metropolis-Hastings MC (MHMC)" width="100%" />
<p class="caption">
Figure 8.30: Metropolis-Hastings MC (MHMC)
</p>
</div>

</div>
<div id="hamiltonian-monte-carlo" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.5</span> Hamiltonian Monte Carlo  <a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Hamiltonian Monte Carlo (HMC)</strong> algorithm, also called <strong>Hybrid Monte Carlo</strong>, extends the concept of the <strong>Metropolis-Hastings</strong> algorithm <span class="citation">(Tianqi Chen et al. <a href="bibliography.html#ref-ref542t">2014</a>)</span>.</p>
<p>We start with the simple <strong>Monte Carlo</strong> concept of operating on a posterior distribution, given by the notation <span class="math inline">\(\mathcal{Q}(q) \propto P(q|D)\)</span> where <strong>q</strong> is a <strong>latent variable</strong> for which we simulate drawing samples from <strong>D</strong> observations. We then introduce an <strong>auxiliary variable</strong>, namely <strong>p</strong>, so that we form a joint distribution like so:</p>
<p><span class="math display" id="eq:equate1100156">\[\begin{align}
\mathcal{Q}(q, p) = \mathcal{Q}(q|p)\mathcal{Q}(p)\ \ \ \ \ \leftarrow \text{(chain rule)} \tag{8.167} 
\end{align}\]</span></p>
<p>Here, <strong>HMC</strong> uses the Hamiltonian dynamics in physics to describe the conservation of energy so that our joint distribution adapts to the <strong>Hamiltonian</strong> equation below:</p>
<p><span class="math display" id="eq:equate1100157">\[\begin{align}
\mathcal{Q}(q, p) \propto exp^{-H(q,p)}  \tag{8.168} 
\end{align}\]</span></p>
<p>The idea is to decompose the equation above into <strong>kinetic (kinematic) energy</strong> and <strong>potential energy</strong> functions so that we then have the following:</p>
<p><span class="math display" id="eq:equate1100159" id="eq:equate1100158">\[\begin{align}
\mathcal{H}(q, p) {}&amp;= - \log_e \mathcal{Q}(q,p)  \tag{8.169} \\
&amp;= \underbrace{ \underbrace{-\log_e \mathcal{Q}(p|q)}_\text{Kinetic Energy}}_\text{K(p)} - 
   \underbrace{ \underbrace{\log_e \mathcal{Q}(q)}_\text{Potential Energy}}_\text{U(q)} \tag{8.170} 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{H(q,p)}\)</span> is the total energy of the system,</li>
<li><span class="math inline">\(\mathbf{U(q)}\)</span> is the potential energy (PE) function,</li>
<li><span class="math inline">\(\mathbf{K(p)}\)</span> is the kinetic energy (KE) function,</li>
<li><strong>q</strong> is the position state, described in terms of d-dimensional vector,</li>
<li><strong>p</strong> is the momentum state, described in terms of d-dimensional vector.</li>
</ul>
<p>The dynamics of particles in motion are described by a <strong>Hamiltonian system</strong> in which particles flow through a landscape of high-dimensionality, governed by forces of kinetic and potential energies. Such particles are sampled and approximated in terms of their position and momentum, for which the potential energy depends only on position (<strong>q</strong>) and the kinetic energy depends only on the momentum (<strong>p</strong>), expressed mathematically as so:</p>
<p><span class="math display" id="eq:equate1100160">\[\begin{align}
\underbrace{ U(q) = \frac{q^2}{2} \ \ \ \ \ \ \ \ \ \  K(p) = \frac{p^2}{2}}_\text{univariate}
\ \ \ or \ \ \ \ \
\underbrace{ U(q) = \frac{q\Sigma^{-1}q}{2} \ \ \ \ \ \ \ \ \ \  K(p) = \frac{p \cdot p}{2}}_\text{multivariate} \tag{8.171} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\Sigma^{-1}\)</span> is a covariance matrix for a gaussian distribution - assuming some rough perturbation.</p>
<p>The Hamiltonian functions are implemented as such:</p>

<div class="sourceCode" id="cb922"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb922-1" data-line-number="1">U &lt;-<span class="st"> </span><span class="cf">function</span>(q) {</a>
<a class="sourceLine" id="cb922-2" data-line-number="2">  d =<span class="st"> </span><span class="dv">2</span> <span class="co"># d-dimension</span></a>
<a class="sourceLine" id="cb922-3" data-line-number="3">  Sigma =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="fl">0.8</span>, <span class="fl">0.8</span>, <span class="dv">1</span>), <span class="dt">nrow=</span>d, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb922-4" data-line-number="4">  <span class="kw">sum</span>( (q <span class="op">%*%</span><span class="st"> </span>Sigma<span class="op">^-</span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>q ) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb922-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb922-6" data-line-number="6">K &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</a>
<a class="sourceLine" id="cb922-7" data-line-number="7">  <span class="kw">sum</span>( p <span class="op">*</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb922-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb922-9" data-line-number="9">H &lt;-<span class="st"> </span><span class="cf">function</span>(q,p) {  <span class="kw">U</span>(q) <span class="op">+</span><span class="st"> </span><span class="kw">K</span>(p) }</a>
<a class="sourceLine" id="cb922-10" data-line-number="10">MH.correction &lt;-<span class="st"> </span><span class="cf">function</span>(q, p, q.new, p.new) {</a>
<a class="sourceLine" id="cb922-11" data-line-number="11">  <span class="kw">exp</span>(<span class="kw">H</span>(q, p) <span class="op">-</span><span class="st"> </span><span class="kw">H</span>(q.new, p.new))</a>
<a class="sourceLine" id="cb922-12" data-line-number="12">}</a></code></pre></div>

<p>Changes in position and momentum that occur over time are described in terms of the following <strong>Hamiltonian equations</strong>:</p>
<p><span class="math display" id="eq:equate1100161">\[\begin{align}
\frac{dq}{dt} = +\frac{\partial H}{\partial p} = +\frac{\partial K}{\partial p}
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\frac{dp}{dt} = - \frac{\partial H}{\partial q} = -\frac{\partial K}{\partial q} - \frac{\partial U}{\partial q}  \tag{8.172} 
\end{align}\]</span></p>
<p>The idea is to calculate the gradient of the Hamiltonian function - the direction towards the next state (next position and momentum):</p>
<p><span class="math display" id="eq:equate1100162">\[\begin{align}
(q_{t+1}, p_{t+1}) = (q_t, p_t) + \epsilon \times \nabla \mathcal{H}(q_t, p_t) \tag{8.173} 
\end{align}\]</span></p>
<p>Alternatively in <strong>HMC</strong>, we use the <strong>LeapFrog algorithm</strong> as outlined below <span class="citation">(Wei-Lun Chao <a href="bibliography.html#ref-ref657w">2015</a>; Mohammed Alfaki M. <a href="bibliography.html#ref-ref1020m">2008</a>)</span>:</p>
<p><span class="math display">\[
\begin{array}{l}
p = p - \frac{\epsilon}{2}\frac{\partial U}{\partial q}(q)\\
\text{for i in 1,...,L},\ \ \ \text{repeat the following}:\\
\ \ \ \ \ \ \ \ \ \  \begin{array}{l}
q = q + \epsilon M^{-1}  p\\
if\ (i\ne L)\ p = p- \frac{\epsilon}{2}\frac{\partial U}{\partial q}(q)\\
\end{array}\\
p = p - \frac{\epsilon}{2}\frac{\partial U}{\partial q}(q)\\
(q^*, p^*) = (q^{(L)}, p^{(L)})
\end{array}
\]</span></p>
<p>where <span class="math inline">\(M^{-1}\)</span> serves as a mass matrix (usually a covariance/identity matrix) and epsilon <span class="math inline">\(\epsilon\)</span> is a time interval (stepsize).</p>
<p>Because time is continuous, we have to discretize time to simulate <strong>Hamiltonian dynamics</strong> using a time interval value as stepsize denoted by <span class="math inline">\(\epsilon\)</span>.</p>
<p>The <strong>LeapFrog</strong> algorithm is implemented in R code like so: </p>

<div class="sourceCode" id="cb923"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb923-1" data-line-number="1"><span class="kw">library</span>(pracma)</a>
<a class="sourceLine" id="cb923-2" data-line-number="2">U.gradient &lt;-<span class="st">  </span>jacobian</a>
<a class="sourceLine" id="cb923-3" data-line-number="3">leap.frog &lt;-<span class="st"> </span><span class="cf">function</span>(q, p, <span class="dt">e =</span> <span class="fl">0.10</span>, <span class="dt">L=</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb923-4" data-line-number="4">  p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>e<span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">U.gradient</span>(U,q)</a>
<a class="sourceLine" id="cb923-5" data-line-number="5">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>L) {</a>
<a class="sourceLine" id="cb923-6" data-line-number="6">    q =<span class="st"> </span>q <span class="op">+</span><span class="st"> </span>e <span class="op">*</span><span class="st"> </span>p</a>
<a class="sourceLine" id="cb923-7" data-line-number="7">    <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>L) { p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>e<span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">U.gradient</span>(U,q) }</a>
<a class="sourceLine" id="cb923-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb923-9" data-line-number="9">  p =<span class="st"> </span>p <span class="op">-</span><span class="st"> </span>e<span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">U.gradient</span>(U,q)</a>
<a class="sourceLine" id="cb923-10" data-line-number="10">  <span class="kw">list</span>(<span class="st">&quot;q&quot;</span> =<span class="st"> </span>q, <span class="st">&quot;p&quot;</span> =<span class="st"> </span>p)</a>
<a class="sourceLine" id="cb923-11" data-line-number="11">}</a></code></pre></div>

<p>With all that, <strong>HMC</strong> has the following algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
\text{Initialize the position state } q_0 \text{ drawn from } q \sim \mathcal{N}(0, \Sigma^{-1})\\
\text{For t in 1,2,...  repeat the following:}\\
\ \ \ \ \ \ \ \text{Propose a candidate momentum by drawing a sample from }\\
\ \ \ \ \ \ \ \ \ \ \ \ \mathcal{Q}(p_{t-1}) \text{ - the canonical distribution}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
p^* \sim \mathcal{N}(0, M) \leftarrow \mathcal{Q}(p_{t-1}) \\
\ \ \ \ \ \ \ \text{Perform LeapFrog Algorithm}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ (q^*, p^*) = LeapFrog(q_{t-1}, p_{t-1}, \epsilon, L=5)\\
\ \ \ \ \ \ \ \text{Calculate the Acceptance Probability (A) - (}\mathbf{\text{MH}} \text{ correction):}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\text{A = min}\left(exp\left[H(q_{t-1},p_{t-1}) - H(q^*, p^*)\right], 1\right)
\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ u \sim \text{Uniform}(0,1)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \text{If u &lt; A, we accept the proposal}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
q_t = q^*\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\text{Otherwise, we reject:}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
q_t = q_{t-1}\\
\text{return q  where  q = }(q_1, q_2,...)
\end{array}
\]</span></p>
<p>Here is an example implementation of <strong>HMC</strong> in R code for a bivariate normal case (motivated by a python implementation by Gergely-Flamich):</p>



<div class="sourceCode" id="cb924"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb924-1" data-line-number="1"><span class="kw">library</span>(MASS)</a>
<a class="sourceLine" id="cb924-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb924-3" data-line-number="3">HMC &lt;-<span class="st"> </span><span class="cf">function</span>(<span class="dt">N =</span> <span class="dv">300</span>, <span class="dt">L =</span> <span class="dv">10</span>, <span class="dt">epsilon =</span> <span class="fl">0.10</span>) {</a>
<a class="sourceLine" id="cb924-4" data-line-number="4">  d =<span class="st"> </span><span class="dv">2</span> <span class="co"># Dimension</span></a>
<a class="sourceLine" id="cb924-5" data-line-number="5">  current.q =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, d) </a>
<a class="sourceLine" id="cb924-6" data-line-number="6">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow =</span> N, <span class="dt">ncol=</span>d, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb924-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb924-8" data-line-number="8">    current.p =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> d, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb924-9" data-line-number="9">    state =<span class="st"> </span><span class="kw">leap.frog</span>(current.q, current.p , <span class="dt">e =</span> epsilon, <span class="dt">L =</span> L)</a>
<a class="sourceLine" id="cb924-10" data-line-number="10">    proposed.q =<span class="st"> </span>state<span class="op">$</span>q</a>
<a class="sourceLine" id="cb924-11" data-line-number="11">    proposed.p =<span class="st"> </span>state<span class="op">$</span>p</a>
<a class="sourceLine" id="cb924-12" data-line-number="12">    A =<span class="st"> </span><span class="kw">min</span>( <span class="kw">MH.correction</span>(current.q, current.p, </a>
<a class="sourceLine" id="cb924-13" data-line-number="13">                           proposed.q, proposed.p), <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb924-14" data-line-number="14">    u =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb924-15" data-line-number="15">    <span class="cf">if</span> ( u <span class="op">&gt;</span><span class="st"> </span>A) {</a>
<a class="sourceLine" id="cb924-16" data-line-number="16">      proposed.q =<span class="st"> </span>current.q</a>
<a class="sourceLine" id="cb924-17" data-line-number="17">    }</a>
<a class="sourceLine" id="cb924-18" data-line-number="18">    q[t,] =<span class="st"> </span>proposed.q</a>
<a class="sourceLine" id="cb924-19" data-line-number="19">  }</a>
<a class="sourceLine" id="cb924-20" data-line-number="20">  q</a>
<a class="sourceLine" id="cb924-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb924-22" data-line-number="22">N =<span class="st"> </span><span class="dv">300</span>; L =<span class="st"> </span><span class="dv">10</span>; epsilon =<span class="st"> </span><span class="fl">0.10</span> </a>
<a class="sourceLine" id="cb924-23" data-line-number="23">q =<span class="st"> </span><span class="kw">HMC</span>(N, L, epsilon)</a></code></pre></div>

<p>Figure <a href="8.2-simulation-and-sampling.html#fig:hmc">8.31</a> shows the left-side plot of only 20 data points using HMM random walk. The right-side plot shows the complete sampling from the HMM random walk.</p>

<div class="sourceCode" id="cb925"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb925-1" data-line-number="1"><span class="kw">library</span>(KernSmooth)</a>
<a class="sourceLine" id="cb925-2" data-line-number="2">data =<span class="st"> </span><span class="kw">cbind</span>(q[,<span class="dv">1</span>], q[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb925-3" data-line-number="3">kde.grid =<span class="st"> </span><span class="kw">bkde2D</span>(<span class="dt">x =</span> data, <span class="dt">bandwidth=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb925-4" data-line-number="4"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb925-5" data-line-number="5"><span class="kw">contour</span>(kde.grid<span class="op">$</span>x1, kde.grid<span class="op">$</span>x2, kde.grid<span class="op">$</span>fhat,</a>
<a class="sourceLine" id="cb925-6" data-line-number="6">        <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), </a>
<a class="sourceLine" id="cb925-7" data-line-number="7">        <span class="dt">xlab=</span><span class="st">&quot;q&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;p&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;darksalmon&quot;</span>,</a>
<a class="sourceLine" id="cb925-8" data-line-number="8">         <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;HMM (Random Walk)&quot;</span>)</a>
<a class="sourceLine" id="cb925-9" data-line-number="9">        )</a>
<a class="sourceLine" id="cb925-10" data-line-number="10">x1 =<span class="st"> </span><span class="dv">0</span>; x2 =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb925-11" data-line-number="11"><span class="kw">points</span>(x1, x2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb925-12" data-line-number="12"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">20</span>) {</a>
<a class="sourceLine" id="cb925-13" data-line-number="13">  x =<span class="st">  </span>q[i,<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span>x1; y =<span class="st">  </span>q[i,<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>x2</a>
<a class="sourceLine" id="cb925-14" data-line-number="14">  <span class="kw">segments</span>(x1, x2, x1 <span class="op">+</span><span class="st"> </span>x, x2, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>)</a>
<a class="sourceLine" id="cb925-15" data-line-number="15">  <span class="kw">segments</span>(x1 <span class="op">+</span><span class="st"> </span>x, x2, x1 <span class="op">+</span><span class="st"> </span>x, x2 <span class="op">+</span><span class="st"> </span>y, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>)</a>
<a class="sourceLine" id="cb925-16" data-line-number="16">  x1 =<span class="st"> </span>q[i,<span class="dv">1</span>]; x2 =<span class="st"> </span>q[i,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb925-17" data-line-number="17">  <span class="kw">points</span>(x1, x2, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb925-18" data-line-number="18">}</a>
<a class="sourceLine" id="cb925-19" data-line-number="19"><span class="kw">contour</span>(kde.grid<span class="op">$</span>x1, kde.grid<span class="op">$</span>x2, kde.grid<span class="op">$</span>fhat,</a>
<a class="sourceLine" id="cb925-20" data-line-number="20">        <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">2.5</span>, <span class="fl">2.5</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">3.5</span>, <span class="fl">3.5</span>), </a>
<a class="sourceLine" id="cb925-21" data-line-number="21">        <span class="dt">xlab=</span><span class="st">&quot;q&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;p&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;darksalmon&quot;</span>,</a>
<a class="sourceLine" id="cb925-22" data-line-number="22">         <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;HMM Sampling (Bivariate)&quot;</span>)</a>
<a class="sourceLine" id="cb925-23" data-line-number="23">        )</a>
<a class="sourceLine" id="cb925-24" data-line-number="24"><span class="kw">points</span>(q[,<span class="dv">1</span>], q[,<span class="dv">2</span>], <span class="dt">pch=</span><span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hmc"></span>
<img src="DS_files/figure-html/hmc-1.png" alt="Hamiltonian Monte Carlo (HMC)" width="70%" />
<p class="caption">
Figure 8.31: Hamiltonian Monte Carlo (HMC)
</p>
</div>

<p>We leave readers to investigate other enhancements to the <strong>HMC</strong> algorithm, for example, tuning the stepsize <span class="math inline">\(\epsilon\)</span>. Also, investigate <strong>MCMC</strong> for <strong>Lagrangian Dynamics</strong>.</p>
</div>
<div id="gibbs-sampling" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.6</span> Gibbs Sampling <a href="8.2-simulation-and-sampling.html#gibbs-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Gibbs Sampler</strong> algorithm is a variant of the <strong>Metropolis-Hastings MCMC</strong> algorithm in that instead of performing an acceptance-rejection evaluation, proposed probabilities are always accepted <span class="citation">(Burke N. <a href="bibliography.html#ref-ref1011n">2018</a>)</span>. The algorithm operates on conditional probabilities such that we estimate the distribution of one random variable conditioned on all other random variables so that given the following (for a case with <strong>p</strong> dimensions),</p>
<p><span class="math display" id="eq:equate1100163">\[\begin{align}
X = (X_1, X_2,...,X_p) \tag{8.174} 
\end{align}\]</span></p>
<p>the goal is to generate a random sample from the following joint distribution:</p>
<p><span class="math display" id="eq:equate1100164">\[\begin{align}
P(X_i|X_1,X_2,...,X_p) =
\frac{P(X_1,X_2,...,X_p)}{\int P(X_1,X_2,...,X_p) d_{X_i}} \tag{8.175} 
\end{align}\]</span></p>
<p>Here, <strong>Gibbs sampler</strong> simplifies the calculation by handling individual random variables like so:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{lll}
X_1^{(k+1)} &amp;\sim P(X_1| X_{\backslash 1 }^{(k)} ) &amp;\equiv P(X_1|X_2^{(k)}, X_3^{(k)},...,X_p^{(k)})\\
X_2^{(k+1)} &amp;\sim P(X_2| X_1^{(k+1)} , X_{\backslash 2 }^{(k)} ) &amp;\equiv  P(X_2|X_1^{(k+1)}, X_3^{(k)},...,X_p^{(k)})\\
X_3^{(k+1)} &amp;\sim P(X_3| X_1^{(k+1)}, X_2^{(k+1)}, X_{\backslash 3 }^{(k)} ) &amp;\equiv P(X_3|X_1^{(k+1)}, X_2^{(k+1)},X_4,...,X_p^{(k)})\\
\vdots&amp;\vdots&amp;\vdots \nonumber \\
X_p^{(k+1)} &amp;\sim P(X_p| X_{\backslash p }^{(k+1)} ) &amp;\equiv P(X_p|X_1^{(k+1)}, X_2^{(k+1)},...,X_{p-1}^{(k+1)})\\
\end{array}
\end{align}\]</span></p>
<p>In general, <strong>Gibbs Sampling</strong> has the following algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
\text{Initialize the variables } X\ where\ X = (X_1, X_2, ..., X_p)\\
\text{For k in 1,2,...  repeat the following:}\\
\ \ \ \ \ \ \ \text{For i in 1,...,p  repeat the following:}\\
\ \ \ \ \ \ \ \ \ \ \ \ \text{Sample }\ X_i\text{, given all variables except the ith X variable}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ X_i^{k+1} = P\left(X_i|X_{\backslash i}^{(k) },X_{\backslash i}^{(k+1)}\right)\\
\text{return X  where  X = }(X_1, X_2,..., X_p)
\end{array}
\]</span></p>
<p>Below is a naive implementation of <strong>Gibbs Sampler</strong> in R code using only two random variables. Note that <strong>Gibbs</strong> sampling for posterior distribution is discussed in <strong>JAGS modeling</strong> in a section ahead:</p>

<div class="sourceCode" id="cb926"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb926-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb926-2" data-line-number="2">gibbs.sampler &lt;-<span class="st"> </span><span class="cf">function</span>(N, <span class="dt">mu0 =</span> <span class="dv">1</span>, <span class="dt">mu1 =</span> <span class="dv">1</span>, <span class="dt">scale =</span> <span class="fl">0.8</span>) {</a>
<a class="sourceLine" id="cb926-3" data-line-number="3">  x1 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N); x2 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb926-4" data-line-number="4">  x1[<span class="dv">1</span>] =<span class="st"> </span>mu0; x2[<span class="dv">1</span>] =<span class="st"> </span>mu1</a>
<a class="sourceLine" id="cb926-5" data-line-number="5">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb926-6" data-line-number="6">    x1[k] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">mean =</span> x2[k], <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>scale<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb926-7" data-line-number="7">    x2[k] =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">1</span>, <span class="dt">mean =</span> x1[k], <span class="dt">sd =</span> <span class="kw">sqrt</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>scale<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb926-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb926-9" data-line-number="9">  <span class="kw">list</span>(<span class="st">&quot;x1&quot;</span> =<span class="st"> </span>x1, <span class="st">&quot;x2&quot;</span> =<span class="st"> </span>x2)</a>
<a class="sourceLine" id="cb926-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb926-11" data-line-number="11">N =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb926-12" data-line-number="12">X =<span class="st"> </span><span class="kw">gibbs.sampler</span>(N)</a>
<a class="sourceLine" id="cb926-13" data-line-number="13"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb926-14" data-line-number="14"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb926-15" data-line-number="15">     <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>,</a>
<a class="sourceLine" id="cb926-16" data-line-number="16">     <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;Gibbs Sampler (Random Walk)&quot;</span>),  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb926-17" data-line-number="17"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb926-18" data-line-number="18">x1 =<span class="st"> </span><span class="dv">0</span>; x2 =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb926-19" data-line-number="19"><span class="kw">points</span>(x1, x2, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb926-20" data-line-number="20"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">20</span>) {</a>
<a class="sourceLine" id="cb926-21" data-line-number="21">  x =<span class="st">  </span>X<span class="op">$</span>x1[i] <span class="op">-</span><span class="st"> </span>x1; y =<span class="st">  </span>X<span class="op">$</span>x2[i] <span class="op">-</span><span class="st"> </span>x2</a>
<a class="sourceLine" id="cb926-22" data-line-number="22">  <span class="kw">segments</span>(x1, x2, x1 <span class="op">+</span><span class="st"> </span>x, x2, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>)</a>
<a class="sourceLine" id="cb926-23" data-line-number="23">  <span class="kw">segments</span>(x1 <span class="op">+</span><span class="st"> </span>x, x2, x1 <span class="op">+</span><span class="st"> </span>x, x2 <span class="op">+</span><span class="st"> </span>y, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>)</a>
<a class="sourceLine" id="cb926-24" data-line-number="24">  x1 =<span class="st"> </span>X<span class="op">$</span>x1[i]; x2 =<span class="st"> </span>X<span class="op">$</span>x2[i]</a>
<a class="sourceLine" id="cb926-25" data-line-number="25">  <span class="kw">points</span>(x1, x2, <span class="dt">pch=</span><span class="dv">20</span>)</a>
<a class="sourceLine" id="cb926-26" data-line-number="26">}</a>
<a class="sourceLine" id="cb926-27" data-line-number="27"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">2</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb926-28" data-line-number="28">     <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>,</a>
<a class="sourceLine" id="cb926-29" data-line-number="29">     <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;Gibbs Sampler (Bivariate)&quot;</span>),  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb926-30" data-line-number="30"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb926-31" data-line-number="31"><span class="kw">points</span>(X<span class="op">$</span>x1, X<span class="op">$</span>x2, <span class="dt">pch=</span><span class="dv">20</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gibbs"></span>
<img src="DS_files/figure-html/gibbs-1.png" alt="Gibbs Sampler" width="70%" />
<p class="caption">
Figure 8.32: Gibbs Sampler
</p>
</div>

<p>We leave readers to investigate <strong>blocked Gibbs sampling</strong>. The idea is to operate on groups (called blocks) of random variables instead of individual variables.</p>
<p>Also, other important control properties for <strong>Gibbs Sampling</strong> are <strong>Burn-in</strong> and <strong>Thinning</strong>. The <strong>Burn-in</strong> or <strong>Warm-up</strong>* is discussed under <strong>JAGS modeling</strong> section further ahead. <strong>Thinning</strong> - which skips samples - is discussed under <strong>Autocorrelation</strong> section.</p>
</div>
<div id="importance-sampling" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.7</span> Importance Sampling <a href="8.2-simulation-and-sampling.html#importance-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Our discussion on <strong>Importance sampling</strong> focuses on the expectation of an arbitrary function, namely <span class="math inline">\(f(x)\)</span>, as a point of comparison between calculating the simple average of its outcome with respect to a given target density, namely <span class="math inline">\(P(x)\)</span>, and the weighted average of its outcome with respect to a chosen approximating density, namely <span class="math inline">\(\mathcal{Q}(x)\)</span>.</p>
<p>Here, the premise is to find an approximating distribution that we can use to sample when the target distribution is difficult to sample easily. For example, sometimes, the tail of a Normal distribution matters. There is a possibility that that important region may not have enough data points to sample; thus, it may be necessary to propose a distribution representing that critical region of the distribution. From there, we simulate sampling from the newly proposed distribution - this is called <strong>importance sampling</strong> <span class="citation">(Bugallo M.F. et al. <a href="bibliography.html#ref-ref1138m">2017</a>; Elvira V. and Martino L. <a href="bibliography.html#ref-ref1129v">2021</a>)</span>.</p>
<p>To illustrate <strong>Importance sampling</strong>, recall the <strong>Law of Unconscious Statistician</strong> in the <strong>Monte Carlo estimation</strong> section. We start by using the following integral of the distribution in question:</p>
<p><span class="math display" id="eq:eqnnumber335">\[\begin{align}
\begin{array}{ll}
\mathcal{I}_p = \int f(x) P(x) dx &amp;\ \ \ \ \ \ \ \ where\ \mathcal{I}_p = \mathbb{E}_p\left[f(x)\right]\\
\end{array} \tag{8.176}
\end{align}\]</span></p>
<p>Here, we prescribe what we call an <strong>importance weight</strong> or a <strong>likelihood ratio</strong> by introducing an approximating distribution like so:  </p>
<p><span class="math display" id="eq:eqnnumber336">\[\begin{align}
\begin{array}{ll}
\mathcal{I}_q = \int f(x) P(x) \frac{\mathbf{Q}(x)}{\mathbf{Q}(x)} dx 
&amp;\ \ \ \ \ \ \ \ where\ \mathcal{I}_q = \mathbb{E}_q\left[\frac{f(x)P(x)}{\mathcal{Q}(x)}\right]
\end{array} \tag{8.177}
\end{align}\]</span></p>
<p>The prescribed <strong>importance weight</strong> or <strong>likelihood ratio</strong> is written as:</p>
<p><span class="math display" id="eq:equate1100165">\[\begin{align}
\mathcal{I}_q  = \int f(x)\mathcal{Q}(x) \omega(x) dx\ \ \ \leftarrow\ \ \ \ \ \omega(x) = \frac{P(x)}{\mathcal{Q}(x)}\ \ \text{(importance weight)} \tag{8.178} 
\end{align}\]</span></p>
<p>This ratio bears weight to the region of interest.</p>
<p>Note that choosing a distribution for <span class="math inline">\(\mathcal{Q}(x)\)</span> is arbitrary with no hard rules. However, ideally, it may be best to choose one that closely matches <span class="math inline">\(P(x)\)</span> or <span class="math inline">\(| f(x) | \mathcal{\omega}(x)\)</span>.</p>
<p>To illustrate, let us define and implement the three functions, namely <strong>f(x)</strong>, <span class="math inline">\(\mathbf{P}(x)\)</span>, and <span class="math inline">\(\mathbf{\mathcal{Q}}(x)\)</span>:</p>

<div class="sourceCode" id="cb927"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb927-1" data-line-number="1">mu1 =<span class="st"> </span><span class="dv">4</span>; mu2 =<span class="dv">6</span></a>
<a class="sourceLine" id="cb927-2" data-line-number="2">f &lt;-<span class="st"> </span><span class="cf">function</span>(X, <span class="dt">rate =</span> <span class="fl">0.4</span>) {  <span class="co"># f(x)</span></a>
<a class="sourceLine" id="cb927-3" data-line-number="3">  <span class="kw">dexp</span>(X, <span class="dt">rate =</span> rate)  <span class="co"># arbitrary function</span></a>
<a class="sourceLine" id="cb927-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb927-5" data-line-number="5">P.normal.pdf &lt;-<span class="st"> </span><span class="cf">function</span>(X, <span class="dt">mu =</span> mu1, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">log=</span><span class="ot">FALSE</span>) { <span class="co"># P(x)</span></a>
<a class="sourceLine" id="cb927-6" data-line-number="6">  <span class="kw">dnorm</span>(X, <span class="dt">mean =</span> mu, <span class="dt">sd=</span>sd, <span class="dt">log =</span> log) </a>
<a class="sourceLine" id="cb927-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb927-8" data-line-number="8">Q.normal.pdf &lt;-<span class="st"> </span><span class="cf">function</span>(X, <span class="dt">mu =</span> mu2, <span class="dt">sd =</span> <span class="dv">1</span>, <span class="dt">log=</span><span class="ot">FALSE</span>) { <span class="co"># Q(x)</span></a>
<a class="sourceLine" id="cb927-9" data-line-number="9">  <span class="kw">dnorm</span>(X, <span class="dt">mean =</span> mu, <span class="dt">sd=</span>sd, <span class="dt">log =</span> log)</a>
<a class="sourceLine" id="cb927-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb927-11" data-line-number="11">N =<span class="st"> </span><span class="dv">1000</span></a>
<a class="sourceLine" id="cb927-12" data-line-number="12">X =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dt">length.out =</span> N)</a>
<a class="sourceLine" id="cb927-13" data-line-number="13">P =<span class="st"> </span><span class="kw">P.normal.pdf</span>(X, <span class="dt">mu =</span> mu1, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb927-14" data-line-number="14">Q =<span class="st"> </span><span class="kw">Q.normal.pdf</span>(X, <span class="dt">mu =</span> mu2, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb927-15" data-line-number="15">G =<span class="st"> </span><span class="kw">f</span>(X, <span class="dt">rate =</span> <span class="fl">0.4</span>)</a></code></pre></div>

<p>See Figure <a href="8.2-simulation-and-sampling.html#fig:importance">8.33</a> for the plot of <span class="math inline">\(f(x) \sim Expo(x)\)</span>, <span class="math inline">\(\mathbf{P}(x) \sim \mathcal{N}(4, 1)\)</span>, and <span class="math inline">\(\mathbf{\mathcal{Q}}(x) \sim \mathcal{N}(6, 1)\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:importance"></span>
<img src="DS_files/figure-html/importance-1.png" alt="Importance Sampling" width="70%" />
<p class="caption">
Figure 8.33: Importance Sampling
</p>
</div>

<p>Now, let us evaluate two calculations (the simple average and weighted average):</p>

<div class="sourceCode" id="cb928"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb928-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb928-2" data-line-number="2">simple.avarage &lt;-<span class="st"> </span><span class="cf">function</span>(N) {</a>
<a class="sourceLine" id="cb928-3" data-line-number="3">  E =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb928-4" data-line-number="4">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb928-5" data-line-number="5">    X =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> mu1, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb928-6" data-line-number="6">    E[i] =<span class="st"> </span><span class="kw">f</span>(X, <span class="dt">rate=</span><span class="fl">0.4</span>)  </a>
<a class="sourceLine" id="cb928-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb928-8" data-line-number="8">  <span class="kw">sum</span>(E) <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb928-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb928-10" data-line-number="10">importance.sampling &lt;-<span class="st"> </span><span class="cf">function</span>(N) {</a>
<a class="sourceLine" id="cb928-11" data-line-number="11">  E =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N)</a>
<a class="sourceLine" id="cb928-12" data-line-number="12">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb928-13" data-line-number="13">    X =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> mu2, <span class="dt">sd =</span> <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb928-14" data-line-number="14">    W =<span class="st"> </span><span class="kw">exp</span>(<span class="kw">P.normal.pdf</span>(X, <span class="dt">mu=</span>mu1, <span class="dt">log=</span><span class="ot">TRUE</span>) <span class="op">-</span><span class="st"> </span></a>
<a class="sourceLine" id="cb928-15" data-line-number="15"><span class="st">            </span><span class="kw">Q.normal.pdf</span>(X, <span class="dt">mu=</span>mu2, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb928-16" data-line-number="16">    E[i] =<span class="st"> </span><span class="kw">f</span>(X, <span class="dt">rate=</span><span class="fl">0.4</span>) <span class="op">*</span><span class="st"> </span>W</a>
<a class="sourceLine" id="cb928-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb928-18" data-line-number="18">  <span class="kw">sum</span>(E) <span class="op">/</span><span class="st"> </span>N</a>
<a class="sourceLine" id="cb928-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb928-20" data-line-number="20"><span class="kw">c</span>(<span class="st">&quot;Simple Average&quot;</span> =<span class="st"> </span><span class="kw">simple.avarage</span>(N), </a>
<a class="sourceLine" id="cb928-21" data-line-number="21">  <span class="st">&quot;Weighted Average&quot;</span> =<span class="st"> </span><span class="kw">importance.sampling</span> (N) )</a></code></pre></div>
<pre><code>##   Simple Average Weighted Average 
##          0.08889          0.09341</code></pre>

<p>We leave readers to investigate <strong>Simulated Annealing</strong> and <strong>Annealed Importance Sampling (AIS)</strong>.</p>
</div>
<div id="rejection-sampling" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.8</span> Rejection Sampling <a href="8.2-simulation-and-sampling.html#rejection-sampling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Rejection Sampling</strong>, also called <strong>Acceptance-Rejection Sampling</strong>, introduces the concept of an <strong>envelope distribution</strong> (our proposal distribution) <span class="citation">(Erraqabi A. et al. <a href="bibliography.html#ref-ref1111a">2016</a>; Gilks, W. R., &amp; Wild, P. <a href="bibliography.html#ref-ref1120g">1992</a>)</span>. The idea is to scale (or stretch) an approximating distribution, namely <span class="math inline">\(\mathcal{Q}(x)\)</span>, such that it is tall and wide enough to <strong>envelope</strong> or <strong>cover</strong> the entire target distribution, <span class="math inline">\(P(x)\)</span>. It is natural and perhaps intuitive to use <strong>uniform distribution</strong> for our <strong>envelope distribution</strong> that is scaled to cover the target distribution; however, we use a <strong>normal distribution</strong>in our example. See Figure <a href="8.2-simulation-and-sampling.html#fig:rejection">8.34</a> for the plot of the <strong>Rejection Sampling</strong>.  </p>
<p>To illustrate, let us implement a <strong>univariate bimodal mixture distribution</strong> for our target distribution using a function called <strong>dnorml.mixt(.)</strong> from a third-party library called <strong>ks</strong>. Our bimodal mixture has an equal proportion.</p>
<p><span class="math display" id="eq:equate1100166">\[\begin{align}
P(x) \sim \mathcal{N}(\mu=25, \sigma = 2 ) + \mathcal{N}(\mu=35, \sigma = 4 ) \tag{8.179} 
\end{align}\]</span></p>
<p>We then can choose any <strong>approximating distribution</strong> that closely match our target distribution. Here, we choose <strong>Gaussian distribution</strong>:</p>
<p><span class="math display" id="eq:equate1100167">\[\begin{align}
\mathcal{Q}(x) \sim \mathcal{N}(\mu = 30, \sigma = 6) \tag{8.180} 
\end{align}\]</span></p>
<p>Next, we compute for the scale factor denoted as <strong>k</strong>:</p>
<p><span class="math display" id="eq:equate1100168">\[\begin{align}
k = max\left(\frac{P(x)}{\mathcal{Q}(x)}\right) \tag{8.181} 
\end{align}\]</span></p>
<p>Therefore, our <strong>envelope distribution</strong> is generated using the following equation - represented by an <strong>envelope function</strong>, <span class="math inline">\(Q^*(x)\)</span>:</p>
<p><span class="math display" id="eq:equate1100169">\[\begin{align}
Q^*(x) = k \times \mathcal{Q}(x) \tag{8.182} 
\end{align}\]</span></p>
<p>We then generate sufficient samples from a uniform distribution with a height equal to our <strong>envelope distribution</strong>.</p>
<p><span class="math display" id="eq:equate1100170">\[\begin{align}
U \sim \mathcal{U}(N=1000, 0, max(Q^*{(x)})) \tag{8.183} 
\end{align}\]</span></p>
<p>We then use the uniformly generated sample to accept data points as our primary sample with a height below our <strong>target distribution</strong>.</p>
<p><span class="math display">\[
if\ U \le P(x)\ \ \ \text{accept region below or along target density}
\]</span></p>
<p>Additionally, we also can sample the region below the envelope density and above the target density for our plotting purpose:</p>
<p><span class="math display">\[
if\ U &lt; \mathcal{Q}^*(x)\ and\ U &gt; P(x)\ \ \ 
\begin{cases}
\begin{array}{l}\text{accept region below envelope density}\\
\text{and above the target density}
\end{array}
\end{cases}
\]</span></p>
<p>Below is a simple implementation of <strong>Rejection sampling</strong>.</p>

<div class="sourceCode" id="cb930"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb930-1" data-line-number="1"><span class="kw">library</span>(ks)</a></code></pre></div>
<pre><code>## 
## Attaching package: &#39;ks&#39;</code></pre>
<pre><code>## The following object is masked _by_ &#39;.GlobalEnv&#39;:
## 
##     kde</code></pre>
<div class="sourceCode" id="cb933"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb933-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb933-2" data-line-number="2">P.normal.pdf &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="co"># P(x)</span></a>
<a class="sourceLine" id="cb933-3" data-line-number="3">  y =<span class="st"> </span><span class="kw">dnorm.mixt</span>(x, <span class="dt">mus=</span><span class="kw">c</span>(<span class="dv">25</span>,<span class="dv">35</span>), <span class="dt">sigmas=</span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>), <span class="dt">props=</span><span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>))</a>
<a class="sourceLine" id="cb933-4" data-line-number="4">  <span class="kw">list</span>(<span class="st">&quot;y&quot;</span> =<span class="st"> </span>y)</a>
<a class="sourceLine" id="cb933-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb933-6" data-line-number="6">Q.normal.pdf &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="co"># Q(x)</span></a>
<a class="sourceLine" id="cb933-7" data-line-number="7">  y =<span class="st"> </span><span class="kw">dnorm.mixt</span>(x, <span class="dt">mus=</span><span class="kw">c</span>(<span class="dv">30</span>), <span class="dt">sigmas=</span><span class="kw">c</span>(<span class="dv">6</span>), <span class="dt">props=</span><span class="kw">c</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb933-8" data-line-number="8">  <span class="kw">list</span>(<span class="st">&quot;y&quot;</span> =<span class="st"> </span>y)</a>
<a class="sourceLine" id="cb933-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb933-10" data-line-number="10">rejection.sampling &lt;-<span class="st"> </span><span class="cf">function</span>(X, <span class="dt">N =</span> <span class="dv">500</span>) {</a>
<a class="sourceLine" id="cb933-11" data-line-number="11">  target =<span class="st"> </span><span class="kw">P.normal.pdf</span>(X)</a>
<a class="sourceLine" id="cb933-12" data-line-number="12">  approximate =<span class="st"> </span><span class="kw">Q.normal.pdf</span>(X)</a>
<a class="sourceLine" id="cb933-13" data-line-number="13">  <span class="co"># compute for the scale factor</span></a>
<a class="sourceLine" id="cb933-14" data-line-number="14">  k =<span class="st">  </span><span class="kw">max</span> ( <span class="kw">exp</span>( <span class="kw">log</span>( target<span class="op">$</span>y, <span class="dv">2</span>) <span class="op">-</span><span class="st"> </span><span class="kw">log</span>( approximate<span class="op">$</span>y, <span class="dv">2</span>)) )</a>
<a class="sourceLine" id="cb933-15" data-line-number="15">  envelop =<span class="st"> </span>approximate<span class="op">$</span>y <span class="op">*</span><span class="st"> </span>k</a>
<a class="sourceLine" id="cb933-16" data-line-number="16">  <span class="co"># generate sufficient samples</span></a>
<a class="sourceLine" id="cb933-17" data-line-number="17">  xu1 =<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min=</span><span class="dv">15</span>, <span class="dt">max=</span><span class="dv">45</span>)</a>
<a class="sourceLine" id="cb933-18" data-line-number="18">  yu1 =<span class="st"> </span><span class="kw">runif</span>(N, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span> <span class="kw">max</span> (envelop))</a>
<a class="sourceLine" id="cb933-19" data-line-number="19">  <span class="co"># density from target distribution</span></a>
<a class="sourceLine" id="cb933-20" data-line-number="20">  p.density =<span class="st"> </span><span class="kw">P.normal.pdf</span>(xu1)<span class="op">$</span>y</a>
<a class="sourceLine" id="cb933-21" data-line-number="21">  <span class="co"># density from envelope distribution</span></a>
<a class="sourceLine" id="cb933-22" data-line-number="22">  q.density =<span class="st"> </span><span class="kw">Q.normal.pdf</span>(xu1)<span class="op">$</span>y <span class="op">*</span><span class="st"> </span>k</a>
<a class="sourceLine" id="cb933-23" data-line-number="23">  <span class="co"># accept region below  target density</span></a>
<a class="sourceLine" id="cb933-24" data-line-number="24">  accept1 =<span class="st"> </span><span class="kw">which</span>(yu1 <span class="op">&lt;=</span><span class="st"> </span>p.density)</a>
<a class="sourceLine" id="cb933-25" data-line-number="25">  <span class="co"># accept region using envelope density</span></a>
<a class="sourceLine" id="cb933-26" data-line-number="26">  accept2 =<span class="st"> </span><span class="kw">which</span>(yu1 <span class="op">&lt;</span><span class="st"> </span>q.density <span class="op">&amp;</span><span class="st"> </span>yu1 <span class="op">&gt;</span><span class="st"> </span>p.density)</a>
<a class="sourceLine" id="cb933-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;P.x&quot;</span> =<span class="st"> </span>xu1[accept1], <span class="st">&quot;P.y&quot;</span> =<span class="st"> </span>yu1[accept1],</a>
<a class="sourceLine" id="cb933-28" data-line-number="28">       <span class="st">&quot;Q.x&quot;</span> =<span class="st"> </span>xu1[accept2], <span class="st">&quot;Q.y&quot;</span> =<span class="st"> </span>yu1[accept2],</a>
<a class="sourceLine" id="cb933-29" data-line-number="29">       <span class="st">&quot;envelop&quot;</span> =<span class="st"> </span>envelop, </a>
<a class="sourceLine" id="cb933-30" data-line-number="30">       <span class="st">&quot;approximate&quot;</span> =<span class="st"> </span>approximate<span class="op">$</span>y, <span class="st">&quot;k&quot;</span> =<span class="st"> </span>k,</a>
<a class="sourceLine" id="cb933-31" data-line-number="31">       <span class="st">&quot;target&quot;</span> =<span class="st"> </span>target<span class="op">$</span>y)</a>
<a class="sourceLine" id="cb933-32" data-line-number="32">}</a>
<a class="sourceLine" id="cb933-33" data-line-number="33">N =<span class="st"> </span><span class="dv">500</span></a>
<a class="sourceLine" id="cb933-34" data-line-number="34">X =<span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">45</span>, <span class="dt">length.out =</span> N)</a>
<a class="sourceLine" id="cb933-35" data-line-number="35">RS =<span class="st"> </span><span class="kw">rejection.sampling</span> (X, N )</a></code></pre></div>

<p>Below is the plot of our final sampling (data points denoted by <strong>P.x</strong> and <strong>P.y</strong>):</p>

<div class="sourceCode" id="cb934"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb934-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">15</span>, <span class="dv">45</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="fl">0.01</span>, <span class="fl">0.25</span>), </a>
<a class="sourceLine" id="cb934-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;X&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Y&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>,  <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>,</a>
<a class="sourceLine" id="cb934-3" data-line-number="3">     <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;Rejection Sampling&quot;</span>),  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb934-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb934-5" data-line-number="5"><span class="kw">points</span>(RS<span class="op">$</span>Q.x, RS<span class="op">$</span>Q.y, <span class="dt">pch=</span><span class="dv">4</span>, <span class="dt">col=</span><span class="st">&quot;grey&quot;</span>)</a>
<a class="sourceLine" id="cb934-6" data-line-number="6"><span class="kw">points</span>(RS<span class="op">$</span>P.x, RS<span class="op">$</span>P.y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;seagreen&quot;</span>)</a>
<a class="sourceLine" id="cb934-7" data-line-number="7"><span class="kw">lines</span>(X, RS<span class="op">$</span>target, <span class="dt">col=</span><span class="st">&quot;seagreen&quot;</span>,  <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb934-8" data-line-number="8"><span class="kw">lines</span>(X, RS<span class="op">$</span>approximate, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb934-9" data-line-number="9"><span class="kw">lines</span>(X, RS<span class="op">$</span>envelop, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb934-10" data-line-number="10">max.y1 =<span class="st"> </span><span class="kw">max</span>(RS<span class="op">$</span>approximate)</a>
<a class="sourceLine" id="cb934-11" data-line-number="11">max.y2 =<span class="st"> </span><span class="kw">max</span>(RS<span class="op">$</span>envelop)</a>
<a class="sourceLine" id="cb934-12" data-line-number="12"><span class="kw">arrows</span>(<span class="dv">30</span>, max.y1, <span class="dv">30</span>, max.y2, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb934-13" data-line-number="13"><span class="kw">text</span>(<span class="fl">31.2</span>, <span class="fl">0.15</span>, <span class="dt">label=</span><span class="st">&quot;scale&quot;</span>)</a>
<a class="sourceLine" id="cb934-14" data-line-number="14"><span class="kw">text</span>(<span class="fl">31.4</span>, <span class="fl">0.13</span>, <span class="dt">label=</span><span class="kw">paste0</span>(<span class="st">&quot;by &quot;</span>, <span class="kw">round</span>(RS<span class="op">$</span>k,<span class="dv">1</span>)))</a>
<a class="sourceLine" id="cb934-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">30</span>, <span class="fl">-0.01</span>, <span class="dt">label=</span><span class="st">&quot;accept region&quot;</span>, <span class="dt">col=</span><span class="st">&quot;seagreen&quot;</span>)</a>
<a class="sourceLine" id="cb934-16" data-line-number="16"><span class="kw">text</span>(<span class="dv">30</span>, <span class="fl">0.10</span>, <span class="dt">label=</span><span class="st">&quot;reject region&quot;</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb934-17" data-line-number="17"><span class="kw">legend</span>(<span class="dv">34</span>, <span class="fl">0.25</span>, </a>
<a class="sourceLine" id="cb934-18" data-line-number="18">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;P(x) - Target Dist&quot;</span>, </a>
<a class="sourceLine" id="cb934-19" data-line-number="19">              <span class="st">&quot;Q(x) - Approx Dist&quot;</span>, </a>
<a class="sourceLine" id="cb934-20" data-line-number="20">              <span class="st">&quot;Envelope Dist&quot;</span>),</a>
<a class="sourceLine" id="cb934-21" data-line-number="21">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;seagreen&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>),  </a>
<a class="sourceLine" id="cb934-22" data-line-number="22">    <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">3</span>,<span class="dv">1</span>), <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rejection"></span>
<img src="DS_files/figure-html/rejection-1.png" alt="Rejection Sampling" width="70%" />
<p class="caption">
Figure 8.34: Rejection Sampling
</p>
</div>

<p>Below are other methods of sampling that are worth investigating:</p>
<ul>
<li>Grid Sampling</li>
<li>Bridge Sampling</li>
<li>Nested Sampling</li>
<li>Stepping Stone Sampling</li>
<li>Path Sampling</li>
<li>Dependent Sampling</li>
<li>Independent Sampling</li>
<li>Slice Sampling</li>
</ul>
<p>We leave readers to investigate the <strong>Adaptive Rejection Sampling (ARS)</strong>, which uses both <strong>envelope function</strong> and <strong>squeezing function</strong> to constraint the <strong>approximating function</strong> to upper and lower bounds <span class="citation">(Gilks, W. R., &amp; Wild, P. <a href="bibliography.html#ref-ref1120g">1992</a>)</span>.</p>
</div>
<div id="jags-modeling" class="section level3 hasAnchor">
<h3><span class="header-section-number">8.2.9</span> JAGS Modeling <a href="8.2-simulation-and-sampling.html#jags-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend the discussion around <strong>flat</strong> prior and <strong>hyperparameter</strong> by introducing a <strong>modeling</strong> technique called <strong>JAGS modeling</strong> relevant to our choice of prior and hyperparameters <span class="citation">(Plummer M. <a href="bibliography.html#ref-ref1252m">2003</a>; Kruschke J.K. <a href="bibliography.html#ref-ref241j">2015</a>)</span>.</p>
<p>We introduce an R package called <strong>rjags</strong> in this section. <strong>JAGS</strong> stands for <strong>Just Another GIBBS Sampler</strong>. There are three overlapping concepts when dealing with JAGS: <strong>GIBBS sampling</strong>, <strong>Markov Chain Monte Carlo (MCMC) simulation</strong>, and <strong>JAGS modeling</strong>. This section focuses on <strong>JAGS modeling</strong> using <strong>JAGS</strong> using the other two concepts to demonstrate a case in modeling and simulating sampling.</p>
<p>To illustrate, let us discuss how to model a few <strong>bayesian</strong> cases relevant to a family of conjugacy which is discussed in previous sections.</p>
<p><strong>Normal-Normal Model</strong></p>
<p>Let us recall <strong>normal-normal conjugacy</strong> with unknown mean, <span class="math inline">\(\mu\)</span>, and known variance, <span class="math inline">\(\sigma^2\)</span>. For this family of <strong>conjugacy</strong>, we have the following description of <strong>prior distribution</strong> and <strong>sampling density (likelihood)</strong>:</p>
<p><span class="math display" id="eq:equate1100172" id="eq:equate1100171">\[\begin{align}
P(\mu) {}&amp;\rightarrow\ \ \ \ \ \mu \sim \mathcal{N}(\mu_0, \sigma^2_0) \tag{8.184} \\
\nonumber \\
Lik(x_1,...,x_n|\mu) &amp;\rightarrow\ \ \ \ \ x_1,...,x_n|\mu, \sigma^2 \sim \mathcal{N}(\mu, \sigma^2)\ \ \ \text{(sampling density)} \tag{8.185} 
\end{align}\]</span></p>
<p>We use BUGS language to model in Jags. Below is an example of implementing Jags model for the prior and sampling density in R based on the two distributions above (note in JAGS that we use precision which is the inverse of the variance):</p>
<pre><code>model {
  for (i in 1:N) {                 # N = sampling size
   x[i] ~ dnorm(mu[i], 1/sigma[i]) # sampling density (likelihood)
   mu[i] ~ dnorm(mu0, 1/sigma0)        # prior (mean)
  }
  mu0 ~ dunif(-1000, 1000) # hyperparameter (mean)
  sigma0 ~ dunif(0, 1000)  # hyperparameter (sd for variance)
}</code></pre>
<p>In the model, we use the following hyperparameters for a flat hyperprior:</p>
<p><span class="math display">\[\begin{align*}
\mu_0 {}&amp;\sim U(-1000,1000) &amp; -\infty &lt; \mu_0 &lt; \infty\\
\sigma^2_0 &amp;\sim  U(0,1000) &amp; \sigma^2_0 &lt; \infty
\end{align*}\]</span></p>
<p>For <strong>normal-normal conjugacy</strong> with known mean <span class="math inline">\(\mu\)</span> and unknown variance <span class="math inline">\(\sigma^2\)</span>, we have the following description of <strong>prior distribution</strong> and <strong>sampling density (likelihood)</strong>:</p>
<p><span class="math display">\[\begin{align*}
P(\sigma^2) {}&amp;\rightarrow\ \ \ \ \ \sigma^2 \sim Inv.\ Gamma(\alpha_0,\beta_0)\\
\\
Lik(x_1,...,x_n|\sigma^2) &amp;\rightarrow\ \ \ \ \ x|\sigma \sim \mathcal{N}(\mu, \sigma^2)\ \ \ \text{(sampling density)}
\end{align*}\]</span></p>
<p>We use BUGS language to model in Jags. Below is an example of implementing the Jags model for the prior and sampling density in R code based on the two distributions above (note in JAGS that we use precision which is the inverse of the variance):</p>
<pre><code>model {
  for (i in 1:N) {                   # N = sampling size
   x[i] ~ dnorm(mu[i], 1/sigma[i])   # sampling density (likelihood)  
   sigma[i] ~ dgamma(alpha0, beta0)  # prior (sd)
  }
  alpha0 ~ dunif(0, 1000)       # hyperparameter (alpha)
  beta0 ~ dunif(0, 1000)        # hyperparameter (beta)
}</code></pre>
<p>In the model, we use the following hyperparameters for a flat hyperprior:</p>
<p><span class="math display">\[\begin{align*}
\alpha_0 {}&amp;\sim U(0,1000) &amp; \alpha_0 &lt; \infty\\
\beta_0 &amp;\sim  U(0,1000) &amp; \beta_0 &lt; \infty
\end{align*}\]</span></p>
<p><strong>Binomial-Beta Model</strong></p>
<p>For <strong>Binomial-Beta conjugacy</strong>, we have the following description of <strong>prior distribution</strong> and <strong>sampling density (likelihood)</strong>:</p>
<p><span class="math display" id="eq:equate1100174" id="eq:equate1100173">\[\begin{align}
P(\rho) {}&amp;\rightarrow\ \ \ \ \ \rho \sim Beta(\alpha_0, \beta_0) \tag{8.186} \\
\nonumber \\
Lik(x_1,...,x_n|\rho) &amp;\rightarrow\ \ \ \ \ x_1,...,x_n|\rho \sim Bin(n, \rho)\ \ \ \text{(sampling density)} \tag{8.187} 
\end{align}\]</span></p>
<p>Below is an example of implementing the Jags model for the prior and sampling density in R code based on the two distributions above:</p>
<pre><code>model {
  for (i in 1:M) {                # M = sampling size
    x[i] ~ dbin(n, rho[i])        # sampling density (likelihood)
    rho[i] ~ dbeta(alpha0, beta0) # prior
  }
  alpha0 ~ dunif(0, 1)       # hyperparameter (alpha)
  beta0 ~ dunif(0, 1)        # hyperparameter (beta)
}</code></pre>
<p>In the model, we use the following hyperparameters for a flat hyperprior:</p>
<p><span class="math display">\[\begin{align*}
\alpha_0 {}&amp;\sim U(0,1) &amp; \alpha_0 &lt; \infty\\
\beta_0 &amp;\sim  U(0,1) &amp; \beta_0 &lt; \infty\\
\end{align*}\]</span></p>
<p>Figure <a href="8.2-simulation-and-sampling.html#fig:rjagmodel">8.35</a> shows a visual representation of the <strong>JAGS model</strong>. In the figure, the circle nodes represent random variables. There are five variables in each model. The shaded circle node labeled as <span class="math inline">\(\mathbf{x}\)</span> represents the <strong>observed data</strong>. The circles inside the rectangular plate are vector variables with a sampling size of <span class="math inline">\(N\ or\ M\)</span>. The model forms a <strong>hierarchical</strong> child-parent relationship and, in our case, follows a DAG model (directed acyclic graph).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rjagmodel"></span>
<img src="rjagmodel.png" alt="JAGS Model" width="100%" />
<p class="caption">
Figure 8.35: JAGS Model
</p>
</div>

<p>To illustrate, let us implement the first model using R code. Here, we only focus on the <strong>likelihood</strong> and <strong>prior</strong> for the <strong>normal-normal</strong> model with unknown <span class="math inline">\(\mu\)</span> and known <span class="math inline">\(\sigma^2\)</span>. We simulate by sampling ten separate groups, each sample with about fifty observations. We then calculate the variance within each sample. For demonstration, we leave the mean unknown for each sample. Our goal is to parameterize the unknown <span class="math inline">\(\mu\)</span>.</p>



<div class="sourceCode" id="cb938"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb938-1" data-line-number="1"><span class="co"># load coda and rjags library first</span></a>
<a class="sourceLine" id="cb938-2" data-line-number="2"><span class="kw">library</span>(coda);</a>
<a class="sourceLine" id="cb938-3" data-line-number="3"><span class="kw">library</span>(rjags)</a>
<a class="sourceLine" id="cb938-4" data-line-number="4"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb938-5" data-line-number="5">sample_size =<span class="st"> </span><span class="dv">50</span>   <span class="co"># observations</span></a>
<a class="sourceLine" id="cb938-6" data-line-number="6">sampling_size =<span class="st"> </span><span class="dv">4</span> <span class="co"># group size</span></a>
<a class="sourceLine" id="cb938-7" data-line-number="7">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">11</span>, <span class="dv">20</span>, <span class="dt">length.out=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb938-8" data-line-number="8">sampling =<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">n=</span>sampling_size, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb938-9" data-line-number="9">                                             <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb938-10" data-line-number="10">mean.with.noise &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  </a>
<a class="sourceLine" id="cb938-11" data-line-number="11">      noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">1</span>, <span class="dt">mean =</span> <span class="dv">0</span>, <span class="dt">sd =</span> <span class="fl">1.2</span>) <span class="co"># introduce error/noise</span></a>
<a class="sourceLine" id="cb938-12" data-line-number="12">      <span class="kw">mean</span>(x)  <span class="op">+</span><span class="st"> </span>noise</a>
<a class="sourceLine" id="cb938-13" data-line-number="13">  }</a>
<a class="sourceLine" id="cb938-14" data-line-number="14">data =<span class="st"> </span><span class="kw">as.data.frame</span>( </a>
<a class="sourceLine" id="cb938-15" data-line-number="15">  <span class="kw">matrix</span>( <span class="kw">c</span>( <span class="kw">seq</span>(<span class="dv">1</span>, sampling_size), </a>
<a class="sourceLine" id="cb938-16" data-line-number="16">          <span class="kw">sqrt</span>(<span class="kw">apply</span>(sampling, <span class="dv">2</span>, var)), <span class="kw">apply</span>(sampling, <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb938-17" data-line-number="17">                                               mean.with.noise )),  </a>
<a class="sourceLine" id="cb938-18" data-line-number="18">     <span class="dt">nrow=</span>sampling_size, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb938-19" data-line-number="19">  )</a>
<a class="sourceLine" id="cb938-20" data-line-number="20"><span class="kw">colnames</span>(data) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;index&quot;</span>, <span class="st">&quot;sd&quot;</span>, <span class="st">&quot;mean&quot;</span>)</a>
<a class="sourceLine" id="cb938-21" data-line-number="21">data</a></code></pre></div>
<pre><code>##   index    sd  mean
## 1     1 3.051 13.65
## 2     2 2.507 13.95
## 3     3 2.862 15.12
## 4     4 3.044 15.86</code></pre>

<p><strong>First</strong>, we construct a list of parameters to pass to the model:</p>

<div class="sourceCode" id="cb940"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb940-1" data-line-number="1">model.data &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;N&quot;</span> =<span class="st"> </span><span class="kw">nrow</span>(data), <span class="st">&quot;X&quot;</span> =<span class="st"> </span>data<span class="op">$</span>mean,  </a>
<a class="sourceLine" id="cb940-2" data-line-number="2">                   <span class="st">&quot;sigma&quot;</span> =<span class="st"> </span>data<span class="op">$</span>sd) </a></code></pre></div>

<p><strong>Second</strong>, we then generate and initialize the model:</p>

<div class="sourceCode" id="cb941"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb941-1" data-line-number="1">modelstring =<span class="st"> &quot;</span></a>
<a class="sourceLine" id="cb941-2" data-line-number="2"><span class="st">  model {</span></a>
<a class="sourceLine" id="cb941-3" data-line-number="3"><span class="st">    for (i in 1:N) {                 # N = sampling size</span></a>
<a class="sourceLine" id="cb941-4" data-line-number="4"><span class="st">     X[i] ~ dnorm(mu[i], 1/sigma[i]) # sampling density (likelihood)</span></a>
<a class="sourceLine" id="cb941-5" data-line-number="5"><span class="st">     mu[i] ~ dnorm(mu0, 1/sigma0)    # prior (mean)</span></a>
<a class="sourceLine" id="cb941-6" data-line-number="6"><span class="st">    }</span></a>
<a class="sourceLine" id="cb941-7" data-line-number="7"><span class="st">    mu0 ~ dunif(-1000, 1000)         # hyperparameter (mean)</span></a>
<a class="sourceLine" id="cb941-8" data-line-number="8"><span class="st">    sigma0 ~ dunif(0, 1000)          # hyperparameter (sd for variance)</span></a>
<a class="sourceLine" id="cb941-9" data-line-number="9"><span class="st">  }</span></a>
<a class="sourceLine" id="cb941-10" data-line-number="10"><span class="st">&quot;</span>  </a>
<a class="sourceLine" id="cb941-11" data-line-number="11">model =<span class="st"> </span><span class="kw">jags.model</span>(<span class="kw">textConnection</span>(modelstring), </a>
<a class="sourceLine" id="cb941-12" data-line-number="12">                   <span class="dt">data=</span>model.data, <span class="dt">n.chains=</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## Compiling model graph
##    Resolving undeclared variables
##    Allocating nodes
## Graph information:
##    Observed stochastic nodes: 4
##    Unobserved stochastic nodes: 6
##    Total graph size: 24
## 
## Initializing model</code></pre>

<p><strong>Third</strong>, with the generated model, let us go through a <strong>burn-in</strong> stage. This stage is a preliminary <strong>warm-up</strong> stage, only hoping that our Markov Chain gets closer to a higher probability.</p>

<div class="sourceCode" id="cb943"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb943-1" data-line-number="1"><span class="co"># Burn-In</span></a>
<a class="sourceLine" id="cb943-2" data-line-number="2"><span class="kw">update</span>(model, <span class="dv">2000</span>)  </a></code></pre></div>

<p><strong>Fourth</strong>, start the <strong>sampling</strong> process and monitor the <strong>prior</strong> hyperparameters: <span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\frac{1}{\sigma^2_0}\)</span>.</p>

<div class="sourceCode" id="cb944"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb944-1" data-line-number="1"><span class="co"># Iteration 5000 times for the mu, mu0 and sigma0 samples.</span></a>
<a class="sourceLine" id="cb944-2" data-line-number="2">cs &lt;-<span class="st"> </span><span class="kw">coda.samples</span>(model, <span class="kw">c</span>(  <span class="st">&quot;mu&quot;</span>, <span class="st">&quot;mu0&quot;</span>, <span class="st">&quot;sigma0&quot;</span> ), <span class="dt">n.iter=</span><span class="dv">5000</span>)</a></code></pre></div>

<p><strong>Fifth</strong>, let us review a summary of the parameters. Here, we obtain statistics of the parameter estimates. It allows us to review the mean, standard deviation, standard error of (<span class="math inline">\(\mu_0\)</span> and <span class="math inline">\(\sigma^2_0\)</span>) hyperparameters and (<span class="math inline">\(\mu\)</span>) parameter for our prior. Notice that the unknown mean across the four samplings has values between 13 and 15 with a standard error of 0.019.</p>

<div class="sourceCode" id="cb945"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb945-1" data-line-number="1"><span class="co"># Summary for the mu, mu0, sigma0 samples</span></a>
<a class="sourceLine" id="cb945-2" data-line-number="2"><span class="kw">summary</span>(cs)</a></code></pre></div>
<pre><code>## 
## Iterations = 3001:8000
## Thinning interval = 1 
## Number of chains = 2 
## Sample size per chain = 5000 
## 
## 1. Empirical mean and standard deviation for each variable,
##    plus standard error of the mean:
## 
##        Mean    SD Naive SE Time-series SE
## mu[1]  14.0  1.54   0.0154         0.0216
## mu[2]  14.2  1.41   0.0141         0.0193
## mu[3]  15.0  1.50   0.0150         0.0192
## mu[4]  15.5  1.55   0.0155         0.0235
## mu0    14.5  3.04   0.0304         0.0915
## sigma0 36.0 68.82   0.6882         7.1510
## 
## 2. Quantiles for each variable:
## 
##          2.5%   25%   50%  75% 97.5%
## mu[1]  10.870 12.94 14.04 15.0  16.9
## mu[2]  11.349 13.25 14.19 15.1  16.9
## mu[3]  12.074 14.02 15.00 16.0  18.0
## mu[4]  12.519 14.40 15.41 16.4  18.6
## mu0     7.638 13.39 14.62 15.9  20.5
## sigma0  0.169  2.87  9.31 34.4 262.5</code></pre>

<p><strong>Sixth</strong>, we use <strong>Gelman-Rubin</strong> diagnostic tool to analyze convergence. The first diagram plots the iterations taken by JAGS for each of the sampled <span class="math inline">\(\mu\)</span>. Notice that each posterior sampling of <span class="math inline">\(\mu\)</span> consistently stays somewhere around 15 through the iterations from 2000 to 5000, accompanied by the corresponding density. </p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gelmanrubin1-1"></span>
<img src="DS_files/figure-html/gelmanrubin1-1.png" alt="Gelman-Rubin (Trace and Density)" width="100%" />
<p class="caption">
Figure 8.36: Gelman-Rubin (Trace and Density)
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gelmanrubin1-2"></span>
<img src="DS_files/figure-html/gelmanrubin1-2.png" alt="Gelman-Rubin (Trace and Density)" width="100%" />
<p class="caption">
Figure 8.37: Gelman-Rubin (Trace and Density)
</p>
</div>

<p>Reviewing the <strong>Potential scare reduction factors</strong>, the parameters <span class="math inline">\(\mu\)</span>s have a point estimate equal to or close to 1 and a confidence interval equal to or close to 1, indicating that convergence is sufficiently met.</p>

<div class="sourceCode" id="cb947"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb947-1" data-line-number="1"><span class="kw">gelman.diag</span>(cs, <span class="dt">autoburnin=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## Potential scale reduction factors:
## 
##        Point est. Upper C.I.
## mu[1]           1          1
## mu[2]           1          1
## mu[3]           1          1
## mu[4]           1          1
## mu0             1          1
## sigma0          1          1
## 
## Multivariate psrf
## 
## 1</code></pre>

<p>To complement that, Figure <a href="8.2-simulation-and-sampling.html#fig:gelmanrubin2">8.38</a> illustrates convergence to 1 of two chains for each of the parameters <span class="math inline">\(\mu\)</span>s.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gelmanrubin2"></span>
<img src="DS_files/figure-html/gelmanrubin2-1.png" alt="Gelman-Rubin (Convergence)" width="100%" />
<p class="caption">
Figure 8.38: Gelman-Rubin (Convergence)
</p>
</div>

<p><strong>Alternatively</strong>, we can manually plot the <strong>parameter</strong> estimates and the <strong>hyperparameter</strong> estimates for the prior:</p>

<div class="sourceCode" id="cb949"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb949-1" data-line-number="1"><span class="co"># Showing Densities</span></a>
<a class="sourceLine" id="cb949-2" data-line-number="2">result =<span class="st"> </span><span class="kw">as.matrix</span>(cs)</a>
<a class="sourceLine" id="cb949-3" data-line-number="3"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb949-4" data-line-number="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>sampling_size) {</a>
<a class="sourceLine" id="cb949-5" data-line-number="5">  <span class="kw">plot</span>(<span class="kw">density</span>(result[, <span class="kw">c</span>(<span class="kw">paste0</span>(<span class="st">&quot;mu[&quot;</span>,i,<span class="st">&quot;]&quot;</span>))]),  </a>
<a class="sourceLine" id="cb949-6" data-line-number="6">       <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;Density of mu[&quot;</span>,i,<span class="st">&quot;]&quot;</span>))</a>
<a class="sourceLine" id="cb949-7" data-line-number="7">}</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:jagsmodel-1"></span>
<img src="DS_files/figure-html/jagsmodel-1.png" alt="Jags Result" width="80%" />
<p class="caption">
Figure 8.39: Jags Result
</p>
</div>
<div class="sourceCode" id="cb950"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb950-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb950-2" data-line-number="2"><span class="kw">plot</span>(<span class="kw">density</span>(result[, <span class="kw">c</span>(<span class="st">&quot;mu0&quot;</span>)]), <span class="dt">main=</span><span class="st">&quot;Density of mu0&quot;</span>)</a>
<a class="sourceLine" id="cb950-3" data-line-number="3"><span class="kw">plot</span>(<span class="kw">density</span>(result[, <span class="kw">c</span>(<span class="st">&quot;sigma0&quot;</span>)]), <span class="dt">main=</span><span class="st">&quot;Density of sigma0&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:jagsmodel-2"></span>
<img src="DS_files/figure-html/jagsmodel-2.png" alt="Jags Result" width="80%" />
<p class="caption">
Figure 8.40: Jags Result
</p>
</div>

<p>We can now reparameterize our prior mean and generate our posterior by obtaining the ten means for the corresponding ten samples. If we are to monitor <span class="math inline">\(X\)</span>, we also should be able to capture the sampling densities:</p>

<div class="sourceCode" id="cb951"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb951-1" data-line-number="1"><span class="co"># Iteration 5000 times for the X samples.</span></a>
<a class="sourceLine" id="cb951-2" data-line-number="2">cs &lt;-<span class="st"> </span><span class="kw">coda.samples</span>(model, <span class="kw">c</span>(  <span class="st">&quot;X&quot;</span> ), <span class="dt">n.iter=</span><span class="dv">5000</span>)</a>
<a class="sourceLine" id="cb951-3" data-line-number="3">result =<span class="st"> </span><span class="kw">as.matrix</span>(cs)</a>
<a class="sourceLine" id="cb951-4" data-line-number="4">samplings =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;X1&quot;</span> =<span class="st"> </span><span class="kw">mean</span>( result[, <span class="kw">c</span>(<span class="st">&quot;X[1]&quot;</span>)]), </a>
<a class="sourceLine" id="cb951-5" data-line-number="5">              <span class="st">&quot;X2&quot;</span> =<span class="st"> </span><span class="kw">mean</span>( result[, <span class="kw">c</span>(<span class="st">&quot;X[2]&quot;</span>)]),</a>
<a class="sourceLine" id="cb951-6" data-line-number="6">  <span class="st">&quot;X3&quot;</span> =<span class="st"> </span><span class="kw">mean</span>( result[, <span class="kw">c</span>(<span class="st">&quot;X[3]&quot;</span>)]), <span class="st">&quot;X4&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(result[, <span class="kw">c</span>(<span class="st">&quot;X[4]&quot;</span>)]))</a></code></pre></div>

<p>Therefore, our estimated likelihood for the first sampling is:</p>

<div class="sourceCode" id="cb952"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb952-1" data-line-number="1">samplings</a></code></pre></div>
<pre><code>##    X1    X2    X3    X4 
## 13.65 13.95 15.12 15.86</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="8.1-bayesian-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="8.3-bayesian-analysis.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
