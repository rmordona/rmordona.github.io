<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Bibliography | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Bibliography | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Bibliography | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="appendix-d.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bibliography" class="section level1 unnumbered hasAnchor">
<h1>Bibliography<a href="bibliography.html#bibliography" class="anchor-section" aria-label="Anchor link to header"></a></h1>

<div id="refs">
<div id="ref-ref780t">
<p>Abebe, T. H. 2020. “The Derivation and Choice of Approppriate Test Statistic (Z, T, F and Chi-Square Test) in Research Methodology.” <em>European Journal of Statistics and Probability, Vol. 8, No.1, Pp. 60-73, April 2020</em>. Department of Economics, Ambo University, Ethiopia: ECRTD-UK.</p>
</div>
<div id="ref-ref1322r">
<p>Adams, R. A. 1995. “Calculus of Several Variables.” <em>3rd Edition</em>.</p>
</div>
<div id="ref-ref260a">
<p>Agresti, A., Franklin, C., Klingenberg, B., &amp; Posner, M. 2017. <em>Statistics: The Art and Science of Learning from Data</em>. <em>4th Edition</em>. Pearson.</p>
</div>
<div id="ref-ref1020m">
<p>Alfaki, M. 2008. “Improving Efficiency in Parameter Estimation Using the Hamiltonian Monte Carlo Algorithm.” A thesis (MSc): University of Bergen, Department of Informatics. <a href="http://www.ii.uib.no/~mohammeda/publications/alfakithesis.pdf">http://www.ii.uib.no/~mohammeda/publications/alfakithesis.pdf</a>.</p>
</div>
<div id="ref-ref166d">
<p>An, D. 2009. “Understanding Krylov Subspace Methods.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=UgyLaAXqlQ4">https://www.youtube.com/watch?v=UgyLaAXqlQ4</a>.</p>
</div>
<div id="ref-ref2159a">
<p>Ang, A. 2014. “Gauss-Markov Theorem for Ols Is the Best Linear Unbiased Estimator.” <a href="https://angms.science/doc/Regression/Regression_3_GaussMarkov.pdf">https://angms.science/doc/Regression/Regression_3_GaussMarkov.pdf</a>.</p>
</div>
<div id="ref-ref1446d">
<p>Anggraeni, D., Sanjaya, W. S. M., Nurasyidiek, M. Y. S., &amp; Munawwaroh, M. 2018. “The Implementation of Speech Recognition Using Mel-Frequency Cepstrum Coefficients (Mfcc) and Support Vector Machine (Svm) Method Based on Python Ton Control Robot Arm.” <em>IOP Conf. Series: Materials Science and Engineering 288 (2018) 012042 Doi:10.1088/1757-899X/288/1/012042</em>. <a href="https://iopscience.iop.org/article/10.1088/1757-899X/288/1/012042/pdf">https://iopscience.iop.org/article/10.1088/1757-899X/288/1/012042/pdf</a>.</p>
</div>
<div id="ref-ref39e">
<p>Anley, E. F. 2016. “The Qr Method for Determining All Eigenvalues of Real Square Matrices.” <em>Pure and Applied Mathematics Journal. Volume 5, Issue 4, August 2016 , Pp. 113-119. Doi: 10.11648/J.pamj.2016050</em>.</p>
</div>
<div id="ref-ref809f">
<p>Ardelean, F. A. 2017. “Case Study Using Analysis of Variance to Determine Group’s Variations.” MATEC Web of Converence 126, 04008 (2017), Annual Session of Scientific Papers IMT ORADEA 2017. <a href="DOI: 10.1051/matecconf/201712604008">DOI: 10.1051/matecconf/201712604008</a>.</p>
</div>
<div id="ref-ref874r">
<p>Armstrong, R. A. 2014. “When to Use the Bonferroni Correction.” <em>The Journal of the College of Optometrists</em>. School of Life; Health Sciences, Aston University, Birmham, UK. <a href="https://doi.org/10.1111/opo.12131">https://doi.org/10.1111/opo.12131</a>.</p>
</div>
<div id="ref-ref546j">
<p>Arreola, J. 2018. “Variational Gaussian Mixtures for Face Detection.” <a href="https://jean9208.github.io/vgmm_fd/">https://jean9208.github.io/vgmm_fd/</a>.</p>
</div>
<div id="ref-ref288k">
<p>Atkinson, K. E. 1989. <em>An Introduction to Numerical Analysis</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-ref1359j">
<p>Baarsch, J., &amp; Celebi, M. E. 2012. “Investigation of Internal Validty Measures for K-Means Clustering.” <em>Proceedings of the International MultiConference of Engineers and Computer Scientists 2012, Vol I, IMECS 2012, Mar 14-16, 2012, Hong Kong</em>. <a href="http://www.iaeng.org/publication/IMECS2012/IMECS2012_pp471-476.pdf">http://www.iaeng.org/publication/IMECS2012/IMECS2012_pp471-476.pdf</a>.</p>
</div>
<div id="ref-ref1317d">
<p>Bahdanau, D., Cho, K., &amp; Bengio, Y. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” <em>Conference Paper at ICLR 2015</em>. <a href="https://arxiv.org/pdf/1409.0473.pdf">https://arxiv.org/pdf/1409.0473.pdf</a>.</p>
</div>
<div id="ref-ref26z">
<p>Bai, Z., Demmel, J., Dongarra, J., Ruhe, A., &amp; van der Vorst, H. 2000. “Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide.” <em>SIAM, Philadelphia, 2000</em>. <a href="https://doi.org/10.1137/1.9780898719581">https://doi.org/10.1137/1.9780898719581</a>.</p>
</div>
<div id="ref-ref1603d">
<p>Bailey, D. H. 2021. “A catalogue of mathematical formulas involving <span class="math inline">\(\pi\)</span> with analysis.” <a href="https://www.davidhbailey.com/dhbpapers/pi-formulas.pdf">https://www.davidhbailey.com/dhbpapers/pi-formulas.pdf</a>.</p>
</div>
<div id="ref-ref554k">
<p>Baker, K. 2013. “Singular Value Decomposition Tutorial.” <em>March 29 2005 (Revised January 14, 2013)</em>. <a href="https://datajobs.com/data-science-repo/SVD-Tutorial-[Kirk-Baker].pdf">https://datajobs.com/data-science-repo/SVD-Tutorial-[Kirk-Baker].pdf</a>.</p>
</div>
<div id="ref-imager">
<p>Barthelme, S. 2020. <em>Imager: Image Processing Library Based on ’Cimg’</em>. <a href="https://CRAN.R-project.org/package=imager">https://CRAN.R-project.org/package=imager</a>.</p>
</div>
<div id="ref-Matrix">
<p>Bates, D., &amp; Maechler, M. 2021. <em>Matrix: Sparse and Dense Matrix Classes and Methods</em>. <a href="https://CRAN.R-project.org/package=Matrix">https://CRAN.R-project.org/package=Matrix</a>.</p>
</div>
<div id="ref-ref1227y">
<p>Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. 2003. “A Neural Probabilistic Language Model.” <em>Machine Learning Research 3 (2003), 1137-1155</em>. <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a>.</p>
</div>
<div id="ref-ref884y">
<p>Benjamini, Y., &amp; Hochberg, Y. 1994. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” <em>Journal of the Royal Statistical Society. B (1995), 57, No.1, Pp. 289-300</em>. Tel Aviv University, Israel. <a href="https://doi.org/10.1111/opo.12131">https://doi.org/10.1111/opo.12131</a>.</p>
</div>
<div id="ref-ref2060f">
<p>Berhane, F. n.d. “Building Your Recurrent Neural Network - Step by Step.” <a href="https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html">https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html</a>.</p>
</div>
<div id="ref-ref482c">
<p>Bishop, C. M. 2006. <em>Pattern Recognition and Machine Learning</em>. <em>Corrected Printing 2009</em>. Springer.</p>
</div>
<div id="ref-ref866b">
<p>Blei, D. M. 2012. “Probabilistic Topic Models.” In <em>Surveying a Suite of Algorithms That Offer a Solution to Managing Large Document Archives</em>. <a href="http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf">http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf</a>.</p>
</div>
<div id="ref-ref393d">
<p>———. n.d. “Variational Inference.” <em>Princeton lecture</em>. <a href="https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf">https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf</a>.</p>
</div>
<div id="ref-ref381d">
<p>Blei, D. M., Kucukelbir A., &amp; McAuliffe, J. D. 2017. “Variational Inference: A Review for Statisticians.” <em>American Statistical Association, 112:518, 859-877</em>. <a href="http://dx.doi.org/10.1080/01621459.2017.1285773">http://dx.doi.org/10.1080/01621459.2017.1285773</a>.</p>
</div>
<div id="ref-ref209d">
<p>Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. 2003. “Latent Dirichlet Allocation.” <em>Machine Learning Research 3 (2003) 993-1022</em>. <a href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf</a>.</p>
</div>
<div id="ref-ref3451d">
<p>Bolotov, D., Jena, T., Williams, G., Lin, W-C., Michael, H., Hahsler, Ishwaran, H., Kogalur, B. U., &amp; Guha, R. 2021. <em>Pmml: Generate Pmml for Various Models</em>. <a href="https://CRAN.R-project.org/package=pmml">https://CRAN.R-project.org/package=pmml</a>.</p>
</div>
<div id="ref-ref1048b">
<p>Borchers, B. 2001. “The Partial Autocorrelation Function.” <a href="http://www.ees.nmt.edu/outside/courses/GEOP505/Docs/pac.pdf">http://www.ees.nmt.edu/outside/courses/GEOP505/Docs/pac.pdf</a>.</p>
</div>
<div id="ref-SnowballC">
<p>Bouchet-Valat, M. 2020. <em>SnowballC: Snowball Stemmers Based on the c ’Libstemmer’ Utf-8 Library</em>. <a href="https://CRAN.R-project.org/package=SnowballC">https://CRAN.R-project.org/package=SnowballC</a>.</p>
</div>
<div id="ref-ref488">
<p>Breiman, L. 1994. “Bagging Predictors.” <a href="https://www.stat.berkeley.edu/~breiman/bagging.pdf">https://www.stat.berkeley.edu/~breiman/bagging.pdf</a>.</p>
</div>
<div id="ref-ref499">
<p>———. 2001. “Random Forests.” <em>Machine Learning 45, 5–32</em>. <a href="https://doi.org/10.1023/A:1010933404324">https://doi.org/10.1023/A:1010933404324</a>.</p>
</div>
<div id="ref-ref930h">
<p>Bruyninckx, H. 2002. “Bayesian Probability.” <em>Department of Mechanical Engineering, K.U. Leuven Belgium</em>. <a href="https://people.cs.kuleuven.be/~danny.deschreye/urks2to4_text.pdf">https://people.cs.kuleuven.be/~danny.deschreye/urks2to4_text.pdf</a>.</p>
</div>
<div id="ref-arulesSequences">
<p>Buchta, C., Hahsler, M., &amp; Diaz, D. 2020. <em>ArulesSequences: Mining Frequent Sequences</em>. <a href="https://CRAN.R-project.org/package=arulesSequences">https://CRAN.R-project.org/package=arulesSequences</a>.</p>
</div>
<div id="ref-ref1138m">
<p>Bugallo, M. F., Martino, L., Elvira, V., &amp; Luengo, D. 2017. “Adaptive Importance Sampling: The Past, the Present, and the Future.” <em>IEEE Signal Processing Magazine, Vol. 34, No. 4, Pp. 60-79, July 2017, Doi: 10.1109/MSP.2017.2699226.</em> <a href="https://ieeexplore.ieee.org/document/7974876">https://ieeexplore.ieee.org/document/7974876</a>.</p>
</div>
<div id="ref-ref196r">
<p>Burden, R. L., Faires, D. J., &amp; Burden, A. M. 2005. <em>Numerical Analysis</em>. (10th Edition, print 2016). CENGAGE Learning.</p>
</div>
<div id="ref-ref1011n">
<p>Burke, N. 2018. “Metropolis, Metropolis-Hastings and Gibbs Sampling Algorithms.” Lakehead University Thunder Bay, Ontario. <a href="https://www.lakeheadu.ca/sites/default/files/uploads/77/Burke.pdf">https://www.lakeheadu.ca/sites/default/files/uploads/77/Burke.pdf</a>.</p>
</div>
<div id="ref-ref1104a">
<p>Cameron, A. C., &amp; Windmeijer, F. A.G.. 1995. “R-Squared Measures for Count Data Regression Models with Applications to Health Care Utilization.” <em>Journal of Business and Economic Statistics (forthcoming)</em>. <a href="http://cameron.econ.ucdavis.edu/research/jbes96preprint.pdf">http://cameron.econ.ucdavis.edu/research/jbes96preprint.pdf</a>.</p>
</div>
<div id="ref-ref468p">
<p>CE 108, University of Southern California. n.d. “Numerical Analysis, Abscissas and Weight Factors for Gaussian Integration.” <a href="http://www-classes.usc.edu/engr/ce/108/gauss_weights.pdf">http://www-classes.usc.edu/engr/ce/108/gauss_weights.pdf</a>.</p>
</div>
<div id="ref-ref389j">
<p>Chacón, J. E., &amp; Duong, T. 2018. “Multivariate Kernel Smoothing and Its Applications.” Taylor &amp; Francis Group, LLC, CRS Press.</p>
</div>
<div id="ref-ref865y">
<p>Chan, Y., &amp; Walmsley, R. P. 1997. “Learning and Understanding the Kruskal-Wallis One-Way Analysis-of-Variance-by-Ranks Test for Differences Among Three or More Independent Groups.” <em>Physical Therapy, Volume 77:1755-1762</em>. <a href="https://doi.org/10.1093/ptj/77.12.1755">https://doi.org/10.1093/ptj/77.12.1755</a>.</p>
</div>
<div id="ref-ref657w">
<p>Chao, W-L., Solomon, J., Michels, D. L., &amp; Sha, F. 2015. “Exponential Integration for Hamiltonian Monte Carlo.” Department of Computer Science, University of South California, Los Angeles, CA 90089. <a href="https://people.csail.mit.edu/jsolomon/assets/exponential_hmc.pdf">https://people.csail.mit.edu/jsolomon/assets/exponential_hmc.pdf</a>.</p>
</div>
<div id="ref-ref1242l">
<p>Chen, L., Yuan, F., Jose, J. M., &amp; Zhang, W. 2018. “Improving Negative Sampling for Word Representation using Self-embedded Features.” <em>The 11th International Conference on Web Searching and Data Mining (WSDM 2018), Los Angeles, CA, USA, 05-09 Feb 2018, pp. 99-107. ISBN 9781450355810</em>. <a href="https://dl.acm.org/doi/10.1145/3159652.3159695">https://dl.acm.org/doi/10.1145/3159652.3159695</a>.</p>
</div>
<div id="ref-ref894s">
<p>Chen, S-Y., Feng, Z., &amp; Yi, X. 2017. “A General Introduction to Adjustment for Multiple Comparisons.” <em>J Thorac Dis. 2017 Jun; 9(6): 1725–1729</em>. <a href="https://dx.doi.org/10.21037%2Fjtd.2017.05.34">https://dx.doi.org/10.21037%2Fjtd.2017.05.34</a>.</p>
</div>
<div id="ref-ref542t">
<p>Chen, T., Fox, E. B., &amp; Guestrin, C. 2014. “Stochastic Gradient Hamiltonian Monte Carlo.” <em>MODE Lab, University of Washington, Seattle, WA.</em> <a href="https://arxiv.org/pdf/1402.4102v2.pdf">https://arxiv.org/pdf/1402.4102v2.pdf</a>.</p>
</div>
<div id="ref-xgboost">
<p>Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y., &amp; Li, Y. 2020. <em>Xgboost: Extreme Gradient Boosting</em>. <a href="https://CRAN.R-project.org/package=xgboost">https://CRAN.R-project.org/package=xgboost</a>.</p>
</div>
<div id="ref-ref381l">
<p>Cheruiyot, L. R., Orwa, G. O., &amp; Otieno, O. R. 2020. “Kernel Function and Nonparametric Regression Estimation: Which Function Is Appropriate?” <em>African Journal of Mathematics and Statistics Studies, ISSN: 2689-5323, Volume 3, Issue 3, 2020 (Pp.51-59)</em>.</p>
</div>
<div id="ref-ref1173k">
<p>Cho, K., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. 2014. “Learning Phase Representations using RNN Encoder-Decoder for Statistical Machine Translation.” <a href="https://arxiv.org/pdf/1406.1078v3.pdf">https://arxiv.org/pdf/1406.1078v3.pdf</a>.</p>
</div>
<div id="ref-ref1158j">
<p>Chung, J., Gulcehre, C., Cho, K., &amp; Bengio, Y. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” <a href="https://arxiv.org/pdf/1412.3555.pdf">https://arxiv.org/pdf/1412.3555.pdf</a>.</p>
</div>
<div id="ref-ref373c">
<p>Cleveland, W. S., &amp; Devlin, S. J. 1988. “Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.” <em>Journal of the American Statistical Association, 83(403), 596–610. Https://Doi.org/10.2307/2289282</em>.</p>
</div>
<div id="ref-ref2976c">
<p>Collings, I. 2021. “How Are the Fourier Series, Fourier Transform, Dtft, Dft, Fft, Lt, and Zt Related.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=hF72sY70_IQ">https://www.youtube.com/watch?v=hF72sY70_IQ</a>.</p>
</div>
<div id="ref-ref1039t">
<p>Cover, T. M., &amp; Thomas, J. A. 2006. “Elements of Information Theory.” John Wiley &amp; Sons, Inc., Hoboken, New Jersey.</p>
</div>
<div id="ref-ref921d">
<p>Cox, D. R. 1972. “Regression Models and Life-Tables.” <em>Journal of the Royal Statistical Society. Series B (Methodological), Vol. 34, No. 2 (1972), 187-220</em>. <a href="https://www.jstor.org/stable/2985181">https://www.jstor.org/stable/2985181</a>.</p>
</div>
<div id="ref-ref966m">
<p>Cox, M., van de Laar, T., &amp; de Vries, B. 2018. “A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms.” Department of Electrical Engineering. Eindhoven University of Technology, PO Box 513, 6500 MB, Eindhoven, the Netherlands. <a href="https://arxiv.org/pdf/1811.03407.pdf">https://arxiv.org/pdf/1811.03407.pdf</a>.</p>
</div>
<div id="ref-ref453b">
<p>Dablander, F. 2018. “A Brief Primer on Variational Inference.” <em>[Web Article] 3rd step Variational Bayes</em>. <a href="https://fabiandablander.com/r/Variational-Inference.html">https://fabiandablander.com/r/Variational-Inference.html</a>.</p>
</div>
<div id="ref-ref326b">
<p>Das, B., &amp; Chakrabarty, D. 2016. “Lagrange’s Interpolation Formula: Representation of Numerical Data by a Polynomial Curve.” <em>International Journal of Mathematics Trends and Technology 34(2):64-72, DOI: 10.14445/22315373/IJMTT-V34P514</em>.</p>
</div>
<div id="ref-ref458p">
<p>Davis, P., &amp; Rabinowitz, P. 1956. “Abscissas and Weights for Gaussian Quadratures of High Order.” <em>Journal of Research of the National Bureau of Standards Vol. 56, 35-37, 1956 RP2645</em>. <a href="https://nvlpubs.nist.gov/nistpubs/jres/56/jresv56n1p35_A1b.pdf">https://nvlpubs.nist.gov/nistpubs/jres/56/jresv56n1p35_A1b.pdf</a>.</p>
</div>
<div id="ref-ref1315s">
<p>Davis, S., &amp; Mermelstein, P. 1980. “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.” <em>IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 4, pp. 357-366, August 1980, doi: 10.1109/TASSP.1980.1163420</em>. <a href="https://doi.org/10.1109/TASSP.1980.1163420">https://doi.org/10.1109/TASSP.1980.1163420</a>.</p>
</div>
<div id="ref-ref608p">
<p>Dawkins, P. 2007. “Linear Algebra.” <a href="http://www.math.utoledo.edu/~melbial2/classes/Linear%20Algebra-2890/LinAlg_Dawkins.pdf">http://www.math.utoledo.edu/~melbial2/classes/Linear%20Algebra-2890/LinAlg_Dawkins.pdf</a>.</p>
</div>
<div id="ref-ref362c">
<p>De Boor, C. 2002. “On Calculating with B-Splines.” <em>Journal of Approximation Theory 6, 50-62 (1972)</em>. Division of Mathematical Sciences, Purdue University, Lafayette, Indiana, 47907: Academmic Press, Inc. <a href="https://web.stanford.edu/class/cme324/classics/deboor.pdf">https://web.stanford.edu/class/cme324/classics/deboor.pdf</a>.</p>
</div>
<div id="ref-ref447d">
<p>Deetoher. 2013. “Conjugate Prior Poisson.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=2mjbmvHPAww">https://www.youtube.com/watch?v=2mjbmvHPAww</a>.</p>
</div>
<div id="ref-ref764a">
<p>de Groot, A. D. 2006. “The Meaning of Significance for Different Types of Research.” <em>Translated and Annotated by Eric-Jan Wagenmakers, et Al.</em></p>
</div>
<div id="ref-ref984f">
<p>Dellaert, F. 2021. “Factor Graphs: Exploiting Structure in Robotics.” School of Interactive Computing, Georgia Institute of Technology, Atlanta, Georgia 30332; Google AI, Mountain View, California 94043, USA. <a href="https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-061520-010504">https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-061520-010504</a>.</p>
</div>
<div id="ref-ref975f">
<p>Dellaert, F., &amp; Kaess, M. 2017. “Factor Graphs for Robot Perception.” Georgia Institute of Technology; Carnegie Mellon niversity. <a href="https://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf">https://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf</a>.</p>
</div>
<div id="ref-agricolae">
<p>de Mendiburu, F. 2020. <em>Agricolae: Statistical Procedures for Agricultural Research</em>. <a href="https://CRAN.R-project.org/package=agricolae">https://CRAN.R-project.org/package=agricolae</a>.</p>
</div>
<div id="ref-ref2350a">
<p>Dey, A. n.d. “Bipartite Graph.” <a href="{https://www.thealgorists.com/Algo/Bipartite}">{https://www.thealgorists.com/Algo/Bipartite}</a>.</p>
</div>
<div id="ref-datatable">
<p>Dowle, M. &amp; Srinivasan, A. 2020. <em>Data.table: Extension of ‘Data.frame‘</em>. <a href="https://CRAN.R-project.org/package=data.table">https://CRAN.R-project.org/package=data.table</a>.</p>
</div>
<div id="ref-ref1057t">
<p>Dozat, T. 2016. “Incorporating Nesterov Momentum into Adam.” <em>Workshop Track - ICLR 2016</em>. <a href="https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf">https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf</a>.</p>
</div>
<div id="ref-ref175t">
<p>Driscoll, T. 2012. “Krylov Subspaces.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=ji__O4deIZo">https://www.youtube.com/watch?v=ji__O4deIZo</a>.</p>
</div>
<div id="ref-ref536w">
<p>Driscoll, T. A., &amp; Braun, R. J. 2020. “Fundamentals of Numerical Computation.” <em>Society of Applied and Industrial Mathematics, 2017</em>. <a href="https://fncbook.github.io/fnc/frontmatter.html">https://fncbook.github.io/fnc/frontmatter.html</a>.</p>
</div>
<div id="ref-HMM">
<p>Dr. Lin Himmelmann et al. 2010. <em>HMM: HMM - Hidden Markov Models</em>. <a href="https://CRAN.R-project.org/package=HMM">https://CRAN.R-project.org/package=HMM</a>.</p>
</div>
<div id="ref-ref589h">
<p>Drucker, H. 1997. “Improving Regressors Using Boosting Techniques.” Proceedings of the 14th International Conference on Machine Learning. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&amp;rep=rep1&amp;type=pdf</a>.</p>
</div>
<div id="ref-ref1013d">
<p>Duchi, J., Hazan, E., &amp; Singer, Y. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” <em>JMLR, 12:2121-2159</em>. <a href="https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf">https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf</a>.</p>
</div>
<div id="ref-ref867d">
<p>Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, S., &amp; Harshman, R. 1988. “Using Latent Semantic Analysis to Improve Information Retrieval.” In <em>CHI ’88: Proceedings of the SIGCHI Conference on Human Factors in Computing SystemsMay 1988 Pages 281–285</em>. <a href="https://doi.org/10.1145/57167.57214">https://doi.org/10.1145/57167.57214</a>.</p>
</div>
<div id="ref-ref1144v">
<p>Dumoulin, V., &amp; Visin, F. 2018. “A guide to convolution arithmetic for deep learning.” <a href="https://arxiv.org/pdf/1603.07285.pdf">https://arxiv.org/pdf/1603.07285.pdf</a>.</p>
</div>
<div id="ref-ks">
<p>Duong, T. 2020. <em>Ks: Kernel Smoothing</em>. <a href="https://CRAN.R-project.org/package=ks">https://CRAN.R-project.org/package=ks</a>.</p>
</div>
<div id="ref-ref1029n">
<p>Ebrahimi, N., Soofi, E. S., &amp; Soyer, R. 2010. “Information Measures in Perspective.” <em>Journal Compilation (2010) International Statistical Institute.</em> Blackwell Publishing Ltd. <a href="https://business.gwu.edu/sites/g/files/zaxdzs1611/f/downloads/Department_decision_sciences_Publicaiton_Refik-soyer_Information.pdf">https://business.gwu.edu/sites/g/files/zaxdzs1611/f/downloads/Department_decision_sciences_Publicaiton_Refik-soyer_Information.pdf</a>.</p>
</div>
<div id="ref-ref207c">
<p>Edwards, C. H., Penney, D. E., &amp; Calvis, D. T. 2018. <em>Differential Equations and Linear Algebra</em>. (4th Edition). Pearson.</p>
</div>
<div id="ref-ref121e">
<p>Ehiwario, J. C. 2014. “Comparative Study of Bisection, Newton-Raphson and Secant Methods of Root- Finding Problems.” <em>IOSR Journal of Engineering. 4. 01-07. 10.9790/3021-04410107</em> 4. <a href="http://www.iosrjen.org/Papers/vol4_issue4%20(part-1)/A04410107.pdf">http://www.iosrjen.org/Papers/vol4_issue4%20(part-1)/A04410107.pdf</a>.</p>
</div>
<div id="ref-ref1210r">
<p>Ekeocha, R. J. O., Uzor, C. &amp; Anetor, C. 2018. “The Use of the Duality Principle to Solve Optimization Problems.” <em>Int. J. Recent Contributions Eng. Sci. IT</em> 6: 33–42.</p>
</div>
<div id="ref-ref1502o">
<p>Elgabry, O. 2019. “The Ultimate Guide to Data Cleaning.” <em>[Article]</em>. <a href="https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4">https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4</a>.</p>
</div>
<div id="ref-ref1912h">
<p>Elharati, H. 2019. “Performance Evaluation of Speech Recognition System Using Conventional and Hybrid Features and Hidden Markov Model Classifier.” <em>[PhD Dissertation]</em>. College of Engineering; Science of Florida Institute of Technology. <a href="https://repository.lib.fit.edu/bitstream/handle/11141/3024/ELHARATI-DISSERTATION-2019.pdf">https://repository.lib.fit.edu/bitstream/handle/11141/3024/ELHARATI-DISSERTATION-2019.pdf</a>.</p>
</div>
<div id="ref-ref1129v">
<p>Elvira, V.*, &amp; Martino, L.†. 2021. “Advances in Importance Sampling.” *School of Mathematics, University of Edinburgh (United Kingdom), †Universidad Rey Juan Carlos de Madrid (Spain). <a href="https://arxiv.org/pdf/2102.05407.pdf">https://arxiv.org/pdf/2102.05407.pdf</a>.</p>
</div>
<div id="ref-ref2149r">
<p>Engle, R. F. 1984. “Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics.” <em>Handbook of Econometrics, Volume II, Edited by Z. Griliches and M.D. Intriligator</em>. Elsevier Science Publishers BV, 1984. <a href="http://hedibert.org/wp-content/uploads/2014/04/W-LR-LM-Tests-in-Econometrics-Engle1984.pdf">http://hedibert.org/wp-content/uploads/2014/04/W-LR-LM-Tests-in-Econometrics-Engle1984.pdf</a>.</p>
</div>
<div id="ref-ref1111a">
<p>Erraqabi, A., Valko, M., Carpentier, A., &amp; Maillard, O-A. 2016. “Pliable Rejection Sampling.” <em>Proceedings of the 33rd International Conference on MachineLearning, New York, NY, USA, 2016. JMLR: W&amp;CP Volume 48</em>. <a href="http://proceedings.mlr.press/v48/erraqabi16.pdf">http://proceedings.mlr.press/v48/erraqabi16.pdf</a>.</p>
</div>
<div id="ref-ref159m">
<p>Ester, M., Kriegel, H-P., Sander, J., &amp; Xu, X. 1996. <em>A Density-Based Algorithm for Discovering Clusters</em>. <em>KDD-96 Proceedings</em>. <a href="https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf">https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf</a>.</p>
</div>
<div id="ref-wordcloud">
<p>Fellows, I. 2018. <em>Wordcloud: Word Clouds</em>. <a href="https://CRAN.R-project.org/package=wordcloud">https://CRAN.R-project.org/package=wordcloud</a>.</p>
</div>
<div id="ref-ref517j">
<p>Fill, J. A., &amp; Fishkind, D. E. 1998. “The Moore-Penrose Generalized Inverse for Sums of Matrices.” <em>AMS 1991 Subject Classifications. Primary 15A09; Secondary 15A18</em>. The Johns Hopkins University, Baltimore, Maryland; Department of MathematicsandStatistics,University of SouthernMaine,Portland,Maine. <a href="https://www.ams.jhu.edu/~fill/papers/MoorePenrose.pdf">https://www.ams.jhu.edu/~fill/papers/MoorePenrose.pdf</a>.</p>
</div>
<div id="ref-ref437d">
<p>Fink, D. 1997. “A Compendium of Conjugate Priors.” <em>Montana State University, Bozeman, MT 59717</em>. <a href="https://www.johndcook.com/CompendiumOfConjugatePriors.pdf">https://www.johndcook.com/CompendiumOfConjugatePriors.pdf</a>.</p>
</div>
<div id="ref-ref232d">
<p>Forsyth, D. 2018. <em>Probability and Statistics for Computer Science</em>. Springer.</p>
</div>
<div id="ref-ref1048f">
<p>Fox, C.W., &amp; Roberts, S.J. 2012. “A Tutorial on Variational Bayesian Inference.” <em>Artif Intell Rev 38, 85–95 (2012)</em>. <a href="https://doi.org/10.1007/s10462-011-9236-8">https://doi.org/10.1007/s10462-011-9236-8</a>.</p>
</div>
<div id="ref-ref1084d">
<p>Fox, D., Hightower, J., Liao, L., Schulz, D., &amp; Borriello, G. 2003. “Bayesian Filtering for Location Estimation.” University of Washington. <a href="https://rse-lab.cs.washington.edu/postscripts/bayes-filter-pervasive-03.pdf">https://rse-lab.cs.washington.edu/postscripts/bayes-filter-pervasive-03.pdf</a>.</p>
</div>
<div id="ref-ref616y">
<p>Freund, Y., &amp; Schapire, R. E. 1996. “Experiments with a New Boosting Algorithm.” <em>Machine Learning: Proceedings of the Thirteenth International Conference, 1996</em>. <a href="https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf">https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf</a>.</p>
</div>
<div id="ref-ref510">
<p>———. 1999. “A Short Introduction to Boosting.” <em>Journal of Japanese Society for Artificial Intelligence,14(5):771-780,September,1999</em>. <a href="http://rob.schapire.net/papers/Schapire99c.pdf">http://rob.schapire.net/papers/Schapire99c.pdf</a>.</p>
</div>
<div id="ref-ref670j">
<p>Friedman, J. H. 1999a. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>IMS 1999 Reitz Lecture, February 24, 1999, (modified March 15, 2000, April 19, 2001)</em>. <a href="https://statweb.stanford.edu/~jhf/ftp/trebst.pdf">https://statweb.stanford.edu/~jhf/ftp/trebst.pdf</a>.</p>
</div>
<div id="ref-ref621j">
<p>———. 1999b. “Stochastic Gradient Boosting.” <a href="https://jerryfriedman.su.domains/ftp/stobst.pdf">https://jerryfriedman.su.domains/ftp/stobst.pdf</a>.</p>
</div>
<div id="ref-ref677j">
<p>———. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” <em>The Annals of Statistics, Vol. 29, No. 5 (Oct., 2001), pp. 1189-1232, Published by: Institute of Mathematical Statistics</em>. <a href="https://www.jstor.org/stable/2699986?origin=JSTOR-pdf">https://www.jstor.org/stable/2699986?origin=JSTOR-pdf</a>.</p>
</div>
<div id="ref-ref679j">
<p>———. 2002. “Stochastic Gradient Boosting.” <em>Computational Statistics &amp; Data Analysis. 38. 367-378. 10.1016/S0167-9473(01)00065-2</em>. <a href="https://www.researchgate.net/profile/Jerome-Friedman/publication/222573328_Stochastic_Gradient_Boosting">https://www.researchgate.net/profile/Jerome-Friedman/publication/222573328_Stochastic_Gradient_Boosting</a>.</p>
</div>
<div id="ref-ref2039g">
<p>Ganchev, T., Fakotakis, N., &amp; George, K. 2005. “Comparative evaluation of various MFCC implementations on the speaker verification task.” <em>Proceedings of the SPECOM. 1</em>. <a href="https://www.researchgate.net/profile/Todor-Ganchev/publication/228756314_Comparative_evaluation_of_various_MFCC_implementations_on_the_speaker_verification_task">https://www.researchgate.net/profile/Todor-Ganchev/publication/228756314_Comparative_evaluation_of_various_MFCC_implementations_on_the_speaker_verification_task</a>.</p>
</div>
<div id="ref-ref527w">
<p>Gander, W. 1980. “Algorithms for the Qr-Decomposition.” <em>Research Report No. 80-02, SEMINAR FUER ANGEWANDTE MATHEMATIK, CH-8092 Zuerich</em>. <a href="https://people.inf.ethz.ch/gander/papers/qrneu.pdf">https://people.inf.ethz.ch/gander/papers/qrneu.pdf</a>.</p>
</div>
<div id="ref-ref739w">
<p>Gautschi, W. 1983. “On the Convergence Behavior of Continued Fractions with Real Elements.” <em>Mathematics of Computation, Volume 40, Number 161, Pp. 337-342</em>. <a href="https://www.ams.org/journals/mcom/1983-40-161/S0025-5718-1983-0679450-2/S0025-5718-1983-0679450-2.pdf">https://www.ams.org/journals/mcom/1983-40-161/S0025-5718-1983-0679450-2/S0025-5718-1983-0679450-2.pdf</a>.</p>
</div>
<div id="ref-ref1070r">
<p>Ge, R., Kakade, S. M., Kidambi, R., &amp; Netrapalli, P. 2019. “The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure for Least Squares.” <a href="https://arxiv.org/pdf/1904.12838.pdf">https://arxiv.org/pdf/1904.12838.pdf</a>.</p>
</div>
<div id="ref-ref250a">
<p>Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. 2013. <em>Bayesian Data Analysis: Texts in Statistical Science</em>. <em>3rd Edition</em>. CRS Press.</p>
</div>
<div id="ref-ref620a">
<p>Gelman, A., Vehtari, A., Sivula, T., Jylänki, P., Tran, D., Sahai, S., Blomstedt, P., Cunningham, J. P., Schiminovich, D., &amp; Robert, C. 2017. “Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data.” <a href="http://www.stat.columbia.edu/~gelman/research/unpublished/ep_arxiv.pdf">http://www.stat.columbia.edu/~gelman/research/unpublished/ep_arxiv.pdf</a>.</p>
</div>
<div id="ref-ref1537w">
<p>Gharbieh, W. 2018. “Connectionist Temporal Classification, Labelling Unsegmented Sequence Data with Rnn.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=UMxvZ9qHwJs">https://www.youtube.com/watch?v=UMxvZ9qHwJs</a>.</p>
</div>
<div id="ref-ref291a">
<p>Ghosh, A. 2019. “Kalman Filter to Stabilize Sensor Readings.” <a href="https://thecustomizewindows.com/2019/03/kalman-filter-to-stabilize-sensor-readings/">https://thecustomizewindows.com/2019/03/kalman-filter-to-stabilize-sensor-readings/</a>.</p>
</div>
<div id="ref-ref1120g">
<p>Gilks, W. R., &amp; Wild, P. 1992. “Adaptive Rejection Sampling for Gibbs Sampling.” <em>Journal of the Royal Statistical Society. Series C (Applied Statistics), 41(2), 337–348</em>. <a href="https://doi.org/10.2307/2347565">https://doi.org/10.2307/2347565</a>.</p>
</div>
<div id="ref-ref1188x">
<p>Glorot, X., &amp; Bengio Y. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” <a href="http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf">http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf</a>.</p>
</div>
<div id="ref-ref1565i">
<p>Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. 2014. “Generative Adversarial Networks.” <a href="https://arxiv.org/pdf/1406.2661.pdf">https://arxiv.org/pdf/1406.2661.pdf</a>.</p>
</div>
<div id="ref-forestplot">
<p>Gordon, M. &amp; Lumley, T. 2019. <em>Forestplot: Advanced Forest Plot Using ’Grid’ Graphics</em>. <a href="https://CRAN.R-project.org/package=forestplot">https://CRAN.R-project.org/package=forestplot</a>.</p>
</div>
<div id="ref-ref1929a">
<p>Graves, A., Fernández, S., Gomez, F., &amp; Schmmidhuber, J. 2006. “Connectionist Temporal Classifications: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.” <em>ICML ’06: Proceedings of the 23rd international conference on Machine learningJune 2006 Pages 369–376</em>. <a href="https://dl.acm.org/doi/10.1145/1143844.1143891">https://dl.acm.org/doi/10.1145/1143844.1143891</a>.</p>
</div>
<div id="ref-ref1879a">
<p>Graves, A., Mohamed, A-r., &amp; Hinton, G. 2013. “Speech Recognition with Deep Recurrent Neural Networks.” <em>Department of Computer Science, University of Toronto</em>. <a href="https://arxiv.org/pdf/1303.5778.pdf">https://arxiv.org/pdf/1303.5778.pdf</a>.</p>
</div>
<div id="ref-nortest">
<p>Gross, J., &amp; Ligges, U. 2015. <em>Nortest: Tests for Normality</em>. <a href="https://CRAN.R-project.org/package=nortest">https://CRAN.R-project.org/package=nortest</a>.</p>
</div>
<div id="ref-topicmodels">
<p>Grün, B., &amp; Hornik, K. 2011. “topicmodels: An R Package for Fitting Topic Models.” <em>Journal of Statistical Software</em> 40 (13): 1–30. <a href="https://doi.org/10.18637/jss.v040.i13">https://doi.org/10.18637/jss.v040.i13</a>.</p>
</div>
<div id="ref-ref1509a">
<p>Gupta, A. 2021. “The 6 Dimensionf of Data Quality.” <em>[Article]</em>. <a href="https://www.collibra.com/us/en/blog/the-6-dimensions-of-data-quality">https://www.collibra.com/us/en/blog/the-6-dimensions-of-data-quality</a>.</p>
</div>
<div id="ref-ref306w">
<p>Hager, W. W., &amp; Zhang, H. 2005. “A Survey of Nonlinear Conjugate Gradient Methods.” <em>The National Science Foundation Under Grant No. 0203270</em>. <a href="https://www.caam.rice.edu/~zhang/caam454/pdf/cgsurvey.pdf">https://www.caam.rice.edu/~zhang/caam454/pdf/cgsurvey.pdf</a>.</p>
</div>
<div id="ref-ref2166l">
<p>Halliwell, L. J. 2015. “The Gauss-Markov Theorem: Beyond the Blue.” <em>Casualty Actuarial Society E-Forum, Fall 2015</em>. <a href="https://www.casact.org/sites/default/files/database/forum_15fforum_halliwell_gm.pdf">https://www.casact.org/sites/default/files/database/forum_15fforum_halliwell_gm.pdf</a>.</p>
</div>
<div id="ref-ref370_2">
<p>Han, J., Kamber, M., &amp; Pei, J. 2002. <em>Data Mining Concepts and Techniques</em>. Third Edition. Morgan Kaufmann.</p>
</div>
<div id="ref-ref380">
<p>Han, J., Pei, J., Mortazavi-Asl, B., Chen, Q., Dayal, U., &amp; Hsu, M-C. 2000. “FreeSpan: Frequent Pattern-Projected Sequential Pattern Mining.” <em>Intelligent Database Systems Research Lab., Burnaby, B.C., Canada V5A 1S6 &amp; Hewlett-Packard Labs, Palo Alto, California 94303-0969</em>. <a href="https://www.cs.sfu.ca/~jpei/publications/freespan.pdf">https://www.cs.sfu.ca/~jpei/publications/freespan.pdf</a>.</p>
</div>
<div id="ref-ref368">
<p>Han, J., Pei, J., Yin, Y., &amp; Mao, R. 2000. “Mining Frequent Patterns Without Candidate Generation: A Frequent-Pattern Tree Approach.” <em>Data Mining and Knowledge Discovery, 8, 53-87, 2004</em>. Simon Fraser University, Canada: Kluwer Academic Publishers. <a href="http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf">http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf</a>.</p>
</div>
<div id="ref-ref856t">
<p>Harris, T., &amp; Hardin, J. W. 2013. “Exact Wilcoxon Signed-Rank and Wilcoxon Mann-Whitney Ranksum Tests.” <em>The Statat Journal (2013), 13, Number 2, pp. 337-343</em>. <a href="https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300208">https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300208</a>.</p>
</div>
<div id="ref-ref577l">
<p>Hasenclever, L., Webb, S., Lienart, T., Vollmer, S., Lakshminarayanan, B., Blundell, C., &amp; Teh, Y. W. 2017. “Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server.” <em>Machine Learning Research 18 (2017) 1-37</em>. <a href="https://dl.acm.org/doi/pdf/10.5555/3122009.3176850?download=true">https://dl.acm.org/doi/pdf/10.5555/3122009.3176850?download=true</a>.</p>
</div>
<div id="ref-ref269t">
<p>Hastie, T., Tibshirani, R., &amp; Friedman, J. 2016. <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em>. <em>2nd Edition, 11th printing 2016</em>. Springer.</p>
</div>
<div id="ref-ref1355k">
<p>He, K., Zhang, X., Ren, S., &amp; Sun, J. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification.” <a href="https://arxiv.org/pdf/1502.01852.pdf">https://arxiv.org/pdf/1502.01852.pdf</a>.</p>
</div>
<div id="ref-ref187m">
<p>Heath, M. T. 2002. <em>Scientific Computing: An Introductory Survey</em>. (Revised 2nd Edition, 2018). Society for Industrial; Applied Mathematics.</p>
</div>
<div id="ref-ref590j">
<p>Hefferon, J. 2020. “Linear Algebra.” <a href="https://joshua.smcvt.edu/linearalgebra">https://joshua.smcvt.edu/linearalgebra</a>.</p>
</div>
<div id="ref-ref1306h">
<p>Hermansky, H. 1990. “Perceptual Linear Predictive (PLP) analysis of speech.” <em>The Journal of the Acoustical Society of America 87, 1738 (1990)</em>. <a href="https://doi.org/10.1121/1.399423">https://doi.org/10.1121/1.399423</a>.</p>
</div>
<div id="ref-ref1154h">
<p>Hochreiter, S., &amp; Schmidhuber, J. 1997. “Long Short-Term Memory.” <em>Neural ComputationVolume 9, Issue 8, November 15, 1997 Pp 1735–1780</em>. <a href="https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735">https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735</a>.</p>
</div>
<div id="ref-ref220t">
<p>Hofmann, T. 1999. “Probabilistic Latent Semantic Analysis.” <em>EECS Department, Computer Science Division, University of California, Berkeley &amp; International Computer Science Institute, Berkeley, CA</em>. <a href="https://arxiv.org/pdf/1301.6705.pdf">https://arxiv.org/pdf/1301.6705.pdf</a>.</p>
</div>
<div id="ref-ref449p">
<p>Holoborodko, P. 2012. “Abscissas and Weights of Classical Gaussian Quadrature Rules.” <a href="https://www.advanpix.com/2012/05/30/abscissas-and-weights-classical-gaussian-quadrature-rules/">https://www.advanpix.com/2012/05/30/abscissas-and-weights-classical-gaussian-quadrature-rules/</a>.</p>
</div>
<div id="ref-ref929l">
<p>Hong, L. 2012. “Probabilistic Latent Semantic Analysis.” <a href="https://arxiv.org/pdf/1212.3900.pdf">https://arxiv.org/pdf/1212.3900.pdf</a>.</p>
</div>
<div id="ref-ref131t">
<p>Hoppe, T. n.d. “An Application of the Lanczos Method.” Drexel Unniversity. <a href="http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_02.pdf">http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_02.pdf</a>.</p>
</div>
<div id="ref-ref123t">
<p>Hoppe T. n.d. “Lanczos Vector Procedures.” Drexel Unniversity. <a href="http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_01.pdf">http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_01.pdf</a>.</p>
</div>
<div id="ref-openNLPmodelsen">
<p>Hornik, K. 2015. <em>openNLPmodels.en: Apache OpenNLP Models for English</em>.</p>
</div>
<div id="ref-openNLP">
<p>———. 2019. <em>OpenNLP: Apache Opennlp Tools Interface</em>. <a href="https://CRAN.R-project.org/package=openNLP">https://CRAN.R-project.org/package=openNLP</a>.</p>
</div>
<div id="ref-NLP">
<p>———. 2020. <em>NLP: Natural Language Processing Infrastructure</em>. <a href="https://CRAN.R-project.org/package=NLP">https://CRAN.R-project.org/package=NLP</a>.</p>
</div>
<div id="ref-ref939m">
<p>Horny, M. 2014. “Bayesian Networks.” Department of Health Policy; Management, Boston University School of Public Health. <a href="https://www.bu.edu/sph/files/2014/05/bayesian-networks-final.pdf">https://www.bu.edu/sph/files/2014/05/bayesian-networks-final.pdf</a>.</p>
</div>
<div id="ref-ref745c">
<p>Hsieh, C-J., Chang, K-W., Lin, C-J., Keerthi, S. S., &amp; Sundararajan, S. 2008. “A Dual Coordiate Descent Method for Large-scale Linear SVM.” <a href="https://icml.cc/Conferences/2008/papers/166.pdf">https://icml.cc/Conferences/2008/papers/166.pdf</a>.</p>
</div>
<div id="ref-ref580h">
<p>Hssina, B., Merbrouha, A., Hanane, E., &amp; Mohammed, E. 2014. “A Comparative Study of Decision Tree Id3 and C4.5.” <em>(IJACSA) International Journal of Advanced Computer Science and Applications. Special Issue on Advances in Vehicular Ad Hoc Networking and Applications. 10.14569/SpecialIssue.2014.040203.</em> University of Sydney, Sydney Australia 2006. <a href="https://www.researchgate.net/publication/265162251_A_comparative_study_of_decision_tree_ID3_and_C45">https://www.researchgate.net/publication/265162251_A_comparative_study_of_decision_tree_ID3_and_C45</a>.</p>
</div>
<div id="ref-ref191z">
<p>Huang, Z., Liang, D., Xu, P., &amp; Xiang, B. 2020. <em>Improve Transformer Models with Better Relative Position Embeddings</em>. <a href="https://arxiv.org/pdf/2009.13658.pdf">https://arxiv.org/pdf/2009.13658.pdf</a>.</p>
</div>
<div id="ref-fpp">
<p>Hyndman, R. J. 2013. <em>Fpp: Data for &quot;Forecasting: Principles and Practice&quot;</em>. <a href="https://CRAN.R-project.org/package=fpp">https://CRAN.R-project.org/package=fpp</a>.</p>
</div>
<div id="ref-ref1023r">
<p>Ihaka, R. n.d. “A Statistics 726 Course.” <em>Levinson-Durbin Code</em>. <a href="https://www.stat.auckland.ac.nz/~ihaka/courses/726/sol02.pdf">https://www.stat.auckland.ac.nz/~ihaka/courses/726/sol02.pdf</a>.</p>
</div>
<div id="ref-ref818b">
<p>Illowsky, B., Dean, S., et al. 2018. “Introductory Statistics.” Rice University. <a href="https://openstax.org/details/books/introductory-statistics">https://openstax.org/details/books/introductory-statistics</a>.</p>
</div>
<div id="ref-ref628v">
<p>Ivrii, V. 2021. “Partial Differential Equations.” Department of Mathematics, University of Toronto. <a href="http://www.math.toronto.edu/ivrii/PDE-textbook/PDE-textbook.pdf">http://www.math.toronto.edu/ivrii/PDE-textbook/PDE-textbook.pdf</a>.</p>
</div>
<div id="ref-ref638p">
<p>Jakobsen, P. K. 2019. “An Introduction to Partial Differential Equations.” <em>arXiv: 1901.03022v1</em>. Department of Mathematics and Statistics, the Artctic University of Norway, 9019 Tromsø, Norway. <a href="https://arxiv.org/pdf/1901.03022.pdf">https://arxiv.org/pdf/1901.03022.pdf</a>.</p>
</div>
<div id="ref-ref316e">
<p>Jarlebring, E. 2018. “Broyden’s Method for Nonlinear Eigenproblems.” <em>arXiv:1802.07322v1</em>. <a href="https://arxiv.org/pdf/1802.07322.pdf">https://arxiv.org/pdf/1802.07322.pdf</a>.</p>
</div>
<div id="ref-ref1562j">
<p>Joshy, J. &amp; Sambyo, K. 2016. “A Comparison and Contrast of the Various Feature Extraction Techniques in Speaker Recognition.” <em>International Journal of Signal Processing, Image Processing and Pattern Recognition Vol.9, No.11, (2016), pp.99-108</em>. <a href="http://dx.doi.org/10.14257/ijsip.2016.9.11.10">http://dx.doi.org/10.14257/ijsip.2016.9.11.10</a>.</p>
</div>
<div id="ref-ref545d">
<p>Kalman, D. 1996. “A Singularly Valuable Decomposition: The Svd of a Matrix.” <em>THE COLLEGE MATHEMATICS JOURNAL; VOL. 27, NO. 1, JANUARY 1996</em>. <a href="http://dankalman.net/AUhome/pdffiles/svd.pdf">http://dankalman.net/AUhome/pdffiles/svd.pdf</a>.</p>
</div>
<div id="ref-ref1186a">
<p>Karpathy, A. 2015. “The Unreasonable Effectiveness of Recurrent Neural Network.” <em>[Article]</em>. <a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</a>.</p>
</div>
<div id="ref-ref49d">
<p>Karunanithi, S., Gajalakshmi, N., Malarvizhi, M., &amp; Saileshwari, M. 2018. “A Study of the Comparison of Jacobi, Gauss-Seidel and Sor Methods for Th eSolution in System of Linear Equations.” <em>International Journal of Mathematics Trends and Technology (IJMTT) - Volume 56 Issue 4 - April 2018</em>.</p>
</div>
<div id="ref-ref443b">
<p>Keng, B. 2018. “Variational Bayes and the Mean-Field Approximation.” <em>[Web Article] 3rd step Variational Bayes</em>. <a href="http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/">http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/</a>.</p>
</div>
<div id="ref-ref1003d">
<p>Kingma, D. P., &amp; Ba, J. L. 2015. “ADAM: A Method for Stochastic Optimization.” <em>Conference Paper at ICLR 2015</em>. <a href="https://arxiv.org/pdf/1412.6980.pdf">https://arxiv.org/pdf/1412.6980.pdf</a>.</p>
</div>
<div id="ref-ref1015d">
<p>Kohler, D. F. 1982. “The Relation Among the Likelihood Ratio-, Wald-, and Lagrange Multiplier Tests and Their Applicability to Small Samples.” <em>The Rand Corpoation, Santa Monica, California 90406</em>. <a href="https://www.rand.org/content/dam/rand/pubs/papers/2008/P6756.pdf">https://www.rand.org/content/dam/rand/pubs/papers/2008/P6756.pdf</a>.</p>
</div>
<div id="ref-ref1602d">
<p>Korzinek, D. 2021. “How Oes Htk Compute Features?” <em>[Article]</em>. <a href="https://notebook.community/danijel3/PyHTK/python-notebooks/HTKFeaturesExplained">https://notebook.community/danijel3/PyHTK/python-notebooks/HTKFeaturesExplained</a>.</p>
</div>
<div id="ref-ref1350a">
<p>Krizhevsky, A. 2009. “Learning Multiple Layers of Features from Tiny Images.” <a href="https://www.cs.toronto.edu/~kriz/cifar.html">https://www.cs.toronto.edu/~kriz/cifar.html</a>.</p>
</div>
<div id="ref-ref241j">
<p>Kruschke, J. K. 2015. <em>Doing Bayesian Data Analsysi: A Tutorial with R, Jags, and Stan</em>. <em>2nd Edition</em>. Academic Press: an imprint of Elsevier.</p>
</div>
<div id="ref-caret">
<p>Kuhn, M. 2020. <em>Caret: Classification and Regression Training</em>. <a href="https://CRAN.R-project.org/package=caret">https://CRAN.R-project.org/package=caret</a>.</p>
</div>
<div id="ref-ref1174a">
<p>Kumar, A. 2021. “Deep Learning 71: Back-propagation in Gated Recurrent Unit (GRU) Architecture.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=P0W3iHKYOHc">https://www.youtube.com/watch?v=P0W3iHKYOHc</a>.</p>
</div>
<div id="ref-ref790s">
<p>Kwak, S. G., &amp; Kim, J. H. 2016. “Central Limit Theorem: The Cornerstone of Modern Statistics.” Departments of Medical Statistics, Anesthesiology and Pain Medicine, School of Medicine, Catholic University of Daegu, Korea. <a href="https://ekja.org/upload/pdf/kjae-70-144.pdf">https://ekja.org/upload/pdf/kjae-70-144.pdf</a>.</p>
</div>
<div id="ref-ref457b">
<p>Lambart, B. 2014. “Proof: Gamma Prior Is Conjugate to Poisson Likelihood.” <em>[Video, Ox educ channel]</em>. <a href="https://www.youtube.com/watch?v=CBFpqjNZXV0">https://www.youtube.com/watch?v=CBFpqjNZXV0</a>.</p>
</div>
<div id="ref-ref240b">
<p>Lambert, B. 2018. <em>A Student’s Guide to Bayesian Statistics</em>. SAGE.</p>
</div>
<div id="ref-ref299c">
<p>Lanczos, C. 1950. <em>An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators</em>. <em>Research of the National Bureau of Standards, Vol. 45, No. 4, October 1950, Research Paper 2133</em>. <a href="https://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255_a1b.pdf">https://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255_a1b.pdf</a>.</p>
</div>
<div id="ref-ref827m">
<p>Larson, M. G. 2008. “Analysis of Variance: Statistical Primer for Cardiovascular Research.” Department of Mathematics and Statistics, Boston University, Boston: American Heart Association, Inc. <a href="https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.107.654335">https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.107.654335</a>.</p>
</div>
<div id="ref-ref216r">
<p>Larson, R., Edwards, B. H., Falvo, D. C. 2006. <em>Calculus: An Applied Approach</em>. (4th Edition). Houghton Mifflin Company.</p>
</div>
<div id="ref-ref903s">
<p>Lee, S., &amp; Lee, D. K. 2018. “What Is the Proper Way to Apply the Multiple Comparison Test.” <em>Korean J Anesthesiol. 2018 Oct; 71(5): 353–360.</em> <a href="https://dx.doi.org/10.4097%2Fkja.d.18.00242">https://dx.doi.org/10.4097%2Fkja.d.18.00242</a>.</p>
</div>
<div id="ref-ResourceSelection">
<p>Lele, S. R., Keim, J. K., &amp; Solymos, P. 2019. <em>ResourceSelection: Resource Selection (Probability) Functions for Use-Availability Data</em>. <a href="https://CRAN.R-project.org/package=ResourceSelection">https://CRAN.R-project.org/package=ResourceSelection</a>.</p>
</div>
<div id="ref-ref748r">
<p>Lempert, R. O. 2008. “The Significance of Statisistical Significance: Two Authors Restate an Inconrovertible Caution. Why a Book?” University of Michigan Law School.</p>
</div>
<div id="ref-ref475r">
<p>LeVeque, R. J. 2007. “Finite Difference Methods for Ordinary and Partial Differential Equations.” Society for Industrial; Applied Mathematics.</p>
</div>
<div id="ref-ref658a">
<p>Lewis, A. D. 2017. “Introduction to Differential Equations (for Smart Kids).” <a href="https://mast.queensu.ca/~andrew/teaching/pdf/237-notes.pdf">https://mast.queensu.ca/~andrew/teaching/pdf/237-notes.pdf</a>.</p>
</div>
<div id="ref-ref685p">
<p>Li, P. 2012. “Robust Logitboost and Adaptive Base Class (Abc) Logitboost.” <a href="https://arxiv.org/pdf/1203.3491.pdf">https://arxiv.org/pdf/1203.3491.pdf</a>.</p>
</div>
<div id="ref-tuneR">
<p>Ligges, U., Krey, S., Mersmann, O., &amp; Schnackenberg, S. 2018. <em>tuneR: Analysis of Music and Speech</em>. <a href="https://CRAN.R-project.org/package=tuneR">https://CRAN.R-project.org/package=tuneR</a>.</p>
</div>
<div id="ref-ref986s">
<p>Loffe, S., &amp; Szegedy, C. 2019. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” <em>Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP Volume 37</em>. <a href="http://proceedings.mlr.press/v37/ioffe15.pdf">http://proceedings.mlr.press/v37/ioffe15.pdf</a>.</p>
</div>
<div id="ref-ref1328m">
<p>Luong, M-T., Pham, H., &amp; Manning, C. D. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” Stanford University, Stanford, cA 94305. <a href="https://arxiv.org/pdf/1508.04025.pdf">https://arxiv.org/pdf/1508.04025.pdf</a>.</p>
</div>
<div id="ref-ref1450d">
<p>Lyon, D. 2009. “The Discrete Fourier Transform, Part 4: Spectral Leakage.” <em>Journal of Object Technology. Vol. 8, No. 7, November-December 2009</em>. <a href="http://www.jot.fm/issues/issue_2009_11/column2.pdf">http://www.jot.fm/issues/issue_2009_11/column2.pdf</a>.</p>
</div>
<div id="ref-ref1407r">
<p>Lyons, R. 2013. “A Quadrature Signals Tutorial: Complex, but Not Complicated.” <a href="https://www.dsprelated.com/showarticle/192.php">https://www.dsprelated.com/showarticle/192.php</a>.</p>
</div>
<div id="ref-splitstackshape">
<p>Mahto, A. 2019. <em>Splitstackshape: Stack and Reshape Datasets After Splitting Concatenated Values</em>. <a href="https://CRAN.R-project.org/package=splitstackshape">https://CRAN.R-project.org/package=splitstackshape</a>.</p>
</div>
<div id="ref-ref714a">
<p>Mathai, A. M. 1993. “A Handbook of Generalized Special Functions for Statistical and Physical Sciences.” Department of Mathematics; Statistics, McGill University: Oxford University Press.</p>
</div>
<div id="ref-ref1298t">
<p>Mathew, T. 1997. “Wishart and Chi-Square Distributions Associated with Matrix Quadratic Forms.” <em>Journal of Multivariate Analysis 61, 129-143 (1997). Article No. MV971665</em>.</p>
</div>
<div id="ref-ref563a">
<p>Máté, A. 2014. “The Jordan Canonical Form.” Brooklyn College of the City University of New York. <a href="http://www.sci.brooklyn.cuny.edu/~mate/misc/jordan_canonical.pdf">http://www.sci.brooklyn.cuny.edu/~mate/misc/jordan_canonical.pdf</a>.</p>
</div>
<div id="ref-ref737m">
<p>McCullagh, P., &amp; Nelder, J. A. 1983. “Generalized Linear Models.” <a href="https://doi.org/10.1201/9780203753736">https://doi.org/10.1201/9780203753736</a>.</p>
</div>
<div id="ref-ref847m">
<p>McHugh, M. L. 2013. “The Chi-Square Test of Independence.” <em>Lessons in Biostatistics</em>. <a href="https://www.biochemia-medica.com/en/journal/23/2/10.11613/BM.2013.018">https://www.biochemia-medica.com/en/journal/23/2/10.11613/BM.2013.018</a>.</p>
</div>
<div id="ref-ref705m">
<p>McLaughlin, M. P. 2016. “Compendium of Common Probability Distributions.” <a href="https://www.causascientia.org/math_stat/Dists/Compendium.pdf">https://www.causascientia.org/math_stat/Dists/Compendium.pdf</a>.</p>
</div>
<div id="ref-ref298f">
<p>Merchant, F., Vatwani, T., Chattopadhyay, A., Raha, S., Nandy, S. K., &amp; Narayan, R. 2018. “Achieving Efficient Realization of Kalman Filter on Cgra Through Algorithm-Architecture Co-Design.” <a href="https://arxiv.org/pdf/1802.03650.pdf">https://arxiv.org/pdf/1802.03650.pdf</a>.</p>
</div>
<div id="ref-ref1470s">
<p>Merity, S. 2015. “Explaining and Illustrating Orthogonal Initialization for Recurrent Neural Networks.” <em>[Article]</em>. <a href="https://smerity.com/articles/2016/orthogonal_init.html">https://smerity.com/articles/2016/orthogonal_init.html</a>.</p>
</div>
<div id="ref-koRpuslangen">
<p>Michalke, M. 2020. <em>koRpus.lang.en: Language Support for ’koRpus’ Package: English</em>. <a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a>.</p>
</div>
<div id="ref-koRpus">
<p>———. 2021. <em>koRpus: Text Analysis with Emphasis on POS Tagging, Readability, and Lexical Diversity</em>. <a href="https://reaktanz.de/?c=hacking&amp;s=koRpus">https://reaktanz.de/?c=hacking&amp;s=koRpus</a>.</p>
</div>
<div id="ref-ref1215m">
<p>Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. 2013. “Efficient Estimation of Word Representations in Vector Space.” Google Inc., Mountain View, CA. <a href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a>.</p>
</div>
<div id="ref-rpartplot">
<p>Milborrow, S. 2019. <em>Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’Plot.rpart’</em>. <a href="https://CRAN.R-project.org/package=rpart.plot">https://CRAN.R-project.org/package=rpart.plot</a>.</p>
</div>
<div id="ref-ref463t">
<p>Minka, T. P. 2001a. “A Family of Algorithms for Approximate Bayesian Inference.” <a href="https://tminka.github.io/papers/ep/minka-thesis.pdf">https://tminka.github.io/papers/ep/minka-thesis.pdf</a>.</p>
</div>
<div id="ref-ref473t">
<p>———. 2001b. “Expectation Propagation for Approximate Bayesian Inference.” <em>Carnegie Mellon University</em>. <a href="https://arxiv.org/pdf/1301.2294.pdf">https://arxiv.org/pdf/1301.2294.pdf</a>.</p>
</div>
<div id="ref-ref972d">
<p>Misra, D. 2019. “Mish: A Self Regularized Non-Monotonic Neural Activation Function.” <a href="https://arxiv.org/vc/arxiv/papers/1908/1908.08681v2.pdf">https://arxiv.org/vc/arxiv/papers/1908/1908.08681v2.pdf</a>.</p>
</div>
<div id="ref-ref499a">
<p>Mohammed, A. S., &amp; Al-jawary, M. A. 2021. “Three Weighted Residuals Method for Solving the Nonlinear Thin Film Flow Problem.” <em>Ibn Al-Haitham International Conference for Pure and Applied Sciences (IHICPS), Journal of Physics: Conference Series, 1879 (2021) 022096, Doi:10.1088/1742-6596/1879/2/022096</em>. Department of Mathematics, College of Education for Pure Sciences, Universy of Baghdad, Iraq.</p>
</div>
<div id="ref-ref463">
<p>Morris, T. P., White, I. R., &amp; Royston, P. 2014. “Tuning Multiple Imputation by Predictive Mean Matching and Local Residual Draws.” <em>BMC Med Res Methodol 14, 75</em>. <a href="https://doi.org/10.1186/1471-2288-14-75">https://doi.org/10.1186/1471-2288-14-75</a>.</p>
</div>
<div id="ref-ref3443s">
<p>Mortimer, S., &amp; Bennett, C. 2017. <em>Aurelius: Generates Pfa Documents from R Code and Optionally Runs Them</em>. <a href="https://CRAN.R-project.org/package=aurelius">https://CRAN.R-project.org/package=aurelius</a>.</p>
</div>
<div id="ref-ref1435l">
<p>Muda, L., Begam, M., &amp; Elamvazuthi, I. 2010. “Voice Recognition Algorithms Using Mel Frequency Cepstral Coefficient (Mfcc) and Dynamic Time Warping (Dtw) Techniques.” <em>Computing, Volume 2, ISSUE 3, March 2010, ISSN 2151-9617</em>. <a href="https://arxiv.org/pdf/1003.4083.pdf">https://arxiv.org/pdf/1003.4083.pdf</a>.</p>
</div>
<div id="ref-ref224k">
<p>Murphy, K. P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. Massachusetts Institute of Technology.</p>
</div>
<div id="ref-ref940c">
<p>Musto, C., Semeraro, G., &amp; Polignano, M. n.d. “A Comparison of Lexicon-Based Approaches for Sentiment Analysis for Microblog Posts.” <em>Department of Computer Science, University of Bari Aldo Moro, Italy</em>. <a href="http://ceur-ws.org/Vol-1314/paper-06.pdf">http://ceur-ws.org/Vol-1314/paper-06.pdf</a>.</p>
</div>
<div id="ref-ref336r">
<p>Muthumalai, R. K. 2012. “Note on Newton Interpolation Formula.” <em>International Journal of Mathematics Analysis, Vol. 6, 2012, No. 50, 2459-2465</em>. <a href="http://www.m-hikari.com/ijma/ijma-2012/ijma-49-52-2012/muthumalaiIJMA49-52-2012.pdf">http://www.m-hikari.com/ijma/ijma-2012/ijma-49-52-2012/muthumalaiIJMA49-52-2012.pdf</a>.</p>
</div>
<div id="ref-ref837c">
<p>Natoli, C. 2017. “Understanding Analysis of Variance: Best Practice.” <em>STAT Center of Excellence, STAT COE-Report-29-2017</em>. 2950 Hobson Way-Wright-Patterson AFB, OH 45433. <a href="https://www.afit.edu/stat/statcoe_files/ANOVA%20Final.pdf">https://www.afit.edu/stat/statcoe_files/ANOVA%20Final.pdf</a>.</p>
</div>
<div id="ref-RColorBrewer">
<p>Neuwirth, E. 2014. <em>RColorBrewer: ColorBrewer Palettes</em>. <a href="https://CRAN.R-project.org/package=RColorBrewer">https://CRAN.R-project.org/package=RColorBrewer</a>.</p>
</div>
<div id="ref-ref948l">
<p>Nguyen, L. 2013. “Overview of Bayesian Network.” University of Technology, Ho Chi Minh city, Vietnam. <a href="https://www.researchgate.net/profile/Loc-Nguyen-101/publication/282685628_Overview_of_Bayesian_Network/links/5f55ff10299bf13a31a7d529/Overview-of-Bayesian-Network.pdf">https://www.researchgate.net/profile/Loc-Nguyen-101/publication/282685628_Overview_of_Bayesian_Network/links/5f55ff10299bf13a31a7d529/Overview-of-Bayesian-Network.pdf</a>.</p>
</div>
<div id="ref-ref1512a">
<p>Noughreche, A., Boulouma, S., &amp; Benbaghdad, M. 2021. “Design and Implementation of an Automatic Speech Recognition Based on Voice Control System.” <a href="https://easychair.org/publications/preprint_download/wzRf">https://easychair.org/publications/preprint_download/wzRf</a>.</p>
</div>
<div id="ref-ref667l">
<p>Olsen-Kettle, L. 2011. “Numerical Solution of Partial Differential Equations.” The University of Queensland, School of Eartch Sciences, Centre for Geoscience Computing. <a href="https://espace.library.uq.edu.au/view/UQ:239427/Lectures_Book.pdf">https://espace.library.uq.edu.au/view/UQ:239427/Lectures_Book.pdf</a>.</p>
</div>
<div id="ref-ref3477j">
<p>Ooms, J. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between Json Data and R Objects.” <em>arXiv:1403.2805 [stat.CO]</em>. <a href="https://arxiv.org/abs/1403.2805">https://arxiv.org/abs/1403.2805</a>.</p>
</div>
<div id="ref-ref1502j">
<p>Osborne, J. W. 2013. “Best Practises in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data.” <a href="http://pzs.dstu.dp.ua/DataMining/preprocessing/bibl/cleaning.pdf">http://pzs.dstu.dp.ua/DataMining/preprocessing/bibl/cleaning.pdf</a>.</p>
</div>
<div id="ref-ref92c">
<p>Paige, C. C. 1975. “Error Analysis of the Lanczos Algorithm for Tridiagonalizing a Sysmmetric Matrix.” <em>J. Inst. Maths Applies (1976, 18, 341-349)</em>. School of Computer Science, McGill University, Montreal, Quebec, Canada. <a href="https://www.cs.mcgill.ca/~chris/pubClassic/76JIMA001.pdf">https://www.cs.mcgill.ca/~chris/pubClassic/76JIMA001.pdf</a>.</p>
</div>
<div id="ref-ref491p">
<p>Papadopoulos, P. 2015. “Introduction to the Finite Element Method.” <em>Department of Mechanical Engineering, University of California, Berkeley</em>.</p>
</div>
<div id="ref-ref1556k">
<p>Papineni, K., &amp; Roukos, S., Ward, T., &amp; Zhu, W-J. 2002. “BLEU: A Method for Automatic Evaluation of Machine Translation.” <em>Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, July 2002</em>. <a href="https://dl.acm.org/doi/10.3115/1073083.1073135">https://dl.acm.org/doi/10.3115/1073083.1073135</a>.</p>
</div>
<div id="ref-ref7b">
<p>Parlett, B. N. 1974. “The Rayleigh Quotient Iteration and Some Generalizations for Nonnormal Matrices.” <em>Mathematics of Computation, Volume 28, Number 127, July 1974, Pages 679-693</em>. <a href="https://www.ams.org/journals/mcom/1974-28-127/S0025-5718-1974-0405823-3/S0025-5718-1974-0405823-3.pdf">https://www.ams.org/journals/mcom/1974-28-127/S0025-5718-1974-0405823-3/S0025-5718-1974-0405823-3.pdf</a>.</p>
</div>
<div id="ref-ref72b">
<p>———. 1994. “Do We Fully Understand the Symmetric Lanczos Algorithm yet?” <em>Supported by ONR, Contract N000014-90-J-1372</em>. Department of Mathematics, University of California, Berkeley, CA 94720, USA. <a href="https://apps.dtic.mil/sti/pdfs/ADA289614.pdf">https://apps.dtic.mil/sti/pdfs/ADA289614.pdf</a>.</p>
</div>
<div id="ref-patchwork">
<p>Pedersen, T. L. 2020. <em>Patchwork: The Composer of Plots</em>. <a href="https://CRAN.R-project.org/package=patchwork">https://CRAN.R-project.org/package=patchwork</a>.</p>
</div>
<div id="ref-ref368_1">
<p>Pei, J., Han, J., Mortazavi-Asl, B., Wang, J., Pinto, H., Chen, Q., Dayal, U., &amp; Hsu, M-C. 2004. “Mining Sequential Patterns by Pattern-Growth: The Prefixspan Approach.” <em>IEEE Transactions on Knowledge and Data Engineering, VOL. 16, NO.10. Oct 2004</em>. IEEE Computer Society. <a href="http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf">http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf</a>.</p>
</div>
<div id="ref-ref957p">
<p>Pernkopf, F., Peharz, R., &amp; Tschiatschek, S. 2014. “Introduction to Probabilistic Graphical Models.” <em>In Academic Press Library in Signal Processing: Signal Processing Theory and Machine Learning (Pp. 989-1064).</em> Academic Press.</p>
</div>
<div id="ref-ref1988j">
<p>Pivarski, J., Bennett, C., &amp; Grossman, R. L. 2016. “Deploying Analytics with the Portable Format for Analytics (Pfa).” <em>Department of Computer Science, University of Toronto</em>. <a href="https://www.kdd.org/kdd2016/papers/files/adp0884-pivarskiAcb.pdf">https://www.kdd.org/kdd2016/papers/files/adp0884-pivarskiAcb.pdf</a>.</p>
</div>
<div id="ref-ref718j">
<p>Platt, J. C. 1998. “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.” <a href="https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/">https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/</a>.</p>
</div>
<div id="ref-ref1252m">
<p>Plummer, M. 2003. <em>Doing Bayesian Data Analsysi: A Tutorial with R, Jags, and Stan</em>. <em>Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003)</em>. <a href="https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf">https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf</a>.</p>
</div>
<div id="ref-rjags">
<p>———. 2019. <em>Rjags: Bayesian Graphical Models Using Mcmc</em>. <a href="https://CRAN.R-project.org/package=rjags">https://CRAN.R-project.org/package=rjags</a>.</p>
</div>
<div id="ref-ref356h">
<p>Prautzsch, H., Boehm, W., &amp; Paluszny, M. 2002. “Bézier- and B-Spline Techniques.” <a href="https://geom.ivd.kit.edu/downloads/pubs/pub-boehm-prautzsch_2002_preview.pdf">https://geom.ivd.kit.edu/downloads/pubs/pub-boehm-prautzsch_2002_preview.pdf</a>.</p>
</div>
<div id="ref-ref215w">
<p>Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp; Flannery, B. P. 2007. <em>Numerical Recipes: The Art of Scientific Computing</em>. (3rd Edition, printed 2007). Cambridge University Press.</p>
</div>
<div id="ref-ref559jr">
<p>Quinlan, J. R. 1986. “Induction of Decision Trees. Mach Learn 1.” <em>81–106 (1986)</em>. <a href="https://doi.org/10.1007/BF00116251">https://doi.org/10.1007/BF00116251</a>.</p>
</div>
<div id="ref-ref563jr">
<p>———. 1996. “Improving Regressors Using Boosting Techniques.” <em>Journal of Artificial Intelligence Research 4 (1996) 77-90</em>. University of Sydney, Sydney Australia 2006. <a href="https://arxiv.org/pdf/cs/9603103.pdf">https://arxiv.org/pdf/cs/9603103.pdf</a>.</p>
</div>
<div id="ref-ref1558l">
<p>Rabiner, L. R. 1988. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” <em>Proceedings of the IEEE, Vol. 77, No. 2, 1989</em>. <a href="https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf">https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf</a>.</p>
</div>
<div id="ref-ref1421m">
<p>Raissi, M. 2021. “Mel-Spectrogram and MFCCs.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=hF72sY70_IQ">https://www.youtube.com/watch?v=hF72sY70_IQ</a>.</p>
</div>
<div id="ref-ref962p">
<p>Ramachandran, P., Zoph, B., &amp; Le, Q. V. 2017. “SWISH: A Self-Gated Activation Function.” <a href="https://arxiv.org/pdf/1710.05941v1.pdf">https://arxiv.org/pdf/1710.05941v1.pdf</a>.</p>
</div>
<div id="ref-ref415c">
<p>Ramachandran, T., D. Udayakumar, D., &amp; Parimala, R. 2017. “Contra-Harmonic Mean Derivative - Based Closed Newton Cotes Quadrature.” <em>Global Journal of Pure and Applied Mathematics. ISSN 0973-1768 Volume 13, Number 5(2017), Pp. 1319-1330</em>. Research India Publications. <a href="https://www.ripublication.com/gjpam17/gjpamv13n5_01.pdf">https://www.ripublication.com/gjpam17/gjpamv13n5_01.pdf</a>.</p>
</div>
<div id="ref-Rlang">
<p>R Core Team. 2019. <em>R: A Language and Environment for Statistical Computing</em>. Vienna, Austria: R Foundation for Statistical Computing. <a href="https://www.R-project.org/">https://www.R-project.org/</a>.</p>
</div>
<div id="ref-ref1014s">
<p>Reddi, S. J., Kale, S., &amp; Kumar, S. 2018. “On the Convergence of Adam and Beyond.” <em>Conference Paper at ICLR 2018</em>. <a href="https://arxiv.org/pdf/1904.09237.pdf">https://arxiv.org/pdf/1904.09237.pdf</a>.</p>
</div>
<div id="ref-sentimentr">
<p>Rinker, T. W. 2019. <em>sentimentr: Calculate Text Polarity Sentiment</em>. Buffalo, New York. <a href="http://github.com/trinker/sentimentr">http://github.com/trinker/sentimentr</a>.</p>
</div>
<div id="ref-ref1002c">
<p>Robert, C., &amp; Casella, G. 2008. “A History of Markov Chain Monte Carlo - Subjective Recollections from Incomplete Data -.” hal-00311793. <a href="https://hal.archives-ouvertes.fr/hal-00311793/document">https://hal.archives-ouvertes.fr/hal-00311793/document</a>.</p>
</div>
<div id="ref-ref523c">
<p>Robert, C. P. 2016. “The Metropolis-Hastings Algorithm.” <a href="https://arxiv.org/pdf/1504.01896.pdf">https://arxiv.org/pdf/1504.01896.pdf</a>.</p>
</div>
<div id="ref-ref984se">
<p>Robertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., &amp; Gatford, M. 1994. “Okapi at TREC-3.” <em>Centre for Interactive Systems Research</em>. Department of Information Science, City University, Northampton Square, London ECIV 0HB, UK. <a href="https://www.computing.dcu.ie/~gjones/Teaching/CA437/city.pdf">https://www.computing.dcu.ie/~gjones/Teaching/CA437/city.pdf</a>.</p>
</div>
<div id="ref-ref912r">
<p>Rodger, R. S., &amp; Roberts, M. 2013. “Comparison of Power for Multiple Comparison Procedures.” <em>Journal of Methods and Measurement in the Social Sciences, Vol.4, No1,20-47, 2013</em>. <a href="https://journals.librarypublishing.arizona.edu/jmmss/article/799/galley/794/view/">https://journals.librarypublishing.arizona.edu/jmmss/article/799/galley/794/view/</a>.</p>
</div>
<div id="ref-ref695s">
<p>Ross, S. 2010. “A First Course in Probability.” University of Southern California: Pearson.</p>
</div>
<div id="ref-ref297y">
<p>Saad, Y., &amp; Schultz, M. H. 1986. <em>GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems</em>. <em>SIAM Journal on Scientific and Statistical Computing</em>.</p>
</div>
<div id="ref-ref756f">
<p>Sabri, F., &amp; Gyateng, T. 2015. “Understanding Statistical Significance: A Short Guide.” <em>NPC’s Data Labs Project, Funded by the Oak Foundation</em>.</p>
</div>
<div id="ref-ref53m">
<p>Saha, M., &amp; Chakrabarty, J. 2018. “On Generalized Jacobi, Gauss-Seidel and Sor Method.” <a href="https://arxiv.org/pdf/1806.07682.pdf">https://arxiv.org/pdf/1806.07682.pdf</a>.</p>
</div>
<div id="ref-ref483a">
<p>Salih, A. A. n.d. “Finite Element Method.” Department of Aerospace Engineering, Indian Institute of Space Science; Technology, Thiruvananthapuram, India. <a href="https://www.iist.ac.in/sites/default/files/people/IN08026/FEM.pdf">https://www.iist.ac.in/sites/default/files/people/IN08026/FEM.pdf</a>.</p>
</div>
<div id="ref-ref1487a">
<p>Sankar, A. 2019. “Sequence to Sequence Learning with Encoder-Decoder Neural Network Models.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=bBBYPuVUnug">https://www.youtube.com/watch?v=bBBYPuVUnug</a>.</p>
</div>
<div id="ref-ref1002s">
<p>Santunkar, S., Tsipras, D., Ilyas, A., &amp; Madry, A. 2018. “How Does Batch Normalization Help Optimization.” <em>32nd International Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada</em>. <a href="http://proceedings.mlr.press/v37/ioffe15.pdf">http://proceedings.mlr.press/v37/ioffe15.pdf</a>.</p>
</div>
<div id="ref-ref599i">
<p>Savov, I. 2017. “No Bullshit Guide to Linear Algebra.” <a href="https://minireference.com/static/excerpts/noBSLA_v2_preview.pdf">https://minireference.com/static/excerpts/noBSLA_v2_preview.pdf</a>.</p>
</div>
<div id="ref-ref501">
<p>Schapire, R. E. 1999. “A Brief Introduction to Boosting.” <em>Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence</em>. <a href="http://rob.schapire.net/papers/Schapire99c.pdf">http://rob.schapire.net/papers/Schapire99c.pdf</a>.</p>
</div>
<div id="ref-ref1232m">
<p>Schuster, M., &amp; Paliwal, K. K. 1997. “Bidirectional Recurrent Neural Network.” <em>Signal Processing, IEEE Transactions on. 45. 2673 - 2681. 10.1109/78.650093</em>. <a href="https://www.researchgate.net/profile/Mike-Schuster-2/publication/3316656_Bidirectional_recurrent_neural_networks">https://www.researchgate.net/profile/Mike-Schuster-2/publication/3316656_Bidirectional_recurrent_neural_networks</a>.</p>
</div>
<div id="ref-ref694s">
<p>Shalev-Shwartz, S., Singer, Y., &amp; Srebro, N. 2007. “Pegasos: Primal Estimated Sub-Gradient Solver for Svm.” <em>ACM International Conference Proceeding Series. 227. 807-814. 10.1145/1273496.1273598.</em> <a href="https://doi.org/10.1007/s10107-010-0420-4">https://doi.org/10.1007/s10107-010-0420-4</a>.</p>
</div>
<div id="ref-ref710s">
<p>Shalev-Shwatz, S., &amp; Zhang, T. 2013. “Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.” <a href="https://arxiv.org/pdf/1209.1873.pdf">https://arxiv.org/pdf/1209.1873.pdf</a>.</p>
</div>
<div id="ref-ref1559x">
<p>Shao, X., &amp; Johnson, S. G. 2009. “Typte-Ii/Iii Dct/Dst Algorithms with Reduced Number of Arithmetic Operations.” <em>Department of Mathematics, Massachusetts Institute of Technology,Cambridge MA 02139</em>. <a href="https://arxiv.org/pdf/cs/0703150.pdf">https://arxiv.org/pdf/cs/0703150.pdf</a>.</p>
</div>
<div id="ref-ref181p">
<p>Shaw, P., Uszkoreit, J., &amp; Vaswani, A. 2018. <em>Self-Attention with Relative Position Representations</em>. <a href="https://arxiv.org/pdf/1803.02155.pdf">https://arxiv.org/pdf/1803.02155.pdf</a>.</p>
</div>
<div id="ref-tidytext">
<p>Silge, J., &amp; Robinson, D. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” <em>JOSS</em> 1 (3). The Open Journal. <a href="https://doi.org/10.21105/joss.00037">https://doi.org/10.21105/joss.00037</a>.</p>
</div>
<div id="ref-ref61g">
<p>Sleijpen, G. 2014. “Krylov Subspace Methods.” <em>Program Lecture 3, Upssala, April 2014</em>. Department of Mathematics, Universiteit Utrecht. <a href="http://www.math.uu.nl/~sleij101/Uppsala/Lecture-handouts3.pdf">http://www.math.uu.nl/~sleij101/Uppsala/Lecture-handouts3.pdf</a>.</p>
</div>
<div id="ref-ref1082l">
<p>Smith, L. N. 2017. “Cyclical Learning Rate for Training Neural Networks.” <a href="https://arxiv.org/pdf/1506.01186.pdf">https://arxiv.org/pdf/1506.01186.pdf</a>.</p>
</div>
<div id="ref-plot3D">
<p>Soetaert, K. 2019. <em>Plot3D: Plotting Multi-Dimensional Data</em>. <a href="https://CRAN.R-project.org/package=plot3D">https://CRAN.R-project.org/package=plot3D</a>.</p>
</div>
<div id="ref-ref600d">
<p>Solomatine, D., &amp; Shrestha, D. 2004. “AdaBoost.RT: A Boosting Algorithm for Regression Problems.” IEEE International Conference on Neural Networks - Conference Proceedings. 2. 1163 - 1168 vol.2. 10.1109/IJCNN.2004.1380102. <a href="https://www.researchgate.net/publication/4116773_AdaBoostRT_A_boosting_algorithm_for_regression_problems">https://www.researchgate.net/publication/4116773_AdaBoostRT_A_boosting_algorithm_for_regression_problems</a>.</p>
</div>
<div id="ref-ref993j">
<p>Speagle, J. S. 2020. “A Conceptual Introduction to Markov Chain Monte Carlo Methods.” Center for Astrophysics, Harvard &amp; Smithsonian, 60 Garden St. Cambridge, MA 02138, USA. <a href="https://arxiv.org/pdf/1909.12313.pdf">https://arxiv.org/pdf/1909.12313.pdf</a>.</p>
</div>
<div id="ref-ref378">
<p>Srikant, R., &amp; Agrawal, R. 1996. “Mining Sequential Patterns: Generalizations and Performance Improvements.” <em>In: Apers P., Bouzeghoub M., Gardarin G. (Eds) Advances in Database Technology — EDBT ’96. EDBT 1996. Lecture Notes in Computer Science, Vol 1057</em>. Berlin, Heidelberg: Springer. <a href="https://doi.org/10.1007/BFb0014140">https://doi.org/10.1007/BFb0014140</a>.</p>
</div>
<div id="ref-ref1196n">
<p>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” <em>Machine Learning Research 15 (2014), 1929-1958</em>. <a href="https://dl.acm.org/doi/pdf/10.5555/2627435.2670313">https://dl.acm.org/doi/pdf/10.5555/2627435.2670313</a>.</p>
</div>
<div id="ref-ref623c">
<p>Stachniss, C. 2013. “SLAM Course - 06 - Unscented Kalman Filter.” <a href="https://www.youtube.com/watch?v=DWDzmweTKsQ&amp;t=233s">https://www.youtube.com/watch?v=DWDzmweTKsQ&amp;t=233s</a>.</p>
</div>
<div id="ref-ref729s">
<p>Stanford. 2009. “The Simplified SMO Algorithm.” <em>CS 229, Automn 2009</em>. <a href="http://cs229.stanford.edu/materials/smo.pdf">http://cs229.stanford.edu/materials/smo.pdf</a>.</p>
</div>
<div id="ref-ref508p">
<p>Stanimirovíc, P. S., &amp; Tasić, M. B. 2011. “Computing Generalized Inverses Using Lu Factorization of Matrix Product.” University of Niš, Department of Mathematics, Faculty of Science, Višegradska, 33, 1800 Niš, Serbia. <a href="https://arxiv.org/pdf/1104.1697.pdf">https://arxiv.org/pdf/1104.1697.pdf</a>.</p>
</div>
<div id="ref-ref1338s">
<p>Sternstein, M. 1996. “Statistics.” Barron’s Educational Series, Inc.</p>
</div>
<div id="ref-ref233w">
<p>Stewart, W. J. 2009. <em>Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling</em>. Princeton University Press.</p>
</div>
<div id="ref-ref115t">
<p>Stobierski, T. 2019. <em>What Is Statistical Modeling for Data Analysis</em>. <em>What Is Statistical Modeleling for Data Analysis</em>. <a href="https://www.northeastern.edu/graduate/blog/statistical-modeling-for-data-analysis/">https://www.northeastern.edu/graduate/blog/statistical-modeling-for-data-analysis/</a>.</p>
</div>
<div id="ref-ref1642g">
<p>Strang, G. 1999. “The Discrete Cosine Transform.” <em>SIAM Review, Vol. 41, No. I, Pp. 135-147</em>. <a href="https://www.unioviedo.es/compnum/transversal_eng/DCT5.pdf">https://www.unioviedo.es/compnum/transversal_eng/DCT5.pdf</a>.</p>
</div>
<div id="ref-ref616w">
<p>———. 2005. “Linear Algebra.” <em>[Video series] Lec 1: MIT 18.06 Linear Algebra, Spring 2005</em>. Massachusetts Institute of Technology: MIT OpenCourseware. <a href="https://www.youtube.com/watch?v=QVKj3LADCnA">https://www.youtube.com/watch?v=QVKj3LADCnA</a>.</p>
</div>
<div id="ref-ref649w">
<p>Strauss, W. A. 2008. “Partial Differential Equations: An Introduction.” John Wiley &amp; Sons, Inc.</p>
</div>
<div id="ref-ref1496j">
<p>Sueur, J. 2018. “Sound Analysis and Synthesis with R.” <em>[1st Edition], p.390</em>. <a href="https://link.springer.com/book/10.1007/978-3-319-77647-7">https://link.springer.com/book/10.1007/978-3-319-77647-7</a>.</p>
</div>
<div id="ref-ref1582s">
<p>Sun, S., &amp; Iyyer, M. 2021. “Revisiting Simple Neural Probabilistic Language Model.” <em>College of Information and Computer Sciences, University of Massachusetts Amherst</em>. <a href="https://arxiv.org/pdf/2104.03474.pdf">https://arxiv.org/pdf/2104.03474.pdf</a>.</p>
</div>
<div id="ref-ref368_2">
<p>Tan, P-N., Kumar, V., &amp; Srivastava, J. 2002. “Selecting the Right Interestingness Measure for Association Patterns.” <em>SIGKDD ’02 Edmonton, Alberta, Canada</em>. <a href="http://www.cse.msu.edu/~ptan/publication/kdd2002.pdf">http://www.cse.msu.edu/~ptan/publication/kdd2002.pdf</a>.</p>
</div>
<div id="ref-ref3435y">
<p>Tang Y., &amp; ONNX Authors. 2021. <em>Onnx: R Interface to ’Onnx’</em>. <a href="https://CRAN.R-project.org/package=onnx">https://CRAN.R-project.org/package=onnx</a>.</p>
</div>
<div id="ref-rpart">
<p>Therneau, T., &amp; Atkinson, B. 2019. <em>Rpart: Recursive Partitioning and Regression Trees</em>. <a href="https://CRAN.R-project.org/package=rpart">https://CRAN.R-project.org/package=rpart</a>.</p>
</div>
<div id="ref-ref1148t">
<p>Therneau, T. M., Atkinson, E. J., &amp; Mayo Foundation. 1997. “An Introduction to Recursive Partitioning Using the Rpart Routines.” <a href="https://www.stat.auckland.ac.nz/~yee/784/files/techrep.pdf">https://www.stat.auckland.ac.nz/~yee/784/files/techrep.pdf</a>.</p>
</div>
<div id="ref-ref480">
<p>———. 2019. “An Introduction to Recursive Partitioning Using the Rpart Routines.” <a href="https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf">https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf</a>.</p>
</div>
<div id="ref-ref1236l">
<p>Thompson, L., &amp; Mimno, D. 1997. “Topic Modeling with Contextualized Word Representation Clusters.” <a href="https://arxiv.org/pdf/2010.12626.pdf">https://arxiv.org/pdf/2010.12626.pdf</a>.</p>
</div>
<div id="ref-ref1022t">
<p>Tieleman, T., &amp; Hinton, G. 2012. “Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude.” <em>COURSERA: Neural Networks for Machine Learning, 4(2), 26-31</em>.</p>
</div>
<div id="ref-ref1075a">
<p>Tolver, A. 2016. “An Introduction to Markov Chains.” Department of Mathematical Sciences, University of Copenhagen, Universitetsparken 5, DK-2100 CopenHagen Ø, Denmark. <a href="http://web.math.ku.dk/noter/filer/stoknoter.pdf">http://web.math.ku.dk/noter/filer/stoknoter.pdf</a>.</p>
</div>
<div id="ref-tseries">
<p>Trapletti, A., &amp; Hornik, K. 2019. <em>Tseries: Time Series Analysis and Computational Finance</em>. <a href="https://CRAN.R-project.org/package=tseries">https://CRAN.R-project.org/package=tseries</a>.</p>
</div>
<div id="ref-ref800a">
<p>Ugoni A., &amp; Walker, B. F. 1995. “The T Test: An Introduction.” <em>COMSIG review, 4(2), 37–40.</em> <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2050377/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2050377/</a>.</p>
</div>
<div id="ref-ref1439s">
<p>Umesh, S., Cohen, L., &amp; Nelson, D. J. 1999. “Fitting the Mel Scale.” In <em>Fitting the Mel scale. 1. 217 - 220 vol.1. 10.1109/ICASSP.1999.758101</em>. <a href="https://www.researchgate.net/publication/3793925_Fitting_the_Mel_scale">https://www.researchgate.net/publication/3793925_Fitting_the_Mel_scale</a>.</p>
</div>
<div id="ref-jpeg">
<p>Urbanek, S. 2019. <em>Jpeg: Read and Write Jpeg Images</em>. <a href="https://CRAN.R-project.org/package=jpeg">https://CRAN.R-project.org/package=jpeg</a>.</p>
</div>
<div id="ref-ref1102c">
<p>Urrea, C. &amp; Agramonte, R. 2021. “Kalman Filter: Historical Overview and Review of Its Use in Robotics 60 Years After Its Creation.” <em>Hindawi, Journal of SensorsVolume 2021, Article ID 9674015, 21 Pages</em>. <a href="https://doi.org/10.1155/2021/9674015">https://doi.org/10.1155/2021/9674015</a>.</p>
</div>
<div id="ref-ref308m">
<p>van Biezen, M. 2015. “The Kalam Filter: A Video Series.” <em>[Video]</em>. <a href="https://www.youtube.com/watch?v=CaCcOwJPytQ">https://www.youtube.com/watch?v=CaCcOwJPytQ</a>.</p>
</div>
<div id="ref-ref619r">
<p>van der Merwe, R. &amp; Wan, E. 2013. “Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models.” <em>OGI School of Science &amp; Engineering</em>. <a href="https://www.gatsby.ucl.ac.uk/~byron/nlds/merwe2003a.pdf">https://www.gatsby.ucl.ac.uk/~byron/nlds/merwe2003a.pdf</a>.</p>
</div>
<div id="ref-ref572v">
<p>Vapnik, V. N. 2000. “The Nature of Statistical Learning Theory: Statistics for Engineering and Information Science.” <em>Second Edition 1999-2000, First Edition 1995-1996</em>. Springer-Verlag New York, Inc.</p>
</div>
<div id="ref-ref1339a">
<p>Vaswani, A., Shadeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. 2017. “Attention Is All You Need.” <em>31st Conference on Neural Information Processing Systems (NIPS 2017)</em>. Long Beach, CA, USA. <a href="https://arxiv.org/pdf/1706.03762.pdf">https://arxiv.org/pdf/1706.03762.pdf</a>.</p>
</div>
<div id="ref-ref1466e">
<p>Vorontsov, E. 2017. “On Orthogonality and Learning Recurrent Networks with Long Term Dependencies.” <a href="https://arxiv.org/pdf/1702.00071.pdf">https://arxiv.org/pdf/1702.00071.pdf</a>.</p>
</div>
<div id="ref-ref1462w">
<p>———. 2020. “Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks.” <em>Conference Paper at ICLR 2020</em>. <a href="https://arxiv.org/pdf/2001.05992.pdf">https://arxiv.org/pdf/2001.05992.pdf</a>.</p>
</div>
<div id="ref-ref685c">
<p>Walck, C. 2007. “Hand-Book on Statistical Distributions for Experimentalists.” <em>Internal Report SUF-PFY/96-01</em>. University of Stockholm. <a href="http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf">http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf</a>.</p>
</div>
<div id="ref-ref1093e">
<p>Wan, E. A., &amp; van der Merwe, R. 2000. “The Unscented Kalman Filter for Nonlinear Estimation.” <em>Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No.00EX373)</em>. <a href="https://ieeexplore.ieee.org/document/882463">https://ieeexplore.ieee.org/document/882463</a>.</p>
</div>
<div id="ref-KernSmooth">
<p>Wand, M. 2019. <em>KernSmooth: Functions for Kernel Smoothing Supporting Wand &amp; Jones (1995)</em>. <a href="https://CRAN.R-project.org/package=KernSmooth">https://CRAN.R-project.org/package=KernSmooth</a>.</p>
</div>
<div id="ref-corrplot">
<p>Wei, T. &amp; Simko, V. 2017. <em>R Package &quot;Corrplot&quot;: Visualization of a Correlation Matrix</em>. <a href="https://github.com/taiyun/corrplot">https://github.com/taiyun/corrplot</a>.</p>
</div>
<div id="ref-ref1065h">
<p>White, H. 1982. “Econometrica: Maximum Likelihood Estimation of Misspecified Models.” <em>Econometrica, Vol. 50, No. 1. (Jan., 1982), Pp. 1-25</em>. The Econometric Society. <a href="https://www.jstor.org/stable/1912526">https://www.jstor.org/stable/1912526</a>.</p>
</div>
<div id="ref-dplyr">
<p>Wickham, H., François, R., Henry, L., &amp; Müller, K. 2021. <em>Dplyr: A Grammar of Data Manipulation</em>. <a href="https://CRAN.R-project.org/package=dplyr">https://CRAN.R-project.org/package=dplyr</a>.</p>
</div>
<div id="ref-scales">
<p>Wickham, H., &amp; Seidel, D. 2019. <em>Scales: Scale Functions for Visualization</em>. <a href="https://CRAN.R-project.org/package=scales">https://CRAN.R-project.org/package=scales</a>.</p>
</div>
<div id="ref-lsa">
<p>Wild, F. 2020. <em>Lsa: Latent Semantic Analysis</em>. <a href="https://CRAN.R-project.org/package=lsa">https://CRAN.R-project.org/package=lsa</a>.</p>
</div>
<div id="ref-DunnettTests">
<p>Xia, F. 2013. <em>DunnettTests: Software Implementation of Step-down and Step-up Dunnett Test Procedures</em>. <a href="https://CRAN.R-project.org/package=DunnettTests">https://CRAN.R-project.org/package=DunnettTests</a>.</p>
</div>
<div id="ref-Knitr">
<p>Xie, Y. 2019. <em>Knitr: A General-Purpose Package for Dynamic Report Generation in R.</em></p>
</div>
<div id="ref-ref1127y">
<p>Yang, Q. 2019. “The Preconditioned Lanczos Method for Solving Fractional Diffusion-Reaction Equations.” <em>AMS Winter School 2019</em>. <a href="https://cai.centre.uq.edu.au/files/10578/Lec3_Preconditioned_Lanczos_for_FDRE.pdf">https://cai.centre.uq.edu.au/files/10578/Lec3_Preconditioned_Lanczos_for_FDRE.pdf</a>.</p>
</div>
<div id="ref-ref1057x">
<p>Yang, X. 2017. “Understanding the Variational Lower Bound.” <a href="http://www.xyang35.umiacs.io/files/understanding-variational-lower.pdf">http://www.xyang35.umiacs.io/files/understanding-variational-lower.pdf</a>.</p>
</div>
<div id="ref-ref457y">
<p>Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. 2018. “Yes, but Did It Work?: Evaluating Variational Inference.” <em>3rd step Variational Bayes</em>. <a href="https://arxiv.org/pdf/1802.02538.pdf">https://arxiv.org/pdf/1802.02538.pdf</a>.</p>
</div>
<div id="ref-ref103y">
<p>YPMA, T. J. 1995. “Historical Development of the Newton-Raphson Method.” <em>SIAM Review, 37(4), 531–551</em>. <a href="http://www.jstor.org/stable/2132904">http://www.jstor.org/stable/2132904</a>.</p>
</div>
<div id="ref-ref71y">
<p>Yuan, Q., Gu, M., &amp; Li, B. 2018. “A Tour of the Lanczos Algorithm and Its Convergence Guarantees Through the Decades.” <em>[PowerPoint Slide]</em>. Department of Mathematics, UC Berkeley. <a href="https://math.berkeley.edu/~mgu/MA128BSpring2018/Lanczos.pdf">https://math.berkeley.edu/~mgu/MA128BSpring2018/Lanczos.pdf</a>.</p>
</div>
<div id="ref-ref423m">
<p>Zafar, F., Saleem, S., O.E.Burg, C., &amp; Minhós, F. 2014. “New Derivative Based Open Newton-Cotes Quadrature Rules.” <em>Abstract and Applied Analysis, Volume 2014,Article ID 109138</em>. Hindawi Publishing Corporation. <a href="{https://doi.org/10.1155/2014/109138}">{https://doi.org/10.1155/2014/109138}</a>.</p>
</div>
<div id="ref-ref372">
<p>Zaki, M. J. 2001. “SPADE: An Efficient Algorithm for Mining Frequent Sequences.” <em>Machine Learning, 42, 31-60, 2001</em>. Computer Science Department, Rensselaer Polytechnic Institute, Troy NY 12180-3590: Kluwer Academic Publishers. <a href="http://www.philippe-fournier-viger.com/spmf/SPADE.pdf">http://www.philippe-fournier-viger.com/spmf/SPADE.pdf</a>.</p>
</div>
<div id="ref-ref1011m">
<p>Zeiler, M. D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” <a href="https://arxiv.org/pdf/1212.5701.pdf">https://arxiv.org/pdf/1212.5701.pdf</a>.</p>
</div>
<div id="ref-ref1947c">
<p>Zhai, C., &amp; Massung, C. 2016. “Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining.” University of Illinois at Urbana-Champaign: Association for Computing Machinery; Morgan & Claypool Publishers.</p>
</div>
<div id="ref-ref424w">
<p>Zhao, W., &amp; Li, H. 2013. “Midpoint Derivative-Based Closed Newton-Cotes Quadrature.” <em>Hindawi Publishing Corporation, Abstract and Applied Analysis, Volume 2013, Article ID 492507, 10 pages</em>. Research India Publications. <a href="http://dx.doi.org/10.1155/2013/492507">http://dx.doi.org/10.1155/2013/492507</a>.</p>
</div>
<div id="ref-ref1090z">
<p>Zhirui, Y., Zhang, Y., &amp; Lord, D. 2012. “Goodness-of-Fit Testing for Accidental Models with Low Means.” <em>Accident analysis and prevention. 61. 10.1016/j.aap.2012.11.007</em>. <a href="https://www.researchgate.net/publication/233877185_Goodness-of-fit_testing_for_accident_models_with_low_means">https://www.researchgate.net/publication/233877185_Goodness-of-fit_testing_for_accident_models_with_low_means</a>.</p>
</div>
<div id="ref-ref602j">
<p>Zhu, J., Rosset, S., Zou, H., &amp; Hastie, T. 2006. “Multi-Class Adaboost.” <a href="https://web.stanford.edu/~hastie/Papers/samme.pdf">https://web.stanford.edu/~hastie/Papers/samme.pdf</a>.</p>
</div>
<div id="ref-ref116z">
<p>Zucchini, W. 2003. <em>A Density-Based Algorithm for Discovering Clusters</em>. <em>KDD-96 Proceedings, (c) 1996, AAAI (Www.aaai.org)</em>. <a href="http://staff.ustc.edu.cn/~zwp/teach/Math-Stat/kernel.pdf">http://staff.ustc.edu.cn/~zwp/teach/Math-Stat/kernel.pdf</a>.</p>
</div>
</div>


</div>






            </section>

          </div>
        </div>
      </div>
<a href="appendix-d.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
