<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Approximating Solutions to Systems of Eqns by Iteration (\(Ax = b\)) | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Approximating Solutions to Systems of Eqns by Iteration (\(Ax = b\)) | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Approximating Solutions to Systems of Eqns by Iteration (\(Ax = b\)) | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-03-06" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="3.3-approximating-root-and-fixed-point-by-iteration.html"/>
<link rel="next" href="3.5-polynomialregression.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i><em>Bibliography</em></a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="approximating-solutions-to-systems-of-eqns-by-iteration-ax-b" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.4</span> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)<a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#approximating-solutions-to-systems-of-eqns-by-iteration-ax-b" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>One of the motivations of linear algebra is to solve for Systems of Linear Equations denoted as:</p>
<p><span class="math display" id="eq:eqnnumber1">\[\begin{align}
A \mathbf{\vec{x}} = b \tag{3.33}
\end{align}\]</span></p>
<p>We can express simple systems of linear equations geometrically in terms of lines - first-degree polynomial equations. For example, in Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:linearsystem">3.7</a>, it is easy to show that if two lines (two linear equations) intersect, it means that there is a point of intersection; and thus <strong>there is one solution to the system</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearsystem"></span>
<img src="linear_system.png" alt="System of Linear Equations" width="90%" />
<p class="caption">
Figure 3.7: System of Linear Equations
</p>
</div>
<p>It is also easy to show that if two lines (two linear equations) are in parallel, they will never intersect. There is no point of intersection, and thus <strong>there is no solution to the system</strong>.</p>
<p>On the other hand, we can show that if two lines (two linear equations) are in parallel and along the same path, it means that every point in both lines hits an intersection, and thus <strong>there is an infinite number of solutions to the system</strong>.</p>
<p>That said, let us look at what we describe here as <strong>solution</strong> - the point of intersection. We can derive the point of intersection by translating the equation into matrix form and then solve for the <strong>unknown coordinates (x,y)</strong> by reducing the matrix into its <strong>reduced echelon</strong> form (using <strong>Gaussian Elimination</strong> and <strong>Lu Factorization</strong>):</p>
<p><span class="math display">\[
\left(\begin{array}{l}  
y = -x + 4\\
y = x
\end{array}\right)
\rightarrow
\left(\begin{array}{r}
x + y = 4\\
-x + y = 0
\end{array}\right)
\]</span></p>
<p>That gives us:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = b
\rightarrow
\left[\begin{array}{rr}  
1 &amp; 1 \\
-1 &amp; 1
 \end{array}\right] 
 \left[\begin{array}{r}  
x \\
y
 \end{array}\right] =
 \left[\begin{array}{r}  
4 \\ 0
 \end{array}\right]
\]</span></p>
<p>in <strong>reduced echelon</strong> form:</p>
<p><span class="math display">\[
\left[\begin{array}{rr}  
1 &amp; 0 \\
0 &amp; 1
 \end{array}\left|\begin{array}{r}2 \\ 2\end{array}\right.\right]
\]</span></p>
<p>Thus by RREF method, we are able to solve for the exact values for the vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
x = 2\\
y = 2
\end{align*}\]</span></p>
<p>Note that vector <span class="math inline">\(\mathbf{\vec{x}}\)</span> has two components: x and y. Alternatively, we can use <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span> to represent the two components.</p>
<p>That is an example of the most elementary way of solving a set of polynomial equations for vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>.</p>
<p>When we say <strong>solve for the solution</strong>, we mean to solve for vector <span class="math inline">\(\mathbf{\vec{x}}\)</span>. If <span class="math inline">\(\mathbf{\vec{x}}\)</span> happens to be a vector variable, then we need to find the value of each entry in the vector. That requires us to perform a simple mathematical manipulation:</p>
<p><span class="math display" id="eq:equate1050031">\[\begin{align}
\mathbf{\vec{x}} = A^{-1}b \ \ \ \ \ \leftarrow A\mathbf{\vec{x}} = b \tag{3.34} 
\end{align}\]</span></p>
<p>For a system of equations translated to a matrix with extremely high dimensionality, computing for the inverse of a matrix is not practical. In Chapter <a href="2-linearalgebra.html#linearalgebra">2</a>, we introduce methods such as <strong>LU decomposition</strong> and <strong>Gauss elimination</strong> as alternatives.</p>
<p>Recall the below R code, which we use in Chapter <a href="2-linearalgebra.html#linearalgebra">2</a> under <strong>LU decomposition</strong>:</p>

<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb58-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb58-2" data-line-number="2">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb58-3" data-line-number="3">LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)  <span class="co"># from Lin Algebra Ch</span></a>
<a class="sourceLine" id="cb58-4" data-line-number="4">uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, b) <span class="co"># REF/RREF section in LinAgb chapter</span></a>
<a class="sourceLine" id="cb58-5" data-line-number="5">x =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy) <span class="co"># REF/RREF section in LinAgb chapter</span></a>
<a class="sourceLine" id="cb58-6" data-line-number="6"></a>
<a class="sourceLine" id="cb58-7" data-line-number="7">A <span class="co"># the matrix (system of equations)</span></a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    5    5
## [2,]    2    4    5
## [3,]    3    3    3</code></pre>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb60-1" data-line-number="1">x <span class="co"># the solution</span></a></code></pre></div>
<pre><code>## [1]  1  2 -1</code></pre>

<p>The following sub-sections review several methods that help us solve systems of equations. Note that some of these methods are also used for optimization problems. For example, we can use the <strong>Conjugate Gradient</strong> method, <strong>Newton’s</strong> method, and <strong>Broyden’s</strong> method for root-finding problems and optimizing objective functions. Therefore while our focus in this section is on systems of equations, we also begin to introduce the concepts of <strong>gradient descent</strong> for <strong>search direction</strong> and <strong>stepsize</strong>.</p>
<p><strong>Optimization Problems</strong> is covered in the later section of this chapter.</p>
<div id="krylovmethods" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.1</span> Krylov Methods<a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Krylov Methods</strong> are iterative methods that make use of a special subspace called <strong>Krylov subspace</strong> denoted as:  </p>
<p><span class="math display" id="eq:equate1050032">\[\begin{align}
K_m(A,v) = \{\ v,\ Av,\ A^2v,\ A^3v,\ ...\ ,\ A^{m-1}v\ \} \tag{3.35} 
\end{align}\]</span></p>
<p>where m = dimension of K.</p>
<p>Here, we reference the works of Heath M.T. <span class="citation">(<a href="bibliography.html#ref-ref187m">2002</a>)</span>, An D. <span class="citation">(<a href="bibliography.html#ref-ref166d">2009</a>)</span>, Driscoll T. <span class="citation">(<a href="bibliography.html#ref-ref175t">2012</a>)</span>, and Sleijpen G. <span class="citation">(<a href="bibliography.html#ref-ref61g">2014</a>)</span>.</p>
<p>Recall Figure <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fig:krylovspace">3.1</a>. The right side illustrates solving for eigenvalues. And the left side illustrates a linear system of solving for <span class="math inline">\(x\)</span>. In Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:krylovmethod">3.8</a>, we show the same methods; but we focus now on <strong>GMRES</strong> and <strong>CG</strong> in the next sections.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:krylovmethod"></span>
<img src="krylovmethod.png" alt="Krylov SubSpace methods" width="70%" />
<p class="caption">
Figure 3.8: Krylov SubSpace methods
</p>
</div>
<p>Though <strong>Krylov</strong> methods are also used to deal with <strong>Eigen problems</strong> (e.g., <strong>Arnoldi and Lanczos</strong> methods), here we continue to focus on solving systems of equations for <span class="math inline">\(\mathbf{\vec{x}}\)</span> in the form of:</p>
<p><span class="math display">\[
A\mathbf{\vec{x}} = b \rightarrow\ \ \ \ \ \mathbf{\vec{x}} = A^{-1}b
\]</span>
Now instead of using analytical methods to compute for the <strong>inverse of A</strong>, we approximate by using <strong>Power Series</strong>, forming the polynomial equation below:</p>
<p><span class="math display" id="eq:equate1050033">\[\begin{align}
A^{-1} \approx p_j(A) =  \frac{1}{c_0} \sum_{j=0}^{m-1}(c_{j+1}) A^j, \ where\ c_0 \neq 0  \tag{3.36} 
\end{align}\]</span></p>
<p>We use ‘<span class="math inline">\(\approx\)</span>’ notation to indicate that we are numerically approximating <span class="math inline">\(\mathbf{A^{-1}}\)</span> rather than using direct solvers. Thus we get:</p>
<p><span class="math display" id="eq:equate1050034">\[\begin{align}
\mathbf{\hat{x}} = A^{-1}b \approx  p_m(A)b \tag{3.37} 
\end{align}\]</span></p>
<p>And in its expanded form:</p>
<p><span class="math display" id="eq:equate1050035">\[\begin{align}
\mathbf{\hat{x}} \approx  p_m(A)b = c_0b + c_1Ab +  c_2A^2b +  c_3A^3b +\ ...\ +  c_mA^{m-1} b \tag{3.38} 
\end{align}\]</span></p>
<p>What we have is a <strong>linear combination</strong> that spans a subspace, <strong>K</strong>:</p>
<p><span class="math display" id="eq:equate1050036">\[\begin{align}
\mathbf{\hat{x}} \approx  p_m(A)b = K_m(A,b) \tag{3.39} 
\end{align}\]</span></p>
<p>We call this subspace <strong>K</strong> the <strong>Krylov subspace</strong> - the span of vectors in multiples of the powers of matrix <strong>A</strong> as shown.</p>
<p>For example (consider <span class="math inline">\(\mathbf{\vec{b}} = \mathbf{\vec{v_0}}\)</span>):</p>
<p><span class="math display" id="eq:equate1050040" id="eq:equate1050039" id="eq:equate1050038" id="eq:equate1050037">\[\begin{align}
A(b) &amp;= Ab &amp; &amp;\rightarrow \text{where} (\mathbf{Ab}) \text{ is a vector denoted as } \mathbf{\vec{v_1}}  \tag{3.40} \\
A(Ab) &amp;= A^2b &amp; &amp;\rightarrow \text{where} (\mathbf{A^2b}) \text{ is a vector denoted as } \mathbf{\vec{v_2}}  \tag{3.41} \\
A(A^2b) &amp;= A^3b &amp; &amp;\rightarrow \text{where} (\mathbf{A^3b}) \text{ is a vector denoted as } \mathbf{\vec{v_3}}  \tag{3.42} \\
&amp; &amp; &amp;\vdots \nonumber \\
A(A^{m-1}b) &amp;= A^{m-1}b &amp; &amp;\rightarrow \text{where} (\mathbf{A^{m-1}b}) \text{  is a vector denoted as  } \mathbf{\vec{v_m}}  \tag{3.43} 
\end{align}\]</span></p>
<p>That creates a subspace that spans the following:</p>
<p><span class="math display" id="eq:equate1050041">\[\begin{align}
K_m(A,b) = span\{ b, Ab, A^2b, A^3b, ..., A^{m-1}b \} 
= \{ \mathbf{\vec{v_0}}, \mathbf{\vec{v_1}}, \mathbf{\vec{v_2}}, \mathbf{\vec{v_3}}, ..., \mathbf{\vec{v_m}} \} \tag{3.44} 
\end{align}\]</span></p>
<p>Because we intend to approximate for <span class="math inline">\(\mathbf{\vec{x}}\)</span>, we can readily say that <span class="math inline">\(\mathbf{\vec{x}}\)</span> is nothing more than a <strong>linear combination</strong> of (almost) <strong>linearly dependent</strong> vectors in <strong>K</strong> subspace. In other words, the approximate solution of <span class="math inline">\(\mathbf{\vec{x}}\)</span> can be anywhere in this subspace - to put it plainly, the solution spans the subspace of K, granting the system of equations is non-singular.</p>
<p><span class="math display">\[
\mathbf{\vec{x}} \in K_m(A,b) \rightarrow \ \ \ \ \mathbf{\vec{x}} \in \{ 
\mathbf{\vec{v_0}}, \mathbf{\vec{v_1}}, \mathbf{\vec{v_2}}, \mathbf{\vec{v_3}}, ..., \mathbf{\vec{v_m}} \}
\rightarrow\ \ \ \ \ \ \mathbf{\vec{x}} \in V_m
\]</span></p>
<p>If the system is singular, there is a chance that the solution may not lie in the space, granting there is even a solution.</p>
<p>Granting there exists a solution, and that our approximate solution <span class="math inline">\(\mathbf{\hat{x}}\)</span> is in the <strong>K</strong> space, <span class="math inline">\(\mathbf{\hat{x}} \in K_m(A,b)\)</span>; it then means:</p>
<p><span class="math display" id="eq:equate1050042">\[\begin{align}
\mathbf{\vec{x}} = A^{-1}b\ \ \ \ \rightarrow\ \ \ \ \mathbf{\hat{x}} \approx K_m\mathbf{\beta}\ \ \ \ \ \ given\ \ \ \ 
\mathbf{\hat{x}} \in K_m(A,b) \tag{3.45} 
\end{align}\]</span></p>
<p>Here, the <strong>solution</strong> denoted as <span class="math inline">\(\mathbf{\hat{x}}\)</span> is an approximation at the <strong>mth</strong> iteration for the <span class="math inline">\(\mathbf{K_m}\)</span> matrix and <span class="math inline">\(\beta\)</span>. The <span class="math inline">\(\beta\)</span> value is derived using the following equation:</p>
<p><span class="math display" id="eq:equate1050043">\[\begin{align}
\beta = ((K_m)^T (K_m))^{-1} (K_m)^T b \tag{3.46} 
\end{align}\]</span></p>
<p>It is not readily apparent that solving for <span class="math inline">\(\mathbf{\hat{x}}\)</span> by iterative approximation gives us the closest solution. For that reason, we minimize the norm of the residual:</p>
<p>We just need to use the following minimal-residual equation:</p>
<p><span class="math display" id="eq:equate1050044">\[\begin{align}
{f(x)}_{min} = \| b - A\mathbf{\vec{x}} \|_{L2} &lt; tol \tag{3.47} 
\end{align}\]</span></p>
<p>and replace <span class="math inline">\(\mathbf{\vec{x}}\)</span> with the approximate <span class="math inline">\(\mathbf{\hat{x}} \approx K_n\beta_n\)</span>:</p>
<p><span class="math display" id="eq:equate1050045">\[\begin{align}
{f(\beta)}_{min} = \| b - A(K_m\beta) \|_{L2} &lt; tol \tag{3.48} 
\end{align}\]</span></p>
<p>To illustrate, given a matrix <strong>A</strong> and <strong>b</strong>:</p>
<p><span class="math display">\[
\mathbf{\vec{x}} = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}^{-1} 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b}
\]</span></p>
<p><strong>First</strong>, initialize our <strong>Krylov matrix</strong>: </p>
<p><span class="math display">\[
K = b\ \ \ \ \rightarrow\ \ \ \ \ K_{0}(A,b) = \{\ b\ \} \rightarrow 
\left\{ \left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} \right\}
\]</span></p>
<p><strong>Then we iterate:</strong> At <span class="math inline">\(m=1\)</span>, we perform the following:</p>
<ul>
<li>multiply the last element of <strong>K</strong> by <strong>A</strong>, e.g., <span class="math inline">\(A \cdotp b\)</span>, adding a new linear combination into the space.</li>
</ul>
<p><span class="math display">\[
K_{1}(A,b) = \{\ b,\ Ab\ \} \rightarrow
K_1 =
\left\{ 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} 
\left[ \begin{array}{rrr} 51 \\ 62 \\ 61 \end{array} \right]_{Ab} 
\right\}
\]</span></p>
<ul>
<li>compute for <span class="math inline">\(\beta\)</span>:</li>
</ul>

<p><span class="math display">\[\begin{align*}
\beta {}&amp;= ((K_1)^T (K_1))^{-1} (K_1)^T b = 
\left(
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}^T
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}
\right)^{-1}
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}^T
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} \\
\\
\beta &amp;= 0.0965965
\end{align*}\]</span>
</p>
<ul>
<li>compute for first approximate of <span class="math inline">\(\mathbf{\hat{x}}\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{x}} = K_1 \cdotp \beta =
\left[ \begin{array}{rrr} 6 &amp; 51 \\ 5 &amp; 62 \\ 6 &amp; 61\end{array} \right]_{K_1}
(0.0965965)_{\beta} =
\left[ \begin{array}{rrr} 0.5795790 \\ 0.4829825 \\ 0.5795790 \end{array} \right]
\]</span></p>
<ul>
<li>Compute for residual:</li>
</ul>
<p><span class="math display">\[\begin{align*}
r_{min} = \| b - A\mathbf{\hat{x}} \|_{L2} =  
\left\|
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} -
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
\left[ \begin{array}{rrr} 0.5795790 \\ 0.4829825 \\ 0.5795790 \end{array} \right]_{\mathbf{\hat{x}}}
\right\|_{L2} = 1.463639
\end{align*}\]</span></p>
<ul>
<li>check if residual reaches tolerance level (e.g. <strong>1e-5</strong>):</li>
</ul>
<p><span class="math display">\[
if\ (\ (r_{min} = 1.463639) &lt; tol\ ) \text{ stop iteration}
\]</span></p>
<ul>
<li>Otherwise, continue for next iteration at n = 2 …</li>
</ul>
<p><strong>We continue to iterate:</strong> At <span class="math inline">\(m=2\)</span>, we perform the following:</p>
<ul>
<li>multiply last element of <strong>K</strong> by <strong>A</strong>, e.g., <span class="math inline">\(A \cdotp Ab\)</span>, adding a new linear combination into the space.</li>
</ul>
<p><span class="math display">\[
K_{2}(A,b) = \{\ b,\ Ab,\ A^2b\ \} \rightarrow
K_2 =
\left\{ 
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} 
\left[ \begin{array}{rrr} 51 \\ 62 \\ 61 \end{array} \right]_{Ab} 
\left[ \begin{array}{rrr} 522 \\ 655 \\ 666 \end{array} \right]_{A^2b} 
\right\}
\]</span></p>
<ul>
<li>compute for <span class="math inline">\(\beta\)</span>:</li>
</ul>
<p><span class="math display">\[\begin{align*}
\beta {}&amp;= ((K_2)^T (K_2))^{-1} (K_2)^T b = 
\left[ \begin{array}{rrr} 0.32604333 \\ -0.02162618 \end{array} \right]
\end{align*}\]</span></p>
<ul>
<li>compute for second approximate of <span class="math inline">\(\mathbf{\hat{x}}\)</span>:</li>
</ul>
<p><span class="math display">\[
\mathbf{\hat{x}} = K_2 \cdotp \beta =
\left[ \begin{array}{rrr} 6 &amp; 51 &amp; 522 \\ 5 &amp; 62 &amp; 655 \\ 6 &amp; 61 &amp; 666\end{array} \right]_{K_2}
\left[ \begin{array}{rrr} 0.32604333 \\ -0.02162618 \end{array} \right]_{\beta} =
\left[ \begin{array}{rrr} 0.8533248 \\ 0.2893935 \\ 0.6370630 \end{array} \right]
\]</span></p>
<ul>
<li>Compute for residual:</li>
</ul>
<p><span class="math display">\[\begin{align*}
r_{min} = \| b - A\mathbf{\hat{x}} \|_{L2} =  
\left\|
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_{b} -
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
\left[ \begin{array}{rrr} 0.8533248 \\ 0.2893935 \\ 0.6370630 \end{array} \right]_{\mathbf{\hat{x}}}
\right\|_{L2} = 1.342609
\end{align*}\]</span></p>
<ul>
<li>check if residual reaches tolerance level (e.g. <strong>1e-5</strong>):</li>
</ul>
<p><span class="math display">\[
if\ (\ (r_{min} = 1.342609) &lt; tol\ ) \text{ stop iteration}
\]</span></p>
<ul>
<li>Otherwise, continue for next iteration at n = 3 …</li>
</ul>
<p><strong>After the third iteration</strong>, we get the approximate solution <span class="math inline">\(\mathbf{\hat{x}} = \left[ \begin{array}{rrr} 1 &amp; 2 &amp; -1 \end{array} \right]^T\)</span></p>
<p>hitting the following residual:</p>
<p><span class="math display">\[
r_{min} = \text{8.683085e-12}
\]</span></p>
<p>Let us review a naive implementation of a simple Krylov method in R code:</p>

<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb62-1" data-line-number="1">naive_krylov_method &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb62-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb62-3" data-line-number="3">    K =<span class="st"> </span>b <span class="co">#  start with K(A,b) = {b}</span></a>
<a class="sourceLine" id="cb62-4" data-line-number="4">    limit=<span class="dv">100</span></a>
<a class="sourceLine" id="cb62-5" data-line-number="5">    tol =<span class="st"> </span><span class="fl">1e-5</span></a>
<a class="sourceLine" id="cb62-6" data-line-number="6">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb62-7" data-line-number="7">        v =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>K</a>
<a class="sourceLine" id="cb62-8" data-line-number="8">        beta =<span class="st"> </span><span class="kw">solve</span>( <span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>v, <span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>b) </a>
<a class="sourceLine" id="cb62-9" data-line-number="9">        appr_x =<span class="st">   </span>K <span class="op">%*%</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb62-10" data-line-number="10">        min_r =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>appr_x)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb62-11" data-line-number="11">        <span class="cf">if</span> (min_r <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb62-12" data-line-number="12">        <span class="co"># keep building krylov space, K(A,b) = {b,Ab,...}</span></a>
<a class="sourceLine" id="cb62-13" data-line-number="13">        K  =<span class="st">  </span><span class="kw">matrix</span>( <span class="kw">cbind</span>(K, v[,<span class="kw">c</span>(m)]),  n )</a>
<a class="sourceLine" id="cb62-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb62-15" data-line-number="15">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span> =<span class="st"> </span><span class="kw">c</span>(x), <span class="st">&quot;residual&quot;</span> =<span class="st"> </span>min_r)</a>
<a class="sourceLine" id="cb62-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb62-17" data-line-number="17">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb62-18" data-line-number="18">x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="op">-</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb62-19" data-line-number="19">b &lt;-<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x</a>
<a class="sourceLine" id="cb62-20" data-line-number="20"><span class="kw">naive_krylov_method</span>(A,b)</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $b
## [1] 6 5 6
## 
## $x
## [1]  1  2 -1
## 
## $residual
## [1] 8.683085e-12</code></pre>

<p>It is notable to mention that the <strong>Krylov matrix</strong> is a linear combination of multiples of <strong>A</strong> and a fixed <strong>b</strong>; in that respect, the <strong>K</strong> matrix becomes ill-conditioned. We can apply preconditioners <span class="math inline">\(M^{-1}\)</span> to <strong>A</strong> to make it well-conditioned.</p>
<p>Successive computation of the normalized residual appears to converge but then eventually diverges further away from zero. That is because the <strong>K space</strong> starts to accumulate vectors that are parallel to each other.</p>
<p>We need an orthonormal basis that spans the <strong>Krylov space</strong>. In the next section, we discuss GMRES, which offers that approach.</p>
</div>
<div id="gmres-generalized-minimal-residual" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.2</span> GMRES (Generalized Minimal Residual)  <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Relevant to <strong>Krylov methods</strong>, let us cover the <strong>GMRES</strong> method developed by Yousef Saad and Martin Schultz <span class="citation">(<a href="bibliography.html#ref-ref297y">1986</a>)</span>, which is used in solving nonsymmetric systems of equations. It may help to recall Figure <a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fig:krylovspace">3.1</a> - particularly, to review the left figure. As we explain the figure further, there are two important notes to emphasize.</p>
<p><span class="math inline">\(\mathbf{Ax}\)</span> as projection:</p>
<p><span class="math display" id="eq:equate1050046">\[\begin{align}
Ax \parallel K_m(A,b)  \tag{3.49} 
\end{align}\]</span></p>
<p><span class="math inline">\(\mathbf{(b - Ax)}\)</span> as orthogonal projection:</p>
<p><span class="math display" id="eq:equate1050047">\[\begin{align}
b - Ax \perp K_m(A,b)   \tag{3.50} 
\end{align}\]</span></p>
<p>On the one hand, <span class="math inline">\(\mathbf{Ax}\)</span> is the projection unto K and thus is parallel to K. On the other hand, <span class="math inline">\(\mathbf{(b - Ax)}\)</span> is the orthogonal projection unto K. And of course, <span class="math inline">\(\mathbf{b}\)</span> is being projected.</p>
<p>We can also denote the orthogonal relationship as:</p>
<p><span class="math display" id="eq:equate1050048">\[\begin{align}
r \perp K_m(A,r) \ \ \ \ \ where\ (r = b - Ax) \tag{3.51} 
\end{align}\]</span></p>
<p>Now, recall the following equation from <strong>Arnoldi</strong> iteration:</p>
<p><span class="math display">\[
AQ_m = Q_mH_m\ \ \leftarrow \ \ \text{Ignore last row of H where } H_{m+1,m}=0\ \ \ \  \therefore\ Q_{m,m} \leftarrow Q_{m,m+1}
\]</span></p>
<p>Here, our <strong>GMRES</strong> method uses the orthogonal projection to construct the <span class="math inline">\(Q\)</span> basis for the <strong>K space</strong>. For example, given an initial arbitrary approximation for <span class="math inline">\(x^0\)</span>:</p>
<p><span class="math display" id="eq:equate1050049">\[\begin{align}
r^0 = b - Ax^0\ \ \ \ \rightarrow\ \ \ \ \ \ \ q = \frac{r^0}{\| r^0 \|_{L2}} \tag{3.52} 
\end{align}\]</span></p>
<p>we then build our <strong>K space</strong> out of <span class="math inline">\(q\)</span>:</p>
<p><span class="math display" id="eq:equate1050050">\[\begin{align}
K_m(A,q) = span\ \{\ q,\ Aq,\ A^2q,\ A^3q,\ ..., A^{m-1}q \} 
= \{\ q_1,\ q_2,\ q_3,\ ...,\ q_m\ \} \tag{3.53} 
\end{align}\]</span></p>
<p>As such, we get an orthonormal basis in the form of <span class="math inline">\(Q\)</span>:</p>
<p><span class="math display" id="eq:equate1050051">\[\begin{align}
Q_{basis} = \{\ q_1,\ q_2,\ q_3,\ ...,\ q_m\ \} \tag{3.54} 
\end{align}\]</span></p>
<p>We use <strong>Arnoldi</strong> method to iterate and build our <span class="math inline">\(Q\)</span> basis (with <strong>Gram-Schmid</strong> orthogonalization - see also <strong>Matrix manipulation</strong> in <strong>Linear Algebra</strong> chapter for the orthogonal projections):</p>
<p><span class="math display" id="eq:equate1050055" id="eq:equate1050054" id="eq:equate1050053" id="eq:equate1050052">\[\begin{align}
q_1 {}&amp;= \frac{r_0}{\| r_0 \|_{L2}} = q \tag{3.55} \\
q_2 &amp;= \frac{Aq_1 - ( h_{1,1}q_1 ) }{\| Aq_1 - ( h_{1,1}q_1 )  \|}  \tag{3.56} \\
q_3 &amp;= \frac{Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) }{\| Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) \|}  \tag{3.57} \\
\vdots \nonumber \\
q_{j+1} &amp;= \frac{Aq_j - ( h_{1,j}q_1 + h_{2,j}q_2 + ... + h_{j,j}q_j ) }
{\| Aq_{j} - ( h_{1,j}q_j + h_{2,j}q_2 + ... + h_{j,j}q_j ) \|}   \tag{3.58} 
\end{align}\]</span></p>
<p>Now, if there is such a target approximate solution vector <span class="math inline">\(\mathbf{\hat{x}}\)</span> that lies in the <strong>K space</strong> such that <span class="math inline">\(\mathbf{\hat{x}} \in K_m(A, q)\)</span>, then <span class="math inline">\(\mathbf{\hat{x}}\)</span> must be a linear combination and thus we can write <span class="math inline">\(\mathbf{\hat{x}}\)</span> as such:</p>
<p><span class="math display">\[
\mathbf{\hat{x}} = c_1q_1 + c_2q_2  + ... + c_mq_m = 
\left[
\begin{array}{rrrrr}
q_{1,1} &amp; q_{1,2} &amp; ... &amp; q_{1,m} \\
q_{2,1} &amp; q_{2,2} &amp; ... &amp; q_{2,m} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
q_{n,1} &amp; q_{n,2} &amp; ... &amp; q_{n,m} \\
\end{array}
\right]_{Q_m}
\left[
\begin{array}{rrrrr}
c_1 \\ c_2 \\ \vdots \\ c_m
\end{array}
\right]_\beta = Q_m\beta
\]</span></p>
<p>Out of which, we formulate our equation for the solution:</p>
<p><span class="math display" id="eq:equate1050056">\[\begin{align}
x = x^0 +  Q_m\beta\ \ \ \ \leftarrow\ \ \ \ x = x^0 + \mathbf{\hat{x}} \tag{3.59} 
\end{align}\]</span></p>
<p>which is derived based on the following mathematical manipulation:</p>
<p><span class="math display">\[
\begin{array}{llll}
Ax   &amp;= b        &amp;\leftarrow&amp; \text{equation for our target solution} \\
Ax^0 &amp;= b - r^0  &amp;\leftarrow&amp; \text{equation for our initial solution}  
\end{array}
\]</span></p>
<p>Because our initial solution is not the actual solution, therefore, we expect some residual <span class="math inline">\(\mathbf{r^0}\)</span></p>
<p>Cancelling out <span class="math inline">\(\mathbf{\vec{b}}\)</span>, we then get:</p>

<p><span class="math display" id="eq:equate1050059" id="eq:equate1050058" id="eq:equate1050057">\[\begin{align}
A(x - x^0) = r^0\ \ \ \ {}&amp;\rightarrow x - x^0 \in K_m(A, r^0)  \tag{3.60} \\
&amp;\rightarrow x \in x^0 + K_m(A, r^0)  \tag{3.61} \\
&amp;\rightarrow x = x^0 + Q_m\beta  \tag{3.62} 
\end{align}\]</span>
</p>
<p>The <span class="math inline">\(\beta\)</span> is a minimizer. Before we compute for <span class="math inline">\(\beta\)</span>, let us consider a few equations first using the <span class="math inline">\(Q\)</span> basis. We know that:</p>
<p><span class="math display" id="eq:equate1050060">\[\begin{align}
r^0 =  q \| r^0 \|_{L2}\ \ \ \leftarrow \ \ \ \ \ \ q = \frac{r^0}{\| r^0 \|_{L2}} \tag{3.63} 
\end{align}\]</span></p>
<p>We also know that <span class="math inline">\(q\)</span> is the first vector in the <span class="math inline">\(Q\)</span> basis, and can thus be written as such:</p>
<p><span class="math display" id="eq:equate1050061">\[\begin{align}
q = Q_{m}e_1 \in R^n \tag{3.64} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
 e_1 = (1,0,0, ..., 0)\ \ \ \leftarrow \text{use to extract first column}
\]</span></p>
<p>Therefore:</p>

<p><span class="math display" id="eq:equate1050065" id="eq:equate1050064" id="eq:equate1050063" id="eq:equate1050062">\[\begin{align}
r^0 {}&amp;=  \| r^0 \|_{L2}\ q  \tag{3.65} \\
r^0 {}&amp;=  \| r^0 \|_{L2}\ Q_{m}e_1  \tag{3.66} \\
Q_{m}^Tr^0 &amp;=  \| r^0 \|_{L2}\ Q_{m}^TQ_{m}\ e_1  \tag{3.67} \\
Q_{m}^Tr^0  &amp;=  \| r^0 \|_{L2}e_1 \tag{3.68} 
\end{align}\]</span>
</p>
<p>Now, to compute for <span class="math inline">\(\beta\)</span>, we perform a few mathematical manipulation.</p>

<p><span class="math display" id="eq:equate1050073" id="eq:equate1050072" id="eq:equate1050071" id="eq:equate1050070" id="eq:equate1050069" id="eq:equate1050068" id="eq:equate1050067" id="eq:equate1050066">\[\begin{align}
b - Ax {}&amp;= 0 &amp; {}&amp; \leftarrow Ax = b  \tag{3.69} \\
b - A(x^0 + Q_{m}\beta ) &amp;= 0 &amp; &amp; \leftarrow x = x^0 + Q_{m}\beta  \tag{3.70} \\
b - Ax^0 - AQ_{m}\beta &amp;= 0   \tag{3.71} \\
r^0 -  AQ_{m}\beta &amp;= 0 &amp; &amp; \leftarrow r^0 = b - Ax^0  \tag{3.72} \\
r^0 -  Q_{m}H_{m}\beta &amp;= 0 &amp; &amp; \leftarrow AQ_{m} = Q_{m}H_{m}  \tag{3.73} \\
Q_{m}H_{m}\beta &amp;= r^0  &amp; &amp;  \tag{3.74} \\
\beta &amp;= H_{m}^{-1}Q_{m}^Tr^0 &amp; &amp; \leftarrow Q_{m}^T = Q_{m}^{-1}\ \ \ \{orthogonal\}   \tag{3.75} \\
\beta &amp;=  H_{m}^{-1}\| r^0\|_{L2} e1 &amp; &amp; \leftarrow Q_{m}^T r^0 = \| r^0\|_{L2}e1 \tag{3.76} 
\end{align}\]</span>
</p>
<p><strong>Finally</strong>, let us now review the <strong>GMRES</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
r^0 = b - Ax^0 \\
&lt;Q,H&gt; = arnoldi\_iteration(A, r^0) \\
\rho = \| r^0 \|_{L2} \in R \\
\beta = ( H^{-1} \times \rho ) \cdotp e1 \in R^n \\
x = x^0 + Q \cdotp \beta
\end{array}
\]</span></p>
<p>Below is a naive implementation of <strong>GMRES</strong> in R code:</p>

<div class="sourceCode" id="cb64"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb64-1" data-line-number="1">gmres &lt;-<span class="st"> </span><span class="cf">function</span>(A, b) {</a>
<a class="sourceLine" id="cb64-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb64-3" data-line-number="3">    e1 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb64-4" data-line-number="4">    x0 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span>, n)</a>
<a class="sourceLine" id="cb64-5" data-line-number="5">    r0 =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x0</a>
<a class="sourceLine" id="cb64-6" data-line-number="6">    arnoldi =<span class="st"> </span><span class="kw">arnoldi_method</span>(A, r0 )</a>
<a class="sourceLine" id="cb64-7" data-line-number="7">    rho =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>( ( r0 )<span class="op">^</span><span class="dv">2</span> )) <span class="co"># minimize normal residual</span></a>
<a class="sourceLine" id="cb64-8" data-line-number="8">    beta =<span class="st"> </span>(<span class="kw">solve</span>(arnoldi<span class="op">$</span>H) <span class="op">*</span><span class="st"> </span>rho) <span class="op">%*%</span><span class="st"> </span>e1</a>
<a class="sourceLine" id="cb64-9" data-line-number="9">    x =<span class="st"> </span>x0 <span class="op">+</span><span class="st"> </span>arnoldi<span class="op">$</span>Q <span class="op">%*%</span><span class="st"> </span>beta</a>
<a class="sourceLine" id="cb64-10" data-line-number="10">    <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span>=A, <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span>=<span class="kw">c</span>(x))</a>
<a class="sourceLine" id="cb64-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb64-12" data-line-number="12">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb64-13" data-line-number="13">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb64-14" data-line-number="14"><span class="kw">gmres</span>(A, b)</a></code></pre></div>
<pre><code>## $matrix
##      [,1] [,2] [,3]
## [1,]    3    3    3
## [2,]    2    4    5
## [3,]    1    5    5
## 
## $b
## [1] 6 5 6
## 
## $x
## [1]  1  2 -1</code></pre>

<p>There are other variations (or modifications) to the original <strong>GMRES</strong> method - introduced by Saad Schultz (1986). We leave them for the readers to investigate.</p>
<p>When it comes to iterative methods in solving for systems of equations, especially suitable for large sparse matrices, there are two other <strong>Krylov-based</strong> iterative methods to mention: <strong>Generalized Minimal Residual (GMRES)</strong> method and <strong>Conjugate Gradient (CG)</strong> method.</p>
<p>The <strong>GMRES</strong> method allows for solving systems of equations whose corresponding matrices are highly sparse. Two other Krylov-based iterative methods are illustrated next.</p>
</div>
<div id="conjugate-gradient-method-cg" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.3</span> Conjugate Gradient Method (CG)  <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Conjugate Gradient</strong> method is suitable for <strong>symmetric positive definite (SPD)</strong> matrices.</p>
<p>There are two ways to look at this.</p>
<p><strong>First way to look at this: </strong> <strong>CG</strong> method is a <strong>Krylov-base</strong> method.</p>
<p>The idea is that if <span class="math inline">\(\mathbf{x}\)</span> is the true solution, then it follows that:</p>
<p><span class="math display">\[
Ax - b = 0
\]</span></p>
<p>But because we are clueless about what the actual value is, we can only approximate for <span class="math inline">\(\mathbf{x}\)</span> iteratively starting with an initial <strong>guess</strong> - call it <span class="math inline">\(\mathbf{x_0}\)</span>. In effect, an approximation yields a residual - call it <span class="math inline">\(\mathbf{r_0}\)</span>. Thus, we modify the equation slightly to account for the residual:</p>
<p><span class="math display">\[
Ax_0 - b= r_0
\]</span></p>
<p>Let us take those two equations to derive an equation that incorporates our approximation, and that manifests a <strong>Krylov subspace</strong>:</p>
<p><span class="math display" id="eq:equate1050079" id="eq:equate1050078" id="eq:equate1050077" id="eq:equate1050076" id="eq:equate1050075" id="eq:equate1050074">\[\begin{align}
Ax {}&amp;= b,\ \ \ \ Ax_0 = b - r_0  \tag{3.77} \\
Ax - Ax_0 &amp;= b - (b - r_0)  \tag{3.78} \\
A(x - x_0) &amp;= r_0  \tag{3.79} \\
(x - x_0) &amp;= A^{-1} r_0  \tag{3.80} \\
x &amp;= x_0 + A^{-1} r_0  \tag{3.81} \\
x &amp;= x_0 + K_m\beta\ \ \ \leftarrow\ \ \ \ if\ x \in x_0 + K_m(A, r_0)\ \tag{3.82} 
\end{align}\]</span></p>
<p>The derivation looks familiar because that is discussed in the <strong>GMRES</strong> method. It comes down to how <strong>GMRES</strong> and <strong>CG</strong> handle the minimizer differently.</p>
<p>In <strong>GMRES</strong> method, the norm of the residual <span class="math inline">\(\| \mathbf{r_k}\|\)</span> is minimized.</p>
<p><span class="math display" id="eq:equate1050080">\[\begin{align}
arg\ min_{x \in K_k} \|Ax_k - b = r_k \|_{L2} \tag{3.83} 
\end{align}\]</span></p>
<p>In <strong>CG</strong> method, the norm of the <strong>error energy</strong> <span class="math inline">\(\| \mathbf{e_k} = x - xk\|\)</span> - also called <strong>A-norm</strong> - is minimized.</p>
<p><span class="math display" id="eq:equate1050081">\[\begin{align}
arg\ min_{x \in K_k} \|x - x_k   \|_A \equiv (x - x_k )^TA(x - x_k)   \tag{3.84} 
\end{align}\]</span></p>
<p>That also becomes apparent when we discuss <strong>steepest descent</strong> where <strong>direction</strong> is optimal if it is orthogonal to the tangent line (See Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:gradientconjugacy">3.9</a>). We will cover more of that statement later.</p>
<p>See <strong>Lanczos</strong> algorithm as a reference to derive <strong>Krylov-based CG</strong> method. It resembles the algorithm we discuss next for a second way of looking at the <strong>CG</strong> method.</p>
<p><strong>Second way to look at this: </strong> If we take the <strong>CG</strong> method as a modified <strong>Steepest Descent</strong> method, let us first understand the concept behind the <strong>Steepest Descent</strong> method. The plotted graph in Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:gradientconjugacy">3.9</a> may help us to get an intuition around the <strong>Steepest Descent</strong> method and the <strong>CG</strong> method. Note that the figure assumes an optimal direction orthogonal to the tangent line. In practice, <strong>Steepest Descent</strong> tends toward a crooked path.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gradientconjugacy"></span>
<img src="embed0006.png" alt="Gradient and Conjugacy" width="70%" />
<p class="caption">
Figure 3.9: Gradient and Conjugacy
</p>
</div>

<p>As shown, the <strong>black arrows</strong> denoted by <span class="math inline">\((\ d_0,\ d_1,\ d_2,\ d_3,\ d_4\ )\)</span> are the vectors called <strong>gradients</strong> - each <strong>gradient</strong> represents the direction (or the path) of travel from a starting point (black dot) to a target point (red dot). A single vector with partial derivative elements is a <strong>gradient</strong> with a <span class="math inline">\(\nabla\)</span> symbol. For example:</p>
<p><span class="math display" id="eq:eqnnumber2">\[\begin{align}
\nabla f(x) = f&#39;(x) =
\left[
\begin{array}{rrrrr}
\frac{ \partial f }{\partial x_1 } &amp; \frac{ \partial f }{\partial x_2 } &amp; ... &amp;
\frac{ \partial f }{\partial x_n }
\end{array}
\right]^T\ \ \ \ \leftarrow \text{ gradient of } f(x) \in R^n \tag{3.85}
\end{align}\]</span></p>
<p>Here, for the sake of explanation, <strong>gradient</strong> <span class="math inline">\(\nabla f(x_n)\)</span> equals the <strong>direction</strong> <span class="math inline">\(d_n\)</span>. Other methods may manipulate the <strong>direction</strong> however for better optimization which becomes apparent later.</p>
<p><span class="math display" id="eq:equate1050082">\[\begin{align}
\nabla f(x_n) = d_n \tag{3.86} 
\end{align}\]</span></p>
<p>In the figure, each <strong>gradient</strong> obeys <strong>conjugacy</strong> between two contour lines if the direction of its path is orthogonal to the curves (contour lines) that it intersects. For example, there are nine contour lines labeled consecutively (5, 10, 15, 20, 25, 30). The point (in black) at which we start our travel intersects at contour line 25. We make a <span class="math inline">\(90^\circ\)</span> (orthogonal) hop to the next contour line at 20. In other words, the direction is orthogonal to the tangent line that follows the slope of the curvatures of the contour lines.</p>
<p>The length of a gradient (e.g. <span class="math inline">\(d_n\)</span>) is denoted as <span class="math inline">\(\mathbf{\alpha_n}\)</span>. We also call the length a <strong>stepsize</strong> (or <strong>steplength</strong>). It can be seen that a combination of the direction (the gradient) and steplength, <span class="math inline">\(\alpha_nd_n\)</span>, represents a <strong>step</strong> to the next sub-solution <span class="math inline">\(\mathbf{x_{k+1}}\)</span> - where <span class="math inline">\(\mathbf{x_k}\)</span> is the current position - which is one step closer towards the final destination. It can be expressed in an equation called the <strong>line search method</strong>:</p>
<p><span class="math display" id="eq:equate1050083">\[\begin{align}
x_{k+1} = x_k + \alpha_{k}d_{k} \tag{3.87} 
\end{align}\]</span></p>
<p>Note that the graph in Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:gradientconjugacy">3.9</a> is quite tuned to try to reach an explanation around the concept of <strong>gradient</strong> and <strong>conjugacy</strong> - however, <strong>steepest descent</strong> and <strong>conjugate gradient</strong> do not necessarily walk the path that lands orthogonal unto each contour lines. Two directions are used to walk the path: the <strong>gradient direction</strong> and the <strong>conjugate direction</strong>.</p>
<p>The <strong>steepest descent</strong> walks the path in a <strong>gradient direction</strong>. Mathematically, it is expressed as:</p>
<p><span class="math display" id="eq:equate1050084">\[\begin{align}
x_{k+1} = x_{k} + \mathbf{\alpha_k}\nabla f(x_k)  \tag{3.88} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\nabla f(x_k)\)</span> is the gradient direction.</p>
<p>The method iterates all the way until it reaches a <strong>critical point</strong> - the solution <span class="math inline">\(\mathbf{x^*}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{x_1} =  x_{0} + \mathbf{\alpha_0}\nabla f(x_0),\ \ \ \ \ \ \ \ \ \
\mathbf{x_2} =  x_{1} + \mathbf{\alpha_1}\nabla f(x_1),\ \ \ \ \ \ \ \ \ \
\mathbf{x_3} =  x_{2} + \mathbf{\alpha_2}\nabla f(x_2)
\end{align*}\]</span></p>
<p><span class="math display" id="eq:equate1050086" id="eq:equate1050085">\[\begin{align}
\mathbf{x^*} &amp;= (( x_{0} + \mathbf{\alpha_0} \nabla f(x_0)) + \mathbf{\alpha_1} \nabla f(x_1)) + \mathbf{\alpha_2} \nabla f(x_2)  \tag{3.89} \\
\mathbf{x^*} &amp;= x_{0} + \sum_k^m \mathbf{\alpha_k} \nabla f(x_k) \tag{3.90} 
\end{align}\]</span></p>
<p>It is notable to talk about <strong>gradient descent</strong> as a side note. The difference between <strong>gradient descent</strong> and <strong>steepest descent</strong> is the use of the <strong>stepsize</strong>. In <strong>gradient descent</strong>, the <strong>stepsize</strong> is initialized to a fixed scalar value and then used repeatedly in the iteration:</p>
<p><span class="math display" id="eq:equate1050087">\[\begin{align}
\alpha {}&amp;=  &lt;initial\ value&gt; \nonumber \\
\mathbf{x^*} &amp;= x_{0} + \sum_k^m \mathbf{\alpha} \nabla f(x_k)   \tag{3.91} 
\end{align}\]</span></p>
<p>As for <strong>conjugate descent</strong>, the method follows a <strong>gradient direction</strong> as the first step, similar to <strong>steepest descent</strong>; after which, the rest of the steps follow a <strong>conjugate direction</strong>.</p>
<p><span class="math display" id="eq:eqnnumber700">\[\begin{align}
d_k = \begin{cases}
\nabla (f{x_k}) &amp; if\ k=0\ \ \ \ \ \ \leftarrow \text{gradient direction}\\ 
\nabla (f{x_k}) + \beta_k d_{k-1} &amp;  if\ k &gt;= 1\ \ \  \leftarrow \text{conjugate direction}
\end{cases} \tag{3.92}
\end{align}\]</span></p>
<p>Here, a <strong>conjugate direction</strong> is a direction in which two vectors, u and v, are A-orthogonal (or conjugate) which is a <strong>conjugate property</strong> expressed as:</p>
<p><span class="math display" id="eq:equate1050088">\[\begin{align}
&lt;u,v&gt;_A = u^TAv = 0, \ \ \ \rightarrow\ \ \ \ \  d_{k}^TAd_{k+1} = 0  \tag{3.93} 
\end{align}\]</span></p>
<p>Set that aside for a moment. In terms of deriving <span class="math inline">\(\alpha_k\)</span>, let us first have a brief description of <strong>conjugate gradient</strong>. The <strong>conjugate gradient</strong> method was introduced by <strong>Hestenes and Stiefel (1952)</strong> with the intent to minimize a non-linear quadratic function: </p>
<p><span class="math display" id="eq:equate1050089">\[\begin{align}
arg\ min_{x \in R^n} \ f(x) = \frac{1}{2}x^TAx - b^Tx + c. \tag{3.94} 
\end{align}\]</span></p>
<p>But by minimizing the function - solving for the <strong>gradient</strong> - it thus also ends up <strong>linearizing</strong> the equation; and, in effect, solving for a linear equation:</p>
<p><span class="math display" id="eq:equate1050090">\[\begin{align}
\nabla f(x) = f&#39;(x) = Ax - b \tag{3.95} 
\end{align}\]</span></p>
<p>Additionally, because we deal with approximation, it is notable to mention here that in the <strong>conjugate gradient</strong> method, the first step takes <strong>gradient</strong> not only as the <strong>gradient direction</strong> but also as the <strong>residual</strong>; after which, the rest of the steps take the <strong>conjugate gradient</strong> as the <strong>residual</strong>:</p>
<p><span class="math display" id="eq:eqnnumber701">\[\begin{align}
r_k = \begin{cases} 
\nabla f(x_k) &amp; if\ k=0\ \ \ \ \ \ \ \leftarrow \text{gradient}\\ 
\nabla f(x_{k-1}) - \alpha_{k-1} A d_{k-1} &amp;  if\ k &gt;= 1\ \ \ \ \leftarrow \text{conjugate gradient}
\end{cases} \tag{3.96}
\end{align}\]</span></p>
<p>Furthermore, we notice the emergence of <strong>alpha</strong> <span class="math inline">\(\alpha\)</span> and <strong>beta</strong> <span class="math inline">\(\beta\)</span> symbols. It is essential to mention that there are choices for solving <span class="math inline">\(\beta_k\)</span> (<strong>conjugate gradient parameter</strong>) and <span class="math inline">\(\alpha_k\)</span> (<strong>stepsize</strong>); though, we leave these choices for the readers to further investigate <span class="citation">(Hager W. W. and Zhang H. <a href="bibliography.html#ref-ref306w">2005</a>)</span>:</p>
<ul>
<li>Hestenes-Stiefel (1952)</li>
<li>Fletcher-Reeves (1964)</li>
<li>Polak-Ribiere-Polyak (1969)</li>
<li>Liu-Storey (1991)</li>
<li>Dai-Yuan (1999)</li>
<li>Hager-Zhang (2005)</li>
</ul>
<p>For example, <strong>Polak-Riebre</strong> method computes for <strong>stepsize</strong> <span class="math inline">\(\alpha_k\)</span> using the following equation: </p>
<p><span class="math display" id="eq:equate1050091">\[\begin{align}
\alpha_k 
= \frac{\nabla f_(x_k)^Td_k}{d_k^TAd_k}
 =\frac{r_k^Td_k}{d_k^TAd_k}
\ \ \ \ \ \rightarrow\ \ \ \  \ where\ \nabla f(x_k)  = r_k = Ax_k - b  \tag{3.97} 
\end{align}\]</span></p>
<p>In our case, we use the <strong>Fletcher-Reeves</strong> method to compute for <span class="math inline">\(\alpha_k\)</span> expressed as such (with no complete derivation included): </p>
<p><span class="math display" id="eq:equate1050092">\[\begin{align}
\alpha_k = \frac{d_k^Td_k}{d_k^TAd_k} \tag{3.98} 
\end{align}\]</span></p>
<p>As for <span class="math inline">\(\beta_k \in R\)</span>, we need this <strong>conjugate gradient parameter</strong> to compute for the next <strong>conjugate direction</strong> <span class="math inline">\(d_{k+1}\)</span>:</p>
<p><span class="math display" id="eq:equate1050093">\[\begin{align}
d_{k+1} = r_{k} + \beta_kd_k \tag{3.99} 
\end{align}\]</span></p>
<p>For that, we also use the <strong>Fletcher-Reeves</strong> method to compute for <span class="math inline">\(\beta_k\)</span> (with no complete derivation included):</p>
<p><span class="math display" id="eq:equate1050094">\[\begin{align}
\beta_k = \frac{r_{k+1}^TAd_k}{d_k^TAd_k} = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \tag{3.100} 
\end{align}\]</span></p>
<p>Overall, suppose we are to find our way to the final destination (<strong>the critical point</strong>) - the final approximate solution, which is <span class="math inline">\(\mathbf{x^*} \in R^n\)</span>. In that case, we need a linear combination of all the paths (which is linearly independent, granting A-orthogonality is zero). In our case, our target is to reach zero - our solution for <span class="math inline">\(\mathbf{x^*}\)</span>.</p>
<p><span class="math display" id="eq:equate1050095">\[\begin{align}
x^* &amp;= x_0 + \sum_{k=1}^m \alpha_{k-1}d_{k-1}  \tag{3.101} \\
&amp;= x_0 + \alpha_0d_0 + \alpha_1d_1 + \alpha_2d_2 + \alpha_3d_3 + ... +  \alpha_{m-1}d_{m-1} = 0 \nonumber
\end{align}\]</span></p>
<p>As final touch, we <strong>minimize</strong> gradient, <span class="math inline">\(r_k = \nabla f = 0\)</span>, which we use to gauge for convergence. The <strong>goal</strong> is to reach tolerance level (e.g. tol=1e-5) which is zero by using the norm of the gradient:</p>
<p><span class="math display">\[
\| r_k  \| &lt; tol
\]</span></p>
<p>With all that explained, let us now illustrate <strong>CG</strong> using <strong>SPD</strong> matrix <strong>A</strong> and vector <strong>b</strong>:</p>
<p><span class="math display">\[
Ax = b\ \ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_x =
\left[\begin{array}{r} 30 \\ 50 \\ 50 \\ 20 \end{array}\right]_b
\]</span>
<strong>First</strong>, initialize <span class="math inline">\(x = x^0\)</span>:</p>
<p><span class="math display">\[
x_0 = \left[\begin{array}{rrrr} 1 &amp; 2 &amp; 1 &amp; 1\end{array}\right]^T
\]</span></p>
<p><strong>Second</strong>, compute for the initial direction <span class="math inline">\(\mathbf{d_0}\)</span> - the <strong>first gradient direction of the steepest descent</strong> equivalent to the <strong>first residual</strong>:</p>
<p><span class="math display">\[
r_0 = d_0 = b - Ax_0 = 
\left[\begin{array}{r} 30 \\ 50 \\ 50 \\ 20 \end{array}\right]_b -
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 1 \\ 2 \\ 1 \\ 1 \end{array}\right]_{x_0} =
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
\]</span>
where <span class="math inline">\(d_0\)</span> is the <strong>gradient direction</strong>; meaning, we come from a zero starting point (the black dot in Figure <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#fig:gradientconjugacy">3.9</a>).</p>
<p><strong>Third</strong>, start the iteration by computing for <strong>stepsize</strong> <span class="math inline">\(\mathbf{\alpha_0}\)</span> using <strong>Fletcher-Reeves</strong> equation:</p>
<p><span class="math display">\[
\alpha_0 = \frac{r_0^Tr_0}{d_0^TAd_0} = 
\frac{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}^T
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
}
{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0}^T
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0}
} =
\frac{1390}{22240}_{\alpha_0}
\]</span>
<strong>Fourth</strong>, then compute for the solution <span class="math inline">\(\mathbf{x_1}\)</span>:</p>
<p><span class="math display">\[
x_1 = x_0 + \alpha_0 d_0 = 
\left[\begin{array}{r} 1 \\ 2 \\ 1 \\ 1 \end{array}\right]_{x_0} +
\frac{1390}{22240}_{\alpha_0}
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 2.1250 \\ 3.3125 \\ 2.5000 \\ 1.4375 \end{array}\right]_{x_1}
\]</span></p>
<p><strong>Fifth</strong>, compute for the next <strong>gradient</strong> (or residual) <span class="math inline">\(\mathbf{r_1}\)</span>:</p>
<p><span class="math display">\[
r_1 = r_0 - \alpha_0 A d_0 = 
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0} -
\frac{1390}{22240}
\left[
\begin{array}{rrrr}
1 &amp; 2 &amp; 3 &amp; 4 \\
2 &amp; 9 &amp; 6 &amp; 3 \\
3 &amp; 6 &amp; 9 &amp; 2 \\
4 &amp; 3 &amp; 2 &amp; 1
\end{array}
\right]_A
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}
\]</span></p>
<p><strong>Sixth</strong>, compute for convergence (expecting the gradient, <span class="math inline">\(r_1\)</span>, to become zero):</p>
<p><span class="math display">\[
if\ (\ \| r_1\|_{L2}\ &lt;\ tol\ )\, then\ it\ converges\  
\]</span></p>
<p><strong>Seventh</strong>, compute for <strong>conjugate gradient parameter</strong> <span class="math inline">\(\beta_0\)</span>:</p>
<p><span class="math display">\[
\beta_0 = \frac{r_1^Tr_1}{r_0^Tr_0} = 
\frac{
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}^T
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1}
}
{
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}^T
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{r_0}
} =
0.07323516_{\beta_0}
\]</span></p>
<p><strong>Eight</strong>, then compute for the next direction <span class="math inline">\(\mathbf{d_1}\)</span> - this is a <strong>conjugate direction</strong>:</p>
<p><span class="math display">\[
d_1 = r_1 + \beta_0 d_0 = 
\left[\begin{array}{r} 8.000 \\ -3.375 \\ -1.625 \\ -4.875 \end{array}\right]_{r_1} +
0.07323516_{\beta_0}
\left[\begin{array}{r} 18 \\ 21 \\ 24 \\ 7 \end{array}\right]_{d_0} = 
\left[\begin{array}{r} 9.3182329 \\ -1.8370616 \\ 0.1326439 \\ -4.3623539 \end{array}\right]_{d_1}
\]</span></p>
<p>From here, we need to <strong>iterate</strong> by repeating from <strong>third</strong> step until <span class="math inline">\(x_{k+1}\)</span> converges (<span class="math inline">\(e_{k+1}\)</span> &lt; tol).</p>
<p><strong>Finally</strong>, after convergence, our solution for <strong>x</strong> is expected to be: <span class="math inline">\(\left[\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \end{array}\right]_{x}^T\)</span></p>
<p>The steps above follow the <strong>Conjugate Gradient</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x_0 \leftarrow \text{initial arbitrary nonzero vector} \\
r_0 = d_0 = b - Ax_0 \\
loop\ k\ in\ 1:\ ... \\
\ \ \ \ \ \alpha_k = \frac{r_k^Tr_k}{r_k^TAr_k}\\
\ \ \ \ \ x_{k+1} = x_k + \alpha_k d_k  \\
\ \ \ \ \ r_{k+1} = r_k - \alpha_k A d_k  \\
\ \ \ \ \ if\ (\ \| r_{k+1} \|_{L2}\ &lt;\ tol\ )\ break \\
\ \ \ \ \ \beta_k = \frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \\
\ \ \ \ \ d_{k+1} = r_{k+1} + \beta_k r_k \\
end\ loop
\end{array}
\]</span></p>
<p>We now show a naive implementation of <strong>Conjugate Gradient</strong> in R code:</p>

<div class="sourceCode" id="cb66"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb66-1" data-line-number="1">conjugate_gradient &lt;-<span class="st"> </span><span class="cf">function</span>(A, x0, b) {</a>
<a class="sourceLine" id="cb66-2" data-line-number="2">    n =<span class="st"> </span><span class="kw">length</span>(x0)</a>
<a class="sourceLine" id="cb66-3" data-line-number="3">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb66-4" data-line-number="4">    limit =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb66-5" data-line-number="5">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb66-6" data-line-number="6">    r =<span class="st"> </span>d =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>(<span class="dt">x=</span>x0)</a>
<a class="sourceLine" id="cb66-7" data-line-number="7">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb66-8" data-line-number="8">        <span class="cf">if</span> (k<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb66-9" data-line-number="9">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(k, x, err))</a>
<a class="sourceLine" id="cb66-10" data-line-number="10">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb66-11" data-line-number="11">            q =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-12" data-line-number="12">            r_k =<span class="st"> </span><span class="kw">c</span>( <span class="kw">t</span>(r) <span class="op">%*%</span><span class="st"> </span>r )</a>
<a class="sourceLine" id="cb66-13" data-line-number="13">            alpha =<span class="st"> </span><span class="kw">c</span>( r_k <span class="op">/</span><span class="st"> </span>( <span class="kw">t</span>(d) <span class="op">%*%</span><span class="st"> </span>q ) )</a>
<a class="sourceLine" id="cb66-14" data-line-number="14">            x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-15" data-line-number="15">            r =<span class="st"> </span>r <span class="op">-</span><span class="st"> </span>alpha <span class="op">*</span><span class="st"> </span>q</a>
<a class="sourceLine" id="cb66-16" data-line-number="16">            err =<span class="st">  </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((r)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb66-17" data-line-number="17">            sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(k, x,  err))</a>
<a class="sourceLine" id="cb66-18" data-line-number="18">            <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol) <span class="cf">break</span></a>
<a class="sourceLine" id="cb66-19" data-line-number="19">            beta =<span class="st"> </span><span class="kw">c</span>( ( <span class="kw">t</span>(r) <span class="op">%*%</span><span class="st"> </span>r ) <span class="op">/</span><span class="st"> </span>r_k )</a>
<a class="sourceLine" id="cb66-20" data-line-number="20">            d =<span class="st"> </span>r <span class="op">+</span><span class="st"> </span>beta <span class="op">*</span><span class="st"> </span>d</a>
<a class="sourceLine" id="cb66-21" data-line-number="21">        }</a>
<a class="sourceLine" id="cb66-22" data-line-number="22">   }</a>
<a class="sourceLine" id="cb66-23" data-line-number="23">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;K&quot;</span>, <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n),<span class="st">&quot;&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>), </a>
<a class="sourceLine" id="cb66-24" data-line-number="24">                          <span class="st">&quot;error&quot;</span>)     </a>
<a class="sourceLine" id="cb66-25" data-line-number="25">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;matrix&quot;</span>=A, </a>
<a class="sourceLine" id="cb66-26" data-line-number="26">        <span class="st">&quot;b&quot;</span>=<span class="kw">c</span>(b), <span class="st">&quot;x&quot;</span>=<span class="kw">c</span>(x),  <span class="st">&quot;error&quot;</span>=err)</a>
<a class="sourceLine" id="cb66-27" data-line-number="27">}</a>
<a class="sourceLine" id="cb66-28" data-line-number="28">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>, <span class="dv">2</span>,<span class="dv">9</span>,<span class="dv">6</span>,<span class="dv">3</span>, <span class="dv">3</span>,<span class="dv">6</span>, <span class="dv">9</span>,<span class="dv">2</span>, <span class="dv">4</span>,<span class="dv">3</span>,<span class="dv">2</span>,<span class="dv">1</span>), <span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb66-29" data-line-number="29">x =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb66-30" data-line-number="30">b =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>x  <span class="co"># (30, 50, 50, 20)</span></a>
<a class="sourceLine" id="cb66-31" data-line-number="31"><span class="kw">conjugate_gradient</span>(A,  <span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">1</span>), b)</a></code></pre></div>
<pre><code>## $Iteration
##      K         x1       x2       x3       x4        error
## [1,] 0  1.0000000 2.000000 1.000000 1.000000 0.000000e+00
## [2,] 1  2.1250000 3.312500 2.500000 1.437500 1.008944e+01
## [3,] 2 -2.4570919 4.215846 2.434774 3.582618 1.269644e+01
## [4,] 3  0.9623681 1.963518 3.074545 3.945714 3.247722e-01
## [5,] 4  1.0000000 2.000000 3.000000 4.000000 3.947232e-14
## 
## $matrix
##      [,1] [,2] [,3] [,4]
## [1,]    1    2    3    4
## [2,]    2    9    6    3
## [3,]    3    6    9    2
## [4,]    4    3    2    1
## 
## $b
## [1] 30 50 50 20
## 
## $x
## [1] 1 2 3 4
## 
## $error
## [1] 3.947232e-14</code></pre>

<p>Note that the closest approximate value is already achieved after the 4th iteration, for which it can be noticed that an SPD matrix <span class="math inline">\(A \in R^{nxn}\)</span> yields at most <span class="math inline">\(nth\)</span> iteration.</p>
<p>For further reading, please consider investigating other <strong>Krylov-subspace</strong> methods for solving <span class="math inline">\(Ax = b\)</span>:</p>
<ul>
<li>LSQR - Least Squares QR-factorization Method</li>
<li>MINRES - Minimal Residual Method</li>
<li>SYMMLQ - Symmetric LQ Method</li>
<li>BiCG - Bi-Conjugate Gradient</li>
<li>QMR - Quasi-minimal Residual Method</li>
<li>FOM - Full Orthogonal Method</li>
</ul>
<p>Now, let us discuss <strong>non-Krylov-subspace</strong> methods for solving <span class="math inline">\(Ax=b\)</span>, starting with <strong>Jacobi and Gauss-Seidel Method</strong>.</p>
</div>
<div id="jacobi-and-gauss-seidel-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.4</span> Jacobi and Gauss-Seidel Method <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>If analytical methods cannot solve a system of polynomial equations, we <strong>use iterative methods to approximate the solution</strong>. The <strong>Jacobi</strong> method and <strong>Gauss-Seidel</strong> method - also known as the <strong>Liebmann</strong> method - are such iterative methods of solving systems of equations.</p>
<p>Given a <strong>system of linear equations</strong> and its expanded form below (See Equation <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#eq:eqnnumber1">(3.33)</a>) <span class="citation">(Dr. S. Karunanithi et al. <a href="bibliography.html#ref-ref49d">2018</a>; Saha M. and Chakrabarty J. <a href="bibliography.html#ref-ref53m">2018</a>)</span>:</p>
<p><span class="math display">\[\begin{align*}
a_{1,1}x_1 +  a_{1,2}x_2 + a_{1,3}x_3 +\ ...\ + a_{1,n}x_n  {}&amp;= b_1\\
a_{2,1}x_1 +  a_{2,2}x_2 + a_{2,3}x_3 +\ ...\ + a_{2,n}x_n  &amp;= b_2\\
\vdots \\
a_{n,1}x_1 +  a_{n,2}x_2 + a_{n,3}x_3 +\ ...\ + a_{n,n}x_n  &amp;= b_n,
\end{align*}\]</span></p>
<p>Both methods iteratively solve for <strong>x</strong> by separation of variables. Here we separate <span class="math inline">\(\mathbf{x_i}\)</span> to the left side of the equation. For example:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\ ...\ - a_{1,n}x_n)/a_{1,1}\\
x_2 &amp;= ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\ ...\ - a_{2,n}x_n)/a_{2,2}\\
\vdots \\
x_3 &amp;= (b_n - a_{n,1}x_1 -  a_{n,2}x_2 -\ ...\ - a_{n,n}x_n)/a_{n,3}\\
\end{align*}\]</span></p>
<p>To illustrate, let us use the following equations (square matrix):</p>
<p><span class="math display">\[\begin{align*}
9x_1 + 1x_2 + 2x_3 {}&amp;= 6\\
3x_1 + 4x_2 + 1x_3 &amp;= 5\\
3x_1 + 1x_2 + 28x_3 &amp;= 6\\
\end{align*}\]</span></p>
<p><strong>First</strong>, let us separate the <strong>x</strong> variables such that we get the following:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= (6 - 1x_2 - 2x_3)/9 = 6/9 - 1/9x_2 - 2/9x_3  \\
x2 &amp;= (5 - 3x_1 - 1x_3)/4 = 5/4 - 3/4x_1 - 1/4x_3  \\
x3 &amp;= (6 - 3x_1 - 1x_2)/28 = 6/28 - 3/28x_1 - 1/28x_2  \\
\end{align*}\]</span></p>
<p>That order of the equation will be used throughout the iteration.</p>
<p><strong>Second</strong>, we assume the following initialization: <span class="math inline">\(x_1=0, x_2=0, x_3=0\)</span>.</p>
<p>Now, in the <strong>Jacobi Method</strong>, we perform the following iteration:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(0) - 1/4(0) = 5/4\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0) - 1/28(0) = 6/28\\
\end{align*}\]</span></p>
<p>Then, we go through the second iteration with the following values: <span class="math inline">\(x_1=6/9, x_2=5/4, x_3=6/28\)</span></p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (5/4) - (6/28) = 168/211\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(6/9) - 1/4(6/28) = 39/56\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(5/4) = 11/112\\
\end{align*}\]</span></p>
<p>We then continue with the iteration using the new <strong>x</strong> values until convergence.</p>
<p>However, in the <strong>Gauss-Seidel Method</strong>, we perform the following iteration instead:</p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(6/9) - 1/4(0) = 3/4\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(3/4) = 13/112\\
\end{align*}\]</span></p>
<p>Then, we go through the second iteration with the following values: <span class="math inline">\(x_1=6/9, x_2=3/4, x_3=13/112\)</span></p>
<p><span class="math display">\[\begin{align*}
x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (3/4) - (13/112) = 0.73765\\
x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3  = 5/4 - 3/4(0.73765) - 1/4(13/112) = 0.66774\\
x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0.73765) - 1/28(0.66774) = 0.11140\\
\end{align*}\]</span></p>
<p>We then continue with the iteration using the new <strong>x</strong> values until convergence.</p>
<p>Notice that the <strong>Jacobi</strong> method uses the values of the previous iteration while the <strong>Gauss-Seidel</strong> method uses new values immediately within the iteration.</p>
<p>Here are the algorithms for the two methods:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{Jacobi}\\
===============\\
loop \\
\ \ \ \ \phi  = x\\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ \phi_i = (b_i - s) / A_{ii}\\
\ \ \ \ end\ loop\\
\ \ \ \ x = \phi \\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop
\end{array}
\left|
\begin{array}{l}
\mathbf{\text{Gauss-Seidel}}\\
===============\\
loop \\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ x_i = (b_i - s) / A_{ii}\\
\ \ \ \ end\ loop\\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop\\
 \\
 \\
 \end{array}
\right.
\]</span></p>
<p>Note that both methods do not always result in convergence. There are matrices that do not converge using the methods. One way to identify them is using <strong>spectral radius</strong> - denoted as <span class="math inline">\(\rho(A)\)</span> - the maximum absolute eigenvalue of a matrix (system of linear equations). The approximate solution has a higher chance of convergence if the <strong>spectral radius</strong> is less than 0.70.</p>
<p><span class="math display">\[
\rho(R/D) = \rho(D^{-1}R)&lt; 0.70
\]</span></p>
<p>where <strong>D</strong> is the diagonal of A, and <span class="math inline">\(\mathbf{R=(L + U)}\)</span> is the non-diagonal of A. We normalize A using its diagonal entries to test if the matrix is diagonally dominant.</p>
<p>Note that we choose to use 0.70 as our level of tolerance instead of 1.0 for smaller iterations for the sake of illustration. One may use 1.00, resulting in more iterations but may still converge.</p>
<p>For other convergence criteria, also investigate <strong>Stein-Rosenberg</strong> theorem (e.g., convergence condition for rectangular matrices). </p>
<p>Here is a naive implementation of the <strong>Jacobi</strong> and <strong>Gauss-Seidel</strong> method with the convergence condition (Note that we include a new method called <strong>SOR</strong> which we discuss in the next section):</p>

<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb68-1" data-line-number="1"><span class="co"># maximum absolute eigenvalue of matrix</span></a>
<a class="sourceLine" id="cb68-2" data-line-number="2">spectral_radius &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb68-3" data-line-number="3">    <span class="kw">max</span> ( <span class="kw">abs</span>( <span class="kw">eigen</span>(A)<span class="op">$</span>values ) )</a>
<a class="sourceLine" id="cb68-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb68-5" data-line-number="5">convergence_condition &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb68-6" data-line-number="6">    n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb68-7" data-line-number="7">    d =<span class="st"> </span><span class="kw">diag</span>(A)</a>
<a class="sourceLine" id="cb68-8" data-line-number="8">    D =<span class="st"> </span>d <span class="op">*</span><span class="st"> </span><span class="kw">diag</span>(n)</a>
<a class="sourceLine" id="cb68-9" data-line-number="9">    R =<span class="st"> </span>A <span class="op">-</span><span class="st"> </span>D </a>
<a class="sourceLine" id="cb68-10" data-line-number="10">    <span class="kw">max</span> ( <span class="kw">abs</span>( <span class="kw">eigen</span>(<span class="kw">solve</span>(D) <span class="op">%*%</span><span class="st"> </span>R)<span class="op">$</span>values ) )</a>
<a class="sourceLine" id="cb68-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb68-12" data-line-number="12">jacobi &lt;-<span class="cf">function</span>(x0, A, b) {</a>
<a class="sourceLine" id="cb68-13" data-line-number="13">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-14" data-line-number="14">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-15" data-line-number="15">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb68-16" data-line-number="16">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-17" data-line-number="17">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-18" data-line-number="18">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-19" data-line-number="19">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-20" data-line-number="20">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-21" data-line-number="21">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-22" data-line-number="22">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb68-23" data-line-number="23">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-24" data-line-number="24">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-25" data-line-number="25">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-26" data-line-number="26">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-27" data-line-number="27">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-28" data-line-number="28">                  }</a>
<a class="sourceLine" id="cb68-29" data-line-number="29">              }</a>
<a class="sourceLine" id="cb68-30" data-line-number="30">              x_[i] =<span class="st"> </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb68-31" data-line-number="31">          }</a>
<a class="sourceLine" id="cb68-32" data-line-number="32">          x =<span class="st"> </span>x_</a>
<a class="sourceLine" id="cb68-33" data-line-number="33">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-34" data-line-number="34">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-35" data-line-number="35">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-36" data-line-number="36">        }</a>
<a class="sourceLine" id="cb68-37" data-line-number="37">    }</a>
<a class="sourceLine" id="cb68-38" data-line-number="38">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-39" data-line-number="39">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-40" data-line-number="40">    <span class="kw">list</span>(<span class="st">&quot;Jacobi Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-41" data-line-number="41">}</a>
<a class="sourceLine" id="cb68-42" data-line-number="42">gauss_seidel &lt;-<span class="cf">function</span>(x0, A, b) {</a>
<a class="sourceLine" id="cb68-43" data-line-number="43">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-44" data-line-number="44">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-45" data-line-number="45">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb68-46" data-line-number="46">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-47" data-line-number="47">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-48" data-line-number="48">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-49" data-line-number="49">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-50" data-line-number="50">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-51" data-line-number="51">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-52" data-line-number="52">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-53" data-line-number="53">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-54" data-line-number="54">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-55" data-line-number="55">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-56" data-line-number="56">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-57" data-line-number="57">                  }</a>
<a class="sourceLine" id="cb68-58" data-line-number="58">              }</a>
<a class="sourceLine" id="cb68-59" data-line-number="59">              x[i] =<span class="st"> </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">/</span><span class="st"> </span>A[i,i]</a>
<a class="sourceLine" id="cb68-60" data-line-number="60">          }</a>
<a class="sourceLine" id="cb68-61" data-line-number="61">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-62" data-line-number="62">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-63" data-line-number="63">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-64" data-line-number="64">        }</a>
<a class="sourceLine" id="cb68-65" data-line-number="65">    }</a>
<a class="sourceLine" id="cb68-66" data-line-number="66">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-67" data-line-number="67">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-68" data-line-number="68">    <span class="kw">list</span>(<span class="st">&quot;Gauss-Seidel Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-69" data-line-number="69">}</a>
<a class="sourceLine" id="cb68-70" data-line-number="70">sor &lt;-<span class="cf">function</span>(x0, A, b,w) {</a>
<a class="sourceLine" id="cb68-71" data-line-number="71">    x =<span class="st"> </span>x0</a>
<a class="sourceLine" id="cb68-72" data-line-number="72">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb68-73" data-line-number="73">    limit =<span class="st"> </span><span class="dv">150</span></a>
<a class="sourceLine" id="cb68-74" data-line-number="74">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-75" data-line-number="75">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb68-76" data-line-number="76">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb68-77" data-line-number="77">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb68-78" data-line-number="78">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-79" data-line-number="79">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-80" data-line-number="80">          <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-81" data-line-number="81">              s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb68-82" data-line-number="82">              <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb68-83" data-line-number="83">                  <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>k) {</a>
<a class="sourceLine" id="cb68-84" data-line-number="84">                      s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>A[i,k] <span class="op">*</span><span class="st"> </span>x[k] </a>
<a class="sourceLine" id="cb68-85" data-line-number="85">                  }</a>
<a class="sourceLine" id="cb68-86" data-line-number="86">              }</a>
<a class="sourceLine" id="cb68-87" data-line-number="87">              x[i] =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>w ) <span class="op">*</span><span class="st"> </span>x[i] <span class="op">+</span><span class="st">  </span>(b[i] <span class="op">-</span><span class="st"> </span>s) <span class="op">*</span><span class="st"> </span>( w <span class="op">/</span><span class="st"> </span>A[i,i] )</a>
<a class="sourceLine" id="cb68-88" data-line-number="88">          }</a>
<a class="sourceLine" id="cb68-89" data-line-number="89">          err =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>((A <span class="op">%*%</span><span class="st"> </span>x <span class="op">-</span><span class="st"> </span>b)<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb68-90" data-line-number="90">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb68-91" data-line-number="91">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb68-92" data-line-number="92">        }</a>
<a class="sourceLine" id="cb68-93" data-line-number="93">    }</a>
<a class="sourceLine" id="cb68-94" data-line-number="94">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb68-95" data-line-number="95">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb68-96" data-line-number="96">    <span class="kw">list</span>(<span class="st">&quot;SOR Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb68-97" data-line-number="97">}</a>
<a class="sourceLine" id="cb68-98" data-line-number="98">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">9</span>,<span class="dv">1</span>,<span class="dv">2</span>, <span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">1</span>, <span class="dv">3</span>,<span class="dv">1</span>,<span class="dv">28</span>),<span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb68-99" data-line-number="99">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb68-100" data-line-number="100"><span class="cf">if</span> ( <span class="kw">convergence_condition</span>(A) <span class="op">&lt;</span><span class="st"> </span><span class="fl">0.70</span> ) {</a>
<a class="sourceLine" id="cb68-101" data-line-number="101">    J =<span class="st"> </span><span class="kw">jacobi</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b)</a>
<a class="sourceLine" id="cb68-102" data-line-number="102">    G =<span class="st"> </span><span class="kw">gauss_seidel</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b)</a>
<a class="sourceLine" id="cb68-103" data-line-number="103">    S =<span class="st"> </span><span class="kw">sor</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>,<span class="dv">0</span>), A, b, <span class="dv">1</span>) <span class="co"># becomes gauss_seidel if w=1</span></a>
<a class="sourceLine" id="cb68-104" data-line-number="104">    <span class="kw">print</span>(J)</a>
<a class="sourceLine" id="cb68-105" data-line-number="105">    <span class="kw">print</span>(G)</a>
<a class="sourceLine" id="cb68-106" data-line-number="106">    <span class="kw">print</span>(S)</a>
<a class="sourceLine" id="cb68-107" data-line-number="107">} <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb68-108" data-line-number="108">    <span class="kw">print</span>(<span class="st">&quot;Spectral Radius of Matrix &gt; 0.70&quot;</span>)</a>
<a class="sourceLine" id="cb68-109" data-line-number="109">}</a></code></pre></div>
<pre><code>## $`Jacobi Iteration`
##        N         x1        x2         x3        error
##  [1,]  0 0.00000000 0.0000000 0.00000000 0.000000e+00
##  [2,]  1 0.11111111 0.5000000 0.10714286 1.182653e+00
##  [3,]  2 0.03174603 0.3898810 0.07738095 4.709345e-01
##  [4,]  3 0.05059524 0.4568452 0.08981718 1.686652e-01
##  [5,]  4 0.04039116 0.4395993 0.08540604 6.478116e-02
##  [6,]  5 0.04328763 0.4483551 0.08711526 2.367872e-02
##  [7,]  6 0.04193493 0.4457555 0.08649221 9.001587e-03
##  [8,]  7 0.04236223 0.4469257 0.08672999 3.321377e-03
##  [9,]  8 0.04217936 0.4465458 0.08664241 1.255004e-03
## [10,]  9 0.04224104 0.4467049 0.08667557 4.656032e-04
## [11,] 10 0.04221600 0.4466503 0.08666329 1.752602e-04
## [12,] 11 0.04222479 0.4466722 0.08666792 6.523372e-05
## [13,] 12 0.04222133 0.4466644 0.08666619 2.449648e-05
## [14,] 13 0.04222258 0.4466675 0.08666684 9.135815e-06
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222258 0.44666745 0.08666684
## 
## $`Gauss-Seidel Iteration`
##      N         x1        x2         x3        error
## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00
## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01
## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02
## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03
## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04
## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05
## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222231 0.44666663 0.08666666
## 
## $`SOR Iteration`
##      N         x1        x2         x3        error
## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00
## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01
## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02
## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03
## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04
## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05
## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07
## 
## $initial
## [1] 0 0 0
## 
## $x
## [1] 0.04222231 0.44666663 0.08666666</code></pre>

</div>
<div id="successive-over-relaxation-sor-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.5</span> Successive Over-Relaxation (SOR) Method  <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Successive Over-relaxation Method (SOR)</strong> improves over the <strong>Gauss-Seidel</strong> method <span class="citation">(Dr. S. Karunanithi et al. <a href="bibliography.html#ref-ref49d">2018</a>; Saha M. and Chakrabarty J. <a href="bibliography.html#ref-ref53m">2018</a>)</span>.</p>
<p>In <strong>SOR</strong>, we introduce a <strong>relaxation factor (w)</strong> used to slow down or speed up convergence. If <span class="math inline">\(\mathbf{w=1}\)</span>, then <strong>SOR</strong> is reduced to <strong>Gauss-Seidel</strong> method.</p>
<p>Here, we make a slight change in the equation used by the <strong>Gauss-Seidel</strong> method:</p>
<p>For example:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= (1 - \omega) x_1 + ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\ ...\ - a_{1,n}x_n) \frac{\omega}{a_{1,1}}\\
x_2 &amp;= (1 - \omega) x_2 + ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\ ...\ - a_{2,n}x_n)\frac{\omega}{a_{2,2}}\\
\vdots \\
x_3 &amp;= (1 - \omega) x_3 + (b_n - a_{n,1}x_1 -  a_{n,2}x_2 -\ ...\ - a_{n,n}x_n)\frac{\omega}{a_{n,3}}\\
\end{align*}\]</span></p>
<p>And we use the same <strong>algorithm</strong> as the <strong>Gauss-Seidel</strong> method with slight modification in the equations used to approximate <span class="math inline">\(\mathbf{x_i}\)</span>:</p>
<p><span class="math display">\[
\begin{array}{l}
\mathbf{\text{Successive Over-Relaxation (SOR)}}\\
======================\\
loop \\
\ \ \ \ loop\ i\ in\ 1..n\\
\ \ \ \ \ \ \ \ s = 0\\
\ \ \ \ \ \ \ \ loop\ k\ in\ 1..n\\
\ \ \ \ \ \ \ \ \ \ \ \ if ( i \ne k):\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ s = s + A_{ik} * x_k \\
\ \ \ \ \ \ \ \ end\ loop\\
\ \ \ \ \ \ \ \ x_i = (1-\omega) * x_i  + (b_i - s) * (w / A_{ii})\\
\ \ \ \ end\ loop\\
\ \ \ \ if\ \|Ax - b \| &lt; tol\ then\ break\\
end\ loop\\
 \\
\end{array}
\]</span>
See also the R implementation covered in previous section.</p>
</div>
<div id="newtons-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.6</span> Newton’s Method <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like <strong>Gauss-Seidel</strong> method, the <strong>Newton’s Method</strong> is also an iterative approximation. We use the following iterative equation (where x is a multivariate, e.g. <span class="math inline">\(\mathbf{x} \in (x_1, x_2)\)</span>):</p>
<p><span class="math display" id="eq:equate1050096">\[\begin{align}
x_{k+1} = x_k - J_k^{-1} f(x_k) \tag{3.102} 
\end{align}\]</span></p>
<p>The equation is derived by <strong>approximation</strong> using first-order <strong>Taylor Series</strong> given a <strong>partial differentiable function</strong> where we force the function to zero to extract the <strong>roots</strong>, <span class="math inline">\(f(x) = 0\)</span> (note that we can also use <span class="math inline">\(f(x) \approx 0\)</span> since here we are approximating the roots):</p>
<p><span class="math display" id="eq:equate1050097">\[\begin{align}
f(x) \approx f(x_k) + J_k(x_{k+1} - x_k) = 0,\ \ \ \ where\ \Delta x = \hat{x} - x\ and\ \hat{x}\ =\ x_{k+1}   \tag{3.103} 
\end{align}\]</span></p>
<p>Later, we show how <strong>Jacobian matrix</strong>, J, can be illustrated using two non-linear <strong>differentiable</strong> functions:</p>
<p><span class="math display" id="eq:equate1050098">\[\begin{align}
f(\mathbf{\vec{x}}) = \{\ \ \ f_1(\mathbf{\vec{x}}),\ \ f_2(\mathbf{\vec{x}})\ \ \} \tag{3.104} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{\vec{x}}\)</span> is a multivariate vector.</p>
<p>Now, when it comes to approximation, we can readily say that <span class="math inline">\(\mathbf{\hat{x}}\)</span> is the approximation for the actual value of <strong>x</strong> and that <span class="math inline">\(\Delta x\)</span> is the difference (or delta change) between the approximate and the true value. Therefore, we can denote this as such:</p>
<p><span class="math display" id="eq:equate1050099">\[\begin{align}
\hat{x} = x + \Delta x, \ \ \ \ \ where\ \Delta x\ \text{is the delta change} \tag{3.105} 
\end{align}\]</span></p>
<p>As we point out, the vector <span class="math inline">\(\mathbf{\hat{x}}\)</span> can otherwise be represented as multivariate <span class="math inline">\(\{ \mathbf{x_1, x_2, x_3, ..., x_n}\}\)</span> and given we deal with approximation, as an example, we, therefore, can denote the equation this way (with two-variable multivariate):</p>
<p><span class="math display" id="eq:equate1050100">\[\begin{align}
f(\mathbf{\vec{x}} + \Delta \vec{x}) 
= \{\ f_1(x_1 + \Delta x_1, x_2 + \Delta x_2),
      \ f_2(x_1 + \Delta x_1, x_2 + \Delta x_2)\ \} \tag{3.106} 
\end{align}\]</span></p>
<p>We can use <strong>Taylor series</strong> to approximate two-variable multivariate function <span class="math inline">\(f_i(\mathbf{\vec{x}})\)</span>, performing <strong>partial derivatives</strong> with respect to each variable:</p>
<p><span class="math display" id="eq:equate1050101">\[\begin{align}
f_i(\mathbf{\vec{x}} + \Delta \vec{x}) 
{}&amp;= f_i(x_1 + \Delta x_1,x_2 + \Delta x_2  ) \nonumber \\
&amp;+ \frac{\partial f_i}{\partial x_1}(x_1 + \Delta x_1,x_2 + \Delta x_2) \Delta x_1 \nonumber  \\
&amp;+ \frac{\partial f_i}{\partial x_2}(x_1 + \Delta x_1,x_2 + \Delta x_2) \Delta x_2 +  R_n(\vec{x} + \Delta \vec{x})  \tag{3.107} 
\end{align}\]</span></p>
<p>It can also be written this way:</p>
<p><span class="math display" id="eq:equate1050102">\[\begin{align}
f_i(\mathbf{\hat{x}}) = f_i(\hat{x_1},\hat{x_2} ) + \frac{\partial f_i}{\partial x_1}(\hat{x_1},\hat{x_2} ) \Delta x_1
+ \frac{\partial f_i}{\partial x_2}(\hat{x_1},\hat{x_2} ) \Delta x_2 + R_n(\hat{x}) \tag{3.108} 
\end{align}\]</span></p>
<p>or this way:</p>
<p><span class="math display" id="eq:equate1050103">\[\begin{align}
f_i(\mathbf{\hat{x}}) = f_i(\hat{x_1},\hat{x_2} ) + \frac{\partial f_i}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_i}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x) + R_n(\hat{x}) \tag{3.109} 
\end{align}\]</span></p>
<p>where the <strong>remainder term</strong> is denoted as:</p>
<p><span class="math display" id="eq:equate1050104">\[\begin{align}
R_n(\hat{x}) = \frac{\partial f_i}{\partial x_n}(\xi ) (\hat{x_n} - x) \tag{3.110} 
\end{align}\]</span></p>
<p>For two non-linear equations, e.g. two differential functions, we expand <span class="math inline">\(f(\mathbf{\hat{x}}) = \{f_1(\mathbf{\hat{x}}), f_2(\mathbf{\hat{x}}) \}\)</span> to show the two functions (as an approximate given we removed the <strong>remainder terms</strong>):</p>
<p><span class="math display" id="eq:equate1050106" id="eq:equate1050105">\[\begin{align}
f_1(\mathbf{\hat{x}}) \approx f_1(\hat{x_1},\hat{x_2} ) + \frac{\partial f_1}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_1}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x)  \tag{3.111} \\
f_2(\mathbf{\hat{x}}) \approx f_2(\hat{x_1},\hat{x_2} ) + \frac{\partial f_2}{\partial x_1}(\hat{x_1},\hat{x_2} ) (\hat{x_1} - x)
+ \frac{\partial f_2}{\partial x_2}(\hat{x_1},\hat{x_2} ) (\hat{x_2} - x)   \tag{3.112} 
\end{align}\]</span></p>
<p>We can translate the partial derivatives into a <strong>Jacobian matrix</strong>. Note that we could have also considered including the <strong>second-order</strong> in the <strong>Taylor Series</strong> and hence be able to translate the derivatives into a <strong>Hessian matrix</strong>. Now, to illustrate <strong>Newton’s method</strong>, we settle with <strong>first-order</strong>.</p>
<p>Let us illustrate the <strong>Newton’s method</strong>.</p>
<p><strong>First</strong>, perform partial derivatives with respect to each unknown variable and build the <strong>Jacobian matrix</strong>:</p>
<p><span class="math display">\[\begin{align*}
f_1(\mathbf{\vec{x}}) {}&amp;=  3x_1^2 + x_2^2 - 4x_1  &amp; 
\ \ \frac{\partial f_1}{\partial x_1} {}&amp;= 6x_1 - 4,&amp;
\ \ \frac{\partial f_1}{\partial x_2} {}&amp;= 2x_2 + 0\\
f_2(\mathbf{\vec{x}}) &amp;= 9x_1^2 + x_2^2 - 2x_2 &amp;
\ \ \frac{\partial f_2}{\partial x_1} &amp;= 18x_1 + 0, &amp;
\ \ \frac{\partial f_2}{\partial x_2} &amp;= 2x_2 - 2  \\
\end{align*}\]</span></p>
<p>We translate the partial derivatives into <strong>Jacobian matrix</strong> form:</p>
<p><span class="math display">\[
J(\mathbf{\vec{x}}) =
\left[
\begin{array}{ccc}
\frac{\partial f_1}{\partial x_1} &amp; \frac{\partial f_1}{\partial x_2} \\
\frac{\partial f_2}{\partial x_1} &amp; \frac{\partial f_2}{\partial x_2}\
\end{array}
\right] = 
\left[
\begin{array}{rr}
6x_1 - 4 &amp; 2x_2 + 0\\
18x_1 + 0  &amp; 2x_2 - 2
\end{array}
\right]
\]</span></p>
<p><strong>Second</strong>: compute for the functions and their partial derivatives given an initial arbitrary nonzero vector:</p>
<p>Assume initial values for vector <span class="math inline">\(\mathbf{\vec{x}}^0\)</span> (note that we are using a superscript for the vector to not confuse with the <strong>x</strong> variable indexes:</p>
<p><span class="math display">\[
\mathbf{\vec{x}}^0 = \left[\begin{array}{rrr} 1 &amp; 1 \end{array}\right]^T
\]</span>
We get result for <span class="math inline">\(f(\mathbf{\vec{x}}^0)\)</span> and <span class="math inline">\(J(\mathbf{\vec{x}}^0)\)</span>:</p>

<p><span class="math display">\[
f(\mathbf{\vec{x}}^0) =
\left[
\begin{array}{ccc}
3x_1^2 + x_2^2 - 4x_1 \\
9x_1^2 + x_2^2 - 2x_2
\end{array}
\right] = 
\left[
\begin{array}{ccc}
0 \\
8 
\end{array}
\right]\ \ \ \ \ \ \ \ 
J(\mathbf{\vec{x}}^0) =
\left[
\begin{array}{ccc}
6x_1 - 4 &amp; 2x_2 + 0 \\
18x_1 + 0  &amp; 2x_2 - 2
\end{array}
\right] = 
\left[
\begin{array}{ccc}
2 &amp; 2 \\
18 &amp; 0
\end{array}
\right]
\]</span>
</p>
<p><strong>Third</strong>, solve for the next <span class="math inline">\(\mathbf{\vec{x}}^{k+1}\)</span> using the below equation:</p>
<p><span class="math display" id="eq:eqnnumber201" id="eq:eqnnumber200">\[\begin{align}
\mathbf{\vec{x}}^{k+1} {}&amp;= \mathbf{\vec{x}}^k - J_f(\mathbf{\vec{x}}^k)^{-1} f(\mathbf{\vec{x}}^k) \tag{3.113} \\
&amp;= \mathbf{\vec{x}}^0 - J(\mathbf{\vec{x}}^0)^{-1}f(\mathbf{\vec{x}}^0) \tag{3.114} \\
&amp;= \left[
\begin{array}{ccc}
1 \\
1 
\end{array}
\right] - \left[
\begin{array}{ccc}
2 &amp; 2 \\
18 &amp; 0
\end{array}
\right]^{-1}
\left[
\begin{array}{ccc}
0 \\
8 
\end{array}
\right] \nonumber \\
\mathbf{\vec{x}}^{1} &amp;= \left[
\begin{array}{r}
0.556 \\
1.444
\end{array}
\right] \nonumber
\end{align}\]</span></p>
<p><strong>Fourth</strong>, <span class="math inline">\(\mathbf{\vec{x}}^{1}\)</span> becomes the next <span class="math inline">\(\mathbf{\vec{x}}^k\)</span>. From there, iterate until convergence (e.g., tolerance is reached):</p>
<p><span class="math display" id="eq:equate1050108" id="eq:equate1050107">\[\begin{align}
\mathbf{\vec{x}}^{k+1} {}&amp;= \mathbf{\vec{x}}^k - J_f(\mathbf{\vec{x}}^k)^{-1} f(\mathbf{\vec{x}}^k)   \tag{3.115} \\
&amp;= \mathbf{\vec{x}}^1 - J(\mathbf{\vec{x}}^1)^{-1}f(\mathbf{\vec{x}}^1)  \tag{3.116} 
\end{align}\]</span></p>
<p>We use the following tolerance and convergence criterion:</p>
<p><span class="math display" id="eq:equate1050109">\[\begin{align}
\left|\sqrt{\sum(x^k)^2} - \sqrt{\sum(x^{k+1})^2}\right| &lt; 1e{-5} \tag{3.117} 
\end{align}\]</span></p>
<p>Convergence is reached with the following result:</p>
<p><span class="math display">\[
x_1 \approx \frac{1}{3}, \ \ \ \ \ \ x_2 \approx 1\ \ \ \ \ \  \text {if } x^0 = [1,1] 
\]</span></p>
<p>and:</p>
<p><span class="math display">\[
x_1 \approx 0, \ \ \ \ \ \ x_2 \approx 0\ \ \ \ \ \  \text {if } x^0 = [-1,1] 
\]</span></p>
<p>The steps above illustrates the <strong>Newton’s Method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ \Delta x^k = -J(x^k)^{-1} f(x^k) \ \ \ \ use\ LU\ decomposition\ for\ J^{-1}\\
\ \ \ \ x^{k+1} = x^k + \Delta x^k \\
end\ loop
\end{array}
\]</span></p>
<p>We show the naive implementation of the <strong>Newton’s Method</strong> in R code:</p>

<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" data-line-number="1">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">3</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>] }</a>
<a class="sourceLine" id="cb70-2" data-line-number="2">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">9</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] }</a>
<a class="sourceLine" id="cb70-3" data-line-number="3">J &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb70-4" data-line-number="4">    <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb70-5" data-line-number="5">        <span class="kw">c</span>( <span class="dv">6</span><span class="op">*</span>x[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>], <span class="dv">18</span><span class="op">*</span>x[<span class="dv">1</span>] , <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb70-6" data-line-number="6">        <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb70-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb70-8" data-line-number="8">F &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">c</span>( <span class="kw">f1</span>(x), <span class="kw">f2</span>(x) ) }</a>
<a class="sourceLine" id="cb70-9" data-line-number="9">newton &lt;-<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb70-10" data-line-number="10">    x0 =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb70-11" data-line-number="11">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb70-12" data-line-number="12">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb70-13" data-line-number="13">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb70-14" data-line-number="14">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb70-15" data-line-number="15">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb70-16" data-line-number="16">        <span class="cf">if</span> (j<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb70-17" data-line-number="17">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb70-18" data-line-number="18">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb70-19" data-line-number="19">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb70-20" data-line-number="20">          LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(<span class="kw">J</span>(x))</a>
<a class="sourceLine" id="cb70-21" data-line-number="21">          uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, <span class="kw">F</span>(x))</a>
<a class="sourceLine" id="cb70-22" data-line-number="22">          delta_x =<span class="st"> </span><span class="op">-</span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb70-23" data-line-number="23">          x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb70-24" data-line-number="24">          <span class="co">#x = x - solve(J(x)) %*% F(x) # alternative</span></a>
<a class="sourceLine" id="cb70-25" data-line-number="25">          a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb70-26" data-line-number="26">          b =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb70-27" data-line-number="27">          err =<span class="st"> </span><span class="kw">abs</span>(a<span class="op">-</span>b)</a>
<a class="sourceLine" id="cb70-28" data-line-number="28">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(j, x, err))</a>
<a class="sourceLine" id="cb70-29" data-line-number="29">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb70-30" data-line-number="30">        }</a>
<a class="sourceLine" id="cb70-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb70-32" data-line-number="32">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb70-33" data-line-number="33">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb70-34" data-line-number="34">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb70-35" data-line-number="35">}</a>
<a class="sourceLine" id="cb70-36" data-line-number="36"><span class="kw">newton</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      N        x1       x2        error
## [1,] 0 1.0000000 1.000000 0.000000e+00
## [2,] 1 0.5555556 1.444444 1.333851e-01
## [3,] 2 0.3858180 1.131770 3.518735e-01
## [4,] 3 0.3388188 1.015405 1.252831e-01
## [5,] 4 0.3334154 1.000241 1.609513e-02
## [6,] 5 0.3333334 1.000000 2.543819e-04
## [7,] 6 0.3333333 1.000000 6.209382e-08
## 
## $initial
## [1] 1 1
## 
## $x
## [1] 0.3333333 1.0000000</code></pre>

<p>The implementation of <strong>Newton’s Method</strong> gives us the result of multivariate <strong>x</strong>:</p>
<p><span class="math display">\[
x_1 = 1/3\ \ \ \ \ \ \ \ \ x_2 = 1\ \ \ \ \ \ where\ x^{(0)} = (1,1)
\]</span></p>
<p>Here is another example:</p>

<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" data-line-number="1"><span class="kw">newton</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      N            x1            x2        error
## [1,] 0 -1.000000e+00  1.000000e+00 0.000000e+00
## [2,] 1 -5.555556e-01 -7.777778e-01 4.583996e-01
## [3,] 2 -1.721440e-02 -9.029734e-01 5.267645e-02
## [4,] 3 -1.082059e-01 -2.061246e-01 6.703374e-01
## [5,] 4 -1.212666e-02 -5.150582e-02 1.798860e-01
## [6,] 5 -7.137358e-04 -1.816708e-03 5.096225e-02
## [7,] 6 -1.202316e-06 -3.927739e-06 1.947775e-03
## [8,] 7 -4.940920e-12 -1.421849e-11 4.107624e-06
## 
## $initial
## [1] -1  1
## 
## $x
## [1] -4.940920e-12 -1.421849e-11</code></pre>

<p>The result of multivariate <strong>x</strong> gives us:</p>
<p><span class="math display">\[
x_1 = 0\ \ \ \ \ \ \ \ \ x_2 = 0\ \ \ \ \ \ where\ x^{(0)} = (-1,1)
\]</span></p>
</div>
<div id="broydens-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.7</span> Broyden’s Method <a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Broyden’s Method</strong> is a <strong>Quasi-Newton method</strong> - it is an enhancement to the <strong>Newton</strong> method by avoiding the repeated evaluation of the Jacobian matrix at every iteration <span class="citation">(Jarlebring E. <a href="bibliography.html#ref-ref316e">2018</a>)</span>.</p>
<p>Given the following equation:</p>
<p><span class="math display" id="eq:equate1050110">\[\begin{align}
f(x) \approx A_{k+1} \Delta x = f(x_{k+1}) - f(x), \ \ \ \ where\ \Delta x = x_{k+1} - x,   \tag{3.118} 
\end{align}\]</span></p>
<p>we derive the <strong>Broyden’s formula</strong>:</p>
<p><span class="math display" id="eq:equate1050111">\[\begin{align}
A_{k+1} = A_x + \frac{(y_k - A_k \Delta x)(\Delta x)^T}{\| \Delta x \|_{L2}^2},\ \ \ \ where\ y_k = f(x_{k+1}) - f(x). \tag{3.119} 
\end{align}\]</span></p>
<p>The steps above illustrates the <strong>Broyden’s method</strong> algorithm:</p>
<p><span class="math display">\[
\begin{array}{l}
x^0 \leftarrow \text{initial arbitrary nonzero vector} \\
A^0 = J_f(x^0) \\
loop\ j\ in\ 1:\ ... \\
\ \ \ \ \Delta x  = -(A^k)^{-1} f(x^k) \ \ \ \ use\ LU\ decomposition\ for\ A^{-1}\\
\ \ \ \ x_{k+1} = x^k + \Delta x \\
\ \ \ \ y^k = f(x^{k+1}) - f(x^k)\\
\ \ \ \ r^k = y^k - A^k\Delta x \\
\ \ \ \ A^{k+1} = A^k + r^k(\Delta x )^T/\| \Delta x \|_{L2}^2  \\
end\ loop
\end{array}
\]</span></p>
<p>We show the naive implementation of <strong>Broyden’s Method</strong> in R code:</p>

<div class="sourceCode" id="cb74"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb74-1" data-line-number="1">f1 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">3</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">4</span><span class="op">*</span>x[<span class="dv">1</span>] }</a>
<a class="sourceLine" id="cb74-2" data-line-number="2">f2 &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">9</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] }</a>
<a class="sourceLine" id="cb74-3" data-line-number="3">J &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb74-4" data-line-number="4">    <span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb74-5" data-line-number="5">        <span class="kw">c</span>( <span class="dv">6</span><span class="op">*</span>x[<span class="dv">1</span>] <span class="op">-</span><span class="st"> </span><span class="dv">4</span>, <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>],<span class="dv">18</span><span class="op">*</span>x[<span class="dv">1</span>] , <span class="dv">2</span><span class="op">*</span>x[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">2</span>),</a>
<a class="sourceLine" id="cb74-6" data-line-number="6">        <span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb74-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb74-8" data-line-number="8">F &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">c</span>( <span class="kw">f1</span>(x), <span class="kw">f2</span>(x) ) }</a>
<a class="sourceLine" id="cb74-9" data-line-number="9">broyden &lt;-<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb74-10" data-line-number="10">    x0 =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb74-11" data-line-number="11">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb74-12" data-line-number="12">    limit =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb74-13" data-line-number="13">    tol =<span class="st"> </span><span class="fl">1e-5</span>; err =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb74-14" data-line-number="14">    sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, n <span class="op">+</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb74-15" data-line-number="15">    A =<span class="st"> </span><span class="kw">J</span>(x)</a>
<a class="sourceLine" id="cb74-16" data-line-number="16">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb74-17" data-line-number="17">        <span class="cf">if</span> (i<span class="op">==</span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb74-18" data-line-number="18">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(i, x, err))</a>
<a class="sourceLine" id="cb74-19" data-line-number="19">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb74-20" data-line-number="20">          x_ =<span class="st"> </span>x</a>
<a class="sourceLine" id="cb74-21" data-line-number="21">          LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)</a>
<a class="sourceLine" id="cb74-22" data-line-number="22">          uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, <span class="kw">F</span>(x))</a>
<a class="sourceLine" id="cb74-23" data-line-number="23">          delta_x =<span class="st"> </span><span class="op">-</span><span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy)</a>
<a class="sourceLine" id="cb74-24" data-line-number="24">          x =<span class="st"> </span>x <span class="op">+</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb74-25" data-line-number="25">          y =<span class="st"> </span><span class="kw">F</span>(x) <span class="op">-</span><span class="st"> </span><span class="kw">F</span>(x_)</a>
<a class="sourceLine" id="cb74-26" data-line-number="26">          r =<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>delta_x</a>
<a class="sourceLine" id="cb74-27" data-line-number="27">          A =<span class="st"> </span>A <span class="op">+</span><span class="st"> </span>(r <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(delta_x)) <span class="op">/</span>( <span class="kw">sqrt</span>(<span class="kw">sum</span>(delta_x<span class="op">^</span><span class="dv">2</span>)) )<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb74-28" data-line-number="28">          a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb74-29" data-line-number="29">          b =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb74-30" data-line-number="30">          err =<span class="st"> </span><span class="kw">abs</span>(a<span class="op">-</span>b)</a>
<a class="sourceLine" id="cb74-31" data-line-number="31">          sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(i, x, err))</a>
<a class="sourceLine" id="cb74-32" data-line-number="32">          <span class="cf">if</span> (err <span class="op">&lt;</span><span class="st"> </span>tol ) <span class="cf">break</span></a>
<a class="sourceLine" id="cb74-33" data-line-number="33">        }</a>
<a class="sourceLine" id="cb74-34" data-line-number="34">    }</a>
<a class="sourceLine" id="cb74-35" data-line-number="35">    <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;N&quot;</span>, </a>
<a class="sourceLine" id="cb74-36" data-line-number="36">          <span class="kw">paste</span>(<span class="st">&quot;x&quot;</span>,<span class="kw">seq</span>(<span class="dv">1</span>,n), <span class="dt">sep=</span><span class="st">&quot;&quot;</span>),  <span class="st">&quot;error&quot;</span>)</a>
<a class="sourceLine" id="cb74-37" data-line-number="37">    <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=sequence, <span class="st">&quot;initial&quot;</span>=x0,  <span class="st">&quot;x&quot;</span>=x) </a>
<a class="sourceLine" id="cb74-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb74-39" data-line-number="39"><span class="kw">broyden</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##        N        x1        x2        error
##  [1,]  0 1.0000000 1.0000000 0.000000e+00
##  [2,]  1 0.5555556 1.4444444 1.333851e-01
##  [3,]  2 0.4639175 1.2061856 2.552740e-01
##  [4,]  3 0.3870957 1.1129019 1.140235e-01
##  [5,]  4 0.3215272 0.9879433 1.393538e-01
##  [6,]  5 0.3269287 0.9877312 1.482875e-03
##  [7,]  6 0.4564969 1.2256526 2.674740e-01
##  [8,]  7 0.3320245 0.9966953 2.573608e-01
##  [9,]  8 0.3330608 0.9992606 2.761418e-03
## [10,]  9 0.3333392 1.0000118 8.006343e-04
## [11,] 10 0.3333336 1.0000005 1.248732e-05
## [12,] 11 0.3333333 1.0000000 5.371434e-07
## 
## $initial
## [1] 1 1
## 
## $x
## [1] 0.3333333 1.0000000</code></pre>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" data-line-number="1"><span class="kw">broyden</span>(<span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>))</a></code></pre></div>
<pre><code>## $Iteration
##        N            x1            x2        error
##  [1,]  0 -1.000000e+00  1.000000e+00 0.000000e+00
##  [2,]  1 -5.555556e-01 -7.777778e-01 4.583996e-01
##  [3,]  2 -1.616267e-01 -1.502607e+00 5.554606e-01
##  [4,]  3  2.120387e-01 -1.478334e+00 1.781104e-02
##  [5,]  4  4.681942e-02 -7.533436e-01 7.386664e-01
##  [6,]  5 -1.218198e-01 -3.135692e-01 4.183959e-01
##  [7,]  6 -4.877114e-02 -4.873996e-02 2.674504e-01
##  [8,]  7 -4.399305e-03 -1.188260e-02 5.627993e-02
##  [9,]  8 -5.441345e-04 -2.080117e-03 1.052073e-02
## [10,]  9 -1.332127e-05 -1.809338e-05 2.127641e-03
## [11,] 10  1.432844e-07 -3.547711e-07 2.208574e-05
## [12,] 11 -5.747636e-09  1.149681e-08 3.697599e-07
## 
## $initial
## [1] -1  1
## 
## $x
## [1] -5.747636e-09  1.149681e-08</code></pre>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="3.3-approximating-root-and-fixed-point-by-iteration.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="3.5-polynomialregression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
