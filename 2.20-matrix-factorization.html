<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>2.20 Matrix Factorization  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="2.20 Matrix Factorization  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="2.20 Matrix Factorization  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="2.19-types-of-matrices.html"/>
<link rel="next" href="2.21-software-libraries.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="matrix-factorization" class="section level2 hasAnchor">
<h2><span class="header-section-number">2.20</span> Matrix Factorization <a href="2.20-matrix-factorization.html#matrix-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Before we jump into <strong>Root Finding and Linear Regressions</strong>, let us first cover some efficient ways to <strong>solve systems of linear equations</strong> involving matrices.</p>
<p>Given the following <strong>matrix equation</strong>:</p>
<p><span class="math display" id="eq:eqnnumber64">\[\begin{align}
Ax = y \tag{2.30}
\end{align}\]</span></p>
<p>we solve for x:</p>
<p><span class="math display" id="eq:eqnnumber97" id="eq:eqnnumber96">\[\begin{align}
A^{-1}Ax {}&amp; = A^{-1}y \tag{2.31}\\
x &amp;= A^{-1}y \tag{2.32}
\end{align}\]</span></p>
<p>In previous sections, we showed how to get the <strong>inverse of A</strong> by multiplying the reciprocal of its determinant by its adjugate. Then, we showed how to solve the equation by performing matrix multiplication with âyâ.</p>
<p>This section covers a list of matrix decomposition methods that may help us deal with matrices more efficiently. We start by recalling one that we have already discussed in this chapter in the context of <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong> - <strong>Eigenvalue Decomposition</strong>.</p>
<div id="eigen-spectral-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.1</span> Eigen (Spectral) Decomposition  <a href="2.20-matrix-factorization.html#eigen-spectral-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to decompose a <strong>matrix</strong> into its <strong>Eigenvectors</strong>, <span class="math inline">\(P\)</span>, and <strong>Eigenvalues</strong>, <span class="math inline">\(\Lambda\)</span>, forming the following equation:</p>
<p><span class="math display" id="eq:eqnnumber98">\[\begin{align}
A = P\Lambda P^{-1} \tag{2.33}
\end{align}\]</span></p>
<p>Details of the decomposition and reconstruction of a matrix are discussed in <strong>Matrix Reconsruction</strong> Section.</p>
</div>
<div id="ludecomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.2</span> LU Decomposition (Doolittle Algorithm)<a href="2.20-matrix-factorization.html#ludecomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p> </p>
<p>The idea is to decompose a <strong>square matrix</strong> into its <strong>Lower-Triangular</strong> and <strong>Upper-Triangular</strong> forms, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber99">\[\begin{align}
A = LU \tag{2.34}
\end{align}\]</span></p>
<p>We transform the following sample <strong>matrix equation</strong>:</p>
<p><span class="math display">\[
\left[
\begin{array}{cccc}
a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\
b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\
c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\
d_1 &amp; d_2 &amp; d_3 &amp; d_4 
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span></p>
<p>into <strong>Lower and Upper Triangular</strong> form (Note that dotted entries here represent zero entries):</p>
<p><span class="math display">\[
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
L_{b_1} &amp; 1 &amp; . &amp; . \\
L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\
L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{cccc}
U_{a_1} &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\
. &amp; U_{b_2} &amp; U_{b_3} &amp; U_{b_4} \\
. &amp; . &amp; U_{c_3} &amp; U_{c_4} \\
. &amp; . &amp; . &amp; U_{d_4} 
\end{array}
\right]_{U_A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span></p>
<p>From here, we end up with a transformed <strong>matrix equation</strong>:</p>
<p><span class="math display" id="eq:eqnnumber100">\[\begin{align}
A = LU \rightarrow LUx = y  \tag{2.35}
\end{align}\]</span></p>
<p>And to solve for the system of equations, we have two extra formulas we can use.</p>
<p><span class="math display" id="eq:eqnnumber101">\[\begin{align}
Lu_y = y, \ \ \ \ Ux = u_y \tag{2.36}
\end{align}\]</span></p>
<p>We first solve for <span class="math inline">\(u_y\)</span> from <span class="math inline">\(Lu_y=y\)</span>, then use <span class="math inline">\(u_y\)</span> to solve for x from <span class="math inline">\(Ux=u_y\)</span>.</p>
<p><span class="math display" id="eq:eqnnumber102">\[\begin{align}
u_y = L^{-1}y \rightarrow\ \ \ \ x = U^{-1}u_y \tag{2.37}
\end{align}\]</span></p>
<p>To do that, we perform <strong>LU</strong> decomposition by the <strong>Gaussian Elimination</strong> algorithm. We covered <strong>Gaussian Elimination</strong> in this chapter to reduce matrices to <strong>Echelon Form</strong>. Here, instead of just dealing with a matrix, let us transform the following system of equations:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
1x_1 + 5x_2 + 5x_3 = 6 \\
2x_1 + 3x_2 + 4x_3 = 5 \\
3x_1 + 3x_2 + 3x_3 = 6 
 \end{array}\right)
\]</span></p>
<p>We start by first mapping the equation into a matrix form:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
1 &amp; 5 &amp; 5 \\
2 &amp; 3 &amp; 4 \\
3 &amp; 3 &amp; 3 \\
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \end{array}\right]_{x}
=
\left[\begin{array}{r} 6 \\ 5 \\ 6 \end{array}\right]_{y}
\]</span></p>
<p>Here, we will use an elementary matrix - the identity matrix - as a utility to help in decomposing the matrix. Let us use an augmented matrix to include vector y - so that we can validate our steps later.</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{I_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array} \left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{A|y} =
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array} \left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{A|y}
\]</span></p>
<p>The idea is to reduce the matrix to <strong>Echelon Form</strong> using <strong>Gaussian Elimination</strong>. We work our way from the left-most column of the lower triangular portion of the identity matrix to the right column. So, starting from the first column, we subtract two times the 1st row from the 2nd row and 3 of the 1st row from the 3rd row:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
-3 &amp; 0 &amp; 1 
\end{array}
\right]_{l_1}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}\left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{A}
=
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5\\
0 &amp; -12 &amp; -12 
\end{array}\left|\begin{array}{r}6 \\ -7 \\ -12\end{array}\right.
\right]_{U_1|u_{y_1}}
\]</span></p>
<p>Next, we use another identity matrix as a utility matrix to solve for the 2nd column. We subtract 2 of the 2nd row from the 3rd row:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; 1 
\end{array}
\right]_{l_2}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; -12 &amp; -12 
\end{array}\left|\begin{array}{r}6 \\ -7 \\ -12\end{array}\right.\right]_{l_1A = U_1}
=
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5\\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}\left|\begin{array}{r}6 \\ -7 \\ 2 \end{array}\right.
\right]_{U_A|u_y}
\]</span></p>
<p>Since we have already achieved an upper-triangular form, <span class="math inline">\(U_A\)</span>, we donât have to continue further. We have completed the <strong>Gaussian elimination</strong> process.</p>
<p>From this point, we need to derive the lower-triangular form, <span class="math inline">\(L_A\)</span>, using the following:</p>
<p><span class="math display">\[
L_A = (l_m \cdot l_{m-1} \cdot \ ...\ \cdot l_2 \cdot l_1)^{-1} = l_1^{-1} \cdot l_2^{-1} \cdot\ ...\  \cdot\ l_{m-1}^{-1} \cdot  l_{m}^{-1}
\]</span></p>
<p>That gives us:</p>
<p><span class="math display">\[
L_A = 
\left(
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; -2 &amp; 1 
\end{array}
\right]_{l_2}
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
-3 &amp; 0 &amp; 1 
\end{array}
\right]_{l_1}
\right)^{-1}
=
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\]</span></p>
<p>Overall, the <strong>LU decomposition</strong> gives us the following equation:</p>
<p><span class="math display" id="eq:eqnnumber103">\[\begin{align}
A = L_AU_A \tag{2.38}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span></p>
<p>Using the <strong>LU</strong> matrices, let us now solve for <strong>x</strong>. There are two equations we will use to solve for x. First, we solve for <span class="math inline">\(u_y\)</span> using <span class="math inline">\(L_A^{-1}y\)</span>. Then with <span class="math inline">\(u_y\)</span>, we solve for x using Equation (<a href="2.20-matrix-factorization.html#eq:eqnnumber102">(2.37)</a>).</p>
<p>Solving for <span class="math inline">\(u_y\)</span> requires <span class="math inline">\(L^{-1}\)</span> and <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
u_y = 
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
-2 &amp; 1 &amp; 0 \\
1 &amp; -2 &amp; 1 
\end{array}
\right]_{L^{-1}}
\left[\begin{array}{r} 6 \\ 5\\ 6 \end{array}\right]_{y} =
\left[\begin{array}{r} 6\\ -7 \\ 2 \end{array}\right]_{u_y}
\]</span></p>
<p>Note that we have already solved for <span class="math inline">\(u_y\)</span> as part of the augmented matrix in the <strong>Gaussian elimination</strong> portion. Let us validate in any case.</p>
<p>For that, we can use two alternative methods.</p>
<p>by <strong>Gauss forward elimination</strong>:</p>
<p><span class="math display">\[
L_A^{\{augmented\ u_y\}} = 
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array} \left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{L_A|y}  \rightarrow
u_y =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array} \left|\begin{array}{r}6 \\ -7 \\ 2\end{array}\right.
\right]_{I_L|u_y}
\]</span></p>
<p>or by <strong>forward substitution</strong>:</p>
<p>Given <span class="math inline">\(L_A\)</span> and <span class="math inline">\(y\)</span>, the equation for <strong>forward substitution</strong> is:</p>
<p><span class="math display" id="eq:eqnnumber105" id="eq:eqnnumber104">\[\begin{align}
u_{y_1} {}&amp;= \frac{ y_1}{L_{A_{1,1}}} \leftarrow initial \tag{2.39}\\
u_{y_i} &amp;=
\frac{ \left( y_{i} - \sum_{j=1}^{i-1} L_{A_{i,j}u_{y_j}} \right)}
{L_{A_{i,i}}}, \ \ \ \ \ \text{where }i =  2,3, ... n \tag{2.40}
\end{align}\]</span></p>
<p>Finally, solving for <span class="math inline">\(x\)</span> requires <span class="math inline">\(U^{-1}\)</span> and <span class="math inline">\(u_y\)</span>:</p>
<p><span class="math display">\[
x = 
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}^{-1}
\left[\begin{array}{r} 6 \\ -7 \\ 2 \end{array}\right]_{u_y} =
\left[\begin{array}{r} 1 \\ 2 \\ -1 \end{array}\right]_{x}
\]</span></p>
<p>For that, we can use two alternative methods:</p>
<p>by <strong>Jordan backward elimination</strong>:</p>
<p><span class="math display">\[
U_A^{\{augmented\ u_y\}} = 
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5  \\
0 &amp; 0 &amp; -2 
\end{array} \left|\begin{array}{r} 6 \\ -7 \\ 2\end{array}\right.
\right]_{U_A^{-1}|u_y} \rightarrow
x =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array} \left|\begin{array}{r}1 \\ 2 \\ -1\end{array}\right.
\right]_{I_U|x}
\]</span></p>
<p>or by <strong>backward substitution</strong>:</p>
<p>Given <span class="math inline">\(U_A\)</span> and <span class="math inline">\(u_y\)</span>, the equation for <strong>backward substitution</strong> is:</p>
<p><span class="math display" id="eq:eqnnumber108" id="eq:eqnnumber107">\[\begin{align}
x_n {}&amp;= \frac{ u_{y_n}}{U_{A_{n,n}}} \leftarrow initial \tag{2.41}\\
x_i &amp;= 
\frac{ \left(u_{y_i} - \sum_{j=i+1}^n U_{A_{i,j}x_j}\right)}
{ U_{A_{i,i}}}
, \ \ \ \ \ \text{where }i =  n-1, n-2, ..., 1 \tag{2.42}
\end{align}\]</span></p>
<p>Therefore, our solution for x is:</p>
<p><span class="math display">\[
x_1 = 1,\ \ \ \ x_2 = 2, \ \ \ \ x_3 = -1
\]</span></p>
<p>For a sample implementation of solving equations by <strong>LU decomposition</strong>, see the <strong>Doolittle</strong> algorithm in the following few sections.</p>
<p><strong>Gaussian Elimination with Partial Pivot (row swapping):</strong></p>
<p>Sometimes, it helps to swap rows to perform <strong>Gaussian elimination</strong> easily, especially when we have more leading zeros in the middle of our elimination for the next rows that we need to pivot. In a case where our leading diagonal entry happens to be zero, we can swap the row with any other subsequent row that has a non-zero diagonal entry, e.g., we can swap row 2nd and row 3rd below:</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 \\
1 &amp; 2 &amp; 3 \\
\end{array}
\right]
\]</span></p>
<p>To swap 1st row with 3rd row, we use a <strong>Permutation matrix (<span class="math inline">\(P_{r_a,r_b}\)</span>)</strong> as a <strong>swap utility</strong>. See below:</p>
<p><span class="math display">\[
P_{1,3}A = \left[
\begin{array}{ccc}
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; 0 
\end{array}
\right]_{P_{1,3}}
\left[
\begin{array}{ccc}
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 \\
1 &amp; 2 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 6 \\
0 &amp; 8 &amp; 9 
\end{array}
\right]_{A_{1,3}}
\]</span></p>
<p>To swap 2nd row with 3rd row:</p>
<p><span class="math display">\[
P_{2,3}A = \left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
0 &amp; 1 &amp; 0 
\end{array}
\right]_{P_{2,3}}
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 0 &amp; 6 \\
0 &amp; 8 &amp; 9 
\end{array}
\right]_{A_{1,3}} =
\left[
\begin{array}{ccc}
1 &amp; 2 &amp; 3 \\
0 &amp; 8 &amp; 9 \\
0 &amp; 0 &amp; 6 
\end{array}
\right]_{A_p}
\]</span></p>
<p>Overall, we performed two swaps (permutations):</p>
<p><span class="math display">\[
P_{2,3}P_{1,3}A = A_p
\]</span></p>
<p>Now, putting the <strong>LU factorization</strong> in the picture, there are times when we need to perform <strong>partial pivoting</strong> in the middle of the operation:</p>
<p><span class="math display">\[
l_2A\ (\ P_{1,3}l_1(\ P_{1,2}A\ )\ ) = L_AU_A
\]</span>
That reads as Swap 1st and 2nd row of matrix A, then perform <strong>Gaussian elimination (GE)</strong> on 1st column. Then swap the 1st and 3rd row of the resultant matrix, then continue with the <strong>GE</strong> on the second column.</p>
<p>Here, <strong>partial pivot</strong> may refer to swapping rows then performing the <strong>GE</strong> for the row and column - where we expect the <strong>pivot</strong> to be ( the non-zero leading entry). On the other hand, <strong>full pivoting</strong> is when swapping both rows and columns.</p>
<p>On that note, see if you can get the original systems of equations above into the following matrix below using a swap utility as we discussed:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
3x_1 + 3x_2 + 3x_3 = 6 \\
2x_1 + 3x_2 + 4x_3 = 5 \\
1x_1 + 5x_2 + 5x_3 = 6 
 \end{array}\right)
\]</span></p>
<p>There are two alternative solution for âxâ (see what operations to use to arrive at the solution):</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 1,\ \ \ \ x_2 = 2, \ \ \ \ x_3 = -1\ or \\
x_1 &amp;= 1,\ \ \ \ x_2 = 1, \ \ \ \ x_3 = 0
\end{align*}\]</span></p>
<p><strong>Gaussian Elimination with Sherman-Morrison:</strong> </p>
<p>There are cases in which our <strong>invertible</strong> matrix is changed, for example, where an entry (<strong>a rank-one</strong> entry) in the matrix is updated, resulting in a new given <strong>y</strong>, call it <span class="math inline">\(y_{upd}\)</span>. To avoid having to iterate through the whole process of <strong>LU factorization</strong> again because of this change, we use the <strong>Sherman-Morrison</strong> formula to solve for <strong>x</strong>:</p>
<p><span class="math display" id="eq:eqnnumber110">\[\begin{align}
x_1 = x_0 + \left( \frac{v^Tx_0}{1 - v^Tz} \right)z,\ \ \ 
where\ uv^T\text{ is given and }z = A^{-1}u \tag{2.43}
\end{align}\]</span></p>
<p>To illustrate, suppose that we updated our original matrix, <strong>A</strong>, in that the entry in <span class="math inline">\(A_{1,3}\)</span> (1st column, 3rd row) changes from 1 to 3.</p>
<p><span class="math display">\[
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3\\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array} \left|\begin{array}{c}6 \\ 5 \\ 6\end{array}\right.
\right]_{A|y} \rightarrow
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 5 &amp; 5 
\end{array} \left|\begin{array}{r}6 \\ 5 \\6\end{array}\right.
\right]_{A_{upd}|y}
\]</span></p>
<p>Suppose also that we have solved <span class="math inline">\(Ax = y\)</span> for the <strong>old x</strong> using <strong>LU factorization</strong> where:</p>
<p><span class="math display">\[
x_{old} = \left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{0}}
\]</span></p>
<p>So how do we solve for the <strong>new x</strong>?</p>
<p><strong>First</strong>, let us use a <strong>rank-one</strong> update matrix expressed as <span class="math inline">\(uv^T\)</span>. A <strong>rank-one</strong> matrix means, in this case, that we are updating the 1st rank order (column-wise).</p>
<p>For example, we use the following matrix, generated from <span class="math inline">\(uv^T\)</span>, to perform a rank-one update against the 1st column of our original matrix. The generated matrix is a representation of a perturbation in <span class="math inline">\(A_{1,3}\)</span> indicating a change in the entry from 1 to 3 (or to change the entire 1st column where <span class="math inline">\(u\)</span> is the update vector, nx1, and <span class="math inline">\(v^T\)</span> is the rank order vector, 1xn; here, it is the rank-one or the 1st rank (column-wise)):</p>
<p><span class="math display">\[
uv^T = \left[ \begin{array}{r} 0 \\ 0 \\  -2 \\ \end{array} \right]_u 
\left[ \begin{array}{ccc} 1 &amp; 0 &amp;  0 \\ \end{array} \right]_{v^T}  =
\left[ \begin{array}{rrr} 
0 &amp; 0 &amp; 0 \\  
0 &amp; 0 &amp; 0 \\ 
-2 &amp; 0 &amp; 0  
\end{array} \right]_{uv^T}
\]</span></p>
<p><strong>Second</strong>, let us solve for z in the equation <span class="math inline">\(Az = u\)</span>.</p>
<p><span class="math display">\[
z = A^{-1}u = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3\\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5
\end{array}
\right]_{A}^{-1}
\left[ \begin{array}{rrr} 0 \\ 0 \\ -2 \end{array} \right]_{u} =
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z} 
\]</span></p>
<p><strong>Third</strong>, let us now use <strong>Sherman-Morrison</strong> formula to solve for the <strong>new x</strong>,</p>
<p><span class="math display">\[
x_{new} = 
x_{old} + \left(1 - v^Tz\right)^{-1}v^Tx_{old}z =
x_{old} + \left( \frac{v^Tx_{old}}{1 - v^Tz} \right) z,
\]</span></p>
<p>for example:</p>

<p><span class="math display">\[\begin{align*}
x_{new} {}&amp;= 
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}} + 
\left(
\frac{ 
\left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \end{array} \right]_{v^T}
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}}
}
{1 - \left[ \begin{array}{rrr} 1 &amp; 0 &amp; 0 \end{array} \right]_{v^T}
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z}
}
\right)
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z}
\\
\\
&amp;= 
\left[ \begin{array}{rrr} 1 \\ 2 \\ -1 \end{array} \right]_{x_{old}} + 
\left(\frac{1}{1 - 0.5}\right)
\left[ \begin{array}{rrr} 0.5 \\ -1.5 \\ 1.0 \end{array} \right]_{z} \\
\\
&amp;=
\left[ \begin{array}{rrr} 2 \\ -1 \\ 1 \end{array} \right]_{x_{new}}
\end{align*}\]</span>
</p>
<p>From here, we can validate the solution using the equation below, given <span class="math inline">\(uv^T\)</span> and the original <span class="math inline">\(y\)</span>:</p>
<p><span class="math display" id="eq:eqnnumber111">\[\begin{align}
(A - uv^T)x_{new} = y \rightarrow\ \ \
x_{new} = (A - uv^T)^{-1}y \tag{2.44}
\end{align}\]</span></p>
<p>Example:</p>

<p><span class="math display">\[
x_{new} = (A - uv^T)^{-1}y_{old} = 
\left(
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
-
 \left[ \begin{array}{rrr} 
0 &amp; 0 &amp; 0 \\ 
0 &amp; 0 &amp; 0 \\ 
-2 &amp; 0 &amp; 0  
\end{array} \right]_{uv^T}
 \right)^{-1}
\left[ \begin{array}{ccc} 6 \\ 5 \\  6 \\ \end{array} \right]_{y_{old}}
\]</span>
</p>
<p>We get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 2,\ \ \ \ x_2 = -1, \ \ \ \ x_3 = 1
\end{align*}\]</span></p>
<p><strong>Gaussian Elimination with Sherman-Morrison-Woodbury (SMW):</strong> </p>
<p>In <strong>Sherman-Morrison</strong> formula, we use <span class="math inline">\(uv^T\)</span> term as rank-one update matrix representing an update (a perturbation) on a specific column given vector <span class="math inline">\(v^T\)</span>, <span class="math inline">\(1 \times n\)</span>, with update values of vector <span class="math inline">\(u\)</span>, <span class="math inline">\(n \times 1\)</span>. The <strong>Woodbury</strong> formula expands on <strong>Sherman-Morrison</strong> formula in that it uses an upper <span class="math inline">\((UI)^TV^T\)</span> term to signify <strong>rank-k</strong> update matrix where <span class="math inline">\(U\)</span> has <span class="math inline">\(n \times k\)</span> dimension, <span class="math inline">\(V^T\)</span> has <span class="math inline">\(k \times n\)</span> dimension, and <span class="math inline">\(I\)</span> has <span class="math inline">\(k \times k\)</span> dimension. The formula is written as <span class="citation">(Fill J. A. and Fishkind D. E. <a href="bibliography.html#ref-ref517j">1998</a>)</span>:</p>
<p><span class="math display" id="eq:eqnnumber112">\[\begin{align}
(A - UV^T)^{-1} = A^{-1} + A^{-1}U(I - V^TA^{-1}U)^{-1}V^TA^{-1} \tag{2.45}
\end{align}\]</span></p>
<p>where equation reduces to a <strong>Sherman-Morrison</strong> equation if the dimension of the identity matrix, I, reduces to 1x1 (where <span class="math inline">\(I=1\)</span>) and <span class="math inline">\(U\)</span> reduces to a dimension of nx1 (where <span class="math inline">\(U = u\)</span>) and <span class="math inline">\(V^T\)</span> reduces to a dimension of 1xn (where <span class="math inline">\(V=v\)</span>).</p>
<p><span class="math display" id="eq:eqnnumber113">\[\begin{align}
(A - uv^T)^{-1} = A^{-1} + A^{-1}u(1 - v^TA^{-1}u)^{-1}v^TA^{-1} \tag{2.46}
\end{align}\]</span></p>
<p>So how does <strong>SWM</strong> work?</p>
<p>Let us use the same matrix, A, but with a different perturbation. This time, the first row is multiplied by 1/3. Also, the 2nd and 3rd columns of the 2nd row are updated from 4 to 5 and 5 to 4, respectively.</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array} \left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{A|y_{old}}
 \rightarrow
\left[ 
\begin{array}{rrr}
1 &amp;1 &amp; 1 \\
2 &amp; 5 &amp; 4 \\
1 &amp; 5 &amp; 5
\end{array} \left|\begin{array}{r}6 \\ 5 \\ 6\end{array}\right.
\right]_{A_{new}|y_{old}}
\]</span>
</p>
<p>Let us construct <strong>a rank-k update matrix</strong> using <span class="math inline">\(UV^T\)</span> term:</p>

<p><span class="math display">\[
UV^T = 
\left[
\begin{array}{rrr}
2 &amp; 0 &amp; 0\\
0 &amp; -1 &amp; 1\\
0 &amp; 0 &amp; 0
\end{array}
\right]_{U}
\left[
\begin{array}{rrr}
1 &amp; 1 &amp; 1 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{V^T} = 
\left[
\begin{array}{rrr}
2 &amp; 2 &amp; 2 \\
0 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0
\end{array}
\right]_{UV^T}
\]</span>
</p>
<p>where K=3, N=3 so that <span class="math inline">\(U_{nxk}\)</span>, <span class="math inline">\(V_{kxn=kxn}\)</span>, and <span class="math inline">\(I_{kxk}\)</span>.</p>
<p>Derive <span class="math inline">\(z\)</span> from <span class="math inline">\(z = A^{-1}u\)</span>:</p>

<p><span class="math display">\[
z = 
\left[
\begin{array}{rrr}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array} 
\right]_{A}^{-1} 
\left[
\begin{array}{rrr}
2 &amp; 0 &amp; 0 \\
2 &amp; -1 &amp; 1 \\
0 &amp; 0 &amp; 0 
\end{array} 
\right]_{U} 
=
\left[
\begin{array}{rrr}
1/12 &amp; 0 &amp; 0\\
1/12 &amp; 1 &amp; -1 \\
-1 &amp; -1 &amp; 1
\end{array}
\right]_{z}
\]</span>
</p>
<p>With <strong>Sherman-Morrison-Woodbury</strong> formula:</p>
<p><span class="math display">\[
x_{new} = \frac{(V^Tx_{old})_n}{(I - V^Tz)_d} = 
\left(I - V^Tz\right)_d^{-1}(V^Tx_{old})_n
\]</span></p>

<p><span class="math display">\[\begin{align*}
\left(I - V^Tz\right)_d^{-1} {}&amp;= 
\left(
\left[
\begin{array}{ccc}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{I}
-
 \left[ \begin{array}{rrr} 
1 &amp; 1 &amp; 1 \\ 
0 &amp; 1 &amp; 0 \\ 
0 &amp; 0 &amp; 1  
\end{array} \right]_{V^T}
\left[
\begin{array}{rrr}
1/12 &amp; 0 &amp; 0\\
1/12 &amp; 1 &amp; -1 \\
-1 &amp; -1 &amp; 1
\end{array}
\right]_{z}
\right)_d^{-1}\\
&amp;=
\left[
\begin{array}{ccc}
1/3 &amp; 0 &amp; 0 \\
-1/12 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{array}
\right]_d^{-1}
\end{align*}\]</span>
</p>
<p>Finally, solving for <span class="math inline">\(x_{new}\)</span>:</p>

<p><span class="math display">\[
x_{new} = 
\left[
\begin{array}{ccc}
1/3 &amp; 0 &amp; 0 \\
-1/12 &amp; 0 &amp; 1 \\
1 &amp; 1 &amp; 0
\end{array}
\right]_{d}^{-1}
\left(
\left[ \begin{array}{rrr} 
1 &amp; 1 &amp; 1 \\ 
0 &amp; 1 &amp; 0 \\ 
0 &amp; 0 &amp; 1  
\end{array} \right]_{V^T}
\left[\begin{array}{r} 1 \\ 2 \\ -1\end{array}\right]_{x_{old}}
\right)_n,
\]</span>
</p>
<p>we get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 6,\ \ \ \ x_2 = -7, \ \ \ \ x_3 = 7.
\end{align*}\]</span></p>
<p>Validate using <span class="math inline">\((A - UV^T)^{-1}y_{old}\)</span>:</p>

<p><span class="math display">\[
x_{new} = (A - UV^T)^{-1}y_{old} = 
\left(
\left[
\begin{array}{ccc}
3 &amp; 3 &amp; 3 \\
2 &amp; 4 &amp; 5 \\
1 &amp; 5 &amp; 5 
\end{array}
\right]_{A}
-
 \left[ \begin{array}{rrr} 
2 &amp; 2 &amp; 2 \\ 
0 &amp; -1 &amp; 1 \\ 
0 &amp; 0 &amp; 0  
\end{array} \right]_{UV^T}
 \right)^{-1}
\left[ \begin{array}{ccc} 6 \\ 5 \\  6 \\ \end{array} \right]_{y_{old}},
\]</span>
</p>
<p>we get the solution for <strong>x</strong>:</p>
<p><span class="math display">\[\begin{align*}
x_1 {}&amp;= 6,\ \ \ \ x_2 = -7, \ \ \ \ x_3 = 7.
\end{align*}\]</span></p>
<p><strong>LU decomposition using Doolittle Algorithm:</strong> </p>
<p>Another algorithm to introduce is the <strong>Doolittle algorithm</strong>, which avoids using the <strong>Gaussian Elimination</strong>.</p>
<p>Below is the notation of the algorithm, given a Matrix, <span class="math inline">\(A_{nxn}\)</span>, an initial sparse lower-triangular matrix, <span class="math inline">\(L_{nxn}\)</span>, and an initial sparse upper-triangular matrix, <span class="math inline">\(U_{nxn}\)</span>. Here, <strong>sparse</strong> denotes a matrix with entries equal to zero:</p>
<p><span class="math display">\[\begin{align*}
for\ i = 1..n\ : \\
\ \ \ \ U_{ij} {}&amp;= A_{ij} - \sum_{k=1}^i L_{ik} U_{k,j},\ for\ j = i..n \\
\\
\ \ \ \ L_{ji} &amp;= \frac{ \left( A_{ji} - \sum_{k=1}^i L_{jk} U_{k,i} \right) }{  U_{ii} }, \
\ for\ j = i..n\ and\ \ L_{jj} = 1\ \ if\ i = k
\end{align*}\]</span></p>
<p>We note that the <strong>Lower</strong> matrix and <strong>Upper</strong> matrix may separately contain a diagonal with all ones. In the latter part of this section, we discuss <strong>Doolittle</strong> decomposition in which <strong>L</strong> has such property. Otherwise, if <strong>U</strong> has diagonal entries containing all ones, it is recognized as <strong>Crout</strong> decomposition. We will also cover <strong>Cholesky decomposition</strong> in which <strong>L</strong> = <strong>U</strong> <span class="citation">(StanimirovÃ­c P. S. et al <a href="bibliography.html#ref-ref508p">2011</a>)</span>.</p>
<p>Here is a naive implementation of the <strong>Doolittle algorithm</strong> in R code:</p>

<div class="sourceCode" id="cb14"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb14-1" data-line-number="1">lu_decomposition_by_doolittle &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb14-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb14-3" data-line-number="3">  u =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n) <span class="co"># Sparse Matrix for Upper Triangular </span></a>
<a class="sourceLine" id="cb14-4" data-line-number="4">  l =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n) <span class="co"># Sparse Matrix for Lower Triangular </span></a>
<a class="sourceLine" id="cb14-5" data-line-number="5">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb14-6" data-line-number="6">    <span class="co"># Upper Triangular Loop</span></a>
<a class="sourceLine" id="cb14-7" data-line-number="7">    <span class="cf">for</span> (j <span class="cf">in</span>  i<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb14-8" data-line-number="8">        m_ =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb14-9" data-line-number="9">        <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb14-10" data-line-number="10">            m_ =<span class="st"> </span>m_ <span class="op">+</span><span class="st"> </span>l[i,k] <span class="op">*</span><span class="st"> </span>u[k,j]</a>
<a class="sourceLine" id="cb14-11" data-line-number="11">        }</a>
<a class="sourceLine" id="cb14-12" data-line-number="12">        u[i,j] =<span class="st"> </span>A[i,j] <span class="op">-</span><span class="st"> </span>m_ </a>
<a class="sourceLine" id="cb14-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb14-14" data-line-number="14">    <span class="co"># Lower Triangular Loop</span></a>
<a class="sourceLine" id="cb14-15" data-line-number="15">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb14-16" data-line-number="16">        <span class="cf">if</span> (i <span class="op">==</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb14-17" data-line-number="17">            l[j,j] =<span class="st"> </span><span class="dv">1</span> <span class="co"># Diagonal Entry = 1</span></a>
<a class="sourceLine" id="cb14-18" data-line-number="18">        } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb14-19" data-line-number="19">            m_ =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb14-20" data-line-number="20">            <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>i) {</a>
<a class="sourceLine" id="cb14-21" data-line-number="21">                m_ =<span class="st"> </span>m_ <span class="op">+</span><span class="st"> </span>l[j,k] <span class="op">*</span><span class="st"> </span>u[k,i]</a>
<a class="sourceLine" id="cb14-22" data-line-number="22">            }</a>
<a class="sourceLine" id="cb14-23" data-line-number="23">            l[j,i] =<span class="st"> </span>( A[j,i] <span class="op">-</span><span class="st"> </span>m_ ) <span class="op">/</span><span class="st"> </span>u[i,i]</a>
<a class="sourceLine" id="cb14-24" data-line-number="24">        }</a>
<a class="sourceLine" id="cb14-25" data-line-number="25">    }</a>
<a class="sourceLine" id="cb14-26" data-line-number="26">  }</a>
<a class="sourceLine" id="cb14-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;matrix&quot;</span> =<span class="st"> </span>A, <span class="st">&quot;lower&quot;</span> =<span class="st"> </span>l, <span class="st">&quot;upper&quot;</span> =<span class="st"> </span>u)</a>
<a class="sourceLine" id="cb14-28" data-line-number="28">}</a></code></pre></div>

<p>Note that the R code and algorithm do not include pivoting. The R code for the <strong>Doolittle algorithm</strong> can be modified further to include pivoting. This book does not discuss the pivoting portion of the algorithm.</p>
<p>Here is solving a system by <strong>LU decomposition</strong> for <span class="math inline">\(Ax = b\)</span>:</p>

<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb15-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb15-2" data-line-number="2">b =<span class="st"> </span><span class="kw">c</span>(<span class="dv">6</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb15-3" data-line-number="3">LU =<span class="st"> </span><span class="kw">lu_decomposition_by_doolittle</span>(A)  <span class="co">#derived from Doolittle section</span></a>
<a class="sourceLine" id="cb15-4" data-line-number="4">uy =<span class="st"> </span><span class="kw">forward_sub</span>(LU<span class="op">$</span>lower, b) <span class="co">#derived from REF/RREF section</span></a>
<a class="sourceLine" id="cb15-5" data-line-number="5">x =<span class="st"> </span><span class="kw">backward_sub</span>(LU<span class="op">$</span>upper, uy) <span class="co">#derived from REF/RREF section</span></a></code></pre></div>

<p>Also, one more algorithm similar to <strong>Doolittle algorithm</strong> is the <strong>Croutâs algorithm</strong>, which has diagonal entries of one for the upper triangular matrix. It may also help to be aware of and understand the algorithm. This book does not discuss the details of <strong>Croutâs algorithm</strong>.</p>
</div>
<div id="ldu-factorization" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.3</span> LDU Factorization <a href="2.20-matrix-factorization.html#ldu-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>LDU factorization</strong> is another form of <strong>LU factorization</strong>. The difference is that <strong>LDU</strong> includes a <strong>diagonal matrix</strong> derived out of decomposing the <strong>upper-triangular matrix</strong> further so that the <strong>U</strong> matrix contains a diagonal of ones.</p>

<p><span class="math display">\[
A = 
\underbrace{
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
L_{b_1} &amp; 1 &amp; . &amp; . \\
L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\
L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 
\end{array}
\right]}_{L_A}
\underbrace{
\left[
\begin{array}{rrrr}
D_{a1}^u &amp; . &amp; . &amp; . \\
. &amp; D_{b2}^u &amp; . &amp; . \\
. &amp; . &amp; D_{c3}^u &amp; . \\
. &amp; . &amp; . &amp; D_{d4}^u
\end{array}
\right]}_{D_U}
\underbrace{
\left[
\begin{array}{cccc}
1 &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\
. &amp; 1 &amp; U_{b_3} &amp; U_{b_4} \\
. &amp; . &amp; 1 &amp; U_{c_4} \\
. &amp; . &amp; . &amp; 1 
\end{array}
\right]}_{U_A}
\]</span>
</p>
<p>For example, using the following <span class="math inline">\(A = LU\)</span>:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span>
</p>
<p>we decompose <span class="math inline">\(U_A\)</span> matrix by extracting its diagonal into a separate <strong>diagonal matrix</strong> and then replacing the diagonal of <span class="math inline">\(U_A\)</span> matrix with ones:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A} \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; -6 &amp; 0 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{D_U}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; 1 &amp; 5/6 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{U_{new}}
\]</span>
</p>
<p>That is achieved by performing the following:</p>
<ul>
<li>Construct a <strong>diagonal matrix</strong>, <span class="math inline">\(D_U\)</span>, by extracting the diagonal entries of <span class="math inline">\(U_A\)</span>.</li>
<li>For R1, no further operation is needed because the first diagonal entry is already a one.</li>
<li>For R2, multiply R2 by -1/6; in notation, ( -1/6 * R2 <span class="math inline">\(\rightarrow\)</span> R2 ).</li>
<li>For R3, multiply R3 by -1/2; in notation, ( -1/2 * R3 <span class="math inline">\(\rightarrow\)</span> R3 ).</li>
</ul>
<p>So for the final result:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; -6 &amp; 0 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{D_U}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; 1 &amp; 5/6 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{U_{new}}
\]</span>
</p>
<p>Note that <strong>LU/LDU decomposition</strong> by <strong>Gauss-Jordan Elimination</strong> works on <strong>invertible square matrices</strong>. Let us now take a look at another decomposition.</p>
</div>
<div id="qr-factorization-gram-schmidt-householder-and-givens" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.4</span> QR Factorization (Gram-Schmidt, Householder, and Givens) <a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to decompose a <strong>full-rank matrix</strong> into its <strong>QR</strong> form; where <strong>Q</strong> is an <strong>orthogonal matrix</strong> and <strong>R</strong> is an <strong>upper-triangular matrix</strong>, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber114">\[\begin{align}
A = QR \tag{2.47}
\end{align}\]</span></p>
<p>We transform the following sample <strong>matrix equation</strong>:</p>

<p><span class="math display">\[
\left[
\begin{array}{cccc}
a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\
b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\
c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\
d_1 &amp; d_2 &amp; d_3 &amp; d_4 
\end{array}
\right]_{A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span>
</p>
<p>into <strong>QR</strong> form:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrr}
Q_{a_1} &amp; Q_{a_2} &amp; Q_{a_3} &amp;  Q_{a_4} \\
Q_{b_1} &amp; Q_{b_2} &amp; Q_{b_3} &amp;  Q_{b_4} \\
Q_{c_1} &amp; Q_{c_2} &amp; Q_{c_3} &amp;  Q_{c_4} \\
Q_{d_1} &amp; Q_{d_2} &amp; Q_{d_3} &amp;  Q_{d_4} 
\end{array}
\right]_{Q_A}
\left[
\begin{array}{cccc}
R_{a_1} &amp; R_{a_2} &amp; R_{a_3} &amp; R_{a_4} \\
. &amp; R_{b_2} &amp; R_{b_3} &amp; R_{b_4} \\
. &amp; . &amp; R_{c_3} &amp; R_{c_4} \\
. &amp; . &amp; . &amp; R_{d_4} 
\end{array}
\right]_{R_A}
\left[\begin{array}{c} x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]_{x}
=
\left[\begin{array}{c} y_1 \\ y_2 \\ y_3 \\ y_4 \end{array}\right]_{y}
\]</span>
</p>
<p>For a quick insight of an <strong>orthogonal matrix</strong>, <span class="math inline">\(Q\)</span>, let us use Figure <a href="2.20-matrix-factorization.html#fig:orthoprojectreflect">2.24</a> for the <strong>orthogonal projection</strong> image - left side. We cover <strong>orthogonal projection</strong> first then <strong>orthogonal reflection</strong> next.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:orthoprojectreflect"></span>
<img src="ortho_project_reflect.png" alt="Orthogonal Projection and Reflection" width="90%" />
<p class="caption">
Figure 2.24: Orthogonal Projection and Reflection
</p>
</div>
<p>The matrix representations of vectors <strong>a</strong>, <strong>b</strong>, <strong>c</strong> with <strong>p</strong> in the figure is as follows:</p>
<p><span class="math display">\[
\left[\begin{array}{cc}0 &amp; 5\\ 3 &amp; 5 \end{array}\right]_{ap}\ \ \ \ \ \ \
\left[\begin{array}{cc}1 &amp; 5\\ 4 &amp; 5 \end{array}\right]_{bp}\ \ \ \ \ \ \
\left[\begin{array}{cc}2 &amp; 5\\ 5 &amp; 5 \end{array}\right]_{cp} 
\]</span></p>
<p>The following projections apply for <strong>a</strong>, <strong>b</strong>, and <strong>c</strong> on to <strong>p</strong>:</p>

<p><span class="math display">\[\begin{align*}
a&#39; = proj_{p}a = \left(\frac{&lt;5,5&gt;^T&lt;0,3&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{15}{50} &lt;5,5&gt; = &lt;1.5,1.5&gt; \\
b&#39; = proj_{p}b = \left(\frac{&lt;5,5&gt;^T&lt;1,4&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{25}{50} &lt;5,5&gt; = &lt;2.5,2.5&gt; \\
c&#39; = proj_{p}c = \left(\frac{&lt;5,5&gt;^T&lt;2,5&gt;}{\|&lt;5,5&gt;\|^2}\right) &lt;5,5&gt; = \frac{35}{50} &lt;5,5&gt; = &lt;3.5,3.5&gt; \\
\end{align*}\]</span>
</p>
<p>The following orthogonal projections apply for <strong>a</strong>, <strong>b</strong>, and <strong>c</strong> on to <strong>p</strong>:</p>

<p><span class="math display">\[\begin{align*}
o&#39;a = a - a&#39; = &lt;0,3&gt; - &lt;1.5, 1.5&gt; = &lt;-1.5, 1.5&gt;\\
o&#39;b = b - b&#39; = &lt;1,4&gt; - &lt;2.5, 2.5&gt; = &lt;-1.5, 1.5&gt;\\
o&#39;c = c - c&#39; = &lt;2,5&gt; - &lt;3.5, 3.5&gt; = &lt;-1.5, 1.5&gt;
\end{align*}\]</span>
</p>
<p>From here, we can form a <strong>Quasi-Orthogonal matrix</strong> for each of <strong>oâa</strong>, <strong>oâb</strong>, and <strong>oâc</strong> with respect to <strong>p</strong>:</p>
<p><span class="math display">\[
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;a}\ \ \ \ \ \ \
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;b}\ \ \ \ \ \ \
\left[\begin{array}{cc}-1.5 &amp; 5\\ 1.5 &amp; 5 \end{array}\right]_{o&#39;c} 
\]</span>
Note that the matrix is only a <strong>Quasi-Orthogonal</strong> matrix. A matrix is considered orthogonal if it meets at least the following:</p>
<p><span class="math display" id="eq:eqnnumber115">\[\begin{align}
Q^{-1} = Q^T,\ \ \ Q^TQ = QQ^T = I \tag{2.48}
\end{align}\]</span></p>
<p>To do that, let us normalize each column of the matrix (for now, let us use <span class="math inline">\(o&#39;a\)</span> to illustrate, though this applies to both <span class="math inline">\(o&#39;b\)</span> and <span class="math inline">\(o&#39;c\)</span> just the same):</p>
<p><span class="math display">\[\begin{align*}
q&#39;a {}&amp;= \frac{o&#39;a}{ \| o&#39;a \|_{L2}} = \frac{&lt;-1.5, 1.5&gt;}{ \|&lt;-1.5, 1.5&gt;\|} = &lt; -0.7071068, 0.7071068&gt; \\
q&#39;p &amp;= \frac{p }{ \| p \|_{L2}} = \frac{&lt;5, 5&gt;}{\|&lt;5, 5&gt;\|} = &lt; 0.7071068, 0.7071068&gt; 
\end{align*}\]</span></p>
<p>This forms the <span class="math inline">\(Q&#39;A\)</span> matrix - an <strong>orthogonal matrix</strong>:</p>
<p><span class="math display">\[
Q_A =  \left[\begin{array}{rrr}
-0.7071068 &amp; 0.7071068  \\
0.7071068 &amp; 0.7071068   
\end{array}\right] =
\left[\begin{array}{r} -0.7071068 \\ 0.7071068 \end{array}\right]_{q&#39;a}
\left[\begin{array}{r} 0.7071068 \\ 0.7071068 \end{array}\right]_{q&#39;p.}
\]</span>
To validate:</p>
<p><span class="math display">\[
Q_A^{-1} = Q_A^T,\ \ \ \ Q_A^TQ_A = Q_AQ_A^T = I.
\]</span></p>
<p>Additionally, if a matrix is a <strong>full-rank</strong> matrix, e.g., all columns are non-singular <strong>linearly independent</strong>, then an <strong>orthogonal matrix</strong> can be formed. </p>
<p>In the Figure <a href="2.20-matrix-factorization.html#fig:orthoprojectreflect">2.24</a>, we sampled vectors <strong>a</strong>, <strong>b</strong>, <strong>c,</strong>, and <strong>p</strong> to form three separate <strong>orthogonal matrices</strong>. However, combining the vectors in one matrix will not form a <strong>full-rank</strong> matrix because each column is not linearly independent of the others.</p>
<p>Below is a matrix we have used in previous <strong>QR decomposition</strong> algorithms. This matrix is <strong>full-rank</strong> where all columns are linearly independent:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A 
\]</span></p>
<p>In this case, we can first get the <strong>orthogonal projections</strong> of each column: </p>
<p><span class="math display">\[\begin{align*}
q_1&#39; {}&amp;= a_1 \\
q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\
q_3&#39; &amp;= a_3 - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3
\end{align*}\]</span></p>
<p>Equivalently, to be more in general terms, here is a list of the <strong>orthogonal basis</strong> of the projections: </p>
<p><span class="math display">\[\begin{align*}
q_1&#39; &amp;= a_1 \\
q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\
q_3&#39; &amp;= a_k - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3  \\
\vdots \\
q_k&#39; &amp;= a_k - proj_{q_1&#39;}a_k - proj_{q_2&#39;}a_k -\ ...\ -\  proj_{q_{k-1}&#39;}a_k   \\
\end{align*}\]</span></p>
<p>Note that the projections are subtracted from the projected vector to get the equivalent <strong>orthogonal vector (projection)</strong> perpendicular to all other <strong>orthogonal projections</strong>.</p>
<p>Finally, normalize each of the <strong>orthogonal projections</strong> to form the <strong>orthogonal matrix columns</strong>:</p>
<p><span class="math display">\[
q_1 = \frac{q_1&#39;} {\|q_1&#39;\|}\ \ \ \ \ q_2 = \frac{q_2&#39;} {\|q_2&#39;\|}\ \ \ \ \ \ 
q_3 = \frac{q_3&#39;} {\|q_3&#39;\|} \ \ \ ... \ \ \  q_k = \frac{q_k&#39;} {\|q_k&#39;\|}
\]</span></p>
<p>with that, we obtain <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(R_A\)</span>:</p>
<p><span class="math display">\[
Q_A = 
 \left[
\begin{array}{r}
q_{1a} \\q_{1b} \\ q_{1c} \\ \vdots  \\ q_{1k}  \\ \end{array}
\left|\begin{array}{r}q_{2a} \\ q_{2b} \\ q_{2c} \\ \vdots \\ q_{2k} \end{array}\right.
\left|\begin{array}{r}q_{3a} \\ q_{3b} \\ q_{3c} \\ \vdots \\ q_{3k} \end{array}\right.
\left|\begin{array}{r} ... \\ ...\\ ...\\ \ddots \\ ... \end{array}\right.
\left|\begin{array}{r}q_{ka} \\ q_{kb} \\ q_{kc} \\ \vdots \\ q_{kk} \end{array}\right.
\right]
\ \ \ \ \ \ \ 
R_A \leftarrow 
\begin{cases} 
r_{kj} = q_{k}^Ta_j\ \ \ if\ k\neq j \\
r_{kj} = \|q_j&#39;\|_2\ \ \ if\ k=j\ \ \ \{classic\}
\end{cases}
\]</span></p>
<p>We now illustrate <strong>QR</strong> factorization using three decomposition methods:</p>
<p><strong>Gram-Schmidt (GS) algorithm:</strong> </p>
<p>There are two versions of <strong>Gram-Schmidt</strong> algorithm, namely <strong>classic</strong> and <strong>modified</strong> <span class="citation">(Gander W. <a href="bibliography.html#ref-ref527w">1980</a>)</span>:</p>

<p><span class="math display">\[
\begin{array}{l}
Classic\ (CGS) \\
------- \\
\text{loop j = 1 : n} \\
\ \ \ \ q_j&#39; = a_j \\
\ \ \ \ \text{loop k = 1 : j } \\
\ \ \ \ \ \ \ \ \ \ r_{kj} = q_k^Tq_j&#39; \\
\ \ \ \ \ \ \ \ \ \ q_j&#39; = q_j&#39; - r_{kj}q_k \\
\ \ \ \ \text{end loop} \\
\ \ \ \ \ r_{jj} = \| q_j&#39; \|_{L2} \\
\ \ \ \ \ q_j = q_j&#39;\ /\ r_{jj} \\
\text{end loop}  
\end{array}
\left|
\begin{array}{l}
 Modified\ (MGS) \\
 -------- \\
   \text{loop j = 1 : n}\\
 \ \ \ \ r_{jj} = \| a_j\|_{L2}\\
 \ \ \ \ q_j = a_j\ /\ \| r_{jj}   \\
 \ \ \ \ \text{loop k = j : n}\\
 \ \ \ \ \ \ \ \ r_{jk} = q_j^Ta_k \\
 \ \ \ \ \ \ \ \ a_k = a_k - r_{jk}q_j  \\
 \ \ \ \ \text{end loop}\\
 \text{end loop}\\
 \ 
\end{array}
\right.
\]</span>
</p>
<p>Let us step through the classic algorithm. Consider our previous sample matrix:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A = 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{a1} 
\left[\begin{array}{r} 5 \\4 \\ 3 \end{array}\right]_{a2} 
\left[\begin{array}{r} 5 \\3 \\ 3 \end{array}\right]_{a3}
\]</span>
</p>
<p><strong>First</strong>, let us handle <span class="math inline">\(q_1\)</span> and <span class="math inline">\(R_{a1}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_1&#39; {}&amp;= a_{j=1} = &lt;1,2,3&gt;\ \ \ \leftarrow\ \ \ where\ j = 1 \\
R_{a1} &amp;= ||q_1&#39;||_{L2} = \sqrt{1^2 + 2^2 + 3^3} = \sqrt{14} \\
q_1 &amp;= \frac{1}{R_{a1}}(q_1&#39;) =  \frac{1} {\sqrt{14}}(&lt;1,2,3&gt;) \\
&amp;= \left(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right)
\end{align*}\]</span>
</p>
<p><strong>Second</strong>, let us now take care of <span class="math inline">\(q_2\)</span>, <span class="math inline">\(R_{a2}\)</span>, <span class="math inline">\(R_{b2}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_2&#39; {}&amp;= a_{j=2} = &lt;5,4,3&gt; \ \ \ \leftarrow\ \ \ where\ j = 2 \\
R_{a2} &amp;= q_1^Tq_2&#39; =  \left(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right)^T
&lt;5,4,3&gt; = \sqrt{14}+ 4 \sqrt{\frac{2}{7}} \\
\\
q_2&#39;  &amp;= q_2&#39; - R_{a2}q_1 \leftarrow (I - q_1q_1^T)q_2&#39; \\
&amp;= &lt;5,4,3&gt; - \left( \sqrt{14}+ 4 \sqrt{\frac{2}{7}}\right)
\left(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right) \\
&amp;= \left(\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right)
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
R_{b2} &amp;= \|q_2&#39;\|_{L2} = 
\sqrt{\left(\frac{24}{7}\right)^2 + 
\left(\frac{6}{7}\right)^2 + \left(\frac{-12}{7}\right)^2} = \frac{6\sqrt{21}}{7}
\\
q_2 &amp;= \frac{1}{R_{b2}}(q_2&#39;) = 
\frac{1}{\frac{6\sqrt{21}}{7}} \left(\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right)  =
\frac{7}{6\sqrt{21}}{\left(\frac{24}{7}, \frac{6}{7},\frac{-12}{7}\right)} \\
&amp;= \left(\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right) 
\end{align*}\]</span>
</p>
<p><strong>Lastly</strong>, let us finally take care of <span class="math inline">\(q_3\)</span>, <span class="math inline">\(R_{a3}\)</span>, <span class="math inline">\(R_{b3}\)</span>, and <span class="math inline">\(R_{c3}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
q_3&#39; {}&amp;= a_{j=3} = &lt;5,5,3&gt; \ \ \ \leftarrow\ \ \ where\ j = 3 \\
R_{a3} &amp;= q_1^Tq_3&#39; =  \left(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right)^T
&lt;5,5,3&gt; = \sqrt{14}+ 5 \sqrt{\frac{2}{7}} \\
\\
R_{b3} &amp;= q_2^Tq_3&#39; =  \left(\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right)^T
&lt;5,5,3&gt; = \frac{19}{\sqrt{21}} \\
\\
q_3&#39; &amp;= q_3&#39; - R_{a3}q_1 - R_{b3}q_2  \leftarrow (I - q_1q_1^T)(I - q_2q_2^T)q_3&#39; \\
&amp;= &lt;5,5,3&gt; - \\
&amp;\ \ \left( \sqrt{14}+ 5 \sqrt{\frac{2}{7}}\right) \left(\frac{1}{\sqrt{14}}, \frac{2}{\sqrt{14}},\frac{3}{\sqrt{14}}\right) -
\left(\frac{19}{\sqrt{21}}\right)
\left(\frac{4}{\sqrt{21}}, \frac{1}{\sqrt{21}},\frac{-2}{\sqrt{21}}\right) \\
&amp;= \left(\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right) 
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
R_{c3} &amp;= \|q_3&#39;\| = \|\left(\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right)\| = 
\sqrt{\left(\frac{-1}{3}\right)^2,\left(\frac{2}{3}\right)^2,\left(\frac{-1}{3}\right)^2} 
=  \sqrt{\frac{2}{3}}\\
q_3 &amp;= \frac{1}{R_{c3}}(q_3&#39;) = \left(\frac{1}{\sqrt{\frac{2}{3}}}\right) \left(\frac{-1}{3},\frac{2}{3},\frac{-1}{3}\right) \\
&amp;= \left(\frac{-1}{\sqrt{6}}, \sqrt{\frac{2}{3}}, \frac{-1}{\sqrt{6}}\right)
\end{align*}\]</span>
</p>
<p>The steps above leads to the following <strong>QR</strong> form:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_A =
\left(\begin{array}{rrr}
\frac{1}{\sqrt{14}} &amp; \frac{4}{\sqrt{21}} &amp; \frac{-1}{\sqrt{6}} \\ 
\frac{2}{\sqrt{14}} &amp; \frac{1}{\sqrt{21}} &amp; \sqrt{\frac{2}{3}} \\ 
\frac{3}{\sqrt{14}} &amp; \frac{-2}{\sqrt{21}} &amp; \frac{-1}{\sqrt{6}} \\
\end{array}
\right)_Q 
\left(\begin{array}{rrr}
\sqrt{14} &amp; \sqrt{14}+ 4 \sqrt{\frac{2}{7}} &amp; \sqrt{14}+ 5 \sqrt{\frac{2}{7}} \\  
. &amp; \frac{6\sqrt{21}}{7} &amp; \frac{19}{\sqrt{21}} \\ 
. &amp; . &amp; \sqrt{\frac{2}{3}}
\end{array}
\right)_R
\]</span></p>
<p>Solving for <strong>R</strong> after computing for <strong>Q</strong>, we can use the following formula (note that <span class="math inline">\(Q^{-1} = Q^T\)</span>):</p>
<p><span class="math display" id="eq:eqnnumber116">\[\begin{align}
R = Q^{-1}A = Q^TA \tag{2.49}
\end{align}\]</span></p>
<p>Below are the <strong>Classic &amp; Modified Gram-Schmidt</strong> algorithms with a naive implementation in R code:</p>

<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">qr_decomposition_by_cgs &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb16-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb16-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb16-4" data-line-number="4">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb16-5" data-line-number="5">  r =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n, <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># Sparse Matrix for R</span></a>
<a class="sourceLine" id="cb16-6" data-line-number="6">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb16-7" data-line-number="7">    qj_ =<span class="st"> </span>A[,j]</a>
<a class="sourceLine" id="cb16-8" data-line-number="8">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb16-9" data-line-number="9">        r[k,j] =<span class="st">  </span><span class="kw">t</span>(q[,k]) <span class="op">%*%</span><span class="st"> </span>qj_</a>
<a class="sourceLine" id="cb16-10" data-line-number="10">        qj_ =<span class="st"> </span>qj_ <span class="op">-</span><span class="st"> </span>r[k,j] <span class="op">*</span><span class="st"> </span>q[,k] </a>
<a class="sourceLine" id="cb16-11" data-line-number="11">    }</a>
<a class="sourceLine" id="cb16-12" data-line-number="12">    r[j,j] =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(qj_<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb16-13" data-line-number="13">    <span class="cf">if</span> (r[j,j]<span class="op">==</span><span class="dv">0</span>) <span class="cf">break</span> <span class="co"># on linear dependency</span></a>
<a class="sourceLine" id="cb16-14" data-line-number="14">    q[,j] =<span class="st"> </span>qj_ <span class="op">/</span><span class="st"> </span>r[j,j]</a>
<a class="sourceLine" id="cb16-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb16-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>r, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>r)</a>
<a class="sourceLine" id="cb16-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb16-18" data-line-number="18">qr_decomposition_by_mgs &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb16-19" data-line-number="19">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb16-20" data-line-number="20">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb16-21" data-line-number="21">  q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, m<span class="op">*</span>n), m, <span class="dt">byrow=</span><span class="ot">TRUE</span> ) <span class="co"># Sparse Matrix for Q</span></a>
<a class="sourceLine" id="cb16-22" data-line-number="22">  r =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n<span class="op">*</span>n), n, <span class="dt">byrow=</span><span class="ot">TRUE</span>) <span class="co"># Sparse Matrix for R</span></a>
<a class="sourceLine" id="cb16-23" data-line-number="23">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb16-24" data-line-number="24">        r[j,j] =<span class="st">  </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(A[,j]<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb16-25" data-line-number="25">        <span class="cf">if</span> (r[j,j]<span class="op">==</span><span class="dv">0</span>) <span class="cf">break</span> <span class="co"># on linear dependency</span></a>
<a class="sourceLine" id="cb16-26" data-line-number="26">        q[,j] =<span class="st"> </span>A[,j] <span class="op">/</span><span class="st"> </span>r[j,j]</a>
<a class="sourceLine" id="cb16-27" data-line-number="27">        <span class="cf">for</span> (k <span class="cf">in</span> j<span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb16-28" data-line-number="28">            r[j,k] =<span class="st">   </span><span class="kw">t</span>(q[,j])  <span class="op">%*%</span><span class="st"> </span>A[,k]</a>
<a class="sourceLine" id="cb16-29" data-line-number="29">            A[,k] =<span class="st"> </span>A[,k] <span class="op">-</span><span class="st">  </span>r[j,k] <span class="op">*</span><span class="st"> </span>q[,j]</a>
<a class="sourceLine" id="cb16-30" data-line-number="30">        }</a>
<a class="sourceLine" id="cb16-31" data-line-number="31">  }</a>
<a class="sourceLine" id="cb16-32" data-line-number="32">  <span class="kw">list</span>(<span class="st">&quot;A&quot;</span> =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>r, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>r)</a>
<a class="sourceLine" id="cb16-33" data-line-number="33">}</a>
<a class="sourceLine" id="cb16-34" data-line-number="34">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb16-35" data-line-number="35"><span class="kw">qr_decomposition_by_mgs</span>(A)</a></code></pre></div>

<p>There are other variations of the <strong>Modified Gram-Schmidt</strong> algorithm; however, the main giveaway for <strong>MGS</strong> is that it handles round-off errors seen in <strong>CGS</strong>.</p>
<p>Now, let us cover the next <strong>QR decomposition</strong> algorithm - the <strong>Householder</strong> algorithm.</p>
<p><strong>Householder (Reflection) algorithm:</strong> </p>
<p>The idea is to decompose a matrix, <span class="math inline">\(A\)</span>, into the following form:</p>
<p><span class="math display" id="eq:eqnnumber117">\[\begin{align}
A = Q_A \left( \begin{array}{c} R_A \\ O \end{array} \right) \tag{2.50}
\end{align}\]</span></p>
<p>where, similar to <strong>L</strong> in <strong>LU factorization</strong> by <strong>Gaussian Elimination (GE)</strong>, <strong>Q</strong> is also a <strong>dot product</strong> accumulation of iterations in <strong>Householder</strong> algorithm.
Recall again <span class="math inline">\(L_A\)</span> when performing <strong>LU factorization using GE</strong>:</p>
<p><span class="math display">\[
L_A = (l_m \cdot l_{m-1} \cdot \ ...\ \cdot l_2 \cdot l_1)^{-1} = l_1^{-1} \cdot l_2^{-1} \cdot\ ...\  \cdot\ l_{m-1}^{-1} \cdot  l_{m}^{-1} 
\]</span>
Similarly, in the <strong>Householder</strong> algorithm, we generate a set of <span class="math inline">\(H_i\)</span> (Hessenberg matrices) to compose <span class="math inline">\(Q_A\)</span> eventually.</p>
<p><span class="math display">\[
Q_A = ( H_m \cdotp H_{m-1}\ \cdotp\ . . .\ \cdotp\ H_2 \cdotp H_1)^{-1} = H_1 \cdotp H_2\ \cdotp\ . . .\ \cdotp\ H_{m-1} \cdotp\ H_m 
\]</span></p>
<p>To arrive at a set of <span class="math inline">\(H_i\)</span>, we use the following reflection formula <span class="citation">(Driscoll T. A. <a href="bibliography.html#ref-ref536w">2020</a>)</span>:</p>
<p><span class="math display" id="eq:eqnnumber118">\[\begin{align}
H_i = I - \frac{2v_iv_i^T}{v_i^Tv_i} \tag{2.51}
\end{align}\]</span></p>
<p>where <span class="math inline">\(v_i\)</span> are reflectors of <span class="math inline">\(a_i\)</span>:</p>
<p><span class="math display">\[
v_i = a_i - sign(a_{ii})|\|a_i\|_{L2}e_i 
\]</span></p>
<p>We also can see an image of <strong>orthogonal reflection</strong> in Figure <a href="2.20-matrix-factorization.html#fig:orthoprojectreflect">2.24</a> - right side.</p>
<p>Let us use the same sample matrix, <span class="math inline">\(A\)</span>, as before and step through the <strong>Householder</strong> algorithm:</p>
<p><span class="math display">\[
A^{(1)} = A = 
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3
\end{array}
\right]_{3x3} = 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{a1} 
\left[\begin{array}{r} 5 \\4 \\ 3 \end{array}\right]_{a2} 
\left[\begin{array}{r} 5 \\3 \\ 3 \end{array}\right]_{a3}, \ \ \ \ where\ n=3
\]</span></p>
<p><strong>First</strong>, solve for <span class="math inline">\(H_1\)</span> by computing for <span class="math inline">\(RF_1\)</span> - reflection formula:</p>

<p><span class="math display">\[\begin{align*}
a_1 {}&amp; = \|A_{1:3,1}^{(1)}\|_{L2} = \sqrt{14} \\
\\
v_1 &amp;= a_1 - sign(A_{11}^{(1)})(a_1)(e_1) \\
&amp;= 
\left[\begin{array}{r} 1 \\2 \\ 3 \end{array}\right]_{1:3,1} - (\sqrt{14})
\left[\begin{array}{r} 1 \\ 0 \\ 0 \end{array}\right]_{e1}
= 
\left[\begin{array}{r} -2.741657 \\ 2 \\ 3 \end{array}\right]_{v_1} \\
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
RF_1 &amp;= I - 2\left(\frac{v_1v_1^T}{v_1^Tv_1}\right)  \\
&amp;= 
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1
\end{array}
\right]_I - 2 \left(\frac{
&lt; -2.741657 , 2 , 3 &gt;&lt; -2.741657 , 2 , 3 &gt;^T
}{
&lt; -2.741657 , 2 , 3 &gt;^T&lt; -2.741657 , 2 , 3 &gt;
}\right)
\\
\\
&amp;= \left[
\begin{array}{rrr}
0.2672614 &amp; 0.5345225 &amp; 0.8017837 \\
0.5345225 &amp; 0.6100734 &amp; -0.5848899 \\
0.8017837 &amp; -0.5848899 &amp; 0.1226652
\end{array}
\right]_{RF_1} \\
\\
H_1 &amp;= RF_1  \\
A^{(2)} &amp;= H_1A^{(1)} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0 &amp; 3.358236 &amp; 3.968310 \\
0 &amp; 2.037355 &amp; 1.452465
\end{array}
\right]_{A_2}
\end{align*}\]</span>
</p>
<p><strong>Second</strong>, solve for <span class="math inline">\(H_2\)</span> by computing for <span class="math inline">\(RF_2\)</span>:</p>

<p><span class="math display">\[\begin{align*}
a_2 {}&amp; = \|A_{2:3,2}^{(2)}\|_{L2} = 3.927921 \\
\\
v_2 &amp;= a_2 +  sign(A_{22}^{(2)})(a_2)(e_1) \\
&amp;= 
\left[\begin{array}{r} 3.358236 \\ 2.037355  \end{array}\right]_{2:3,2} - (3.927921)
\left[\begin{array}{r} 1 \\ 0  \end{array}\right]_{e1}
= 
\left[\begin{array}{r} -0.569685 \\ 2.037355 \end{array}\right]_{v_2} 
\end{align*}\]</span>
<span class="math display">\[\begin{align*}
RF_2 &amp;= I - 2\left(\frac{v_2v_2^T}{v_2^Tv_2}\right)  \\
&amp;= \left[
\begin{array}{rrr}
1 &amp; 0 \\
0 &amp; 1 \\
\end{array}
\right]_I - 2 \left(\frac{
&lt; -0.569685 , 2.037355 &gt;&lt; -0.569685 , 2.037355 &gt;^T
}{
&lt; -0.569685 , 2.037355 &gt;^T&lt; -0.569685 , 2.037355 &gt;
}\right) 
\\
\\
&amp;= \left[
\begin{array}{rrr}
0.8549653 &amp; 0.5186852 \\
0.5186852 &amp; -0.8549653 
\end{array}
\right]_{RF_2} \\
\\
H_2 &amp;= \left[
\begin{array}{rrr}
1 &amp; . \\
. &amp; RF_2 \\
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 0.8549653 &amp; 0.5186852 \\
0 &amp; 0.5186852 &amp; -0.8549653 
\end{array}
\right]_{H_2} \\
\\
A^{(3)} &amp;= H_2A^{(2)} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{A^{(3)}}
\end{align*}\]</span>
</p>
<p><strong>Lastly</strong>, obtain <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= A^{(3)} = H_2H_1A^{(0)},\ \ \ \ \  Q = H_1H_2
\\
\\
R &amp;= \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{R} \ \ \ \ \ \  \\
\\
Q &amp;= \left[
\begin{array}{rrr}
0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\
0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\
0.8017837 &amp; -0.4364360 &amp; -0.4082482 
\end{array}
\right]_{Q}
\end{align*}\]</span></p>
<p>Given a matrix, <span class="math inline">\(A_{mxn}\)</span>, let us formulate the algorithm:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{l}
Q = diag(max(m,n)) \\
R = A \\
loop\ j = 1:n \\
\ \ \ \ blk = m - j + 1 \\
\ \ \ \ if\ (blk &lt;= 1)\ break \\
\ \ \ \ e1 = zeros(blk); e1[1] = 1 \\
\ \ \ \ a = \|R_{j:m,j}\|_{L2} \\
\ \ \ \ v = R_{j:m,j} - sign(R_{jj}) \times a \times e1  \\
\ \ \ \ I = diag(blk) \\
\ \ \ \ H&#39; = I - constant(2 / ( v^T \cdotp v )) \times  ( v \cdotp v^T ) \\
\ \ \ \ H = diag(max(m,n)) \\
\ \ \ \ H_{jm,jm} = H&#39; \\
\ \ \ \ R = HR \\
\ \ \ \ Q = QH \\
end\ loop\\
\end{array}
\end{align*}\]</span></p>
<p>Here is a naive implementation of the <strong>Householder</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">qr_decomposition_by_householder &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb17-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb17-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb17-4" data-line-number="4">  Q =<span class="st"> </span><span class="kw">diag</span>(m)</a>
<a class="sourceLine" id="cb17-5" data-line-number="5">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb17-6" data-line-number="6">      blk =<span class="st"> </span>m <span class="op">-</span><span class="st"> </span>j <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb17-7" data-line-number="7">      <span class="cf">if</span> (blk <span class="op">&lt;=</span><span class="st"> </span><span class="dv">1</span>) <span class="cf">break</span></a>
<a class="sourceLine" id="cb17-8" data-line-number="8">      e1 =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, blk); e1[<span class="dv">1</span>] =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb17-9" data-line-number="9">      a =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(A[j<span class="op">:</span>m,j]<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb17-10" data-line-number="10">      v =<span class="st"> </span>A[j<span class="op">:</span>m,j] <span class="op">-</span><span class="st"> </span><span class="kw">sign</span>(A[j,j])  <span class="op">*</span><span class="st"> </span>a <span class="op">*</span><span class="st"> </span>e1</a>
<a class="sourceLine" id="cb17-11" data-line-number="11">      I =<span class="st"> </span><span class="kw">diag</span>(blk)</a>
<a class="sourceLine" id="cb17-12" data-line-number="12">      <span class="co"># c() converts from 1x1 2D to 1D (constant)</span></a>
<a class="sourceLine" id="cb17-13" data-line-number="13">      H_ =<span class="st">  </span>I <span class="op">-</span><span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span><span class="op">/</span>(<span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>(v))) <span class="op">*</span><span class="st"> </span>(v <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(v)) </a>
<a class="sourceLine" id="cb17-14" data-line-number="14">      H =<span class="st"> </span><span class="kw">diag</span>(m) </a>
<a class="sourceLine" id="cb17-15" data-line-number="15">      H[j<span class="op">:</span>m,j<span class="op">:</span>m] =<span class="st"> </span>H_</a>
<a class="sourceLine" id="cb17-16" data-line-number="16">      A =<span class="st"> </span>H <span class="op">%*%</span><span class="st"> </span>A </a>
<a class="sourceLine" id="cb17-17" data-line-number="17">      Q =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>H</a>
<a class="sourceLine" id="cb17-18" data-line-number="18">  }  </a>
<a class="sourceLine" id="cb17-19" data-line-number="19">  <span class="kw">list</span>(<span class="st">&quot;Matrix&quot;</span>=<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>A, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>Q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>A)</a>
<a class="sourceLine" id="cb17-20" data-line-number="20">}</a>
<a class="sourceLine" id="cb17-21" data-line-number="21">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb17-22" data-line-number="22"><span class="kw">qr_decomposition_by_householder</span>(A)</a></code></pre></div>

<p><strong>Givenâs rotation algorithm:</strong> </p>
<p>The idea is to decompose a matrix, A, into the following form:</p>
<p><span class="math display" id="eq:eqnnumber119">\[\begin{align}
A = QR \tag{2.52}
\end{align}\]</span></p>
<p>For the algorithm to work, we need a transformation matrix that rotates a given 2x1 vector by the angle, <span class="math inline">\(\theta\)</span>, effectively reducing the second item to zero.</p>
<p><span class="math display">\[
\left[
\begin{array}{rr}
cos\ \theta &amp; -sin\ \theta \\
sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{rotate}^T
\left[ \begin{array}{rr} v_1 \\ v_2 \end{array} \right]_V =
\left[ \begin{array}{rr} r \\ 0 \end{array} \right]_{transformed}
\ \ \ \  where\ r = \sqrt{v_1^2 + v_2^2}
\]</span>
We use the below formula to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(\sin \theta\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{v_1}{\sqrt{v_1^2 + v_2^2}}\ \ \ \ \ \ \ sin\ \theta = \frac{v_2}{\sqrt{v_1^2 + v_2^2}}
\]</span></p>
<p>To illustrate, let us use our system of equations to solve for <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[
\left(\begin{array}{lll}  
1x_1 + 5x_2 + 5x_3 = 6 \\
2x_1 + 4x_2 + 5x_3 = 5 \\
3x_1 + 3x_2 + 3x_3 = 6 \\
 \end{array}\right) \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 \\
\end{array}
\right]
\left[ \begin{array}{rrr} x_1 \\ x_2 \\ x_3 \end{array} \right] =
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right] 
\]</span></p>
<p>Our approach would be bottom-up, left-right, extracting each vector at a time and annihilating its lower element, effectively rotating the affected rows, until we form an upper-triangular matrix:</p>
<p><span class="math display">\[\begin{align*}
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
x_{21} &amp; x_{22} &amp; x_{23} \\
x_{31} &amp; x_{32} &amp; x_{33} \\
\end{array}
\right]_A {}&amp;\rightarrow \\ 
\\
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
\mathbf{x_{21}} &amp; \mathbf{x_{22}} &amp; \mathbf{x_{23}} \\
0 &amp; \mathbf{x_{32}} &amp; \mathbf{x_{33}} \\
\end{array}
\right]_{G_1} \rightarrow 
\left[
\begin{array}{lll}
\mathbf{x_{11}} &amp; \mathbf{x_{12}} &amp; \mathbf{x_{13}} \\
0 &amp; \mathbf{x_{22}} &amp; \mathbf{x_{23}} \\
0 &amp; x_{32} &amp; x_{33} \\
\end{array}
\right]_{G_2} &amp;\rightarrow
\left[
\begin{array}{lll}
x_{11} &amp; x_{12} &amp; x_{13} \\
0 &amp; \mathbf{x_{22}} &amp; \mathbf{x_{23}} \\
0 &amp; 0 &amp; \mathbf{x_{33}}\\
\end{array}
\right]_{G3}
\end{align*}\]</span></p>
<p><strong>First</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{21} \\ x_{31} \end{array} \right] = \left[ \begin{array}{rrr} 2 \\ 3 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{31}\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{2}{\sqrt{2^2 + 3^2}} = 0.5547002\ \ \ \ \ \ \
sin\ \theta = \frac{3}{\sqrt{2^2 + 3^2}} = 0.8320503
\]</span></p>
<p>We then improvise a transformation matrix (a givenâs matrix), <span class="math inline">\(G_1\)</span> to transform matrix, A.</p>
<p><span class="math display">\[\begin{align*}
G_{1} {}&amp;=
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; cos\ \theta &amp; -sin\ \theta \\
0 &amp; sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{utility}^T \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
0 &amp; 0.5547002 &amp; -0.8320503  \\
0 &amp; 0.8320503 &amp; 0.5547002 \\
\end{array}
\right]_{rotate}^T \\
\\
A_{1} &amp;= (G_{1})^T\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 \\ 
\end{array}
\right]_A =
\left[ \begin{array}{rrr} 6 \\ 5 \\ 6 \end{array} \right]_y
\end{align*}\]</span></p>
<p>We get our first transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[
A_{1} = \left[
\begin{array}{rrr}
1.000000 &amp; 5.000000 &amp; 5.000000 \\
3.605551 &amp; 4.714952 &amp; 5.269652 \\
0.000000 &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_1} =
\left[ \begin{array}{rrr} 6.0000000 \\ 7.7658028 \\ -0.8320503 \end{array} \right]_{y_1}
\]</span></p>
<p><strong>Second</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{11} \\ x_{21} \end{array} \right] = \left[ \begin{array}{rrr} 1.000000 \\ 3.605551 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{21}\)</span>:</p>
<p><span class="math display">\[
cos\ \theta = \frac{1}{\sqrt{1^2 + 3.605551^2}} = 0.2672613\ \ \ \ \ \ \
sin\ \theta = \frac{3.605551}{\sqrt{1^2 + 3.605551^2}} = 0.9636241
\]</span></p>
<p>We then improvise another transformation matrix, <span class="math inline">\(G_2\)</span>, for the transformed matrix, <span class="math inline">\(A_1\)</span>.</p>
<p><span class="math display">\[\begin{align*}
G_{2} {}&amp;= \left[
\begin{array}{rrr}
cos\ \theta &amp; -sin\ \theta &amp; 0 \\
sin\ \theta &amp; cos\ \theta &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\end{array}
\right]_{utility} \rightarrow
\left[
\begin{array}{rrr}
0.2672613 &amp; -0.9636241 &amp; 0\\
0.9636241 &amp; 0.2672613 &amp; 0\\
0 &amp; 0 &amp; 1 \\
\end{array}
\right]_{rotate} \\
\\
A_{2} &amp;= (G_{2})^T
\left[
\begin{array}{rrr}
1.000000 &amp; 5.000000 &amp; 5.000000 \\
3.605551 &amp; 4.714952 &amp; 5.269652 \\
0.000000 &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_1} =
\left[ \begin{array}{rrr} 6.0000000 \\ 7.7658028 \\ -0.8320503 \end{array} \right]_{y_1}
\end{align*}\]</span></p>
<p>We get our second transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[
A_{2} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0.000000  &amp; -3.557996 &amp; -3.409746 \\
0.000000  &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_2} =
\left[ \begin{array}{rrr} 9.0868825 \\ -3.7062460 \\ -0.8320503 \end{array} \right]_{y_2}
\]</span></p>
<p><strong>Third</strong>, we extract the vector, <span class="math inline">\(\left[ \begin{array}{rrr} x_{22} \\ x_{23} \end{array} \right] = \left[ \begin{array}{rrr} -3.557996 \\ -1.664101 \end{array} \right]\)</span>, to compute for <span class="math inline">\(cos\ \theta\)</span> and <span class="math inline">\(sin\ \theta\)</span> and zero out <span class="math inline">\(x_{23}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
cos\ \theta {}&amp;= \frac{-3.557996}{\sqrt{-3.557996^2 + -1.664101^2}} = -0.9058216\ \ \ \ \ \ \ \\
\\
sin\ \theta &amp;= \frac{-1.664101}{\sqrt{-3.557996^2 + -1.664101^2}} = -0.4236594
\end{align*}\]</span></p>
<p>We then improvise another transformation matrix, <span class="math inline">\(G_3\)</span>, for the transformed matrix, <span class="math inline">\(A_2\)</span>.</p>
<p><span class="math display">\[\begin{align*}
G_{3} {}&amp;= \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0\\
0 &amp; cos\ \theta &amp; -sin\ \theta \\
0 &amp; sin\ \theta &amp; cos\ \theta \\
\end{array}
\right]_{utility} \rightarrow
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0  \\
0 &amp; -0.9058216 &amp; 0.4236594 \\
0 &amp; -0.4236594 &amp; -0.9058216 \\
\end{array}
\right]_{rotate} \\
\\
A_{3} &amp;= (G_3)^T\left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0.000000 &amp; -3.557996 &amp; -3.409746 \\
0.000000 &amp; -1.664101 &amp; -2.496151 \\ 
\end{array}
\right]_{A_2} =
\left[ \begin{array}{rrr} 9.0868825 \\ -3.7062460 \\ -0.8320503 \end{array} \right]_{y_2}
\end{align*}\]</span></p>
<p>We get our third transformation (annihilate lower element by rotation):</p>
<p><span class="math display">\[\begin{align*}
A_{3} = \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.414270 \\
0.000000  &amp; 3.927922 &amp; 4.1461398 \\
0.000000  &amp; 0.000000  &amp; 0.8164963 \\ 
\end{array}
\right]_{A_3} =
\left[ \begin{array}{rrr} 9.0868825 \\ 3.7097037 \\ -0.8164968 \end{array} \right]_{y_3}
\end{align*}\]</span></p>
<p><strong>Lastly</strong>, obtain <span class="math inline">\(Q\)</span> and <span class="math inline">\(R\)</span>:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= G_3^TG_2^TG_1^TA,\ \ \ \ \  Q = G_1G_2G_3 
\\
\\
R &amp;= \left[
\begin{array}{rrr}
3.741657 &amp; 5.879748 &amp; 6.4142703 \\
0 &amp; 3.927921 &amp; 4.1461394 \\
0 &amp; 0 &amp; 0.8164965 
\end{array}
\right]_{R} \ \ \ \ \ \  \\
\\
Q &amp;= \left[
\begin{array}{rrr}
0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\
0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\
0.8017837 &amp; -0.4364360 &amp; -0.4082482 
\end{array}
\right]_{Q}
\end{align*}\]</span></p>
<p>Given a matrix, <span class="math inline">\(A_{mxn}\)</span>, let us formulate the algorithm:</p>

<p><span class="math display">\[
\begin{array}{l}
function\ givens(v_1, v_2) \\
\ \ \ \ r = \sqrt{v_1^2 + v_2^2} \\
\ \ \ \ cos = v_1 / r \\
\ \ \ \ sin = v_2 / r \\
\ \ \ \ list&lt;cos, sin&gt; \\
end \\
\\
function\ rotate\_tool(&lt;c,s&gt;, k) \\
\ \ \ \ G = diag(m) \\
\ \ \ \ G_{k-1,k-1} = c \\
\ \ \ \ G_{k-1,k} = -s \\
\ \ \ \ G_{k,k-1} = c \\
\ \ \ \ G_{k,k} = s \\
end \\
\end{array} 
\left|
\begin{array}{l}
Q = diag(m) \\
loop\ j = 1:n \\
\ \ \ \ loop\ k = m:j \\
\ \ \ \ \ \ \ \ if\ j\ &lt; k: \\ 
\ \ \ \ \ \ \ \ \ \ \ \ &lt;c,s&gt; = givens(A_{k-1,j}, A{k,j}) \\
\ \ \ \ \ \ \ \ \ \ \ \ if\ NaN(&lt;c,s&gt;)\ next\ iterate \\
\ \ \ \ \ \ \ \ \ \ \ \ G = rotate\_tool(&lt;c,s&gt;,k)  \\
\ \ \ \ \ \ \ \ \ \ \ \ A = G^TA \leftarrow \{annihilate\ lower\ element\}\\
\ \ \ \ \ \ \ \ \ \ \ \ Q = QG \\
\ \ \ \ end\ loop \\
end\ loop\\
\end{array}
\right.
\]</span>
</p>
<p>Here is a naive implementation of the <strong>Givens</strong> algorithm in R code:</p>

<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">qr_decomposition_by_givens &lt;-<span class="st"> </span><span class="cf">function</span>(A) {</a>
<a class="sourceLine" id="cb18-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">ncol</span>(A)</a>
<a class="sourceLine" id="cb18-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb18-4" data-line-number="4">  givens &lt;-<span class="st"> </span><span class="cf">function</span>(v1, v2) {</a>
<a class="sourceLine" id="cb18-5" data-line-number="5">      r =<span class="st"> </span><span class="kw">sqrt</span>(v1<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span>v2<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb18-6" data-line-number="6">      cos =<span class="st"> </span>v1 <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb18-7" data-line-number="7">      sin =<span class="st"> </span>v2 <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb18-8" data-line-number="8">      <span class="kw">list</span>(<span class="st">&quot;cos&quot;</span>=cos,<span class="st">&quot;sin&quot;</span>=sin)</a>
<a class="sourceLine" id="cb18-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb18-10" data-line-number="10">  rotate_tool &lt;-<span class="st"> </span><span class="cf">function</span>(cs, k) {</a>
<a class="sourceLine" id="cb18-11" data-line-number="11">      G =<span class="st"> </span><span class="kw">diag</span>(m)  </a>
<a class="sourceLine" id="cb18-12" data-line-number="12">      G[k<span class="dv">-1</span>,k<span class="dv">-1</span>] =<span class="st"> </span>cs<span class="op">$</span>cos</a>
<a class="sourceLine" id="cb18-13" data-line-number="13">      G[k<span class="dv">-1</span>,k] =<span class="st"> </span><span class="op">-</span>cs<span class="op">$</span>sin</a>
<a class="sourceLine" id="cb18-14" data-line-number="14">      G[k,k<span class="dv">-1</span>] =<span class="st"> </span>cs<span class="op">$</span>sin</a>
<a class="sourceLine" id="cb18-15" data-line-number="15">      G[k,k] =<span class="st"> </span>cs<span class="op">$</span>cos</a>
<a class="sourceLine" id="cb18-16" data-line-number="16">      G</a>
<a class="sourceLine" id="cb18-17" data-line-number="17">  }</a>
<a class="sourceLine" id="cb18-18" data-line-number="18">  Q =<span class="st"> </span><span class="kw">diag</span>(m)  </a>
<a class="sourceLine" id="cb18-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb18-20" data-line-number="20">      <span class="cf">for</span> (k <span class="cf">in</span> m<span class="op">:</span>j) {</a>
<a class="sourceLine" id="cb18-21" data-line-number="21">          <span class="cf">if</span> (j <span class="op">&lt;</span><span class="st"> </span>k ) {</a>
<a class="sourceLine" id="cb18-22" data-line-number="22">              cs =<span class="st"> </span><span class="kw">givens</span>(A[k<span class="dv">-1</span>,j],A[k,j])</a>
<a class="sourceLine" id="cb18-23" data-line-number="23">              <span class="cf">if</span> (<span class="kw">is.nan</span>(cs<span class="op">$</span>cos) <span class="op">||</span><span class="st"> </span><span class="kw">is.nan</span>(cs<span class="op">$</span>sin)) <span class="cf">next</span></a>
<a class="sourceLine" id="cb18-24" data-line-number="24">              G =<span class="st"> </span><span class="kw">rotate_tool</span>(cs,k)</a>
<a class="sourceLine" id="cb18-25" data-line-number="25">              A =<span class="st"> </span><span class="kw">t</span>(G) <span class="op">%*%</span><span class="st"> </span>A</a>
<a class="sourceLine" id="cb18-26" data-line-number="26">              Q =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>G</a>
<a class="sourceLine" id="cb18-27" data-line-number="27">          }</a>
<a class="sourceLine" id="cb18-28" data-line-number="28">      }</a>
<a class="sourceLine" id="cb18-29" data-line-number="29">  }</a>
<a class="sourceLine" id="cb18-30" data-line-number="30">  <span class="kw">list</span>(<span class="st">&quot;Matrix&quot;</span> =<span class="st"> </span>Q <span class="op">%*%</span><span class="st"> </span>A, <span class="st">&quot;Q&quot;</span> =<span class="st"> </span>Q, <span class="st">&quot;R&quot;</span> =<span class="st"> </span>A)</a>
<a class="sourceLine" id="cb18-31" data-line-number="31">}</a>
<a class="sourceLine" id="cb18-32" data-line-number="32">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,  <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,  <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb18-33" data-line-number="33"><span class="kw">qr_decomposition_by_givens</span>(A)</a></code></pre></div>

<p>One recent evolution of Givens rotation is <strong>Generalized Givens Rotation (GGR)</strong> for <strong>QR decomposition</strong>. This topic is left for readers to follow and investigate.</p>
</div>
<div id="cholesky-factorization" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.5</span> Cholesky Factorization <a href="2.20-matrix-factorization.html#cholesky-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to decompose an <strong>invertible symmetric positive definite square matrix</strong> into its <span class="math inline">\(LL^T\)</span> form or <span class="math inline">\(LDL^T\)</span> form, if and only if <span class="math inline">\(U=L^T\)</span>, forming the equation below:</p>
<p><span class="math display" id="eq:eqnnumber120">\[\begin{align}
A = LL^T = LDL^T\ \ \ \ \ \ \ \ \ \  \text{where}\ L\ \text{is called Cholesky Factor} \tag{2.53}
\end{align}\]</span></p>
<p>That has the advantage over <strong>LU factorization</strong> if the <strong>square matrix</strong> is <strong>symmetric positive definite</strong> (a <strong>Hermitian</strong> matrix). We only work on one matrix instead of two, saving storage computationally.</p>
<p>To perform <strong>Cholesky factorization</strong> and get the <strong>lower-triangular matrix</strong>, we need to perform the <strong>Gaussian elimination</strong> method for <strong>LU factorization</strong>.</p>
<p>For example, using <strong>LU factorization by GE</strong>, we get the following:</p>
<p><span class="math display">\[
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}
\right]_A =
\left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}
\left[
\begin{array}{rrr}
1 &amp; 5 &amp; 5 \\
0 &amp; -6 &amp; -5 \\
0 &amp; 0 &amp; -2 
\end{array}
\right]_{U_A}
\]</span></p>
<p>We use <span class="math inline">\(L_A\)</span> and its transpose:</p>
<p><span class="math display">\[
A = LL^T = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0 \\
2 &amp; 1 &amp; 0 \\
3 &amp; 2 &amp; 1 
\end{array}
\right]_{L_A}\left[
\begin{array}{rrr}
1 &amp; 2 &amp; 3 \\
0 &amp; 1 &amp; 2 \\
0 &amp; 0 &amp; 1 
\end{array}
\right]_{L_A^T}
\]</span></p>
<p>To review the difference between <strong>L</strong> and <strong>U</strong> matrices for <strong>Doolittle</strong> and <strong>Cholesky</strong> decomposition [StanimirovÃ­c P. S. et al. <span class="citation">(<a href="bibliography.html#ref-ref508p">2011</a>)</span>, see <strong>LU Decomposition</strong> in the previous subsection.</p>
</div>
<div id="svd-factorization" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.6</span> SVD Factorization <a href="2.20-matrix-factorization.html#svd-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to decompose a matrix, A, into its <span class="math inline">\(U\Sigma V^T\)</span> form; where <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are both <strong>orthogonal matrices</strong>, and <span class="math inline">\(\Sigma\)</span> is a <strong>diagonal matrix</strong> of <strong>Eigenvalues</strong>, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber130">\[\begin{align}
A = U\Sigma V^T \tag{2.54}
\end{align}\]</span></p>
<p>We extract the <strong>singular positive real values</strong> of a matrix into <span class="math inline">\(\Sigma\)</span>; hence, the reason for the term <strong>singular value decomposition</strong>.</p>

<p><span class="math display">\[
A_{mxn} =
\left[
\begin{array}{rrrr}
u_{11} &amp; \ldots &amp; u_{1m} \\
u_{21} &amp; \ldots &amp; u_{2m} \\
\vdots &amp; \ddots &amp; \vdots \\
u_{m1} &amp; \ldots&amp; u_{mm}\\
\end{array}
\right]_{U_{mxm}}
\left[
\begin{array}{rrrr}
\sigma_{11} &amp; . &amp; . &amp; . \\
. &amp; \sigma_{22} &amp; . &amp; . \\
. &amp; . &amp; \sigma_{33} &amp; . \\
. &amp; . &amp; . &amp; \sigma_{mn}
\end{array}
\right]_{\Sigma_{mxn}}
\left[
\begin{array}{rrrr}
v_{11}  &amp; \ldots &amp; v_{1n} \\
v_{21}  &amp; \ldots &amp; v_{2n} \\
\vdots  &amp; \ddots &amp; \vdots \\
v_{n1}  &amp; \ldots&amp; v_{nn}\\
\end{array}
\right]_{V_{nxn}}^T
\]</span>
</p>
<p>Recall the <strong>Eigen equation</strong> (<a href="2.14-eigenvectors-and-eigenvalues.html#eq:eqnnumber58">(2.17)</a>), where <span class="math inline">\(v\)</span> is an <strong>Eigenvector</strong> and <span class="math inline">\(\lambda\)</span> is an <strong>Eigenvalue</strong>. We know that <span class="math inline">\(A\)</span> is a transformation matrix - it is a utility to scale the Eigenvector <span class="math inline">\(v\)</span>. We also know that <span class="math inline">\(\lambda\)</span> is a transformation scalar - it is a utility to scale the same Eigenvector <span class="math inline">\(v\)</span>. In essence, both matrices <span class="math inline">\(A\)</span> and scalar <span class="math inline">\(\lambda\)</span> fulfill the same purpose - to scale the Eigenvector <span class="math inline">\(v\)</span>. We can say that, based on the formula, we can derive an <strong>Eigen decomposition</strong> formula:</p>
<p><span class="math display" id="eq:eqnnumber140">\[\begin{align}
A = v \lambda v^{-1} = P \Lambda P^{-1} \tag{2.55}
\end{align}\]</span></p>
<p>Now, let us introduce related formula in the context of <strong>SVD</strong> decomposition:</p>
<p><span class="math display" id="eq:eqnnumber131">\[\begin{align}
A \cdotp v = \sigma \cdotp u,\ \ \ \ \ A^T \cdotp u = \sigma \cdotp v \tag{2.56}
\end{align}\]</span></p>
<p>We can think of matrix A as a transformation matrix such that vector <span class="math inline">\(v\)</span> gets transformed into vector <span class="math inline">\(u\)</span> by rotation; additionally, the transformed vector <span class="math inline">\(u\)</span> is scaled by <span class="math inline">\(\sigma\)</span>. Of course, we do not have to deal with just one vector <span class="math inline">\(v\)</span> or one vector <span class="math inline">\(u\)</span>. We can deal with a matrix <span class="math inline">\(V\)</span>, a matrix <span class="math inline">\(U\)</span>, and a matrix <span class="math inline">\(\Sigma\)</span> where:</p>
<p><span class="math display">\[\begin{align*}
U_m &amp;= \{u_1, u_2, u_3,..., u_m\} \rightarrow \text{an orthogonal matrix of m column vectors} \\
\Sigma_m &amp;= \{\sigma_1, \sigma_2, \sigma_3, ..., \sigma_n\} \rightarrow \text{a diagonal matrix of n positive diagonal entries} \\
V_n {}&amp;= \{v_1, v_2, v_3, ..., v_n\} \rightarrow \text{an orthogonal matrix of n column vectors} \\
\end{align*}\]</span></p>
<p>With that, we can show the formula as:</p>
<p><span class="math display" id="eq:eqnnumber65">\[\begin{align}
A \cdotp V = U \cdotp \Sigma,\ \ \ \ \  A^T \cdotp U = V \cdotp \Sigma \tag{2.57}
\end{align}\]</span></p>
<p>And because <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> are orthogonal matrices, it goes to show that:</p>
<p><span class="math display" id="eq:eqnnumber66">\[\begin{align}
V^T = V^{-1},
\ \ \ \ \ \ \ \ V^TV = VV^T = I,
\ \ \ \ \ \ \ \ U^T = U^{-1},
\ \ \ \ \ \ \ \ U^TU = UU^T = I \tag{2.58}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display" id="eq:eqnnumber67">\[\begin{align}
A = U\Sigma V^T,
\ \ \ \ \ \ \ \ \ \ \ \ A^{T} = V\Sigma^TU^T,
\ \ \ \ \ \ \ \ \ \ \ \ A^{-1} = V\Sigma^{-1}U^T  \tag{2.59}
\end{align}\]</span></p>
<p>So how do we solve for <span class="math inline">\(U\)</span>, <span class="math inline">\(\Sigma\)</span>, and <span class="math inline">\(V\)</span>.</p>
<p><strong>First</strong>, we perform some mathematical transformations by adding <span class="math inline">\(A^T\)</span> to both sides of the equation:</p>

<p><span class="math display" id="eq:equate1040012" id="eq:equate1040011" id="eq:equate1040010" id="eq:equate1040009">\[\begin{align}
A^TA &amp;= A^{(1)} = A^T(U\Sigma V^T) = (U\Sigma V^T)^T(U\Sigma V^T) \tag{2.60} \\
A^TA &amp;= A^{(1)} =  (V\Sigma^TU^T)(U\Sigma V^T)  = V\Sigma^TU^TU\Sigma V^T  \tag{2.61} \\
A^TA &amp;= A^{(1)} =  V\Sigma^T\Sigma V^T &amp; \{V^TV = I\}  \tag{2.62} \\
A^TA &amp;= A^{(1)} =  V\Sigma^2V^T &amp; \{\Sigma^T\Sigma = \Sigma^2\} \tag{2.63} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1040016" id="eq:equate1040015" id="eq:equate1040014" id="eq:equate1040013">\[\begin{align}
AA^T &amp;= A^{(2)} =  (U\Sigma V^T)A^T = (U\Sigma V^T)(U\Sigma V^T)^T \tag{2.64} \\
AA^T &amp;= A^{(2)} =  (U\Sigma V^T)(V\Sigma^TU^T) = U\Sigma V^TV\Sigma^TU^T   \tag{2.65} \\
AA^T &amp;= A^{(2)} =  U\Sigma \Sigma^TU^T &amp; \{U^TU = I\} \tag{2.66} \\
AA^T &amp;= A^{(2)} =  U\Sigma^2U^T &amp; \{\Sigma\Sigma^T = \Sigma^2\} \tag{2.67} 
\end{align}\]</span>
</p>
<p><strong>Second</strong>, we transform the formulas into an <strong>Eigen equation</strong> by adding <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span> to both sides of the equation, respectively:</p>

<p><span class="math display" id="eq:equate1040019" id="eq:equate1040018" id="eq:equate1040017">\[\begin{align}
A^{(1)}V &amp;= V\Sigma^2V^TV &amp; let\ A^TA = A^{(1)}  \tag{2.68} \\
A^{(1)}V &amp;= V\Sigma^2 &amp; \{V^TV = I\}  \tag{2.69} \\
A^{(1)}V &amp;= \Sigma^2V &amp; \{A \cdot v = \lambda \cdot v, where\ \lambda = \Sigma^2 \}  \tag{2.70} 
\end{align}\]</span></p>
<p><span class="math display" id="eq:equate1040022" id="eq:equate1040021" id="eq:equate1040020">\[\begin{align}
A^{(2)}U &amp;= U\Sigma^2U^TU &amp; let\ AA^T = A^{(2)}  \tag{2.71} \\
A^{(2)}U &amp;= U\Sigma^2 &amp; \{U^TU = I\}  \tag{2.72} \\
A^{(2)}U &amp;= \Sigma^2U &amp; \{A \cdot u = \lambda \cdot u, where\ \lambda = \Sigma^2 \} \tag{2.73} 
\end{align}\]</span>
</p>
<p><strong>Third</strong>, recall the derived equations below from <strong>Eigen Equation</strong> (<a href="2.14-eigenvectors-and-eigenvalues.html#eq:eqnnumber58">(2.17)</a>) and (<a href="2.14-eigenvectors-and-eigenvalues.html#eq:eqnnumber60">(2.19)</a>):</p>
<p><span class="math display">\[
(A - \lambda I)v = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A v = \lambda v
\]</span></p>
<p>Equivalently, we get:</p>
<p><span class="math display" id="eq:equate1040024" id="eq:equate1040023">\[\begin{align}
(A^{(1)} - \Sigma^2 I)V = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A^{(1)} V = \Sigma^2 V  \tag{2.74} \\
(A^{(2)} - \Sigma^2 I)U = 0 \ \ \ \ \ \ \ \ \ \leftarrow \ \ \ A^{(2)} U = \Sigma^2 U  \tag{2.75} 
\end{align}\]</span></p>
<p>We solve for <span class="math inline">\(\Sigma\)</span> first by using the below <strong>characteristic equations</strong>:</p>
<p><span class="math display" id="eq:equate1040026" id="eq:equate1040025">\[\begin{align}
det(A^{(1)} - \Sigma^2 I) = |A^{(1)} - \Sigma^2 I| = 0  \tag{2.76} \\
det(A^{(2)} - \Sigma^2 I) = |A^{(2)} - \Sigma^2 I| = 0  \tag{2.77} 
\end{align}\]</span></p>
<p>One highlight to note is that <span class="math inline">\(\Sigma\)</span> or <span class="math inline">\(\sigma\)</span> is in a positive decreasing order:</p>
<p><span class="math display">\[
\sigma^1 \geq \sigma^2 \geq \sigma^3 \geq \  ... \ \geq \sigma^{n-1} \geq q^{n} \geq 0 
\]</span></p>
<p>Once we get the <span class="math inline">\(\Sigma\)</span>, we then plug into the equations below to get <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span>:</p>
<p><span class="math display" id="eq:equate1040028" id="eq:equate1040027">\[\begin{align}
(A^{(1)} - \Sigma^2 I)V = 0  \tag{2.78} \\
(A^{(2)} - \Sigma^2 I)U = 0  \tag{2.79} 
\end{align}\]</span></p>
<p>From there, in the context of a linear system of equations, we can plug those matrices, <span class="math inline">\(U\)</span>, <span class="math inline">\(V\)</span>, and <span class="math inline">\(\Sigma\)</span> into the equation to solve for <span class="math inline">\(x\)</span>:</p>
<p><span class="math display" id="eq:eqnnumber68">\[\begin{align}
Ax = y \rightarrow\ \ \ \ \ \ \ x = A^{-1}y \rightarrow\ \ \ \ \ \ x = V\Sigma^{-1}U^Ty \tag{2.80}
\end{align}\]</span></p>
<p>Let us use our sample matrix A to illustrate <strong>SVD</strong> decomposition.</p>
<p><strong>First</strong>, let us compose for <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span>:</p>

<p><span class="math display">\[
A = \left[\begin{array}{lll}
1 &amp; 5 &amp; 5 \\
2 &amp; 4 &amp; 5 \\
3 &amp; 3 &amp; 3 
\end{array}\right],\ \ \ 
A^TA = A^{(1)} = \left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\ 
22 &amp; 50 &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(1)}},
\]</span></p>
<p><span class="math display">\[
AA^T = A^{(2)} = \left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27  
\end{array}\right]_{A^{(2)}}
\]</span>
</p>
<p><strong>Second</strong>, let us solve for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p>Solving for <span class="math inline">\(\Sigma^2\)</span> based on <span class="math inline">\(A^{(1)}\)</span> and <span class="math inline">\(V\)</span>:</p>

<p><span class="math display" id="eq:equate1040029">\[\begin{align}
det(A^{(1)} - \Sigma^2 I) = |A^{(1)} - \Sigma^2 I| = 0  \tag{2.81} 
\end{align}\]</span></p>
<p><span class="math display">\[
\left|
\left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\
22 &amp; 50 &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(1)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right| = 
\left|
\left[\begin{array}{lll}
14-\Sigma^2 &amp; 22 &amp; 24 \\
22 &amp; 50 - \Sigma^2 &amp; 54 \\
24 &amp; 54 &amp; 59 - \Sigma^2 
\end{array}\right]
\right|
= 0
\]</span>
</p>
<p>Use the following <strong>characteristic equation</strong> of matrix <span class="math inline">\(A^{(1)}\)</span> for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p><span class="math display">\[
-\Sigma^6 + 123\Sigma^4-500\Sigma^2 + 144 = 0
\]</span></p>
<p>Therefore, by computing for the <strong>roots</strong>, we get our <span class="math inline">\(\Sigma^2\)</span> and <strong>Eigen values</strong>, <span class="math inline">\(\Sigma\)</span>, for <span class="math inline">\(A^{(1)}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
\Sigma^2 {}&amp;= &lt;118.80150,\ 3.88663,\ 0.31187&gt;\\
\Sigma &amp;= &lt; 10.89961, 1.971454, 0.5584532&gt;
\end{align*}\]</span></p>
<p>Let us also solve for <span class="math inline">\(\Sigma^2\)</span> based on <span class="math inline">\(A^{(2)}\)</span> and <span class="math inline">\(U\)</span> (as a validation):</p>
<p><span class="math display" id="eq:equate1040030">\[\begin{align}
det(A^{(2)} - \Sigma^2 I) = |A^{(2)} - \Sigma^2 I| = 0  \tag{2.82} 
\end{align}\]</span></p>

<p><span class="math display">\[
\left|
\left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right| = 
\left|
\left[\begin{array}{lll}
51-\Sigma^2 &amp; 47 &amp; 33 \\
47 &amp; 45 - \Sigma^2 &amp; 33 \\
33 &amp; 33 &amp; 27 - \Sigma^2 
\end{array}\right]
\right|
= 0
\]</span>
</p>
<p>That takes us to the following <strong>characteristic polynomial</strong> of matrix <span class="math inline">\(A^{(2)}\)</span>for <span class="math inline">\(\Sigma^2\)</span>:</p>
<p><span class="math display">\[
p(\Sigma) = -\Sigma^6 + 123\Sigma^4-500\Sigma^2 + 144
\]</span></p>
<p>Notice that the <strong>characteristic polynomial</strong> of the matrix <span class="math inline">\(A^{(2)}\)</span> is the same as that obtained from <span class="math inline">\(A^{(1)}\)</span>. That makes the two <strong>matrices</strong> considered to be as <strong>similar matrices</strong>.</p>
<p><strong>Third</strong>, let us derive the <strong>Eigenvectors</strong> for <span class="math inline">\(V^{(a)}\)</span> - using <strong>Gaussian Elimination</strong>:</p>

<p><span class="math display">\[
(A^{(1)} - \Sigma^2I)V^{(a)} = 
\left(
\left[\begin{array}{lll}
14 &amp; 22 &amp; 24 \\
22 &amp; 50  &amp; 54 \\
24 &amp; 54 &amp; 59 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right)V
= 0
\]</span></p>
<p><span class="math display">\[\begin{align*}
v_1 {}&amp;= \left[
\begin{array}{rrr}
-104.80 &amp; 22 &amp; 24 \\
22 &amp; -68.80 &amp; 54 \\
24 &amp; 54 &amp; -59.80
\end{array}
\right] = \left[
\begin{array}{rrr}
1  &amp; 0 &amp; -0.422097 \\
0 &amp; 1 &amp; -0.919837 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
0.422097  \\ 0.919837 \\ 1
\end{array}
\right]
\\
v_2 &amp;= \left[
\begin{array}{rrr}
10.11 &amp; 22 &amp; 24 \\
22 &amp; 46.11 &amp; 54 \\
24 &amp; 54 &amp; 55.11
\end{array}
\right] \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 4.608032 \\
0 &amp; 1 &amp; -1.027396 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-4.608032  \\ 1.027396 \\ 1
\end{array}
\right]
\\ 
v_3 &amp;= \left[
\begin{array}{rrr}
13.69 &amp; 22 &amp; 24 \\
22 &amp; 49.69 &amp; 54 \\
24 &amp; 54 &amp; 58.69
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0.023021 \\
0 &amp; 1 &amp; 1.076586 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-0.023021  \\ -1.076586 \\ 1
\end{array}
\right]
\end{align*}\]</span>
</p>
<p><strong>Fourth</strong>, let us derive the <strong>Eigenvectors</strong> for <span class="math inline">\(V^{(b)}\)</span> - using <strong>Gaussian Elimination</strong>:</p>

<p><span class="math display">\[
(A^{(2)} - \Sigma^2I)U^{(a)} = 
\left(
\left[\begin{array}{lll}
51 &amp; 47 &amp; 33 \\
47 &amp; 45 &amp; 33 \\
33 &amp; 33 &amp; 27 
\end{array}\right]_{A^{(2)}} - \Sigma^2 
\left[\begin{array}{lll}
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 
\end{array}\right]_I
\right)U
= 0
\]</span></p>
<p><span class="math display">\[\begin{align*}
u_1 {}&amp;= \left[
\begin{array}{rrr}
-67.80 &amp; 47 &amp; 33 \\
47 &amp; -73.80 &amp; 33 \\
33 &amp; 33 &amp; -91.80
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; -1.426354 \\
0 &amp; 1 &amp; -1.355510 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
1.426354 \\ 1.355510 \\ 1 
\end{array}
\right]
\\
u_2 &amp;= \left[
\begin{array}{rrr}
47.11 &amp; 47 &amp; 33 \\
47 &amp; 41.11 &amp; 33 \\
33 &amp; 33 &amp; 23.11
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; 0.714159 \\
0 &amp; 1 &amp; -0.013754 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
-0.714159 \\ 0.013754 \\ 1 
\end{array}
\right]
\\
u_3 &amp;= \left[
\begin{array}{rrr}
50.69 &amp; 47 &amp; 33 \\
47 &amp; 44.69 &amp; 33 \\
33 &amp; 33 &amp; 26.69
\end{array}
\right] = \left[
\begin{array}{rrr}
1 &amp; 0 &amp; -1.358512 \\
0 &amp; 1 &amp; 2.167243 \\
0 &amp; 0 &amp; 0
\end{array}
\right] = \left[
\begin{array}{rrr}
1.358512 \\ -2.167243 \\ 1 
\end{array}
\right]\\
\end{align*}\]</span>
</p>
<p><strong>Fifth</strong>, normalize each <strong>column vectors</strong> to convert to <strong>orthonormal</strong> matrices for <span class="math inline">\(V^{(a)}\)</span>, and <span class="math inline">\(U^{(a)}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
\Sigma {}&amp;= \left[\begin{array}{rrr} 10.89961 &amp; 1.971454 &amp; 0.5584532 \end{array}\right] \cdotp I \\
\\
U^{(a)} &amp;= \left[\begin{array}{rrr} 
1.426354 &amp; -0.714159 &amp; 1.358512 \\
1.355510 &amp; 0.013754 &amp; -2.167243 \\
1 &amp; 1 &amp; 1
\end{array}\right] \\
&amp;= \left[\begin{array}{rrr} 
0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\
0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\
0.4530552 &amp; 0.81373127 &amp; 0.3641182
\end{array}\right]
\\
\\ 
V^{(a)} &amp;= \left[\begin{array}{rrr} 
0.422097 &amp; -4.608032 &amp; -0.023021 \\
0.919837 &amp; 1.027396 &amp; -1.076586 \\
1 &amp; 1 &amp; 1
\end{array}\right] \\
&amp;= \left[\begin{array}{rrr} 
0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\
0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\
0.7028558 &amp; 0.2072144 &amp; 0.68048197
\end{array}\right]
\end{align*}\]</span>
</p>
<p><strong>Finally</strong>, let us get the actual <span class="math inline">\(V\)</span> and <span class="math inline">\(U\)</span>:</p>
<p>If we use <span class="math inline">\(V^{(a)}\)</span> for <span class="math inline">\(V\)</span>, then use the following formula to get <span class="math inline">\(U\)</span>:</p>
<p><span class="math display" id="eq:equate1040031">\[\begin{align}
u_i = \frac{A \cdot v_i}{\Sigma}, \tag{2.83} 
\end{align}\]</span></p>
<p>obtaining the following:</p>
<p><span class="math display">\[\begin{align*}
V^{(a)} = V {}&amp;= 
\left[\begin{array}{rrr} 
0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\
0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\
0.7028558 &amp; 0.2072144 &amp; 0.68048197
\end{array}\right] \\
\left[\begin{array}{r} u_1, u_2, ... u_n \end{array}\right] =
U &amp;= \left[\begin{array}{rrr} 
0.6462171 &amp; 0.58113351 &amp; -0.4946590 \\
0.6141209 &amp; -0.01119206 &amp; 0.7891327 \\
0.4530552 &amp; -0.81373127 &amp; -0.3641182
\end{array}\right]
\end{align*}\]</span></p>
<p>If we use <span class="math inline">\(U^{(a)}\)</span> for <span class="math inline">\(U\)</span>, then use the following formula to get <span class="math inline">\(V\)</span>:</p>
<p><span class="math display" id="eq:equate1040032">\[\begin{align}
v_i = \frac{A^T \cdot u_i}{\Sigma},  \tag{2.84} 
\end{align}\]</span></p>
<p>obtaining the following matrices:</p>
<p><span class="math display">\[\begin{align*}
U^{(a)} = U {}&amp;= 
\left[\begin{array}{rrr} 
0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\
0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\
0.4530552 &amp; 0.81373127 &amp; 0.3641182
\end{array}\right] \\
\left[\begin{array}{r} v_1, v_2, ... v_n \end{array}\right] = 
V &amp;= 
\left[\begin{array}{rrr} 
0.2966733 &amp; 0.9548505 &amp; 0.01566538 \\
0.6465127 &amp; -0.2128912 &amp; 0.73259736 \\
0.7028558 &amp; -0.2072144 &amp; -0.68048197
\end{array}\right]
\end{align*}\]</span></p>
<p>We can validate by using Equation <a href="2.20-matrix-factorization.html#eq:eqnnumber130">(2.54)</a>.</p>
<p>Here is an implementation of <strong>SVD</strong> in R code using Râs library (notice a third option produced by the library in addition to the two previous solutions we provided - also see the discrepancy in accuracy):</p>

<div class="sourceCode" id="cb19"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb19-1" data-line-number="1">A =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">5</span>,<span class="dv">5</span>,   <span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">5</span>,   <span class="dv">3</span>,<span class="dv">3</span>,<span class="dv">3</span>), <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb19-2" data-line-number="2">(<span class="dt">M =</span> <span class="kw">svd</span>(A))</a></code></pre></div>
<pre><code>## $d
## [1] 10.899610  1.971455  0.558449
## 
## $u
##            [,1]        [,2]       [,3]
## [1,] -0.6462172  0.58113330  0.4946589
## [2,] -0.6141207 -0.01119169 -0.7891327
## [3,] -0.4530552 -0.81373143  0.3641183
## 
## $v
##            [,1]       [,2]        [,3]
## [1,] -0.2966734 -0.9548505  0.01566518
## [2,] -0.6465125  0.2128913  0.73259732
## [3,] -0.7028559  0.2072144 -0.68048201</code></pre>

<p>Reconstructing the original matrix, we get the following:</p>

<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb21-1" data-line-number="1">M<span class="op">$</span>u <span class="op">%*%</span><span class="st"> </span><span class="kw">diag</span>(M<span class="op">$</span>d) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(M<span class="op">$</span>v)</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3]
## [1,]    1    5    5
## [2,]    2    4    5
## [3,]    3    3    3</code></pre>

<p>For further reading about <strong>SVD</strong>, we can reference Dan Kalmanâs paper <span class="citation">(<a href="bibliography.html#ref-ref545d">1996</a>)</span> and Kirk Bakerâs draft <span class="citation">(<a href="bibliography.html#ref-ref554k">2013</a>)</span>.</p>
<p>In the next chapter, we discuss <strong>iterative</strong> algorithms that can be used to compute SVD numerically for matrices with very high dimensions.</p>
<p>Apart from solving for a linear system of equations, there are other applications of <strong>SVD</strong> decomposition, to name a few: PCA (primary component analysis) and LSA (latent semantics analysis). Also, when dealing with large dimensionality features in machine learning, we may need to reduce duplicate features or features with multicollinearity using <strong>SVD</strong>.</p>
</div>
<div id="jordan-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.7</span> Jordan Decomposition <a href="2.20-matrix-factorization.html#jordan-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(VJV^{-1}\)</span> form; where <span class="math inline">\(J\)</span> is a matrix of <strong>jordan canonical form</strong> and <span class="math inline">\(V\)</span> is an <strong>invertible</strong> matrix of <strong>eigenvectors</strong> (with possible <strong>generalized eigenvectors</strong>), forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber70">\[\begin{align}
A = VJV^{-1} \tag{2.85}
\end{align}\]</span></p>
<p>A <strong>jordan canonical form</strong> has the following:</p>
<p><span class="math display">\[
J = \left[\begin{array}{cccccc}
\lambda &amp; 1 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; \lambda  &amp; 1 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 0 &amp; \lambda &amp; 1&amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; \lambda &amp; 1 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \lambda
\end{array}\right]
\]</span></p>
<p>Recall <strong>Rank and Nullity</strong> in this chapter in which we described <strong>Rank</strong> as the dimension of the column space, complementary to the <strong>nullity</strong>, which is the dimension of the null space. Here, we illustrate <strong>Jordan</strong> decomposition around these two spaces, but more importantly, around the <strong>Eigen equation</strong> (<a href="2.14-eigenvectors-and-eigenvalues.html#eq:eqnnumber58">(2.17)</a>).</p>
<p>Let us manipulate the equation to reduce it to the following format (note that <span class="math inline">\(\mathbf{\vec{v}}\)</span> is expressed based on a generalized eigenvector):</p>
<p><span class="math display">\[
\therefore A_\lambda v = 0\ \ \ \ \leftarrow\ \ \ \ \ where\ (A - \lambda I) = A_{\lambda} 
\]</span>
In this case, <span class="math inline">\(A_\lambda\)</span> is what we call the <strong>characteristic matrix</strong>.</p>
<p>We are interested in the <strong>Eigenspace</strong> - the <strong>Nullspace</strong> of the <strong>characteristic matrix</strong> (See Equation <a href="2.14-eigenvectors-and-eigenvalues.html#eq:eqnnumber92">(2.23)</a>).</p>
<p>To illustrate <strong>Jordan decomposition</strong>, having the <strong>characteristic matrix</strong> format in mind, and given a <span class="math inline">\(4\times4\)</span> matrix where <span class="math inline">\(n=4\)</span>, let us construct several samples (or cases) of <strong>Jordan form</strong> first.</p>
<p><strong>First Case</strong>: Suppose that the matrix is <strong>rank-three</strong>. The <strong>nullity is one</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ \   A \cdotp v_2 = v_1 + \lambda \times v_2\\
A_\lambda^3 v_3 = 0 \rightarrow\ \  A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda^4 v_4 = 0 \rightarrow\ \   A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; 1  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right]=
\left[\begin{array}{ccccc} 
\square &amp; \square  &amp; \square &amp; \square  \\
\square &amp;  . &amp; . &amp; \square \\
\square &amp; . &amp;  .  &amp; \square \\
\square &amp; \square  &amp; \square &amp; \square \\
\end{array}\right]
\end{array}
\end{array}
\]</span>
</p>
<p>We have one whole block of size four (four <strong>Eigenvectors</strong>).</p>
<p>Here we have assumed that <span class="math inline">\(\lambda\)</span> has an <strong>algebraic multiplicity</strong> of four, e.g. <span class="math inline">\((\lambda - 1)^4\)</span>, in the <strong>Eigenspace</strong>, but the <strong>geometric multiplicity</strong> is one, e.g. <span class="math inline">\(dim(N(\lambda - 1)) = 1\)</span>. Because of that, the <strong>Jordan form</strong> has only one full <strong>Jordan block</strong> with one <strong>standard eigenvector</strong>, <span class="math inline">\(v_1\)</span>, and three non-zero <strong>generalized eigenvectors</strong>, <span class="math inline">\(v_2, v_3, v_4\)</span>. If a <strong>Jordan block</strong> contains <strong>, generalized eigenvectors</strong>, then there is a chain in the block.</p>
<p><span class="math display">\[
(v, A_\lambda v, A_\lambda^2 v, A_\lambda^3 v, ..., A_\lambda^{n-1} v)
\]</span></p>
<p>In the example above, we notice <span class="math inline">\(A_\lambda^4 v_4 = 0\)</span>. That is derived as such:</p>
<p><span class="math display">\[\begin{align*}
A \cdotp v_4 {}&amp;= v_3 + \lambda \times v_4 \\
(A - \lambda I) \cdotp v_4 &amp;= v_3 \leftarrow let\ (A - \lambda I) = A_\lambda\\
A_\lambda \cdotp v_4 &amp;= v_3 \leftarrow \ \ \ A_\lambda v_4 \neq 0\ \ \text{\{ not in Eigenspace}\}\\
A_\lambda^2 \cdotp v_4 &amp;= A_\lambda v_3 = v_2 \leftarrow \ \ \ A_\lambda^2 v_4 \neq 0\ \ \ \text{\{ add }A_\lambda\text{ to each side \}} \\
A_\lambda^3 \cdotp v_4 &amp;= A_\lambda^2 v_3 = A_\lambda v_2 = v_1 \leftarrow \ \ \ A_\lambda^3 v_4 \neq 0 \\
A_\lambda^4 \cdotp v_4 &amp;= A_\lambda^3 v_3 = A_\lambda^2 v_2 = A_\lambda v_1 = 0 \leftarrow \ \ \ A_\lambda^4 v_4 = 0 \\
\end{align*}\]</span></p>
<p><strong>Second Case</strong>: Suppose that the matrix is <strong>rank-two</strong> instead. The <strong>nullity is two</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{cccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \   A \cdotp v_2 = \lambda \times v_2\\
A_\lambda^2 v_3 = 0 \rightarrow\ \   A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda^3 v_4 = 0 \rightarrow\ \  A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{llll}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\square &amp; .  &amp; . &amp; .  \\
. &amp; \square  &amp; \square &amp; \square \\
. &amp; \square &amp; .  &amp; \square  \\
. &amp; \square &amp; \square &amp; \square \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have two <strong>Jordan blocks</strong>. The first block has size 1. The other block has a size three.</p>
<p>Suppose we have the following equations, still for <strong>rank-two</strong>, then we form a different system of equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ A \cdotp v_1 = \lambda_1 \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ A \cdotp v_2 = v_1 + \lambda_1 \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ A \cdotp v_3 = \lambda_2 \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ A \cdotp v_4 = v_3 + \lambda_2 \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{llll}
\lambda_1 &amp; 1  &amp; . &amp; .\\
. &amp; \lambda_1  &amp; . &amp; .\\
. &amp; . &amp; \lambda_2  &amp; 1\\
. &amp; . &amp; . &amp; \lambda_2\\
\end{array}\right] =
\left[\begin{array}{ccccc}
\square &amp; \square   &amp; . &amp; .  \\
\square  &amp; \square  &amp; . &amp; . \\
.  &amp; . &amp; \square  &amp; \square  \\
. &amp; . &amp; \square &amp; \square \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We still have two <strong>Jordan blocks</strong>. Each block size has size two. That could be a case where we have two <span class="math inline">\(\lambda s\)</span>, e.g., <span class="math inline">\((\lambda-1)^2(\lambda-2)^2\)</span>; but each <span class="math inline">\(\lambda\)</span> may only have one <strong>geometric multiplicity</strong>; thus may need one <strong>generalized eigenvector</strong> for each <span class="math inline">\(\lambda\)</span>.</p>
<p><strong>Third Case</strong>: Suppose that the matrix is <strong>rank-one</strong>. The <strong>nullity is three</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \   A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ \   A \cdotp v_4 = v_3 + \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; 1  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\square &amp; .  &amp; . &amp; .  \\
. &amp; \square  &amp; . &amp; . \\
. &amp; . &amp; \square  &amp; \square \\
. &amp; . &amp; \square &amp; \square \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have three <strong>Jordan blocks</strong>. The first two blocks have size one. The last block has size two.</p>
<p>Suppose we have the following equations, still for <strong>rank-one</strong>, then we form a different system of equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda^2 v_2 = 0 \rightarrow\ \   A \cdotp v_2 = v1 + \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \   A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda^2 v_4 = 0 \rightarrow\ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; 1  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\square  &amp; \square &amp; . &amp; .\\
\square &amp; \square &amp; . &amp; .\\
.  &amp; . &amp;  \square &amp; .\\
. &amp; .  &amp; . &amp; \square\\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We still have three <strong>Jordan blocks</strong>. The first block has a size two, and the two other blocks have a size one each.</p>
<p><strong>Fourth Case</strong>: Suppose that the matrix is <strong>rank-zero</strong> (all elements are zero). The <strong>nullity is four</strong> in this case. So forming the equations:</p>

<p><span class="math display">\[
\begin{array}{ccccc}
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \  A \cdotp v_1 = \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda v_3 = 0 \rightarrow\ \  A \cdotp v_3 = \lambda \times v_3 \\
A_\lambda v_4 = 0 \rightarrow\ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; . &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] = 
\left[\begin{array}{ccccc}
\square &amp; .  &amp; . &amp; .  \\
. &amp; \square  &amp; . &amp; . \\
. &amp; . &amp; \square  &amp; .  \\
. &amp; . &amp; . &amp; \square \\
\end{array}\right]_{nxn}
\end{array}
\end{array}
\]</span>
</p>
<p>We have four Jordan blocks. Each block has a size 1.</p>
<p>The critical point to all this is choosing which case to use that dictates the set of equations and thus the number of <strong>Jordan blocks</strong> that determine the <strong>Jordan form</strong> to use.</p>
<p>To know the number of <strong>Jordan blocks</strong>, we need to work on the <strong>Eigenspace</strong> of the <strong>characteristic matrix</strong>: <span class="math inline">\(N(A - \lambda I)\)</span></p>
<p>Let us illustrate that by first evaluating the <strong>RREF</strong> (reduced row-echelon form) of the <strong>characteristic matrix</strong> for the <strong>free variables</strong> (Note here that we work on the <strong>Nullspace</strong>, <span class="math inline">\(N(A -\lambda I) \leftarrow (A - \lambda I)v = 0\)</span>:</p>
<p><span class="math display">\[
A = \left[\begin{array}{cccc}
1 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 4 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; 4 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 1 \\ 
\end{array}\right]
\]</span></p>
<p>Our <strong>characteristic polynomial</strong> in the equation gives us:</p>
<p><span class="math display">\[\begin{align*}
\lambda^4 - 10\lambda^3+24\lambda^2 - 22\lambda + 7 {}&amp;= 0\\
(\lambda-7)(\lambda-1)^3 &amp;=0
\end{align*}\]</span></p>
<p>However, our <strong>minimal polynomial</strong> is below (as that is the minimal polynomials whose corresponding characteristic matrices, when multiplied together, result in zero matrices):</p>
<p><span class="math display">\[
(\lambda-7)(\lambda-1)^2 =0
\]</span></p>
<p>That gives us a clue that we have one <strong>Jordan block</strong> for <span class="math inline">\(\lambda=7\)</span> and two <strong>Jordan blocks</strong> for <span class="math inline">\(\lambda=1\)</span> based on the <strong>algebraic multiplicity</strong> of the <strong>minimal polynomials</strong>. We can further validate that by finding the <strong>regular Eigenvectors</strong> of the <strong>characteristic matrix</strong> of each corresponding <strong>Eigenvalues</strong>.</p>
<p>Here, we know our <strong>Eigenvalues</strong>: <span class="math inline">\(\lambda=7\)</span> and <span class="math inline">\(\lambda=1\)</span>. Now, we need to find our <strong>Eigenvectors</strong>.</p>
<p><strong>First</strong>, let us solve for <span class="math inline">\(v_4\)</span> using <span class="math inline">\(\lambda=7\)</span>:</p>
<p><span class="math display">\[\begin{align*}
(A - \lambda I) {}&amp;= 
\left[\begin{array}{rrrr}
-6 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; -3 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; -3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; -6 \\
\end{array}\right]
\rightarrow RREF(A - (7)I) \rightarrow 
\left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; -10/7\\
0 &amp; 1 &amp; 0 &amp; -6/7\\
0 &amp; 0 &amp; 1 &amp; -6/7 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array}\right] \\
\end{align*}\]</span></p>
<p>The <strong>RREF</strong> of the matrix shows one free variable: <strong>r</strong>. Because <strong>r</strong> is a free variable, we can assume any value. In our case, we use <span class="math inline">\(r=7\)</span> only to discard the denominator seven from the other variables.</p>
<p><span class="math display">\[
\text{solution for x}\ \begin{cases}
x_1 + \frac{-10}{7}x_4 = 0 \\
x_2 + \frac{-10}{7}x_4 = 0 \\
x_3 + \frac{-10}{7}x_4 = 0
\end{cases}\ \ \text{simplied into}\ \
\begin{cases}
x_1 = \frac{10}{7}r \\
x_2 = \frac{6}{7}r \\
x_3 = \frac{6}{7}r \\
x_4 = r = 7\\
\end{cases} \rightarrow
x = 
r\left[\begin{array}{rrrrr}10 \\ 6 \\ 6 \\ 7 \end{array}\right]_{v_4}
\]</span></p>
<p>That gives us the <strong>standard Eigenvector</strong> for <span class="math inline">\(\lambda=7\)</span>:</p>
<p><span class="math display">\[
v_4 = \left[\begin{array}{rrrrr} 10 &amp; 6 &amp; 6 &amp; 7\end{array}\right]^T
\]</span></p>
<p><strong>Second</strong>, let us solve for <span class="math inline">\(v_3\)</span> using <span class="math inline">\(\lambda=1\)</span>:</p>
<p>We know that our <strong>algebraic multiplicity</strong> for <span class="math inline">\(\lambda=1\)</span> is three. However, if we compute the number of <strong>Eigenvectors</strong>, we only get two <strong>Eigenvectors</strong>.</p>
<p><span class="math display">\[\begin{align*}
(A -\lambda I) {}&amp;= 
\left[\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0\\
0 &amp; 3 &amp; 3 &amp; 0\\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}\right]
\rightarrow RREF(A - (1)I) \rightarrow 
\left[\begin{array}{rrrr}
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 \\
\end{array}\right]
\left[\begin{array}{rrrrr}x_1 \\ x_2 \\ x_3 \\ x_4 \end{array}\right]
\end{align*}\]</span></p>
<p>The <strong>RREF</strong> of the matrix shows two free variables: <strong>r</strong> and <strong>s</strong>. Because they are free variables, we can assume values, <span class="math inline">\(r=1, s=1\)</span>:</p>
<p><span class="math display">\[
\text{solution for x}\ \begin{cases}
x_2 = 0 \\
x_3 = 0
\end{cases}\ \ \text{simplied into}\ \
\begin{cases}
x_1 = r \\
x_2 = 0 \\
x_3 = 0 \\
x_4 = s \\
\end{cases} \rightarrow
x = 
r\left[\begin{array}{rrrrr}1 \\ 0 \\ 0 \\ 0 \end{array}\right]_{v_1} +
s\left[\begin{array}{rrrrr}0 \\ 0 \\ 0 \\ 1 \end{array}\right]_{v_2}
\]</span></p>
<p>That gives us a set of only two <strong>Eigenvectors</strong> for <span class="math inline">\(\lambda=1\)</span>:</p>
<p><span class="math display">\[
v_1 = \left[\begin{array}{rrrr} 1 &amp; 0 &amp; 0 &amp; 0\end{array}\right]^T, \ \ \ \ 
v_2 = \left[\begin{array}{rrrr} 0 &amp; 0 &amp; 0 &amp; 1\end{array}\right]^T
\]</span></p>
<p>Overall, our <strong>Eigenspace</strong>, <span class="math inline">\(N(A - \lambda I)\)</span>, spans only <strong>three Eigenvectors</strong> <span class="math inline">\(\rightarrow\)</span> <strong>Span {</strong> <span class="math inline">\(v_1, v_2, v_4\)</span> <strong>}</strong>.</p>
<p><span class="math display">\[
N(A - \lambda I) = Span
\left\{
\left[\begin{array}{r}1 \\ 0 \\ 0 \\ 0 \end{array}\right]_{v_1}
\left[\begin{array}{r}0 \\ 0 \\ 0 \\ 1 \end{array}\right]_{v_2}
\left[\begin{array}{r}10 \\ 6 \\ 6 \\ 7 \end{array}\right]_{v_4}
\right\}
\]</span></p>
<p>where dim(<span class="math inline">\(N(A - \lambda I)\)</span>) = 3 indicating a <strong>geometric multiplicity</strong> of three which equals <strong>three Jordan blocks</strong>.</p>
<p>In the context of <strong>solutions</strong>, the three <strong>Eigenvectors</strong> are found to be solutions in the <strong>Null-space</strong>, and so the system for which <span class="math inline">\((A - \lambda I) = 0\)</span> does hold. On the contrary, they do not form a solution in the <strong>Column-space</strong> of the <strong>characteristic matrix</strong> thus the system for which <span class="math inline">\((A - \lambda I) \neq 0\)</span> does not hold.</p>
<p>With all that being said, we come down now to the following <strong>Jordan form</strong>:</p>

<p><span class="math display">\[
\begin{array}{ccccc} 
\begin{array}{lllll}
A_\lambda v_1 = 0 \rightarrow\ \ \ \ A \cdotp v_1 =  \lambda \times v_1\\
A_\lambda v_2 = 0 \rightarrow\ \ \ \  A \cdotp v_2 = \lambda \times v_2\\
A_\lambda^2 v_3 = 0 \rightarrow\ \ \ \  A \cdotp v_3 = v_2 + \lambda \times v_3 \\
A_\lambda v_4 = 0 \rightarrow\ \ \ \  A \cdotp v_4 = \lambda \times v_4 \\
\end{array} &amp; 
\begin{array}{cc}
J = \left[\begin{array}{ccccc}
\lambda &amp; .  &amp; . &amp; .  \\
. &amp; \lambda  &amp; 1 &amp; . \\
. &amp; . &amp; \lambda  &amp; .  \\
. &amp; . &amp; . &amp; \lambda \\
\end{array}\right] =
\left[\begin{array}{ccccc}
\square &amp; .  &amp; . &amp; .  \\
. &amp; \square  &amp; \square &amp; .\\
. &amp; \square &amp; \square &amp; .\\
. &amp; . &amp; . &amp; \square  
\end{array}\right]
\end{array}
\end{array}
\]</span>
</p>
<p>That is because <span class="math inline">\(\lambda=1\)</span> gives us only two <strong>Eigenvectors</strong>. So we need one <strong>generalized Eigenvector</strong>, <span class="math inline">\(v_3\)</span>; that makes one of the <strong>Jordan blocks</strong> size two.</p>
<p>To solve for <span class="math inline">\(v_3\)</span>, let us, therefore, use the equation:</p>
<p><span class="math display">\[
A_\lambda^2 v_3 = 0 \rightarrow\ \ \ \  A \cdotp v_3 = v_2 + \lambda \times v_3  
\]</span></p>
<p>The <strong>regular Eigenvector</strong> <span class="math inline">\(v_2\)</span> will be disposed of as a new one because <span class="math inline">\(v_2\)</span> participates now in a <strong>Jordan block</strong> of size two instead of this vector being isolated to its own <strong>Jordan block</strong>.</p>
<p>Let us put aside <span class="math inline">\(v_2\)</span> for now to solve for <span class="math inline">\(v_3\)</span>. To do that, we simplify the equation further:</p>
<p><span class="math display">\[\begin{align*}
A  v_3 {}&amp;= v_2 + \lambda \times v_3 \\
(A - \lambda I) v_3 &amp;= v_2 \\
A_\lambda v_3 &amp;= v_2 \\
\end{align*}\]</span></p>
<p>However, that does not give us a solution in the <strong>Null-space</strong> for <span class="math inline">\(v_3\)</span>; therefore, let us convert the equation into one with <strong>Nilpotent polynomial</strong>. The trick is to add <span class="math inline">\(A_\lambda\)</span> to each side of the equation.</p>
<p><span class="math display">\[\begin{align*}
A_\lambda v_3 {}&amp;= v_2 \\
A_\lambda  A_\lambda v_3 &amp;= A_\lambda v_2 \\
A_\lambda^2 v_3 &amp;= A_\lambda v_2 \\
A_\lambda^2 v_3 &amp;= 0, \ \ \ \ \ \because A_\lambda v_2 = 0
\end{align*}\]</span></p>
<p>Our <strong>characteristic matrix</strong> corresponding to <span class="math inline">\(\lambda=1\)</span> becomes:</p>
<p><span class="math display">\[
(A\lambda)^2v_3 = 
\left[
\begin{array}{cccc}
0 &amp; 30 &amp; 30 &amp; 0 \\
0 &amp; 18 &amp; 18 &amp; 0 \\
0 &amp; 18 &amp; 18 &amp; 0 \\
0 &amp; 21 &amp; 21 &amp; 0 \\
\end{array}
\right]v_3 = 0 \rightarrow
\begin{cases}
30_{x_2} + 30_{x_3} = 0 \\
18_{x_2} + 18_{x_3} = 0 \\
18_{x_2} + 18_{x_3} = 0 \\
21_{x_2} + 21_{x_3} = 0
\end{cases} \rightarrow
\begin{cases}
x_2 = -x_3  
\end{cases}
\]</span>
That gives us our <span class="math inline">\(v_3\)</span> vector:</p>
<p><span class="math display">\[
v_3 = \left[\begin{array}{rrrr}0 &amp; 1 &amp; -1 &amp; 0 \end{array}\right]^T
\]</span></p>
<p><strong>Third</strong>, let us now solve for <span class="math inline">\(v_2\)</span>. We know that we can solve for <span class="math inline">\(v_2\)</span> using the following previous equation:</p>
<p><span class="math display">\[
A \cdotp v_3 = v_2 + \lambda \times v_3\ \ \ \ \rightarrow\ \ \ \ (A - \lambda I)v_3 = v_2 
\]</span>
Therefore, our new <span class="math inline">\(v_2\)</span> becomes:</p>
<p><span class="math display">\[
v2 = (A - \lambda I)v_3 =
\left[
\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}
\right]_{(A - \lambda I)}
\left[\begin{array}{ccc} 0 \\ 1 \\ -1 \\ 0 \end{array} \right]_{v_3} =  
\left[\begin{array}{ccc} -8 \\ 0 \\ 0 \\ 5 \end{array} \right]_{v_2}   
\]</span></p>
<p>Let us validate if <span class="math inline">\(v_2\)</span> is the end of the <strong>Jordan block</strong> chain - though we mentioned that the size is two.</p>
<p><span class="math display">\[
(A - \lambda I)v_2 =
\left[
\begin{array}{cccc}
0 &amp; 1 &amp; 9 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 3 &amp; 3 &amp; 0 \\
0 &amp; 6 &amp; 1 &amp; 0 \\
\end{array}
\right]_{(A - \lambda I)}
\left[\begin{array}{ccc} -8 \\ 0 \\ 0 \\ 5 \end{array} \right]_{v_2} =  
\left[\begin{array}{ccc} 0 \\ 0 \\ 0 \\ 0 \end{array} \right]_{0}   
\]</span>
That shows that the new <span class="math inline">\(v_2\)</span> is also a solution in the <strong>Eigenspace</strong>.</p>
<p><strong>Finally</strong>, we have obtained our <span class="math inline">\(V\)</span> and <span class="math inline">\(J\)</span>:</p>
<p><span class="math display">\[
J = \left[\begin{array}{rrrr}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 7\\
\end{array}\right],\ \ \ \ \ 
V = \left[\begin{array}{rrrr}
1 &amp; -8 &amp; 0 &amp; 10\\
0 &amp; 0 &amp; 1 &amp; 6 \\
0 &amp; 0 &amp; -1 &amp; 6\\
0 &amp; 5 &amp; 0 &amp; 7\\
\end{array}\right]
\]</span>
What we just did by using a new <span class="math inline">\(v_2\)</span> and <span class="math inline">\(v_3\)</span> to form <span class="math inline">\(V\)</span> is what we call a <strong>change of basis</strong>.</p>
<p>With that, we can reconstruct our original matrix <span class="math inline">\(A\)</span> using Equation (<a href="2.20-matrix-factorization.html#eq:eqnnumber70">(2.85)</a>):</p>
<p>As an exercise, try the following matrix to arrive at the following hints:</p>
<p><span class="math display">\[
A = \left[\begin{array}{cccc}
1 &amp; 6 &amp; 1 &amp; 0\\
0 &amp; 3 &amp; 4 &amp; 0\\
0 &amp; 4 &amp; 3 &amp; 0 \\
0 &amp; 1 &amp; 9 &amp; 1 \\ 
\end{array}\right]
\]</span>
Hint for the <strong>characteristic equation</strong></p>
<p><span class="math display">\[\begin{align*}
\lambda^4-8\lambda^3+6\lambda^2 + 8\lambda -7 {}&amp;= 0 \\
(\lambda^3-9\lambda^2+15\lambda + -7)(x+1) &amp;= 0 \\
(\lambda^2-8\lambda+7)(x-1)(x+1) &amp;= 0 \\
(x-1)(x-7)(x-1)(x+1) &amp;= 0 \\
(x-7)(x-1)^2(x+1) &amp;= 0 \\
\end{align*}\]</span></p>
<p>Hint for <strong>J</strong> and <strong>V</strong>:</p>
<p><span class="math display">\[
J = \left[\begin{array}{rrrr}
7 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; -1\\
\end{array}\right],\ \ \ \ \ 
V = \left[\begin{array}{rrrr}
7 &amp; 1 &amp; 0 &amp; 5\\
6 &amp; 0 &amp; 0 &amp; -2 \\
6 &amp; 0 &amp; 0 &amp; 2\\
10 &amp; 0 &amp; 1 &amp; -8\\
\end{array}\right]
\]</span></p>
<p>Note that other literature use <strong>transposed Jordan form</strong> instead and thus carries on altered methods, e.g.:</p>
<p><span class="math display">\[
J = \left[\begin{array}{cccccc}
\lambda &amp; 0 &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
1 &amp; \lambda  &amp; 0 &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 1 &amp; \lambda &amp; 0 &amp; \ldots &amp; 0 \\
0 &amp; 0 &amp; 1 &amp; \lambda &amp; \ldots &amp; 0 \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp;\ddots &amp; 0 \\
0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; \lambda
\end{array}\right]
\]</span></p>
<p>For further reading about <strong>Jordan block matrices</strong> and finding a matrixâs <strong>minimal polynomial</strong>, we reference Attila MÃ¡tÃ©âs paper <span class="citation">(<a href="bibliography.html#ref-ref563a">2014</a>)</span>.</p>
</div>
<div id="other-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">2.20.8</span> Other Decomposition<a href="2.20-matrix-factorization.html#other-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Other decompositions are equally important in solving problems. However, this book only introduces the decomposition equations and does not discuss the methods in detail.</p>
<ul>
<li>Schur Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(QUQ^{T} or\ QUQ^{-1}\)</span> form; where <span class="math inline">\(Q\)</span> is an <strong>orthogonal matrix</strong> and <span class="math inline">\(U\)</span> is an <strong>Upper Triangular matrix</strong>, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber71">\[\begin{align}
A = QUQ^{-1} \tag{2.86}
\end{align}\]</span></p>
<ul>
<li>Hessenberg Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(PHP^T\)</span> form; where <span class="math inline">\(H\)</span> is a <strong>Hessenberg matrix</strong> and <span class="math inline">\(P\)</span> is a <strong>unitary matrix</strong>, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber72">\[\begin{align}
A = PHP^{T} \tag{2.87}
\end{align}\]</span></p>
<ul>
<li>Polar Decomposition</li>
</ul>
<p>The idea is to decompose a <strong>matrix</strong> into its <span class="math inline">\(UP\)</span> form; where <span class="math inline">\(U\)</span> is a <strong>unitary matrix</strong> and <span class="math inline">\(P\)</span> is a <strong>positive semi-definite Hermitian matrix</strong>, forming the equation:</p>
<p><span class="math display" id="eq:eqnnumber73">\[\begin{align}
A = UP \tag{2.88}
\end{align}\]</span></p>
<p>A matrix, being regarded as a transformation utility, can stretch or rotate a vector:</p>
<p><span class="math display">\[
Av = b\ \ \ \ \text{ where b is a transformed vector}
\]</span>
The matrix can also be regarded as a deformation utility. It deforms a vector. Polar decomposition can be helpful when factoring in a deformation matrix and thus has a geometric representation.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="2.19-types-of-matrices.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="2.21-software-libraries.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
