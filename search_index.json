[["index.html", "The Power and Art of Approximation Preface 0.1 Acknowledgment and Motivations 0.2 Caveat 0.3 About the Author", " The Power and Art of Approximation A Data Science and Engineering Handbook - Omnibus Edition - Raymond Michael Ofiaza Ordoña 2023-02-14 Preface Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book is based on the following six major areas corresponding to a regimen of Data Science undertakings: Data Mining - the process of gathering raw data and transforming it into useful information. This includes discovering patterns, trends, and relationships in data sets. Data Cleaning - the process of cleaning data which includes determining accuracy, completeness, validity, consistency, uniformity, and volatility of data (Han J. et al 2002). Exploratory Data Analysis (EDA) - the process of investigating and analyzing the characteristics of data. Feature Engineering - the process of extracting and selecting relevant features. ML Modeling - the process of generating ML models by feeding training data to ML algorithms. Data Visualization - the process of presenting insightful information in visual forms such as charts, graphs, and so on. Of the six areas, a majority of volume III tackles topics around Exploratory Data Analysis, Feature Engineering, and Machine Learning Modeling with a brief introductory discussion on Data Mining and Data Cleaning. Here, we cover Feature Extraction, including Model Building and Model Management. We review Statistical Modeling and Bayesian Modeling and show that available software tools and frameworks are developed to simplify Model building and Model management. Our objective is to understand what a model is, how to come up with a better model, and how to preserve and re-use a model. We also talk about the life-cycle management of models, including managing and maintaining the dataset and parameters (and hyperparameters) used. When it comes to Machine Learning and Deep Learning, we require a bit of software engineering and skill in programming using Java, Python, or R language, to mention a few. In this book, we focus on using R. We will demonstrate using a few R packages once we start covering image processing and machine translation. First, we shall show how to write up a simple vanilla framework for CNN and RNN, two popular Deep Neural Network (DNN) types, followed by a cover of MLP, Resnet, and Transformer architectures. While this book may not cover the processing of data at scale using Big Data, being able to model a solution is just half of the story. The other half requires skill in Data engineering. We can only acquire such skill through IT training, IT experience, and exposure to Distributed Systems, Cloud Computing Environments, and other technologies that can help with distributed and parallel processing. Additionally, it also helps to show insights using Data Visualization techniques along with the use of frontend tools and Web Frameworks that are helpful in creatively designing such visualization. So as one can imagine, Data Science covers many things. But as we fare through our endeavors in this field, bear in mind that it is not about learning all the mathematical or Machine Learning formulas and algorithms that only count throughout this book. What also matters, in the end, is that we understand that most of these are designed to approximate. And as such, unless we can genuinely exact the solution, there may always be more novel and better crafts waiting to be discovered. Many techniques presented here are equipped with knobs to be tuned, yet they may also come with flaws and constraints that require awareness; although, discussing them all is impossible to fit in one book. Some are simple. Others are complex. But we may be able to build upon them our creativity in finding better solutions to problems in our domain if only perhaps we stick to the fundamental idea of what each solution provides and weed through the hints or the breadcrumbs left with us through old classic solutions. 0.1 Acknowledgment and Motivations First, let me acknowledge and give high regard to the authors of the books listed below, as the books have become valuable references in writing this book. In addition, some of the books were recommended (a couple or so of them were authored) by my UIUC professors, who, themselves, composed the online courses that I took for my Master’s in Computer Science at the University of Illinois Urbana-Champaign (UIUC). Book Title (In no particular order) Author Calculus of Several Variables (3rd Edition) Robert A. Adams Calculus an Applied Approach (7th Edition) Ron Larson, Bruce H. Edwards Differential Equations and Linear Algebra (4th Edition) C. Henry Edwards, David E. Penney, David T.Calvis Scientific Computing: An Introductory Survey (revised 2nd Edition) Michael T. Heath Numerical Analysis Richard L. Burden, Douglass J. Faires, Annette M.Burden Numerical Recipes: The Art of Scientific Computing (3rd Edition) William H. Press, Saul A. Teukolsky, William T. Vetterling, Brian P. Flannery Statistics (College Review Series) Martin Sternstein A Concise Course in Statistical Inference Larry Wasserman Statistics: the Art and Science of Learning from Data (4th Edition) Alan Agresti, Christine Franklin, Bernhard Klingenberg, Michael Posner Statistical Rethinking: A Bayesian Course with Examples in R and Stan Richard McElreath A Student’s Guide to Bayesian Statistics Ben Lambert Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan John K. Kruschke The Nature of Statistical Learning Theory Vladimir N. Vapnik The Elements of Statistical Learning (2nd Edition) Trevor Hastie, Robert Tibshirani, Jerome Friedman Machine Learning: A Probabilistic Perspective Kevin P. Murphy Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling William J. Stewart Text Data Management and Analysis: A Practical Introduction to Information Retrieval and Text Mining ChengXiang Zhai, Sean Massung Data Mining: Concepts and Techniques Jiawei Han, Micheline Kamber, Jian Pei Probability and Statistics for Computer Science David Forsyth Bayesian Data Analysis (3rd Edition) Andrew Gelman, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin The book authored by Chengiang Zhai et al. and the lectures he delivered to graduate students gave me a perspective on text management analysis, especially natural language processing, semantic analysis, syntactic analysis, sentimental analysis, and opinion analysis. It also gave me an understanding of natural language processing. I get to review similarity distance measurements, N-grams, TF-IDF, Okapi BM25, nDCG, and other evaluation metrics. That motivated me to look deeper into NLP and eventually get the chance to write about topic modeling with emphasis on LSA, pLSA, and LDA. The book authored by David Forsyth and the lectures he delivered to graduate students gave me a perspective on applied machine learning, covering regressions, classifications, and dimensionality reduction using PCA. It also gave me an understanding of dynamic systems and Markov chain methods to approximate system states. That is followed by an understanding of other applied solutions such as Variational Autoencoder, Vector Quantization, Boltzmann machine, Neural Networks, and Generative Adversarial Networks, including Probabilities, Distributions, Mixture Distributions, Likelihood, Estimation and Maximization, and many others. The book authored by Jiawei Han et al. and the lectures he delivered to graduate students gave me a perspective on information retrieval concepts, sequential pattern mining and pattern discovery, association, classification, and clustering. That motivated me to look deeper into Data Mining concepts and get the chance to write about sequence processing in terms of frequent sequential pattern mining methods such as Apriori, FP-growth, PrefixSpan algorithms. And to write about clustering methods such as Hierarchical clustering and DBSCAN. The book authored by Michael T. Heath and the lectures he delivered to graduate students helped me to realize the importance of numerical computations. The course refreshed my understanding of Numerical Analysis and Scientific Computing. Undoubtedly, researchers rely upon such Numerical Methods to bring about novel ideas. I give acknowledgment to Trevor Park for his lectures on Advance Bayesian modeling. That gave me a perspective on sampling distributions, probabilities, and prior and posterior concepts, including conjugate priors. It also gave me concepts around probabilistic graphical models, sampling, and simulations using Markov Chain Monte Carlo and other tools such as JAGS for Bayesian modeling. It helped me to understand Convergence assessment, Autocorrelation Mixing, Posterior Predictive checking, modeling using General Linear Models, modeling with Distributions, and many others. I also give acknowledgment to David Unger for his lectures in Applied Statistics, delivering concepts in Probability and Statistics, Inference for Linear Regressions, Categorical Interactions, Analysis of Variance, Model Building and Diagnostics, Transformation, Collinearity, Variable Selection, Statistical Modeling, and many others. Additionally, I acknowledge Feng Liang for her lectures in Practical Statistical Learning. The lectures contributed a hundredfold to my knowledge of Machine Learning concepts; in particular around Variable Selections, Variable Criteria, Least-Squares, Regularization, Regression Trees, Classification Trees, Random Forests, Out-of-Bags, Boosting, Adaboost, Regression Splines, Kernel Smoothing, Dimension Reduction, Clustering, Vector Quantization, Mixture Models, Gaussian Mixtures, KL distance, Linear Discrimination, KKT conditions, Primal and Dual relationships, SVMs, Kernel SVMS, Recommender systems, Time-series forecasting, Neural Networks, and many others. I also learned from complementary books by C. Henry Edwards et al. and by William H. Press for classic methods such as Runge-Kutta methods and Euler’s methods, by William J. Stewart for distributions and probability concepts, and most significantly by Kevin P. Murphy for further understanding of Markov Chain, Variational Bayes and Inference, Expectation Propagation, and Monte Carlo Inference. That motivated me to look deeper into Numerical Analysis and Bayesian Analysis, which eventually got me to write about the subject matter. However, it is the book from Vladimir N. Vapnik that sets me to realize that many things, if not everything, around us are all about estimation - or approximation. That is why it is essential to know about Learning Theory in his book, which covers Learning Problems, Learning Processes, Convergence of Learning Processes, Generalization Ability of Learning Processes, Methods of Pattern Recognition, Methods of Function Estimation, and many others. No doubt that, many times, other resources helped ensure the consistency and validity of equations and notes. A few of the shared resources are listed below. They serve as platforms for discussions and sharing of knowledge. I have indeed benefitted even from anonymous contributors. Thus, I shout kudos to everyone that I fail to mention here. Other Resources Description symbolab.com Online Calculator from Simple to Complex Equations stackoverflow.com Online community for programmers stackexchange.com Question and Answer platform quora.com Question and Answer platform wikipedia.com Online Encyclopedia google.com Search engine for the World’s Information youtube.com A platform for sharing videos A few video channels are worth mentioning for the quality and richness of information presented. Every time I felt incomplete after reading some white papers, the videos connected the dots and saved my day. Video Channel Description 3Blue1Brown Linear Algebra Professor Leonard Calculus StatQuest (Josh Starmer) Statistics Ben Lambert Statistics Michel van Biezen Kalman Filter Valerio Velardo Audio Signal Processing Artificial Intelligence - All in One Machine Learning - Andrew Ng Deetoher Conjugate Prior Derivations The following tools helped me write the book, render the R code, design the diagrams and images in the book, and ultimately, render the PDF file. Tools Description BookDown Creating a Book (Yihui Xie) R Studio Rendering R code Libre Office Rendering diagrams, plots, images Pandoc and LaTeX rendering PDF Lastly, but most importantly, I am forever indebted to my wife, Agnes, for her ever-loving support and understanding, especially amid the COVID-19 Pandemic. Her share of time and energy in helping the family get through these difficult times is more rewarding a feat of accomplishment than writing this book itself. I also owe my kids Moises, Cody, and Sachiko precious moments lost or stolen between work and finishing this book. I give special mention to my Mom, siblings, in-laws, nephews, nieces, pets, friends, and acquaintances for just being healthy. 0.2 Caveat Note that some ideas are common knowledge and thus are offered in academic settings as standard lectures; hence, just a succinct coverage warrants sufficient recollection of their salient relevance for our theme. In such cases, acknowledgment cannot be singled out to a few; instead, implicit kudos are proper to the lecturers and professors who spend time and dedication imparting such knowledge to committed individuals like me. Nevertheless, I came to learn much by writing, and I claim no credit for any ideas presented here. They belong to their respected owners. Therefore, it is within my remit to apologize for missed acknowledgments or inadvertently misconstrued views. No doubt that writing three volumes of a book risks unavoidable mistakes. Errors and deficiencies can be inevitable. For that reason, I take it upon myself to genuinely apologize. Furthermore, it would be my sincerest gratitude to know about such shortcomings so that I may be able to correct them in future editions. For corrections and feedback, please send an email to rmordona@gmail.com. For additional Caveat and Errata, visit the link below: https://agnespublishing.com/thepowerandartofapproximation 0.3 About the Author Raymond Michael Ofiaza Ordoña (Ray) received his B.S. degree in Computer Science from Saint Louis University, Baguio City, Philippines, in 1994 and his Master’s degree in Computer Science from the University of Illinois Urbana-Champaign, Illinois, in 2017. The long pause between his undergraduate and graduate undertakings was due to his early pursuit of a long-standing career in the computer industry. As a result, he has over 27 years of exposure to technology since 1995. He was involved in escalation support and in managing mission-critical enterprise systems. In addition, he led a global team of top-caliber technical staff to help design and develop a self-healing platform to detect system anomalies and perform prescriptive and corrective measures using machine learning techniques. Out of which, the team filed eight patents for novel solutions. For this reason, he gives acknowledgment to the team. Ray hopes to one day visit his undergraduate university in Baguio City to share his experiences with students as an expression of giving back what he learned. That is also to recognize the support, mentorship, and guidance offered to him by Dean Florian Generalao. Finally, he is eternally grateful for the opportunity given to him as part of the first class in Computer Science College, for accepting him as a student worker, and for trusting him with a real-world project for his College Thesis. "],["introduction.html", "Introduction", " Introduction The field of Data Science is broad and far-reaching. Thus, for an aspiring Data Scientist, it is natural to assume that this field may require many things. However, despite a growing list of distinct focus areas associated with this field, such as ML Engineering, ML Research, Data Engineering, Data Engineering Research, Data Analysis, and Distributed Systems Engineering, it can be said that the fundamental core requirements of becoming a Data Scientist are mathematics and the ability to carry out research work. On the other hand, being specific to ML Engineering has its distinct requirements in that one has to have some level of software engineering and must be familiar with ML frameworks needed for development work and ML modeling. In addition, one has to have a balanced awareness of the mathematical nuances behind the frameworks. Regarding Data engineering and Distributed Systems Engineering, one has to have a background in Distributed Systems, Cloud Computing, and Big Data. Given such fundamental prerequisites, the separation of duty can be evident in practice, yet from time to time, there is an overlap when it comes to actual work. As a result, the delineation between responsibilities may taper off. For example, in a relatively small organization, we may see a Data Scientist also wearing the hat of an ML Engineer, an ML Researcher, a Data Engineer, or maybe even a Data Engineering Researcher. For that reason, perhaps just knowing a little bit of everything goes a long way for us. That said, this book highlights the power and art of approximation from a Data Science and ML Engineering perspective. We focus on many approximation strategies and build some intuition around them. Our approach starts with essential concepts for each selected subject matter, accompanied by mathematical formulation (with some derivations at times) and illustrations in the form of diagrams, along with simple examples in the form of computer code. Regrettably, we do not cover Data Engineering. Nevertheless, that area deserves its book, even if only to introduce Distributed Systems and Cloud Computing. Additionally, if one prefers to be in Data (and Distributed Systems) Engineering Research, it helps to delve into foundational concepts and become familiar with classic algorithms such as Consistent Hashing, Vector and Lamport Timestamps, FIFO and Causal Multicasts, Gossip Protocols for Failure Detection problems, Paxos algorithm for Consistency Synchronization problems, Ricart-Agrawal and Maekawa’s algorithms for Election problems, and many others. Also, we do not cover streaming technologies such as Kafka, automation, orchestration and deployment technologies such as Kubernetes, web frameworks such as Django, React, and Angular, and DNN frameworks such as Keras and Tensorflow. Let alone the other frameworks and services made available and offered by Enterprise Companies. In retrospect to most Data Science materials, it may be fair to discuss one area that is as equally important, which is Data Mining. Such an area includes many aspects associated with information retrieval and processing of Data, namely Data cleaning, Data provenance, Data curation, Data visualization, and Data management (including ETL), among many others. This book will briefly cover Data Mining in the early part of the Machine Learning chapters, which does not give justice as it takes at least an entire book to examine the subject. Here, we split this book into three volumes. The first volume (Volume I) references mathematical approximations in the context of Linear Algebra and Numerical Analysis. The second volume (Volume II) references mathematical approximations in the context of Statistical Analysis and Bayesian Analysis. The third volume (Volume III) references Machine Learning and Deep Learning. As the book emphasizes the scientific and artful mechanics of approximation, we reiterate the four essential areas in mathematics that are relevant at some point to the idea of approximation when we get to Machine Learning and Deep Learning. The selected areas in this book are Linear Algebra, Calculus, Statistics, and Bayesian. We arrange them in such an order as to follow the natural progression of ideas evolving from simplicity to complexity. For example, we work our way from simple static linear equations to complex dynamic non-linear equations, expressing the idea of starting from static systems to dynamic systems and from memory-less (stateless) systems to memory-based (stateful) systems. The underlying approximation techniques contained in each area are arranged further in a similar orderly fashion, though not only following the natural progression of ideas but also following specific commonly shared properties. For example, some, if not most, curricula find it necessary to cover Euler’s Method, Heun’s Method, and Runge-Kutta Method together in that order as they are founded upon the same set of mathematical formulations. The same can be said for Power Method, Inverse Power Method, and Rayleigh Quotient method, as they are founded upon the same simple classic iterative algorithm. The same can be said for the Laplace equation, the Heat equation, and the Wave equation in PDE as they follow the known Elliptic, Parabolic, and Hyperbolic patterns. The same can be said for Arnoldi and Lanczos Method versus GMRES and CG Methods, for which they all use and share the same concept around the Krylov subspace. Lastly, the same can be said for Least-Square, Galerkin, Petrov-Galerkin, and Rayleigh-Ritz Methods, as they share the same WRM methods under the Finite Element Method. Statistics and Bayesian follow a similar progression. First, we introduce concepts that are precursory to the more advanced topics. For example, it helps to navigate Descriptive Statistics and become familiar with histograms and charts. We then proceed to refresh our knowledge on Central Tendencies and Moments. Then we begin to cover Statistical Modeling, the Significance of Difference, the Significance of Regressions, and Statistical Inference. In line with Bayesian, we mainly work on uncertainty. We rely on Posterior and Prior probabilities following different families of distributions. In this area, we focus on stateful systems and how we use Markov Chain to approximate latent states. Finally, we cover Simulation, Sampling, Kalman Filter, and other Kalman Filter variants. Finally, we proceed with arrangements that underscore Machine Learning and Deep Learning. The methods follow the same progression as commonly outlined in other literature. For example, it is common to see the arrangement of ML methods according to the following categories: supervised vs. non-supervised, regression vs. classification, and classification vs. clustering. Let us go through the following outline: Volume I Starting with Chapter 1 (Direct and Indirect Methods), we briefly review the concept of simplification, optimization, and approximation. We introduce closed forms and briefly describe the difference between Analytical and Numerical methods. We also review numerical concepts in the context of well-posed and well-conditioned solutions, including describing numerical measurements such as accuracy, precision, and stability. Chapter 2 (Numerical Linear Algebra I) requires a refresh of specific data structures such as vectors, matrices, and tensors. Here, we learn to manipulate the structures and perform matrix multiplication and factorization. Then, we further review Eigenvectors and Eigenvalues, which are widely covered in other ML literature. That is followed by working on Linear combination equations. It is worth noting that in root-finding problems, we use determinants to solve for Eigenvalues analytically. However, as issues become complex, we need to find alternative numerical solutions. In Chapter 3 (Numerical Linear Algebra II), we review numerical solutions to classic problems that have to do with approximations. Numerically, we employ iterations and determine convergence. Problems in which we expose numerical solutions include root-finding problems, smoothing problems, and interpolation problems. We introduce the Newton-Raphson method and other methods for root-finding problems. We also cover BFGS, Krylov, GMRES, and Conjugate Gradient methods for polynomial solutions. We discuss the Least-Squares method for regression problems. We discuss Lagrange, Newton, and B-spline methods for interpolation problems, and we cover LOWESS &amp; LOESS solutions for smoothing problems. A good grasp of these classic numerical solutions helps to understand simple classic approximation problems. In the following chapters, we introduce problems involving more complex dynamic systems. In Chapter 4 (Numerical Calculus), we begin to review dynamic systems. Certain physical laws govern dynamic systems. For example, the trajectory of a rocket, the transfer of heat, and the movement of waves are all governed by physical laws. Here, to solve dynamic system problems, we review the use of Euler, Heun, and Runge-Kutta methods, including a review of Poisson equations, Heat Equations, and Wave equations. We also cover the Finite Element, which brings the idea of partitioning materials into pieces. This idea plays a long way into the concept of variational inference when we split problems into pieces. Additionally, we also cover Fourier Series to tackle data series, which is valuable knowledge to have, especially when introducing encoder-decoder applications and machine translation applications. Volume II In Chapter 5 (Probability and Distribution), we start to focus on Random Events. Here, we work on Probabilities and introduce various Data Distributions. Chapter 6 (Statistical Computation) covers Statistical Modeling. We first emphasize dealing with Statistical Inferences. We describe hypotheses and the different methods of hypothesis and comparison tests. That includes ANOVA and Post-HOC analysis. We then discuss the Significance of Regressions. Lastly, we discuss Variable Selection and Regression Modeling. In Chapters 7 (Bayesian Computation I) and 8 (Bayesian Computation II), we extend Chapter 5 concepts by expounding on Uncertainty of events. We cover Information Theory, Posterior-Prior Conjugacy, Bayesian Inference, Bayesian Analysis, Markov Chain, and Kalman Filter. We also cover Simulation and Sampling, such as Monte Carlo Sampling and Hamiltonian Sampling. In Information Theory, we discuss Entropy, Gini Index, KL Divergence, and Information Gain. Here, we pay close attention to Sampling. We sample data, considered as an approximate representation of the whole. For example, self-driving cars use Lidar devices to sample data from the surrounding environment, processed by Kalman Filters and other analysis methods. Volume III In Chapter 9 (Computational Learning I), we cover Exploratory Data Analysis and Feature Engineering. We introduce Data Mining and Dimensionality Reduction methods. In Chapter 10 (Computational Learning II), we discuss Regression and Classification. Here, we begin our journey into the field of Machine Learning. First, we get to cover approximate solutions through Regression methods. Then, we continue to discuss Logistic Regression which can also be used for Binary Classification. That becomes a pre-cursor to our topic around Classifications. Finally, we cover Classifications, covering known techniques such as SVM, Random Forest, AdaBoost, and XGBoost. In Chapter 11 (Computational Learning III), we continue the discussion of Machine Learning by covering Clustering methods such as Hierarchical clustering and DBSCAN. Then, we briefly cover three everyday use cases of Machine learning, namely Natural Language Processing (NLP), Time-Series Forecasting, and Recommender Systems. In Chapter 12 (Computational Deep Learning I), we begin to discuss Deep Neural networks (DNN), starting with Simple Perceptrons and Multiple-Perceptrons (MLP). Then we discuss Convolutional Neural Network (CNN) in great detail. Finally, we get to cover approximate solutions through neural network methods. Chapter 13 (Computational Deep Learning II) extends Deep Learning topics with other Neural Networks such as ResNet, RNN, LSTM, GRU, and Transformer. We end the chapter with a discussion around Encoder-Decoder and Machine Translation applications, specifically on Speech Recognition. In Chapter 14 (Distributed Computation), we finally summarize our book by briefly introducing ML Pipeline and Open Standard Formats such as PMML, PFA, and ONNX. These are knowledge helpful in the life-cycle management and deployment of portable and sharable ML models in a production environment. So without further ado, let us begin! "],["mathematical-notation.html", "Mathematical Notation 0.4 Notation 0.5 Number System 0.6 Implementation", " Mathematical Notation Below are a list of mathematical notations and number systems that we will be using throughout the book. 0.4 Notation Table 0.1: Common Math and Latex Notations Notation Latex Description \\(X_{i}=1 \\quad Y_{i}=1\\) \\quad Spacing between expressions \\(\\phi\\) \\phi Predicate or Proposition \\(\\forall\\) \\forall Universal Quantifier \\(\\exists\\) \\exists Existential Quantifier \\(\\in\\) or \\(\\ni\\) \\in or \\ni element in set \\(\\notin\\) \\notin not in set \\(\\cap\\) \\cap intersection \\(\\cup\\) \\cup union \\(\\neg\\) \\neg Logical Negation \\(\\land\\) \\land Logical AND \\(\\lor\\) \\lor Logical OR \\(\\oplus\\) \\oplus Logical XOR \\(\\otimes\\) \\otimes Kronecker Product \\(\\oslash\\) \\oslash Hadamard Product \\(\\odot\\) \\odot Element-Wise Product \\(\\veebar\\) \\veebar Propositional Logic \\(\\sim\\) \\sim Similar \\(\\propto\\) \\propto Proportional to \\(\\geq\\) \\geq Greater than or equal \\(\\leq\\) \\leq Lesser than or equal \\(\\ne\\) \\ne Not Equal \\(\\approx\\) \\approx Approximate \\(\\cong\\) \\cong Congruent to \\(\\pm\\) \\pm Plus Minus \\(\\leftarrow\\) \\leftarrow implies (if) \\(\\rightarrow\\) \\rightarrow implies (only if) \\(\\nearrow\\) \\nearrow NE Arrow \\(\\searrow\\) \\searrow SE arrow \\(\\iff\\) \\iff implies (if and only if) \\(\\equiv\\) \\equiv equivalent (if and only if) \\(\\langle Pm,Pn \\rangle\\) \\langle Pm, Pn \\rangle left,right angle brackets \\(\\left[ x \\right]\\) \\left[ x \\right] Square Brackets \\(\\left( x \\right)\\) \\left( x \\right) Parentheses Brackets \\(\\bigl\\{ x \\bigr\\}\\) \\left{ x \\right} Curly Brackets \\(\\top\\) \\top True \\(\\perp\\) \\perp False or Perpendicular \\(\\vdash\\) \\vdash Entails (Proves) \\(\\models\\) \\models Entails (Therefore) \\(\\perp\\!\\!\\perp\\) \\perp\\!\\!\\perp Conditional Independence \\(\\cdotp\\) \\cdotp Dot Product \\(\\vdots\\) \\vdots Vertical Ellipsis \\(\\ldots\\) \\ldots Horizontal Ellipsis \\(\\ddots\\) \\ddots Diagonal Ellipsis \\(\\sum_{i=a}^{b}\\) \\sum_{i=a}^{b} Sum \\(\\prod_{i=a}^{b}\\) \\prod_{i=a}^{b} Product \\(\\circ\\) \\circ, e.g. \\(f\\circ g\\) Composition of f and g, f(g(x)) \\(\\ast\\) \\ast, e.g. \\(f\\ast g\\) Convolution of f and g \\(\\infty\\) \\infty Infinity \\(\\lim_{x\\to\\infty}\\) \\lim_{x\\to\\infty} Limits \\(\\int_{a}^{b}\\) \\int_{a}^{b} Integral Table 0.2: Additional Common Math and Latex Notations Notation Latex Description \\(\\iint\\) \\iint Double Integral \\(\\iiint\\) \\iint Triple Integral \\(\\oint\\) \\oint_{a}^{b} Close-Path Integral \\(\\partial\\) \\partial Partial Derivative \\(\\nabla\\) \\nabla Gradient \\(\\vec{x}\\) \\vec{x} Vector \\(\\hat{y}\\) \\hat{y} Hat \\(\\bar{y}\\) \\bar{y} Mean \\(\\overline{bel}\\) \\overline{bel} Belief \\(\\tilde{y}\\) \\tilde{y} Tilde (general purpose) \\(\\angle\\) \\angle Angle \\(\\mathcal{P}\\) \\mathcal{P} Probability \\(\\mathcal{L}\\) \\mathcal{L} Likelihood \\(\\binom{n}{k}\\) \\binom{n}{k} Binomial \\(\\underset{\\theta}{argmax}\\) \\underset{\\theta}{argmax} Argmax \\(\\hookrightarrow\\) \\hookrightarrow Embedding \\(\\because\\) \\because Because \\(\\therefore\\) \\therefore Therefore \\(\\left[\\begin{array}{rr} 2 &amp; 3 \\\\ 4 &amp; 5 \\end{array}\\right]\\) \\left[\\ begin{array}{rr} Array / Vector 2 &amp; 3 \\\\ 4 &amp; 5 \\end{array}\\right] \\(\\begin{cases} 0 &amp; if\\ x = 0 \\\\ 0 &amp; if\\ x\\neq\\ 0 \\end{cases}\\) \\ begin{cases} … \\end{cases} Case \\(\\left. \\frac{x}{2}\\right\\vert_{0}^{1}\\) \\left.\\frac{x}{2} \\right\\vert_{0}^{1} Limit Evaluation \\(\\underbrace{a + b}_{underbrace}\\) \\underbrace{a + b}_{underbrace} Underbrace \\(\\overbrace{a + b}^\\text{overbrace}\\) \\overbrace{a + b}^\\text{overbrace} Overbrace Table 0.3: 21 Common Greek Symbols Notation Latex Description \\(\\alpha\\) \\alpha Alpha \\(\\beta\\) \\beta Beta \\(\\gamma\\ or\\ \\Gamma\\) \\gamma or \\Gamma Gamma \\(\\delta\\ or\\ \\Delta\\) \\delta or \\Delta Delta \\(\\epsilon\\) \\epsilon Epsilon \\(\\zeta\\) \\zeta Zeta \\(\\eta\\) \\eta Eta \\(\\theta\\ or\\ \\Theta\\ or\\ \\vartheta\\) \\theta or \\Theta or \\vartheta Theta \\(\\lambda\\ or\\ \\Lambda\\) \\lambda or \\Lambda Lambda \\(\\mu\\) \\mu Mu \\(\\nu\\) \\nu Nu \\(\\xi\\) \\xi Xi \\(\\pi\\) \\pi Pi \\(\\rho\\) \\rho Rho \\(\\sigma\\ or \\ \\Sigma\\) \\sigma or \\Sigma Sigma \\(\\tau\\) \\tau Tau \\(\\upsilon\\ or\\ \\Upsilon\\) \\upsilon or \\Upsilon Upsilon \\(\\phi\\ or\\ \\Phi\\ or\\ \\varphi\\) \\phi or \\Phi or \\varphi Phi \\(\\chi\\) \\chi Chi \\(\\psi\\ or\\ \\Psi\\) \\psi or \\Psi Psi \\(\\omega\\ or\\ \\Omega\\) \\omega or \\Omega Omega 0.5 Number System Table 0.4: Number System Symbol Latex System \\(\\mathbb{R}\\) \\mathbb{R} Real Numbers \\(\\mathbb{Z}\\) \\mathbb{Z} Integer Numbers \\(\\mathbb{N}\\) \\mathbb{N} Natural Numbers \\(\\mathbb{Q}\\) \\mathbb{Q} Rational Numbers \\(\\mathbb{P}\\) \\mathbb{P} Irrational Numbers \\(\\mathbb{C}\\) \\mathbb{C} Complex Numbers \\(\\mathbb{I}\\) \\mathbb{I} Imaginary Numbers \\(\\mathbb{H}\\) \\mathbb{H} Quaternion (skip) \\(\\mathbb{O}\\) \\mathbb{O} Octonion (skip) The first seven number systems can be best illustrated using the following representation: Figure 0.1: Number System Complex numbers (\\(\\mathbb{C}\\)) are a superset of real numbers (and imaginary numbers) which are a superset of rational numbers (and irrational numbers) which are a superset of integer numbers, and so on. Natural numbers (\\(\\mathbb{N}\\)) can be regarded as counting numbers. If we can naturally and casually count from one to one million and still continue counting, then all those counting numbers are natural numbers. Integer numbers (\\(\\mathbb{Z}\\)) are numbers that include natural numbers but also include zero and negative numbers. Figure 0.2: Integer Rational numbers (\\(\\mathbb{Q}\\)) are numbers that include integer numbers but also have fractions (or ratios) such as ½ and ¼. In other words, what are the numbers that exist between 0 and 1, or between 1 and 2, or between -10 and -9? Numbers such as 0.234, 1.454, and -9.678 are rational numbers. Irrational numbers (\\(\\mathbb{P}\\)) are numbers that cannot be expressed or written as a fraction (or ratio). It is common to think that 0.3333 is irrational. However, because we can write 0.3333 as 1/3 in its fraction form, this number is rational. So what are irrational numbers? The Pi (\\(\\pi\\)) and Euler’s number (e) are examples of irrational numbers because the numbers are non-terminating. (\\(\\pi\\)) = 3.141592653589 … e = 2.718281828459 … Complex numbers (\\(\\mathbb{C}\\)) come in the form of a + bi where (i) is an imaginary number (\\(\\mathbb{I}\\)); and a and b are real numbers. The premise of complex numbers has to do with the common idea that it is not possible to square a real number and get a negative number. See below: \\[i^2 = -1\\] But with complex number (\\(\\mathbb{C}\\)) systems, the equation above can be solved. Here, it is easy to show that: \\[ \\sqrt{10} = 10^{\\frac{1}{2}} \\text{ and that } (10^{\\frac{1}{2}})^2 = 10^{\\frac{2}{2}} = 10. \\] That is, \\[ \\sqrt{x} = x^{\\frac{1}{2}} \\text{ and that } (x^{\\frac{1}{2}})^2 = x^{\\frac{2}{2}} = x \\] Therefore: \\[ if\\ x = -1\\ and\\ i = x^{\\frac{1}{2}}, then\\ i = (-1)^{\\frac{1}{2}} and\\ i^2 = ((-1)^\\frac{1}{2})^2 = -1^{\\frac{2}{2}} = -1. \\] 0.6 Implementation While mathematics is our means of expressing the things around us, in this book, we choose to use the R language to implement the operations and computations described by mathematics. Furthermore, most of our implementations are written in a rather generic way such that we can easily translate them into other programming languages, namely Python, Java, and C. Note that this book does not tutor the use of the R language. Nonetheless, we encourage readers to be familiar with the following special R functions that are commonly used in this book: R Function Description matrix(.) Creating a Matrix. solve(.) Solving a Linear Equation. apply(.) Performing Matrix Operations. sweep(.) Performing Matrix Operations with additional input. which.max(.) Get argmax(.) - index of maximum value. which.min(.) Get argmin(.) - index of minimum value. plot(.) Plots - e.g. bar charts, line graphs, histograms, etc. While we try to minimize the use of R packages, there are cases in which it is necessary to use them. Below is a list of some of the packages used in this book (in no particular order). Package Citation DunnettTests Xia, F. (2013) HMM Dr. Lin Himmelmann et al. (2010) lsa Wild, F. (2020) KernSmooth Wand, M. (2019) Matrix Bates, D., &amp; Maechler, M. (2021) NLP Hornik, K. (2020) RColorBrewer Neuwirth, E. (2014) ResourceSelection Lele, S. R., Keim, J. K., &amp; Solymos, P. (2019) SnowballC Bouchet-Valat, M. (2020) agricolae de Mendiburu, F. (2020) arulesSequences Buchta, C., Hahsler, M., &amp; Diaz, D. (2020) caret Kuhn, M. (2020) corrplot Wei, T. &amp; Simko, V. (2017) data.table Dowle, M. &amp; Srinivasan, A. (2020) dplyr Wickham, H., et al. (2021) forestplot Gordon, M. &amp; Lumley, T. (2019) fpp Hyndman, R. J. (2013) imager Barthelme, S. (2020) jpeg Urbanek, S. (2019) koRpus Michalke, M. (2021) koRpus.lang.en Michalke, M. (2020) ks Duong, T. (2020) nortest Gross, J., &amp; Ligges, U. (2015) openNLP Hornik, K. (2019) openNLP.models.en Hornik, K. (2015) patchwork Pedersen, T. L. (2020) plot3D Soetaert, K. (2019) Rlang R Core Team (2019) rjags Plummer, M. (2019) rpart Therneau, T., &amp; Atkinson, B. (2019) rpartplot Milborrow, S. (2019) scales Wickham, H., &amp; Seidel, D. (2019) sentimentr Rinker, T. W. (2019) splitstackshape Mahto, A. (2019) tseries Trapletti, A., &amp; Hornik, K. (2019) tuneR Ligges, U., et al. (2018) wordcloud Fellows, I. (2018) xgboost Chen, T., et al. (2020) Knitr Xie, Y. (2019) pmml Bolotov, D., et al. (2021) aurelius Mortimer, S., &amp; Bennett, C. (2017) onnx Tang Y., &amp; ONNX Authors (2021) jsonlite Ooms, J. (2014) topicmodels Grün, B., &amp; Hornik, K. (2011) tidytext Silge, J., &amp; Robinson, D. (2016) "],["numericalmethods.html", "Chapter 1 Direct and Indirect Methods 1.1 Closed-form equation 1.2 Analytical and Numerical solutions 1.3 Significant figures 1.4 Accuracy 1.5 Precision 1.6 Stability and Sensitivity 1.7 Stiffness and Implicitness 1.8 Conditioning and Posedness ", " Chapter 1 Direct and Indirect Methods In a rather simple definition, Direct Methods are methods that provide exact solutions. Examples of such methods are Matrix Decomposition and Gaussian Elimination methods. Chapter 2 (Numerical Linear Algebra I) covers most of the Direct Methods. On the other hand, Indirect Methods, also known as Numerical Methods, provide approximations in the absence of exact solutions. These methods offer alternative solutions to other Direct Methods that are otherwise considered unreliable if used at scale. Chapter 3 (Numerical Linear Algebra II) covers most of the Numerical Methods. With these two simple definitions, it is worth mentioning that not all methods provide the appropriate solutions we seek. Therefore, the onus is upon us to assure that our methods do not cause more harm than necessary. For this reason, there is a field dedicated to ensuring we analyze the problems and numerical solutions carefully. This field is called Numerical Analysis and is complemented by what is otherwise known as applied Numerical Analysis geared toward Scientific Computing. Here, we reference the works of Heath M.T. (2002) and Edwards H. et al. (2018) and other additional references for consistency. Because Numerical Analysis requires depth, it posts a challenge even to summarize the field in only three chapters. Nonetheless, let us instead summarize a few essential concepts that will help us become more critical of using Numerical Methods in the context of Approximation. After that, it pays to review some of the classic ideas that contributed to the evolution of some of the more elegant solutions we use today to understand why we need to cover Numerical Methods. Gaining some fundamental understanding of the Numerical Methods may help build an essential foundation for practicing Data Science and Machine Learning. This chapter briefly introduces three important concepts, namely Simplification, Optimization, and Approximation. Simplification is about reducing complicated problems and solutions into simplified, manageable, acceptable, and affordable units (s) or form(s). An example is decomposing (or factorizing) matrices into sub-components (smaller shapes, e.g., vectors and scalars), resulting in a more straightforward computation without losing the same integrity as the original. Another example is reducing data structures from a higher dimension, \\(\\mathbb{R}^{mxn}\\), to a lower dimension, \\(\\mathbb{R}^{n}\\). Linearization of non-linear equations is another example. And lastly, using simpler methods or functions by dividing bigger convoluted tasks into more granular simpler tasks (a.l.a divide and conquer approach) is another example. Optimization is about adjusting solutions into more performant and efficient methods within a more acceptable and reasonable timeframe. Here, we describe performance and efficiency in terms of delivery and productivity. The idea is to be able to use lesser effort, operation, work, or step and still produce the expected result without loss of time. Again, this is about evaluating more efficient methods and algorithms. Approximation is about using solutions that can provide estimates with acceptable and reasonable accuracy without loss of data, loss of stability, and loss of reliability. Linear Regression is an example. Here, we describe Regression in terms of trying to calculate how close the estimated value is to the actual value - the ground truth. Being only an estimate, we consider the residual or difference - delta. If possible, our goal is to reduce the residual. If we cannot reduce it to zero, we need to settle on an acceptable and reasonable minimum residual. Some methods, such as Gradient Descent (GC), tend to be iterative (or even recursive). For example, GC requires multiple executions of the method such that for every iteration, a set of hyperparameters, such as step size, are fine-tuned. At the same time, a group of parameters called weights is re-adjusted - all these to approximate the target value. In later chapters, we will talk more about the Gradient Descent method. It is important to note that we use the terms acceptable and reasonable because the idea of an estimated solution at times is best left to the eyes of the beholder for the decision. That said, the tolerance level is somewhat driven by the level of risk that stakeholders with authority are willing to take, sacrificing and balancing between cost over interest, interest over safety and security, safety and security over cost, etc. Thus, being computationally scientific depends upon acceptable and reasonable tolerance at times. In this endeavor, we deal with different problems and solutions. For example, we list systems characterized by the availability of solutions and/or bounded with restrictions. a system (or a set of problems) with no solution a system with only one solution a system with a finite number of solutions a system with an infinite number of solutions a system with start and end points a system with bounded restrictions Furthermore, we may characterize problems in terms of linearity (or non-linearity), or whether they conclude solutions in a steady-state or dynamic state, and possibly with perturbation. Here are a few combinations: a linear problem a non-linear problem a problem with an initial state a problem in a steady-state, with no perturbation a problem in a steady-state, with perturbation (maybe arbitrary noisy data) a problem in a dynamic state, with an initial state and with some boundary a problem in a dynamic state, with perturbation (data change) through time. a problem in motion with transition (such as speed, acceleration, and jerk in physics) In any case, it can be said that there are two types of solutions: an exact solution an approximate solution We will cover these two types, but before we continue further, let us start this chapter by covering two essential topics being contested: The first topic is closed forms of an expression (or solution). The second topic has to do with analytical methods and numerical methods. 1.1 Closed-form equation Suppose an equation exists in which it characterizes a set of operations so that the corresponding solution is deemed non-converging or nonterminating with no other known means (or known available tools/utilities) of solving the equation. In that case, the solution is open-ended - we cannot determine a terminating boundary. Otherwise, it is a closed-form solution (or equation). An example of an open-ended solution is shown below - this is an infinite series of any generally accepted operations (e.g., multiplication, summation, cosine, sine, and logarithms): \\[ \\text{infinite series} \\rightarrow \\sum_{i=0}^\\infty cos(x_{i}) \\] If we can find a solution that allows the expression above to converge exactly, we can say that we have found a closed-form solution. 1.2 Analytical and Numerical solutions It can be said that analytical computation uses closed-form expressions to perform exact solutions. Closed-form expressions indeed help us to compute solutions exactly. Otherwise, we use Numerical Methods for estimating (approximating) intractable solutions. In this regard, the term tractability emphasizes the ease of computation. If there is no easy way to compute a solution because of the degree of complexity and the demand for resources, then the solution may be deemed intractable. There may be cases when the process of exacting a solution is costly. Thus, we resort to approximation - surrendering to the notion that an approximate solution is more tolerable (and therefore acceptable) than the cost of the exact solution. We face day-to-day problems that necessitate us to weigh between Cost and Tolerance versus Accuracy and Stability. Apart from the required resources (e.g., time and effort), all these also depend upon the type of problems at hand - usually in the form of observed data produced by agents or events (whether random or not). In our case, our journey in numerical analysis starts by evaluating solutions to elementary problems - simple linear systems first before delving into non-linear systems next. Perhaps start with problems that call for solutions on steady-state systems with no perturbations, and then advance to solutions around dynamic states with perturbations with respect to time, e.g., entropy - the decline of a system from steady informative state to a disorderly less-informative state. Some solutions go beyond Brute Force algorithms, Greedy/Gradient algorithms, and Divide-and-Conquer algorithms. In addition, there are solutions that go beyond simple linear systems (e.g., Least-Squares) and beyond simple non-linear systems (e.g., B-Splines) into ordinary/partial differential systems that require us to look closer into Fourier series and Integration/Differentiation. In other cases, certain problems require us to look into iterative/recursive and stochastic (non-deterministic or probabilistic) approaches (e.g., Random Walks and Markov Chain), or even sampling and estimating of random conditions (e.g., Monte Carlo algorithm), and other Stochastic Variational methods. And so, in the process of choosing a solution, we may end up fine-tuning operationally (e.g., shorter gradient steps vs. longer gradient steps, finite series over infinite series, lower-order or lower-degree terms over higher-order terms), parametrically (e.g., maximization/minimization with boundaries or constraints), structurally (e.g., decomposition or dimensional reduction against high-dimensional matrices), and programmatically/computationally (e.g., optimized algorithms taking big-O notation into account). Now, imagine what it would be like to get a computer to perform mathematics for a moment. What would be the most efficient way to get the computer to compute for a solution and still achieve some reasonable and acceptable level of accuracy and stability? First, let us consider what is reasonable and acceptable. At times, being reasonable and acceptable is based on a stakeholder’s comfort zone (and the risk level). For example, one might feel that 99.99% is good enough in some cases. In other cases, one might insist upon a target accuracy of 100%. Or one might think that 99.999999% is as good as 100%. In other words, sometimes, an acceptable level of Accuracy is based on what the stakeholder can take, accept, or tolerate - this is the tolerance level. And one can say that a solution has reached its Accuracy if the solution converges to a given tolerance level. So let us try to understand the following measurements and characteristics of problems and solutions: Accuracy - This measures the closeness of our solution to the actual value, measured in terms of significant digits. Precision - This measures the number of digits expressed. Stability vs. Sensitivity solutions - This measures the robustness and reliability of an algorithm. Well-posed vs Well-conditioned problems - this is a reaction ratio. For example, we expect that a small pebble creates a small ripple when dropped into water (a small reaction). And a larger pebble creates a larger ripple. 1.3 Significant figures There are three rules to follow to determine the number of significant figures of a number. Non-zero digits are significant digits. 34 - 2 significant digits ( 3 and 4) 340 - 2 significant digits (3 and 4) 0.034 - 2 significant digits (3 and 4) 5691 - 4 significant digits (5, 6, 9, and 1) 5690 - 3 significant digits (5, 6, 9) Zeroes in between significant digits are significant digits. 304 - 3 significant digits ( 3, 0 and 4) 340 - 2 significant digits (3 and 4). The zero is not in between significant digits. 034 - 2 significant digits (3 and 4). The zero is not in between significant digits. 5001 - 4 significant digits (5, 0, 0, and 1) 5100 - 2 significant digits (5 and 1). The zeroes are not in between significant digits. Trailing zeroes after the decimal point are significant digits. 3040 - 3 significant digits ( 3, 0 and 4) 3040.0 - 5 significant digits (3, 0, 4, 0, and 0). The 3rd zero is significant because it is a trailing zero after the decimal point. The 2nd zero is also significant because it is between 4 and the trailing zero. The 1st zero is significant because it is between 3 and 4. 034 - 2 significant digits (3 and 4). The zero is not in between significant digits. 5001 - 4 significant digits (5, 0, 0, and 1). The two zeroes are between 5 and 1. 51.00 - 4 significant digits (5, 1, 0, 0). The two zeroes are trailing after the decimal point. Additionally, in multiplication and division operations, choosing the correct significant figure is based on the LEAST NUMBER OF significant figures. 3.04 x 3.1 = 9.424 = 9.4 3.04 has three significant figures in that example, and 3.1 has two significant figures. Here, we have two as the least number of significant figures. Therefore, the result would be 9.4 (with two significant figures). 1.4 Accuracy To explain accuracy, let us assume that the actual value of PI (\\(\\pi\\)) is 3.141592653589793 - that is about 15-decimal digits in precision (but let us ignore the concept of precision for a moment). Now, suppose we are estimating (or approximating) the value of PI (\\(\\pi\\)). Let us choose two ways from many other methods to estimate PI (\\(\\pi\\)). Gregory-Leibniz series Use the below formula to estimate PI using Gregory-Leibniz series: \\[\\begin{align*} \\pi {} &amp; = 4/1 - 4/3 + 4/5 - 4/7 + 4/9 - 4/11 + 4/13 - 4/15\\ ... \\\\ \\pi &amp; = 3.017071817071818 \\end{align*}\\] Nilakantha series \\[\\begin{align*} \\pi {} &amp; = 3 + 4/(2\\times3\\times4) - 4/(4\\times5\\times6) + 4/(6\\times7\\times8) - 4/(8\\times9\\times10) \\\\ &amp;\\ \\ \\ + 4/(10\\times11\\times12) - 4/(12\\times13\\times14)\\ ... \\\\ \\pi &amp; = 3.1408813408813407 \\end{align*}\\] So then, which of the two series resolves closer to the actual value? As can be seen here, the result of the Nilakantha series is much more relative to the actual value. \\[\\begin{align*} \\text{Gregory-Leibniz series} &amp;: 3.017071817071818\\ vs\\ 3.141592653589793\\ (actual\\ value)\\\\ \\text{Nilakantha series} &amp;: 3.1408813408813407\\ vs\\ 3.141592653589793\\ (actual\\ value) \\end{align*}\\] Quantitatively, to be able to measure the closeness of accuracy, we use absolute and relative error: Absolute Error: \\[\\begin{align} \\text{Absolute Error} = |\\text{(Actual Value)} - \\text{(Approximate Value)}| \\end{align}\\] Relative Error: \\[\\begin{align} \\text{Relative Error} = \\frac{|\\text{(Actual Value)} - \\text{(Approximate Value)}|}{|\\text{(Actual Value)}|} \\end{align}\\] For the Gregory-Leibniz series, the relative error computes to around 0.0396 or 3.96% error: \\[\\begin{align*} \\text{Relative Error} &amp; = \\frac{|(3.141592653589793) - (3.017071817071818)|}{|(3.141592653589793)|}\\\\ \\text{Relative Error} &amp; = 0.03963621329954713 \\end{align*}\\] For the Nilakantha series, the relative error computes to around 0.0002 or 0.00% error: \\[\\begin{align*} \\text{Relative Error} &amp; = \\frac{|(3.141592653589793) - (3.1408813408813407)|}{|(3.141592653589793)|} \\\\ \\text{Relative Error} &amp; = 0.00022641786726855837 \\end{align*}\\] Based on the unsigned (absolute) relative error, it shows that the result of the Nilakantha series is much closer to absolute zero. Thus the approximate \\(PI(\\pi)\\) is closer to the actual value and, therefore, more accurate. Note that in terms of accuracy, both series gradually improve (e.g., by converging to the actual value) depending on the number of additional terms added into the series. In the case of the Gregory-Leibniz series, the accuracy is far from the actual value, and this is because we used only nine terms (we ended at 4/15) in the series. Any further higher-order terms may render a better accuracy. It turns out that the GL series is slow to converge; nonetheless, it may eventually converge. The topic of convergence will be seen more in action in a later part of the book. 1.5 Precision In the context of numerical analysis, precision refers to the number of digits expressed. An example is the precision estimate of PI (\\(\\pi\\)). Three examples of precision are provided below: Pi (\\(\\pi\\)) with 5 digits after decimal point. 3.14159 … Pi (\\(\\pi\\)) with 25 digits after decimal point. 3.14159 26535 89793 23846 26433 … Pi (\\(\\pi\\)) with 50 digits after decimal point. 3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37510 … The larger the number of digits expressed, the more precise the number is. Therefore, of the three PI (\\(\\pi\\)) numbers, the 3rd one with 50 digits after the decimal point is the most precise number. However, it is essential to know that precision does not necessarily equate to accuracy. For example, using the same example above with a perturbed number (e.g., instead of 3.14159-, we use 3.1519-), the 3rd example is the most precise of the 1st three examples. However, compared to the 4th example representing the actual value of PI (\\(\\pi\\)) with the same precision - that is, with 50 digits after the decimal point, it is not accurate in that it has an unsigned (absolute) relative error of 0.3%: PI (\\(\\pi\\)) with 5 digits after decimal point. 3.15159 … PI (\\(\\pi\\)) with 25 digits after decimal point. 3.15159 26535 89793 23846 26433 … PI (\\(\\pi\\)) with 50 digits after decimal point. 3.15159 26535 89793 23846 26433 83279 50288 41971 69399 37510 … PI (\\(\\pi\\)) with 50 digits after decimal point (Actual value) 3.14159 26535 89793 23846 26433 83279 50288 41971 69399 37510 … Also, the below example can be said to be accurate at 10-decimal digit precision: 10-decimal digit precision for the actual estimate of PI (\\(\\pi\\)): 3.14159 26535 … On rounding-off and truncation: Pi (\\(\\pi\\)) is an irrational number - meaning that the number is non-terminating. The number of digits after the decimal point goes on continuously without end. The question is, what would represent a Pi (\\(\\pi\\))? Where do we choose to truncate the number of digits after the decimal point? If we choose only 10-decimal digits, do we need to round off or not? Truncating Pi (\\(\\pi\\)) at 10-decimal digits: 3.14159 26535 Rounding off Pi (\\(\\pi\\)) at 10-decimal digits: 3.14159 26536 1.6 Stability and Sensitivity Stability and Sensitivity tend to have the same interpretation, but there is a slight subtlety. Sensitivity is a measure used in data perturbation analysis. Here, it helps to understand the impact of change in data on the firmness, sturdiness, steadiness, or robustness of a solution. Stability is a measure used in computation or algorithm analysis. Here, it helps to understand the reliability of an algorithm or calculation on the firmness, sturdiness, steadiness, or robustness. The steadiness of a solution implies that under uncertainty - e.g., new input data - the solution remains intact. That helps us perform a list of evaluations for further calibration and enhancements. Noise evaluation Uncertainty evaluation Data relation evaluation ( input data vs output data) Prediction evaluation Forecasting evaluation Error evaluation For example, a dataset with a uniform interval tends to be more sensitive than a Chebyshev interval, generating oscillation at the end of the curves, as can be seen in Figure 1.1 when interpolating a Runge function (Heath M.T. 2002). Figure 1.1: Runge Phenomenon 1.7 Stiffness and Implicitness We will be discussing stiffness and implicitness in this chapter around differential equations. It becomes more apparent and intuitive as we go through the different states of a solution with respect to time. Stiffness is a phenomenon much like the Runge phenomenon in that a solution tends to be unstable under a certain step size configuration, more so in the context of differential equations which we will cover later in this chapter. Implicitness describes a system in terms of computing for the current state and the next state of a solution to the system. Along with Stiffness, the motivation is to imply an iterative approximation of a solution using parameters such as step size, etc. More on Implicitness to cover later. 1.8 Conditioning and Posedness Conditioning can be explained by dropping a small pebble into the water, creating a tiny ripple (a slight reaction), versus dropping a larger stone, creating a larger ripple. There is a corresponding relationship (a distortion or perturbation ratio) between the weight of the pebble and the size of the ripple. \\[\\begin{align*} \\text{distortion A} &amp;= \\frac{\\text{small ripple}}{\\text{lighter pebble}} = \\frac{\\text{3 ripples}}{\\text{0.2 gms}} \\\\ \\text{distortion B} &amp;= \\frac{\\text{large ripple}}{\\text{heavier pebble}} = \\frac{\\text{15 ripples}}{\\text{1 gms}} \\end{align*}\\] \\[ \\text{well-conditioned ratio (relationship)} = \\frac{A}{B} = 1 \\] We can say that the situation presented is well-conditioned if the ratio is closer to one. Otherwise, it is not well-conditioned. A system is well-conditioned if the sensitivity (condition) number is 1; otherwise, if the absolute value is greater than one, then it is ill-conditioned. To compute the condition number, let us first determine the relative changes for input data (call it x) and output data (call it y), given a change in x (\\(\\delta{x}\\)) and a change in y (\\(\\delta{y}\\)): \\[\\begin{align*} \\text{Relative Change in Input (X)} &amp;= \\frac{\\hat{x} - x}{x}\\\\ \\text{Relative Change in Output (Y)} &amp;= \\frac{\\hat{y} - y}{y}\\\\ \\\\ where\\ \\hat{x} = x + \\delta{x}, and\\ \\hat{y} = y + \\delta{y} \\end{align*}\\] The following formula is used to determine the condition number of a system. \\[ \\text{Condition Number} = \\frac{\\text{Relative Change in Output (Y)}}{\\text{Relative Change in Input (X)}} \\] In terms of a matrix, the formula for the condition number of a square non-singular matrix is given as follows: \\[ \\text{Condition Number} = \\|A\\|_{L2}\\cdotp\\|A^{-1}\\|_{L2} \\] Note that a matrix that is ill-conditioned tends to be singular. The condition number tends to be infinite. This can also be geometrically shown when the determinant is zero. And for other common functions, below is a table of condition numbers: Table 1.1: Condition Number of Functions Name Functions Condition Number Exponential \\(e^x\\) x Natural Logarithm \\(ln(x)\\) \\(\\frac{1}{ln(x)}\\) Sine \\(sin(x)\\) \\(x cot(x)\\) Cosine \\(cos(x)\\) \\(x tan(x)\\) Tangent \\(tan(x)\\) \\(x(tan(x) + cot(x))\\) A system is well-posed if the solution meets the following criteria: solution exists solution is unique solution is stable otherwise, a system is ill-posed if the solution fails to meet any of those criteria. "],["linearalgebra.html", "Chapter 2 Numerical Linear Algebra I 2.1 System of Linear Equations 2.2 Scalar, Vector, and Matrix, Tensor 2.3 Transposition and Multiplication 2.4 Magnitude, Direction, Unit Vectors 2.5 Linear Combination and Independence 2.6 Space, Span, and Basis 2.7 Determinants 2.8 Minors, Cofactors, and Adjugate Forms 2.9 Inverse Form and Row-Echelon Form 2.10 Linear Transformations 2.11 Rank and Nullity 2.12 Singularity and Triviality 2.13 Orthogonality and Orthonormality 2.14 Eigenvectors and Eigenvalues 2.15 Matrix Reconstruction using Eigenvalues and Eigenvectors 2.16 Diagonalizability of a Matrix 2.17 Trace of a Square Matrix 2.18 Algebraic and Geometric Multiplicity 2.19 Types of Matrices 2.20 Matrix Factorization 2.21 Software libraries 2.22 Summary", " Chapter 2 Numerical Linear Algebra I Linear Algebra is a precursory requirement needed to advance our knowledge in Data Science. That becomes apparent as we course through the many solutions discussed throughout the book leading to the advanced topics in Deep Learning. Understandably, Linear Algebra is too large a topic to fit in one chapter. Be that as it may, it still helps to discuss some of the known classic fundamental concepts of Linear Algebra. That becomes our goal in this chapter, which is to collate formulas, rules, and techniques that have become quite ubiquitous in many forums relevant to Data Science. Moreover, we support our discussions with diagrams and implementations. Also, while it may appear pedagogical, take them as referential hints only in this chapter and in subsequent chapters; thus, it is still preferable to take the full comprehensive courses and text materials - complete with logical and practical exercises for students. Though, at this point, it is essential to know that our theme in this chapter and all the following chapters is to build on end an intuitive narrative around how we are solving problems and genuinely seeing through the power and art of the strategies employed by way of approximation. In this chapter, before we jump to Indirect Methods in the latter part of the chapter, we first concentrate on Direct Methods in the context of Linear Algebra. Here, we reference the great works of Atkinson K. E. (1989), Press W.H et al. (2007), and Edwards H. et al. (2018), along with other additional references for consistency, including lectures from the great Gilbert Strang (2005) in the form of a series of videos. 2.1 System of Linear Equations In this section, we describe a few concepts around linear equation, system of linear equations, and solutions to a system of linear equations. Linear Equation: A linear equation is a mathematical equation describing a line as shown geometrically in Figure 2.1. In the figure, we are using a 2D cartesian coordinate system. Figure 2.1: System of Linear Equations Mathematically and generally, a linear equation is represented by the following simple formula, where m is the slope and b is the y-intercept: \\[\\begin{align} y = mx + b \\end{align}\\] The slope m is computed using the following formula: \\[ m = \\frac{\\Delta{y}}{\\Delta{x}} = \\frac{\\text{change of y}}{\\text{change of x}} = \\frac{y_2 - y_1}{ x_2 - x_1} \\] Note that the vertical distance parallel to the y-axis represents the change of y between two points. Similarly, the horizontal distance parallel to the x-axis represents the change of x between two points. The m represents the rate of change. System of Linear Equations: A System of Linear Equations is a set of linear equations sharing the same variables, e.g. x and y. In Figure 2.1, there are two linear equations in the system: \\[\\begin{align*} y {}&amp;= -05.x + 4\\\\ y &amp;= x \\end{align*}\\] The first linear equation is computed as: \\[ y = mx + b = \\frac{-2-4}{4-0} \\times x + 4 = \\frac{-2}{4}x + 4 = -0.5x + 4 \\] The second linear equation is computed as: \\[ y = mx + b = \\frac{4-1}{4-1} \\times x + 0 = \\frac{1}{1}x + 0 = x + 0 \\] Solutions to a System of Linear Equations: In the context of a System of Linear Equations, if the values in the variables satisfy all the linear equations in the system, then a Solution is said to be found. However, variables have unknown values, so it is natural to solve for the unknowns. In the example above, where we have a system of linear equations involving three common variables, \\(x_1,\\ x_2,\\ x_3\\), we seek to know the values of the three given variables that satisfy the three equations to get the Solution. In Figure 2.1, the two lines intersect at a point. That point is the Solution to the system of linear equations below: \\[\\begin{align*} y {}&amp;= -0.5x + 4\\\\ y &amp;= x \\end{align*}\\] where x = 2.67 and y = 2.67. However, if a system has no solution, the system is called an inconsistent system. A simple example is when two linear equations (two lines) do not intersect. On the other hand, if a system has at least one solution, then the system is called a consistent system. A good example of a system that always has a solution is a homogeneous system in which the right-hand side is zero. Here is another example of a homogenous system involving three unknowns (or variables), \\(x_1,\\ x_2,\\ x_3\\): \\[ \\left(\\begin{array}{lll} 3x_1 + 3x_2 + 3x_3 {}&amp;= 0 \\\\ 2x_1 + 3x_2 + 4x_3 &amp;= 0 \\\\ 1x_1 + 5x_2 + 5x_3 &amp;= 0 \\end{array}\\right) \\] If the number of equations in a system is less than the number of unknowns, then the system is called under-determined system. If there are more unknowns than the number of equations, then the system is called over-determined system. The following sample system is under-determined because it has only two equations but three unknowns. \\[ \\left(\\begin{array}{lll} 3x_1 + 3x_2 + 3x_3 {}&amp;= 0 \\\\ 2x_1 + 3x_2 + 4x_3 &amp;= 0 \\\\ \\end{array}\\right) \\] Solutions to under-determined systems are further explained in Rank and Nullity section. We continue to cover systems of linear equations and show how to solve other linear equation problems when we cover the matrix factorization in this chapter, e.g., LU factorization by Gaussian Elimination. For now, the following section introduces structures widely covered in linear algebra: scalar, vector, matrix. 2.2 Scalar, Vector, and Matrix, Tensor In Linear Algebra, it helps to know the data structures used. There are four common structures to be familiar with, namely Scalar, Vector, Matrix, and Tensor. Scalar is represented by a variable that holds a single number. \\[x_i = 2.54\\] Vector is represented by a variable that holds a collection (or array) of elements (e.g., real numbers, complex numbers, etc.). Given index (i), e.g. i = 1,2,..,n \\[\\begin{align*} \\mathbf{\\vec{V_{n}}} &amp;= \\left[\\begin{array}{l} x_{i=1} \\\\ x_{i=2} \\\\ \\vdots \\\\ x_{i=n} \\end{array}\\right], \\ \\ \\ \\ \\ \\mathbf{\\vec{V_1}} = \\left[\\begin{array}{r} 1 \\\\ 4 \\\\ -3 \\\\ 5 \\end{array}\\right], \\\\ \\mathbf{\\vec{V_2}} &amp;= \\left[\\begin{array}{c} 2 + 3i \\\\ ln(2) \\\\ 3.0 + 4.0i \\\\ sin({\\frac{\\pi}{2}}) \\end{array}\\right], \\ \\ \\ \\ \\ \\mathbf{\\vec{V_3}} = \\left[\\begin{array}{ccccc} 6.251 \\\\ 20.3e^{10} \\\\ \\frac{1}{2} \\\\ \\sqrt{2} \\end{array}\\right] \\end{align*}\\] A vector can be scaled by multiplying it with a scalar number. \\[ \\mathbf{\\vec{V_{n}}} = a\\left[\\begin{array}{ccccc} x_{i=1} \\\\ x_{i=2} \\\\ \\vdots \\\\ x_{i=n} \\end{array}\\right] = \\left[\\begin{array}{ccccc} ax_{1} \\\\ ax_{2} \\\\ ax_{3} \\\\ ax_{4} \\end{array}\\right], \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\vec{V_1}} = 2\\left[\\begin{array}{ccccc} 1 \\\\ 4 \\\\ 3 \\\\ 5 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 2 \\\\ 8 \\\\ 6 \\\\ 10 \\end{array}\\right] \\] Matrix is represented by a variable that holds a 2-dimensional array of numbers. Given row index (i), e.g. i = 1,2,..,n, and column index (j), j = 1,2,..m: \\[ \\mathbf{A}_{m \\times n} = \\left[\\begin{array}{ccccc} x_{i=1,j=1} &amp; x_{1,2} &amp; \\dots &amp; x_{1,n} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; x_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{m,1} &amp; x_{m,2} &amp; \\dots &amp; x_{i=m,j=n} \\end{array}\\right]_{m \\times n} \\] A matrix can be scaled by multiplying it with a scalar number. \\[ \\mathbf{A} = a\\left[\\begin{array}{ccccc} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} \\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} \\\\ x_{3,1} &amp; x_{3,2} &amp; x_{3,3} \\end{array}\\right] = \\left[\\begin{array}{ccccc} ax_{1,1} &amp; ax_{1,2} &amp; ax_{1,3} \\\\ ax_{2,1} &amp; ax_{2,2} &amp; ax_{2,3} \\\\ ax_{3,1} &amp; ax_{3,2} &amp; ax_{3,3} \\end{array}\\right] \\] \\[ \\mathbf{A} = 2\\left[\\begin{array}{ccccc} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 7 &amp; 8 &amp; 9 \\end{array}\\right] = \\left[\\begin{array}{rrr} 2 &amp; 4 &amp; 6 \\\\ 8 &amp; 10 &amp; 12 \\\\ 14 &amp; 16 &amp; 18 \\end{array}\\right] \\] The last structure is the Tensor. Tensor is considered a generalized form of Matrix. A vector is considered a first-order tensor in 1D (one-dimension) space. A matrix is considered a second-order tensor in 2D (two-dimension) space Beyond a second-order tensor, it is natural to begin to cover hyper-cubes and hyper-planes in a higher N-dimension or N-order tensor. In the N-dimension space, N&gt;2, we begin to consider talking about dynamic interaction, field theory, and so on. Revisiting system of linear equations: Having a vector structure and a matrix structure now allow us to represent any system of linear equations and solutions to the system in matrix equation: \\[\\begin{align} A\\mathbf{\\vec{x}} = \\mathbf{\\vec{b}} \\label{eqn:eqnnumber50} \\end{align}\\] The variable x is a vector of unknown variables - the solution or solution space. In other words, this vector is what we are trying to solve. The variable A is a matrix of coefficients - the transformation matrix. More about transformations are covered in the section about linear transformation. The variable b is a vector of constants - the result of the Matrix-vector multiplication. This topic becomes more apparent when we cover Rank and Nullity in a later section. A few properties of scalar and matrix multiplication are shown: \\[ (x + y)A = xA + yA\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x(A + B) = xA + xB \\] 2.3 Transposition and Multiplication 2.3.1 Transposition Vectors are implicitly column vectors. To convert vectors to row vectors, we use Transposition . It is another way of switching the vector from column-wise to row-wise form. We can also transpose row vectors back to column vectors. See the following: \\[\\begin{align*} \\vec{V} (column\\ vector)&amp;= \\left[\\begin{array}{ccccc} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\end{array}\\right] \\rightarrow \\ \\ \\ \\ \\ \\vec{V}^T (transposed\\ row\\ vector) \\\\ &amp;= \\left[\\begin{array}{ccccc} x_{1} &amp; x_{2} &amp; \\cdots &amp; x_{n} \\end{array}\\right] \\end{align*}\\] To transpose a matrix, we flip the matrix over its diagonal. Figure 2.2: Transpose As can be seen, the small white square at the lower left side gets flipped over to the top right. Similarly, the following matrix gets flipped over where for example, the entry \\(x_{4,1}\\) at the lower-left corner gets flipped to the top right corner, and \\(x_{1,3}\\) gets flipped to the lower-left corner, so does the other non-diagonal entries. \\[ \\mathbf{A} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; x_{1,4} \\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; x_{2,4} \\\\ x_{3,1} &amp; x_{3,2} &amp; x_{3,3} &amp; x_{3,4} \\\\ x_{4,1} &amp; x_{4,2} &amp; x_{4,3} &amp; x_{4,4} \\end{array}\\right] \\rightarrow \\mathbf{A}^T = \\left[\\begin{array}{ccccc} x_{1,1} &amp; x_{2,1} &amp; x_{3,1} &amp; x_{4,1} \\\\ x_{1,2} &amp; x_{2,2} &amp; x_{3,2} &amp; x_{4,2} \\\\ x_{1,3} &amp; x_{2,3} &amp; x_{3,3} &amp; x_{4,3} \\\\ x_{1,4} &amp; x_{2,4} &amp; x_{3,4} &amp; x_{4,4} \\end{array}\\right] \\] Example: Transposing a 4x4 matrix: \\[ \\mathbf{A} = \\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12\\\\ 13 &amp; 14 &amp; 15 &amp; 16 \\end{array}\\right] \\rightarrow \\mathbf{A}^T = \\left[\\begin{array}{rrrr} 1 &amp; 5 &amp; 9 &amp; 13\\\\ 2 &amp; 6 &amp; 10 &amp; 14\\\\ 3 &amp; 7 &amp; 11 &amp; 15\\\\ 4 &amp; 8 &amp; 12 &amp; 16 \\end{array}\\right] \\] Transposing a 3x4 matrix: \\[ \\mathbf{A} = \\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 5 &amp; 6 &amp; 7 &amp; 8 \\\\ 9 &amp; 10 &amp; 11 &amp; 12 \\end{array}\\right] \\rightarrow \\mathbf{A}^T = \\left[\\begin{array}{rrrr} 1 &amp; 5 &amp; 9 \\\\ 2 &amp; 6 &amp; 10 \\\\ 3 &amp; 7 &amp; 11 \\\\ 4 &amp; 8 &amp; 12 \\end{array}\\right] \\] A few properties to be aware. Transposition reverses everything in the parenthesis like so: \\[\\begin{align} (a^TAb)^T = aA^Tb^T \\end{align}\\] Transposition of a symmetric matrix yields an identity matrix: \\[\\begin{align} A^TA = I \\end{align}\\] Other properties of transposition: \\[\\begin{align} (AB)^T = B^TA^T &amp; &amp; (A + B)^T = A^T + B^T \\\\ (xA)^T = xA^T &amp; &amp; A^T(x)A + A^T(y)A = A^T(x+y)A \\\\ xA^T + xB^T = x(A+B)^T &amp; &amp; xA^T + yA^T = (x+y)A^T\\\\ A^Txy + A^Tzw = A^T(xy + zw) \\end{align}\\] 2.3.2 Dot Product We use the following formula to compute for the dot product of two arbitrary vectors : \\[\\begin{align} \\vec{a} \\cdotp \\vec{b} = \\|a\\| \\times \\|b\\| \\times cos(\\theta) \\end{align}\\] From Figure 2.3, given vector \\(\\mathbf{\\vec{a}}\\), \\(\\left[\\begin{array}{c} 1 \\\\ 3 \\end{array}\\right]\\), and vector \\(\\mathbf{\\vec{b}}\\), \\(\\left[\\begin{array}{c} 4 \\\\ 1 \\end{array}\\right]\\): \\[\\begin{align*} \\vec{a} \\cdotp \\vec{b} {} &amp;= \\|a\\| \\times \\|b\\| \\times cos(\\theta) \\\\ &amp;= \\sqrt{1^2 + 3^2} \\times \\sqrt{4^2 + 1^2} \\times cos(\\theta)\\\\ &amp;= \\sqrt{10} \\times \\sqrt{17} \\times cos(57.53^{\\circ} \\times (\\pi/180)) \\\\ &amp;= 7 \\end{align*}\\] Figure 2.3: Dot Product The following formula is another method to get the dot product of two vectors. First, we multiply the corresponding entries of vectors, and then we add all the resulting products. \\[\\begin{align} \\vec{a} \\cdotp \\vec{b} = \\sum_{i=1}^n \\left( a_i \\times b_i \\right) \\label{eqn:eqnnumber51} \\end{align}\\] Using the same vectors from Figure 2.3, we get the same dot product result: \\[ \\vec{a} \\cdotp \\vec{b} = a_{1} \\times b_{1} + a_{2} \\times b_{2} = 1 \\times 3 + 4 \\times 1 = 7 \\] Generally, given vector \\(\\mathbf{\\vec{a}}\\) and vector \\(\\mathbf{\\vec{b}}\\): \\[ \\mathbf{\\vec{a}} = \\left[\\begin{array}{c} a_{1} \\\\ a_{2} \\\\ \\vdots \\\\ a_{n} \\end{array}\\right], \\mathbf{\\vec{b}} = \\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{array}\\right] \\] Note that we use the transposed row vector to multiply with a column vector. \\[ \\mathbf{\\vec{a}}^T \\cdotp \\mathbf{\\vec{b}} = \\left[\\begin{array}{ccccc} a_{1} &amp; a_{2} &amp; \\cdots &amp; b_{n} \\end{array}\\right] \\cdotp \\left[\\begin{array}{c} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{n} \\end{array}\\right] \\rightarrow \\ \\ \\ \\ \\ a_{1}b_{1} + a_{2}b_{2} + ... a_{n}b_{n} \\] Example: \\[ \\mathbf{\\vec{a}}^T \\cdotp \\mathbf{\\vec{b}} \\ \\ \\ =\\ \\ \\ \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\end{array}\\right] \\cdotp \\left[\\begin{array}{c} 4 \\\\ 5 \\\\ 6 \\end{array}\\right] \\ \\ \\ = \\ \\ \\ \\ \\ (1)(4) + (2)(5) + (3)(6)\\ \\ \\ =\\ \\ \\ 32 \\] We get a scalar value of 32. If we switch and use the column vector first to multiply to the transposed row vector, we get a 3x3 matrix instead of a scalar result. \\[ \\mathbf{\\vec{b}} \\cdotp \\mathbf{\\vec{a}}^T \\ \\ \\ =\\ \\ \\ \\ \\left[\\begin{array}{ccc} 4 \\\\ 5 \\\\ 6 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\end{array}\\right] \\ \\ \\ = \\left[\\begin{array}{rrrr} 4 &amp; 8 &amp; 12 \\\\ 5 &amp; 10 &amp; 15 \\\\ 6 &amp; 12 &amp; 18 \\end{array}\\right] \\] If we expand the \\(\\mathbf{\\vec{b}}\\) and \\(\\mathbf{\\vec{a}}\\) vectors into 3x3 matrices, we get the following dot product based on matrix multiplication: \\[ \\mathbf{\\vec{b}} \\cdotp \\mathbf{\\vec{a}}^T \\ \\ \\ \\ = \\left[\\begin{array}{ccccc} 4 &amp; 0 &amp; 0 \\\\ 5 &amp; 0 &amp; 0 \\\\ 6 &amp; 0 &amp; 0 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccccc} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right] \\ \\ \\ = \\left[\\begin{array}{rrrr} 4 &amp; 8 &amp; 12 \\\\ 5 &amp; 10 &amp; 15 \\\\ 6 &amp; 12 &amp; 18 \\end{array}\\right] \\] To perform matrix multiplication given matrix A and B: \\[ \\mathbf{A} = \\left[\\begin{array}{ccccc} a_1 &amp; a_2 &amp; a_3 \\\\ a_4 &amp; a_5 &amp; a_6 \\\\ a_7 &amp; a_8 &amp; a_9 \\\\ \\end{array}\\right],\\ \\ \\ \\ \\mathbf{B} = \\left[\\begin{array}{ccccc} b_1 &amp; b_2 &amp; b_3 \\\\ b_4 &amp; b_5 &amp; b_6 \\\\ b_7 &amp; b_8 &amp; b_9 \\\\ \\end{array}\\right] \\] first, we take the individual rows of A, and the individual columns of B and perform the following computation: \\[ \\mathbf{A_{1st\\ row}} = \\left[\\begin{array}{ccccc} a_1 &amp; a_2 &amp; a_3\\end{array}\\right],\\ \\ \\ \\mathbf{B_{1st\\ col}} = \\left[\\begin{array}{ccccc} b_1 \\\\ b_4 \\\\ b_7 \\end{array}\\right] \\rightarrow (a_1 \\times b_1 + a_2 \\times b_4 + a_3 \\times b_7) \\] \\[ \\mathbf{A_{1st\\ row}} = \\left[\\begin{array}{ccccc} a_1 &amp; a_2 &amp; a_3\\end{array}\\right],\\ \\ \\ \\mathbf{B_{2nd\\ col}} = \\left[\\begin{array}{ccccc} b_2 \\\\ b_5 \\\\ b_8 \\end{array}\\right] \\rightarrow (a_1 \\times b_2 + a_2 \\times b_5 + a_3 \\times b_8) \\] \\[ \\mathbf{A_{1st\\ row}} = \\left[\\begin{array}{ccccc} a_1 &amp; a_2 &amp; a_3\\end{array}\\right],\\ \\ \\ \\mathbf{B_{3rd\\ col}} = \\left[\\begin{array}{ccccc} b_3 \\\\ b_6 \\\\ b_9 \\end{array}\\right] \\rightarrow (a_1 \\times b_3 + a_2 \\times b_6 + a_3 \\times b_9) \\] We then repeat the process for the 2nd row of A and the 3rd row of A. The resulting dot product matrix is: \\[ \\mathbf{A \\cdotp B} = \\left[\\begin{array}{ccccc} (a_1b_1 + a_2 b_4 + a_3b_7) &amp; (a_1b_2 + a_2b_5 + a_3b_8) &amp; (a_1b_3 + a_2b_6 + a_3b_9)\\\\ (a_4b_1 + a_5b_4 + a_6b_7) &amp; (a_4b_2 + a_5b_5 + a_6 b_8) &amp; (a_4b_3 + a_5b_6 + a_6 b_9)\\\\ (a_7b_1 + a_8 b_4 + a_9 b_7) &amp; (a_7b_2 + a_8 b_5 + a_9 b_8) &amp; (a_7b_3 + a_8 b_6 + a_9 b_9)\\\\ \\end{array}\\right] \\] In the case above, we get the following computation: \\[\\begin{align*} \\mathbf{B \\cdotp A^T} {}&amp; = \\left[\\begin{array}{ccccc} (4 \\times 1 + 0 \\times 0 + 0 \\times 0) &amp; (4 \\times 2 + 0 \\times 0 + 0 \\times 0) &amp; (4 \\times 3 + 0 \\times 0 + 0 \\times 0)\\\\ (5 \\times 1 + 0 \\times 0 + 0 \\times 0) &amp; (5 \\times 2 + 0 \\times 0 + 0 \\times 0) &amp; (5 \\times 3 + 0 \\times 0 + 0 \\times 0)\\\\ (6 \\times 1 + 0 \\times 0 + 0 \\times 0) &amp; (6 \\times 2 + 0 \\times 0 + 0 \\times 0) &amp; (6 \\times 3 + 0 \\times 0 + 0 \\times 0)\\\\ \\end{array}\\right] \\\\ \\\\ \\mathbf{B \\cdotp A^T} &amp; = \\left[\\begin{array}{ccccc} 4 &amp; 8 &amp; 12 \\\\ 5 &amp; 10 &amp; 15 \\\\ 6 &amp; 12 &amp; 18 \\end{array}\\right] \\end{align*}\\] 2.3.3 Hadamard Product Hadamard product is an element-wise multiplication of two arbitrary matrix. See the following: \\[ \\left[\\begin{array}{ccccc} a &amp; b\\\\ c &amp; d \\end{array}\\right] \\circ \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] = \\left[\\begin{array}{ccccc} ae &amp; bf\\\\ cd &amp; dh \\end{array}\\right] \\] 2.3.4 Kronecker Product Kronecker product is an element-to-matrix multiplication of two arbitrary matrix to obtain a block matrix. See the following: \\[ \\left[\\begin{array}{ccccc} a &amp; b\\\\ c &amp; d \\end{array}\\right] \\otimes \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] = \\left[\\begin{array}{cc} a \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] b \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] \\\\ c \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] d \\left[\\begin{array}{ccccc} e &amp; f\\\\ g &amp; h \\end{array}\\right] \\end{array} \\right] = \\left[ \\begin{array}{cccc} ae &amp; af &amp; be &amp; bf \\\\ ag &amp; ah &amp; bg &amp; bh \\\\ ce &amp; cf &amp; de &amp; df \\\\ cg &amp; ch &amp; dg &amp; dh \\\\ \\end{array} \\right] \\] 2.4 Magnitude, Direction, Unit Vectors Vectors can be represented geometrically using a 2-D or a 3-D Cartesian coordinate system. Direction: The direction of a vector is where the arrow points. It is measured using the angle of a vector in a counter-clockwise direction. Figure 2.4 shows 4 vectors, {U,V,W,X}, in a 2D coordinate system. Vector U shows an angle that stretches counter-clockwise from the angle zero at the x-axis to the vector U in the 1st quadrant of the Cartesian plane. Similarly, vector V shows an angle that stretches from angle 0 to vector V in the 2nd quadrant of the Cartesian plane. Figure 2.4: Vectors in 2D plane Figure 2.5 shows a vector, {U}, in a 3D coordinate system. The direction of vector U shows a counter-clockwise rotation of the vector and can only be reflected based on the projected vectors on the three 2d planes (X-Y plane, Y-Z plane, Z-X plane). There are three projections of vector U: projection of U to the X-Y plane. The angle to the projection is the direction in that plane. projection of U to the Y-Z plane. The angle to the projection is the direction in that plane. projection of U to the Z-X plane. The angle to the projection is the direction in that plane. Figure 2.5: A vector in 3D plane Using the point coordinates of the unit vector Another way to determine the direction of a vector is to normalize the vector into its unit vector and use its point coordinates. Magnitude: The magnitude of a vector is its length. In euclidean space, given an n-dimension vector, \\(\\mathbf{\\vec{v}}\\) = {\\(x_1, x_2, ..., x_n\\)}, the magnitude is computed based on the following formula: \\[ magnitude(v) = \\|v\\| = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2 } \\] Therefore, vector U in Figure 2.5 has the following magnitude: \\[ magnitude(U) = \\|U\\| = \\sqrt{3^2 + 4^2 + 5^2 } = 50 \\] Unit Vectors: Unit vectors are vectors that have magnitude of 1. Figure 2.6 shows two unit vectors, \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\) in 2D coordinate system and three unit vectors, \\(\\mathbf{\\vec{i}}\\), \\(\\mathbf{\\vec{j}}\\), and \\(\\mathbf{\\vec{k}}\\) in 3D coordinate system. Figure 2.6: Unit Vectors The magnitude of the unit vector \\(\\mathbf{\\vec{i}}\\), \\(\\left[\\begin{array}{cc} 1 \\\\ 0\\end{array}\\right]\\), in the 2D coordinate system, is computed as such: \\[ Magnitude\\ of\\ i = ||i|| = \\sqrt{1^2 + 0^2 } = 1 \\] The magnitude of the unit vector \\(\\mathbf{\\vec{i}}\\), \\(\\left[\\begin{array}{ccc} 1 \\\\ 0 \\\\ 0 \\end{array}\\right]\\), in the 3D coordinate system, is computed as such: \\[ Magnitude\\ of\\ i = ||i|| = \\sqrt{1^2 + 0^2 + 0^2} = 1 \\] To compute for the unit vector, \\(\\mathbf{\\vec{u}}\\), of a vector, \\(\\mathbf{\\vec{v}}\\), we use the following formula: \\[ u = \\frac{v}{||v||} \\] Therefore, given a vector, \\(\\mathbf{\\vec{v}}\\) = \\(\\left[\\begin{array}{ccc} 4.0 \\\\ 2.5 \\end{array}\\right]\\), to derive the unit vector, we first compute for the magnitude of \\(\\mathbf{\\vec{v}}\\): \\[ |v|| = \\sqrt{4.0^2 + 2.5^2 } = 4.716991 \\] Now, let’s get the unit vector: \\[ u = \\frac{v}{||v||} = \\frac{1}{||v||} * v = \\frac{1}{4.716991} * \\left[\\begin{array}{ccc} 4.0 \\\\ 2.5 \\end{array}\\right] = \\left[\\begin{array}{ccc} \\frac{4.0}{4.716991} \\\\ \\frac{2.5}{4.716991} \\end{array}\\right] = \\left[\\begin{array}{ccc} 0.85 \\\\ 0.53 \\end{array}\\right] \\] 2.5 Linear Combination and Independence We explain linear combination and linear independence using vectors. Linear Combination: A linear combination is the sum of the product of scalar coefficients and their respective vectors so that given a set of scalar coefficients \\(\\{c_1, c_2, c_3,..., c_{n}\\}\\) and the corresponding vectors \\(\\{v_1, v_2, v_3,..., v_{n}\\}\\), we get the following formula: \\[ c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} + ... + c_{n}v_{n} = \\mathbb{R}^n \\] Here, coefficients may represent any useful values that can be interpreted as scale factors for the vectors or their weights. Figure 2.7: Linear Combination For example, vector V1 in Figure 2.7 is a new vector derived from a linear combination of vectors U and W, 5(j) + 2(i), producing the following entries, \\(\\left[\\begin{array}{ccccc} 2 \\\\ 5 \\end{array}\\right]\\). The result is based on the following computation: \\[ U + W = 5(j) + 2(i) = 5\\left[\\begin{array}{ccccc} 0 \\\\ 1 \\end{array}\\right] + 2\\left[\\begin{array}{ccccc} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 0 \\\\ 5 \\end{array}\\right] + \\left[\\begin{array}{ccccc} 2 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 2 \\\\ 5 \\end{array}\\right] \\] Here, j (j-hat) and i (i-hat) are unit vectors. Using those two unit vectors, we can scale them by multiplying them with a scalar value so that 5(j) means that we are scaling the j unit vector by 5, e.g. \\(5\\left[\\begin{array}{ccccc} 0 \\\\ 1 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 0 \\\\ 5 \\end{array}\\right]\\). In the Cartesian plane, 5(j) vector stretches from the x-y coordinate (0,0) to (0,5). For another example, vector V2 in Figure 2.7 is a linear combination of 4(j) + 3(i) resulting in \\(\\left[\\begin{array}{ccccc} 3 \\\\ 4 \\end{array}\\right]\\) which is based on the following computation: \\[ 4(j) + 3(i) = 4\\left[\\begin{array}{ccccc} 0 \\\\ 1 \\end{array}\\right] + 3\\left[\\begin{array}{ccccc} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 0 \\\\ 4 \\end{array}\\right] + \\left[\\begin{array}{ccccc} 3 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 3 \\\\ 4 \\end{array}\\right] \\] Linear Independence: Using Figure 2.8, we have three unit vectors, \\(\\mathbf{\\vec{i}}\\), \\(\\mathbf{\\vec{j}}\\), and \\(\\mathbf{\\vec{k}}\\). Unit vector i rests along the x-axis. Unit vector \\(\\mathbf{\\vec{j}}\\) rests along the y-axis. And unit vector \\(\\mathbf{\\vec{k}}\\) is rotated counter-clockwise from x-axis to about 53.13° so that if we scale \\(\\mathbf{\\vec{k}}\\) by 5, we get vector \\(\\mathbf{\\vec{W}}\\), \\(\\left[\\begin{array}{ccccc} 4 \\\\ 3 \\end{array}\\right]\\). Vector W is derived based on the following computation: \\[ 5 \\times k = 5\\left[\\begin{array}{ccccc} sin(53.13^{\\circ} ) \\\\ cos(53.13^{\\circ} ) \\end{array}\\right] = \\left[\\begin{array}{ccccc} 4 \\\\ 3 \\end{array}\\right] \\] Figure 2.8: Linear Dependence If we add the two vectors, U + W, we get the following linear combination, V1 = \\(\\left[\\begin{array}{l} 4 \\\\ 5 \\end{array}\\right]\\): \\[ U + W = 2(j) + 5(k) = 2\\left[\\begin{array}{l} 0 \\\\ 1 \\end{array}\\right] + 5\\left[\\begin{array}{l} 0.8 \\\\ 0.6 \\end{array}\\right] = \\left[\\begin{array}{l} 0 \\\\ 2 \\end{array}\\right] + \\left[\\begin{array}{l} 4 \\\\ 3 \\end{array}\\right] = \\left[\\begin{array}{l} 4 \\\\ 5 \\end{array}\\right] \\] On the other hand, if we add the two vectors, U + V, we get the following linear combination, V2 = \\(\\left[\\begin{array}{ccccc} 5 \\\\ 2 \\end{array}\\right]\\): \\[ U + V = 2(j) + 5(i) = 2\\left[\\begin{array}{ccccc} 0 \\\\ 1 \\end{array}\\right] + 5\\left[\\begin{array}{ccccc} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 0 \\\\ 2 \\end{array}\\right] + \\left[\\begin{array}{ccccc} 5 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 5 \\\\ 2 \\end{array}\\right] \\] It can be said that for a vector to be a linear combination, it needs two other vectors. Here, vector V1 is expressed as a linear combination of vectors U and W. Any vector expressed as a linear combination is considered to be a linearly dependent vector. And any vector that is used for the summation to arrive at a linear combination is a linearly independent vector. Vector V1, as a linear combination vector, is linearly dependent on vectors U and W, whereas vectors U and W are linearly independent vectors used for the linear combination. Similarly, vector V2, as a linear combination vector, is linearly dependent on vectors U and V, whereas vectors U and V are linearly independent vectors used for the linear combination. Now, let us pause for a moment and ask: is vector V2 a linear combination of vectors U and W? The answer is No. Vector V2 is not a linearly dependent vector based on U and W. In fact, if we use the unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{k}}\\) as our basis for choosing a set of linearly independent vectors, no matter how much we scale (multiply scalars) to the unit vectors, we will not be able to get a linear combination that is vector V2. The vector V2 is completely unreachable using \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{k}}\\) vectors in any linear combination. \\[ c1 \\times j + c2 \\times i \\rightarrow &lt;any\\ linearly\\ dependent\\ vector&gt; \\] On the other hand, V1 is actually reachable by using unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{i}}\\). If we scale \\(\\mathbf{\\vec{j}}\\) by 5, and scale \\(\\mathbf{\\vec{i}}\\) by 4, we will get V1. \\[ v1 = 5(j) + 4(i) = 5\\left[\\begin{array}{ccccc} 0 \\\\ 1 \\end{array}\\right] + 4\\left[\\begin{array}{ccccc} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 0 \\\\ 5 \\end{array}\\right] + \\left[\\begin{array}{ccccc} 4 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{ccccc} 4 \\\\ 5 \\end{array}\\right] \\] Given all that, it can be said that V2 is not in the span of unit vectors j and k. But that V1 and V2 are both in the span of unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{i}}\\). Now, if we, however, closely compare the linear combination, V1, using two different unit vectors, we get two different entries for the vector. Using unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{k}}\\) as our basis: \\[ v1 = 2(j) + 5(k) = \\left[\\begin{array}{l} 4 \\\\ 5 \\end{array}\\right] \\] Using unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{i}}\\) as our basis: \\[ v1 = 5(j) + 4(i) = \\left[\\begin{array}{c} 4 \\\\ 5 \\end{array}\\right] \\] One additional concept to note: To determine if a set of vectors, \\(\\{v_1, v_2, v_3, ..., v_n\\}\\), are linearly dependent, if there are scalars \\(\\{c_1, c_2, c_3, ..., c_n\\}\\), with at least one being non-zero, then we can define the following formula: \\[ c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} + ... + c_{n}v_{n} = 0 \\] Suppose one of the vectors is a linear combination - linearly dependent - of the others, arbitrarily using v1 as an example. Using the above formula, we can extract \\(c_{1}v_{1}\\) such that: \\[ c_{2}v_{2} + c_{3}v_{3} + ... + c_{n}v_{n} = -c_{1}v_{1} \\] For example, given a system of 3 linear equations: \\[\\begin{align*} 1x + 3y + 6z= 0\\\\ 2x + 7y + 3z= 0\\\\ 0x + 1y - 9z = 0 \\end{align*}\\] We form the following augmented matrix : \\[ A = x\\left[\\begin{array}{r} 1 \\\\ 2 \\\\ 0 \\end{array}\\right] + y\\left[\\begin{array}{r} 3 \\\\ 7 \\\\ 1 \\end{array}\\right] + z\\left[\\begin{array}{r} 6 \\\\ 3 \\\\ -9 \\end{array}\\right] = \\left[\\begin{array}{rrr} 1 &amp; 3 &amp; 6 \\\\ 2 &amp; 7 &amp; 3 \\\\ 0 &amp; 1 &amp; - 9 \\end{array}\\left|\\begin{array}{r}0 \\\\ 0 \\\\ 0 \\end{array}\\right. \\right] \\] In the later section, we talk about how to convert a matrix to its row-echelon form. We also talk about pivot columns and free variables. But for now, let us show the row-echelon form of matrix A. Here, we let a free variable, z, take any non-zero arbitrary value, say z = 1 (or for now, let us use z = a where \\(z \\ne 0\\)): \\[ \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 33 \\\\ 0 &amp; 1 &amp; -9 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\left|\\begin{array}{r}0 \\\\ 0 \\\\ 0 \\end{array}\\right. \\right] \\rightarrow \\left[\\begin{array}{rrr} x - 33\\ a \\\\ y + 9\\ a \\\\ z \\end{array}\\left|\\begin{array}{r} 0 \\\\ 0 \\\\ a \\end{array}\\right. \\right] \\rightarrow \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} -33\\ a \\\\ 9\\ a \\\\ \\ a \\end{array}\\right] \\] In other words, we have the following: \\[ c_{1}v_{1} + c_{2}v_{2} + c_{3}v_{3} \\rightarrow x \\times v_1 + y \\times v_2 + z \\times v_3 \\rightarrow x = -33a, y = 9a, z=a \\] 2.6 Space, Span, and Basis Basis: In systems of linear equations, a basis is a set of all linearly independent vectors that are in the system. Any linear combination consists of independent linear vectors, also called basis vectors. A matrix B is called a basis if its elements are all basis vectors. None of those vectors depend on other independent vectors to become linearly dependent in the basis. It also means none of the vectors is a multiple of the others. \\[ B= \\{b_1, b_2, ..., b_n\\} \\] Space: There is probably nothing interesting about combining two linearly independent vectors, say vectors \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\) to get just one dependent vector (one linear combination). But what if we are interested to know all the possible linear combinations - all the potential dependent vectors we can derive out of any (or all possible) scales of those two linearly independent vectors. For example, if we use two unit vectors \\(\\mathbf{\\vec{j}}\\) and \\(\\mathbf{\\vec{i}}\\) as our basis vectors, and scale each one, we may get some arbitrary linear combination of vectors such as the following: scaling \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\) by 2 gives us: 2(\\(\\mathbf{\\vec{i}}\\) + \\(\\mathbf{\\vec{j}}\\)) = [2,2] scaling \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\) by 3 gives us: 3(\\(\\mathbf{\\vec{i}}\\) + \\(\\mathbf{\\vec{j}}\\)) = [3,3] scaling \\(\\mathbf{\\vec{i}}\\) by 2 and \\(\\mathbf{\\vec{j}}\\) by 3 gives us: 2(\\(\\mathbf{\\vec{i}}\\)) + 3(\\(\\mathbf{\\vec{j}}\\)) = [2,3] scaling \\(\\mathbf{\\vec{i}}\\) by 3 and \\(\\mathbf{\\vec{j}}\\) by 2 gives us: 3(\\(\\mathbf{\\vec{i}}\\)) + 2(\\(\\mathbf{\\vec{j}}\\)) = [3,2] scaling \\(\\mathbf{\\vec{i}}\\) by 20 and \\(\\mathbf{\\vec{j}}\\) by 50 gives us: 20(\\(\\mathbf{\\vec{i}}\\)) + 50(\\(\\mathbf{\\vec{j}}\\)) = [20,50] scaling \\(\\mathbf{\\vec{i}}\\) by -20 and \\(\\mathbf{\\vec{j}}\\) by -50 gives us: -20(\\(\\mathbf{\\vec{i}}\\)) + -50(\\(\\mathbf{\\vec{j}}\\)) = [-20,-50] We can go on and on and we may end up collecting a set of these linear combinations. That set of linear combinations forms what we call a vector space, V which forms a subspace in the \\(\\mathbb{R}^2\\) space. Recall that the system of equations is denoted by the following: \\[ A\\mathbf{\\vec{x}} = \\mathbf{\\vec{b}} \\] We describe vector \\(\\mathbf{\\vec{x}}\\) as the solution space . The intent is not just to solve for only one vector that is x and say that we have solved the equation. The intent is to assume that vector x can hold a set of all possible solutions - that set is called the solution space for the system. So, in other words, if the system is such that we have: \\[ A\\mathbf{\\vec{x}} \\neq 0 \\ \\ \\ \\ \\ \\ where\\ \\mathbf{\\vec{b}} \\neq 0 \\] what we have done is to constrain or limit our system of linear equations such that vector x holds only a set of solutions for which \\(\\mathbf{\\vec{b}} \\ne 0\\). That set of solutions for which \\(\\mathbf{\\vec{b}} \\ne 0\\) is our solution space for which the system is \\(A\\mathbf{\\vec{x}} \\neq 0\\). Therefore, if \\(\\mathbf{\\vec{x}}\\) is a solution to the system, \\(A\\mathbf{\\vec{x}} \\neq 0\\), it also means that the corresponding vector \\(\\mathbf{\\vec{b}}\\) is a linear combination of the column space , \\(C(A)\\). On the other hand, if the system is such that we have: \\[ A\\mathbf{\\vec{x}} = 0 \\ \\ \\ \\ \\ \\ where\\ \\mathbf{\\vec{b}} = 0 \\] what we have done is to constrain or limit our system of linear equations such that vector x holds only a set of solutions for which \\(b = 0\\). That set of solutions for which \\(\\mathbf{\\vec{b}} = 0\\) is our solution space for which the system is \\(A\\mathbf{\\vec{x}} = 0\\). Lastly, the row space of a matrix is the column space of its transpose - \\(C(A^T)\\). And the left null-space of a matrix is the null space of its transpose - \\(N(A^T)\\). Therefore, the following subspace dimensions are like so: A Vector Space is a set of all linear combinations in a system, denoted by V(A). A Column Space is a set of all linearly independent vectors of a system’s matrix, denoted as C(A). See rank and nullity in later section. A Null-space is a set of all linearly dependent vectors of a system’s matrix, denoted as N(A) or Nul(A) or Ker(A). See rank and nullity in later section. A Solution Space is a set of all possible solutions of a system. The solution space in a system for which \\(A\\mathbf{\\vec{x}} \\neq 0\\) is different from the solution space in a system for which \\(A\\mathbf{\\vec{x}} = 0\\). a Row Space is a Column space of a system’s matrix transpose, denoted by \\(C(A^T)\\). a Left Null-space is a Null-space of a system’s matrix transpose, denoted by \\(N(A^T)\\). Span: A Vector Space is limited only by the number of possible linear combinations and will grow in space as long as the unit vectors have enough unique coefficients to scale them. To grow the vector space is to span the basis. In another way to put it, the amount of space covered in V is the span of the basis B and can be denoted as: \\[ span\\{B\\} = V \\] The longer version is: \\[ span\\{b_1, b_2, ..., b_m\\} = \\{v_1, v_2, v_3, ..., v_n\\} \\] where {\\(b_1,b_2,..., b_m\\)} are all basis vectors of basis B and {\\(v_1, v_2, v_3, ..., v_n\\)} are all the linear combinations of those basis vectors, forming a vector space V which is spanned by the basis B. We therefore can say that Span is the set of linear combination of a vector space, V. For example: \\[ span\\ \\{ i\\} \\rightarrow \\text{span of basis vector i in 1D space} \\] For example: \\[ span\\ \\{ i, j\\} \\rightarrow \\text{span of basis vectors i and j in 2D space} \\] For example: \\[ span\\ \\{ i, j, k\\} \\rightarrow \\text{span of basis vectors i and j and k in 3D space} \\] It is necessary not to miss the point that each element in the notation span { \\(b_1, b_2, ...,b_m\\) } is a basis vector. In other words, each element is a linearly independent vector that can be paired (matched) with other linearly independent vectors to form any linear combination in the subspace. In other words, given a span S, with {\\(b_1, b_2, ..., b_m\\)}, all the vectors are basis vectors if they’re all linearly independent. It may also be necessary not to miss the point that basis vectors can best be represented by unit vectors because unit vectors are easier to point out as being scalable. Consider Figure 2.9, there are two unit vectors { \\(\\mathbf{\\vec{i}}, \\mathbf{\\vec{j}}\\) } used as basis vectors. In the example figure, the span of those two basis vectors, written as span { \\(\\mathbf{\\vec{i}}, \\mathbf{\\vec{j}}\\)}, covers a finite set of 16 unique linear combinations - based on \\(c_1 * i + c_2 * j\\), using different combination of coefficients (scales) forming a subspace V in \\(\\mathbb{R}^2\\) space whose vectors { \\(v_1, v_2, ..., v_{16}\\) } in the subspace are linearly dependent: \\[ span\\ \\{ i, j\\} \\rightarrow c_1 \\times i + c_2 \\times j \\rightarrow \\{ v_1, v_2, v_3, ..., v_{16}\\} \\in \\mathbb{R}^2 \\] Figure 2.9 shows that the basis vectors, unit vectors { \\(\\mathbf{\\vec{i}}, \\mathbf{\\vec{j}}\\) }, can be scaled by multiplying each one with any possible scalar value (any possible coefficients) and therefore the resulting possibility of any linear combination can span across the entire plane in the 2D coordinate system. Figure 2.9: Span 2D Now, compare that with Figure 2.10. The basis vectors, unit vectors { \\(\\mathbf{\\vec{i}}, \\mathbf{\\vec{j}}\\) }, are rotated such that they are now squashed together into one dimension space, \\(\\mathbb{R}^1\\), both are fitting along a diagonal line (instead of a 2D plane). Because \\(\\mathbf{\\vec{j}}\\) = [1,1] and \\(\\mathbf{\\vec{i}}\\) = [1,1], no matter how much we scale any one or both of those basis vectors, the scaled version of the vectors will always fall on the same diagonal line. In this condition of the figure, the span of those two unit vectors, written as span { \\(\\mathbf{\\vec{i}}, \\mathbf{\\vec{j}}\\)}, yields a set of 10 unique linear combinations - based on \\(c_1 \\times i + c_2 \\times j\\), forming a subspace V in \\(\\mathbb{R}^1\\) space whose vectors { \\(v_1, v_2, ..., v_{10}\\) } in the subspace are linearly dependent. Notice that there is a rogue vector U not belonging to the subspace. No matter how we scale the basis vectors, the span never reaches whatever subspace the vector U belongs to. In this situation, we can assume that the vector U must be a separate independent vector, having its span, and can therefore scale on its own to form its subspace in \\(\\mathbb{R}^1\\) space. Vector U can also partner with any of the unit vectors \\(\\mathbf{\\vec{i}}\\) or \\(\\mathbf{\\vec{j}}\\) to form a new span and thus a new subspace in \\(\\mathbb{R}^2\\). Figure 2.10: Span 2D 2.7 Determinants Let us use vectors first to explain determinants. To get an insight into a determinant, we may need to draw two vectors in a 2D Cartesian plane; each vector reflected at the tip of the other to form a geometric area (or region) similar to Figure 2.11. Figure 2.11: Determinant In Figure 2.11, there are 6 sample linear combinations with corresponding determinants. A quick view of the figure may show a relationship between the determinant and the area of the closed region formed between two basis vectors. That being so, a determinant is a scaling factor - \\(\\mathbb{R}^2\\) for a 2D plane - of a given unit area (e.g., the unit area formed by the unit vectors \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\)). We start with a unit area by computing for the magnitude of unit vectors \\(\\mathbf{\\vec{i}}\\) and \\(\\mathbf{\\vec{j}}\\): \\[ Area_{\\{i\\ and\\ j\\}} = |\\mathbf{\\vec{i}}| \\times |\\mathbf{\\vec{j}}| = \\sqrt{1^2 + 0^1} \\times \\sqrt{0^2 + 1^1} = 1 * 1 = 1 \\] In the first coordinate system, we scaled the magnitudes of the unit vectors using (3,1). Then we compute the area. \\[ Area_{\\{i\\ and\\ j\\}} = 3|\\mathbf{\\vec{i}}| \\times 1|\\mathbf{\\vec{j}}| = 3(1) \\times 1(1) = 3 \\] In the second coordinate system, we scaled the magnitudes of the unit vectors using (3,2). Then we compute the area. \\[ Area_{\\{i\\ and\\ j\\}} = 3|\\mathbf{\\vec{i}}| \\times 2|\\mathbf{\\vec{j}}| = 3(1) \\times 2(1) = 6 \\] In the fourth coordinate system, we rotated unit vector \\(\\mathbf{\\vec{j}}\\) by 45 degrees and that gives a different coordinate, \\(\\left[\\begin{array}{c} 0.707 \\\\ 0.707 \\end{array}\\right]\\). It also gives us a parallelogram shape instead of a rectangular shape. Nonetheless, we will still compute for the area as if the shape is rectangular. Now for the area of the parallelogram, first, we need to compute for the magnitude of the projected version of the unit vector \\(\\mathbf{\\vec{j}}\\) using: \\(|\\mathbf{\\vec{j}}| * sin(45^{\\circ})\\). See Figure 2.12. Figure 2.12: Compute for Adjacent \\[ a = |\\mathbf{\\vec{j}}| \\times sin(45^{\\circ}) = (1) (0.707) = 0.707 \\] Then we compute for the area. \\[ Area_{\\{\\mathbf{\\vec{i}}\\ and\\ \\mathbf{\\vec{j}}\\}} = 3|\\mathbf{\\vec{i}}| \\times 2a = 3(1) \\times 2(0.707) = 4.242 \\] To validate the result, let us use a different way to compute the area. We will calculate the determinant using the Leibnitz formula. \\[\\begin{align*} Given: 3 \\times \\mathbf{\\vec{i}} {}&amp; = 3\\left[\\begin{array}{cc} 1 \\\\ 0 \\end{array}\\right] = \\left[\\begin{array}{cc} 3 \\\\ 0 \\end{array}\\right]\\\\ Given: 2 \\times \\mathbf{\\vec{j}} &amp; = 2\\left[\\begin{array}{cc} 0.707 \\\\ 0.707 \\end{array}\\right] = \\left[\\begin{array}{cc} 1.414 \\\\ 1.414 \\end{array}\\right] \\end{align*}\\] We get the matrix form: \\[ A = \\left[\\begin{array}{cc} 3 &amp; 1.414 \\\\ 0 &amp; 1.414 \\end{array}\\right] \\] Now computing for the determinant using the Leibniz formula, we get: \\[ det(A) = 3 \\times 1.414 - 0 \\times 1.414 = 3 \\times 1.414 = 4.242 \\] The computed area of the parallelogram ends up being equal to the computed determinant using the Leibniz formula. In 2D space, if two vectors span a line (transformed into a lower dimension, e.g., 1D), then it is said that the two vectors are linearly dependent on each other, and thus the determinant is zero. The area turns into a line or even into a point. In 3D space, if three vectors span a plane (transformed into a lower dimension, e.g., 2D), then it is said that the three vectors are linearly dependent on one another, and thus the determinant is zero. The volume turns into an area, or even into a line, or even into a point. If a determinant is negative, the space is flipped over. See Figure 2.13. Figure 2.13: Flipped over Methods in computing for determinants: Let us recall a few formulas that help to compute for determinants: Leibniz Formula Given the following 2x2 matrix ( or two vectors ): \\[ A = \\left[\\begin{array}{cc} a &amp; b \\\\ c &amp; d \\end{array}\\right]_{2 \\times 2} \\] we get the following Leibniz formula: \\[\\begin{align} det(A) = |A| = a \\times d - b \\times c \\label{eqn:eqnnumber52} \\end{align}\\] Notice that the determinant of a matrix uses the following notation \\(|A|\\). Laplace Formula Given the following 3x3 matrix ( or three vectors ): \\[ A = \\left[\\begin{array}{ccccc} a &amp; b &amp; c\\\\ d &amp; e &amp; f \\\\g &amp; h &amp; i \\end{array}\\right]_{3 \\times 3} \\] the matrix gets expanded into: \\[ det(A) = |A| = a\\left|\\begin{array}{ccccc} e &amp; f \\\\ h &amp; i \\end{array}\\right| - b\\left|\\begin{array}{ccccc} d &amp; f \\\\ g &amp; i \\end{array}\\right| + c\\left|\\begin{array}{ccccc} d &amp; e \\\\ g &amp; h \\end{array}\\right| \\] so then, we get the following Laplace expansion formula \\[\\begin{align} det(A) = |A| = a \\times (e \\times i - f \\times h) - b \\times (d \\times i - f \\times g) + c (d \\times h - e \\times g) \\label{eqn:eqnnumber53} \\end{align}\\] The formula is recursive in that we can apply it to any \\(n\\times n\\) matrix, though computation gets inefficient with higher dimensions. As an example, for 4x4 matrix, see the following: \\[ A = \\left[\\begin{array}{ccccc} a &amp; b &amp; c &amp; d\\\\ e &amp; f &amp; g &amp; h \\\\i &amp; j &amp; k &amp; l \\\\ m &amp; n &amp; o &amp; p \\end{array}\\right]_{4 \\times 4} \\] Using Laplace formula, we get the expanded version: \\[ det(A) = |A| = a \\left|\\begin{array}{ccccc} f &amp; g &amp; h\\\\ j &amp; k &amp; l \\\\n &amp; o &amp; p \\end{array}\\right| - b \\left|\\begin{array}{ccccc} e &amp; g &amp; h\\\\ i &amp; k &amp; l \\\\m &amp; o &amp; p \\end{array}\\right| + c \\left|\\begin{array}{ccccc} e &amp; f &amp; h\\\\ i &amp; j &amp; l \\\\m &amp; n &amp; p \\end{array}\\right| - d \\left|\\begin{array}{ccccc} e &amp; f &amp; g\\\\ i &amp; j &amp; k \\\\m &amp; n &amp; o \\end{array}\\right| \\] Then compute for the determinant of each \\(3\\times 3\\) matrix using the Laplace expansion formula again. Sarrus Rule (Basketweave method) Sarrus Rule is another way to compute 3x3 matrices though it may not necessarily be reliable for higher dimensions. We compute by subtracting the sum of the products of the backward diagonal line from the sum of the product of the forward diagonal lines. Figure 2.14: Sarrus Rule Given a 3x3 matrix (A): \\[ det(A) = |A| = \\left|\\begin{array}{ccc} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\\\g &amp; h &amp; i \\end{array} \\left|\\begin{array}{cc}a &amp; b \\\\ d &amp; e\\\\ g &amp; h \\end{array}\\right. \\right| \\] We get: \\[ det(A) = |A| = ( a \\times e \\times i + b \\times f \\times g + c \\times d \\times h ) - ( c \\times e \\times g + a \\times f \\times h + b \\times d \\times i) \\] Gaussian and Gauss-Jordan Elimination Determinants, especially for matrices with higher dimensions (e.g., &gt;3) and non-square matrices, can be calculated based on converting matrices into their row-echelon form using Gaussian elimination. We introduce the method in the next section in the context of deriving the row-echelon form of a matrix. Moreover, Gaussian and Gauss-Jordan elimination is discussed later in this chapter in the context of matrix factorization. Use of Determinants One of the usefulness of determinants is in computing for system of equations using Cramer’s Rule. Given the following systems of equations: \\[\\begin{align*} 3x + 2y = 4 \\\\ 9x + 4y = 12 \\end{align*}\\] Express the system of equations in matrix form: \\[ A = \\left[\\begin{array}{rr} C_{x1} &amp; C_{y1} \\\\ C_{x2} &amp; C_{y2}\\end{array}\\left|\\begin{array}{r}C_{b1} \\\\ C_{b2} \\end{array}\\right.\\right] = \\left[\\begin{array}{rr} 3 &amp; 2 \\\\ 9 &amp; 5 \\end{array}\\left|\\begin{array}{r}4 \\\\ 12\\end{array}\\right.\\right] \\] Then, compute for the determinants using Leibniz formula: \\[\\begin{align*} det(A) {} &amp; = |A| = \\left|\\begin{array}{rr} C_{x1} &amp; C_{y1} \\\\ C_{x2} &amp; C_{y2} \\end{array}\\right| = \\left|\\begin{array}{rr} 3 &amp; 2 \\\\ 9 &amp; 5 \\end{array}\\right| = 3 \\times 5 - 2 \\times 9 = -3 \\\\ det(A_x) &amp; = |A_x| = \\left|\\begin{array}{rr} C_{b1} &amp; C_{y1} \\\\ C_{b2} &amp; C_{y2} \\end{array}\\right| = \\left|\\begin{array}{rr} 4 &amp; 2 \\\\ 12 &amp; 5 \\end{array}\\right| = 4 \\times 5 - 2 \\times 12 = -4 \\\\ det(A_y) &amp; = |A_y| = \\left|\\begin{array}{rr} C_{x1} &amp; C_{b1} \\\\ C_{x2} &amp; C_{b2} \\end{array}\\right| = \\left|\\begin{array}{rr} 3 &amp; 4 \\\\ 9 &amp; 12 \\end{array}\\right| = 3 \\times 12 - 4 \\times 9 = 0 \\\\ \\end{align*}\\] Now, solve for x and y: \\[\\begin{align*} x = \\frac{det(A_x)}{det(A)} = \\frac{|Ax|}{|A|} = \\frac{-4}{-3} = \\frac{4}{3} \\\\ y = \\frac{det(A_y)}{det(A)} = \\frac{|Ay|}{|A|} = \\frac{0}{-3} = 0 \\end{align*}\\] Note that there are other methods more efficient in computing for systems of equations than Cramer’s Rule. The chapter about numerical analysis focuses on optimized methods. It is also noteworthy to mention that a system of equation does not have a solution if the det(A) = 0 and \\(det(A_x) \\ne 0\\). That is, the \\(x = \\frac{|A_x|}{|A|}\\) cannot be solved because the denominator is \\(|A| = 0\\) For example, the following system of equations cannot be solved: \\[\\begin{align*} x + 2y = 6 \\\\ 3x + 6y = 4 \\end{align*}\\] That is because the determinant of the matrix is zero: \\[\\begin{align*} det(A) = |A| = \\left|\\begin{array}{rr} C_{x1} &amp; C_{y1} \\\\ C_{x2} &amp; C_{y2} \\end{array}\\right| = \\left|\\begin{array}{rr} 1 &amp; 2 \\\\ 3 &amp; 6 \\end{array}\\right| = 1 \\times 6 - 2 \\times 3 = 0 \\\\ \\end{align*}\\] Figure 2.15 shows that vector Y is a multiple - 2X that - of vector X. Both vectors point to the same direction and thus show a determinant of zero - geometrically, it has no area as it follows a line. Since Y and X can both be multiples of each other, x has infinite solution. Figure 2.15: Zero Determinant 2.8 Minors, Cofactors, and Adjugate Forms In the previous section, we introduced two methods that can help compute the determinants of matrices. Now, let us introduce minors, cofactors, and adjugates in this chapter. Minor Form: Consider a 3x3 matrix: \\[ A = \\left[\\begin{array}{ccc} a &amp; b &amp; c\\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{array}\\right]_{3 \\times 3} \\] The A matrix can be formed into an M matrix - the minor form of the matrix: \\[ M = \\left[\\begin{array}{ccc} M_{1,1} &amp; M_{1,2} &amp; M_{1,3} \\\\ M_{2,1} &amp; M_{2,2} &amp; M_{2,3} \\\\ M_{3,1} &amp; M_{3,2} &amp; M_{3,3} \\\\ \\end{array}\\right|_{3 \\times 3} = \\begin{pmatrix} \\left|\\begin{array}{rrr} e &amp; f \\\\ h &amp; i \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} d &amp; f \\\\ g &amp; i \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} d &amp; e \\\\g &amp; h \\end{array}\\right|_{2 \\times 2} \\\\ \\\\ \\left|\\begin{array}{rrr} b &amp; c \\\\ h &amp; i \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} a &amp; c \\\\ g &amp; i \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} a &amp; b \\\\ g &amp; h \\end{array}\\right|_{2 \\times 2} \\\\ \\\\ \\left|\\begin{array}{rrr} b &amp; c \\\\ e &amp; f \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} a &amp; c \\\\ d &amp; f \\end{array}\\right|_{2 \\times 2} &amp; \\left|\\begin{array}{rrr} a &amp; b \\\\ d &amp; e \\end{array}\\right|_{2 \\times 2} \\end{pmatrix}_{3 \\times 3} \\] Each of the elements in the M matrix is called a minor - \\(M_{i,j}\\) - which is the determinant of the corresponding sub-matrix. We remove the ith row and jth column of the A matrix, and the remaining rows and columns form the sub-matrix. For example, the first minor \\(M_{1,1}\\) yields the following: \\[ M_{1,1} = \\left|\\begin{array}{rrr} \\square &amp; \\square &amp; \\square \\\\ \\square &amp; e &amp; f \\\\ \\square &amp; h &amp; i \\end{array}\\right| = \\left|\\begin{array}{rrr} e &amp; f \\\\ h &amp; i \\end{array}\\right|_{2 \\times 2} = det(\\text{lower right submatrix}) = e \\times i - f \\times h \\] Another example, the minor \\(M_{2,3}\\) yields the following: \\[ M_{1,1} = \\left|\\begin{array}{rrr} a &amp; b &amp; \\square \\\\ \\square &amp; \\square &amp; \\square \\\\ g &amp; h &amp; \\square \\end{array}\\right| = \\left|\\begin{array}{rrr} a &amp; b \\\\ g &amp; h \\end{array}\\right|_{2 \\times 2} = det(\\text{lower right submatrix}) = a \\times h - b \\times g \\] The matrix can be expanded into its minor form: \\[ M = \\begin{pmatrix} \\left|\\begin{array}{rrr} \\square &amp; \\square &amp; \\square \\\\ \\square &amp; e &amp; f \\\\ \\square &amp; h &amp; i \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} \\square &amp; \\square &amp; \\square \\\\ d &amp; \\square &amp; f \\\\ g &amp; \\square &amp; i \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} \\square &amp; \\square &amp; \\square \\\\ d &amp; e &amp; \\square \\\\ g &amp; h &amp; \\square \\end{array}\\right| \\\\ \\\\ \\left|\\begin{array}{rrr} \\square &amp; b &amp; c \\\\ \\square &amp; \\square &amp; \\square \\\\ \\square &amp; h &amp; i \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} a &amp; \\square &amp; c\\\\ \\square &amp; \\square &amp; \\square \\\\ g &amp; \\square &amp; i \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} a &amp; b &amp; \\square \\\\ \\square &amp; \\square &amp; \\square \\\\ g &amp; h &amp; \\square \\end{array}\\right| \\\\ \\\\ \\left|\\begin{array}{rrr} \\square &amp; b &amp; c \\\\ \\square &amp; e &amp; f \\\\ \\square &amp; \\square &amp; \\square \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} a &amp; \\square &amp; c \\\\ d &amp; \\square &amp; f \\\\ \\square &amp; \\square &amp; \\square \\end{array}\\right| &amp; \\left|\\begin{array}{rrr} a &amp; b &amp; \\square \\\\ d &amp; e &amp; \\square \\\\ \\square &amp; \\square &amp; \\square \\end{array}\\right| \\end{pmatrix}_{3 \\times 3} \\] Cofactor Form: To convert the M matrix into the cofactor form - the C matrix - we multiply the minor elements of the M matrix by \\((-1)^{i + j}\\). For example, the cofactor \\(C_{1,1}\\) is derived based on \\((-1)^{1+1}M_{1,1}\\). Let us show the following general cofactor form of a matrix: \\[ C = \\left[\\begin{array}{ccccc} (-1)^{1+1}M_{1,1} &amp; (-1)^{1+2}M_{1,2} &amp; \\dots &amp; (-1)^{1+n}M_{1,n} \\\\ (-1)^{2+1}M_{2,1} &amp; (-1)^{2+2}M_{2,2} &amp; \\dots &amp; (-1)^{2+n}M_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ (-1)^{n+1}M_{n,1} &amp; (-1)^{n+2}M_{n,2} &amp; \\dots &amp; (-1)^{n+n}M_{n,n} \\end{array}\\right]_{n \\times n} \\] That yields the cofactor form of a 3x3 M matrix: \\[ C = \\left[\\begin{array}{ccccc} +M_{1,1} &amp; -M_{1,2} &amp; &amp; +M_{1,3} \\\\ -M_{2,1} &amp; +M_{2,2} &amp; &amp; -M_{2,3} \\\\ +M_{3,1} &amp; -M_{3,2} &amp; &amp; +M_{3,3} \\\\ \\end{array}\\right]_{3 \\times 3} = \\left[\\begin{array}{ccccc} C_{1,1} &amp; C_{1,2} &amp; &amp; C_{1,3} \\\\ C_{2,1} &amp; C_{2,2} &amp; &amp; C_{2,3} \\\\ C_{3,1} &amp; C_{3,2} &amp; &amp; C_{3,3} \\\\ \\end{array}\\right]_{3 \\times 3} \\] where \\(C_{1,1} = + M_{1,1}\\ \\ \\ \\ and \\ \\ \\ \\ \\ C_{1,2} = - M_{1,2}\\) Therefore, the cofactor form of matrix A is the following: \\[\\begin{align*} A &amp;= \\left[\\begin{array}{ccc} a &amp; b &amp; c\\\\ d &amp; e &amp; f \\\\ g &amp; h &amp; i \\end{array}\\right]_{3 \\times 3}\\\\ &amp;\\rightarrow C = \\left[\\begin{array}{ccccc} +(e \\times i - f \\times h) &amp; - (d \\times i - f \\times g) &amp; +(d \\times h - e \\times g) \\\\ -(b \\times i - c \\times h) &amp; + (a \\times i - c \\times g) &amp; - (a \\times h - b \\times g) \\\\ +(b \\times f - c \\times e) &amp; - (a \\times f - c \\times d) &amp; +(a \\times e - b \\times d) \\end{array}\\right]_{3 \\times 3} \\end{align*}\\] Adjugate (Adjoint) Form: To convert the C matrix to the adjugate form of the A matrix - Adj(A) - we transpose the C matrix. \\[\\begin{align} adj(A) = C^T \\label{eqn:eqnnumber54} \\end{align}\\] By transposing the cofactor form of a \\(3\\times 3\\) matrix, we get the following adjugate form: \\[ adj(A) = C^T = \\left[\\begin{array}{ccccc} C_{1,1} &amp; C_{1,2} &amp; &amp; C_{1,3} \\\\ C_{2,1} &amp; C_{2,2} &amp; &amp; C_{2,3} \\\\ C_{3,1} &amp; C_{3,2} &amp; &amp; C_{3,3} \\\\ \\end{array}\\right]_{3 \\times 3}^T = \\left[\\begin{array}{ccccc} C_{1,1} &amp; C_{2,1} &amp; &amp; C_{3,1} \\\\ C_{1,2} &amp; C_{2,2} &amp; &amp; C_{3,2} \\\\ C_{1,3} &amp; C_{2,3} &amp; &amp; C_{3,3} \\\\ \\end{array}\\right]_{3 \\times 3} \\] 2.9 Inverse Form and Row-Echelon Form Converting a matrix to its inverse form The inverse of a matrix is expressed as such: \\[ A \\rightarrow A^{-1} \\] To compute for the inverse of a matrix, we multiply the reciprocal of the determinant of matrix A by its adjugate, yielding the following formula: \\[\\begin{align} A^{-1} = \\frac{1}{det(A)} \\times adj(A) \\label{eqn:eqnnumber55} \\end{align}\\] Previously, we have shown how to calculate the determinant and adjugate of a matrix. Let us use a simple example: Given the following 3x3 matrix: \\[ A = \\left[\\begin{array}{ccc} 2 &amp; 1 &amp; 3\\\\ 1 &amp; 2 &amp; 0 \\\\ 1 &amp; 2 &amp; 3 \\end{array}\\right]_{3 \\times 3} \\] First, let us derive the adjugate of the matrix: \\[ adj(A) = \\left[\\begin{array}{rrr} 6 &amp; 3 &amp; -6\\\\ -3 &amp; 3 &amp; 3 \\\\ 0 &amp; -3 &amp; 3 \\end{array}\\right]_{3 \\times 3} \\] Then, using the Laplace formula, we compute for the determinant: \\[ det(A) = |A| = + 2(2 \\times 3 - 0 \\times 2) - 1(1 \\times 3 - 0 \\times 1) + 3(1 \\times 2 - 2 \\times 1) = 9 \\] And then, lastly, we compute for the inverse matrix: \\[ A^{-1} = \\frac{1}{det(A)} \\times adj(A) = \\frac{1}{9} \\left[\\begin{array}{rrr} 6 &amp; 3 &amp; -6\\\\ -3 &amp; 3 &amp; 3 \\\\ 0 &amp; -3 &amp; 3 \\end{array}\\right] = \\left[\\begin{array}{rrr} 2/3 &amp; 1/3 &amp; -2/3\\\\ -1/3 &amp; 1/3 &amp; 1/3 \\\\ 0 &amp; -1/3 &amp; 1/3 \\end{array}\\right] \\] It is essential to highlight that computing for the inverse of a matrix can be very expensive. We will cover other more efficient computing methods for the inverse of a matrix later in the chapter. Converting a matrix to its echelon form A matrix has two row-echelon forms: Row-Echelon Form (REF) - using Gaussian Elimination Reduced Row-Echelon Form (RREF) - using Gauss-Jordan Elimination Review Figure 2.16 to visualize the difference. Figure 2.16: Row-Echelon There are four properties to consider for a matrix to be in row-echelon form: The first non-zero element in each row is a 1. That is the leading entry called pivot. The leading entry of each row should be to the right of the leading entry of previous rows. Rows with zero elements should be below rows with non-zero elements. A column with a non-zero entry holds a free variablewithout a leading entry. An example of the fourth property is shown in Figure 2.16 where the third column has no pivot. Thus, that column is represented by a free variable . Additionally, there are three other properties to consider for a matrix to be in reduced row-echelon form: The form follows the three rules of row-echelon form. Additionally, the leading entry (or pivot) is the only non-zero entry in its column. E.g., Given the following system of equations: \\[\\begin{align*} 1x + 3y + 6z= 0\\\\ 2x + 7y + 3z= 0\\\\ 0x + 1y - 9z = 0 \\end{align*}\\] We get the reduced echelon form with a free variable, z, in which we can assign any non-zero arbitrary value, e.g. z = a: \\[ \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 33 \\\\ 0 &amp; 1 &amp; -9 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\left|\\begin{array}{r}0 \\\\ 0 \\\\ 0\\end{array}\\right.\\right] \\rightarrow \\left[\\begin{array}{r} x - 33\\ a\\\\ y + 9\\ a \\\\ z \\end{array}\\left|\\begin{array}{r}0 \\\\ 0 \\\\ a\\end{array}\\right.\\right] \\rightarrow \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} -33\\ a \\\\ 9\\ a \\\\ \\ a \\end{array}\\right] \\] We can use three primary matrix operations to get to the echelon forms. Multiply a row by a scalar constant - equivalent to scaling a row Switch or interchange two rows Add two rows Let us go through those operations to solve for the echelon form of the following matrix: \\[ A=\\left[\\begin{array}{rrrr} 8 &amp; 2 &amp; 4 \\\\ 9 &amp; 2 &amp; 2 \\\\ -3 &amp; 1 &amp; 6 \\\\ 4 &amp; 3 &amp; 2 \\end{array}\\right]_{4\\times3} \\] To begin - and as note - just for convenience, for every operation performed on the matrix, the resulting row will be appended with an asterisk (*): \\[ A=\\left[ \\begin{array} {c}1/8R_1 \\\\--------\\\\ \\begin{array}{rrrr} 8 &amp; 2 &amp; 4 \\\\ 9 &amp; 2 &amp; 2 \\\\ -3 &amp; 1 &amp; 6 \\\\ 4 &amp; 3 &amp; 2 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}-9R_1 + R_2 \\\\--------\\\\ \\begin{array}{rrrr} *1 &amp; 1/4 &amp; 1/2 \\\\ 9 &amp; 2 &amp; 2 \\\\ -3 &amp; 1 &amp; 6 \\\\ 4 &amp; 3 &amp; 2 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}3R_1 + R_3 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ *0 &amp; -1/4 &amp; -5/2 \\\\ -3 &amp; 1 &amp; 6 \\\\ 4 &amp; 3 &amp; 2 \\end{array} \\end{array} \\right] \\] To recap, three operations were performed: 1/8R_1 : Multiply Row 1 by 1/8 to arrive at a new Row 1. -9R_1 + R_2 : Multiply Row 1 by -9 then add result to Row 2. 3R_1 + R_3 : Multiply Row 1 by 3 then add result to Row 3. Now, to continue: \\[ A=\\left[ \\begin{array} {c}-4R_1 + R_4 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; -1/4 &amp; -5/2 \\\\ *0 &amp; 7/4 &amp; 15/2 \\\\ 4 &amp; 3 &amp; 2 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}-4R_2 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; -1/4 &amp; -5/2 \\\\ 0 &amp; 7/4 &amp; 15/2 \\\\ *0 &amp; 2 &amp; 0 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}-7/4R_2 + R_3 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ *0 &amp; 1 &amp; 10 \\\\ 0 &amp; 7/4 &amp; 15/2 \\\\ 0 &amp; 2 &amp; 0 \\end{array} \\end{array} \\right] \\] Next, we perform another three operations: -4R_1 + R_4 : Multiply Row 1 by -4 then add result to Row 4. -4R_2 : Multiply Row 2 by -4 to arrive at a new Row 2. -7/4R_2 + R_3 : Multiply Row 2 by -7/4 then add result to Row 3. To continue: \\[ A=\\left[ \\begin{array} {c}-2R_2 + R_4 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 10 \\\\ *0 &amp; 0 &amp; -10\\\\ 0 &amp; 2 &amp; 0 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}-1/10R_3 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 10\\\\ 0 &amp; 0 &amp; -10 \\\\ *0 &amp; 0 &amp; -20 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c}20R_3 + R_4 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 10\\\\ *0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; -20 \\end{array} \\end{array} \\right] \\] Next, we perform another three operations: -2R_2 + R_4 : Multiply Row 2 by -2 then add result to Row 4. -1/10R_3 : Multiply Row 3 by -1/10 to arrive at a new Row 3. 20R_3 + R_4 : Multiply Row 3 by 20 then add result to Row 4. Finally, we get the Row-Echelon Form (REF). \\[ echelon(A) = \\left[ \\begin{array} {c} \\text{Echelon Form} \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 10\\\\ 0 &amp; 0 &amp; 1\\\\ *0 &amp; 0 &amp; 0 \\end{array} \\end{array} \\right] \\] To compute for the reduced echelon form, we continue further: \\[ A=\\left[ \\begin{array} {c}-10R_3 + R_2 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ 0 &amp; 1 &amp; 10 \\\\ 0 &amp; 0 &amp; 1 \\\\ *0 &amp; 0 &amp; 0 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c} -1/2R_3 + R_1 \\\\--------\\\\ \\begin{array}{rrrr} 1 &amp; 1/4 &amp; 1/2 \\\\ *0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\end{array} \\right] \\rightarrow \\left[ \\begin{array} {c} -1/4R_2 + R_1 \\\\--------\\\\ \\begin{array}{rrrr} *1 &amp; 1/4 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 \\end{array} \\end{array} \\right] \\] We performed three operations: -10R_3 + R_2 : Multiply Row 3 by -10 then add result to Row 2. -1/2R_3 + R_1 : Multiply Row 3 by -1/2 then add result to Row 1. -1/4R_2 + R_1 : Multiply Row 2 by -1/4 to add result to Row 1. Finally, we get the Reduced Row-Echelon Form (RREF) form: \\[ reduced\\ echelon(A) = \\left[ \\begin{array} {c} \\text{Reduced Echelon} \\\\--------\\\\ \\begin{array}{rrrr} *1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1\\\\ 0 &amp; 0 &amp; 0 \\end{array} \\end{array} \\right] \\] Just seeing a diagonal matrix with 1s does not give any meaning; however, to transform a matrix equation using Gauss-Jordan Elimination along with the vector y is a way to solve the system of equations for x: \\[ Ax = y \\] Let us implement Row-Echelon and Reduced Row-Echelon in R language to solve a system of linear equations (this naive implementation uses simple partial pivoting - more on this topic later in the chapter): \\[ \\left(\\begin{array}{lll} 3x_1 + 3x_2 + 3x_3 = 6 \\\\ 2x_1 + 4x_2 + 5x_3 = 5 \\\\ 1x_1 + 5x_2 + 5x_3 = 6 \\end{array}\\right) \\rightarrow \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right] \\left[ \\begin{array}{rrr} x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right] \\] ref &lt;- function(A) { n = ncol(A) m = nrow(A) j = 1 i = 1 tol = 1e-10 while (j &lt;= m &amp;&amp; i &lt;= n) { print(paste(&quot;Row &quot;, j)) max_row = which( abs(A[,i]) == max(abs(A[,i])) )[1] if (j &lt; max_row ) { if (abs( A[max_row,i] ) &gt; tol) { print(paste(&quot;swap row &quot;, j, &quot; and row &quot;, max_row)) A[c(j,max_row),] = A[c(max_row,j),] } } if (abs( A[j,i] ) &lt; tol) { i = i + 1; next } if (A[j,i] != 1) { print(paste(&quot;scale: multiply row &quot;, j, &quot; by &quot;, 1/A[j,i] )) scale_to_1 = 1 / A[j,i] A[j,i:n] = A[j,i:n] * scale_to_1 } k = j while (k &lt; m) { r = A[k + 1, i] if (abs(r) &gt; tol ) { print(paste(&quot;multiply row &quot;, j, &quot; by &quot;, r, &quot; then subtract from row &quot;, k + 1)) A[k+1,i:n] = A[k+1,i:n] - r * A[j,i:n] } k = k + 1 } j = j + 1; i = i + 1 } A } rref &lt;- function(A) { n = ncol(A) m = nrow(A) j = 1 i = 1 tol = 1e-10 while (j &lt;= m &amp;&amp; i &lt;= n) { print(paste(&quot;Row &quot;, j)) max_row = which( abs(A[,i]) == max(abs(A[,i])) )[1] if (j &lt; max_row ) { if (abs( A[max_row,i] ) &gt; tol) { print(paste(&quot;swap row &quot;, j, &quot; and row &quot;, max_row)) A[c(j,max_row),] = A[c(max_row,j),] } } if (abs( A[j,i] ) &lt; tol) { i = i + 1; next } if (A[j,i] != 1) { print(paste(&quot;scale: multiply row &quot;, j, &quot; by &quot;, 1/A[j,i] )) scale_to_1 = 1 / A[j,i] A[j,i:n] = A[j,i:n] * scale_to_1 } k = 1 while (k &lt;= m) { r = A[k, i] if ( k!= j &amp;&amp; abs(r) &gt; tol) { print(paste(&quot;multiply row &quot;, j, &quot; by &quot;, r, &quot; then subtract from row &quot;, k)) A[k,i:n] = A[k,i:n] - r * A[j,i:n] } k = k + 1 } j = j + 1; i = i + 1 } A } backward_sub &lt;- function(A, b) { pivots = min(ncol(A), nrow(A)) for (i in pivots:1) { b[i] = b[i] / A[i,i] for (j in (i-1):1) { if (j == 0) break b[j] = b[j] - A[j,i] * b[i] } } b } forward_sub &lt;- function(A, b) { pivots = min(ncol(A), nrow(A)) for (i in 1:pivots) { b[i] = b[i] / A[i,i] for (j in (i+1):pivots) { if (j &gt; pivots) break b[j] = b[j] - A[j,i] * b[i] } } b } (A = matrix(c(3,3,3,6, 2,4,5,5, 1,5,5,6), 3, byrow=TRUE)) ## [,1] [,2] [,3] [,4] ## [1,] 3 3 3 6 ## [2,] 2 4 5 5 ## [3,] 1 5 5 6 (R = rref(A)) ## [1] &quot;Row 1&quot; ## [1] &quot;scale: multiply row 1 by 0.333333333333333&quot; ## [1] &quot;multiply row 1 by 2 then subtract from row 2&quot; ## [1] &quot;multiply row 1 by 1 then subtract from row 3&quot; ## [1] &quot;Row 2&quot; ## [1] &quot;swap row 2 and row 3&quot; ## [1] &quot;scale: multiply row 2 by 0.25&quot; ## [1] &quot;multiply row 2 by 1 then subtract from row 1&quot; ## [1] &quot;multiply row 2 by 2 then subtract from row 3&quot; ## [1] &quot;Row 3&quot; ## [1] &quot;multiply row 3 by 1 then subtract from row 2&quot; ## [,1] [,2] [,3] [,4] ## [1,] 1 0 0 1 ## [2,] 0 1 0 2 ## [3,] 0 0 1 -1 # If solving linear systems for &#39;x&#39; and ref() is used, # then use backward substitution, assuming matrix # is augmented (with a &#39;b&#39; column) # b = R[, ncol(R)] # (x = backward_sub(R, b)) Here, we have solved for x: \\[ x_1 = 1,\\ \\ \\ \\ \\ x_2 = 2,\\ \\ \\ \\ \\ x_3 = -1 \\] 2.10 Linear Transformations Let us use a 2D coordinate system to explain linear transformation geometrically. We will use a vector to sample performing transformation. Below is a notation expressing the case where a 2x1 input vector (\\(\\mathbb{R}^2\\)) goes through a transformation, yielding a 2x1 output vector (\\(\\mathbb{R}^2\\)). \\[ T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 \\] Given a unit vector: \\[ \\mathbf{\\vec{u}} = \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{2 \\times 1} \\] let us start by using a 2x2 identity matrix as a transformation matrix to transform the unit vector: \\[ I = \\left[\\begin{array}{ccc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\right]_{2 \\times 2} \\] We use matrix multiplication for the transformation, yielding a dot product. \\[ I \\cdotp u = \\left[\\begin{array}{ccc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{ccc} (1 \\times 1) + ( 0 \\times 1) \\\\ (0 \\times 1) + ( 1 \\times 1) \\end{array}\\right] =\\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{output\\}} \\] Here, the result of the transformation is a vector of identical dimension, \\(\\mathbb{R}^2\\). 2.10.1 Scaling Let us try another example, but this time, we scale the vector by 9 using a different 2x2 matrix as our transformer: \\[ I \\cdotp u = \\left[\\begin{array}{ccc} 9 &amp; 0 \\\\ 0 &amp; 9 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{ccc} (9\\times 1) + ( 0 \\times 1) \\\\ (0 \\times 1) + ( 9 \\times 1) \\end{array}\\right] = \\left[\\begin{array}{ccc} 9 \\\\ 9 \\end{array}\\right]_{\\{output\\}} \\] Here, the vector output is scaled to 9. We can also use a scalar value to scale it: \\[ \\left[\\begin{array}{ccc} 9 &amp; 0 \\\\ 0 &amp; 9 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{input\\}} = 9_{\\{scalar\\}} \\times \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right] = \\left[\\begin{array}{ccc} 9 \\\\ 9 \\end{array}\\right]_{\\{output\\}} \\] Notice that the input vector, \\(\\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]\\) can be transformed using a matrix or a scalar value. \\[ \\left[\\begin{array}{ccc} 9 &amp; 0 \\\\ 0 &amp; 9 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{input\\}} = 9_{\\{scalar\\}} \\times \\left[\\begin{array}{ccc} 1 \\\\ 1 \\end{array}\\right]_{\\{input\\}} \\] The equation above can be expressed as an equation in its more general form: \\[ A \\cdotp v = \\lambda \\times v \\] Later in this chapter, we cover eigenvectors and eigenvalues. 2.10.2 Transvection (Shearing) Transvection (or shearing) is a transformation that displaces vectors by scaling only a portion of the entries in the vector. For example, an \\(\\mathbb{R}^2\\) vector has the following coordinate entries, \\(\\left[\\begin{array}{rrr} x \\\\ y \\end{array}\\right]\\). By scaling only the y-coordinate entry, e.g. \\(\\left[\\begin{array}{rrr} x \\\\ cy \\end{array}\\right]\\), this is effectively displacing the vector’s y-coordinate. As an example, transform matrix A using a transvection transformation matrix by multiplying only the x-coordinate entry of vector a by 1/3, effectively displacing the vector horizontally: \\[ T_{\\{shear\\}} \\cdotp A = \\left[\\begin{array}{rrr} 1 &amp; 1/3 \\\\ 0 &amp; 1 \\end{array}\\right]_{\\{transform\\}} \\cdotp \\left[\\begin{array}{ccc} 4 &amp; 0 \\\\ 0 &amp; 3 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{rrr} 4 &amp; 1 \\\\ 0 &amp; 3 \\end{array}\\right]_{\\{output\\}} \\] Here, the top side of the rectangular figure formed by vectors a and b is shifted horizontally by 1 grid to the right. The vector a is the one being displaced from \\(\\left[\\begin{array}{rrr} 0 \\\\ 3 \\end{array}\\right]\\) to \\(\\left[\\begin{array}{rrr} 1 \\\\ 3 \\end{array}\\right]\\). Note that this displacement is not equivalent to rotating the vector. Figure 2.17: Transvection 2.10.3 Rotation Figure 2.18 illustrates two vectors being rotated counter-clockwise at \\(90^{\\circ}\\), equivalent to \\(\\pi / 2\\) radians. The two vectors are outlined in matrix-form: \\[ A = \\left[\\begin{array}{rr} 1 &amp; 4\\\\ 3 &amp; 1 \\end{array}\\right] \\] We then perform rotation transformation: \\[ T_{\\{rotation\\}}: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2 \\] Here, we use a rotation transformation matrix: \\[ R_{\\theta} = \\left[\\begin{array}{rr} cos(\\theta) &amp; -sin(\\theta) \\\\ sin(\\theta) &amp; cos(\\theta) \\end{array}\\right] = \\left[\\begin{array}{rr} cos(\\pi/2) &amp; -sin(\\pi/2) \\\\ sin(\\pi/2) &amp; cos(\\pi/2) \\end{array}\\right] = \\left[\\begin{array}{rr} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\] Figure 2.18: Rotation Now, let’s use the rotation matrix as a transformer: \\[ R_{\\theta} \\cdotp A = \\left[\\begin{array}{rr} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{rr} 1 &amp; 4\\\\ 3 &amp; 1 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{rr} -3 &amp; -1\\\\ 1 &amp; 4 \\end{array}\\right]_{\\{output\\}} \\] 2.10.4 Reflection Figure 2.20 illustrates how vector a gets reflected over the line (vector c), forming vector b at the opposite side of the line. Using a reflection transformation matrix, we get the output vector as shown in 2.20. In the example, the entries of the vector are switched. \\[ T_{\\{reflect\\}} \\cdotp A = \\left[\\begin{array}{rrr} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{ccc} 1 \\\\ 3 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{rrr} 3 \\\\ 1 \\end{array}\\right]_{\\{output\\}} \\] In fact, the following transformation matrices can be used to transform the input (original) vector either for reflection purposes or for rotation purposes: \\[\\begin{align*} \\left[\\begin{array}{rrr} 0 &amp; -1 \\\\ -1 &amp; 0 \\end{array}\\right]_{(3)} \\rightarrow \\left[\\begin{array}{rrr} -3 \\\\ -1 \\end{array}\\right] ,\\ \\ \\ \\ \\ \\ \\left[\\begin{array}{rrr} 1 &amp; 0 \\\\ 0 &amp; -1 \\end{array}\\right]_{(5)} \\rightarrow \\left[\\begin{array}{rrr} 1 \\\\ -3 \\end{array}\\right] \\\\ \\left[\\begin{array}{rrr} -1 &amp; 0 \\\\ 0 &amp; -1 \\end{array}\\right]_{(4)} \\rightarrow \\left[\\begin{array}{rrr} -1 \\\\ -3 \\end{array}\\right] ,\\ \\ \\ \\ \\ \\ \\left[\\begin{array}{rrr} -1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\right]_{(1)} \\rightarrow \\left[\\begin{array}{rrr} -1 \\\\ 3 \\end{array}\\right] \\end{align*}\\] Figure 2.19 shows more results of reflection/rotation: Figure 2.19: Reflect and Rotation 2.10.5 Projection Figure 2.20 also illustrates how vector a gets projected into the subspace V, forming vector b along the line. Figure 2.20: Reflect and Project The projection can be expressed using the following formulas, e.g. projecting vector a to subspace V, yielding vector b: Computing for the scale factor - C: \\[ C = \\frac{V \\cdotp a}{\\|V\\| \\times \\|V\\|} = \\frac{V \\cdotp a}{\\|V\\|^2} \\] This C scales the projection, e.g. \\(C \\times V\\). Then we have: \\[ b = proj_{(V)}a = C \\times V = \\frac{V \\cdotp a}{\\|V\\|^2} \\times V \\] The computation is as follows: \\[ b = \\text{projection of a to V} = \\frac { \\left[\\begin{array}{rrr} 4 \\\\ 4 \\end{array}\\right] \\cdotp \\left[\\begin{array}{rrr} 1 \\\\ 3 \\end{array}\\right] } {\\left\\|\\begin{array}{rrr} 4 \\\\ 4 \\end{array}\\right\\|^2} \\times \\left[\\begin{array}{rrr} 4 \\\\ 4 \\end{array}\\right] = \\frac{16}{32} \\times \\left[\\begin{array}{rrr} 4 \\\\ 4 \\end{array}\\right] = 0.5 \\times \\left[\\begin{array}{rrr} 4 \\\\ 4 \\end{array}\\right] = \\left[\\begin{array}{rrr} 2 \\\\ 2 \\end{array}\\right] \\] 2.10.6 Translation Figure 2.21 shows 4 vectors, forming a square, and is represented by the below matrix: \\[\\begin{align*} \\ \\ \\ {} &amp;\\left(\\begin{array}{rrrr} a &amp; b &amp; c &amp; d \\end{array}\\right) \\\\ \\left[\\begin{array}{c} x \\\\ y \\end{array}\\right] \\ \\ \\ &amp;\\left[\\begin{array}{cccc} 1 &amp; 4 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 1 &amp; 1 \\end{array}\\right] \\rightarrow 1 + \\left[\\begin{array}{cccc} 1 &amp; 4 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 1 &amp; 1 \\end{array}\\right] \\rightarrow \\left[\\begin{array}{cccc} 2 &amp; 5 &amp; 2 &amp; 5 \\\\ 5 &amp; 5 &amp; 2 &amp; 2 \\end{array}\\right] \\end{align*}\\] Translation is a transformation that pushes or moves a figure to a new position in the coordinate system by displacing the vectors forming the figure. In Figure 2.21, we add a scalar value (1) to the matrix (of column vectors), each of the four vectors extending by 1, in effect, geometrically pushing each side of the square by one grid up and one grid to the right. Figure 2.21: Translation 2.10.7 Dilation and Composition Figure 2.22 shows four vectors, forming a square figure which is represented by the below matrix: \\[\\begin{align*} \\ \\ \\ {} &amp;\\left(\\begin{array}{rrrr} a &amp; b &amp; c &amp; d \\end{array}\\right) \\\\ \\left[\\begin{array}{c} x \\\\ y \\end{array}\\right] \\ \\ \\ &amp;\\left[\\begin{array}{rrrr} 2 &amp; 3 &amp; 2 &amp; 3 \\\\ 3 &amp; 3 &amp; 2 &amp; 2 \\end{array}\\right] \\rightarrow 3 \\times \\left[\\begin{array}{rrrr} 2 &amp; 3 &amp; 2 &amp; 3 \\\\ 3 &amp; 3 &amp; 2 &amp; 2 \\end{array}\\right] \\rightarrow \\left[\\begin{array}{cccc} 6 &amp; 9 &amp; 6 &amp; 9 \\\\ 9 &amp; 9 &amp; 6 &amp; 6 \\end{array}\\right] \\end{align*}\\] Here, dilation is a transformation that expands or contracts the size of a figure geometrically. One way is to multiply the matrix by a scalar value. In Figure 2.22, the small square gets larger on all sides by a scalar value of 1 grid. Figure 2.22: Dilation Notice in Figure 2.22 that the coordinates of the large square do not match the coordinates of the column vectors a, b, c, and d of the newly transformed matrix after dilation. That is because we have basically applied another transformation to the matrix by adding -5 like so: \\[\\begin{align*} \\ \\ \\ {} &amp;\\left(\\begin{array}{rrrr} a &amp; b &amp; c &amp; d \\end{array}\\right) \\\\ -5 + \\left[\\begin{array}{cccc} 6 &amp; 9 &amp; 6 &amp; 9 \\\\ 9 &amp; 9 &amp; 6 &amp; 6 \\end{array}\\right] \\rightarrow &amp;\\left[\\begin{array}{cccc} 1 &amp; 4 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 1 &amp; 1 \\end{array}\\right] \\end{align*}\\] Performing multiple transformations against a vector or matrix is called composition. In the example above, we have applied dilation to the square figure formed by the four vectors, and then we applied translation to move the large square to a new location where it surrounds the smaller square. So if we apply a composition transformation by dilating the input matrix first and then translating it, the output matrix will look like so (operation is applied from right to left): \\[\\begin{align*} \\ \\ \\ {} &amp;\\left(\\begin{array}{rrrr} a &amp; b &amp; c &amp; d \\end{array}\\right) \\\\ -5_{(translate)} + 3_{(dilate)} \\times \\left[\\begin{array}{rrrr} 2 &amp; 3 &amp; 2 &amp; 3 \\\\ 3 &amp; 3 &amp; 2 &amp; 2 \\end{array}\\right] \\rightarrow -5 + \\left[\\begin{array}{cccc} 6 &amp; 9 &amp; 6 &amp; 9 \\\\ 9 &amp; 9 &amp; 6 &amp; 6 \\end{array}\\right] \\rightarrow &amp;\\left[\\begin{array}{cccc} 1 &amp; 4 &amp; 1 &amp; 4 \\\\ 4 &amp; 4 &amp; 1 &amp; 1 \\end{array}\\right] \\end{align*}\\] 2.11 Rank and Nullity First of all, Rank complements Nullity. That statement can be expressed this way: \\[ ncol(A) = rank + nullity = dim(C(A)) + dim(N(A)),\\ \\ \\ \\ \\text{where ncol = no of columns} \\] The following difference holds between the two properties: Rank is represented as rank(Matrix) = C(Matrix) it is the number of dimensions in the column space represents the number of linearly independent vectors The number of pivot column vectors in RREF Nullity is represented as N(Matrix) it is the number of dimensions in the null space represents the number of linearly dependent vectors The number of non-pivot column vectors in RREF, equivalent to the number of free variables. The rank of a matrix is denoted by the number of column vectors (or row vectors) in the matrix that is linearly independent - meaning that those vectors can form linear combinations. The rank of a matrix is based on either the column space or row space. Either way, suppose the number of row vectors in a \\(3\\times 3\\) matrix is 3, all linearly independent vectors. In that case, the row rank of the matrix is full-rank. If a matrix has 3 column vectors with one linearly dependent vector and two linearly independent vectors, then we say that the column rank of the matrix is 2. A full-rank matrix is when all the column vectors are linearly independent. Therefore, in a 3x3 matrix, the matrix is full-rank if all three column vectors are linearly independent. The nullity (or dimension of the null space) of a matrix, on the other hand, is denoted by the number of column vectors (or row vectors) that are linearly dependent on the matrix - meaning that those vectors are formed by linear combinations of other existing column vectors that are linearly independent. A 3x3 full-rank matrix has a nullity equal to zero. A rank-2 3x3 matrix has a nullity equal to one. We learned about free variables when we covered the topic of row-echelon. Especially if a system is under-determined, we may find free variables indicating that solutions may exist but may not be unique. An augmented matrix in its echelon form explains this better, \\[ \\left[\\begin{array}{rrrrr} 1 &amp; 2 &amp; 0 &amp; 0 &amp; 9 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array}\\left|\\begin{array}{r}0 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right.\\right]_{(RREF)} \\left[\\begin{array}{r} x_1\\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5\\end{array}\\right]_{x} . \\] The solutions for each of the x variables in the system is then represented as: \\[ \\begin{cases} x_1 + 2x_2 + 9x_5 = 0 \\\\ x_3 + x_5 = 0 \\\\ x_4 + 2x_5 = 0 \\end{cases} \\text{simplified into} \\begin{cases} x_1 = -2r -9s \\\\ x_2 = r\\\\ x_3 = -s\\\\ x_4 = -2s \\\\ x_5 = s \\end{cases} \\rightarrow x = r\\left[\\begin{array}{r} -2\\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\end{array}\\right]_{r} + s\\left[\\begin{array}{r} -9\\\\ 0 \\\\ -1 \\\\ -2 \\\\ 1\\end{array}\\right]_{s} \\] where r and s are free non-zero variables. There are three pivots in the column space - \\(rank: dim(C(A)) = 3\\). There are two free variables - non-pivot - in the null-space - \\(nullity: dim(N(A)) = 2\\). There are five column vectors in the matrix. \\[ ncol(A) = rank + nullity = dim(C(A)) + dim(N(A)) = 3 + 2 = 5 \\] And if we are to transpose the matrix and evaluate the column space and null space, we observe the following: There are three pivots in the row space - transposed echelon form. There is 1 free variable in the left null-space - transposed echelon form. Now, if rank(A) &lt; ncol(A) or dim(N(A)) &gt; 0, it means that the matrix is rank-deficient and is, thus, not full-rank. We discuss more of this topic about rank and nullity later in this chapter under Jordan Decomposition. 2.12 Singularity and Triviality Singular: A matrix is said to be singular if the determinant is zero. And if the determinant is zero (e.g., the area is zero), then the matrix is non-invertible. There is nothing to invert. \\[ det(A) = |A| = 0 \\] A few examples of a matrix that results in a determinant equal to zero: \\[ \\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 0 \\\\ 4 &amp; 5 &amp; 6 \\end{array}\\right], \\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 3 \\\\ 4 &amp; 5 &amp; 6 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right], \\left[\\begin{array}{rrrr} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right], \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\end{array}\\right], \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] \\] That also shows that the diagonal of a matrix that is not all non-zero is singular. Additionally, see example 6 of Figure 2.11 which illustrates two vectors forming a singular matrix. \\[ \\left[\\begin{array}{rrrr} 3(i)\\\\ 2(j) \\end{array}\\right] \\rightarrow \\left|\\begin{array}{rrrr} 3 &amp; 2 \\\\ 0 &amp; 0 \\end{array}\\right| = (3 \\times 0) + (2 \\times 0) = 0 \\] Non-Singular: A matrix is non-singular if and only if its determinant is non-zero and is invertible. \\[ det(A) = |A| \\ne 0 \\] That also shows that the diagonal of a matrix that is all non-zero is non-singular. Below is an example of a non-singular matrix with the least number of non-zero elements, forming an identity matrix. \\[ I = \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] \\] Triviality: The term trivial (and non-trivial) applies to solutions of a system of linear equations. In particular, a trivial solution applies to homogeneous equations where all constants (or coordinates) are zero. Given the following equation: \\[ \\left(\\begin{array}{rrr} 2x_1 + 4x_2 + 6x_3 = 0 \\\\ 1x_1 + 2x_2 + 3x_3 = 0 \\\\ 5x_1 + 3x_2 + 7x_3 = 0 \\end{array}\\right) \\rightarrow \\left[\\begin{array}{rrrr} 2 &amp; 4 &amp; 6 \\\\ 1 &amp; 2 &amp; 3 \\\\ 5 &amp; 3 &amp; 7 \\end{array}\\right] \\left[\\begin{array}{rrr} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right] = \\left[\\begin{array}{rrr} 0 \\\\ 0 \\\\ 0 \\end{array}\\right] \\] Here a solution for the given equation is trivial if \\(x=0\\), meaning that \\(x_1 = 0,\\ \\ \\ x_2 = 0,\\ \\ \\ x_3 = 0\\). In a null space where \\(b=0\\) for \\(Ax=b\\): \\[ Ax = 0 \\leftarrow \\begin{cases} trivial &amp; if\\ \\sum_i x_i = 0 \\\\ non-trivial &amp; if\\ \\sum_i x_i \\ne 0 \\end{cases} \\] On the other hand, a vector, \\(x\\), has a non-trivial solution if at least one element is non-zero. 2.13 Orthogonality and Orthonormality In this section, let us define and describe an orthogonal matrix. It may help to note that an orthogonal matrix is the same as an orthonormal matrix. It becomes more apparent by definition: An orthogonal matrix is an invertible matrix whose columns are unit vectors and geometrically form a right angle (mutually orthogonal) with one another. By virtue of the matrix having unit vector columns, it makes it an orthonormal matrix. By describing this further, a matrix is orthogonal if each of its column vectors results in a zero dot product with the other column vectors. For example, given a matrix: \\[ A = \\left[\\begin{array}{rr} 1/\\sqrt{2} &amp; \\sqrt{2}/2 \\\\ 1/\\sqrt{2} &amp; -\\sqrt{2}/2 \\end{array}\\right] \\rightarrow \\left(\\begin{array}{r} \\frac{1}{\\sqrt{2}} \\times \\frac{\\sqrt{2}}{2} + \\frac{1}{\\sqrt{2}} \\times -\\frac{\\sqrt{2}}{2} \\end{array}\\right) = 0 \\] Note that \\(1/\\sqrt{2} = \\sqrt{2}/2\\) - the use of both in the matrix is just to show the effect of the transposition. Recall a unit vector has a magnitude of 1, \\(\\|u\\| = 1\\): \\[ \\|u\\| = \\sqrt{\\sum_{i=1}^n(x_i)^2} = \\sqrt{x_1^2 + x_2^2+ ... + x_n^2} = 1 \\] Example: \\[ A = \\left[\\begin{array}{rr} V_{1} &amp; V_{2}\\\\ ---- &amp; ----\\\\ 1/\\sqrt{2} &amp; \\sqrt{2}/2 \\\\ 1/\\sqrt{2} &amp; -\\sqrt{2}/2 \\end{array}\\right] \\] \\[ \\|V_{1}\\| = \\left[\\begin{array}{rr} 1/\\sqrt{2} \\\\ 1/\\sqrt{2} \\end{array}\\right] = \\sqrt{(1/\\sqrt{2})^2 + (1/\\sqrt{2})^2} = 1 \\] \\[ \\|V_{2}\\| = \\left[\\begin{array}{rr} \\sqrt{2}/2 \\\\ \\sqrt{2}/2 \\end{array}\\right] = \\sqrt{(\\sqrt{2}/2)^2 + (-\\sqrt{2}/2)^2} = 1 \\] Additionally, a matrix is orthonormal if its dot product with its transposed version results in an identity matrix and if its inverse form equals its transpose form. \\[\\begin{align} A \\cdotp A^T = A^T \\cdotp A = I,\\ \\ \\ \\ A^{-1} = A^T \\label{eqn:eqnnumber56} \\end{align}\\] Example: \\[ A \\cdotp A^T = \\left[\\begin{array}{c} \\begin{array}{rr} 1/\\sqrt{2} &amp; \\sqrt{2}/2 \\\\ 1/\\sqrt{2} &amp; -\\sqrt{2}/2 \\end{array} \\end{array}\\right]_A \\cdotp \\left[\\begin{array}{c} \\begin{array}{rr} 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\\\ \\sqrt{2}/2 &amp; -\\sqrt{2}/2 \\end{array} \\end{array}\\right]_{A^T} \\rightarrow I = \\left[\\begin{array}{c} \\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\end{array}\\right]_{I} \\] In the same manner, we have this: \\[ A^T = A^{-1} \\rightarrow \\left[\\begin{array}{c} \\begin{array}{rr} 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\\\ \\sqrt{2}/2 &amp; -\\sqrt{2}/2 \\end{array} \\end{array}\\right]_{A^T} = \\left[\\begin{array}{c} \\begin{array}{rr} 1/\\sqrt{2} &amp; 1/\\sqrt{2} \\\\ 1/\\sqrt{2} &amp; -1/\\sqrt{2} \\end{array} \\end{array}\\right]_{A^{-1}} \\] Lastly, because the determinant of an identity matrix is equal to one; therefore, the determinant of the dot product of its transposed version is equal to \\(\\pm 1\\). Given that \\(A^TA = AA^T = I\\), then: \\[ det(A^TA) = det(I) = det(A) \\cdot det(A^T) = det(A)^2 = \\pm 1 \\] Note that in other literature, an orthogonal matrix has a symbol \\(Q\\). For example: \\[\\begin{align} Q^TQ = QQ^T = I \\label{eqn:eqnnumber57} \\end{align}\\] In the later part of this chapter, we cover more topics around the orthogonal matrix under the QR decomposition section. 2.14 Eigenvectors and Eigenvalues One of the most important topics in Linear Algebra is about Eigenvectors and Eigenvalues. The word Eigen is a German term translated to the word own or characteristic (reference: Wiktionary). Let us start by introducing the following Eigen equation: \\[\\begin{align} A \\cdotp \\mathbf{\\vec{v}} = \\lambda \\times \\mathbf{\\vec{v}} \\label{eqn:eqnnumber58} \\end{align}\\] It is easier to understand Eigenvectors by understanding what the formula is all about. First, we start by noting that the formula is about transforming the vector, v. \\[ T:\\mathbb{R}^n \\rightarrow \\mathbb{R}^n \\] To transform the vector, we need a matrix transformer, A. In Figure 2.18, we use a rotation matrix as a transformer to rotate two vectors. Recall the following: \\[ R_{\\theta} \\cdotp A = \\left[\\begin{array}{rr} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{rr} 1 &amp; 4\\\\ 3 &amp; 1 \\end{array}\\right]_{\\{input\\}} = \\left[\\begin{array}{rr} -3 &amp; -1\\\\ 1 &amp; 4 \\end{array}\\right]_{\\{output\\}} \\] Here, we use \\(R_{\\theta}\\) matrix to transform another matrix, A. In fact, we can use the same \\(R_{\\theta}\\) matrix to rotate just the individual column vectors in the matrix, like so: The first vector in the matrix: \\[ A \\cdotp v1 = R_{\\theta} \\cdotp v1 = \\left[\\begin{array}{rr} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{rr} 1 \\\\ 3 \\end{array}\\right]_{\\{input\\ vector\\}} = \\left[\\begin{array}{rr} -3 \\\\ 1 \\end{array}\\right]_{\\{output\\ vector\\}} \\] The second vector in the matrix: \\[ A \\cdotp v2 = R_{\\theta} \\cdotp v2 = \\left[\\begin{array}{rr} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{rr} 4 \\\\ 1 \\end{array}\\right]_{\\{input\\ vector\\}} = \\left[\\begin{array}{rr} -4 \\\\ 1 \\end{array}\\right]_{\\{output\\ vector\\}} \\] As we can see, the two vectors have been rotated. Moreover, the new direction of both vectors follows a counter-clockwise rotation. There is one crucial aspect of this whole process, however. And that is, that the direction of the vectors is changed. In contrast, the one characteristic of an Eigenvector is that the direction does not change when a transformation is applied. And so, an essential highlight to remember is that the only transformation we can perform against an Eigenvector is a scaling (stretching) transformation - which does not affect the direction but only affects the magnitude. Therefore, any scalar we use to stretch an Eigenvector is called an Eigenvalue. Figure @ref(fig:eigenvector} illustrates a set of vectors. The vectors {\\(\\mathbf{\\vec{r}},\\mathbf{\\vec{s}},\\mathbf{\\vec{t}}\\)} shifted in both location and direction; but the Eigenvectors {\\(\\mathbf{\\vec{u}},\\mathbf{\\vec{v}},\\mathbf{\\vec{w}}\\)) shifted in location but not direction. Figure 2.23: Eigenvector on Shearing For any arbitrary matrix, we can find its Eigenvector and Eigenvalue using Equation (\\(\\ref{eqn:eqnnumber58}\\)). The formula explains that the transformation of vector \\(\\mathbf{\\vec{v}}\\) using matrix A equals the transformation of the same vector \\(\\mathbf{\\vec{v}}\\) using scalar \\(\\lambda\\). Example: \\[\\begin{align*} A \\cdotp v &amp;= \\lambda \\times v \\\\ &amp; \\rightarrow \\left[\\begin{array}{rr} 9 &amp; 0 \\\\ 0 &amp; 9 \\end{array}\\right]_{\\{transformer\\}} \\cdotp \\left[\\begin{array}{rr} 1 \\\\ 0 \\end{array}\\right]_{\\{EigenVector\\}} = 9_{\\{EigenValue\\}} \\times \\left[\\begin{array}{rr} 1 \\\\ 0 \\end{array}\\right]_{\\{EigenVector\\}} \\end{align*}\\] Now, to find the Eigenvector and Eigenvalue of an arbitrary matrix, we perform a simple mathematical transformation to arrive at the following characteristic equation by evaluating the determinant: \\[\\begin{align} det(A - \\lambda I ) = 0 \\label{eqn:eqnnumber59} \\end{align}\\] derived from: \\[\\begin{align} A \\cdotp v = \\lambda \\times v\\ \\ \\rightarrow\\ \\ \\ A \\cdotp v = (\\lambda I) \\cdotp v\\ \\ \\rightarrow\\ \\ \\ (A - \\lambda I) \\cdotp v = 0 \\label{eqn:eqnnumber60} \\end{align}\\] where \\(\\lambda\\) is intrinsically equivalent in transformation function as \\(\\lambda I\\): \\[ \\lambda \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_I = \\left[ \\begin{array}{rrr} \\lambda &amp; 0 &amp; 0 \\\\ 0 &amp; \\lambda &amp; 0 \\\\ 0 &amp; 0 &amp; \\lambda \\end{array} \\right]_{\\lambda I} \\] Now, the idea is to find a \\(\\lambda\\) so that the determinant becomes zero when subtracted from the diagonal entries of matrix A. \\[ A - \\lambda I = \\left[ \\begin{array}{rrr} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\\\ \\end{array} \\right]_A - \\lambda \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_I = \\left[ \\begin{array}{rrr} x_{11}- \\lambda &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22}- \\lambda &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33}- \\lambda \\\\ \\end{array} \\right] \\] If the determinant is zero, then, as we have already covered in previous discussions, the geometric area is zero, and eigenvector is the axis of rotation, unchanging in direction but changing only in scale and therefore cannot be off from the span of the scale. \\[ det(A - \\lambda{I}) = |A - \\lambda{I}| = \\left|\\begin{array}{rr} \\left[\\begin{array}{rr} 9 &amp; 0 \\\\ 0 &amp; 9 \\end{array}\\right] - 9\\left[\\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\right] \\end{array}\\right| = 0 \\] Let us use a sample matrix to illustrate how to find the Eigenvalues and Eigenvectors. First, compose the characteristic polynomial of a given matrix by computing for the determinant: \\[ A = \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\rightarrow \\left|\\begin{array}{rrr} \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] - \\lambda \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right] \\end{array}\\right| \\rightarrow \\left|\\begin{array}{rrr} 4 - \\lambda &amp; 1 &amp; 0 \\\\ 1 &amp; 2 - \\lambda &amp; 1 \\\\ 0 &amp; 1 &amp; 4 - \\lambda \\end{array}\\right| \\] We then obtain the following characteristic polynomial of matrix \\(A\\): \\[\\begin{align*} {}&amp;= (4-\\lambda)((2-\\lambda)(4-\\lambda) - 1) - (1(4-\\lambda) - 1 \\times 0)\\\\ &amp;= (4-\\lambda)(\\lambda^2 -6\\lambda + 7) - (4-\\lambda)\\\\ &amp;= -\\lambda^3 + 10\\lambda^2 - 31\\lambda + 28 - 4 - \\lambda \\\\ &amp;= -\\lambda^3 + 10\\lambda^2 - 30\\lambda + 24 \\end{align*}\\] By a simple definition, the characteristic polynomial of a matrix is a polynomial whose roots are the eigenvalues of the matrix. Therefore, by computing for the roots, we get the Eigenvalues \\(\\lambda\\) for matrix \\(A\\): Second, solve for the roots (\\(\\lambda\\)) of the polynomial. That gives us the following Eigenvalues: \\[ \\lambda_1 = 3 + \\sqrt{3},\\ \\ \\ \\ \\ \\lambda_2 = 4,\\ \\ \\ \\lambda_3 = 3 - \\sqrt{3} \\] Third, we obtain the Eigenvectors based on the discovered Eigenvalues above. We use the following formula: \\[\\begin{align} A v = \\lambda v\\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ (A - \\lambda I)v = 0 \\label{eqn:eqnnumber61} \\end{align}\\] Substitute each of the \\(\\lambda\\)s, starting with \\(\\lambda_1=3 + \\sqrt{3}\\): \\[ (A - (3 + \\sqrt{3}){I})v = 0\\ \\ \\ \\rightarrow\\ \\ \\ \\ (A - (3 + \\sqrt{3}){I}) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = 0 \\] That gives us: \\[ \\left(\\begin{array}{c} \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0\\\\ 1 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] - (3 + \\sqrt{3}) \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1 \\end{array}\\right] \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] and a simplified version (reduced row-echelon form): \\[ \\left(\\begin{array}{rrr} 1 &amp; -1-\\sqrt{3} &amp; 1\\\\0 &amp; -1 &amp; -1+\\sqrt{3}\\\\0 &amp; 0 &amp; 0 \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] Here, we let the free variable, z, take any non-zero arbitrary value, say z = 1 (or for now, let us use z = a where \\(z \\ne 0\\)). We get the Eigenvector for \\(\\lambda_1=3 + \\sqrt{3}\\) \\[\\begin{align*} v_1 {}&amp;= \\left\\{\\begin{array}{r} x + (-1-\\sqrt{3})y + z = 0 \\\\ -y - (1 + \\sqrt{3})z= 0\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = 2z - z \\\\ y = -(1 - \\sqrt{3})z\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = a \\\\ y = (\\sqrt{3} - 1)a \\\\ z = a \\end{array}\\right\\} \\\\ \\\\ v_1 &amp;= \\left[\\begin{array}{l}a \\\\ (\\sqrt{3} - 1)a\\\\ a \\end{array}\\right] \\rightarrow \\text{for } \\lambda_1 = 3 + \\sqrt{3} \\end{align*}\\] Next, substitute the second \\(\\lambda_2=4\\): \\[ (A - 4I)v = 0\\ \\ \\ \\rightarrow\\ \\ \\ \\ (A - 4I) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = 0 \\] That gives us: \\[ \\left(\\begin{array}{c} \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0\\\\ 1 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] - 4 \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1 \\end{array}\\right] \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] and a simplified version (reduced row-echelon form): \\[ \\left(\\begin{array}{rrr} 1 &amp; -2 &amp; 1\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 0 \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] Here, we let the free variable, z, take any non-zero arbitrary value, say z = 1 (or for now, let us use z = a where \\(z \\ne 0\\)). We get the Eigenvector for \\(\\lambda_2=4\\) \\[\\begin{align*} v_2 {}&amp;= \\left\\{\\begin{array}{r} x -2y + z = 0 \\\\ y = 0\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = -z \\\\ y = 0\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = -a \\\\ y = 0\\\\ z = a \\end{array}\\right\\} \\\\ \\\\ v_2 &amp;= \\left[\\begin{array}{l} -a \\\\ 0\\\\ a \\end{array}\\right] \\rightarrow \\text{for } \\lambda_2 = 4 \\end{align*}\\] Next, substitute the third \\(\\lambda_3=3 - \\sqrt{3}\\): \\[ (A - (3 - \\sqrt{3}){I})v = 0\\ \\ \\ \\rightarrow\\ \\ \\ \\ (A - (3 - \\sqrt{3}){I}) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = 0 \\] That gives us: \\[ \\left(\\begin{array}{c} \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0\\\\ 1 &amp; 2 &amp; 1\\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] - (3 - \\sqrt{3}) \\left[\\begin{array}{rrr} 1 &amp; 0 &amp; 0\\\\0 &amp; 1 &amp; 0\\\\0 &amp; 0 &amp; 1 \\end{array}\\right] \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] and a simplified version (reduced row-echelon form): \\[ \\left(\\begin{array}{rrr} 1+\\sqrt{3} &amp; 1 &amp; 0\\\\0 &amp; 1 &amp; 1+\\sqrt{3}\\\\0 &amp; 0 &amp; 0 \\end{array}\\right) \\left[\\begin{array}{r} x \\\\ y \\\\ z \\end{array}\\right] = \\left[\\begin{array}{r} 0\\\\0\\\\0 \\end{array}\\right] \\] Here, we let the free variable, z, take any non-zero arbitrary value, say z = 1 (or for now, let us use z = a where \\(z \\ne 0\\)). We get the Eigenvector for \\(\\lambda_3=3 - \\sqrt{3}\\) \\[\\begin{align*} v_3 {}&amp;= \\left\\{\\begin{array}{r} (1+\\sqrt{3})x + y = 0 \\\\ y + (1 + \\sqrt{3})z= 0\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = z \\\\ y = -(1 + \\sqrt{3})z\\\\ z = a \\end{array}\\right\\} \\rightarrow \\left\\{\\begin{array}{l} x = a \\\\ y = -(1 + \\sqrt{3})a\\\\ z = a \\end{array}\\right\\} \\\\ \\\\ v_3 &amp;= \\left[\\begin{array}{l} a \\\\ (-\\sqrt{3} -1)a\\\\ a \\end{array}\\right] \\rightarrow \\text{for } \\lambda_3 = 3 - \\sqrt(3) \\end{align*}\\] The Eigenvector to Eigenvalue mapping is therefore as follows (let a = 1): \\[ \\left(\\begin{array}{c} \\text{Matrix of Eigenvectors}\\\\ -------------\\\\ \\left[\\begin{array}{r}v_1\\\\---\\\\1 \\\\ \\sqrt{3} -1 \\\\ 1 \\end{array}\\right] \\left[\\begin{array}{r}v_2\\\\---\\\\-1 \\\\ 0 \\\\ 1 \\end{array}\\right] \\left[\\begin{array}{r}v_3\\\\---\\\\1 \\\\ -\\sqrt{3} -1 \\\\ 1 \\end{array}\\right] \\end{array}\\right) \\iff \\left[\\begin{array}{l}Eigenvalues\\\\------\\\\\\lambda_1=3 + \\sqrt{3} \\\\ \\lambda_2=4 \\\\ \\lambda_3=3 - \\sqrt{3} \\end{array}\\right] \\] In real situations, often, if not always, we do encounter high dimensional matrices of eigenvectors, where n &gt; 3. We may see n &gt; 1000. Without loss of generality, we get the following: \\[ \\left(\\begin{array}{cccc} \\text{Matrix of Eigenvectors}\\\\ -------------\\\\ \\begin{array}{ccccc} v_1 &amp; v_2 &amp; \\dots &amp; v_n \\\\ --- &amp; --- &amp; --- &amp; --- \\\\ x_{1,1} &amp; x_{1,2} &amp; \\dots &amp; x_{1,n}\\\\ x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; x_{2,n}\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n,1} &amp; x_{n,2} &amp; \\dots &amp; x_{n,n}\\\\ \\end{array} \\end{array}\\right) \\iff \\left(\\begin{array}{cccc} \\text{Matrix of Eigenvalues}\\\\ -------------\\\\ \\begin{array}{ccccc} \\lambda_1 &amp; \\lambda_2 &amp; \\dots &amp; \\lambda_n \\\\ --- &amp; --- &amp; --- &amp; --- \\\\ \\lambda_{1,1} &amp; 0 &amp; \\dots &amp; 0\\\\ 0 &amp; x_{2,2} &amp; \\dots &amp; 0\\\\ 0 &amp; 0 &amp; \\dots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; \\lambda_{n,n}\\\\ \\end{array} \\end{array}\\right) \\] There are a few pointers to note about Eigenvectors and Eigenvalues, among many others: First, we now know that a scalar \\(\\lambda\\) is an Eigenvalue of a matrix if the following characteristic equation holds: \\[\\begin{align} det(A - \\lambda I) = 0 \\label{eqn:eqnnumber90} \\end{align}\\] Second, Eigenvalues are derived from the roots of a matrix’s characteristic polynomial. A characteristic polynomial can be obtained using the characteristic equation above. Third, Eigenvectors are derived using the following equations provided Eigenvalues are determined: \\[\\begin{align} (A - \\lambda I)v = 0 \\label{eqn:eqnnumber91} \\end{align}\\] Fourth, the Eigenspace is the set of Eigenvector solutions for which Equation (\\(\\ref{eqn:eqnnumber91}\\)) holds true. And so, therefore, we can also derive the equation for Eigenspace of the characteristic matrix as such: \\[\\begin{align} E_v = N(A - \\lambda I) \\label{eqn:eqnnumber92} \\end{align}\\] Lastly, there are algorithms and formulas worth looking into as they are designed to solve the roots (eigenvalues) of characteristic polynomials, to name a few: Jacobi Formula Samuelson Formula Le Verrier or Faddeev-Leverrier Algorithm Krylov Algorithm Weber-Voetter Newton Raphson Algorithm 2.15 Matrix Reconstruction using Eigenvalues and Eigenvectors Previously, we have shown an example of decomposing (factorizing) a matrix into smaller components by deriving the Eigenvectors and Eigenvalues. An important rule to note is that given both Eigenvectors and Eigenvalues, we should be able to reconstruct the original matrix. There are reasons why we need to decompose a matrix and maybe eventually reconstruct back the matrix. One reason is to be able to use the smaller components of a decomposed matrix for computational efficiency. Another reason, particularly for Eigenvalues, is to be able to sequence them in decreasing order to find the ones of greater value or greater importance, e.g., we cover PCA (primary component analysis) in the chapter about machine learning later. Lastly, when dealing with very high dimensional matrices, we need to perform matrix decomposition to reduce the dimensionality to a more manageable lower dimension. Other examples of matrix decomposition are: LU decomposition QR decomposition Cholesky decomposition Singular Value Decomposition (SVD) Jordan decomposition Schur decomposition Hessenberg Decomposition Polar Decomposition We cover them in depth later in the chapter. Let us perform a reconstruction of a matrix using the following Eigen Decomposition equation: \\[\\begin{align} A = P\\Lambda P^{-1} \\label{eqn:eqnnumber95} \\end{align}\\] where \\(P\\) is the derived matrix of Eigenvectors and \\(\\Lambda = (\\lambda \\cdotp I)\\) is the derived diagonal matrix of Eigenvalues: Let us use the same matrix A from previous section as our target matrix for the reconstruction: \\[ A = \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\] Given the following Eigenvectors, \\(P\\), and Eigenvalues, \\(\\Lambda\\): \\[\\begin{align*} P {}&amp;= \\left[\\begin{array}{ccc} 1 &amp; -1 &amp; 1\\\\ \\sqrt{3}-1 &amp; 0 &amp; -\\sqrt{3} - 1\\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\\\ \\\\ \\lambda &amp;= \\left[\\begin{array}{l} 3 + \\sqrt{3} \\\\ 4 \\\\ 3 - \\sqrt{3} \\end{array}\\right] \\rightarrow diagonalized \\rightarrow \\Lambda = \\left[\\begin{array}{rrr} 3 + \\sqrt{3} &amp; 0 &amp; 0\\\\ 0 &amp; 4 &amp; 0 \\\\ 0 &amp; 0 &amp; 3 - \\sqrt{3} \\end{array}\\right]_\\Lambda \\end{align*}\\] we start reconstructing: \\[\\begin{align*} A {}&amp;= P\\cdotp \\Lambda \\cdotp P^{-1} \\\\ &amp; = \\left[\\begin{array}{ccc} 1 &amp; -1 &amp; 1\\\\ \\sqrt{3}-1 &amp; 0 &amp; -\\sqrt{3} - 1\\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\cdotp \\left[\\begin{array}{rrr} \\lambda_1 &amp; 0 &amp; 0\\\\ 0 &amp; \\lambda_2 &amp; 0 \\\\ 0 &amp; 0 &amp; \\lambda_3 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccc} \\frac{\\sqrt{3}+1}{4\\sqrt{3}} &amp; \\frac{1}{2\\sqrt{3}} &amp; \\frac{\\sqrt{3}+1}{4\\sqrt{3}}\\\\ \\frac{-1}{2} &amp; 0 &amp;\\frac{1}{2} \\\\ \\frac{(\\sqrt{3} - 1}{4\\sqrt{3}} &amp; -\\frac{1}{2\\sqrt{3}} &amp; \\frac{(\\sqrt{3} - 1}{4\\sqrt{3}} \\end{array}\\right] \\\\ \\\\ A &amp;= \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\left[\\begin{array}{l}\\lambda_1 \\\\ \\lambda_2 \\\\ \\lambda_3 \\end{array}\\right]_{(diag)} = \\left[\\begin{array}{l} 3 + \\sqrt{3} \\\\ 4 \\\\ 3 - \\sqrt{3} \\end{array}\\right]_{(diag)} \\end{align*}\\] As an exercise, try to decompose and reconstruct the following matrix: \\[ A = \\left[\\begin{array}{rrr} 3 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 3 \\end{array}\\right] \\] Hint for the Eigenvalues: \\[ \\lambda_1 = 4, \\lambda_2 =3, \\lambda_3 = 1 \\] derived from its characteristic equation: \\[ -\\lambda^3 + 8\\lambda^2 - 19\\lambda + 12 = 0 \\] where its characteristic polynomial is the left-hand side of the equation: \\[ p(\\lambda) = -\\lambda^3 + 8\\lambda^2 - 19\\lambda + 12 \\] 2.16 Diagonalizability of a Matrix As much as we can use Eigenvectors and Eigenvalues to reconstruct a matrix, we can also use Eigenvectors to diagonalize a matrix. Let us use the same matrix A from the previous section as our target matrix for diagonalization: \\[ A = \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\] To perform a diagonalization of matrix A, we use the following equation: \\[\\begin{align} A_D = P^{-1}AP \\label{eqn:eqnnumber75} \\end{align}\\] So given the following Eigenvectors, \\(P\\): \\[\\begin{align*} P {}&amp;= \\left[\\begin{array}{ccc} 1 &amp; -1 &amp; 1\\\\ \\sqrt{3}-1 &amp; 0 &amp; -\\sqrt{3} - 1\\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\end{align*}\\] we can derive the diagonal of a matrix this way: \\[\\begin{align*} A_D &amp;= P^{-1}\\cdotp A \\cdotp P\\\\ &amp; = \\left[\\begin{array}{ccc} \\frac{\\sqrt{3}+1}{4\\sqrt{3}} &amp; \\frac{1}{2\\sqrt{3}} &amp; \\frac{\\sqrt{3}+1}{4\\sqrt{3}}\\\\ \\frac{-1}{2} &amp; 0 &amp;\\frac{1}{2} \\\\ \\frac{(\\sqrt{3} - 1}{4\\sqrt{3}} &amp; -\\frac{1}{2\\sqrt{3}} &amp; \\frac{(\\sqrt{3} - 1}{4\\sqrt{3}} \\end{array}\\right] \\cdotp \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\cdotp \\left[\\begin{array}{ccc} 1 &amp; -1 &amp; 1\\\\ \\sqrt{3}-1 &amp; 0 &amp; -\\sqrt{3} - 1\\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\end{align*}\\] \\[ A_D = \\left[\\begin{array}{rrr} 3 + \\sqrt{3} &amp; . &amp; . \\\\ . &amp; 4 &amp; . \\\\ . &amp; . &amp; 3 - \\sqrt{3} \\end{array}\\right] \\] Notice that diagonalizing a matrix, given its Eigenvectors, yields a matrix whose diagonal entries are the corresponding Eigenvalues. There may be cases when a matrix is not diagonalizable. That can be determined by looking at the characteristic polynomial of the matrix and comparing the algebraic multiplicity against its geometric multiplicity. 2.17 Trace of a Square Matrix We can use the Eigenvalues to trace a matrix. That means that we can sum up the main diagonal (the Eigenvalues) of a matrix starting from the upper left. It is expressed as: \\[\\begin{align} tr(A) = \\sum_{i=1}^n a_{ii} = a_{11} + a_{22} + ... + a_{nn} \\label{eqn:eqnnumber76} \\end{align}\\] Let us use the same matrix A from previous section as our target matrix for tracing: \\[ A = \\left[\\begin{array}{rrr} 4 &amp; 1 &amp; 0 \\\\ 1 &amp; 2 &amp; 1 \\\\ 0 &amp; 1 &amp; 4 \\end{array}\\right] \\] To perform a trace of matrix A, we use the following equation: \\[ P(A) = 4 + 2 + 4 = 10 \\] Also, it helps to know two properties of trace matrix specially when dealing with derivations. Given three matrices: \\[ X = \\left[\\begin{array}{cc}1 &amp; 3 \\\\ 2 &amp; 4 \\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Y = \\left[\\begin{array}{cc}8 &amp; 8 \\\\ 8 &amp; 8 \\end{array}\\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Z = \\left[\\begin{array}{cc}7 &amp; 7 \\\\ 7 &amp; 7 \\end{array}\\right] \\] # naive implementation tr &lt;- function(m) { sum(diag(m)) } x = matrix(c(1,2,3,4), 2) y = matrix(c(8,8,8,8), 2) z = matrix(c(7,7,7,7), 2) trace matrix has cyclic permutation property, e..g.: \\[ tr(XYZ) = tr(YZX) = tr(ZYX) = 1120 \\] c( tr(x %*% y %*% z), tr(y %*% z %*% x), tr(z %*% x %*% y) ) ## [1] 1120 1120 1120 trace matrix has additive property, e..g.: \\[ tr(X + Y + Z) = tr(X) + tr(Y) + tr(Z) = 1120 \\] c( tr(x + y + z), tr(x) + tr(y) + tr(z) ) ## [1] 35 35 2.18 Algebraic and Geometric Multiplicity The multiplicity of a matrix can be determined by solving for the eigenvectors and eigenvalues of a matrix: Let us use the following 4x4 matrix: \\[ A = \\left[\\begin{array}{cccc} 1 &amp; 1 &amp; 9 &amp; 0\\\\ 0 &amp; 4 &amp; 3 &amp; 0\\\\ 0 &amp; 3 &amp; 4 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; 1 \\\\ \\end{array}\\right] \\] First, we solve for the eigenvalues by evaluating the determinant: \\[ det \\left( \\begin{array}{rrrr} 1-\\lambda &amp; 1 &amp; 9 &amp; 0\\\\ 0 &amp; 4-\\lambda &amp; 3 &amp; 0\\\\ 0 &amp; 3 &amp; 4-\\lambda &amp; 0\\\\ 0 &amp; 6 &amp; 1 &amp; 1-\\lambda\\\\ \\end{array} \\right) = 0 \\] We get the following characteristic equation along with its minimal polynomials : \\[\\begin{align*} \\lambda^4 - 10\\lambda^3 + 24\\lambda^2 - 22\\lambda + 7 {}&amp;= 0\\\\ (\\lambda-1)(\\lambda^3 - 9\\lambda^2 + 15\\lambda - 7) {}&amp;= 0\\\\ (\\lambda-1)(\\lambda-1)(\\lambda^2 - 8\\lambda + 7) {}&amp;= 0\\\\ (\\lambda-1)(\\lambda-1)(\\lambda-1)(\\lambda-7) {}&amp;= 0\\\\ (\\lambda-1)^3(\\lambda-7) &amp;= 0 \\end{align*}\\] Here, we get four eigenvalues: \\[ \\lambda = 1\\ \\ \\ \\ \\ \\lambda = 1\\ \\ \\ \\ \\ \\lambda = 1\\ \\ \\ \\ \\ \\lambda=7 \\] As for Algebraic multiplicity, the eigenvalues for \\(\\lambda\\) repeats three times for \\(\\lambda=1\\), and only one time for \\(\\lambda=7\\). Therefore, the algebraic multiplicity for \\(\\lambda=1\\) is three and the algebraic multiplicity for \\(\\lambda=7\\) is one. It can be expressed this way: \\[ M_a(\\lambda=1) = 3 \\ \\ \\ \\ \\ \\ M_a(\\lambda=7) = 1 \\] Furthermore, if we inject the eigenvalues into Equation (\\(\\ref{eqn:eqnnumber61}\\)), then we get the following eigenvectors: \\[\\begin{align*} for\\ \\lambda {}&amp;=1 ,\\ \\ \\ \\ v1 = \\left[\\begin{array}{c} 1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right] ,\\ \\ \\ \\ v2 = \\left[\\begin{array}{c} 0 \\\\ 0 \\\\ 0 \\\\ 1\\end{array}\\right] \\\\ \\\\ for\\ \\lambda &amp;=7 ,\\ \\ \\ \\ v3 = \\left[\\begin{array}{c} 10 \\\\ 6 \\\\ 6 \\\\ 7 \\end{array}\\right] \\end{align*}\\] As for Geometric multiplicity, the eigenvalue \\(\\lambda=1\\) yields two distinct eigenvectors \\(\\mathbf{\\vec{v1}}\\) and \\(\\mathbf{\\vec{v2}}\\), and only one eigenvector \\(\\mathbf{\\vec{v3}}\\) for eigenvalue \\(\\lambda=7\\) . Therefore, the geometric multiplicity for \\(\\lambda=1\\) is two and the geometric multiplicity for \\(\\lambda=7\\) is one. It can be expressed this way: \\[ M_g(\\lambda=1) = 2 \\ \\ \\ \\ \\ \\ M_g(\\lambda=7) = 1 \\] We can also say that the geometric multiplicity of eigenvectors equals the dimension of the eigenspace of their corresponding eigenvalue \\(\\lambda\\) of matrix A. \\[ M_g(\\lambda) = E_a(\\lambda) \\] Now, notice the following: \\[ M_a(\\lambda=1)=3\\ \\ \\ &gt;\\ \\ \\ M_g(\\lambda=1)=2 \\] The Algebraic multiplicity is greater than the geometric multiplicity. That means that the matrix is not diagonalizable. We can show this by analyzing the minimal polynomial of the characteristic polynomial. Note that the corresponding characteristic matrices of a minimum set of polynomials - the minimal polynomials, when multiplied together, will result in a zero matrix. \\[ (\\lambda-1)^2(\\lambda-7) = 0 \\leftarrow \\text{\\{minimal polynomials\\}} \\] It shows that the algebraic multiplicity of the minimal polynomial is two, matching the geometric multiplicity. If we count the number of geometric multiplicity for all Eigenvectors, we see three, which is lesser than the expected full rank of a matrix which is supposed to be four. So we are missing one eigenvector, therefore. That missing non-zero eigenvector is what we call a generalized eigenvector. We show how to find a generalized eigenvector later in this chapter under Jordan decomposition section. 2.19 Types of Matrices Let us review a few types of matrices: Invertible Matrix A matrix is Invertible based on the following few listed properties (which we have covered and discussed), among many others: its determinant is non-zero its columns are linearly independent it is non-singular it is full-rank, and therefore its nullity is zero Positive Definite Matrix A matrix is Positive Definite based on the following few properties, among many others: it is symmetric and its eigenvalues are all positive it is symmetric and its pivots, in row-echelon form, are all positive if there exists a vector \\(\\vec{v}\\) such that \\(\\vec{v}^TA\\vec{v} &gt; 0\\ where\\ \\vec{v} \\neq 0\\). Identity Matrix An Identity Matrix is a square matrix with the diagonal entries, from upper left to lower right, being one, and all others being zero. The following notations are used: \\[ \\mathbf{1}_n = I_n = \\left[\\begin{array}{ccccc} 1_{1,1} &amp; 0_{1,2} &amp; \\dots &amp; 0_{1,n} \\\\ 0_{2,1} &amp; 1_{2,2} &amp; \\dots &amp; 0_{2,n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0_{n,1} &amp; 0_{n,2} &amp; \\dots &amp; 1_{n,n} \\end{array}\\right], \\ \\ \\ \\ \\ \\ \\ e.g.\\ 10\\mathbf{1}_2 = 10I_2 = \\left[ \\begin{array}{cc} 10 &amp; 0 \\\\ 0 &amp; 10 \\end{array} \\right] \\] Diagonal and TriDiagonal Matrices Both Diagonal and TriDiagonal Matrices have diagonal entries, from upper left to lower right, being non-zero, and all others being zero. Additionally, the sum of the non-zero diagonal entries is what we call the trace of the matrix. \\[ \\mathbf{A_{i,j}} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; 0_{1,2} &amp; \\dots &amp; 0_{1,j} \\\\ 0_{2,1} &amp; x_{2,2} &amp; \\dots &amp; 0_{2,j} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0_{i,1} &amp; 0_{i,2} &amp; \\dots &amp; x_{i,j} \\end{array}\\right] \\mathbf{A_{i,j}} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; x_{1,2} &amp; \\dots &amp; 0_{1,j} \\\\ x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; 0_{2,j} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; x_{i-1,j} \\\\ 0_{i,1} &amp; 0_{i,2} &amp; x_{i,j-1} &amp; x_{i,j} \\end{array}\\right] \\] Hermitian (Symmetric) Matrix A Hermitian Matrix is a matrix that is self-adjoint in which its conjugate is equal to the transpose of its complex conjugate. \\[ \\mathbf{A_{i,j}} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; c_{1,2} &amp; b_{1,3} &amp; a_{1,j} \\\\ c_{2,1} &amp; x_{2,2} &amp; c_{2,3} &amp; b_{2,j} \\\\ b_{3,1} &amp; c_{3,2} &amp; \\ddots &amp; c_{3,j} \\\\ a_{i,1} &amp; b_{i,2} &amp; c_{i,3} &amp; x_{i,j} \\end{array}\\right] \\] Hessenberg Matrix A Hessenberg Matrix comes as an Upper Hessenberg matrix or Lower Hessenberg matrix. Upper Hessenberg matrix is an upper triangular matrix with an upper super diagonal and can be expressed as: for all i,j where i &gt; j + 1 then \\(a_{i,j}\\) = 0. Lower Hessenberg matrix is a lower triangular matrix with a lower super diagonal and can be expressed as: for all i,j where j &gt; i + 1 then \\(a_{i,j}\\) = 0. Also, a Hessenberg matrix is considered unreduced if the super diagonal entries are non-zero. \\[ \\mathbf{H_{upper}} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; x_{1,2} &amp; x_{1,3} &amp; x_{1,j} \\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; x_{2,j} \\\\ 0 &amp; x_{3,2} &amp; \\ddots &amp; x_{3,j} \\\\ 0 &amp; 0 &amp; x_{i,3} &amp; x_{i,j} \\end{array}\\right] \\mathbf{H_{lower}} = \\left[\\begin{array}{ccccc} x_{1,1} &amp; c_{1,2} &amp; 0 &amp; 0 \\\\ x_{2,1} &amp; x_{2,2} &amp; x_{2,3} &amp; 0 \\\\ x_{3,1} &amp; x_{3,2} &amp; \\ddots &amp; x_{3,j} \\\\ x_{4,1} &amp; x_{4,2} &amp; x_{i,3} &amp; x_{i,j} \\end{array}\\right] \\] Orthogonal Matrix An Orthogonal Matrix is a matrix in which each of its column vectors results in a zero dot product with the other column vectors. \\[ A^{-1} = A^T \\iff A^TA = AA^T = I \\] \\[ \\mathbf{A^TA} = \\left[\\begin{array}{ccccc} a^t_{1}a_{1} &amp; a^t_{1}a_{2} &amp; \\dots &amp; a^t_{1}a_{j} \\\\ a^t_{2}a_{1} &amp; a^t_{2}a_{2} &amp; \\dots &amp; a^t_{2}a_{j} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a^t_{i}a_{1} &amp; a^t_{i}a_{1} &amp; \\vdots &amp; a^t_{i}a_{j} \\end{array}\\right] \\] Vandermonde Matrix A Vandermonde Matrix has the following format: \\[\\begin{align*} rowwise {}&amp; \\rightarrow \\{1, x^1, x^2,..., x^{n-1} \\} \\\\ columnwise &amp; \\rightarrow \\{1, x^1, x^2,..., x^{n-1} \\}^T \\end{align*}\\] From the following equations: \\[\\begin{align*} y_0 {}&amp;= c_0 + c_1x_0^1 + c_2x_0^2 + c_3x_0^3 + ... + c_nx_0^n \\\\ y_1&amp; = c_0 + c_1x_1^1 + c_2x_1^2 + c_3x_1^3 + ... + c_nx_1^n \\\\ &amp;\\vdots \\\\ y_n &amp;= c_0 + c_1x_n^1 + c_2x_n^2 + c_3x_n^3 + ... + c_nx_n^n \\end{align*}\\] yields the Vandermonde Matrix: \\[ A = \\left[\\begin{array}{ccccc} 1 &amp; x_{1,1}^1 &amp; x_{1,2}^2 &amp; \\dots &amp; x_{1,n}^{n-1} \\\\ 1 &amp; x_{2,1}^1 &amp; x_{2,2}^2 &amp; \\dots &amp; x_{2,n}^{n-1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_{n,1}^1 &amp; x_{n,2}^2 &amp; \\dots &amp; x_{n,n}^{n-1} \\end{array}\\right]\\ or\\ A = \\left[\\begin{array}{ccccc} 1 &amp; 1 &amp; 1 &amp; \\dots &amp; 1 \\\\ x_{1,1}^1 &amp; x_{1,2}^1 &amp; x_{1,3}^2 &amp; \\dots &amp; x_{1,n}^{n-1} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n,1}^1 &amp; x_{n,2}^1 &amp; x_{n,3}^2 &amp; \\dots &amp; x_{n,n}^{n-1} \\end{array}\\right] \\] Jacobian Matrix A Jacobian Matrix is a matrix containing first-degree partial derivatives of a multivariate function. \\[ J = \\left[\\begin{array}{ccccc} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\dots &amp; \\frac{\\partial f_m}{\\partial x_n} \\end{array}\\right] \\] Hessian Matrix A Hessian Matrix is a matrix containing second-degree partial derivatives of a multivariate function. \\[ H = \\left[\\begin{array}{ccccc} \\frac{\\partial^2 f}{\\partial x_1^2} &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n} \\\\ \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_2^2} &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial^2 f}{\\partial x_n \\partial x_1} &amp; \\frac{\\partial^2 f}{\\partial x_n \\partial x_2} &amp; \\dots &amp; \\frac{\\partial^2 f}{\\partial x_n^2} \\end{array}\\right] \\] Covariance and Correlation Matrix Covariance and Correlation Matrix contain covariance and correlation of variables. Below is the variance of a variable (x): \\[\\begin{align} var(x) = \\sigma^2(x) = \\sum(x - \\bar{x})^2 \\ \\ \\ \\ \\ x \\in \\mathbb{R}^n \\label{eqn:eqnnumber62} \\end{align}\\] Below is (co)variance of two variables (x,y): \\[\\begin{align} cov(x, y) = \\sigma(x,y) = \\sum(x - \\bar{x})(y - \\bar{y}) \\ \\ \\ \\ \\ x,y \\in \\mathbb{R}^n \\label{eqn:eqnnumber63} \\end{align}\\] The covariance matrix for x, y, and z variables is expressed as: \\[ \\Sigma(x,y,z) = \\left[ \\begin{array}{ccc} var(x,x) &amp; cov(x,y) &amp; cov(x,z)\\\\ cov(y,x) &amp; var(y,y) &amp; cov(y,z)\\\\ cov(z,x) &amp; cov(z,y) &amp; var(z,z)\\\\ \\end{array} \\right] = \\left[ \\begin{array}{ccc} \\sigma(x,x) &amp; \\sigma(x,y) &amp; \\sigma(x,z)\\\\ \\sigma(y,x) &amp; \\sigma(y,y) &amp; \\sigma(y,z)\\\\ \\sigma(z,x) &amp; \\sigma(z,y) &amp; \\sigma(z,z)\\\\ \\end{array} \\right] \\] The correlation matrix for x,y,z variables is expressed as: \\[ \\rho(x,y,z) = \\left[ \\begin{array}{ccc} \\frac{var(x,x)}{\\sigma^2(x)\\sigma^2(x)} &amp; \\frac{cov(x,y)}{\\sigma^2(x)\\sigma^2(y)} &amp; \\frac{cov(x,z)}{\\sigma^2(x)\\sigma^2(z)}\\\\ \\frac{cov(y,x)}{\\sigma^2(y)\\sigma^2(x)} &amp; \\frac{var(y,y)}{\\sigma^2(y)\\sigma^2(y)} &amp; \\frac{cov(y,z)}{\\sigma^2(y)\\sigma^2(z)}\\\\ \\frac{cov(z,x)}{\\sigma^2(z)\\sigma^2(x)} &amp; \\frac{cov(z,y)}{\\sigma^2(z)\\sigma^2(y)} &amp; \\frac{var(z,z)}{\\sigma^2(z)\\sigma^2(z)}\\\\ \\end{array} \\right] \\] We shall explore this matrix more in Chapter 9 (Computational Learning I) when we cover PCA in which we discuss how Eigenvalues are known to follow the direction of the maximum variance of a matrix. Cholesky-Covariance Matrix A Cholesky-Covariance Matrix is a matrix that can be decomposed using Cholesky decomposition against a covariance matrix, forming a lower-triangular (L) matrix. More about LU (lower-upper) decomposition discussion to follow later in this chapter. \\[ A = \\left[\\begin{array}{ccccc} x_{1,1} &amp; 0 &amp; \\dots &amp; 0 \\\\ x_{2,1} &amp; x_{2,2} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{i,1} &amp; x_{i,2} &amp; \\dots &amp; x_{i,j} \\end{array}\\right] \\] Sparse Matrix A Sparse Matrix is a matrix with most of its elements equal to zero. An example is a diagonal and an identity matrix. Computationally, there are methods that can be used to convert sparse matrices into structures for storage efficiency. \\[ A = \\left[\\begin{array}{ccccc} x_{1,1} &amp; 0 &amp; \\dots &amp; 0 \\\\ 0 &amp; x_{2,2} &amp; \\dots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\dots &amp; x_{i,j} \\end{array}\\right] \\] Nilpotent Matrix A Nilpotent Matrix is a matrix with the following few characteristics, among others: the determinant is zero the diagonal entries are zeroes the characteristic polynomial for N is \\(x^n\\). See Jordan form later in this chapter. Unitary Matrix A Unitary Matrix is a matrix whose inverse equals its conjugate transpose. \\[\\begin{align} A^{-1} = A^* \\label{eqn:eqnnumber163} \\end{align}\\] where * denotes conjugate transpose 2.20 Matrix Factorization Before we jump into Root Finding and Linear Regressions, let us first cover some efficient ways to solve systems of linear equations involving matrices. Given the following matrix equation: \\[\\begin{align} Ax = y \\label{eqn:eqnnumber64} \\end{align}\\] we solve for x: \\[\\begin{align} A^{-1}Ax {}&amp; = A^{-1}y \\label{eqn:eqnnumber96}\\\\ x &amp;= A^{-1}y \\label{eqn:eqnnumber97} \\end{align}\\] In previous sections, we showed how to get the inverse of A by multiplying the reciprocal of its determinant by its adjugate. Then, we showed how to solve the equation by performing matrix multiplication with ‘y’. This section covers a list of matrix decomposition methods that may help us deal with matrices more efficiently. We start by recalling one that we have already discussed in this chapter in the context of Eigenvectors and Eigenvalues - Eigenvalue Decomposition. 2.20.1 Eigen (Spectral) Decomposition The idea is to decompose a matrix into its Eigenvectors, \\(P\\), and Eigenvalues, \\(\\Lambda\\), forming the following equation: \\[\\begin{align} A = P\\Lambda P^{-1} \\label{eqn:eqnnumber98} \\end{align}\\] Details of the decomposition and reconstruction of a matrix are discussed in Matrix Reconsruction Section. 2.20.2 LU Decomposition (Doolittle Algorithm) The idea is to decompose a square matrix into its Lower-Triangular and Upper-Triangular forms, forming the equation: \\[\\begin{align} A = LU \\label{eqn:eqnnumber99} \\end{align}\\] We transform the following sample matrix equation: \\[ \\left[ \\begin{array}{cccc} a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\\\ b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\\\ c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\\\ d_1 &amp; d_2 &amp; d_3 &amp; d_4 \\end{array} \\right]_{A} \\left[\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right]_{x} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{array}\\right]_{y} \\] into Lower and Upper Triangular form (Note that dotted entries here represent zero entries): \\[ \\left[ \\begin{array}{rrrr} 1 &amp; . &amp; . &amp; . \\\\ L_{b_1} &amp; 1 &amp; . &amp; . \\\\ L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\\\ L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 \\end{array} \\right]_{L_A} \\left[ \\begin{array}{cccc} U_{a_1} &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\\\ . &amp; U_{b_2} &amp; U_{b_3} &amp; U_{b_4} \\\\ . &amp; . &amp; U_{c_3} &amp; U_{c_4} \\\\ . &amp; . &amp; . &amp; U_{d_4} \\end{array} \\right]_{U_A} \\left[\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right]_{x} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{array}\\right]_{y} \\] From here, we end up with a transformed matrix equation: \\[\\begin{align} A = LU \\rightarrow LUx = y \\label{eqn:eqnnumber100} \\end{align}\\] And to solve for the system of equations, we have two extra formulas we can use. \\[\\begin{align} Lu_y = y, \\ \\ \\ \\ Ux = u_y \\label{eqn:eqnnumber101} \\end{align}\\] We first solve for \\(u_y\\) from \\(Lu_y=y\\), then use \\(u_y\\) to solve for x from \\(Ux=u_y\\). \\[\\begin{align} u_y = L^{-1}y \\rightarrow\\ \\ \\ \\ x = U^{-1}u_y \\label{eqn:eqnnumber102} \\end{align}\\] To do that, we perform LU decomposition by the Gaussian Elimination algorithm. We covered Gaussian Elimination in this chapter to reduce matrices to Echelon Form. Here, instead of just dealing with a matrix, let us transform the following system of equations: \\[ \\left(\\begin{array}{lll} 1x_1 + 5x_2 + 5x_3 = 6 \\\\ 2x_1 + 3x_2 + 4x_3 = 5 \\\\ 3x_1 + 3x_2 + 3x_3 = 6 \\end{array}\\right) \\] We start by first mapping the equation into a matrix form: \\[ \\left[ \\begin{array}{ccc} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 3 &amp; 4 \\\\ 3 &amp; 3 &amp; 3 \\\\ \\end{array} \\right]_{A} \\left[\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\end{array}\\right]_{x} = \\left[\\begin{array}{r} 6 \\\\ 5 \\\\ 6 \\end{array}\\right]_{y} \\] Here, we will use an elementary matrix - the identity matrix - as a utility to help in decomposing the matrix. Let us use an augmented matrix to include vector y - so that we can validate our steps later. \\[ \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{I_A} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A|y} = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A|y} \\] The idea is to reduce the matrix to Echelon Form using Gaussian Elimination. We work our way from the left-most column of the lower triangular portion of the identity matrix to the right column. So, starting from the first column, we subtract two times the 1st row from the 2nd row and 3 of the 1st row from the 3rd row: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ -3 &amp; 0 &amp; 1 \\end{array} \\right]_{l_1} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array}\\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A} = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5\\\\ 0 &amp; -12 &amp; -12 \\end{array}\\left|\\begin{array}{r}6 \\\\ -7 \\\\ -12\\end{array}\\right. \\right]_{U_1|u_{y_1}} \\] Next, we use another identity matrix as a utility matrix to solve for the 2nd column. We subtract 2 of the 2nd row from the 3rd row: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -2 &amp; 1 \\end{array} \\right]_{l_2} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; -12 &amp; -12 \\end{array}\\left|\\begin{array}{r}6 \\\\ -7 \\\\ -12\\end{array}\\right.\\right]_{l_1A = U_1} = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5\\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array}\\left|\\begin{array}{r}6 \\\\ -7 \\\\ 2 \\end{array}\\right. \\right]_{U_A|u_y} \\] Since we have already achieved an upper-triangular form, \\(U_A\\), we don’t have to continue further. We have completed the Gaussian elimination process. From this point, we need to derive the lower-triangular form, \\(L_A\\), using the following: \\[ L_A = (l_m \\cdot l_{m-1} \\cdot \\ ...\\ \\cdot l_2 \\cdot l_1)^{-1} = l_1^{-1} \\cdot l_2^{-1} \\cdot\\ ...\\ \\cdot\\ l_{m-1}^{-1} \\cdot l_{m}^{-1} \\] That gives us: \\[ L_A = \\left( \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -2 &amp; 1 \\end{array} \\right]_{l_2} \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ -3 &amp; 0 &amp; 1 \\end{array} \\right]_{l_1} \\right)^{-1} = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A} \\] Overall, the LU decomposition gives us the following equation: \\[\\begin{align} A = L_AU_A \\label{eqn:eqnnumber103} \\end{align}\\] where: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{U_A} \\] Using the LU matrices, let us now solve for x. There are two equations we will use to solve for x. First, we solve for \\(u_y\\) using \\(L_A^{-1}y\\). Then with \\(u_y\\), we solve for x using Equation (\\(\\ref{eqn:eqnnumber102}\\)). Solving for \\(u_y\\) requires \\(L^{-1}\\) and \\(y\\): \\[ u_y = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ -2 &amp; 1 &amp; 0 \\\\ 1 &amp; -2 &amp; 1 \\end{array} \\right]_{L^{-1}} \\left[\\begin{array}{r} 6 \\\\ 5\\\\ 6 \\end{array}\\right]_{y} = \\left[\\begin{array}{r} 6\\\\ -7 \\\\ 2 \\end{array}\\right]_{u_y} \\] Note that we have already solved for \\(u_y\\) as part of the augmented matrix in the Gaussian elimination portion. Let us validate in any case. For that, we can use two alternative methods. by Gauss forward elimination: \\[ L_A^{\\{augmented\\ u_y\\}} = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{L_A|y} \\rightarrow u_y = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\left|\\begin{array}{r}6 \\\\ -7 \\\\ 2\\end{array}\\right. \\right]_{I_L|u_y} \\] or by forward substitution: Given \\(L_A\\) and \\(y\\), the equation for forward substitution is: \\[\\begin{align} u_{y_1} {}&amp;= \\frac{ y_1}{L_{A_{1,1}}} \\leftarrow initial \\label{eqn:eqnnumber104}\\\\ u_{y_i} &amp;= \\frac{ \\left( y_{i} - \\sum_{j=1}^{i-1} L_{A_{i,j}u_{y_j}} \\right)} {L_{A_{i,i}}}, \\ \\ \\ \\ \\ \\text{where }i = 2,3, ... n \\label{eqn:eqnnumber105} \\end{align}\\] Finally, solving for \\(x\\) requires \\(U^{-1}\\) and \\(u_y\\): \\[ x = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{U_A}^{-1} \\left[\\begin{array}{r} 6 \\\\ -7 \\\\ 2 \\end{array}\\right]_{u_y} = \\left[\\begin{array}{r} 1 \\\\ 2 \\\\ -1 \\end{array}\\right]_{x} \\] For that, we can use two alternative methods: by Jordan backward elimination: \\[ U_A^{\\{augmented\\ u_y\\}} = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\left|\\begin{array}{r} 6 \\\\ -7 \\\\ 2\\end{array}\\right. \\right]_{U_A^{-1}|u_y} \\rightarrow x = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\left|\\begin{array}{r}1 \\\\ 2 \\\\ -1\\end{array}\\right. \\right]_{I_U|x} \\] or by backward substitution: Given \\(U_A\\) and \\(u_y\\), the equation for backward substitution is: \\[\\begin{align} x_n {}&amp;= \\frac{ u_{y_n}}{U_{A_{n,n}}} \\leftarrow initial \\label{eqn:eqnnumber107}\\\\ x_i &amp;= \\frac{ \\left(u_{y_i} - \\sum_{j=i+1}^n U_{A_{i,j}x_j}\\right)} { U_{A_{i,i}}} , \\ \\ \\ \\ \\ \\text{where }i = n-1, n-2, ..., 1 \\label{eqn:eqnnumber108} \\end{align}\\] Therefore, our solution for x is: \\[ x_1 = 1,\\ \\ \\ \\ x_2 = 2, \\ \\ \\ \\ x_3 = -1 \\] For a sample implementation of solving equations by LU decomposition, see the Doolittle algorithm in the following few sections. Gaussian Elimination with Partial Pivot (row swapping): Sometimes, it helps to swap rows to perform Gaussian elimination easily, especially when we have more leading zeros in the middle of our elimination for the next rows that we need to pivot. In a case where our leading diagonal entry happens to be zero, we can swap the row with any other subsequent row that has a non-zero diagonal entry, e.g., we can swap row 2nd and row 3rd below: \\[ \\left[ \\begin{array}{ccc} 0 &amp; 8 &amp; 9 \\\\ 0 &amp; 0 &amp; 6 \\\\ 1 &amp; 2 &amp; 3 \\\\ \\end{array} \\right] \\] To swap 1st row with 3rd row, we use a Permutation matrix (\\(P_{r_a,r_b}\\)) as a swap utility. See below: \\[ P_{1,3}A = \\left[ \\begin{array}{ccc} 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\end{array} \\right]_{P_{1,3}} \\left[ \\begin{array}{ccc} 0 &amp; 8 &amp; 9 \\\\ 0 &amp; 0 &amp; 6 \\\\ 1 &amp; 2 &amp; 3 \\end{array} \\right]_A = \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 6 \\\\ 0 &amp; 8 &amp; 9 \\end{array} \\right]_{A_{1,3}} \\] To swap 2nd row with 3rd row: \\[ P_{2,3}A = \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\end{array} \\right]_{P_{2,3}} \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 0 &amp; 6 \\\\ 0 &amp; 8 &amp; 9 \\end{array} \\right]_{A_{1,3}} = \\left[ \\begin{array}{ccc} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 8 &amp; 9 \\\\ 0 &amp; 0 &amp; 6 \\end{array} \\right]_{A_p} \\] Overall, we performed two swaps (permutations): \\[ P_{2,3}P_{1,3}A = A_p \\] Now, putting the LU factorization in the picture, there are times when we need to perform partial pivoting in the middle of the operation: \\[ l_2A\\ (\\ P_{1,3}l_1(\\ P_{1,2}A\\ )\\ ) = L_AU_A \\] That reads as Swap 1st and 2nd row of matrix A, then perform Gaussian elimination (GE) on 1st column. Then swap the 1st and 3rd row of the resultant matrix, then continue with the GE on the second column. Here, partial pivot may refer to swapping rows then performing the GE for the row and column - where we expect the pivot to be ( the non-zero leading entry). On the other hand, full pivoting is when swapping both rows and columns. On that note, see if you can get the original systems of equations above into the following matrix below using a swap utility as we discussed: \\[ \\left(\\begin{array}{lll} 3x_1 + 3x_2 + 3x_3 = 6 \\\\ 2x_1 + 3x_2 + 4x_3 = 5 \\\\ 1x_1 + 5x_2 + 5x_3 = 6 \\end{array}\\right) \\] There are two alternative solution for ‘x’ (see what operations to use to arrive at the solution): \\[\\begin{align*} x_1 {}&amp;= 1,\\ \\ \\ \\ x_2 = 2, \\ \\ \\ \\ x_3 = -1\\ or \\\\ x_1 &amp;= 1,\\ \\ \\ \\ x_2 = 1, \\ \\ \\ \\ x_3 = 0 \\end{align*}\\] Gaussian Elimination with Sherman-Morrison: There are cases in which our invertible matrix is changed, for example, where an entry (a rank-one entry) in the matrix is updated, resulting in a new given y, call it \\(y_{upd}\\). To avoid having to iterate through the whole process of LU factorization again because of this change, we use the Sherman-Morrison formula to solve for x: \\[\\begin{align} x_1 = x_0 + \\left( \\frac{v^Tx_0}{1 - v^Tz} \\right)z,\\ \\ \\ where\\ uv^T\\text{ is given and }z = A^{-1}u \\label{eqn:eqnnumber110} \\end{align}\\] To illustrate, suppose that we updated our original matrix, A, in that the entry in \\(A_{1,3}\\) (1st column, 3rd row) changes from 1 to 3. \\[ \\left[ \\begin{array}{ccc} 3 &amp; 3 &amp; 3\\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\left|\\begin{array}{c}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A|y} \\rightarrow \\left[ \\begin{array}{ccc} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 5 &amp; 5 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\6\\end{array}\\right. \\right]_{A_{upd}|y} \\] Suppose also that we have solved \\(Ax = y\\) for the old x using LU factorization where: \\[ x_{old} = \\left[ \\begin{array}{rrr} 1 \\\\ 2 \\\\ -1 \\end{array} \\right]_{x_{0}} \\] So how do we solve for the new x? First, let us use a rank-one update matrix expressed as \\(uv^T\\). A rank-one matrix means, in this case, that we are updating the 1st rank order (column-wise). For example, we use the following matrix, generated from \\(uv^T\\), to perform a rank-one update against the 1st column of our original matrix. The generated matrix is a representation of a perturbation in \\(A_{1,3}\\) indicating a change in the entry from 1 to 3 (or to change the entire 1st column where \\(u\\) is the update vector, nx1, and \\(v^T\\) is the rank order vector, 1xn; here, it is the rank-one or the 1st rank (column-wise)): \\[ uv^T = \\left[ \\begin{array}{r} 0 \\\\ 0 \\\\ -2 \\\\ \\end{array} \\right]_u \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ \\end{array} \\right]_{v^T} = \\left[ \\begin{array}{rrr} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 \\end{array} \\right]_{uv^T} \\] Second, let us solve for z in the equation \\(Az = u\\). \\[ z = A^{-1}u = \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3\\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A}^{-1} \\left[ \\begin{array}{rrr} 0 \\\\ 0 \\\\ -2 \\end{array} \\right]_{u} = \\left[ \\begin{array}{rrr} 0.5 \\\\ -1.5 \\\\ 1.0 \\end{array} \\right]_{z} \\] Third, let us now use Sherman-Morrison formula to solve for the new x, \\[ x_{new} = x_{old} + \\left(1 - v^Tz\\right)^{-1}v^Tx_{old}z = x_{old} + \\left( \\frac{v^Tx_{old}}{1 - v^Tz} \\right) z, \\] for example: \\[\\begin{align*} x_{new} {}&amp;= \\left[ \\begin{array}{rrr} 1 \\\\ 2 \\\\ -1 \\end{array} \\right]_{x_{old}} + \\left( \\frac{ \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\end{array} \\right]_{v^T} \\left[ \\begin{array}{rrr} 1 \\\\ 2 \\\\ -1 \\end{array} \\right]_{x_{old}} } {1 - \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\end{array} \\right]_{v^T} \\left[ \\begin{array}{rrr} 0.5 \\\\ -1.5 \\\\ 1.0 \\end{array} \\right]_{z} } \\right) \\left[ \\begin{array}{rrr} 0.5 \\\\ -1.5 \\\\ 1.0 \\end{array} \\right]_{z} \\\\ \\\\ &amp;= \\left[ \\begin{array}{rrr} 1 \\\\ 2 \\\\ -1 \\end{array} \\right]_{x_{old}} + \\left(\\frac{1}{1 - 0.5}\\right) \\left[ \\begin{array}{rrr} 0.5 \\\\ -1.5 \\\\ 1.0 \\end{array} \\right]_{z} \\\\ \\\\ &amp;= \\left[ \\begin{array}{rrr} 2 \\\\ -1 \\\\ 1 \\end{array} \\right]_{x_{new}} \\end{align*}\\] From here, we can validate the solution using the equation below, given \\(uv^T\\) and the original \\(y\\): \\[\\begin{align} (A - uv^T)x_{new} = y \\rightarrow\\ \\ \\ x_{new} = (A - uv^T)^{-1}y \\label{eqn:eqnnumber111} \\end{align}\\] Example: \\[ x_{new} = (A - uv^T)^{-1}y_{old} = \\left( \\left[ \\begin{array}{ccc} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A} - \\left[ \\begin{array}{rrr} 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 \\\\ -2 &amp; 0 &amp; 0 \\end{array} \\right]_{uv^T} \\right)^{-1} \\left[ \\begin{array}{ccc} 6 \\\\ 5 \\\\ 6 \\\\ \\end{array} \\right]_{y_{old}} \\] We get the solution for x: \\[\\begin{align*} x_1 {}&amp;= 2,\\ \\ \\ \\ x_2 = -1, \\ \\ \\ \\ x_3 = 1 \\end{align*}\\] Gaussian Elimination with Sherman-Morrison-Woodbury (SMW): In Sherman-Morrison formula, we use \\(uv^T\\) term as rank-one update matrix representing an update (a perturbation) on a specific column given vector \\(v^T\\), \\(1 \\times n\\), with update values of vector \\(u\\), \\(n \\times 1\\). The Woodbury formula expands on Sherman-Morrison formula in that it uses an upper \\((UI)^TV^T\\) term to signify rank-k update matrix where \\(U\\) has \\(n \\times k\\) dimension, \\(V^T\\) has \\(k \\times n\\) dimension, and \\(I\\) has \\(k \\times k\\) dimension. The formula is written as (Fill J. A. and Fishkind D. E. 1998): \\[\\begin{align} (A - UV^T)^{-1} = A^{-1} + A^{-1}U(I - V^TA^{-1}U)^{-1}V^TA^{-1} \\label{eqn:eqnnumber112} \\end{align}\\] where equation reduces to a Sherman-Morrison equation if the dimension of the identity matrix, I, reduces to 1x1 (where \\(I=1\\)) and \\(U\\) reduces to a dimension of nx1 (where \\(U = u\\)) and \\(V^T\\) reduces to a dimension of 1xn (where \\(V=v\\)). \\[\\begin{align} (A - uv^T)^{-1} = A^{-1} + A^{-1}u(1 - v^TA^{-1}u)^{-1}v^TA^{-1} \\label{eqn:eqnnumber113} \\end{align}\\] So how does SWM work? Let us use the same matrix, A, but with a different perturbation. This time, the first row is multiplied by 1/3. Also, the 2nd and 3rd columns of the 2nd row are updated from 4 to 5 and 5 to 4, respectively. \\[ \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A|y_{old}} \\rightarrow \\left[ \\begin{array}{rrr} 1 &amp;1 &amp; 1 \\\\ 2 &amp; 5 &amp; 4 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\left|\\begin{array}{r}6 \\\\ 5 \\\\ 6\\end{array}\\right. \\right]_{A_{new}|y_{old}} \\] Let us construct a rank-k update matrix using \\(UV^T\\) term: \\[ UV^T = \\left[ \\begin{array}{rrr} 2 &amp; 0 &amp; 0\\\\ 0 &amp; -1 &amp; 1\\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right]_{U} \\left[ \\begin{array}{rrr} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{V^T} = \\left[ \\begin{array}{rrr} 2 &amp; 2 &amp; 2 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right]_{UV^T} \\] where K=3, N=3 so that \\(U_{nxk}\\), \\(V_{kxn=kxn}\\), and \\(I_{kxk}\\). Derive \\(z\\) from \\(z = A^{-1}u\\): \\[ z = \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A}^{-1} \\left[ \\begin{array}{rrr} 2 &amp; 0 &amp; 0 \\\\ 2 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right]_{U} = \\left[ \\begin{array}{rrr} 1/12 &amp; 0 &amp; 0\\\\ 1/12 &amp; 1 &amp; -1 \\\\ -1 &amp; -1 &amp; 1 \\end{array} \\right]_{z} \\] With Sherman-Morrison-Woodbury formula: \\[ x_{new} = \\frac{(V^Tx_{old})_n}{(I - V^Tz)_d} = \\left(I - V^Tz\\right)_d^{-1}(V^Tx_{old})_n \\] \\[\\begin{align*} \\left(I - V^Tz\\right)_d^{-1} {}&amp;= \\left( \\left[ \\begin{array}{ccc} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{I} - \\left[ \\begin{array}{rrr} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{V^T} \\left[ \\begin{array}{rrr} 1/12 &amp; 0 &amp; 0\\\\ 1/12 &amp; 1 &amp; -1 \\\\ -1 &amp; -1 &amp; 1 \\end{array} \\right]_{z} \\right)_d^{-1}\\\\ &amp;= \\left[ \\begin{array}{ccc} 1/3 &amp; 0 &amp; 0 \\\\ -1/12 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\end{array} \\right]_d^{-1} \\end{align*}\\] Finally, solving for \\(x_{new}\\): \\[ x_{new} = \\left[ \\begin{array}{ccc} 1/3 &amp; 0 &amp; 0 \\\\ -1/12 &amp; 0 &amp; 1 \\\\ 1 &amp; 1 &amp; 0 \\end{array} \\right]_{d}^{-1} \\left( \\left[ \\begin{array}{rrr} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{V^T} \\left[\\begin{array}{r} 1 \\\\ 2 \\\\ -1\\end{array}\\right]_{x_{old}} \\right)_n, \\] we get the solution for x: \\[\\begin{align*} x_1 {}&amp;= 6,\\ \\ \\ \\ x_2 = -7, \\ \\ \\ \\ x_3 = 7. \\end{align*}\\] Validate using \\((A - UV^T)^{-1}y_{old}\\): \\[ x_{new} = (A - UV^T)^{-1}y_{old} = \\left( \\left[ \\begin{array}{ccc} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A} - \\left[ \\begin{array}{rrr} 2 &amp; 2 &amp; 2 \\\\ 0 &amp; -1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right]_{UV^T} \\right)^{-1} \\left[ \\begin{array}{ccc} 6 \\\\ 5 \\\\ 6 \\\\ \\end{array} \\right]_{y_{old}}, \\] we get the solution for x: \\[\\begin{align*} x_1 {}&amp;= 6,\\ \\ \\ \\ x_2 = -7, \\ \\ \\ \\ x_3 = 7. \\end{align*}\\] LU decomposition using Doolittle Algorithm: Another algorithm to introduce is the Doolittle algorithm, which avoids using the Gaussian Elimination. Below is the notation of the algorithm, given a Matrix, \\(A_{nxn}\\), an initial sparse lower-triangular matrix, \\(L_{nxn}\\), and an initial sparse upper-triangular matrix, \\(U_{nxn}\\). Here, sparse denotes a matrix with entries equal to zero: \\[\\begin{align*} for\\ i = 1..n\\ : \\\\ \\ \\ \\ \\ U_{ij} {}&amp;= A_{ij} - \\sum_{k=1}^i L_{ik} U_{k,j},\\ for\\ j = i..n \\\\ \\\\ \\ \\ \\ \\ L_{ji} &amp;= \\frac{ \\left( A_{ji} - \\sum_{k=1}^i L_{jk} U_{k,i} \\right) }{ U_{ii} }, \\ \\ for\\ j = i..n\\ and\\ \\ L_{jj} = 1\\ \\ if\\ i = k \\end{align*}\\] We note that the Lower matrix and Upper matrix may separately contain a diagonal with all ones. In the latter part of this section, we discuss Doolittle decomposition in which L has such property. Otherwise, if U has diagonal entries containing all ones, it is recognized as Crout decomposition. We will also cover Cholesky decomposition in which L = U (Stanimirovíc P. S. et al 2011). Here is a naive implementation of the Doolittle algorithm in R code: lu_decomposition_by_doolittle &lt;- function(A) { n = ncol(A) u = matrix(rep(0, n*n), n) # Sparse Matrix for Upper Triangular l = matrix(rep(0, n*n), n) # Sparse Matrix for Lower Triangular for (i in 1:n) { # Upper Triangular Loop for (j in i:n) { m_ = 0 for (k in 1:j) { m_ = m_ + l[i,k] * u[k,j] } u[i,j] = A[i,j] - m_ } # Lower Triangular Loop for (j in i:n) { if (i == j) { l[j,j] = 1 # Diagonal Entry = 1 } else { m_ = 0 for (k in 1:i) { m_ = m_ + l[j,k] * u[k,i] } l[j,i] = ( A[j,i] - m_ ) / u[i,i] } } } list(&quot;matrix&quot; = A, &quot;lower&quot; = l, &quot;upper&quot; = u) } Note that the R code and algorithm do not include pivoting. The R code for the Doolittle algorithm can be modified further to include pivoting. This book does not discuss the pivoting portion of the algorithm. Here is solving a system by LU decomposition for \\(Ax = b\\): A = matrix(c(1,5,5,2,4,5,3,3,3), 3, byrow=TRUE) b = c(6,5,6) LU = lu_decomposition_by_doolittle(A) #derived from Doolittle section uy = forward_sub(LU$lower, b) #derived from REF/RREF section x = backward_sub(LU$upper, uy) #derived from REF/RREF section Also, one more algorithm similar to Doolittle algorithm is the Crout’s algorithm, which has diagonal entries of one for the upper triangular matrix. It may also help to be aware of and understand the algorithm. This book does not discuss the details of Crout’s algorithm. 2.20.3 LDU Factorization LDU factorization is another form of LU factorization. The difference is that LDU includes a diagonal matrix derived out of decomposing the upper-triangular matrix further so that the U matrix contains a diagonal of ones. \\[ A = \\underbrace{ \\left[ \\begin{array}{rrrr} 1 &amp; . &amp; . &amp; . \\\\ L_{b_1} &amp; 1 &amp; . &amp; . \\\\ L_{c_1} &amp; L_{c_2} &amp; 1 &amp; . \\\\ L_{d_1} &amp; L_{d_2} &amp; L_{d_3} &amp; 1 \\end{array} \\right]}_{L_A} \\underbrace{ \\left[ \\begin{array}{rrrr} D_{a1}^u &amp; . &amp; . &amp; . \\\\ . &amp; D_{b2}^u &amp; . &amp; . \\\\ . &amp; . &amp; D_{c3}^u &amp; . \\\\ . &amp; . &amp; . &amp; D_{d4}^u \\end{array} \\right]}_{D_U} \\underbrace{ \\left[ \\begin{array}{cccc} 1 &amp; U_{a_2} &amp; U_{a_3} &amp; U_{a_4} \\\\ . &amp; 1 &amp; U_{b_3} &amp; U_{b_4} \\\\ . &amp; . &amp; 1 &amp; U_{c_4} \\\\ . &amp; . &amp; . &amp; 1 \\end{array} \\right]}_{U_A} \\] For example, using the following \\(A = LU\\): \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{U_A} \\] we decompose \\(U_A\\) matrix by extracting its diagonal into a separate diagonal matrix and then replacing the diagonal of \\(U_A\\) matrix with ones: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{U_A} \\rightarrow \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -6 &amp; 0 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{D_U} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; 1 &amp; 5/6 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{U_{new}} \\] That is achieved by performing the following: Construct a diagonal matrix, \\(D_U\\), by extracting the diagonal entries of \\(U_A\\). For R1, no further operation is needed because the first diagonal entry is already a one. For R2, multiply R2 by -1/6; in notation, ( -1/6 * R2 \\(\\rightarrow\\) R2 ). For R3, multiply R3 by -1/2; in notation, ( -1/2 * R3 \\(\\rightarrow\\) R3 ). So for the final result: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A} \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -6 &amp; 0 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{D_U} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; 1 &amp; 5/6 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{U_{new}} \\] Note that LU/LDU decomposition by Gauss-Jordan Elimination works on invertible square matrices. Let us now take a look at another decomposition. 2.20.4 QR Factorization (Gram-Schmidt, Householder, and Givens) The idea is to decompose a full-rank matrix into its QR form; where Q is an orthogonal matrix and R is an upper-triangular matrix, forming the equation: \\[\\begin{align} A = QR \\label{eqn:eqnnumber114} \\end{align}\\] We transform the following sample matrix equation: \\[ \\left[ \\begin{array}{cccc} a_1 &amp; a_2 &amp; a_3 &amp; a_4 \\\\ b_1 &amp; b_2 &amp; b_3 &amp; b_4 \\\\ c_1 &amp; c_2 &amp; c_3 &amp; c_4 \\\\ d_1 &amp; d_2 &amp; d_3 &amp; d_4 \\end{array} \\right]_{A} \\left[\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right]_{x} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{array}\\right]_{y} \\] into QR form: \\[ \\left[ \\begin{array}{rrrr} Q_{a_1} &amp; Q_{a_2} &amp; Q_{a_3} &amp; Q_{a_4} \\\\ Q_{b_1} &amp; Q_{b_2} &amp; Q_{b_3} &amp; Q_{b_4} \\\\ Q_{c_1} &amp; Q_{c_2} &amp; Q_{c_3} &amp; Q_{c_4} \\\\ Q_{d_1} &amp; Q_{d_2} &amp; Q_{d_3} &amp; Q_{d_4} \\end{array} \\right]_{Q_A} \\left[ \\begin{array}{cccc} R_{a_1} &amp; R_{a_2} &amp; R_{a_3} &amp; R_{a_4} \\\\ . &amp; R_{b_2} &amp; R_{b_3} &amp; R_{b_4} \\\\ . &amp; . &amp; R_{c_3} &amp; R_{c_4} \\\\ . &amp; . &amp; . &amp; R_{d_4} \\end{array} \\right]_{R_A} \\left[\\begin{array}{c} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right]_{x} = \\left[\\begin{array}{c} y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\end{array}\\right]_{y} \\] For a quick insight of an orthogonal matrix, \\(Q\\), let us use Figure 2.24 for the orthogonal projection image - left side. We cover orthogonal projection first then orthogonal reflection next. Figure 2.24: Orthogonal Projection and Reflection The matrix representations of vectors a, b, c with p in the figure is as follows: \\[ \\left[\\begin{array}{cc}0 &amp; 5\\\\ 3 &amp; 5 \\end{array}\\right]_{ap}\\ \\ \\ \\ \\ \\ \\ \\left[\\begin{array}{cc}1 &amp; 5\\\\ 4 &amp; 5 \\end{array}\\right]_{bp}\\ \\ \\ \\ \\ \\ \\ \\left[\\begin{array}{cc}2 &amp; 5\\\\ 5 &amp; 5 \\end{array}\\right]_{cp} \\] The following projections apply for a, b, and c on to p: \\[\\begin{align*} a&#39; = proj_{p}a = \\left(\\frac{&lt;5,5&gt;^T&lt;0,3&gt;}{\\|&lt;5,5&gt;\\|^2}\\right) &lt;5,5&gt; = \\frac{15}{50} &lt;5,5&gt; = &lt;1.5,1.5&gt; \\\\ b&#39; = proj_{p}b = \\left(\\frac{&lt;5,5&gt;^T&lt;1,4&gt;}{\\|&lt;5,5&gt;\\|^2}\\right) &lt;5,5&gt; = \\frac{25}{50} &lt;5,5&gt; = &lt;2.5,2.5&gt; \\\\ c&#39; = proj_{p}c = \\left(\\frac{&lt;5,5&gt;^T&lt;2,5&gt;}{\\|&lt;5,5&gt;\\|^2}\\right) &lt;5,5&gt; = \\frac{35}{50} &lt;5,5&gt; = &lt;3.5,3.5&gt; \\\\ \\end{align*}\\] The following orthogonal projections apply for a, b, and c on to p: \\[\\begin{align*} o&#39;a = a - a&#39; = &lt;0,3&gt; - &lt;1.5, 1.5&gt; = &lt;-1.5, 1.5&gt;\\\\ o&#39;b = b - b&#39; = &lt;1,4&gt; - &lt;2.5, 2.5&gt; = &lt;-1.5, 1.5&gt;\\\\ o&#39;c = c - c&#39; = &lt;2,5&gt; - &lt;3.5, 3.5&gt; = &lt;-1.5, 1.5&gt; \\end{align*}\\] From here, we can form a Quasi-Orthogonal matrices for each of o’a, o’b, and o’c with respect to p: \\[ \\left[\\begin{array}{cc}-1.5 &amp; 5\\\\ 1.5 &amp; 5 \\end{array}\\right]_{o&#39;a}\\ \\ \\ \\ \\ \\ \\ \\left[\\begin{array}{cc}-1.5 &amp; 5\\\\ 1.5 &amp; 5 \\end{array}\\right]_{o&#39;b}\\ \\ \\ \\ \\ \\ \\ \\left[\\begin{array}{cc}-1.5 &amp; 5\\\\ 1.5 &amp; 5 \\end{array}\\right]_{o&#39;c} \\] Note that the matrix is only a Quasi-Orthogonal matrix. A matrix is considered orthogonal if it meets at least the following: \\[\\begin{align} Q^{-1} = Q^T,\\ \\ \\ Q^TQ = QQ^T = I \\label{eqn:eqnnumber115} \\end{align}\\] To do that, let us normalize each column of the matrix (for now, let us use \\(o&#39;a\\) to illustrate, though this applies to both \\(o&#39;b\\) and \\(o&#39;c\\) just the same): \\[\\begin{align*} q&#39;a {}&amp;= \\frac{o&#39;a}{ \\| o&#39;a \\|_{L2}} = \\frac{&lt;-1.5, 1.5&gt;}{ \\|&lt;-1.5, 1.5&gt;\\|} = &lt; -0.7071068, 0.7071068&gt; \\\\ q&#39;p &amp;= \\frac{p }{ \\| p \\|_{L2}} = \\frac{&lt;5, 5&gt;}{\\|&lt;5, 5&gt;\\|} = &lt; 0.7071068, 0.7071068&gt; \\end{align*}\\] This forms the \\(Q&#39;A\\) matrix - an orthogonal matrix: \\[ Q_A = \\left[\\begin{array}{rrr} -0.7071068 &amp; 0.7071068 \\\\ 0.7071068 &amp; 0.7071068 \\end{array}\\right] = \\left[\\begin{array}{r} -0.7071068 \\\\ 0.7071068 \\end{array}\\right]_{q&#39;a} \\left[\\begin{array}{r} 0.7071068 \\\\ 0.7071068 \\end{array}\\right]_{q&#39;p.} \\] To validate: \\[ Q_A^{-1} = Q_A^T,\\ \\ \\ \\ Q_A^TQ_A = Q_AQ_A^T = I. \\] Additionally, if a matrix is a full-rank matrix, e.g., all columns are non-singular linearly independent, then an orthogonal matrix can be formed. In the Figure 2.24, we sampled vectors a, b, c,, and p to form three separate orthogonal matrices. However, combining the vectors in one matrix will not form a full-rank matrix because each column is not linearly independent of the others. Below is a matrix we have used in previous QR decomposition algorithms. This matrix is full-rank where all columns are linearly independent: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A \\] In this case, we can first get the orthogonal projections of each column: \\[\\begin{align*} q_1&#39; {}&amp;= a_1 \\\\ q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\\\ q_3&#39; &amp;= a_3 - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3 \\end{align*}\\] Equivalently, to be more in general terms, here is a list of the orthogonal basis of the projections: \\[\\begin{align*} q_1&#39; &amp;= a_1 \\\\ q_2&#39; &amp;= a_2 - proj_{q_1&#39;}a_2 \\\\ q_3&#39; &amp;= a_k - proj_{q_1&#39;}a_3 - proj_{q_2&#39;}a_3 \\\\ \\vdots \\\\ q_k&#39; &amp;= a_k - proj_{q_1&#39;}a_k - proj_{q_2&#39;}a_k -\\ ...\\ -\\ proj_{q_{k-1}&#39;}a_k \\\\ \\end{align*}\\] Note that the projections are subtracted from the projected vector to get the equivalent orthogonal vector (projection) perpendicular to all other orthogonal projections. Finally, normalize each of the orthogonal projections to form the orthogonal matrix columns: \\[ q_1 = \\frac{q_1&#39;} {\\|q_1&#39;\\|}\\ \\ \\ \\ \\ q_2 = \\frac{q_2&#39;} {\\|q_2&#39;\\|}\\ \\ \\ \\ \\ \\ q_3 = \\frac{q_3&#39;} {\\|q_3&#39;\\|} \\ \\ \\ ... \\ \\ \\ q_k = \\frac{q_k&#39;} {\\|q_k&#39;\\|} \\] with that, we obtain \\(Q_A\\) and \\(R_A\\): \\[ Q_A = \\left[ \\begin{array}{r} q_{1a} \\\\q_{1b} \\\\ q_{1c} \\\\ \\vdots \\\\ q_{1k} \\\\ \\end{array} \\left|\\begin{array}{r}q_{2a} \\\\ q_{2b} \\\\ q_{2c} \\\\ \\vdots \\\\ q_{2k} \\end{array}\\right. \\left|\\begin{array}{r}q_{3a} \\\\ q_{3b} \\\\ q_{3c} \\\\ \\vdots \\\\ q_{3k} \\end{array}\\right. \\left|\\begin{array}{r} ... \\\\ ...\\\\ ...\\\\ \\ddots \\\\ ... \\end{array}\\right. \\left|\\begin{array}{r}q_{ka} \\\\ q_{kb} \\\\ q_{kc} \\\\ \\vdots \\\\ q_{kk} \\end{array}\\right. \\right] \\ \\ \\ \\ \\ \\ \\ R_A \\leftarrow \\begin{cases} r_{kj} = q_{k}^Ta_j\\ \\ \\ if\\ k\\neq j \\\\ r_{kj} = \\|q_j&#39;\\|_2\\ \\ \\ if\\ k=j\\ \\ \\ \\{classic\\} \\end{cases} \\] We now illustrate QR factorization using three decomposition methods: Gram-Schmidt (GS) algorithm: There are two versions of Gram-Schmidt algorithm, namely classic and modified (Gander W. 1980): \\[ \\begin{array}{l} Classic\\ (CGS) \\\\ ------- \\\\ \\text{loop j = 1 : n} \\\\ \\ \\ \\ \\ q_j&#39; = a_j \\\\ \\ \\ \\ \\ \\text{loop k = 1 : j } \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ r_{kj} = q_k^Tq_j&#39; \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ q_j&#39; = q_j&#39; - r_{kj}q_k \\\\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\ \\ r_{jj} = \\| q_j&#39; \\|_{L2} \\\\ \\ \\ \\ \\ \\ q_j = q_j&#39;\\ /\\ r_{jj} \\\\ \\text{end loop} \\end{array} \\left| \\begin{array}{l} Modified\\ (MGS) \\\\ -------- \\\\ \\text{loop j = 1 : n}\\\\ \\ \\ \\ \\ r_{jj} = \\| a_j\\|_{L2}\\\\ \\ \\ \\ \\ q_j = a_j\\ /\\ \\| r_{jj} \\\\ \\ \\ \\ \\ \\text{loop k = j : n}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ r_{jk} = q_j^Ta_k \\\\ \\ \\ \\ \\ \\ \\ \\ \\ a_k = a_k - r_{jk}q_j \\\\ \\ \\ \\ \\ \\text{end loop}\\\\ \\text{end loop}\\\\ \\ \\end{array} \\right. \\] Let us step through the classic algorithm. Consider our previous sample matrix: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left[\\begin{array}{r} 1 \\\\2 \\\\ 3 \\end{array}\\right]_{a1} \\left[\\begin{array}{r} 5 \\\\4 \\\\ 3 \\end{array}\\right]_{a2} \\left[\\begin{array}{r} 5 \\\\3 \\\\ 3 \\end{array}\\right]_{a3} \\] First, let us handle \\(q_1\\) and \\(R_{a1}\\): \\[\\begin{align*} q_1&#39; {}&amp;= a_{j=1} = &lt;1,2,3&gt;\\ \\ \\ \\leftarrow\\ \\ \\ where\\ j = 1 \\\\ R_{a1} &amp;= ||q_1&#39;||_{L2} = \\sqrt{1^2 + 2^2 + 3^3} = \\sqrt{14} \\\\ q_1 &amp;= \\frac{1}{R_{a1}}(q_1&#39;) = \\frac{1} {\\sqrt{14}}(&lt;1,2,3&gt;) \\\\ &amp;= \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right) \\end{align*}\\] Second, let us now take care of \\(q_2\\), \\(R_{a2}\\), \\(R_{b2}\\): \\[\\begin{align*} q_2&#39; {}&amp;= a_{j=2} = &lt;5,4,3&gt; \\ \\ \\ \\leftarrow\\ \\ \\ where\\ j = 2 \\\\ R_{a2} &amp;= q_1^Tq_2&#39; = \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right)^T &lt;5,4,3&gt; = \\sqrt{14}+ 4 \\sqrt{\\frac{2}{7}} \\\\ \\\\ q_2&#39; &amp;= q_2&#39; - R_{a2}q_1 \\leftarrow (I - q_1q_1^T)q_2&#39; \\\\ &amp;= &lt;5,4,3&gt; - \\left( \\sqrt{14}+ 4 \\sqrt{\\frac{2}{7}}\\right) \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right) \\\\ &amp;= \\left(\\frac{24}{7}, \\frac{6}{7},\\frac{-12}{7}\\right) \\end{align*}\\] \\[\\begin{align*} R_{b2} &amp;= \\|q_2&#39;\\|_{L2} = \\sqrt{\\left(\\frac{24}{7}\\right)^2 + \\left(\\frac{6}{7}\\right)^2 + \\left(\\frac{-12}{7}\\right)^2} = \\frac{6\\sqrt{21}}{7} \\\\ q_2 &amp;= \\frac{1}{R_{b2}}(q_2&#39;) = \\frac{1}{\\frac{6\\sqrt{21}}{7}} \\left(\\frac{24}{7}, \\frac{6}{7},\\frac{-12}{7}\\right) = \\frac{7}{6\\sqrt{21}}{\\left(\\frac{24}{7}, \\frac{6}{7},\\frac{-12}{7}\\right)} \\\\ &amp;= \\left(\\frac{4}{\\sqrt{21}}, \\frac{1}{\\sqrt{21}},\\frac{-2}{\\sqrt{21}}\\right) \\end{align*}\\] Lastly, let us finally take care of \\(q_3\\), \\(R_{a3}\\), \\(R_{b3}\\), and \\(R_{c3}\\): \\[\\begin{align*} q_3&#39; {}&amp;= a_{j=3} = &lt;5,5,3&gt; \\ \\ \\ \\leftarrow\\ \\ \\ where\\ j = 3 \\\\ R_{a3} &amp;= q_1^Tq_3&#39; = \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right)^T &lt;5,5,3&gt; = \\sqrt{14}+ 5 \\sqrt{\\frac{2}{7}} \\\\ \\\\ R_{b3} &amp;= q_2^Tq_3&#39; = \\left(\\frac{4}{\\sqrt{21}}, \\frac{1}{\\sqrt{21}},\\frac{-2}{\\sqrt{21}}\\right)^T &lt;5,5,3&gt; = \\frac{19}{\\sqrt{21}} \\\\ \\\\ q_3&#39; &amp;= q_3&#39; - R_{a3}q_1 - R_{b3}q_2 \\leftarrow (I - q_1q_1^T)(I - q_2q_2^T)q_3&#39; \\\\ &amp;= &lt;5,5,3&gt; - \\\\ &amp;\\ \\ \\left( \\sqrt{14}+ 5 \\sqrt{\\frac{2}{7}}\\right) \\left(\\frac{1}{\\sqrt{14}}, \\frac{2}{\\sqrt{14}},\\frac{3}{\\sqrt{14}}\\right) - \\left(\\frac{19}{\\sqrt{21}}\\right) \\left(\\frac{4}{\\sqrt{21}}, \\frac{1}{\\sqrt{21}},\\frac{-2}{\\sqrt{21}}\\right) \\\\ &amp;= \\left(\\frac{-1}{3},\\frac{2}{3},\\frac{-1}{3}\\right) \\end{align*}\\] \\[\\begin{align*} R_{c3} &amp;= \\|q_3&#39;\\| = \\|\\left(\\frac{-1}{3},\\frac{2}{3},\\frac{-1}{3}\\right)\\| = \\sqrt{\\left(\\frac{-1}{3}\\right)^2,\\left(\\frac{2}{3}\\right)^2,\\left(\\frac{-1}{3}\\right)^2} = \\sqrt{\\frac{2}{3}}\\\\ q_3 &amp;= \\frac{1}{R_{c3}}(q_3&#39;) = \\left(\\frac{1}{\\sqrt{\\frac{2}{3}}}\\right) \\left(\\frac{-1}{3},\\frac{2}{3},\\frac{-1}{3}\\right) \\\\ &amp;= \\left(\\frac{-1}{\\sqrt{6}}, \\sqrt{\\frac{2}{3}}, \\frac{-1}{\\sqrt{6}}\\right) \\end{align*}\\] The steps above leads to the following QR form: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left(\\begin{array}{rrr} \\frac{1}{\\sqrt{14}} &amp; \\frac{4}{\\sqrt{21}} &amp; \\frac{-1}{\\sqrt{6}} \\\\ \\frac{2}{\\sqrt{14}} &amp; \\frac{1}{\\sqrt{21}} &amp; \\sqrt{\\frac{2}{3}} \\\\ \\frac{3}{\\sqrt{14}} &amp; \\frac{-2}{\\sqrt{21}} &amp; \\frac{-1}{\\sqrt{6}} \\\\ \\end{array} \\right)_Q \\left(\\begin{array}{rrr} \\sqrt{14} &amp; \\sqrt{14}+ 4 \\sqrt{\\frac{2}{7}} &amp; \\sqrt{14}+ 5 \\sqrt{\\frac{2}{7}} \\\\ . &amp; \\frac{6\\sqrt{21}}{7} &amp; \\frac{19}{\\sqrt{21}} \\\\ . &amp; . &amp; \\sqrt{\\frac{2}{3}} \\end{array} \\right)_R \\] Solving for R after computing for Q, we can use the following formula (note that \\(Q^{-1} = Q^T\\)): \\[\\begin{align} R = Q^{-1}A = Q^TA \\label{eqn:eqnnumber116} \\end{align}\\] Below are the Classic &amp; Modified Gram-Schmidt algorithms with a naive implementation in R code: qr_decomposition_by_cgs &lt;- function(A) { n = ncol(A) m = nrow(A) q = matrix(rep(0, m*n), m, byrow=TRUE ) # Sparse Matrix for Q r = matrix(rep(0, n*n), n, byrow=TRUE) # Sparse Matrix for R for (j in 1:n) { qj_ = A[,j] for (k in 1:j) { r[k,j] = t(q[,k]) %*% qj_ qj_ = qj_ - r[k,j] * q[,k] } r[j,j] = sqrt(sum(qj_^2)) if (r[j,j]==0) break # on linear dependency q[,j] = qj_ / r[j,j] } list(&quot;A&quot; = q %*% r, &quot;Q&quot; = q, &quot;R&quot; = r) } qr_decomposition_by_mgs &lt;- function(A) { n = ncol(A) m = nrow(A) q = matrix(rep(0, m*n), m, byrow=TRUE ) # Sparse Matrix for Q r = matrix(rep(0, n*n), n, byrow=TRUE) # Sparse Matrix for R for (j in 1:n) { r[j,j] = sqrt(sum(A[,j]^2)) if (r[j,j]==0) break # on linear dependency q[,j] = A[,j] / r[j,j] for (k in j:n) { r[j,k] = t(q[,j]) %*% A[,k] A[,k] = A[,k] - r[j,k] * q[,j] } } list(&quot;A&quot; = q %*% r, &quot;Q&quot; = q, &quot;R&quot; = r) } A = matrix(c(1,5,5, 2,4,5, 3,3,3), 3, byrow=TRUE) qr_decomposition_by_mgs(A) There are other variations of the Modified Gram-Schmidt algorithm; however, the main giveaway for MGS is that it handles round-off errors seen in CGS. Now, let us cover the next QR decomposition algorithm - the Householder algorithm. Householder (Reflection) algorithm: The idea is to decompose a matrix, \\(A\\), into the following form: \\[\\begin{align} A = Q_A \\left( \\begin{array}{c} R_A \\\\ O \\end{array} \\right) \\label{eqn:eqnnumber117} \\end{align}\\] where, similar to L in LU factorization by Gaussian Elimination (GE), Q is also a dot product accumulation of iterations in Householder algorithm. Recall again \\(L_A\\) when performing LU factorization using GE: \\[ L_A = (l_m \\cdot l_{m-1} \\cdot \\ ...\\ \\cdot l_2 \\cdot l_1)^{-1} = l_1^{-1} \\cdot l_2^{-1} \\cdot\\ ...\\ \\cdot\\ l_{m-1}^{-1} \\cdot l_{m}^{-1} \\] Similarly, in the Householder algorithm, we generate a set of \\(H_i\\) (Hessenberg matrices) to compose \\(Q_A\\) eventually. \\[ Q_A = ( H_m \\cdotp H_{m-1}\\ \\cdotp\\ . . .\\ \\cdotp\\ H_2 \\cdotp H_1)^{-1} = H_1 \\cdotp H_2\\ \\cdotp\\ . . .\\ \\cdotp\\ H_{m-1} \\cdotp\\ H_m \\] To arrive at a set of \\(H_i\\), we use the following reflection formula (Driscoll T. A. 2020): \\[\\begin{align} H_i = I - \\frac{2v_iv_i^T}{v_i^Tv_i} \\label{eqn:eqnnumber118} \\end{align}\\] where \\(v_i\\) are reflectors of \\(a_i\\): \\[ v_i = a_i - sign(a_{ii})|\\|a_i\\|_{L2}e_i \\] We also can see an image of orthogonal reflection in Figure 2.24 - right side. Let us use the same sample matrix, \\(A\\), as before and step through the Householder algorithm: \\[ A^{(1)} = A = \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_{3x3} = \\left[\\begin{array}{r} 1 \\\\2 \\\\ 3 \\end{array}\\right]_{a1} \\left[\\begin{array}{r} 5 \\\\4 \\\\ 3 \\end{array}\\right]_{a2} \\left[\\begin{array}{r} 5 \\\\3 \\\\ 3 \\end{array}\\right]_{a3}, \\ \\ \\ \\ where\\ n=3 \\] First, solve for \\(H_1\\) by computing for \\(RF_1\\) - reflection formula: \\[\\begin{align*} a_1 {}&amp; = \\|A_{1:3,1}^{(1)}\\|_{L2} = \\sqrt{14} \\\\ \\\\ v_1 &amp;= a_1 - sign(A_{11}^{(1)})(a_1)(e_1) \\\\ &amp;= \\left[\\begin{array}{r} 1 \\\\2 \\\\ 3 \\end{array}\\right]_{1:3,1} - (\\sqrt{14}) \\left[\\begin{array}{r} 1 \\\\ 0 \\\\ 0 \\end{array}\\right]_{e1} = \\left[\\begin{array}{r} -2.741657 \\\\ 2 \\\\ 3 \\end{array}\\right]_{v_1} \\\\ \\end{align*}\\] \\[\\begin{align*} RF_1 &amp;= I - 2\\left(\\frac{v_1v_1^T}{v_1^Tv_1}\\right) \\\\ &amp;= \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_I - 2 \\left(\\frac{ &lt; -2.741657 , 2 , 3 &gt;&lt; -2.741657 , 2 , 3 &gt;^T }{ &lt; -2.741657 , 2 , 3 &gt;^T&lt; -2.741657 , 2 , 3 &gt; }\\right) \\\\ \\\\ &amp;= \\left[ \\begin{array}{rrr} 0.2672614 &amp; 0.5345225 &amp; 0.8017837 \\\\ 0.5345225 &amp; 0.6100734 &amp; -0.5848899 \\\\ 0.8017837 &amp; -0.5848899 &amp; 0.1226652 \\end{array} \\right]_{RF_1} \\\\ \\\\ H_1 &amp;= RF_1 \\\\ A^{(2)} &amp;= H_1A^{(1)} = \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.414270 \\\\ 0 &amp; 3.358236 &amp; 3.968310 \\\\ 0 &amp; 2.037355 &amp; 1.452465 \\end{array} \\right]_{A_2} \\end{align*}\\] Second, solve for \\(H_2\\) by computing for \\(RF_2\\): \\[\\begin{align*} a_2 {}&amp; = \\|A_{2:3,2}^{(2)}\\|_{L2} = 3.927921 \\\\ \\\\ v_2 &amp;= a_2 + sign(A_{22}^{(2)})(a_2)(e_1) \\\\ &amp;= \\left[\\begin{array}{r} 3.358236 \\\\ 2.037355 \\end{array}\\right]_{2:3,2} - (3.927921) \\left[\\begin{array}{r} 1 \\\\ 0 \\end{array}\\right]_{e1} = \\left[\\begin{array}{r} -0.569685 \\\\ 2.037355 \\end{array}\\right]_{v_2} \\end{align*}\\] \\[\\begin{align*} RF_2 &amp;= I - 2\\left(\\frac{v_2v_2^T}{v_2^Tv_2}\\right) \\\\ &amp;= \\left[ \\begin{array}{rrr} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ \\end{array} \\right]_I - 2 \\left(\\frac{ &lt; -0.569685 , 2.037355 &gt;&lt; -0.569685 , 2.037355 &gt;^T }{ &lt; -0.569685 , 2.037355 &gt;^T&lt; -0.569685 , 2.037355 &gt; }\\right) \\\\ \\\\ &amp;= \\left[ \\begin{array}{rrr} 0.8549653 &amp; 0.5186852 \\\\ 0.5186852 &amp; -0.8549653 \\end{array} \\right]_{RF_2} \\\\ \\\\ H_2 &amp;= \\left[ \\begin{array}{rrr} 1 &amp; . \\\\ . &amp; RF_2 \\\\ \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.8549653 &amp; 0.5186852 \\\\ 0 &amp; 0.5186852 &amp; -0.8549653 \\end{array} \\right]_{H_2} \\\\ \\\\ A^{(3)} &amp;= H_2A^{(2)} = \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.4142703 \\\\ 0 &amp; 3.927921 &amp; 4.1461394 \\\\ 0 &amp; 0 &amp; 0.8164965 \\end{array} \\right]_{A^{(3)}} \\end{align*}\\] Lastly, obtain \\(Q\\) and \\(R\\): \\[\\begin{align*} R {}&amp;= A^{(3)} = H_2H_1A^{(0)},\\ \\ \\ \\ \\ Q = H_1H_2 \\\\ \\\\ R &amp;= \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.4142703 \\\\ 0 &amp; 3.927921 &amp; 4.1461394 \\\\ 0 &amp; 0 &amp; 0.8164965 \\end{array} \\right]_{R} \\ \\ \\ \\ \\ \\ \\\\ \\\\ Q &amp;= \\left[ \\begin{array}{rrr} 0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\\\ 0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\\\ 0.8017837 &amp; -0.4364360 &amp; -0.4082482 \\end{array} \\right]_{Q} \\end{align*}\\] Given a matrix, \\(A_{mxn}\\), let us formulate the algorithm: \\[\\begin{align*} \\begin{array}{l} Q = diag(max(m,n)) \\\\ R = A \\\\ loop\\ j = 1:n \\\\ \\ \\ \\ \\ blk = m - j + 1 \\\\ \\ \\ \\ \\ if\\ (blk &lt;= 1)\\ break \\\\ \\ \\ \\ \\ e1 = zeros(blk); e1[1] = 1 \\\\ \\ \\ \\ \\ a = \\|R_{j:m,j}\\|_{L2} \\\\ \\ \\ \\ \\ v = R_{j:m,j} - sign(R_{jj}) \\times a \\times e1 \\\\ \\ \\ \\ \\ I = diag(blk) \\\\ \\ \\ \\ \\ H&#39; = I - constant(2 / ( v^T \\cdotp v )) \\times ( v \\cdotp v^T ) \\\\ \\ \\ \\ \\ H = diag(max(m,n)) \\\\ \\ \\ \\ \\ H_{jm,jm} = H&#39; \\\\ \\ \\ \\ \\ R = HR \\\\ \\ \\ \\ \\ Q = QH \\\\ end\\ loop\\\\ \\end{array} \\end{align*}\\] Here is a naive implementation of the Householder algorithm in R code: qr_decomposition_by_householder &lt;- function(A) { n = ncol(A) m = nrow(A) Q = diag(m) for (j in 1:n) { blk = m - j + 1 if (blk &lt;= 1) break e1 = rep(0, blk); e1[1] = 1 a = sqrt(sum(A[j:m,j]^2)) v = A[j:m,j] - sign(A[j,j]) * a * e1 I = diag(blk) # c() converts from 1x1 2D to 1D (constant) H_ = I - c(2/(t(v) %*% (v))) * (v %*% t(v)) H = diag(m) H[j:m,j:m] = H_ A = H %*% A Q = Q %*% H } list(&quot;Matrix&quot;= Q %*% A, &quot;Q&quot; = Q, &quot;R&quot; = A) } A = matrix(c(1,5,5, 2,4,5, 3,3,3), 3, byrow=TRUE) qr_decomposition_by_householder(A) Given’s rotation algorithm: The idea is to decompose a matrix, A, into the following form: \\[\\begin{align} A = QR \\label{eqn:eqnnumber119} \\end{align}\\] For the algorithm to work, we need a transformation matrix that rotates a given 2x1 vector by the angle, \\(\\theta\\), effectively reducing the second item to zero. \\[ \\left[ \\begin{array}{rr} cos\\ \\theta &amp; -sin\\ \\theta \\\\ sin\\ \\theta &amp; cos\\ \\theta \\\\ \\end{array} \\right]_{rotate}^T \\left[ \\begin{array}{rr} v_1 \\\\ v_2 \\end{array} \\right]_V = \\left[ \\begin{array}{rr} r \\\\ 0 \\end{array} \\right]_{transformed} \\ \\ \\ \\ where\\ r = \\sqrt{v_1^2 + v_2^2} \\] We use the below formula to compute for \\(cos\\ \\theta\\) and \\(\\sin \\theta\\): \\[ cos\\ \\theta = \\frac{v_1}{\\sqrt{v_1^2 + v_2^2}}\\ \\ \\ \\ \\ \\ \\ sin\\ \\theta = \\frac{v_2}{\\sqrt{v_1^2 + v_2^2}} \\] To illustrate, let us use our system of equations to solve for \\(Q\\) and \\(R\\): \\[ \\left(\\begin{array}{lll} 1x_1 + 5x_2 + 5x_3 = 6 \\\\ 2x_1 + 4x_2 + 5x_3 = 5 \\\\ 3x_1 + 3x_2 + 3x_3 = 6 \\\\ \\end{array}\\right) \\rightarrow \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\\\ \\end{array} \\right] \\left[ \\begin{array}{rrr} x_1 \\\\ x_2 \\\\ x_3 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right] \\] Our approach would be bottom-up, left-right, extracting each vector at a time and annihilating its lower element, effectively rotating the affected rows, until we form an upper-triangular matrix: \\[\\begin{align*} \\left[ \\begin{array}{lll} x_{11} &amp; x_{12} &amp; x_{13} \\\\ x_{21} &amp; x_{22} &amp; x_{23} \\\\ x_{31} &amp; x_{32} &amp; x_{33} \\\\ \\end{array} \\right]_A {}&amp;\\rightarrow \\\\ \\\\ \\left[ \\begin{array}{lll} x_{11} &amp; x_{12} &amp; x_{13} \\\\ \\mathbf{x_{21}} &amp; \\mathbf{x_{22}} &amp; \\mathbf{x_{23}} \\\\ 0 &amp; \\mathbf{x_{32}} &amp; \\mathbf{x_{33}} \\\\ \\end{array} \\right]_{G_1} \\rightarrow \\left[ \\begin{array}{lll} \\mathbf{x_{11}} &amp; \\mathbf{x_{12}} &amp; \\mathbf{x_{13}} \\\\ 0 &amp; \\mathbf{x_{22}} &amp; \\mathbf{x_{23}} \\\\ 0 &amp; x_{32} &amp; x_{33} \\\\ \\end{array} \\right]_{G_2} &amp;\\rightarrow \\left[ \\begin{array}{lll} x_{11} &amp; x_{12} &amp; x_{13} \\\\ 0 &amp; \\mathbf{x_{22}} &amp; \\mathbf{x_{23}} \\\\ 0 &amp; 0 &amp; \\mathbf{x_{33}}\\\\ \\end{array} \\right]_{G3} \\end{align*}\\] First, we extract the vector, \\(\\left[ \\begin{array}{rrr} x_{21} \\\\ x_{31} \\end{array} \\right] = \\left[ \\begin{array}{rrr} 2 \\\\ 3 \\end{array} \\right]\\), to compute for \\(cos\\ \\theta\\) and \\(sin\\ \\theta\\) and zero out \\(x_{31}\\): \\[ cos\\ \\theta = \\frac{2}{\\sqrt{2^2 + 3^2}} = 0.5547002\\ \\ \\ \\ \\ \\ \\ sin\\ \\theta = \\frac{3}{\\sqrt{2^2 + 3^2}} = 0.8320503 \\] We then improvise a transformation matrix (a given’s matrix), \\(G_1\\) to transform matrix, A. \\[\\begin{align*} G_{1} {}&amp;= \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; cos\\ \\theta &amp; -sin\\ \\theta \\\\ 0 &amp; sin\\ \\theta &amp; cos\\ \\theta \\\\ \\end{array} \\right]_{utility}^T \\rightarrow \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0.5547002 &amp; -0.8320503 \\\\ 0 &amp; 0.8320503 &amp; 0.5547002 \\\\ \\end{array} \\right]_{rotate}^T \\\\ \\\\ A_{1} &amp;= (G_{1})^T\\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\\\ \\end{array} \\right]_A = \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_y \\end{align*}\\] We get our first transformation (annihilate lower element by rotation): \\[ A_{1} = \\left[ \\begin{array}{rrr} 1.000000 &amp; 5.000000 &amp; 5.000000 \\\\ 3.605551 &amp; 4.714952 &amp; 5.269652 \\\\ 0.000000 &amp; -1.664101 &amp; -2.496151 \\\\ \\end{array} \\right]_{A_1} = \\left[ \\begin{array}{rrr} 6.0000000 \\\\ 7.7658028 \\\\ -0.8320503 \\end{array} \\right]_{y_1} \\] Second, we extract the vector, \\(\\left[ \\begin{array}{rrr} x_{11} \\\\ x_{21} \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1.000000 \\\\ 3.605551 \\end{array} \\right]\\), to compute for \\(cos\\ \\theta\\) and \\(sin\\ \\theta\\) and zero out \\(x_{21}\\): \\[ cos\\ \\theta = \\frac{1}{\\sqrt{1^2 + 3.605551^2}} = 0.2672613\\ \\ \\ \\ \\ \\ \\ sin\\ \\theta = \\frac{3.605551}{\\sqrt{1^2 + 3.605551^2}} = 0.9636241 \\] We then improvise another transformation matrix, \\(G_2\\), for the transformed matrix, \\(A_1\\). \\[\\begin{align*} G_{2} {}&amp;= \\left[ \\begin{array}{rrr} cos\\ \\theta &amp; -sin\\ \\theta &amp; 0 \\\\ sin\\ \\theta &amp; cos\\ \\theta &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array} \\right]_{utility} \\rightarrow \\left[ \\begin{array}{rrr} 0.2672613 &amp; -0.9636241 &amp; 0\\\\ 0.9636241 &amp; 0.2672613 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\\\ \\end{array} \\right]_{rotate} \\\\ \\\\ A_{2} &amp;= (G_{2})^T \\left[ \\begin{array}{rrr} 1.000000 &amp; 5.000000 &amp; 5.000000 \\\\ 3.605551 &amp; 4.714952 &amp; 5.269652 \\\\ 0.000000 &amp; -1.664101 &amp; -2.496151 \\\\ \\end{array} \\right]_{A_1} = \\left[ \\begin{array}{rrr} 6.0000000 \\\\ 7.7658028 \\\\ -0.8320503 \\end{array} \\right]_{y_1} \\end{align*}\\] We get our second transformation (annihilate lower element by rotation): \\[ A_{2} = \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.414270 \\\\ 0.000000 &amp; -3.557996 &amp; -3.409746 \\\\ 0.000000 &amp; -1.664101 &amp; -2.496151 \\\\ \\end{array} \\right]_{A_2} = \\left[ \\begin{array}{rrr} 9.0868825 \\\\ -3.7062460 \\\\ -0.8320503 \\end{array} \\right]_{y_2} \\] Third, we extract the vector, \\(\\left[ \\begin{array}{rrr} x_{22} \\\\ x_{23} \\end{array} \\right] = \\left[ \\begin{array}{rrr} -3.557996 \\\\ -1.664101 \\end{array} \\right]\\), to compute for \\(cos\\ \\theta\\) and \\(sin\\ \\theta\\) and zero out \\(x_{23}\\): \\[\\begin{align*} cos\\ \\theta {}&amp;= \\frac{-3.557996}{\\sqrt{-3.557996^2 + -1.664101^2}} = -0.9058216\\ \\ \\ \\ \\ \\ \\ \\\\ \\\\ sin\\ \\theta &amp;= \\frac{-1.664101}{\\sqrt{-3.557996^2 + -1.664101^2}} = -0.4236594 \\end{align*}\\] We then improvise another transformation matrix, \\(G_3\\), for the transformed matrix, \\(A_2\\). \\[\\begin{align*} G_{3} {}&amp;= \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0\\\\ 0 &amp; cos\\ \\theta &amp; -sin\\ \\theta \\\\ 0 &amp; sin\\ \\theta &amp; cos\\ \\theta \\\\ \\end{array} \\right]_{utility} \\rightarrow \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; -0.9058216 &amp; 0.4236594 \\\\ 0 &amp; -0.4236594 &amp; -0.9058216 \\\\ \\end{array} \\right]_{rotate} \\\\ \\\\ A_{3} &amp;= (G_3)^T\\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.414270 \\\\ 0.000000 &amp; -3.557996 &amp; -3.409746 \\\\ 0.000000 &amp; -1.664101 &amp; -2.496151 \\\\ \\end{array} \\right]_{A_2} = \\left[ \\begin{array}{rrr} 9.0868825 \\\\ -3.7062460 \\\\ -0.8320503 \\end{array} \\right]_{y_2} \\end{align*}\\] We get our third transformation (annihilate lower element by rotation): \\[\\begin{align*} A_{3} = \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.414270 \\\\ 0.000000 &amp; 3.927922 &amp; 4.1461398 \\\\ 0.000000 &amp; 0.000000 &amp; 0.8164963 \\\\ \\end{array} \\right]_{A_3} = \\left[ \\begin{array}{rrr} 9.0868825 \\\\ 3.7097037 \\\\ -0.8164968 \\end{array} \\right]_{y_3} \\end{align*}\\] Lastly, obtain \\(Q\\) and \\(R\\): \\[\\begin{align*} R {}&amp;= G_3^TG_2^TG_1^TA,\\ \\ \\ \\ \\ Q = G_1G_2G_3 \\\\ \\\\ R &amp;= \\left[ \\begin{array}{rrr} 3.741657 &amp; 5.879748 &amp; 6.4142703 \\\\ 0 &amp; 3.927921 &amp; 4.1461394 \\\\ 0 &amp; 0 &amp; 0.8164965 \\end{array} \\right]_{R} \\ \\ \\ \\ \\ \\ \\\\ \\\\ Q &amp;= \\left[ \\begin{array}{rrr} 0.2672612 &amp; 0.8728717 &amp; -0.4082482 \\\\ 0.5345225 &amp; 0.2182178 &amp; 0.8164967 \\\\ 0.8017837 &amp; -0.4364360 &amp; -0.4082482 \\end{array} \\right]_{Q} \\end{align*}\\] Given a matrix, \\(A_{mxn}\\), let us formulate the algorithm: \\[ \\begin{array}{l} function\\ givens(v_1, v_2) \\\\ \\ \\ \\ \\ r = \\sqrt{v_1^2 + v_2^2} \\\\ \\ \\ \\ \\ cos = v_1 / r \\\\ \\ \\ \\ \\ sin = v_2 / r \\\\ \\ \\ \\ \\ list&lt;cos, sin&gt; \\\\ end \\\\ \\\\ function\\ rotate\\_tool(&lt;c,s&gt;, k) \\\\ \\ \\ \\ \\ G = diag(m) \\\\ \\ \\ \\ \\ G_{k-1,k-1} = c \\\\ \\ \\ \\ \\ G_{k-1,k} = -s \\\\ \\ \\ \\ \\ G_{k,k-1} = c \\\\ \\ \\ \\ \\ G_{k,k} = s \\\\ end \\\\ \\end{array} \\left| \\begin{array}{l} Q = diag(m) \\\\ loop\\ j = 1:n \\\\ \\ \\ \\ \\ loop\\ k = m:j \\\\ \\ \\ \\ \\ \\ \\ \\ \\ if\\ j\\ &lt; k: \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ &lt;c,s&gt; = givens(A_{k-1,j}, A{k,j}) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if\\ NaN(&lt;c,s&gt;)\\ next\\ iterate \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ G = rotate\\_tool(&lt;c,s&gt;,k) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ A = G^TA \\leftarrow \\{annihilate\\ lower\\ element\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Q = QG \\\\ \\ \\ \\ \\ end\\ loop \\\\ end\\ loop\\\\ \\end{array} \\right. \\] Here is a naive implementation of the Givens algorithm in R code: qr_decomposition_by_givens &lt;- function(A) { n = ncol(A) m = nrow(A) givens &lt;- function(v1, v2) { r = sqrt(v1^2 + v2^2) cos = v1 / r sin = v2 / r list(&quot;cos&quot;=cos,&quot;sin&quot;=sin) } rotate_tool &lt;- function(cs, k) { G = diag(m) G[k-1,k-1] = cs$cos G[k-1,k] = -cs$sin G[k,k-1] = cs$sin G[k,k] = cs$cos G } Q = diag(m) for (j in 1:n) { for (k in m:j) { if (j &lt; k ) { cs = givens(A[k-1,j],A[k,j]) if (is.nan(cs$cos) || is.nan(cs$sin)) next G = rotate_tool(cs,k) A = t(G) %*% A Q = Q %*% G } } } list(&quot;Matrix&quot; = Q %*% A, &quot;Q&quot; = Q, &quot;R&quot; = A) } A = matrix(c(1,5,5, 2,4,5, 3,3,3), 3, byrow=TRUE) qr_decomposition_by_givens(A) One recent evolution of Givens rotation is Generalized Givens Rotation (GGR) for QR decomposition. This topic is left for readers to follow and investigate. 2.20.5 Cholesky Factorization The idea is to decompose an invertible symmetric positive definite square matrix into its \\(LL^T\\) form or \\(LDL^T\\) form, if and only if \\(U=L^T\\), forming the equation below: \\[\\begin{align} A = LL^T = LDL^T\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where}\\ L\\ \\text{is called Cholesky Factor} \\label{eqn:eqnnumber120} \\end{align}\\] That has the advantage over LU factorization if the square matrix is symmetric positive definite (a Hermitian matrix). We only work on one matrix instead of two, saving storage computationally. To perform Cholesky factorization and get the lower-triangular matrix, we need to perform the Gaussian elimination method for LU factorization. For example, using LU factorization by GE, we get the following: \\[ \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array} \\right]_A = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A} \\left[ \\begin{array}{rrr} 1 &amp; 5 &amp; 5 \\\\ 0 &amp; -6 &amp; -5 \\\\ 0 &amp; 0 &amp; -2 \\end{array} \\right]_{U_A} \\] We use \\(L_A\\) and its transpose: \\[ A = LL^T = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0 \\\\ 2 &amp; 1 &amp; 0 \\\\ 3 &amp; 2 &amp; 1 \\end{array} \\right]_{L_A}\\left[ \\begin{array}{rrr} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_{L_A^T} \\] To review the difference between L and U matrices for Doolittle and Cholesky decomposition [Stanimirovíc P. S. et al. (2011), see LU Decomposition in the previous subsection. 2.20.6 SVD Factorization The idea is to decompose a matrix, A, into its \\(U\\Sigma V^T\\) form; where \\(U\\) and \\(V\\) are both orthogonal matrices, and \\(\\Sigma\\) is a diagonal matrix of Eigenvalues, forming the equation: \\[\\begin{align} A = U\\Sigma V^T \\label{eqn:eqnnumber130} \\end{align}\\] We extract the singular positive real values of a matrix into \\(\\Sigma\\); hence, the reason for the term singular value decomposition. \\[ A_{mxn} = \\left[ \\begin{array}{rrrr} u_{11} &amp; \\ldots &amp; u_{1m} \\\\ u_{21} &amp; \\ldots &amp; u_{2m} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ u_{m1} &amp; \\ldots&amp; u_{mm}\\\\ \\end{array} \\right]_{U_{mxm}} \\left[ \\begin{array}{rrrr} \\sigma_{11} &amp; . &amp; . &amp; . \\\\ . &amp; \\sigma_{22} &amp; . &amp; . \\\\ . &amp; . &amp; \\sigma_{33} &amp; . \\\\ . &amp; . &amp; . &amp; \\sigma_{mn} \\end{array} \\right]_{\\Sigma_{mxn}} \\left[ \\begin{array}{rrrr} v_{11} &amp; \\ldots &amp; v_{1n} \\\\ v_{21} &amp; \\ldots &amp; v_{2n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ v_{n1} &amp; \\ldots&amp; v_{nn}\\\\ \\end{array} \\right]_{V_{nxn}}^T \\] Recall the Eigen equation (\\(\\ref{eqn:eqnnumber58}\\)), where \\(v\\) is an Eigenvector and \\(\\lambda\\) is an Eigenvalue. We know that \\(A\\) is a transformation matrix - it is a utility to scale the Eigenvector \\(v\\). We also know that \\(\\lambda\\) is a transformation scalar - it is a utility to scale the same Eigenvector \\(v\\). In essence, both matrices \\(A\\) and scalar \\(\\lambda\\) fulfill the same purpose - to scale the Eigenvector \\(v\\). We can say that, based on the formula, we can derive an Eigen decomposition formula: \\[\\begin{align} A = v \\lambda v^{-1} = P \\Lambda P^{-1} \\label{eqn:eqnnumber140} \\end{align}\\] Now, let us introduce related formula in the context of SVD decomposition: \\[\\begin{align} A \\cdotp v = \\sigma \\cdotp u,\\ \\ \\ \\ \\ A^T \\cdotp u = \\sigma \\cdotp v \\label{eqn:eqnnumber131} \\end{align}\\] We can think of matrix A as a transformation matrix such that vector \\(v\\) gets transformed into vector \\(u\\) by rotation; additionally, the transformed vector \\(u\\) is scaled by \\(\\sigma\\). Of course, we do not have to deal with just one vector \\(v\\) or one vector \\(u\\). We can deal with a matrix \\(V\\), a matrix \\(U\\), and a matrix \\(\\Sigma\\) where: \\[\\begin{align*} U_m &amp;= \\{u_1, u_2, u_3,..., u_m\\} \\rightarrow \\text{an orthogonal matrix of m column vectors} \\\\ \\Sigma_m &amp;= \\{\\sigma_1, \\sigma_2, \\sigma_3, ..., \\sigma_n\\} \\rightarrow \\text{a diagonal matrix of n positive diagonal entries} \\\\ V_n {}&amp;= \\{v_1, v_2, v_3, ..., v_n\\} \\rightarrow \\text{an orthogonal matrix of n column vectors} \\\\ \\end{align*}\\] With that, we can show the formula as: \\[\\begin{align} A \\cdotp V = U \\cdotp \\Sigma,\\ \\ \\ \\ \\ A^T \\cdotp U = V \\cdotp \\Sigma \\label{eqn:eqnnumber65} \\end{align}\\] And because \\(V\\) and \\(U\\) are orthogonal matrices, it goes to show that: \\[\\begin{align} V^T = V^{-1}, \\ \\ \\ \\ \\ \\ \\ \\ V^TV = VV^T = I, \\ \\ \\ \\ \\ \\ \\ \\ U^T = U^{-1}, \\ \\ \\ \\ \\ \\ \\ \\ U^TU = UU^T = I \\label{eqn:eqnnumber66} \\end{align}\\] Therefore: \\[\\begin{align} A = U\\Sigma V^T, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ A^{T} = V\\Sigma^TU^T, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ A^{-1} = V\\Sigma^{-1}U^T \\label{eqn:eqnnumber67} \\end{align}\\] So how do we solve for \\(U\\), \\(\\Sigma\\), and \\(V\\). First, we perform some mathematical transformations by adding \\(A^T\\) to both sides of the equation: \\[\\begin{align} A^TA &amp;= A^{(1)} = A^T(U\\Sigma V^T) = (U\\Sigma V^T)^T(U\\Sigma V^T)\\\\ A^TA &amp;= A^{(1)} = (V\\Sigma^TU^T)(U\\Sigma V^T) = V\\Sigma^TU^TU\\Sigma V^T \\\\ A^TA &amp;= A^{(1)} = V\\Sigma^T\\Sigma V^T &amp; \\{V^TV = I\\} \\\\ A^TA &amp;= A^{(1)} = V\\Sigma^2V^T &amp; \\{\\Sigma^T\\Sigma = \\Sigma^2\\} \\end{align}\\] \\[\\begin{align} AA^T &amp;= A^{(2)} = (U\\Sigma V^T)A^T = (U\\Sigma V^T)(U\\Sigma V^T)^T\\\\ AA^T &amp;= A^{(2)} = (U\\Sigma V^T)(V\\Sigma^TU^T) = U\\Sigma V^TV\\Sigma^TU^T \\\\ AA^T &amp;= A^{(2)} = U\\Sigma \\Sigma^TU^T &amp; \\{U^TU = I\\}\\\\ AA^T &amp;= A^{(2)} = U\\Sigma^2U^T &amp; \\{\\Sigma\\Sigma^T = \\Sigma^2\\} \\end{align}\\] Second, we transform the formulas into an Eigen equation by adding \\(V\\) and \\(U\\) to both sides of the equation, respectively: \\[\\begin{align} A^{(1)}V &amp;= V\\Sigma^2V^TV &amp; let\\ A^TA = A^{(1)} \\\\ A^{(1)}V &amp;= V\\Sigma^2 &amp; \\{V^TV = I\\} \\\\ A^{(1)}V &amp;= \\Sigma^2V &amp; \\{A \\cdot v = \\lambda \\cdot v, where\\ \\lambda = \\Sigma^2 \\} \\end{align}\\] \\[\\begin{align} A^{(2)}U &amp;= U\\Sigma^2U^TU &amp; let\\ AA^T = A^{(2)} \\\\ A^{(2)}U &amp;= U\\Sigma^2 &amp; \\{U^TU = I\\} \\\\ A^{(2)}U &amp;= \\Sigma^2U &amp; \\{A \\cdot u = \\lambda \\cdot u, where\\ \\lambda = \\Sigma^2 \\} \\end{align}\\] Third, recall the derived equations below from Eigen Equation (\\(\\ref{eqn:eqnnumber58}\\)) and (\\(\\ref{eqn:eqnnumber60}\\)): \\[ (A - \\lambda I)v = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ A v = \\lambda v \\] Equivalently, we get: \\[\\begin{align} (A^{(1)} - \\Sigma^2 I)V = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ A^{(1)} V = \\Sigma^2 V \\\\ (A^{(2)} - \\Sigma^2 I)U = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ A^{(2)} U = \\Sigma^2 U \\end{align}\\] We solve for \\(\\Sigma\\) first by using the below characteristic equations: \\[\\begin{align} det(A^{(1)} - \\Sigma^2 I) = |A^{(1)} - \\Sigma^2 I| = 0 \\\\ det(A^{(2)} - \\Sigma^2 I) = |A^{(2)} - \\Sigma^2 I| = 0 \\end{align}\\] One highlight to note is that \\(\\Sigma\\) or \\(\\sigma\\) is in a positive decreasing order: \\[ \\sigma^1 \\geq \\sigma^2 \\geq \\sigma^3 \\geq \\ ... \\ \\geq \\sigma^{n-1} \\geq q^{n} \\geq 0 \\] Once we get the \\(\\Sigma\\), we then plug into the equations below to get \\(V\\) and \\(U\\): \\[\\begin{align} (A^{(1)} - \\Sigma^2 I)V = 0 \\\\ (A^{(2)} - \\Sigma^2 I)U = 0 \\end{align}\\] From there, in the context of a linear system of equations, we can plug those matrices, \\(U\\), \\(V\\), and \\(\\Sigma\\) into the equation to solve for \\(x\\): \\[\\begin{align} Ax = y \\rightarrow\\ \\ \\ \\ \\ \\ \\ x = A^{-1}y \\rightarrow\\ \\ \\ \\ \\ \\ x = V\\Sigma^{-1}U^Ty \\label{eqn:eqnnumber68} \\end{align}\\] Let us use our sample matrix A to illustrate SVD decomposition. First, let us compose for \\(A^TA\\) and \\(AA^T\\): \\[ A = \\left[\\begin{array}{lll} 1 &amp; 5 &amp; 5 \\\\ 2 &amp; 4 &amp; 5 \\\\ 3 &amp; 3 &amp; 3 \\end{array}\\right],\\ \\ \\ A^TA = A^{(1)} = \\left[\\begin{array}{lll} 14 &amp; 22 &amp; 24 \\\\ 22 &amp; 50 &amp; 54 \\\\ 24 &amp; 54 &amp; 59 \\end{array}\\right]_{A^{(1)}}, \\] \\[ AA^T = A^{(2)} = \\left[\\begin{array}{lll} 51 &amp; 47 &amp; 33 \\\\ 47 &amp; 45 &amp; 33 \\\\ 33 &amp; 33 &amp; 27 \\end{array}\\right]_{A^{(2)}} \\] Second, let us solve for \\(\\Sigma^2\\): Solving for \\(\\Sigma^2\\) based on \\(A^{(1)}\\) and \\(V\\): \\[\\begin{align} det(A^{(1)} - \\Sigma^2 I) = |A^{(1)} - \\Sigma^2 I| = 0 \\end{align}\\] \\[ \\left| \\left[\\begin{array}{lll} 14 &amp; 22 &amp; 24 \\\\ 22 &amp; 50 &amp; 54 \\\\ 24 &amp; 54 &amp; 59 \\end{array}\\right]_{A^{(1)}} - \\Sigma^2 \\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]_I \\right| = \\left| \\left[\\begin{array}{lll} 14-\\Sigma^2 &amp; 22 &amp; 24 \\\\ 22 &amp; 50 - \\Sigma^2 &amp; 54 \\\\ 24 &amp; 54 &amp; 59 - \\Sigma^2 \\end{array}\\right] \\right| = 0 \\] Use the following characteristic equation of matrix \\(A^{(1)}\\) for \\(\\Sigma^2\\): \\[ -\\Sigma^6 + 123\\Sigma^4-500\\Sigma^2 + 144 = 0 \\] Therefore, by computing for the roots, we get our \\(\\Sigma^2\\) and Eigen values, \\(\\Sigma\\), for \\(A^{(1)}\\): \\[\\begin{align*} \\Sigma^2 {}&amp;= &lt;118.80150,\\ 3.88663,\\ 0.31187&gt;\\\\ \\Sigma &amp;= &lt; 10.89961, 1.971454, 0.5584532&gt; \\end{align*}\\] Let us also solve for \\(\\Sigma^2\\) based on \\(A^{(2)}\\) and \\(U\\) (as a validation): \\[\\begin{align} det(A^{(2)} - \\Sigma^2 I) = |A^{(2)} - \\Sigma^2 I| = 0 \\end{align}\\] \\[ \\left| \\left[\\begin{array}{lll} 51 &amp; 47 &amp; 33 \\\\ 47 &amp; 45 &amp; 33 \\\\ 33 &amp; 33 &amp; 27 \\end{array}\\right]_{A^{(2)}} - \\Sigma^2 \\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]_I \\right| = \\left| \\left[\\begin{array}{lll} 51-\\Sigma^2 &amp; 47 &amp; 33 \\\\ 47 &amp; 45 - \\Sigma^2 &amp; 33 \\\\ 33 &amp; 33 &amp; 27 - \\Sigma^2 \\end{array}\\right] \\right| = 0 \\] That takes us to the following characteristic polynomial of matrix \\(A^{(2)}\\)for \\(\\Sigma^2\\): \\[ p(\\Sigma) = -\\Sigma^6 + 123\\Sigma^4-500\\Sigma^2 + 144 \\] Notice that the characteristic polynomial of the matrix \\(A^{(2)}\\) is the same as that obtained from \\(A^{(1)}\\). That makes the two matrices considered to be as similar matrices. Third, let us derive the Eigenvectors for \\(V^{(a)}\\) - using Gaussian Elimination: \\[ (A^{(1)} - \\Sigma^2I)V^{(a)} = \\left( \\left[\\begin{array}{lll} 14 &amp; 22 &amp; 24 \\\\ 22 &amp; 50 &amp; 54 \\\\ 24 &amp; 54 &amp; 59 \\end{array}\\right]_{A^{(2)}} - \\Sigma^2 \\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]_I \\right)V = 0 \\] \\[\\begin{align*} v_1 {}&amp;= \\left[ \\begin{array}{rrr} -104.80 &amp; 22 &amp; 24 \\\\ 22 &amp; -68.80 &amp; 54 \\\\ 24 &amp; 54 &amp; -59.80 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -0.422097 \\\\ 0 &amp; 1 &amp; -0.919837 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 0.422097 \\\\ 0.919837 \\\\ 1 \\end{array} \\right] \\\\ v_2 &amp;= \\left[ \\begin{array}{rrr} 10.11 &amp; 22 &amp; 24 \\\\ 22 &amp; 46.11 &amp; 54 \\\\ 24 &amp; 54 &amp; 55.11 \\end{array} \\right] \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 4.608032 \\\\ 0 &amp; 1 &amp; -1.027396 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} -4.608032 \\\\ 1.027396 \\\\ 1 \\end{array} \\right] \\\\ v_3 &amp;= \\left[ \\begin{array}{rrr} 13.69 &amp; 22 &amp; 24 \\\\ 22 &amp; 49.69 &amp; 54 \\\\ 24 &amp; 54 &amp; 58.69 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0.023021 \\\\ 0 &amp; 1 &amp; 1.076586 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} -0.023021 \\\\ -1.076586 \\\\ 1 \\end{array} \\right] \\end{align*}\\] Fourth, let us derive the Eigenvectors for \\(V^{(b)}\\) - using Gaussian Elimination: \\[ (A^{(2)} - \\Sigma^2I)U^{(a)} = \\left( \\left[\\begin{array}{lll} 51 &amp; 47 &amp; 33 \\\\ 47 &amp; 45 &amp; 33 \\\\ 33 &amp; 33 &amp; 27 \\end{array}\\right]_{A^{(2)}} - \\Sigma^2 \\left[\\begin{array}{lll} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\end{array}\\right]_I \\right)U = 0 \\] \\[\\begin{align*} u_1 {}&amp;= \\left[ \\begin{array}{rrr} -67.80 &amp; 47 &amp; 33 \\\\ 47 &amp; -73.80 &amp; 33 \\\\ 33 &amp; 33 &amp; -91.80 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1.426354 \\\\ 0 &amp; 1 &amp; -1.355510 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1.426354 \\\\ 1.355510 \\\\ 1 \\end{array} \\right] \\\\ u_2 &amp;= \\left[ \\begin{array}{rrr} 47.11 &amp; 47 &amp; 33 \\\\ 47 &amp; 41.11 &amp; 33 \\\\ 33 &amp; 33 &amp; 23.11 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 0.714159 \\\\ 0 &amp; 1 &amp; -0.013754 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} -0.714159 \\\\ 0.013754 \\\\ 1 \\end{array} \\right] \\\\ u_3 &amp;= \\left[ \\begin{array}{rrr} 50.69 &amp; 47 &amp; 33 \\\\ 47 &amp; 44.69 &amp; 33 \\\\ 33 &amp; 33 &amp; 26.69 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1.358512 \\\\ 0 &amp; 1 &amp; 2.167243 \\\\ 0 &amp; 0 &amp; 0 \\end{array} \\right] = \\left[ \\begin{array}{rrr} 1.358512 \\\\ -2.167243 \\\\ 1 \\end{array} \\right]\\\\ \\end{align*}\\] Fifth, normalize each column vectors to convert to orthonormal matrices for \\(V^{(a)}\\), and \\(U^{(a)}\\): \\[\\begin{align*} \\Sigma {}&amp;= \\left[\\begin{array}{rrr} 10.89961 &amp; 1.971454 &amp; 0.5584532 \\end{array}\\right] \\cdotp I \\\\ \\\\ U^{(a)} &amp;= \\left[\\begin{array}{rrr} 1.426354 &amp; -0.714159 &amp; 1.358512 \\\\ 1.355510 &amp; 0.013754 &amp; -2.167243 \\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\\\ &amp;= \\left[\\begin{array}{rrr} 0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\\\ 0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\\\ 0.4530552 &amp; 0.81373127 &amp; 0.3641182 \\end{array}\\right] \\\\ \\\\ V^{(a)} &amp;= \\left[\\begin{array}{rrr} 0.422097 &amp; -4.608032 &amp; -0.023021 \\\\ 0.919837 &amp; 1.027396 &amp; -1.076586 \\\\ 1 &amp; 1 &amp; 1 \\end{array}\\right] \\\\ &amp;= \\left[\\begin{array}{rrr} 0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\\\ 0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\\\ 0.7028558 &amp; 0.2072144 &amp; 0.68048197 \\end{array}\\right] \\end{align*}\\] Finally, let us get the actual \\(V\\) and \\(U\\): If we use \\(V^{(a)}\\) for \\(V\\), then use the following formula to get \\(U\\): \\[\\begin{align} u_i = \\frac{A \\cdot v_i}{\\Sigma}, \\end{align}\\] obtaining the following: \\[\\begin{align*} V^{(a)} = V {}&amp;= \\left[\\begin{array}{rrr} 0.2966733 &amp; -0.9548505 &amp; -0.01566538 \\\\ 0.6465127 &amp; 0.2128912 &amp; -0.73259736 \\\\ 0.7028558 &amp; 0.2072144 &amp; 0.68048197 \\end{array}\\right] \\\\ \\left[\\begin{array}{r} u_1, u_2, ... u_n \\end{array}\\right] = U &amp;= \\left[\\begin{array}{rrr} 0.6462171 &amp; 0.58113351 &amp; -0.4946590 \\\\ 0.6141209 &amp; -0.01119206 &amp; 0.7891327 \\\\ 0.4530552 &amp; -0.81373127 &amp; -0.3641182 \\end{array}\\right] \\end{align*}\\] If we use \\(U^{(a)}\\) for \\(U\\), then use the following formula to get \\(V\\): \\[\\begin{align} v_i = \\frac{A^T \\cdot u_i}{\\Sigma}, \\end{align}\\] obtaining the following matrices: \\[\\begin{align*} U^{(a)} = U {}&amp;= \\left[\\begin{array}{rrr} 0.6462171 &amp; -0.58113351 &amp; 0.4946590 \\\\ 0.6141209 &amp; 0.01119206 &amp; -0.7891327 \\\\ 0.4530552 &amp; 0.81373127 &amp; 0.3641182 \\end{array}\\right] \\\\ \\left[\\begin{array}{r} v_1, v_2, ... v_n \\end{array}\\right] = V &amp;= \\left[\\begin{array}{rrr} 0.2966733 &amp; 0.9548505 &amp; 0.01566538 \\\\ 0.6465127 &amp; -0.2128912 &amp; 0.73259736 \\\\ 0.7028558 &amp; -0.2072144 &amp; -0.68048197 \\end{array}\\right] \\end{align*}\\] We can validate by using Equation \\(\\ref{eqn:eqnnumber130}\\). Here is an implementation of SVD in R code using R’s library (notice a third option produced by the library in addition to the two previous solutions we provided - also see the discrepancy in accuracy): A = matrix(c(1,5,5, 2,4,5, 3,3,3), 3, byrow=TRUE) (M = svd(A)) ## $d ## [1] 10.899610 1.971455 0.558449 ## ## $u ## [,1] [,2] [,3] ## [1,] -0.6462172 0.58113330 0.4946589 ## [2,] -0.6141207 -0.01119169 -0.7891327 ## [3,] -0.4530552 -0.81373143 0.3641183 ## ## $v ## [,1] [,2] [,3] ## [1,] -0.2966734 -0.9548505 0.01566518 ## [2,] -0.6465125 0.2128913 0.73259732 ## [3,] -0.7028559 0.2072144 -0.68048201 Reconstructing the original matrix, we get the following: M$u %*% diag(M$d) %*% t(M$v) ## [,1] [,2] [,3] ## [1,] 1 5 5 ## [2,] 2 4 5 ## [3,] 3 3 3 For further reading about SVD, we can reference Dan Kalman’s paper (1996) and Kirk Baker’s draft (2013). In the next chapter, we discuss iterative algorithms that can be used to compute SVD numerically for matrices with very high dimensions. Apart from solving for a linear system of equations, there are other applications of SVD decomposition, to name a few: PCA (primary component analysis) and LSA (latent semantics analysis). Also, when dealing with large dimensionality features in machine learning, we may need to reduce duplicate features or features with multicollinearity using SVD. 2.20.7 Jordan Decomposition The idea is to decompose a matrix into its \\(VJV^{-1}\\) form; where \\(J\\) is a matrix of jordan canonical form and \\(V\\) is an invertible matrix of eigenvectors (with possible generalized eigenvectors), forming the equation: \\[\\begin{align} A = VJV^{-1} \\label{eqn:eqnnumber70} \\end{align}\\] A jordan canonical form has the following: \\[ J = \\left[\\begin{array}{cccccc} \\lambda &amp; 1 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; \\lambda &amp; 1 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 0 &amp; \\lambda &amp; 1&amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;\\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\lambda &amp; 1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; \\lambda \\end{array}\\right] \\] Recall Rank and Nullity in this chapter in which we described Rank as the dimension of the column space, complementary to the nullity, which is the dimension of the null space. Here, we illustrate Jordan decomposition around these two spaces, but more importantly, around the Eigen equation (\\(\\ref{eqn:eqnnumber58}\\)). Let us manipulate the equation to reduce it to the following format (note that \\(\\mathbf{\\vec{v}}\\) is expressed based on a generalized eigenvector): \\[ \\therefore A_\\lambda v = 0\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ where\\ (A - \\lambda I) = A_{\\lambda} \\] In this case, \\(A_\\lambda\\) is what we call the characteristic matrix. We are interested in the Eigenspace - the Nullspace of the characteristic matrix (See Equation \\(\\ref{eqn:eqnnumber92}\\)). To illustrate Jordan decomposition, having the characteristic matrix format in mind, and given a \\(4\\times4\\) matrix where \\(n=4\\), let us construct several samples (or cases) of Jordan form first. First Case: Suppose that the matrix is rank-three. The nullity is one in this case. So forming the equations: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda^2 v_2 = 0 \\rightarrow\\ \\ A \\cdotp v_2 = v_1 + \\lambda \\times v_2\\\\ A_\\lambda^3 v_3 = 0 \\rightarrow\\ \\ A \\cdotp v_3 = v_2 + \\lambda \\times v_3 \\\\ A_\\lambda^4 v_4 = 0 \\rightarrow\\ \\ A \\cdotp v_4 = v_3 + \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{ccccc} \\lambda &amp; 1 &amp; . &amp; . \\\\ . &amp; \\lambda &amp; 1 &amp; . \\\\ . &amp; . &amp; \\lambda &amp; 1 \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right]= \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; \\square &amp; \\square \\\\ \\square &amp; . &amp; . &amp; \\square \\\\ \\square &amp; . &amp; . &amp; \\square \\\\ \\square &amp; \\square &amp; \\square &amp; \\square \\\\ \\end{array}\\right] \\end{array} \\end{array} \\] We have one whole block of size four (four Eigenvectors). Here we have assumed that \\(\\lambda\\) has an algebraic multiplicity of four, e.g. \\((\\lambda - 1)^4\\), in the Eigenspace, but the geometric multiplicity is one, e.g. \\(dim(N(\\lambda - 1)) = 1\\). Because of that, the Jordan form has only one full Jordan block with one standard eigenvector, \\(v_1\\), and three non-zero generalized eigenvectors, \\(v_2, v_3, v_4\\). If a Jordan block contains , generalized eigenvectors, then there is a chain in the block. \\[ (v, A_\\lambda v, A_\\lambda^2 v, A_\\lambda^3 v, ..., A_\\lambda^{n-1} v) \\] In the example above, we notice \\(A_\\lambda^4 v_4 = 0\\). That is derived as such: \\[\\begin{align*} A \\cdotp v_4 {}&amp;= v_3 + \\lambda \\times v_4 \\\\ (A - \\lambda I) \\cdotp v_4 &amp;= v_3 \\leftarrow let\\ (A - \\lambda I) = A_\\lambda\\\\ A_\\lambda \\cdotp v_4 &amp;= v_3 \\leftarrow \\ \\ \\ A_\\lambda v_4 \\neq 0\\ \\ \\text{\\{ not in Eigenspace}\\}\\\\ A_\\lambda^2 \\cdotp v_4 &amp;= A_\\lambda v_3 = v_2 \\leftarrow \\ \\ \\ A_\\lambda^2 v_4 \\neq 0\\ \\ \\ \\text{\\{ add }A_\\lambda\\text{ to each side \\}} \\\\ A_\\lambda^3 \\cdotp v_4 &amp;= A_\\lambda^2 v_3 = A_\\lambda v_2 = v_1 \\leftarrow \\ \\ \\ A_\\lambda^3 v_4 \\neq 0 \\\\ A_\\lambda^4 \\cdotp v_4 &amp;= A_\\lambda^3 v_3 = A_\\lambda^2 v_2 = A_\\lambda v_1 = 0 \\leftarrow \\ \\ \\ A_\\lambda^4 v_4 = 0 \\\\ \\end{align*}\\] Second Case: Suppose that the matrix is rank-two instead. The nullity is two in this case. So forming the equations: \\[ \\begin{array}{cccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda v_2 = 0 \\rightarrow\\ \\ A \\cdotp v_2 = \\lambda \\times v_2\\\\ A_\\lambda^2 v_3 = 0 \\rightarrow\\ \\ A \\cdotp v_3 = v_2 + \\lambda \\times v_3 \\\\ A_\\lambda^3 v_4 = 0 \\rightarrow\\ \\ A \\cdotp v_4 = v_3 + \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{llll} \\lambda &amp; . &amp; . &amp; . \\\\ . &amp; \\lambda &amp; 1 &amp; . \\\\ . &amp; . &amp; \\lambda &amp; 1 \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; . &amp; . &amp; . \\\\ . &amp; \\square &amp; \\square &amp; \\square \\\\ . &amp; \\square &amp; . &amp; \\square \\\\ . &amp; \\square &amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{nxn} \\end{array} \\end{array} \\] We have two Jordan blocks. The first block has size 1. The other block has a size three. Suppose we have the following equations, still for rank-two, then we form a different system of equations: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ A \\cdotp v_1 = \\lambda_1 \\times v_1\\\\ A_\\lambda^2 v_2 = 0 \\rightarrow\\ A \\cdotp v_2 = v_1 + \\lambda_1 \\times v_2\\\\ A_\\lambda v_3 = 0 \\rightarrow\\ A \\cdotp v_3 = \\lambda_2 \\times v_3 \\\\ A_\\lambda^2 v_4 = 0 \\rightarrow\\ A \\cdotp v_4 = v_3 + \\lambda_2 \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{llll} \\lambda_1 &amp; 1 &amp; . &amp; .\\\\ . &amp; \\lambda_1 &amp; . &amp; .\\\\ . &amp; . &amp; \\lambda_2 &amp; 1\\\\ . &amp; . &amp; . &amp; \\lambda_2\\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; . &amp; . \\\\ \\square &amp; \\square &amp; . &amp; . \\\\ . &amp; . &amp; \\square &amp; \\square \\\\ . &amp; . &amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{nxn} \\end{array} \\end{array} \\] We still have two Jordan blocks. Each block size has size two. That could be a case where we have two \\(\\lambda s\\), e.g., \\((\\lambda-1)^2(\\lambda-2)^2\\); but each \\(\\lambda\\) may only have one geometric multiplicity; thus may need one generalized eigenvector for each \\(\\lambda\\). Third Case: Suppose that the matrix is rank-one. The nullity is three in this case. So forming the equations: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda v_2 = 0 \\rightarrow\\ \\ A \\cdotp v_2 = \\lambda \\times v_2\\\\ A_\\lambda v_3 = 0 \\rightarrow\\ \\ A \\cdotp v_3 = \\lambda \\times v_3 \\\\ A_\\lambda^2 v_4 = 0 \\rightarrow\\ \\ A \\cdotp v_4 = v_3 + \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{ccccc} \\lambda &amp; . &amp; . &amp; . \\\\ . &amp; \\lambda &amp; . &amp; . \\\\ . &amp; . &amp; \\lambda &amp; 1 \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; . &amp; . &amp; . \\\\ . &amp; \\square &amp; . &amp; . \\\\ . &amp; . &amp; \\square &amp; \\square \\\\ . &amp; . &amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{nxn} \\end{array} \\end{array} \\] We have three Jordan blocks. The first two blocks have size one. The last block has size two. Suppose we have the following equations, still for rank-one, then we form a different system of equations: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda^2 v_2 = 0 \\rightarrow\\ \\ A \\cdotp v_2 = v1 + \\lambda \\times v_2\\\\ A_\\lambda v_3 = 0 \\rightarrow\\ \\ A \\cdotp v_3 = \\lambda \\times v_3 \\\\ A_\\lambda^2 v_4 = 0 \\rightarrow\\ \\ A \\cdotp v_4 = \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{ccccc} \\lambda &amp; 1 &amp; . &amp; . \\\\ . &amp; \\lambda &amp; . &amp; . \\\\ . &amp; . &amp; \\lambda &amp; . \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; . &amp; .\\\\ \\square &amp; \\square &amp; . &amp; .\\\\ . &amp; . &amp; \\square &amp; .\\\\ . &amp; . &amp; . &amp; \\square\\\\ \\end{array}\\right]_{nxn} \\end{array} \\end{array} \\] We still have three Jordan blocks. The first block has a size two, and the two other blocks have a size one each. Fourth Case: Suppose that the matrix is rank-zero (all elements are zero). The nullity is four in this case. So forming the equations: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda v_2 = 0 \\rightarrow\\ \\ A \\cdotp v_2 = \\lambda \\times v_2\\\\ A_\\lambda v_3 = 0 \\rightarrow\\ \\ A \\cdotp v_3 = \\lambda \\times v_3 \\\\ A_\\lambda v_4 = 0 \\rightarrow\\ \\ A \\cdotp v_4 = \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{ccccc} \\lambda &amp; . &amp; . &amp; . \\\\ . &amp; \\lambda &amp; . &amp; . \\\\ . &amp; . &amp; \\lambda &amp; . \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; . &amp; . &amp; . \\\\ . &amp; \\square &amp; . &amp; . \\\\ . &amp; . &amp; \\square &amp; . \\\\ . &amp; . &amp; . &amp; \\square \\\\ \\end{array}\\right]_{nxn} \\end{array} \\end{array} \\] We have four Jordan blocks. Each block has a size 1. The critical point to all this is choosing which case to use that dictates the set of equations and thus the number of Jordan blocks that determine the Jordan form to use. To know the number of Jordan blocks, we need to work on the Eigenspace of the characteristic matrix: \\(N(A - \\lambda I)\\) Let us illustrate that by first evaluating the RREF (reduced row-echelon form) of the characteristic matrix for the free variables (Note here that we work on the Nullspace, \\(N(A -\\lambda I) \\leftarrow (A - \\lambda I)v = 0\\): \\[ A = \\left[\\begin{array}{cccc} 1 &amp; 1 &amp; 9 &amp; 0\\\\ 0 &amp; 4 &amp; 3 &amp; 0\\\\ 0 &amp; 3 &amp; 4 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; 1 \\\\ \\end{array}\\right] \\] Our characteristic polynomial in the equation gives us: \\[\\begin{align*} \\lambda^4 - 10\\lambda^3+24\\lambda^2 - 22\\lambda + 7 {}&amp;= 0\\\\ (\\lambda-7)(\\lambda-1)^3 &amp;=0 \\end{align*}\\] However, our minimal polynomial is below (as that is the minimal polynomials whose corresponding characteristic matrices, when multiplied together, result in zero matrices): \\[ (\\lambda-7)(\\lambda-1)^2 =0 \\] That gives us a clue that we have one Jordan block for \\(\\lambda=7\\) and two Jordan blocks for \\(\\lambda=1\\) based on the algebraic multiplicity of the minimal polynomials. We can further validate that by finding the regular Eigenvectors of the characteristic matrix of each corresponding Eigenvalues. Here, we know our Eigenvalues: \\(\\lambda=7\\) and \\(\\lambda=1\\). Now, we need to find our Eigenvectors. First, let us solve for \\(v_4\\) using \\(\\lambda=7\\): \\[\\begin{align*} (A - \\lambda I) {}&amp;= \\left[\\begin{array}{rrrr} -6 &amp; 1 &amp; 9 &amp; 0\\\\ 0 &amp; -3 &amp; 3 &amp; 0\\\\ 0 &amp; 3 &amp; -3 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; -6 \\\\ \\end{array}\\right] \\rightarrow RREF(A - (7)I) \\rightarrow \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 &amp; -10/7\\\\ 0 &amp; 1 &amp; 0 &amp; -6/7\\\\ 0 &amp; 0 &amp; 1 &amp; -6/7 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array}\\right] \\\\ \\end{align*}\\] The RREF of the matrix shows one free variable: r. Because r is a free variable, we can assume any value. In our case, we use \\(r=7\\) only to discard the denominator seven from the other variables. \\[ \\text{solution for x}\\ \\begin{cases} x_1 + \\frac{-10}{7}x_4 = 0 \\\\ x_2 + \\frac{-10}{7}x_4 = 0 \\\\ x_3 + \\frac{-10}{7}x_4 = 0 \\end{cases}\\ \\ \\text{simplied into}\\ \\ \\begin{cases} x_1 = \\frac{10}{7}r \\\\ x_2 = \\frac{6}{7}r \\\\ x_3 = \\frac{6}{7}r \\\\ x_4 = r = 7\\\\ \\end{cases} \\rightarrow x = r\\left[\\begin{array}{rrrrr}10 \\\\ 6 \\\\ 6 \\\\ 7 \\end{array}\\right]_{v_4} \\] That gives us the standard Eigenvector for \\(\\lambda=7\\): \\[ v_4 = \\left[\\begin{array}{rrrrr} 10 &amp; 6 &amp; 6 &amp; 7\\end{array}\\right]^T \\] Second, let us solve for \\(v_3\\) using \\(\\lambda=1\\): We know that our algebraic multiplicity for \\(\\lambda=1\\) is three. However, if we compute the number of Eigenvectors, we only get two Eigenvectors. \\[\\begin{align*} (A -\\lambda I) {}&amp;= \\left[\\begin{array}{cccc} 0 &amp; 1 &amp; 9 &amp; 0\\\\ 0 &amp; 3 &amp; 3 &amp; 0\\\\ 0 &amp; 3 &amp; 3 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; 0 \\\\ \\end{array}\\right] \\rightarrow RREF(A - (1)I) \\rightarrow \\left[\\begin{array}{rrrr} 0 &amp; 1 &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\\\ \\end{array}\\right] \\left[\\begin{array}{rrrrr}x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right] \\end{align*}\\] The RREF of the matrix shows two free variables: r and s. Because they are free variables, we can assume values, \\(r=1, s=1\\): \\[ \\text{solution for x}\\ \\begin{cases} x_2 = 0 \\\\ x_3 = 0 \\end{cases}\\ \\ \\text{simplied into}\\ \\ \\begin{cases} x_1 = r \\\\ x_2 = 0 \\\\ x_3 = 0 \\\\ x_4 = s \\\\ \\end{cases} \\rightarrow x = r\\left[\\begin{array}{rrrrr}1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right]_{v_1} + s\\left[\\begin{array}{rrrrr}0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right]_{v_2} \\] That gives us a set of only two Eigenvectors for \\(\\lambda=1\\): \\[ v_1 = \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 &amp; 0\\end{array}\\right]^T, \\ \\ \\ \\ v_2 = \\left[\\begin{array}{rrrr} 0 &amp; 0 &amp; 0 &amp; 1\\end{array}\\right]^T \\] Overall, our Eigenspace, \\(N(A - \\lambda I)\\), spans only three Eigenvectors \\(\\rightarrow\\) Span { \\(v_1, v_2, v_4\\) }. \\[ N(A - \\lambda I) = Span \\left\\{ \\left[\\begin{array}{r}1 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right]_{v_1} \\left[\\begin{array}{r}0 \\\\ 0 \\\\ 0 \\\\ 1 \\end{array}\\right]_{v_2} \\left[\\begin{array}{r}10 \\\\ 6 \\\\ 6 \\\\ 7 \\end{array}\\right]_{v_4} \\right\\} \\] where dim(\\(N(A - \\lambda I)\\)) = 3 indicating a geometric multiplicity of three which equals three Jordan blocks. In the context of solutions, the three Eigenvectors are found to be solutions in the Null-space, and so the system for which \\((A - \\lambda I) = 0\\) does hold. On the contrary, they do not form a solution in the Column-space of the characteristic matrix thus the system for which \\((A - \\lambda I) \\neq 0\\) does not hold. With all that being said, we come down now to the following Jordan form: \\[ \\begin{array}{ccccc} \\begin{array}{lllll} A_\\lambda v_1 = 0 \\rightarrow\\ \\ \\ \\ A \\cdotp v_1 = \\lambda \\times v_1\\\\ A_\\lambda v_2 = 0 \\rightarrow\\ \\ \\ \\ A \\cdotp v_2 = \\lambda \\times v_2\\\\ A_\\lambda^2 v_3 = 0 \\rightarrow\\ \\ \\ \\ A \\cdotp v_3 = v_2 + \\lambda \\times v_3 \\\\ A_\\lambda v_4 = 0 \\rightarrow\\ \\ \\ \\ A \\cdotp v_4 = \\lambda \\times v_4 \\\\ \\end{array} &amp; \\begin{array}{cc} J = \\left[\\begin{array}{ccccc} \\lambda &amp; . &amp; . &amp; . \\\\ . &amp; \\lambda &amp; 1 &amp; . \\\\ . &amp; . &amp; \\lambda &amp; . \\\\ . &amp; . &amp; . &amp; \\lambda \\\\ \\end{array}\\right] = \\left[\\begin{array}{ccccc} \\square &amp; . &amp; . &amp; . \\\\ . &amp; \\square &amp; \\square &amp; .\\\\ . &amp; \\square &amp; \\square &amp; .\\\\ . &amp; . &amp; . &amp; \\square \\end{array}\\right] \\end{array} \\end{array} \\] That is because \\(\\lambda=1\\) gives us only two Eigenvectors. So we need one generalized Eigenvector, \\(v_3\\); that makes one of the Jordan blocks size two. To solve for \\(v_3\\), let us, therefore, use the equation: \\[ A_\\lambda^2 v_3 = 0 \\rightarrow\\ \\ \\ \\ A \\cdotp v_3 = v_2 + \\lambda \\times v_3 \\] The regular Eigenvector \\(v_2\\) will be disposed of as a new one because \\(v_2\\) participates now in a Jordan block of size two instead of this vector being isolated to its own Jordan block. Let us put aside \\(v_2\\) for now to solve for \\(v_3\\). To do that, we simplify the equation further: \\[\\begin{align*} A v_3 {}&amp;= v_2 + \\lambda \\times v_3 \\\\ (A - \\lambda I) v_3 &amp;= v_2 \\\\ A_\\lambda v_3 &amp;= v_2 \\\\ \\end{align*}\\] However, that does not give us a solution in the Null-space for \\(v_3\\); therefore, let us convert the equation into one with Nilpotent polynomial. The trick is to add \\(A_\\lambda\\) to each side of the equation. \\[\\begin{align*} A_\\lambda v_3 {}&amp;= v_2 \\\\ A_\\lambda A_\\lambda v_3 &amp;= A_\\lambda v_2 \\\\ A_\\lambda^2 v_3 &amp;= A_\\lambda v_2 \\\\ A_\\lambda^2 v_3 &amp;= 0, \\ \\ \\ \\ \\ \\because A_\\lambda v_2 = 0 \\end{align*}\\] Our characteristic matrix corresponding to \\(\\lambda=1\\) becomes: \\[ (A\\lambda)^2v_3 = \\left[ \\begin{array}{cccc} 0 &amp; 30 &amp; 30 &amp; 0 \\\\ 0 &amp; 18 &amp; 18 &amp; 0 \\\\ 0 &amp; 18 &amp; 18 &amp; 0 \\\\ 0 &amp; 21 &amp; 21 &amp; 0 \\\\ \\end{array} \\right]v_3 = 0 \\rightarrow \\begin{cases} 30_{x_2} + 30_{x_3} = 0 \\\\ 18_{x_2} + 18_{x_3} = 0 \\\\ 18_{x_2} + 18_{x_3} = 0 \\\\ 21_{x_2} + 21_{x_3} = 0 \\end{cases} \\rightarrow \\begin{cases} x_2 = -x_3 \\end{cases} \\] That gives us our \\(v_3\\) vector: \\[ v_3 = \\left[\\begin{array}{rrrr}0 &amp; 1 &amp; -1 &amp; 0 \\end{array}\\right]^T \\] Third, let us now solve for \\(v_2\\). We know that we can solve for \\(v_2\\) using the following previous equation: \\[ A \\cdotp v_3 = v_2 + \\lambda \\times v_3\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ (A - \\lambda I)v_3 = v_2 \\] Therefore, our new \\(v_2\\) becomes: \\[ v2 = (A - \\lambda I)v_3 = \\left[ \\begin{array}{cccc} 0 &amp; 1 &amp; 9 &amp; 0 \\\\ 0 &amp; 3 &amp; 3 &amp; 0 \\\\ 0 &amp; 3 &amp; 3 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; 0 \\\\ \\end{array} \\right]_{(A - \\lambda I)} \\left[\\begin{array}{ccc} 0 \\\\ 1 \\\\ -1 \\\\ 0 \\end{array} \\right]_{v_3} = \\left[\\begin{array}{ccc} -8 \\\\ 0 \\\\ 0 \\\\ 5 \\end{array} \\right]_{v_2} \\] Let us validate if \\(v_2\\) is the end of the Jordan block chain - though we mentioned that the size is two. \\[ (A - \\lambda I)v_2 = \\left[ \\begin{array}{cccc} 0 &amp; 1 &amp; 9 &amp; 0 \\\\ 0 &amp; 3 &amp; 3 &amp; 0 \\\\ 0 &amp; 3 &amp; 3 &amp; 0 \\\\ 0 &amp; 6 &amp; 1 &amp; 0 \\\\ \\end{array} \\right]_{(A - \\lambda I)} \\left[\\begin{array}{ccc} -8 \\\\ 0 \\\\ 0 \\\\ 5 \\end{array} \\right]_{v_2} = \\left[\\begin{array}{ccc} 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array} \\right]_{0} \\] That shows that the new \\(v_2\\) is also a solution in the Eigenspace. Finally, we have obtained our \\(V\\) and \\(J\\): \\[ J = \\left[\\begin{array}{rrrr} 1 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 7\\\\ \\end{array}\\right],\\ \\ \\ \\ \\ V = \\left[\\begin{array}{rrrr} 1 &amp; -8 &amp; 0 &amp; 10\\\\ 0 &amp; 0 &amp; 1 &amp; 6 \\\\ 0 &amp; 0 &amp; -1 &amp; 6\\\\ 0 &amp; 5 &amp; 0 &amp; 7\\\\ \\end{array}\\right] \\] What we just did by using a new \\(v_2\\) and \\(v_3\\) to form \\(V\\) is what we call a change of basis. With that, we can reconstruct our original matrix \\(A\\) using Equation (\\(\\ref{eqn:eqnnumber70}\\)): As an exercise, try the following matrix to arrive at the following hints: \\[ A = \\left[\\begin{array}{cccc} 1 &amp; 6 &amp; 1 &amp; 0\\\\ 0 &amp; 3 &amp; 4 &amp; 0\\\\ 0 &amp; 4 &amp; 3 &amp; 0 \\\\ 0 &amp; 1 &amp; 9 &amp; 1 \\\\ \\end{array}\\right] \\] Hint for the characteristic equation \\[\\begin{align*} \\lambda^4-8\\lambda^3+6\\lambda^2 + 8\\lambda -7 {}&amp;= 0 \\\\ (\\lambda^3-9\\lambda^2+15\\lambda + -7)(x+1) &amp;= 0 \\\\ (\\lambda^2-8\\lambda+7)(x-1)(x+1) &amp;= 0 \\\\ (x-1)(x-7)(x-1)(x+1) &amp;= 0 \\\\ (x-7)(x-1)^2(x+1) &amp;= 0 \\\\ \\end{align*}\\] Hint for J and V: \\[ J = \\left[\\begin{array}{rrrr} 7 &amp; 0 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; -1\\\\ \\end{array}\\right],\\ \\ \\ \\ \\ V = \\left[\\begin{array}{rrrr} 7 &amp; 1 &amp; 0 &amp; 5\\\\ 6 &amp; 0 &amp; 0 &amp; -2 \\\\ 6 &amp; 0 &amp; 0 &amp; 2\\\\ 10 &amp; 0 &amp; 1 &amp; -8\\\\ \\end{array}\\right] \\] Note that other literature use transposed Jordan form instead and thus carries on altered methods, e.g.: \\[ J = \\left[\\begin{array}{cccccc} \\lambda &amp; 0 &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 1 &amp; \\lambda &amp; 0 &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 1 &amp; \\lambda &amp; 0 &amp; \\ldots &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; \\lambda &amp; \\ldots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp;\\ddots &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; \\lambda \\end{array}\\right] \\] For further reading about Jordan block matrices and finding a matrix’s minimal polynomial, we reference Attila Máté’s paper (2014). 2.20.8 Other Decomposition Other decompositions are equally important in solving problems. However, this book only introduces the decomposition equations and does not discuss the methods in detail. Schur Decomposition The idea is to decompose a matrix into its \\(QUQ^{T} or\\ QUQ^{-1}\\) form; where \\(Q\\) is an orthogonal matrix and \\(U\\) is an Upper Triangular matrix, forming the equation: \\[\\begin{align} A = QUQ^{-1} \\label{eqn:eqnnumber71} \\end{align}\\] Hessenberg Decomposition The idea is to decompose a matrix into its \\(PHP^T\\) form; where \\(H\\) is a Hessenberg matrix and \\(P\\) is a unitary matrix, forming the equation: \\[\\begin{align} A = PHP^{T} \\label{eqn:eqnnumber72} \\end{align}\\] Polar Decomposition The idea is to decompose a matrix into its \\(UP\\) form; where \\(U\\) is a unitary matrix and \\(P\\) is a positive semi-definite Hermitian matrix, forming the equation: \\[\\begin{align} A = UP \\label{eqn:eqnnumber73} \\end{align}\\] A matrix, being regarded as a transformation utility, can stretch or rotate a vector: \\[ Av = b\\ \\ \\ \\ \\text{ where b is a transformed vector} \\] The matrix can also be regarded as a deformation utility. It deforms a vector. Polar decomposition can be helpful when factoring in a deformation matrix and thus has a geometric representation. 2.21 Software libraries Computationally, software libraries are built to allow optimized low-level operations and subroutines for linear algebra, including computation for Eigenvectors and Eigenvalues and other matrix factorization (or decompositions). BLAS - Basic Linear Algebra Subroutine LAPACK - Linear Algebra PACKage LINPACK - LINear Algebra PACKage EISPACK - EIgenSystem PACKage Because these built-in libraries come with optimized linear algebra operations and subroutines, it is perhaps ideal to use them instead of re-implementing algorithms from scratch. 2.22 Summary Many fundamental concepts of Linear Algebra are available in search engines nowadays. Moreover, it is common to find tutorials and notes that give simple intuition about the subjects we learned in this chapter. Among them, a few other references come to mind, namely, Dawkins P. (2007), Ivan Savov I. (2017), and Jim Hefferon J. (2020). While software subroutines allow us to perform linear algebra computationally, it also helps to understand the reliability of certain linear algebra operations. It pays to be wary of the performance implications of linear algebra. That said, we cover approximation and optimization topics in the next chapter. "],["numericallinearalgebra.html", "Chapter 3 Numerical Linear Algebra II 3.1 Iteration and Convergence 3.2 Approximating Eigenvalues and EigenVectors by Iteration (\\(Av = \\lambda v\\)) 3.3 Approximating Root and Fixed-Point by Iteration 3.4 Approximating Solutions to Systems of Eqs by Iteration (\\(Ax = b\\)) 3.5 Approximating Polynomial Functions by Regression 3.6 Approximating Polynomial Functions by Series Expansion 3.7 Approximating Polynomial Functions by Interpolation 3.8 Approximating Polynomial Functions by Smoothing 3.9 Polynomial Optimization 3.10 Summary", " Chapter 3 Numerical Linear Algebra II It can be said that the oldest way of approximating a solution, especially in finding the root value, is the trial-and-error approach. We start with some random false position and then work our way up iteratively to the point of hitting the correct position. This archaic iterative method is described in the classic Regula-Falsi method, also called the False Position method. Other traditional methods, such as Archimedes iteration and Brent-Salamin iteration - to name a few - also adopt the same approach. They were developed to find the most accurate value of the famous PI (\\(\\pi\\)) (Bailey D.H. 2021). In this chapter, we shall show many different Numerical methods rooted in the same idea. Here, we begin to focus on Indirect Methods in the context of Linear Algebra as we reference the great works of Atkinson K. E. (1989), Heath M.T. (2002), Burden R.L. et al. (2005), Press W.H. et al. (2007), and Edwards H. et al. (2018) along with other additional references for consistency. A few places in the discussion may bring about solving for derivatives, which is part of Calculus covered in the next chapter when we talk about Numerical Calculus. Note that it helps to review Calculus first for some of the methods discussed in this chapter. Let us start with the concept of iteration and convergence. 3.1 Iteration and Convergence We rely on Iterative methods in situations where no standard analytical approaches are available to provide an exact result or where analytical techniques are impractical. An iterative method aims to get as close to the actual value or target value as possible; in other words, we expect the approach to lead us to a Convergence - it is an approximation method. Convergence can be associated with tolerance. The key idea is to iterate through steps, re-evaluating the solution until it converges, meaning it hits the expected target. Here, we describe the target in terms of tolerance level against the accuracy of the solution. So if the tolerance level is at 99.9% accuracy and we reach the target accuracy, then the iteration ends, and an approximate value of the actual value is delivered as the final solution. 3.2 Approximating Eigenvalues and EigenVectors by Iteration (\\(Av = \\lambda v\\)) In our discussion about matrix decomposition, we aim to find the Eigenvalues and corresponding Eigenvectors of a matrix. In such an analytical process, we rely on evaluating the determinant of a characteristic matrix, in the form of \\(det(A - \\lambda I) = 0\\) to generate a characteristic polynomial that leads to a set of solutions for our Eigenvalues. It may seem simple and easy to take that approach, but if we begin to perform the same steps against an extensive matrix - which is practically common - then deriving the characteristic equation becomes extremely complex and unimaginable. For this reason, we seek other alternatives. Numerically, we can use iterative methods to approximate Eigenvalues and Eigenvectors. And that is what we shall cover in this section. We may accept only a reasonable finite number of loops or iterations for any process involving iteration before we stop the iteration. Therefore, for most discussions around iterations, let us use a tolerance level (our tolerance threshold) such as the value below: tol = 1e-5 Before we begin, let us use eigen(.) function to generate the actual Eigenvalues and Eigenvectors, which we will use to validate our solutions later. Our sample matrix and the result of the eigen(.) function are shown below: (A = matrix(c(3,3,3,2,4,5,1,5,5), 3, byrow=TRUE)) ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 eigen(A) ## eigen() decomposition ## $values ## [1] 10.6771903 1.9109438 -0.5881341 ## ## $vectors ## [,1] [,2] [,3] ## [1,] -0.4836669 -0.91825423 0.07759288 ## [2,] -0.6128191 -0.05832852 -0.74984844 ## [3,] -0.6249152 0.39167200 0.65704388 3.2.1 Power Method The idea is to use iteration to approximate an Eigenvalue of a matrix using an initial arbitrary vector until the iteration stops at a tolerance level. The Power Method estimates only the dominant Eigenvalue - or the maximum absolute Eigenvalue (Atkinson K. E. 1989; Bai Z. et al 2000). \\[ |\\lambda_1| &gt; |\\lambda_2|\\ge |\\lambda_3| \\ge ... \\ge |\\lambda_n| \\] The restriction imposed by this method is to find only the one largest dominant Eigenvalue along with its corresponding Eigenvector for a matrix with multiple Eigenvalues or even with one Eigenvalue but having Multiplicity greater than one. Additionally, the condition above applies to Diagonalizable matrices in which any column is a linear combination of Eigenvectors. Below illustrates the sequence against which we iterate until convergence. \\[ Av_0, A^2v_1, A^3v_2,\\ ...\\ \\ \\ \\ \\ \\text{where } v_j \\text{ is an Eigenvector}. \\] We normalize the Eigenvector at each iteration using the below equation: \\[\\begin{align} v_j = \\frac{v_j} {\\|v_j\\|_{L2}} = \\frac{v_j} {\\sqrt{\\sum{v_j^2}}} \\end{align}\\] For the sake of illustration, we use the Rayleigh Equation below to derive the EigenValue (B. N. Parlett 1974). We also use the value to compute the error. \\[\\begin{align} \\lambda = \\frac{v^T \\cdotp Av}{v^Tv} \\end{align}\\] Here, \\(\\lambda\\) is a natural approximation of an Eigenvalue if \\(\\mathbf{v}\\) is close to the actual Eigenvector. That said, let us now introduce the normalized Power Method algorithm: \\[ \\begin{array}{l} v_0 \\leftarrow \\text{initial arbitrary nonzero vector}\\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ v_j = A \\cdotp v_{j-1} \\\\ \\ \\ \\ \\ v_j = v_j / \\|v_j\\|_{L2}\\ \\ \\ \\leftarrow \\text{normalized Eigenvector}\\\\ \\ \\ \\ \\ \\lambda_j = v_j^TAv_j / v_j^Tv_j \\ \\ \\ \\leftarrow \\text{Eigenvalue} \\\\ \\ \\ \\ \\ if\\ err(\\lambda_j, \\lambda_{j-1}) &lt; \\text{tolerance then break} \\\\ end\\ loop \\end{array} \\] where \\(\\mathbf{\\vec{v}}\\) is the approximate eigenvector and \\(\\lambda\\) is the approximate (dominant) eigenvalue. Our naive implementation of normalized Power Method in R code shows the following (note that the eigenvector output is scaled): power_method &lt;-function(A) { n = ncol(A) sequence = matrix(0, 0, n + 3) vj = c(1,rep(0,ncol(A)-1)) # initial arbitrary nonzero vector old_evalue = 0; evalue = 0 limit = 100 tol=1e-5; err = 0 for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, evalue, vj, err)) } else { vj = A %*% vj vj = vj / sqrt(sum(vj^2)) # normalized Eigenvector evalue = (t(vj) %*% (A %*% vj))/ (t(vj) %*% vj) # Eigvenvalue err = (evalue - old_evalue)/evalue sequence = rbind(sequence, c(j, evalue, vj, err)) if (abs(err) &lt; tol) break } old_evalue = evalue } colnames(sequence) = c(&quot;J&quot;, &quot;eigenval&quot;, paste(&quot;eigenvec&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;matrix&quot;=A, &quot;eigenvalue&quot;=evalue, &quot;eigenvector&quot;=sequence[nrow(sequence),3:(ncol(A)+2)]) } A = matrix(c(3,3,3,2,4,5,1,5,5), 3, byrow=TRUE) (E.normalized = power_method(A)) ## $Iteration ## J eigenval eigenvec1 eigenvec2 eigenvec3 error ## [1,] 0 0.000000 1.0000000 0.0000000 0.0000000 0.000000e+00 ## [2,] 1 7.857143 0.8017837 0.5345225 0.2672612 1.000000e+00 ## [3,] 2 10.368682 0.5666657 0.5981471 0.5666657 2.422236e-01 ## [4,] 3 10.633900 0.4992259 0.6111735 0.6141991 2.494084e-02 ## [5,] 4 10.669882 0.4864789 0.6125083 0.6230344 3.372238e-03 ## [6,] 5 10.675896 0.4841708 0.6127657 0.6245773 5.633452e-04 ## [7,] 6 10.676959 0.4837571 0.6128095 0.6248548 9.957788e-05 ## [8,] 7 10.677149 0.4836831 0.6128174 0.6249044 1.777917e-05 ## [9,] 8 10.677183 0.4836698 0.6128188 0.6249133 3.180791e-06 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $eigenvalue ## [,1] ## [1,] 10.67718 ## ## $eigenvector ## eigenvec1 eigenvec2 eigenvec3 ## 0.4836698 0.6128188 0.6249133 The iteration stops after the tolerance is reached, giving us an approximate Eigenvalue of \\(\\lambda\\)=10.6771829 with the corresponding approximate Eigenvector, v=(0.4836698, 0.6128188, 0.6249133). 3.2.2 Inverse Power Method (using LU Decomposition) In contrast to the Power Method, the Inverse Power Method restricts us to the smallest Eigenvalue (Bai Z. et al. 2000). The method is similar to the Power Method. We follow the same iteration, except we use an inverse matrix this time. To avoid the cost of calculating the inverse of a matrix, we decompose the matrix into LU form and use the decomposed components. In our case, we use LU decomposition by Doolittle (See Section LU Decomposition in Chapter 2 (Numerical Linear Algebra I)) to extract the LU form, then perform substitution to solve for the system. That said, let us now introduce the Inverse Power Method algorithm (a modified version with Doolittle Decomposition): \\[ \\begin{array}{l} v_0 \\leftarrow \\text{initial arbitrary nonzero vector}\\\\ LU = lu\\_decomposition\\_by\\_doolittle(A)\\\\ \\text{loop}\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ u_y = forward\\_sub(L, v_{j-1} ) \\\\ \\ \\ \\ \\ v_j = backward\\_sub(U, u_y ) \\\\ \\ \\ \\ \\ v_j = v_j / \\|v_j\\|_{L2}\\ \\ \\ \\leftarrow \\text{normalize Eigenvector}\\\\ \\ \\ \\ \\ \\lambda_j = v_j^TAv_j / v_j^Tv_j \\ \\ \\ \\leftarrow \\text{Eigenvalue} \\\\ \\ \\ \\ \\ if\\ err(\\lambda_j, \\lambda_{j-1}) &lt; \\text{tolerance then break} \\\\ \\text{end loop} \\end{array} \\] Note in our algorithm that we have not introduced the use of shift which we cover in the next method. Below is a naive implementation of the Inverse Power Method in R (note that the eigenvector output is scaled): inverse_power_method &lt;-function(A) { LU = lu_decomposition_by_doolittle(A) #derived from Lin Algebra Ch n = ncol(A) sequence = matrix(0, 0, n + 3) vj = c(1,rep(0,ncol(A)-1)) # initial arbitrary nonzero vector old_evalue = 0; evalue = 0 limit = 100 tol=1e-5; err = 0 for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, evalue, vj, err)) } else { uy = forward_sub(LU$lower, vj) vj = backward_sub(LU$upper, uy) vj = vj / sqrt(sum(vj^2)) # Normalize Eigenvector evalue = (t(vj) %*% (A %*% vj))/ (t(vj) %*% vj) # Eigvenvalue err = (evalue - old_evalue)/ evalue sequence = rbind(sequence, c(j, evalue, vj, err)) if (abs(err) &lt; tol) break } old_evalue = evalue } colnames(sequence) = c(&quot;J&quot;, &quot;eigenval&quot;, paste(&quot;eigenvec&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;matrix&quot;=A, &quot;eigenvalue&quot;=evalue, &quot;eigenvector&quot;=sequence[nrow(sequence),3:(ncol(A)+2)]) } A = matrix(c(3,3,3,2,4,5,1,5,5), 3, byrow=TRUE) (E.scaled = inverse_power_method(A)) ## $Iteration ## J eigenval eigenvec1 eigenvec2 eigenvec3 error ## [1,] 0 0.0000000 1.000000000 0.0000000 0.0000000 0.000000e+00 ## [2,] 1 0.6976744 0.539163866 0.5391639 -0.6469966 1.000000e+00 ## [3,] 2 -0.5317854 0.361791913 -0.7488251 0.5553085 2.311947e+00 ## [4,] 3 -0.5177750 0.006673751 0.7368606 -0.6760117 -2.705868e-02 ## [5,] 4 -0.6025525 0.104197718 -0.7527992 0.6499509 1.406972e-01 ## [6,] 5 -0.5829676 -0.069460484 0.7488318 -0.6591101 -3.359522e-02 ## [7,] 6 -0.5896563 0.080101260 -0.7501514 0.6563967 1.134351e-02 ## [8,] 7 -0.5876591 -0.076821393 0.7497542 -0.6572420 -3.398616e-03 ## [9,] 8 -0.5882797 0.077830375 -0.7498773 0.6569828 1.054838e-03 ## [10,] 9 -0.5880892 -0.077519795 0.7498395 -0.6570627 -3.238135e-04 ## [11,] 10 -0.5881479 0.077615379 -0.7498512 0.6570381 9.973976e-05 ## [12,] 11 -0.5881298 -0.077585960 0.7498476 -0.6570457 -3.068956e-05 ## [13,] 12 -0.5881354 0.077595014 -0.7498487 0.6570433 9.446082e-06 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $eigenvalue ## [,1] ## [1,] -0.5881354 ## ## $eigenvector ## eigenvec1 eigenvec2 eigenvec3 ## 0.07759501 -0.74984870 0.65704333 3.2.3 Rayleigh Quotient Method (using LU Decomposition) The Rayleigh Quotient Method is a variant of Inverse Power Method (B. N. Parlett 1974). Here, the LU decomposition takes a shifted matrix using the calculated Eigenvector, \\(\\lambda_{j-1}\\), (via Rayleigh Quotient) at every iteration. The shift is expressed as such: \\[ A - \\lambda_{j-1} I \\] Here, the calculated Eigenvector is called the shift, which allows for faster convergence (Heath M.T. p.176, 2002). That said, let us now introduce the Rayleigh Quotient Iteration algorithm (a modified version with Doolittle Decomposition): \\[ \\begin{array}{l} v_0 \\leftarrow \\text{initial arbitrary nonzero vector}\\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ LU = lu\\_decomposition\\_by\\_doolittle(A - \\lambda_{j-1} I)\\\\ \\ \\ \\ \\ u_y = forward\\_sub(L, v_{j-1} ) \\\\ \\ \\ \\ \\ v_j = backward\\_sub(U, u_y ) \\\\ \\ \\ \\ \\ v_j = v_j / \\|v_j\\|_{L2}\\ \\ \\ \\leftarrow \\text{normalized Eigenvector}\\\\ \\ \\ \\ \\ \\lambda_j = v_j^TAv_j / v_j^Tv_j \\ \\ \\ \\leftarrow \\text{Eigenvalue} \\\\ \\ \\ \\ \\ if\\ err(v_j, v_{j-1}) &lt; \\text{tolerance then break} \\\\ end\\ loop \\end{array} \\] Below is a naive implementation of Rayleigh Quotient Method in R (note that the eigenvector output is scaled): rayleighquotient_method &lt;-function(A, eigenvector) { n = ncol(A) sequence = matrix(0, 0, n + 3) vj = eigenvector # initial arbitrary nonzero vector old_evalue = 0; evalue = 0 limit = 100 tol=1e-5; err = 0 for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, evalue, vj, err)) } else { B = A - c(evalue) * diag(n) LU = lu_decomposition_by_doolittle(B) # from Lin Algebra Ch uy = forward_sub(LU$lower, vj) vj = backward_sub(LU$upper, uy) vj = vj / sqrt(sum(vj^2)) evalue = (t(vj) %*% (A %*% vj)) / (t(vj) %*% vj) err = (evalue - old_evalue)/ evalue sequence = rbind(sequence, c(j, evalue, vj, err)) if (abs(err) &lt; tol) break } old_evalue = evalue } colnames(sequence) = c(&quot;J&quot;, &quot;eigenval&quot;, paste(&quot;eigenvec&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;matrix&quot;=A, &quot;eigenvalue&quot;=evalue, &quot;eigenvector&quot;=sequence[nrow(sequence),3:(ncol(A)+2)]) } A = matrix(c(3,3,3,2,4,5,1,5,5), 3, byrow=TRUE) neigenvector = E.scaled$eigenvector #eigenvector = E.normalized$eigenvector rayleighquotient_method(A,neigenvector ) ## $Iteration ## J eigenval eigenvec1 eigenvec2 eigenvec3 error ## [1,] 0 0.0000000 0.07759501 -0.7498487 0.6570433 0.000000e+00 ## [2,] 1 -0.5881337 -0.07759223 0.7498484 -0.6570441 1.000000e+00 ## [3,] 2 -0.5881341 0.07759288 -0.7498484 0.6570439 6.841798e-07 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $eigenvalue ## [,1] ## [1,] -0.5881341 ## ## $eigenvector ## eigenvec1 eigenvec2 eigenvec3 ## 0.07759288 -0.74984844 0.65704388 Given an actual Eigenvector, we can use the Rayleigh Quotient Method to discover the corresponding unknown Eigenvalue. More importantly, the method allows discovering interior Eigenvalues, which is an advantage over Power Method, which only discovers the largest Eigenvalue and over Inverse Power Method, which only finds the smallest Eigenvalue. 3.2.4 QR Method (using QR Decomposition by Givens) Let us now look into the QR Method, which allows us to find the list of Eigenvalues of a given matrix. In Linear Algebra, we have discussed how to decompose a matrix into its QR form. We use QR decomposition by Givens in the algorithm to derive all the Eigenvalues. \\[ \\begin{array}{l} loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ QR = qr\\_decomposition\\_by\\_givens(A_{j-1})\\\\ \\ \\ \\ \\ A_j = Q^T A_{j-1} Q\\\\ \\ \\ \\ \\ \\lambda_j = diag(Q) \\\\ \\ \\ \\ \\ if\\ err(\\|\\lambda_j\\|_{L2}, \\|\\lambda_{j-1}\\|_{L2}) &lt; \\text{tolerance then break} \\\\ end\\ loop \\end{array} \\] We implement the QR Method in R code like so: qr_method &lt;-function(A) { n = ncol(A) sequence = matrix(0, 0, n + 2) old_evalue = evalues = rep(0, n) limit = 100 tol = 1e-5; err = 0 A_ = A for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, evalues, err)) } else { QR = qr_decomposition_by_givens(A) # from Lin Algebra Ch A = t(QR$Q) %*% A %*% QR$Q evalues = diag(QR$R) ls_ev = sqrt(sum(evalues^2)) ls_ov = sqrt(sum(old_evalues^2)) err = (ls_ev - ls_ov) / ls_ev sequence = rbind(sequence, c(j, evalues, err)) if (abs(err) &lt; tol) break } old_evalues = evalues } colnames(sequence) = c(&quot;J&quot;, paste(&quot;eigenval&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;matrix&quot;= A_, &quot;eigenvalues&quot;=evalues) } A = matrix(c(3,3,3,2,4,5,1,5,5), 3, byrow=TRUE) qr_method(A) ## $Iteration ## J eigenval1 eigenval2 eigenval3 error ## [1,] 0 0.000000 0.000000 0.0000000 0.000000e+00 ## [2,] 1 3.741657 3.927922 -0.8164966 1.000000e+00 ## [3,] 2 8.489489 2.227705 -0.6345152 3.765876e-01 ## [4,] 3 10.404981 1.993459 -0.5785391 1.706118e-01 ## [5,] 4 10.635189 1.907776 -0.5914371 1.950956e-02 ## [6,] 5 10.669923 1.915441 -0.5871527 3.247539e-03 ## [7,] 6 10.675897 1.910184 -0.5884393 4.617532e-04 ## [8,] 7 10.676959 1.911289 -0.5880405 1.120004e-04 ## [9,] 8 10.677149 1.910858 -0.5881629 1.078783e-05 ## [10,] 9 10.677183 1.910974 -0.5881252 4.771086e-06 ## ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $eigenvalues ## [1] 10.6771829 1.9109740 -0.5881252 As long as QR Method yields all the Eigenvalues, this saves us from composing the characteristic polynomials using determinants. From here, we plug the individual Eigenvalues into the original matrix and form each corresponding characteristic matrix. Then, we derive the RREF of each characteristic matrix and perform a substitution to get the corresponding Eigenvectors. Note that QR iteration may not necessarily converge for Eigenvalues that are negative or for matrices with complex numbers. For this, workarounds such as conjugate shifts are being used. Also, note that other QR decomposition such as Householder can be used and evaluated, though best to play around with the “-sign” in Householder as that tends to vary the signs of Eigenvalues (Anley E. F. 2016). 3.2.5 Jacobi Eigenvalue Method (using Jacobi Rotation) The Jacobi Eigenvalue Method reduces a real symmetric matrix into its diagonal form. This reduction is an orthogonalization transformation called Jacobi rotation based on Givens rotation (Heath M.T. 2002; Burden R.L. et al. 2005). \\[ G = \\left[ \\begin{array}{rr} cos &amp; -sin \\\\ sin &amp; cos \\end{array} \\right] \\] Recall that Givens rotation is used in QR decomposition to transform a matrix into its upper triangular form by annihilating its lower triangular region, excluding the diagonal entries. In the same fashion, the Jacobi Eigenvalue Method uses Jacobi rotation to annihilate non-diagonal entries, leaving a diagonal with Eigenvalues as entries. To illustrate, here is a simple symmetric matrix: \\[ A = \\left[ \\begin{array}{rrrrr} a_{ii} &amp; a_{ij}\\\\ a_{ji} &amp; a_{jj} \\end{array} \\right] = \\left[ \\begin{array}{rrrrr} 9 &amp; 1\\\\ 1 &amp; 9 \\end{array} \\right] \\] First, we compute for the angle - \\(\\theta\\) - that we need to plug into our cos and sin: \\[\\begin{align*} tan 2\\theta {}&amp;= \\frac{2A_{ij}}{(A_{jj} - A_{ii})} = \\frac{2(1)}{9 - 9)} = \\infty \\\\ \\theta &amp;= \\frac{1}{2} arctan(\\infty) = -0.7853982 \\end{align*}\\] thus, we get: \\[\\begin{align*} cos (\\theta) {}&amp;= cos(-0.7853982) = 0.7071068 \\\\ sin (\\theta) &amp;= sin(-0.7853982) = -0.7071068 \\end{align*}\\] We then construct the Jacobi rotation matrix: \\[ J(i,j) = \\left[ \\begin{array}{rr} cos &amp; sin \\\\ -sin &amp; cos \\end{array} \\right] = \\left[ \\begin{array}{rrrrr} 0.7071608 &amp; -0.7071608 \\\\ 0.7071608 &amp; 0.7071608 \\end{array} \\right] \\] Then, using Jacobi rotation matrix, we transform matrix \\(A\\), into a diagonal matrix, \\(A&#39;\\): \\[\\begin{align*} A&#39; = J^TAJ {}&amp;= \\left[ \\begin{array}{rrrrr} cos &amp; -sin \\\\ sin &amp; cos \\end{array} \\right]_{J^T} \\left[ \\begin{array}{rrrrr} 9 &amp; 1 \\\\ 1 &amp; 9 \\end{array} \\right]_A \\left[ \\begin{array}{rrrrr} cos &amp; sin \\\\ -sin &amp; cos \\end{array} \\right]_J \\\\ &amp;= \\left[ \\begin{array}{rrrrr} 0.7071608 &amp; 0.7071608 \\\\ -0.7071608 &amp; 0.7071608 \\end{array} \\right] \\left[ \\begin{array}{rrrrr} 9 &amp; 1 \\\\ 1 &amp; 9 \\end{array} \\right] \\left[ \\begin{array}{rrrrr} 0.7071608 &amp; -0.7071608 \\\\ 0.7071608 &amp; 0.7071608 \\end{array} \\right] \\\\ A&#39; = D &amp;= \\left[ \\begin{array}{rrrrr} 10 &amp; 0 \\\\ 0 &amp; 8 \\end{array} \\right] \\end{align*}\\] In general, the algorithm is described simply by the following equation: \\[\\begin{align} A&#39; = J^TAJ\\ \\leftarrow \\text{iterate until}\\ A&#39; = D \\label{eqn:eqnnumber6} \\end{align}\\] We iterate until \\(A&#39;\\) transforms into a diagonal matrix. Here J is the Jacobi rotation matrix: \\[ J(i,j) = \\left[ \\begin{array}{rrrrr} 1 &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; c_{ii} &amp; . &amp; s_{ji} &amp; . \\\\ . &amp; . &amp; 1 &amp; . &amp; . \\\\ . &amp; -s_{ij} &amp; . &amp; c_{jj} &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 1 \\\\ \\end{array} \\right] \\] To construct the Jacobi rotation matrix, we need to identify or select a region in the matrix using \\(i\\) and \\(j\\) indices. Below are six choices we can make to choose a region from which to base our rotation matrix. \\[ \\begin{array}{l} \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; . &amp; .\\\\ \\square &amp; \\square &amp;. &amp; .\\\\ . &amp; . &amp; . &amp; .\\\\ . &amp; . &amp; . &amp; . \\end{array}\\right]_{i=1, j=2} \\left[\\begin{array}{ccccc} . &amp; . &amp; . &amp; .\\\\ . &amp; \\square &amp; \\square &amp; . \\\\ . &amp; \\square &amp; \\square &amp; . \\\\ . &amp; . &amp; . &amp; . \\end{array}\\right]_{i=2,j=2} \\left[\\begin{array}{ccccc} . &amp; . &amp; . &amp; .\\\\ . &amp; . &amp; . &amp; .\\\\ . &amp;. &amp; \\square &amp; \\square \\\\ . &amp; . &amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{i=3,j=4} \\\\ \\\\ \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; \\square &amp;.\\\\ \\square &amp; . &amp; \\square &amp; .\\\\ \\square &amp; \\square&amp; \\square &amp; . \\\\ . &amp; . &amp; . &amp; . \\end{array}\\right]_{i=1,j=3} \\left[\\begin{array}{ccccc} . &amp; . &amp; . &amp; .\\\\ .&amp; \\square &amp; \\square &amp; \\square \\\\ . &amp; \\square &amp; . &amp; \\square \\\\ . &amp; \\square &amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{i=2,j=4} \\left[\\begin{array}{ccccc} \\square &amp; \\square &amp; \\square &amp; \\square \\\\ \\square &amp; . &amp; . &amp; \\square\\\\ \\square &amp; . &amp; . &amp; \\square\\\\ \\square &amp; \\square&amp; \\square &amp; \\square \\\\ \\end{array}\\right]_{i=1,j=4} \\end{array} \\] For example, if we are to select a region with \\(i=1, j=3\\), then our Jacobi rotation matrix becomes: \\[ J(i=1,j=3) = \\left[ \\begin{array}{rrrrr} c_{ii} &amp; . &amp; s_{ji} &amp; . \\\\ . &amp; 1 &amp; . &amp; . \\\\ -s_{ij} &amp; . &amp; c_{jj} &amp; . \\\\ . &amp; . &amp; . &amp; 1 \\end{array} \\right]_{i=1, j=3} \\] The most simple selection is based on searching for a region with the largest off-diagonal entry: \\[ max\\{A\\} = max_{i&lt;j}\\{\\|A_{ij}\\|\\} \\] This allows for faster convergence. The stop is when the max is less or equal to a tolerance level (e.g., in our case \\(\\text{1e-5}\\)). Unfortunately, it becomes a challenge to perform a search against a matrix with extreme high-dimension. In that case, we can use the Cyclic Jacobi Method. We leave this method to the reader to investigate. In our illustration below, we use a simple selection algorithm for the region for the Jacobi rotation matrix. Now, assume a selected region with indices \\(i\\) and \\(j\\), our next step is then to compute for both \\(c = cos\\) and \\(s=sin\\): \\[\\begin{align*} tan 2\\theta {}&amp;= \\left(\\frac{2A_{ij}}{A_{jj}-A_{ii}}\\right) \\\\ \\theta &amp;= \\frac{1}{2} arctan\\left(\\frac{2A_{ij}}{A_{jj}-A_{ii}}\\right) \\\\ c &amp;= cos(\\theta) \\\\ s &amp;= sin(\\theta) \\\\ \\end{align*}\\] For a more stable solution, we can also perform trigonometric manipulation to compute for cos and sin given \\(\\theta\\): \\[\\begin{align*} \\\\ t {}&amp;= \\frac{sign(\\theta)}{|\\theta| + \\sqrt{\\theta^2 + 1}}\\ \\ \\ \\leftarrow \\ \\ \\ t^2 + 2\\theta t - 1 = 0\\\\ c &amp;= \\frac{1}{\\sqrt{t^2 + 1}} \\\\ s &amp;= ct \\end{align*}\\] From there, we construct the \\(J\\) matrix and use that to approximate a diagonal matrix using Equation \\(\\ref{eqn:eqnnumber6}\\) or perhaps to validate, we can use the following list of equations to construct \\(A&#39;\\): \\[\\begin{align} A&#39;_{ii} {}&amp;= c^2A_{ii} - 2csA_{ij} + s^2A_{jj} \\\\ A&#39;_{jj} &amp;= s^2A_{ii} + 2csA_{ij} + c^2A_{jj} \\\\ A&#39;_{ij} &amp;= A&#39;_{ji} = (c^2 - s^2)A_{ij} + cs(A_{ii} - A_{jj}) = 0\\\\ A&#39;_{ik} &amp;= A&#39;_{ki} = cA_{ki} - sA_{kj} \\rightarrow for\\ k\\ \\neq i\\ and\\ k \\neq j \\\\ A&#39;_{jk} &amp;= A&#39;_{kj} = sA_{ki} + cA_{kj} \\rightarrow for\\ k\\ \\neq i\\ and\\ k \\neq j \\end{align}\\] Note that the diagonal form may not readily be apparent the first time we compute for \\(A&#39;\\). Hence, it may take a few iterations to get an approximation of a diagonal form in which the entries are the Eigenvalues of the symmetric matrix. For example: \\[\\begin{align*} A_1 {}&amp;= J^T_0A_0J_0 \\rightarrow choose\\ i,j\\ to\\ construct\\ J_0\\\\ A_2 &amp;= J_1^TA_1J_1 \\rightarrow choose\\ i,j\\ to\\ construct\\ J_1\\\\ &amp;\\vdots \\\\ D &amp;= J_k^TA_kJ_k \\rightarrow choose\\ i,j\\ to\\ construct\\ J_k \\end{align*}\\] After every step in the Jacobi iteration method, the off-diagonal elements, \\(A&#39;_{ij}\\) and \\(A&#39;_{ji}\\), are annihilated (turn into zeros). We illustrate using the following symmetric matrix: \\[ A = \\left[ \\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 9 &amp; 6 &amp; 3 \\\\ 3 &amp; 6 &amp; 9 &amp; 2 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{array} \\right] \\] For the first Iteration, let us formulate our first \\(J_1\\) matrix and \\(A_1\\) matrix. We see six being a max off-diagonal entry in \\(A_{i=2,j=3}\\). The corresponding Jacobi rotation matrix is: \\[ J_1 = \\left[ \\begin{array}{rrrr} 1 &amp; . &amp; . &amp; . \\\\ . &amp; 0.7071068 &amp; 0.7071068 &amp; . \\\\ . &amp; -0.7071068 &amp; 0.7071068 &amp; . \\\\ . &amp; . &amp; . &amp; 1 \\end{array} \\right] \\] We end up with matrix \\(A_1\\): \\[ A_1 = J_1^TAJ_1 = \\left[ \\begin{array}{rrrr} 1.0000000 &amp; -7.071068e-01 &amp; 3.535534e+00 &amp; 4.0000000 \\\\ -0.7071068 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 0.7071068 \\\\ 3.5355339 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 3.5355339 \\\\ 4.0000000 &amp; 7.071068e-01 &amp; 3.535534e+00 &amp; 1.0000000 \\end{array} \\right] \\] For the second Iteration, we construct our next \\(J_2\\) matrix and \\(A_2\\) matrix. We see four being a max off-diagonal entry in \\(A_{i=1,j=4}\\). The corresponding Jacobi rotation matrix is: \\[ J_2 = \\left[ \\begin{array}{rrrr} 0.7071068 &amp; . &amp; . &amp; 0.7071068 \\\\ . &amp; 1 &amp; . &amp; . \\\\ . &amp; . &amp; 1 &amp; . \\\\ -0.7071068 &amp; . &amp; . &amp; 0.7071068 \\end{array} \\right] \\] We end up with matrix \\(A_2\\): \\[ A_2 = J_2^TA_1J_2 = \\left[ \\begin{array}{rrrr} -3.000000e+00 &amp; -1.000000e+00 &amp; 4.440892e-16 &amp; 4.440892e-16 \\\\ -1.000000e+00 &amp; 3.000000e+00 &amp; 1.110223e-15 &amp; 5.551115e-16 \\\\ 4.440892e-16 &amp; 8.881784e-16 &amp; 1.500000e+01 &amp; 5.000000e+00 \\\\ 8.881784e-16 &amp; 5.551115e-16 &amp; 5.000000e+00 &amp; 5.000000e+00 \\end{array} \\right] \\] We continue until we hit the fifth iteration. The stop is when the search for max off-diagonal entry ends at zero. We end up with the final matrix \\(A_5\\): \\[ A_5 = J_5^TA_4J_5 = \\left[ \\begin{array}{rrrr} -3.162278 &amp; . &amp; . &amp; . \\\\ . &amp; 3.162278 &amp; . &amp; . \\\\ . &amp; . &amp; 17.07107 &amp; . \\\\ . &amp; . &amp; . &amp; 2.928932 \\end{array} \\right] \\] Hence, the approximate Eigenvalues are (sorted in descending order): \\[ \\tilde \\lambda_1=17.07107 \\ \\ \\ \\ \\tilde \\lambda_2=3.162278\\ \\ \\ \\ \\ \\tilde \\lambda_3=2.928932\\ \\ \\ \\ \\ \\tilde \\lambda_4=-3.162278 \\] It is notable to mention that there are applications of the algorithm in which we may not require to seek all of the eigenvalues. For example, suppose a matrix is extremely large. It may be enough to get the first few of them using a different stopping criterion versus just evaluating a max off-diagonal entry. Finally, we illustrate a naive implementation of the Jacobi Eigenvalue Method algorithm in R code: offdiagonal_maxsearch &lt;- function(A, tol) { n = ncol(A) m = ncol(A) max = tol index = c(0,0) for (i in 1:n) { for (j in 1:m) { if (i != j) { if ( max &lt; abs( A[i,j]) ) { if (i &lt; j) { max = abs( A[i,j] ) index = c(i,j) } } } } } i = min(index) j = max(index) return( list(&quot;max&quot;=max, &quot;i&quot;=i, &quot;j&quot;=j) ) } rotation_matrix &lt;- function(i,j, A) { R = diag(ncol(A)) theta = 1/2 * atan( 2 * A[i,j] / ( A[j,j] - A[i,i])) cos = cos(theta) sin = sin(theta) R[i,i] = cos; R[i,j] = sin; R[j,i] = -sin; R[j,j] = cos list(&quot;J&quot;=R, &quot;c&quot;=cos, &quot;s&quot;=sin) } jacobi_eigenvalue_method &lt;-function(A) { iterate = 0 tol = 1e-5 limit=100 for (k in 1:limit) { S = offdiagonal_maxsearch(A, tol) if (S$max &lt;= tol) break R = rotation_matrix(S$i,S$j, A) A = t(R$J) %*% A %*% R$J iterate = k } list(&quot;matrix&quot;=A, &quot;iterate&quot;=k, &quot;eigenvalues&quot;=sort(diag(A), decreasing=TRUE)) } A = matrix(c(1,2,3,4, 2,9,6,3, 3,6,9,2, 4,3,2,1), 4, byrow=TRUE) jacobi_eigenvalue_method(A) ## $matrix ## [,1] [,2] [,3] [,4] ## [1,] -3.162278e+00 0.000000e+00 7.710670e-16 2.513307e-16 ## [2,] 0.000000e+00 3.162278e+00 1.129214e-15 4.835791e-17 ## [3,] 9.059580e-16 8.994983e-16 1.707107e+01 -8.881784e-16 ## [4,] 6.699290e-16 6.651312e-17 0.000000e+00 2.928932e+00 ## ## $iterate ## [1] 5 ## ## $eigenvalues ## [1] 17.071068 3.162278 2.928932 -3.162278 We validate the result using eigen(.) function like so: eigen(A)$values ## [1] 17.071068 3.162278 2.928932 -3.162278 3.2.6 Arnoldi Method (using Gram-Schmidt in Krylov Subspace) Arnoldi Method is one of the orthogonal projection methods of finding the approximation of both Eigenvectors and Eigenvalues. To understand the idea behind this method, let us discuss what we are projecting orthogonally using Figure 3.1. The left side portion of the figure covers generalized minimal residual (GMRES) and Conjugate Gradient(CG) for solving systems of linear equations. But here, our discussion is about the right side portion, which covers solving for Eigenvalue problems (Heath M.T. 2002; Sleijpen G. 2014). Figure 3.1: Orthogonal Projection unto Krylov Space Recall the Eigen equation (\\(A \\cdotp \\mathbf{\\vec{v}} = \\lambda \\times \\mathbf{\\vec{v}}\\)) under Section Eigenvectors and Eigenvalues in Chapter 2 (Numerical Linear Algebra I). Here, we emphasis the actual Eigenvectors denoted as v and Eigenvalues, \\(\\lambda\\). Equivalently, we can designate a notation denoting approximation of Eigenvectors and Eigenvalues: \\[\\begin{align} A \\tilde u = \\tilde \\lambda \\tilde u \\end{align}\\] Then we can find a subspace, K, into which we can project \\(A \\tilde u\\) such that the equation becomes: \\[\\begin{align} Proj_{(K)}(A \\tilde u)\\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ (A \\tilde u - \\tilde \\lambda \\tilde u) \\perp K \\end{align}\\] Here, we introduce the Krylov subspace , K, such that \\((A \\tilde u - \\tilde \\lambda \\tilde u)\\) is orthogonal to K. For more intuition about Krylov subspace, it may also help to review Section Krylov Method. Given a system of linear equations in matrix form, \\(Ax = b\\), the Krylov subspace is a subspace spanned by {Ab + AAb + AAAb + … + AAA…Ab} , which can be expressed this way (m-th order of Krylov sequence): \\[\\begin{align} K_m(A,b) = span\\ \\{b,\\ Ab,\\ A^2b,\\ A^3b,\\ ...,\\ A^{m-1}b\\} \\end{align}\\] In the same manner, in this particular case for eigenvalue problems, Krylov subspace is spanned by a linear combination like so (using an orthogonal basis, Q): \\[\\begin{align} K_m(A,q) = span\\ \\{ q_1,\\ Aq_2,\\ A^2q_3, ...,\\ A^{m-1}q_m \\} \\end{align}\\] We derive a change of basis called the Arnoldi basis from the subspace, K. And we let this basis be an orthogonal matrix denoted as Q: \\[\\begin{align} Q_{basis} = \\{ \\ q_1,\\ q_2,\\ q_3, ...,\\ q_{m}\\ \\} \\end{align}\\] See Krylov Methods and GMRES in a few sections ahead, covering an example of the construction of the Q basis for the K space. The basis is derived by Arnoldi Method, decomposing an \\(n \\times n\\) square matrix, A, into the form \\(QHQ^T\\). The algorithm also shows the combined \\(AQ\\) equal to a combined upper Hessenberg matrix, H, and a basis matrix, Q, expressed as: \\[\\begin{align} AQ_m = Q_{m+1}H_m \\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ (AQ - QH) \\perp K \\end{align}\\] We show the equation in matrix forms: \\[ \\left[ \\begin{array}{rrrrr} &amp; &amp; \\\\ &amp; &amp; \\\\ &amp; A_{mxm} &amp; \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{array} \\right] \\left[ \\begin{array}{rrrr} &amp; &amp; \\\\ &amp; &amp; \\\\ q1 &amp; ... &amp; q_m \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{array} \\right] = \\left[ \\begin{array}{rrrr} &amp; &amp; \\\\ &amp; &amp; \\\\ q1 &amp; ... &amp; q_{m+1} \\\\ &amp; &amp; \\\\ &amp; &amp; \\\\ \\end{array} \\right] \\left[ \\begin{array}{llll} h_{1,1} &amp; h_{1,2} &amp; \\ldots &amp; h_{1,m} \\\\ \\alpha_{2,1} &amp; h_{2,2} &amp; \\ldots &amp; h_{2,m} \\\\ . &amp;\\alpha_{3,2} &amp; \\ldots &amp; h_{3,m} \\\\ . &amp; . &amp; \\ddots &amp; \\vdots \\\\ . &amp; . &amp; . &amp; \\alpha_{m+1,m} \\\\ \\end{array} \\right] \\] Based on the equation, we can derive the equation for any column. For example, for the 2nd column and 3rd columns, we can use the following equations to compute for \\(Aq_2\\) and \\(Aq_3\\) respectively: \\[ Aq_2 = h_{1,2}q_1 + h_{2,2}q_2 + \\alpha_{3,2}q_3\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Aq_3 = h_{1,3}q_1 + h_{2,3}q_2 + h_{3,3}q_3 + \\alpha_{4,3}q_4 \\] where \\(\\alpha_{j+1,j} = \\| Aq_j - \\sum_k(h_{k,j}q_j) \\|_{L2}\\). On the other hand, using the Arnoldi basis, Q, we can also generate the upper Hessenberg matrix like so: \\[\\begin{align} H_m = Q_{m}^TAQ_m \\label{eqn:eqnnumber7} \\end{align}\\] This can be interpreted as H being the projection of A onto K. Now, given \\(Q\\) and the approximation of Eigenvalues and Eigenvectors, we can perform a mathematical conversion: \\[\\begin{align} A \\tilde u = \\tilde \\lambda \\tilde u\\ \\rightarrow\\ \\ \\ \\ Q^TAQ \\tilde u = \\lambda Q^TQ \\tilde u \\rightarrow\\ \\ \\ \\ H \\tilde u = \\tilde \\lambda \\tilde u \\end{align}\\] This pair \\(\\tilde \\lambda \\tilde u\\) is called the Ritz pair. We can find the Ritz value, \\(\\tilde \\lambda\\), and Ritz vector, \\(\\tilde u\\), using QR Method to solve the equation below (recall the equation \\((A - \\lambda I) \\cdotp v = 0\\) under Section Eigenvectors and Eigenvalues in Chapter 2 (Numerical Linear Algebra I)): \\[ (H - \\tilde \\lambda I)\\tilde u = 0 \\] That said, let us now review the following Arnoldi Method algorithm: \\[ \\begin{array}{l} v \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ q_1 = v / \\|v\\|_{L2}\\\\ loop\\ j\\ in\\ 1:m \\\\ \\ \\ \\ \\ v = Aq_j\\\\ \\ \\ \\ \\ loop\\ k\\ in\\ 1:j \\rightarrow \\text{Gram-Schmidt} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ h_{kj} = q_k^T v\\\\ \\ \\ \\ \\ \\ \\ \\ \\ v = v - h_{kj}q_k \\\\ \\ \\ \\ \\ end\\ loop \\\\ \\ \\ \\ \\ h_{j+1,j} = \\|v\\|_{L2}\\\\ \\ \\ \\ \\ if\\ (h_{j+1,j} &lt; tol\\ )\\ break \\\\ \\ \\ \\ \\ q_{j+1} = v / h_{j+1,j}\\\\ end\\ loop \\end{array} \\] We follow this with a naive implementation of Arnoldi Method in R code: arnoldi_method &lt;- function(A, v) { n = ncol(A) m = nrow(A) q = matrix(rep(0, n*(m+1)), n, byrow=TRUE ) # Q n x n+1 h = matrix(rep(0, (m+1)*m), (m+1), byrow=TRUE) # H n+1 x n q[,1] = v / sqrt(sum(v^2)) tol = 1e-5 for (j in 1:m) { v = A %*% q[,j] for (k in 1:j) { # Gram-Schmidt Orthogonalization h[k,j] = t(q[,k]) %*% v v = v - h[k,j] * q[,k] # subtracting projection } h[j+1,j] = sqrt(sum(v^2)) if (abs( h[j+1,j]) &lt; tol) break q[,j+1] = v / h[j+1,j] } h_m = h[1:m,1:m]; q_m = q[1:m,1:m] list(&quot;Q&quot; = q_m , &quot;H&quot; = h_m, &quot;AQ&quot; = A %*% q_m, &quot;QH&quot; = q %*% h, &quot;H=QtAQ&quot; = t(q_m) %*% A %*% q_m, &quot;A=QHQt&quot;= q_m %*% h_m %*% t(q_m) ) } A = matrix(c(3,3,3, 2,4,5, 1,5,5), 3, byrow=TRUE) b=c(6,5,6) (arnoldi = arnoldi_method(A, b)) ## $Q ## [,1] [,2] [,3] ## [1,] 0.6092077 -0.65018500 -0.4540104 ## [2,] 0.5076731 0.75958120 -0.4065765 ## [3,] 0.6092077 0.01720066 0.7928241 ## ## $H ## [,1] [,2] [,3] ## [1,] 10.123711 3.127356 1.5019617 ## [2,] 1.521378 1.194132 1.2436289 ## [3,] 0.000000 1.649767 0.6821563 ## ## $AQ ## [,1] [,2] [,3] ## [1,] 5.178265 0.3797906 -0.2032882 ## [2,] 6.295146 1.8239581 1.4297940 ## [3,] 6.193612 3.2337243 1.4772279 ## ## $QH ## [,1] [,2] [,3] ## [1,] 5.178265 0.3797906 -0.2032882 ## [2,] 6.295146 1.8239581 1.4297940 ## [3,] 6.193612 3.2337243 1.4772279 ## ## $`H=QtAQ` ## [,1] [,2] [,3] ## [1,] 1.012371e+01 3.127356 1.5019617 ## [2,] 1.521378e+00 1.194132 1.2436289 ## [3,] 3.941292e-15 1.649767 0.6821563 ## ## $`A=QHQt` ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 (eigenvalues = qr_method(arnoldi$H)$eigenvalues) ## [1] 10.6771883 1.9107948 -0.5881801 We validate the result using eigen(.) function like so: eigen(A)$values ## [1] 10.6771903 1.9109438 -0.5881341 We can easily validate using the following equations (See also Equation \\(\\ref{eqn:eqnnumber7}\\)) below: \\[\\begin{align} H = Q^TAQ\\ \\ \\ \\ \\ \\ \\ \\ A=QHQ^T \\end{align}\\] It is notable to mention that the Arnoldi Method tends to converge towards the largest Eigenvector similar to that of the Power Method and so is ill-conditioned. To work around that, Arnoldi Method uses Gram Schmidt orthogonalization, resulting in a more stable convergence, arising in a better conditioned Hessenberg matrix. Other literature may illustrate using other orthogonalization methods in place of Gram-Schmidt orthogonalization. 3.2.7 Lanczos Method (using Gram-Schmidt in Krylov Subspace) The Lanczos Method method (Lanczos, C. 1950) is useful for Symmetric Positive Definite (SPD) matrices and Tridiagonal Hermitian types of matrices, including highly sparse matrices. This method is also an orthogonal projection method similar to Arnoldi Method but it handles Hermitian matrices. We illustrate the method by starting with a Tridiagonal Hermitian matrix with the (super/sub)diagonal entries denoted by \\(\\beta\\), and the main diagonal entries denoted by \\(\\alpha\\). \\[ A = \\left[ \\begin{array}{rrrrr} \\alpha_1 &amp; \\beta_1 &amp; . &amp; . &amp; . \\\\ \\beta_1 &amp; \\alpha_2 &amp; \\beta_2 &amp; . &amp; . \\\\ . &amp; \\beta_2 &amp; \\alpha_3 &amp; \\ddots &amp; . \\\\ . &amp; . &amp; \\ddots &amp; \\ddots &amp; \\beta_{j-1} \\\\ . &amp; . &amp; . &amp; \\beta_{j-1} &amp; \\alpha_j \\\\ \\end{array} \\right] \\] Now recall the equation below from Arnoldi method: \\[\\begin{align} AQ_m = Q_{m+1}H_m \\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ (AQ - QH) \\perp K \\end{align}\\] In Lanczos method, because we deal with SPD matrix, we have the following equation instead: \\[\\begin{align} AQ_m = Q_{m}T_m \\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ (AQ - QT) \\perp K \\end{align}\\] where \\(T\\) is an SPD Tridiagonal matrix. Because we deal with a symmetric tridiagonal matrix, \\(T\\), and an orthogonal matrix \\(Q^TQ = I\\), we can therefore use the 3-term recurrence (Parlett B. N. 1994): \\[\\begin{align*} Aq_j {}&amp;= \\beta_{(j-1)}q_{(j-1)} + \\alpha_{(j)} q_{(j)} + \\beta_{(j)} q_{(j+1)} \\end{align*}\\] Here follows the Lanczos Method algorithm: \\[ \\begin{array}{l} \\mathbf{\\text{Standard Lanczos}}\\\\ \\\\ \\beta_0 = 0, q_0 = 0 \\leftarrow \\text{initial zero } \\\\ b \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ q_1 = b / \\|b\\|_{L2}\\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ r = Aq_j \\\\ \\ \\ \\ \\ \\alpha_j = q_j^T r \\\\ \\ \\ \\ \\ r = r - \\alpha_j q_j - \\beta_{(j)}q_{(j-1)} \\\\ \\ \\ \\ \\ \\beta_j = \\|r\\|_{L2} \\\\ \\ \\ \\ \\ if\\ (\\beta_j &lt; tol)\\ break \\\\ \\ \\ \\ \\ q_{j+1} = r / \\beta_j\\\\ end\\ loop \\end{array} \\left| \\begin{array}{l} \\mathbf{\\text{Chris Paige Proposal}}\\\\ \\\\ \\beta_0 = 0, q_0 = 0 \\leftarrow \\text{initial zero } \\\\ b \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ q_1 = b / \\|b\\|_{L2}\\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ r = Aq_j - \\beta_{(j-1)}q_{(j-1)}\\\\ \\ \\ \\ \\ \\alpha_j = q_j^T r \\\\ \\ \\ \\ \\ r = r - \\alpha_j q_j \\\\ \\ \\ \\ \\ \\beta_j = \\|r\\|_{L2} \\\\ \\ \\ \\ \\ if\\ (\\beta_j &lt; tol)\\ break \\\\ \\ \\ \\ \\ q_{j+1} = r / \\beta_j\\\\ end\\ loop \\end{array} \\right. \\] Note that, at each iteration in the vanilla Lanczos method, the equation \\(r = Aq_j\\) creates a new basis vector and the equation \\(r = r - \\alpha_j q_j - \\beta_{(j)}q_{(j-1)}\\) subtracts the new vector from its projection. The subtraction is a remedy for the loss of orthogonality for the Lanczos Method infinite precision (Hoppe T., n.d.; Hoppe T., n.d.; Qianqian Yang 2019) To illustrate, let us use our naive implementation of the Lanczos Method algorithm based on Paige’s Proposal (1975): set.seed(1) lanczos_method &lt;- function(A) { n = ncol(A) m = nrow(A) q = matrix(rep(0, m*n), m, byrow=TRUE ) # Sparse Matrix for Q beta = rep(0, n) alpha = rep(0, n) b = rnorm(n) # initialize randomly q[,1] = b / sqrt(sum(b^2)) # normalize tol = 1e-5 prev.beta = 0 prev.q = q[,1] for (j in 1:(n)) { r = A %*% q[,j] - prev.beta * prev.q alpha[j] = t(q[,j]) %*% r r = r - alpha[j] * q[,j] # subtract from its projection beta[j] = sqrt(sum(r^2)) if (j &gt;=n || abs(beta[j]) &lt; tol) break q[,j+1] = r / beta[j] # then scale prev.beta = beta[j] prev.q = q[,j] } T = t(q) %*% A %*% q T[which(abs(T) &lt; tol) ] = 0 list(&quot;matrix&quot;= A , &quot;T&quot;= T, &quot;Q&quot; = q , &quot;beta&quot; = beta, &quot;alpha&quot; = alpha ) } A = matrix(c(1,2,0,0,0,2,3,3,0,0,0,3,3,4,0,0,0,4,4,5, 0,0,0,5,5),5, byrow=TRUE) (lanczos = lanczos_method(A)) ## $matrix ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 0 0 0 ## [2,] 2 3 3 0 0 ## [3,] 0 3 3 4 0 ## [4,] 0 0 4 4 5 ## [5,] 0 0 0 5 5 ## ## $T ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.726379 5.939271 0.000000 0.0000000 0.000000 ## [2,] 5.939271 5.223253 2.542124 0.0000000 0.000000 ## [3,] 0.000000 2.542124 6.710107 1.2006169 0.000000 ## [4,] 0.000000 0.000000 1.200617 0.8125495 1.296137 ## [5,] 0.000000 0.000000 0.000000 1.2961366 1.527711 ## ## $Q ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.32230088 0.07123367 0.3943700 -0.85660737 -0.04176164 ## [2,] 0.09448167 -0.30542846 0.7022612 0.28996500 -0.56615912 ## [3,] -0.42991811 0.50829179 0.4628082 0.39608594 0.43096916 ## [4,] 0.82074751 0.16736559 0.3444932 -0.15551579 0.39433690 ## [5,] 0.16952670 0.78438985 -0.1357740 -0.03278521 -0.58007225 ## ## $beta ## [1] 5.939271e+00 2.542124e+00 1.200617e+00 1.296137e+00 4.913656e-14 ## ## $alpha ## [1] 1.7263794 5.2232530 6.7101074 0.8125495 1.5277108 (eigenvalues = qr_method(lanczos$T)$eigenvalues) ## [1] 10.7729509 6.1028726 2.9759549 2.3984001 0.2983397 We validate the result (in decreasing order using) eigen(.) function like so: sort(abs(eigen(lanczos$T)$values), decreasing=TRUE) ## [1] 10.7729806 6.1028984 2.9759370 2.3983977 0.2983397 We can easily use \\(Q\\) to construct a tridiagonal symmetric matrix: \\[\\begin{align} T = Q^TAQ \\end{align}\\] We can derive a tridiagonal symmetric matrix, \\(T = Q^TAQ\\), from \\(Q\\) to then find the approximate Eigenvalues, and Eigenvectors - the Ritz pair. Like Arnoldi Method, we use QR Method to generate the Eigenvalues but this time using the tridiagonal symmetric matrix derived from Lanczos Method. Given \\(Q = (q_1, q_2, ..., q_j)\\), Laczos basis, where \\(Q\\) is an orthogonal matrix derived from Lanczos Method, we can then perform the following equations. \\[\\begin{align} Av = \\lambda v\\ \\rightarrow\\ \\ \\ \\ Q^TAQv = \\lambda Q^TQv \\rightarrow\\ \\ \\ \\ Tv = \\lambda v \\end{align}\\] Improvements of the Lanczos method have evolved through the years. We leave readers to investigate Lanczos algorithm for SVD and Randomized Block Lanczos (Yuan Q. et al 2018). Other recent methods such as Jacobi-Davidson Method uses Ritz-Galerkin procedure instead of Krylov subspace. And like Power Method, the Jacobi-Davidson method converges to the largest Eigenvalue. 3.2.8 Fine-Tuning of Iteration and Convergence Before we close the topic around iteration and convergence for Eigenvalue problems, it is notable, for further reading, to introduce procedures that are essential for a more stable and faster convergence. Restarting Restarting is simply just restarting the iteration in the middle of it after a certain threshold - usually when we reach a predetermined set of vectors. This allows us to use a new set of initial vectors at restart. Shifting Shifts are commonly seen in Iteration methods in the form of: \\[\\begin{align} (A - \\alpha I) \\end{align}\\] where \\(\\alpha\\) shifts a matrix. We use shift in Eigenvalue problems to get an approximate Eigenvalue much closer to one actual Eigenvalue than to another in the case in which a matrix has multiple (interior) Eigenvalues. While we have not covered shifting, the methods we previously discussed can implement shifting, e.g. Inverse Power Method with shift, QR Method with shift, etc. Preconditioning Preconditioning transforms a system of equations to a new form as a way to reduce or eliminate the system from being ill-conditioned. A common equation with preconditioner is applied on systems of equations where \\(M^{-1}\\) denotes a preconditioner: \\[\\begin{align} Ax = b \\rightarrow M^{-1}Ax = M^{-1}b \\end{align}\\] There are three ways to use preconditioners: Left preconditioning \\[\\begin{align} Ax = b \\rightarrow M^{-1}Ax = M^{-1}b \\end{align}\\] Right preconditioning \\[\\begin{align} Ax = b \\rightarrow AM^{-1}u = b \\ \\ \\ where\\ u = Mx \\end{align}\\] Split or Symmetric preconditioning \\[\\begin{align} Ax = b \\rightarrow M^{-1}AM^{-1}u = M^{-1}b \\ \\ \\ where\\ u = Mx \\end{align}\\] A simple preconditioner is in the form of diagonal of a matrix: e.g. \\[\\begin{align} M^{-1} = diag(A) \\end{align}\\] We leave readers to investigate other preconditioners. We also leave readers to investigate Deflation and Augmentation, which are techniques used to optimize convergence (in the context of Krylov subspace). For Deflation, we have Hotelling’s Deflation and Wielandt deflation to be familiar. 3.3 Approximating Root and Fixed-Point by Iteration We have covered iteration and convergence for Eigenvalue problems to this section. We now change course away from solving for the Eigenvalues and instead, move to a course that starts with Root-Finding problems and Fixed-Point problems using iterative methods of approximating the intersection of a curve. Afterwhich, we will cover iterative methods of solving systems of equations. 3.3.1 Root-Finding Method (\\(f(x) = 0\\)) Before we extend the idea of solving systems of equations for vector \\(\\mathbf{\\vec{x}}\\), let us first have a quick overview of Root-Finding and Fixed-Point finding, which can be used to approximate a root. There is a subtle difference between finding a Fixed Point and Root-Finding. See Figure 3.2. Figure 3.2: Root Finding vs Fixed-Point In Root-Finding Method, we are solving for x using the following equation (Driscoll T. A. 2020): \\[ f(x) = 0\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ y = 0 \\] This is solving for x to find the zeroes of function f. In Fixed-point Method, we are solving for x using the following equation (Driscoll T. A. 2020): \\[ f(x) = x\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ y = x \\] We illustrate the Fixed-Point method in the next section. Here, for Root-Finding, consider a third-degree polynomial equation where \\(f(x)=0\\): \\[\\begin{align*} f(x) = \\frac{1}{2}x^3 - x = 0\\\\ \\end{align*}\\] Solving for x, we get three roots (three solutions): \\[ x =0,\\ \\ \\ \\ \\ x = - \\sqrt{2},\\ \\ \\ \\ \\ \\ \\ x = \\sqrt{2} \\] We show iterative method in solving for roots next. 3.3.2 Fixed-Point Method (\\(f(x) = x\\)) Fixed-point Method solves for x iteratively where x is the Fixed-Point output of a function that is equal to its input; expresssed as \\(f(x) = x\\). Here, we also can interpret it this way: \\[ f(x) = y = x \\] The method iterates until convergence: \\[\\begin{align} x_{k+1} = g(x_k)\\ \\ \\ \\ \\leftarrow\\ \\ \\text{iterates until } \\mathbf{ab}s(x_k - x_{k+1}) &lt; tol \\end{align}\\] To use the Fixed-Point method for Root-Finding, we need to convert the equation, \\(f(x)=0\\) into \\((fx)=x\\). To do that, suppose we have the following equation \\[ f(x) = f(x) = x^3 + x^2 - 1 = y,\\ \\ \\ \\ \\ where\\ y=0 \\] We need to assume y = x Therefore: \\[ f(x) = x^3 + x^2 - 1 = x\\ \\ \\ \\ \\ \\leftarrow \\text{root-find form} \\] We now implement the fixed-point iteration in R code. Let us first construct an helper function for plotting the graphs: plot_par &lt;- function(f) { par(pty=&quot;m&quot;) xl = -1; yl = 4 plot(NULL, xlim=range(xl,yl), ylim=range(xl,yl), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(c(xl,yl), c(xl,yl), col=&quot;darksalmon&quot;, lwd=2) curve( f(x), xl, yl, col=&quot;navyblue&quot;, add=TRUE, lwd=2 ) points(c(0,5),c(0,5), col=&quot;navyblue&quot;, pch=16, lwd=2) lines(c(4,4), c(-1,3.2), col=&quot;red&quot;, lty=3) x_loc = c(3.5, -0.5, 0.15, 1.2 ) y_loc = c(0, 0.25, -0.2, 1.5 ) labels= c(&quot;x=4 (initial)&quot;, &quot;f(x) = .2x^2&quot;, &quot;(x, g(x))&quot;, &quot;y=x&quot;) text(x_loc, y_loc, labels=labels, offset=0.5, col=&quot;darkgreen&quot; ) } Here is a naive implementation of fixed-point method in R code: polynomial &lt;- function(x) { 0.2*x^2 } fixed_point &lt;- function(x) { limit=100 tol = 1e-5 g = polynomial x_ = g(x) for (n in 1:limit) { plot_iteration(x, x_, n) x_ = x x = g(x) # fixed-point formula if (abs(x - x_) &lt; tol) break } list(&quot;x&quot;=round(x_), &quot;y&quot;=round(x)) } plot_iteration &lt;- function(x,y,n) { g = polynomial if (n&gt;1) { lines(c(x,x), c(x, g(x)), col=&quot;brown&quot; ) lines(c(x,y), c(x, g(y)), col=&quot;brown&quot; ) } } plot_par(polynomial) root = fixed_point(4) Figure 3.3: Fixed-Point Iteration 3.3.3 Bisection Method Bisection Method is a classic way of solving for the root of a continuous function. It iterates until convergence using the following equation (Ehiwario J.C. 2014). \\[\\begin{align} x_{k+1} = \\frac{a_k + b_k}{2} \\end{align}\\] If \\(abs(x_{k+1})\\) &lt; tolerance level, then we found the root. However, if \\(sign(x_{i+1}) = sign(a_k)\\), then we replace \\(a_k\\) with \\(x_{k+1}\\). And if \\(sign(x_{i+1}) = sign(b_k)\\), then we replace \\(b_k\\) with \\(x_{k+1}\\). We then repeat evaluating the equation to find the root or stop if we reach the tolerance level. Here is a naive implementation of Bisection Method in R code: f &lt;- function(x) { tanh(x*pi/sqrt(4)) } bisection &lt;- function(a, b) { limit=20 tol = 1e-3 points(c(a,b), c(f(a),f(b)), col=&quot;brown&quot;, pch=16) text(c(a,b), c(f(a) + 0.1, f(b) - 0.1), label=c(a,b), col=&quot;black&quot;, pch=16) f.x = NULL for (i in 1:limit) { x = ( a + b ) / 2 y = f(x) if (abs(y) &lt; tol) { points(c(x), c(y), col=&quot;black&quot;, pch=16) return(x) } if (sign(a) == sign(x)) { a = x } else { b = x } points(c(x), c(y), col=&quot;darksalmon&quot;, pch=16) } } plot(NULL, xlim=range(-2:2), ylim=range(-1:1), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Bisection Iteration&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;red&quot;, lty=3) curve( f(x), col=&quot;navyblue&quot;, add=TRUE, lwd=2 ) (root = bisection(-1.9, 1.5)) ## [1] 4.882813e-05 Figure 3.4: Bisection Iteration 3.3.4 Newton-Raphson Method (using the Tangent Line) The Newton-Raphson method iterates until convergence using the following equation (Ypma T. J. 1995). \\[\\begin{align} x_{k+1} = x_k - \\frac{f(x_k)}{f&#39;(x_k)} \\end{align}\\] The equation is derived by approximation using first-order Taylor Series given a differentiable function, \\(f(x) = 0\\) (note that we can also use \\(f(x) \\approx 0\\) since here we are approximating the root): \\[\\begin{align} f(x) \\approx f(x_k) + f&#39;(x_k)(x_{k+1} - x_k) = 0 \\end{align}\\] This iterative method uses a tangent line to the curve, and where the tangent line intersects, the x-axis becomes the next x. Here is a naive implementation of Newton-Raphson in R code: polynomial &lt;- function(x) { 0.2*x^2 } dpolynomial &lt;- function(x) { 0.4 * x } newton_raphson &lt;- function(x) { limit=100 tol = 1e-5 g = polynomial dg = dpolynomial x_ = x - g(x)/dg(x) for (n in 1:limit) { plot_iteration(x_, x, n) x = x_ x_ = x - g(x)/dg(x) # Newton-Raphson formula if (abs(x - x_) &lt; tol) break } list(&quot;x&quot;=round(x_), &quot;y&quot;=round(x)) } plot_iteration &lt;- function(x_, x, n) { f = polynomial slope = dpolynomial(x) y1 = slope * ( x_ ) lines(c(x_, x), c(0, y1), col=&quot;brown&quot;) points(x ,f(x), col=&quot;brown&quot;, pch=16, lwd=2) lines(c(x_, x_), c(0, f(x_)), col=&quot;brown&quot;, lty=3) } plot_par(polynomial) text(2, -0.1, labels=&quot;tangent&quot;, offset=0.5, col=&quot;darkgreen&quot;) root = newton_raphson(4) Figure 3.5: Newton-Raphson Iteration 3.3.5 Secant Method (using the Secant Line) The Secant method iterates until convergence using the following equation (Ehiwario J.C. 2014). \\[\\begin{align} x_{k+1} = x_k - \\frac{f(x_k)(x_k - x_{k-1})}{f(x_k)-f(x_{k-1})} \\ \\ \\ \\leftarrow \\ \\ \\ \\ \\ f&#39;(x) \\approx \\frac{ f(x) - f(x+h)}{ x - (x + h)} \\end{align}\\] This iterative method is similar to Newton method, but here it uses the secant line instead, and where the secant line intersects, the x-axis becomes the next x. Here is a naive implementation of the secant method in R code: polynomial &lt;- function(x) { 0.2*x^2 } g &lt;- function(x, x_) { f = polynomial gx_ = x - ( f(x) * (x - x_) ) / ( f(x) - f(x_)) # secant formula list(&quot;x&quot;=gx_, &quot;y&quot;=0, &quot;x_&quot;=x, &quot;y_&quot;=f(x), &quot;ox_&quot;=x_, &quot;oy_&quot;=f(x_)) } secant &lt;- function(x,x_) { limit=100 tol = 1e-2 g_ = g(x, x_) for (n in 1:limit) { plot_iteration(g_, n) g_ = g(g_$x, g_$x_) if (abs(g_$x - g_$x_) &lt; tol) break } list(&quot;x&quot;=round(g_$x), &quot;y&quot;=round(g_$y)) } plot_iteration &lt;- function(g_,n) { lines(c(g_$x, g_$ox_), c(g_$y,g_$oy_ ), col=&quot;brown&quot;) points(g_$ox_,g_$oy_, col=&quot;brown&quot;, pch=16, lwd=2) lines(c(g_$x, g_$x), c(0, polynomial(g_$x)), col=&quot;brown&quot;, lty=3) } plot_par(polynomial) text(1.8, -0.1, labels=&quot;secant&quot;, offset=0.5, col=&quot;darkgreen&quot;) root = secant(3,4) Figure 3.6: Secant Iteration 3.4 Approximating Solutions to Systems of Eqs by Iteration (\\(Ax = b\\)) One of the motivations of linear algebra is to solve for Systems of Polynomial Equations (whether linear or non-linear) denoted as: \\[\\begin{align} A \\mathbf{\\vec{x}} = b \\label{eqn:eqnnumber1} \\end{align}\\] We can express simple systems of linear equations geometrically in terms of lines - first-degree polynomial equations. For example, in Figure 3.7, it is easy to show that if two lines (two linear equations) intersect, it means that there is a point of intersection; and thus there is one solution to the system. Figure 3.7: System of Linear Equations It is also easy to show that if two lines (two linear equations) are in parallel, they will never intersect. There is no point of intersection, and thus there is no solution to the system. On the other hand, we can show that if two lines (two linear equations) are in parallel and along the same path, it means that every point in both lines hits an intersection, and thus there is an infinite number of solutions to the system. That said, let us look at what we describe here as solution - the point of intersection. We can derive the point of intersection by translating the equation into matrix form and then solve for the unknown coordinates (x,y) by reducing the matrix into its reduced echelon form (using Gaussian Elimination and Lu Factorization): \\[ \\left(\\begin{array}{l} y = -x + 4\\\\ y = x \\end{array}\\right) \\rightarrow \\left(\\begin{array}{r} x + y = 4\\\\ -x + y = 0 \\end{array}\\right) \\] That gives us: \\[ A\\mathbf{\\vec{x}} = b \\rightarrow \\left[\\begin{array}{rr} 1 &amp; 1 \\\\ -1 &amp; 1 \\end{array}\\right] \\left[\\begin{array}{r} x \\\\ y \\end{array}\\right] = \\left[\\begin{array}{r} 4 \\\\ 0 \\end{array}\\right] \\] in reduced echelon form: \\[ \\left[\\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array}\\left|\\begin{array}{r}2 \\\\ 2\\end{array}\\right.\\right] \\] Thus by RREF method, we are able to solve for the exact values for the vector \\(\\mathbf{\\vec{x}}\\): \\[\\begin{align*} x = 2\\\\ y = 2 \\end{align*}\\] Note that vector \\(\\mathbf{\\vec{x}}\\) has two components: x and y. Alternatively, we can use \\(x_1\\) and \\(x_2\\) to represent the two components. That is an example of the most elementary way of solving a set of polynomial equations for vector \\(\\mathbf{\\vec{x}}\\). When we say solve for the solution, we mean to solve for vector \\(\\mathbf{\\vec{x}}\\). If \\(\\mathbf{\\vec{x}}\\) happens to be a vector variable, then we need to find the value of each entry in the vector. That requires us to perform a simple mathematical manipulation: \\[\\begin{align} \\mathbf{\\vec{x}} = A^{-1}b \\ \\ \\ \\ \\ \\leftarrow A\\mathbf{\\vec{x}} = b \\end{align}\\] For a system of equations translated to a matrix with extremely high dimensionality, computing for the inverse of a matrix is not practical. In Chapter , we introduce methods such as LU decomposition and Gauss elimination as alternatives. Recall the below R code, which we use in Chapter under LU decomposition: A = matrix(c(1,5,5,2,4,5,3,3,3), 3, byrow=TRUE) b = c(6,5,6) LU = lu_decomposition_by_doolittle(A) # from Lin Algebra Ch uy = forward_sub(LU$lower, b) # REF/RREF section in LinAgb chapter x = backward_sub(LU$upper, uy) # REF/RREF section in LinAgb chapter A # the matrix (system of equations) ## [,1] [,2] [,3] ## [1,] 1 5 5 ## [2,] 2 4 5 ## [3,] 3 3 3 x # the solution ## [1] 1 2 -1 3.4.1 Krylov Methods Krylov Methods are iterative methods that make use of a special subspace called Krylov subspace denoted as: \\[\\begin{align} K_m(A,v) = \\{\\ v,\\ Av,\\ A^2v,\\ A^3v,\\ ...\\ ,\\ A^{m-1}v\\ \\} \\end{align}\\] where m = dimension of K. Here, we reference the works of Heath M.T. (2002), An D. (2009), Driscoll T. (2012), and Sleijpen G. (2014). Recall Figure 3.1. The right side illustrates solving for eigenvalues. And the left side illustrates a linear system of solving for \\(x\\). In Figure 3.8, we show the same methods; but we focus now on GMRES and CG in the next sections. Figure 3.8: Krylov SubSpace methods Though Krylov methods are also used to deal with Eigen problems (e.g., Arnoldi and Lanczos methods), here we continue to focus on solving systems of equations for \\(\\mathbf{\\vec{x}}\\) in the form of: \\[ A\\mathbf{\\vec{x}} = b \\rightarrow\\ \\ \\ \\ \\ \\mathbf{\\vec{x}} = A^{-1}b \\] Now instead of using analytical methods to compute for the inverse of A, we approximate by using Power Series, forming the polynomial equation below: \\[\\begin{align} A^{-1} \\approx p_j(A) = \\frac{1}{c_0} \\sum_{j=0}^{m-1}(c_{j+1}) A^j, \\ where\\ c_0 \\neq 0 \\end{align}\\] We use ‘\\(\\approx\\)’ notation to indicate that we are numerically approximating \\(\\mathbf{A^{-1}}\\) rather than using direct solvers. Thus we get: \\[\\begin{align} \\mathbf{\\hat{x}} = A^{-1}b \\approx p_m(A)b \\end{align}\\] And in its expanded form: \\[\\begin{align} \\mathbf{\\hat{x}} \\approx p_m(A)b = c_0b + c_1Ab + c_2A^2b + c_3A^3b +\\ ...\\ + c_mA^{m-1} b \\end{align}\\] What we have is a linear combination that spans a subspace, K: \\[\\begin{align} \\mathbf{\\hat{x}} \\approx p_m(A)b = K_m(A,b) \\end{align}\\] We call this subspace K the Krylov subspace - the span of vectors in multiples of the powers of matrix A as shown. For example (consider \\(\\mathbf{\\vec{b}} = \\mathbf{\\vec{v_0}}\\)): \\[\\begin{align} A(b) &amp;= Ab &amp; &amp;\\rightarrow \\text{where} (\\mathbf{Ab}) \\text{ is a vector denoted as } \\mathbf{\\vec{v_1}} \\\\ A(Ab) &amp;= A^2b &amp; &amp;\\rightarrow \\text{where} (\\mathbf{A^2b}) \\text{ is a vector denoted as } \\mathbf{\\vec{v_2}} \\\\ A(A^2b) &amp;= A^3b &amp; &amp;\\rightarrow \\text{where} (\\mathbf{A^3b}) \\text{ is a vector denoted as } \\mathbf{\\vec{v_3}} \\\\ &amp; &amp; &amp;\\vdots \\nonumber \\\\ A(A^{m-1}b) &amp;= A^{m-1}b &amp; &amp;\\rightarrow \\text{where} (\\mathbf{A^{m-1}b}) \\text{ is a vector denoted as } \\mathbf{\\vec{v_m}} \\end{align}\\] That creates a subspace that spans the following: \\[\\begin{align} K_m(A,b) = span\\{ b, Ab, A^2b, A^3b, ..., A^{m-1}b \\} = \\{ \\mathbf{\\vec{v_0}}, \\mathbf{\\vec{v_1}}, \\mathbf{\\vec{v_2}}, \\mathbf{\\vec{v_3}}, ..., \\mathbf{\\vec{v_m}} \\} \\end{align}\\] Because we intend to approximate for \\(\\mathbf{\\vec{x}}\\), we can readily say that \\(\\mathbf{\\vec{x}}\\) is nothing more than a linear combination of (almost) linearly dependent vectors in K subspace. In other words, the approximate solution of \\(\\mathbf{\\vec{x}}\\) can be anywhere in this subspace - to put it plainly, the solution spans the subspace of K, granting the system of equations is non-singular. \\[ \\mathbf{\\vec{x}} \\in K_m(A,b) \\rightarrow \\ \\ \\ \\ \\mathbf{\\vec{x}} \\in \\{ \\mathbf{\\vec{v_0}}, \\mathbf{\\vec{v_1}}, \\mathbf{\\vec{v_2}}, \\mathbf{\\vec{v_3}}, ..., \\mathbf{\\vec{v_m}} \\} \\rightarrow\\ \\ \\ \\ \\ \\ \\mathbf{\\vec{x}} \\in V_m \\] If the system is singular, there is a chance that the solution may not lie in the space, granting there is even a solution. Granting there exists a solution, and that our approximate solution \\(\\mathbf{\\hat{x}}\\) is in the K space, \\(\\mathbf{\\hat{x}} \\in K_m(A,b)\\); it then means: \\[\\begin{align} \\mathbf{\\vec{x}} = A^{-1}b\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\mathbf{\\hat{x}} \\approx K_m\\mathbf{\\beta}\\ \\ \\ \\ \\ \\ given\\ \\ \\ \\ \\mathbf{\\hat{x}} \\in K_m(A,b) \\end{align}\\] Here, the solution denoted as \\(\\mathbf{\\hat{x}}\\) is an approximation at the mth iteration for the \\(\\mathbf{K_m}\\) matrix and \\(\\beta\\). The \\(\\beta\\) value is derived using the following equation: \\[\\begin{align} \\beta = ((K_m)^T (K_m))^{-1} (K_m)^T b \\end{align}\\] It is not readily apparent that solving for \\(\\mathbf{\\hat{x}}\\) by iterative approximation gives us the closest solution. For that reason, we minimize the norm of the residual: We just need to use the following minimal-residual equation: \\[\\begin{align} {f(x)}_{min} = \\| b - A\\mathbf{\\vec{x}} \\|_{L2} &lt; tol \\end{align}\\] and replace \\(\\mathbf{\\vec{x}}\\) with the approximate \\(\\mathbf{\\hat{x}} \\approx K_n\\beta_n\\): \\[\\begin{align} {f(\\beta)}_{min} = \\| b - A(K_m\\beta) \\|_{L2} &lt; tol \\end{align}\\] To illustrate, given a matrix A and b: \\[ \\mathbf{\\vec{x}} = \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A}^{-1} \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} \\] First, initialize our Krylov matrix: \\[ K = b\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ K_{0}(A,b) = \\{\\ b\\ \\} \\rightarrow \\left\\{ \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} \\right\\} \\] Then we iterate: At \\(m=1\\), we perform the following: multiply the last element of K by A, e.g., \\(A \\cdotp b\\), adding a new linear combination into the space. \\[ K_{1}(A,b) = \\{\\ b,\\ Ab\\ \\} \\rightarrow K_1 = \\left\\{ \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} \\left[ \\begin{array}{rrr} 51 \\\\ 62 \\\\ 61 \\end{array} \\right]_{Ab} \\right\\} \\] compute for \\(\\beta\\): \\[\\begin{align*} \\beta {}&amp;= ((K_1)^T (K_1))^{-1} (K_1)^T b = \\left( \\left[ \\begin{array}{rrr} 6 &amp; 51 \\\\ 5 &amp; 62 \\\\ 6 &amp; 61\\end{array} \\right]_{K_1}^T \\left[ \\begin{array}{rrr} 6 &amp; 51 \\\\ 5 &amp; 62 \\\\ 6 &amp; 61\\end{array} \\right]_{K_1} \\right)^{-1} \\left[ \\begin{array}{rrr} 6 &amp; 51 \\\\ 5 &amp; 62 \\\\ 6 &amp; 61\\end{array} \\right]_{K_1}^T \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} \\\\ \\\\ \\beta &amp;= 0.0965965 \\end{align*}\\] compute for first approximate of \\(\\mathbf{\\hat{x}}\\): \\[ \\mathbf{\\hat{x}} = K_1 \\cdotp \\beta = \\left[ \\begin{array}{rrr} 6 &amp; 51 \\\\ 5 &amp; 62 \\\\ 6 &amp; 61\\end{array} \\right]_{K_1} (0.0965965)_{\\beta} = \\left[ \\begin{array}{rrr} 0.5795790 \\\\ 0.4829825 \\\\ 0.5795790 \\end{array} \\right] \\] Compute for residual: \\[\\begin{align*} r_{min} = \\| b - A\\mathbf{\\hat{x}} \\|_{L2} = \\left\\| \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} - \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A} \\left[ \\begin{array}{rrr} 0.5795790 \\\\ 0.4829825 \\\\ 0.5795790 \\end{array} \\right]_{\\mathbf{\\hat{x}}} \\right\\|_{L2} = 1.463639 \\end{align*}\\] check if residual reaches tolerance level (e.g. 1e-5): \\[ if\\ (\\ (r_{min} = 1.463639) &lt; tol\\ ) \\text{ stop iteration} \\] Otherwise, continue for next iteration at n = 2 … We continue to iterate: At \\(m=2\\), we perform the following: multiply last element of K by A, e.g., \\(A \\cdotp Ab\\), adding a new linear combination into the space. \\[ K_{2}(A,b) = \\{\\ b,\\ Ab,\\ A^2b\\ \\} \\rightarrow K_2 = \\left\\{ \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} \\left[ \\begin{array}{rrr} 51 \\\\ 62 \\\\ 61 \\end{array} \\right]_{Ab} \\left[ \\begin{array}{rrr} 522 \\\\ 655 \\\\ 666 \\end{array} \\right]_{A^2b} \\right\\} \\] compute for \\(\\beta\\): \\[\\begin{align*} \\beta {}&amp;= ((K_2)^T (K_2))^{-1} (K_2)^T b = \\left[ \\begin{array}{rrr} 0.32604333 \\\\ -0.02162618 \\end{array} \\right] \\end{align*}\\] compute for second approximate of \\(\\mathbf{\\hat{x}}\\): \\[ \\mathbf{\\hat{x}} = K_2 \\cdotp \\beta = \\left[ \\begin{array}{rrr} 6 &amp; 51 &amp; 522 \\\\ 5 &amp; 62 &amp; 655 \\\\ 6 &amp; 61 &amp; 666\\end{array} \\right]_{K_2} \\left[ \\begin{array}{rrr} 0.32604333 \\\\ -0.02162618 \\end{array} \\right]_{\\beta} = \\left[ \\begin{array}{rrr} 0.8533248 \\\\ 0.2893935 \\\\ 0.6370630 \\end{array} \\right] \\] Compute for residual: \\[\\begin{align*} r_{min} = \\| b - A\\mathbf{\\hat{x}} \\|_{L2} = \\left\\| \\left[ \\begin{array}{rrr} 6 \\\\ 5 \\\\ 6 \\end{array} \\right]_{b} - \\left[ \\begin{array}{rrr} 3 &amp; 3 &amp; 3 \\\\ 2 &amp; 4 &amp; 5 \\\\ 1 &amp; 5 &amp; 5 \\end{array} \\right]_{A} \\left[ \\begin{array}{rrr} 0.8533248 \\\\ 0.2893935 \\\\ 0.6370630 \\end{array} \\right]_{\\mathbf{\\hat{x}}} \\right\\|_{L2} = 1.342609 \\end{align*}\\] check if residual reaches tolerance level (e.g. 1e-5): \\[ if\\ (\\ (r_{min} = 1.342609) &lt; tol\\ ) \\text{ stop iteration} \\] Otherwise, continue for next iteration at n = 3 … After the third iteration, we get the approximate solution \\(\\mathbf{\\hat{x}} = \\left[ \\begin{array}{rrr} 1 &amp; 2 &amp; -1 \\end{array} \\right]^T\\) hitting the following residual: \\[ r_{min} = \\text{8.683085e-12} \\] Let us review a naive implementation of a simple Krylov method in R code: naive_krylov_method &lt;- function(A, b) { n = nrow(A) K = b # start with K(A,b) = {b} limit=100 tol = 1e-5 for (m in 1:limit) { v = A %*% K beta = solve( t(v) %*% v, t(v) %*% b) appr_x = K %*% beta min_r = sqrt(sum((b - A %*% appr_x)^2)) if (min_r &lt; tol) break # keep building krylov space, K(A,b) = {b,Ab,...} K = matrix( cbind(K, v[,c(m)]), n ) } list(&quot;matrix&quot;=A, &quot;b&quot;=c(b), &quot;x&quot; = c(x), &quot;residual&quot; = min_r) } A = matrix(c(3,3,3, 2,4,5, 1,5,5), 3, byrow=TRUE) x = c(1,2,-1) b &lt;- A %*% x naive_krylov_method(A,b) ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $b ## [1] 6 5 6 ## ## $x ## [1] 1 2 -1 ## ## $residual ## [1] 8.683085e-12 It is notable to mention that the Krylov matrix is a linear combination of multiples of A and a fixed b; in that respect, the K matrix becomes ill-conditioned. We can apply preconditioners \\(M^{-1}\\) to A to make it well-conditioned. Successive computation of the normalized residual appears to converge but then eventually diverges further away from zero. That is because the K space starts to accumulate vectors that are parallel to each other. We need an orthonormal basis that spans the Krylov space. In the next section, we discuss GMRES, which offers that approach. 3.4.2 GMRES (Generalized Minimal Residual) Relevant to Krylov methods, let us cover the GMRES method developed by Yousef Saad and Martin Schultz (1986), which is used in solving nonsymmetric systems of equations. It may help to recall Figure 3.1 - particularly, to review the left figure. As we explain the figure further, there are two important notes to emphasize. \\(\\mathbf{Ax}\\) as projection: \\[\\begin{align} Ax \\parallel K_m(A,b) \\end{align}\\] \\(\\mathbf{(b - Ax)}\\) as orthogonal projection: \\[\\begin{align} b - Ax \\perp K_m(A,b) \\end{align}\\] On the one hand, \\(\\mathbf{Ax}\\) is the projection unto K and thus is parallel to K. On the other hand, \\(\\mathbf{(b - Ax)}\\) is the orthogonal projection unto K. And of course, \\(\\mathbf{b}\\) is being projected. We can also denote the orthogonal relationship as: \\[\\begin{align} r \\perp K_m(A,r) \\ \\ \\ \\ \\ where\\ (r = b - Ax) \\end{align}\\] Now, recall the following equation from Arnoldi iteration: \\[ AQ_m = Q_mH_m\\ \\ \\leftarrow \\ \\ \\text{Ignore last row of H where } H_{m+1,m}=0\\ \\ \\ \\ \\therefore\\ Q_{m,m} \\leftarrow Q_{m,m+1} \\] Here, our GMRES method uses the orthogonal projection to construct the \\(Q\\) basis for the K space. For example, given an initial arbitrary approximation for \\(x^0\\): \\[\\begin{align} r^0 = b - Ax^0\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ q = \\frac{r^0}{\\| r^0 \\|_{L2}} \\end{align}\\] we then build our K space out of \\(q\\): \\[\\begin{align} K_m(A,q) = span\\ \\{\\ q,\\ Aq,\\ A^2q,\\ A^3q,\\ ..., A^{m-1}q \\} = \\{\\ q_1,\\ q_2,\\ q_3,\\ ...,\\ q_m\\ \\} \\end{align}\\] As such, we get an orthonormal basis in the form of \\(Q\\): \\[\\begin{align} Q_{basis} = \\{\\ q_1,\\ q_2,\\ q_3,\\ ...,\\ q_m\\ \\} \\end{align}\\] We use Arnoldi method to iterate and build our \\(Q\\) basis (with Gram-Schmid orthogonalization - see also Matrix manipulation in Linear Algebra chapter for the orthogonal projections): \\[\\begin{align} q_1 {}&amp;= \\frac{r_0}{\\| r_0 \\|_{L2}} = q\\\\ q_2 &amp;= \\frac{Aq_1 - ( h_{1,1}q_1 ) }{\\| Aq_1 - ( h_{1,1}q_1 ) \\|} \\\\ q_3 &amp;= \\frac{Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) }{\\| Aq_2 - ( h_{1,2}q_1 + h_{2,2}q_2 ) \\|} \\\\ \\vdots \\nonumber \\\\ q_{j+1} &amp;= \\frac{Aq_j - ( h_{1,j}q_1 + h_{2,j}q_2 + ... + h_{j,j}q_j ) } {\\| Aq_{j} - ( h_{1,j}q_j + h_{2,j}q_2 + ... + h_{j,j}q_j ) \\|} \\end{align}\\] Now, if there is such a target approximate solution vector \\(\\mathbf{\\hat{x}}\\) that lies in the K space such that \\(\\mathbf{\\hat{x}} \\in K_m(A, q)\\), then \\(\\mathbf{\\hat{x}}\\) must be a linear combination and thus we can write \\(\\mathbf{\\hat{x}}\\) as such: \\[ \\mathbf{\\hat{x}} = c_1q_1 + c_2q_2 + ... + c_mq_m = \\left[ \\begin{array}{rrrrr} q_{1,1} &amp; q_{1,2} &amp; ... &amp; q_{1,m} \\\\ q_{2,1} &amp; q_{2,2} &amp; ... &amp; q_{2,m} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ q_{n,1} &amp; q_{n,2} &amp; ... &amp; q_{n,m} \\\\ \\end{array} \\right]_{Q_m} \\left[ \\begin{array}{rrrrr} c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_m \\end{array} \\right]_\\beta = Q_m\\beta \\] Out of which, we formulate our equation for the solution: \\[\\begin{align} x = x^0 + Q_m\\beta\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ x = x^0 + \\mathbf{\\hat{x}} \\end{align}\\] which is derived based on the following mathematical manipulation: \\[ \\begin{array}{llll} Ax &amp;= b &amp;\\leftarrow&amp; \\text{equation for our target solution} \\\\ Ax^0 &amp;= b - r^0 &amp;\\leftarrow&amp; \\text{equation for our initial solution} \\end{array} \\] Because our initial solution is not the actual solution, therefore, we expect some residual \\(\\mathbf{r^0}\\) Cancelling out \\(\\mathbf{\\vec{b}}\\), we then get: \\[\\begin{align} A(x - x^0) = r^0\\ \\ \\ \\ {}&amp;\\rightarrow x - x^0 \\in K_m(A, r^0) \\\\ &amp;\\rightarrow x \\in x^0 + K_m(A, r^0) \\\\ &amp;\\rightarrow x = x^0 + Q_m\\beta \\end{align}\\] The \\(\\beta\\) is a minimizer. Before we compute for \\(\\beta\\), let us consider a few equations first using the \\(Q\\) basis. We know that: \\[\\begin{align} r^0 = q \\| r^0 \\|_{L2}\\ \\ \\ \\leftarrow \\ \\ \\ \\ \\ \\ q = \\frac{r^0}{\\| r^0 \\|_{L2}} \\end{align}\\] We also know that \\(q\\) is the first vector in the \\(Q\\) basis, and can thus be written as such: \\[\\begin{align} q = Q_{m}e_1 \\in R^n \\end{align}\\] where: \\[ e_1 = (1,0,0, ..., 0)\\ \\ \\ \\leftarrow \\text{use to extract first column} \\] Therefore: \\[\\begin{align} r^0 {}&amp;= \\| r^0 \\|_{L2}\\ q \\\\ r^0 {}&amp;= \\| r^0 \\|_{L2}\\ Q_{m}e_1 \\\\ Q_{m}^Tr^0 &amp;= \\| r^0 \\|_{L2}\\ Q_{m}^TQ_{m}\\ e_1 \\\\ Q_{m}^Tr^0 &amp;= \\| r^0 \\|_{L2}e_1 \\end{align}\\] Now, to compute for \\(\\beta\\), we perform a few mathematical manipulation. \\[\\begin{align} b - Ax {}&amp;= 0 &amp; {}&amp; \\leftarrow Ax = b \\\\ b - A(x^0 + Q_{m}\\beta ) &amp;= 0 &amp; &amp; \\leftarrow x = x^0 + Q_{m}\\beta \\\\ b - Ax^0 - AQ_{m}\\beta &amp;= 0 \\\\ r^0 - AQ_{m}\\beta &amp;= 0 &amp; &amp; \\leftarrow r^0 = b - Ax^0 \\\\ r^0 - Q_{m}H_{m}\\beta &amp;= 0 &amp; &amp; \\leftarrow AQ_{m} = Q_{m}H_{m} \\\\ Q_{m}H_{m}\\beta &amp;= r^0 &amp; &amp; \\\\ \\beta &amp;= H_{m}^{-1}Q_{m}^Tr^0 &amp; &amp; \\leftarrow Q_{m}^T = Q_{m}^{-1}\\ \\ \\ \\{orthogonal\\} \\\\ \\beta &amp;= H_{m}^{-1}\\| r^0\\|_{L2} e1 &amp; &amp; \\leftarrow Q_{m}^T r^0 = \\| r^0\\|_{L2}e1 \\end{align}\\] Finally, let us now review the GMRES algorithm: \\[ \\begin{array}{l} x^0 \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ r^0 = b - Ax^0 \\\\ &lt;Q,H&gt; = arnoldi\\_iteration(A, r^0) \\\\ \\rho = \\| r^0 \\|_{L2} \\in R \\\\ \\beta = ( H^{-1} \\times \\rho ) \\cdotp e1 \\in R^n \\\\ x = x^0 + Q \\cdotp \\beta \\end{array} \\] Below is a naive implementation of GMRES in R code: gmres &lt;- function(A, b) { n = ncol(A) e1 = c(1,rep(0, n-1)) x0 = rep(1, n) r0 = b - A %*% x0 arnoldi = arnoldi_method(A, r0 ) rho = sqrt(sum( ( r0 )^2 )) # minimize normal residual beta = (solve(arnoldi$H) * rho) %*% e1 x = x0 + arnoldi$Q %*% beta list(&quot;matrix&quot;=A, &quot;b&quot;=c(b), &quot;x&quot;=c(x)) } A = matrix(c(3,3,3, 2,4,5, 1,5,5), 3, byrow=TRUE) b = c(6,5,6) gmres(A, b) ## $matrix ## [,1] [,2] [,3] ## [1,] 3 3 3 ## [2,] 2 4 5 ## [3,] 1 5 5 ## ## $b ## [1] 6 5 6 ## ## $x ## [1] 1 2 -1 There are other variations (or modifications) to the original GMRES method - introduced by Saad Schultz (1986). We leave them for the readers to investigate. When it comes to iterative methods in solving for systems of equations, especially suitable for large sparse matrices, there are two other Krylov-based iterative methods to mention: Generalized Minimal Residual (GMRES) method and Conjugate Gradient (CG) method. The GMRES method allows for solving systems of equations whose corresponding matrices are highly sparse. Two other Krylov-based iterative methods are illustrated next. 3.4.3 Conjugate Gradient Method (CG) The Conjugate Gradient method is suitable for symmetric positive definite (SPD) matrices. There are two ways to look at this. First way to look at this: CG method is a Krylov-base method. The idea is that if \\(\\mathbf{x}\\) is the true solution, then it follows that: \\[ Ax - b = 0 \\] But because we are clueless about what the actual value is, we can only approximate for \\(\\mathbf{x}\\) iteratively starting with an initial guess - call it \\(\\mathbf{x_0}\\). In effect, an approximation yields a residual - call it \\(\\mathbf{r_0}\\). Thus, we modify the equation slightly to account for the residual: \\[ Ax_0 - b= r_0 \\] Let us take those two equations to derive an equation that incorporates our approximation, and that manifests a Krylov subspace: \\[\\begin{align} Ax {}&amp;= b,\\ \\ \\ \\ Ax_0 = b - r_0 \\\\ Ax - Ax_0 &amp;= b - (b - r_0) \\\\ A(x - x_0) &amp;= r_0 \\\\ (x - x_0) &amp;= A^{-1} r_0 \\\\ x &amp;= x_0 + A^{-1} r_0 \\\\ x &amp;= x_0 + K_m\\beta\\ \\ \\ \\leftarrow\\ \\ \\ \\ if\\ x \\in x_0 + K_m(A, r_0)\\ \\end{align}\\] The derivation looks familiar because that is discussed in the GMRES method. It comes down to how GMRES and CG handle the minimizer differently. In GMRES method, the norm of the residual \\(\\| \\mathbf{r_k}\\|\\) is minimized. \\[\\begin{align} arg\\ min_{x \\in K_k} \\|Ax_k - b = r_k \\|_{L2} \\end{align}\\] In CG method, the norm of the error energy \\(\\| \\mathbf{e_k} = x - xk\\|\\) - also called A-norm - is minimized. \\[\\begin{align} arg\\ min_{x \\in K_k} \\|x - x_k \\|_A \\equiv (x - x_k )^TA(x - x_k) \\end{align}\\] That also becomes apparent when we discuss steepest descent where direction is optimal if it is orthogonal to the tangent line (See Figure 3.9). We will cover more of that statement later. See Lanczos algorithm as a reference to derive Krylov-based CG method. It resembles the algorithm we discuss next for a second way of looking at the CG method. Second way to look at this: If we take the CG method as a modified Steepest Descent method, let us first understand the concept behind the Steepest Descent method. The plotted graph in Figure 3.9 may help us to get an intuition around the Steepest Descent method and the CG method. Note that the figure assumes an optimal direction orthogonal to the tangent line. In practice, Steepest Descent tends toward a crooked path. Figure 3.9: Gradient and Conjugacy As shown, the black arrows denoted by \\((\\ d_0,\\ d_1,\\ d_2,\\ d_3,\\ d_4\\ )\\) are the vectors called gradients - each gradient represents the direction (or the path) of travel from a starting point (black dot) to a target point (red dot). A single vector with partial derivative elements is a gradient with a \\(\\nabla\\) symbol. For example: \\[\\begin{align} \\nabla f(x) = f&#39;(x) = \\left[ \\begin{array}{rrrrr} \\frac{ \\partial f }{\\partial x_1 } &amp; \\frac{ \\partial f }{\\partial x_2 } &amp; ... &amp; \\frac{ \\partial f }{\\partial x_n } \\end{array} \\right]^T\\ \\ \\ \\ \\leftarrow \\text{ gradient of } f(x) \\in R^n \\label{eqn:eqnnumber2} \\end{align}\\] Here, for the sake of explanation, gradient \\(\\nabla f(x_n)\\) equals the direction \\(d_n\\). Other methods may manipulate the direction however for better optimization which becomes apparent later. \\[\\begin{align} \\nabla f(x_n) = d_n \\end{align}\\] In the figure, each gradient obeys conjugacy between two contour lines if the direction of its path is orthogonal to the curves (contour lines) that it intersects. For example, there are nine contour lines labeled consecutively (5, 10, 15, 20, 25, 30). The point (in black) at which we start our travel intersects at contour line 25. We make a \\(90^\\circ\\) (orthogonal) hop to the next contour line at 20. In other words, the direction is orthogonal to the tangent line that follows the slope of the curvatures of the contour lines. The length of a gradient (e.g. \\(d_n\\)) is denoted as \\(\\mathbf{\\alpha_n}\\). We also call the length a stepsize (or steplength). It can be seen that a combination of the direction (the gradient) and steplength, \\(\\alpha_nd_n\\), represents a step to the next sub-solution \\(\\mathbf{x_{k+1}}\\) - where \\(\\mathbf{x_k}\\) is the current position - which is one step closer towards the final destination. It can be expressed in an equation called the line search method: \\[\\begin{align} x_{k+1} = x_k + \\alpha_{k}d_{k} \\end{align}\\] Note that the graph in Figure 3.9 is quite tuned to try to reach an explanation around the concept of gradient and conjugacy - however, steepest descent and conjugate gradient do not necessarily walk the path that lands orthogonal unto each contour lines. Two directions are used to walk the path: the gradient direction and the conjugate direction. The steepest descent walks the path in a gradient direction. Mathematically, it is expressed as: \\[\\begin{align} x_{k+1} = x_{k} + \\mathbf{\\alpha_k}\\nabla f(x_k) \\end{align}\\] where \\(\\nabla f(x_k)\\) is the gradient direction. The method iterates all the way until it reaches a critical point - the solution \\(\\mathbf{x^*}\\): \\[\\begin{align*} \\mathbf{x_1} = x_{0} + \\mathbf{\\alpha_0}\\nabla f(x_0),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{x_2} = x_{1} + \\mathbf{\\alpha_1}\\nabla f(x_1),\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{x_3} = x_{2} + \\mathbf{\\alpha_2}\\nabla f(x_2) \\end{align*}\\] \\[\\begin{align} \\mathbf{x^*} &amp;= (( x_{0} + \\mathbf{\\alpha_0} \\nabla f(x_0)) + \\mathbf{\\alpha_1} \\nabla f(x_1)) + \\mathbf{\\alpha_2} \\nabla f(x_2) \\\\ \\mathbf{x^*} &amp;= x_{0} + \\sum_k^m \\mathbf{\\alpha_k} \\nabla f(x_k) \\end{align}\\] It is notable to talk about gradient descent as a side note. The difference between gradient descent and steepest descent is the use of the stepsize. In gradient descent, the stepsize is initialized to a fixed scalar value and then used repeatedly in the iteration: \\[\\begin{align} \\alpha {}&amp;= &lt;initial\\ value&gt; \\nonumber \\\\ \\mathbf{x^*} &amp;= x_{0} + \\sum_k^m \\mathbf{\\alpha} \\nabla f(x_k) \\end{align}\\] As for conjugate descent, the method follows a gradient direction as the first step, similar to steepest descent; after which, the rest of the steps follow a conjugate direction. \\[\\begin{align} d_k = \\begin{cases} \\nabla (f{x_k}) &amp; if\\ k=0\\ \\ \\ \\ \\ \\ \\leftarrow \\text{gradient direction}\\\\ \\nabla (f{x_k}) + \\beta_k d_{k-1} &amp; if\\ k &gt;= 1\\ \\ \\ \\leftarrow \\text{conjugate direction} \\end{cases} \\label{eqn:eqnnumber700} \\end{align}\\] Here, a conjugate direction is a direction in which two vectors, u and v, are A-orthogonal (or conjugate) which is a conjugate property expressed as: \\[\\begin{align} &lt;u,v&gt;_A = u^TAv = 0, \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ d_{k}^TAd_{k+1} = 0 \\end{align}\\] Set that aside for a moment. In terms of deriving \\(\\alpha_k\\), let us first have a brief description of conjugate gradient. The conjugate gradient method was introduced by Hestenes and Stiefel (1952) with the intent to minimize a non-linear quadratic function: \\[\\begin{align} arg\\ min_{x \\in R^n} \\ f(x) = \\frac{1}{2}x^TAx - b^Tx + c. \\end{align}\\] But by minimizing the function - solving for the gradient - it thus also ends up linearizing the equation; and, in effect, solving for a linear equation: \\[\\begin{align} \\nabla f(x) = f&#39;(x) = Ax - b \\end{align}\\] Additionally, because we deal with approximation, it is notable to mention here that in the conjugate gradient method, the first step takes gradient not only as the gradient direction but also as the residual; after which, the rest of the steps take the conjugate gradient as the residual: \\[\\begin{align} r_k = \\begin{cases} \\nabla f(x_k) &amp; if\\ k=0\\ \\ \\ \\ \\ \\ \\ \\leftarrow \\text{gradient}\\\\ \\nabla f(x_{k-1}) - \\alpha_{k-1} A d_{k-1} &amp; if\\ k &gt;= 1\\ \\ \\ \\ \\leftarrow \\text{conjugate gradient} \\end{cases} \\label{eqn:eqnnumber701} \\end{align}\\] Furthermore, we notice the emergence of alpha \\(\\alpha\\) and beta \\(\\beta\\) symbols. It is essential to mention that there are choices for solving \\(\\beta_k\\) (conjugate gradient parameter) and \\(\\alpha_k\\) (stepsize); though, we leave these choices for the readers to further investigate (Hager W. W. and Zhang H. 2005): Hestenes-Stiefel (1952) Fletcher-Reeves (1964) Polak-Ribiere-Polyak (1969) Liu-Storey (1991) Dai-Yuan (1999) Hager-Zhang (2005) For example, Polak-Riebre method computes for stepsize \\(\\alpha_k\\) using the following equation: \\[\\begin{align} \\alpha_k = \\frac{\\nabla f_(x_k)^Td_k}{d_k^TAd_k} =\\frac{r_k^Td_k}{d_k^TAd_k} \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ where\\ \\nabla f(x_k) = r_k = Ax_k - b \\end{align}\\] In our case, we use the Fletcher-Reeves method to compute for \\(\\alpha_k\\) expressed as such (with no complete derivation included): \\[\\begin{align} \\alpha_k = \\frac{d_k^Td_k}{d_k^TAd_k} \\end{align}\\] As for \\(\\beta_k \\in R\\), we need this conjugate gradient parameter to compute for the next conjugate direction \\(d_{k+1}\\): \\[\\begin{align} d_{k+1} = r_{k} + \\beta_kd_k \\end{align}\\] For that, we also use the Fletcher-Reeves method to compute for \\(\\beta_k\\) (with no complete derivation included): \\[\\begin{align} \\beta_k = \\frac{r_{k+1}^TAd_k}{d_k^TAd_k} = \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \\end{align}\\] Overall, suppose we are to find our way to the final destination (the critical point) - the final approximate solution, which is \\(\\mathbf{x^*} \\in R^n\\). In that case, we need a linear combination of all the paths (which is linearly independent, granting A-orthogonality is zero). In our case, our target is to reach zero - our solution for \\(\\mathbf{x^*}\\). \\[\\begin{align} x^* &amp;= x_0 + \\sum_{k=1}^m \\alpha_{k-1}d_{k-1} \\\\ &amp;= x_0 + \\alpha_0d_0 + \\alpha_1d_1 + \\alpha_2d_2 + \\alpha_3d_3 + ... + \\alpha_{m-1}d_{m-1} = 0 \\nonumber \\end{align}\\] As final touch, we minimize gradient, \\(r_k = \\nabla f = 0\\), which we use to gauge for convergence. The goal is to reach tolerance level (e.g. tol=1e-5) which is zero by using the norm of the gradient: \\[ \\| r_k \\| &lt; tol \\] With all that explained, let us now illustrate CG using SPD matrix A and vector b: \\[ Ax = b\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\left[ \\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 9 &amp; 6 &amp; 3 \\\\ 3 &amp; 6 &amp; 9 &amp; 2 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{array} \\right]_A \\left[\\begin{array}{r} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{array}\\right]_x = \\left[\\begin{array}{r} 30 \\\\ 50 \\\\ 50 \\\\ 20 \\end{array}\\right]_b \\] First, initialize \\(x = x^0\\): \\[ x_0 = \\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 1 &amp; 1\\end{array}\\right]^T \\] Second, compute for the initial direction \\(\\mathbf{d_0}\\) - the first gradient direction of the steepest descent equivalent to the first residual: \\[ r_0 = d_0 = b - Ax_0 = \\left[\\begin{array}{r} 30 \\\\ 50 \\\\ 50 \\\\ 20 \\end{array}\\right]_b - \\left[ \\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 9 &amp; 6 &amp; 3 \\\\ 3 &amp; 6 &amp; 9 &amp; 2 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{array} \\right]_A \\left[\\begin{array}{r} 1 \\\\ 2 \\\\ 1 \\\\ 1 \\end{array}\\right]_{x_0} = \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0} \\] where \\(d_0\\) is the gradient direction; meaning, we come from a zero starting point (the black dot in Figure 3.9). Third, start the iteration by computing for stepsize \\(\\mathbf{\\alpha_0}\\) using Fletcher-Reeves equation: \\[ \\alpha_0 = \\frac{r_0^Tr_0}{d_0^TAd_0} = \\frac{ \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0}^T \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0} } { \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{d_0}^T \\left[ \\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 9 &amp; 6 &amp; 3 \\\\ 3 &amp; 6 &amp; 9 &amp; 2 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{array} \\right]_A \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{d_0} } = \\frac{1390}{22240}_{\\alpha_0} \\] Fourth, then compute for the solution \\(\\mathbf{x_1}\\): \\[ x_1 = x_0 + \\alpha_0 d_0 = \\left[\\begin{array}{r} 1 \\\\ 2 \\\\ 1 \\\\ 1 \\end{array}\\right]_{x_0} + \\frac{1390}{22240}_{\\alpha_0} \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{d_0} = \\left[\\begin{array}{r} 2.1250 \\\\ 3.3125 \\\\ 2.5000 \\\\ 1.4375 \\end{array}\\right]_{x_1} \\] Fifth, compute for the next gradient (or residual) \\(\\mathbf{r_1}\\): \\[ r_1 = r_0 - \\alpha_0 A d_0 = \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0} - \\frac{1390}{22240} \\left[ \\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\\\ 2 &amp; 9 &amp; 6 &amp; 3 \\\\ 3 &amp; 6 &amp; 9 &amp; 2 \\\\ 4 &amp; 3 &amp; 2 &amp; 1 \\end{array} \\right]_A \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{d_0} = \\left[\\begin{array}{r} 8.000 \\\\ -3.375 \\\\ -1.625 \\\\ -4.875 \\end{array}\\right]_{r_1} \\] Sixth, compute for convergence (expecting the gradient, \\(r_1\\), to become zero): \\[ if\\ (\\ \\| r_1\\|_{L2}\\ &lt;\\ tol\\ )\\, then\\ it\\ converges\\ \\] Seventh, compute for conjugate gradient parameter \\(\\beta_0\\): \\[ \\beta_0 = \\frac{r_1^Tr_1}{r_0^Tr_0} = \\frac{ \\left[\\begin{array}{r} 8.000 \\\\ -3.375 \\\\ -1.625 \\\\ -4.875 \\end{array}\\right]_{r_1}^T \\left[\\begin{array}{r} 8.000 \\\\ -3.375 \\\\ -1.625 \\\\ -4.875 \\end{array}\\right]_{r_1} } { \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0}^T \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{r_0} } = 0.07323516_{\\beta_0} \\] Eight, then compute for the next direction \\(\\mathbf{d_1}\\) - this is a conjugate direction: \\[ d_1 = r_1 + \\beta_0 d_0 = \\left[\\begin{array}{r} 8.000 \\\\ -3.375 \\\\ -1.625 \\\\ -4.875 \\end{array}\\right]_{r_1} + 0.07323516_{\\beta_0} \\left[\\begin{array}{r} 18 \\\\ 21 \\\\ 24 \\\\ 7 \\end{array}\\right]_{d_0} = \\left[\\begin{array}{r} 9.3182329 \\\\ -1.8370616 \\\\ 0.1326439 \\\\ -4.3623539 \\end{array}\\right]_{d_1} \\] From here, we need to iterate by repeating from third step until \\(x_{k+1}\\) converges (\\(e_{k+1}\\) &lt; tol). Finally, after convergence, our solution for x is expected to be: \\(\\left[\\begin{array}{rrrr} 1 &amp; 2 &amp; 3 &amp; 4 \\end{array}\\right]_{x}^T\\) The steps above follow the Conjugate Gradient algorithm: \\[ \\begin{array}{l} x_0 \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ r_0 = d_0 = b - Ax_0 \\\\ loop\\ k\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ \\ \\alpha_k = \\frac{r_k^Tr_k}{r_k^TAr_k}\\\\ \\ \\ \\ \\ \\ x_{k+1} = x_k + \\alpha_k d_k \\\\ \\ \\ \\ \\ \\ r_{k+1} = r_k - \\alpha_k A d_k \\\\ \\ \\ \\ \\ \\ if\\ (\\ \\| r_{k+1} \\|_{L2}\\ &lt;\\ tol\\ )\\ break \\\\ \\ \\ \\ \\ \\ \\beta_k = \\frac{r_{k+1}^Tr_{k+1}}{r_k^Tr_k} \\\\ \\ \\ \\ \\ \\ d_{k+1} = r_{k+1} + \\beta_k r_k \\\\ end\\ loop \\end{array} \\] We now show a naive implementation of Conjugate Gradient in R code: conjugate_gradient &lt;- function(A, x0, b) { n = length(x0) sequence = matrix(0, 0, n + 2) limit = 100 tol = 1e-5; err = 0 r = d = b - A %*% (x=x0) for (k in 0:limit) { if (k==0) { sequence = rbind(sequence, c(k, x, err)) } else { q = A %*% d r_k = c( t(r) %*% r ) alpha = c( r_k / ( t(d) %*% q ) ) x = x + alpha * d r = r - alpha * q err = sqrt(sum((r)^2)) sequence = rbind(sequence, c(k, x, err)) if (err &lt; tol) break beta = c( ( t(r) %*% r ) / r_k ) d = r + beta * d } } colnames(sequence) = c(&quot;K&quot;, paste(&quot;x&quot;,seq(1,n),&quot;&quot;, sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;matrix&quot;=A, &quot;b&quot;=c(b), &quot;x&quot;=c(x), &quot;error&quot;=err) } A = matrix(c(1,2,3,4, 2,9,6,3, 3,6, 9,2, 4,3,2,1), 4, byrow=TRUE) x = c(1,2,3,4) b = A %*% x # (30, 50, 50, 20) conjugate_gradient(A, c(1,2,1,1), b) ## $Iteration ## K x1 x2 x3 x4 error ## [1,] 0 1.0000000 2.000000 1.000000 1.000000 0.000000e+00 ## [2,] 1 2.1250000 3.312500 2.500000 1.437500 1.008944e+01 ## [3,] 2 -2.4570919 4.215846 2.434774 3.582618 1.269644e+01 ## [4,] 3 0.9623681 1.963518 3.074545 3.945714 3.247722e-01 ## [5,] 4 1.0000000 2.000000 3.000000 4.000000 3.947232e-14 ## ## $matrix ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 2 9 6 3 ## [3,] 3 6 9 2 ## [4,] 4 3 2 1 ## ## $b ## [1] 30 50 50 20 ## ## $x ## [1] 1 2 3 4 ## ## $error ## [1] 3.947232e-14 Note that the closest approximate value is already achieved after the 4th iteration, for which it can be noticed that an SPD matrix \\(A \\in R^{nxn}\\) yields at most \\(nth\\) iteration. For further reading, please consider investigating other Krylov-subspace methods for solving \\(Ax = b\\): LSQR - Least Squares QR-factorization Method MINRES - Minimal Residual Method SYMMLQ - Symmetric LQ Method BiCG - Bi-Conjugate Gradient QMR - Quasi-minimal Residual Method FOM - Full Orthogonal Method Now, let us discuss non-Krylov-subspace methods for solving \\(Ax=b\\), starting with Jacobi and Gauss-Seidel Method. 3.4.4 Jacobi and Gauss-Seidel Method If analytical methods cannot solve a system of polynomial equations, we use iterative methods to approximate the solution. The Jacobi method and Gauss-Seidel method - also known as the Liebmann method - are such iterative methods of solving systems of equations. Given a system of linear equations and its expanded form below (See Equation \\(\\ref{eqn:eqnnumber1}\\)) (Dr. S. Karunanithi et al. 2018; Saha M. and Chakrabarty J. 2018): \\[\\begin{align*} a_{1,1}x_1 + a_{1,2}x_2 + a_{1,3}x_3 +\\ ...\\ + a_{1,n}x_n {}&amp;= b_1\\\\ a_{2,1}x_1 + a_{2,2}x_2 + a_{2,3}x_3 +\\ ...\\ + a_{2,n}x_n &amp;= b_2\\\\ \\vdots \\\\ a_{n,1}x_1 + a_{n,2}x_2 + a_{n,3}x_3 +\\ ...\\ + a_{n,n}x_n &amp;= b_n, \\end{align*}\\] Both methods iteratively solve for x by separation of variables. Here we separate \\(\\mathbf{x_i}\\) to the left side of the equation. For example: \\[\\begin{align*} x_1 {}&amp;= ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\\ ...\\ - a_{1,n}x_n)/a_{1,1}\\\\ x_2 &amp;= ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\\ ...\\ - a_{2,n}x_n)/a_{2,2}\\\\ \\vdots \\\\ x_3 &amp;= (b_n - a_{n,1}x_1 - a_{n,2}x_2 -\\ ...\\ - a_{n,n}x_n)/a_{n,3}\\\\ \\end{align*}\\] To illustrate, let us use the following equations (square matrix): \\[\\begin{align*} 9x_1 + 1x_2 + 2x_3 {}&amp;= 6\\\\ 3x_1 + 4x_2 + 1x_3 &amp;= 5\\\\ 3x_1 + 1x_2 + 28x_3 &amp;= 6\\\\ \\end{align*}\\] First, let us separate the x variables such that we get the following: \\[\\begin{align*} x1 {}&amp;= (6 - 1x_2 - 2x_3)/9 = 6/9 - 1/9x_2 - 2/9x_3 \\\\ x2 &amp;= (5 - 3x_1 - 1x_3)/4 = 5/4 - 3/4x_1 - 1/4x_3 \\\\ x3 &amp;= (6 - 3x_1 - 1x_2)/28 = 6/28 - 3/28x_1 - 1/28x_2 \\\\ \\end{align*}\\] That order of the equation will be used throughout the iteration. Second, we assume the following initialization: \\(x_1=0, x_2=0, x_3=0\\). Now, in the Jacobi Method, we perform the following iteration: \\[\\begin{align*} x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\\\ x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3 = 5/4 - 3/4(0) - 1/4(0) = 5/4\\\\ x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0) - 1/28(0) = 6/28\\\\ \\end{align*}\\] Then, we go through the second iteration with the following values: \\(x_1=6/9, x_2=5/4, x_3=6/28\\) \\[\\begin{align*} x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (5/4) - (6/28) = 168/211\\\\ x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3 = 5/4 - 3/4(6/9) - 1/4(6/28) = 39/56\\\\ x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(5/4) = 11/112\\\\ \\end{align*}\\] We then continue with the iteration using the new x values until convergence. However, in the Gauss-Seidel Method, we perform the following iteration instead: \\[\\begin{align*} x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (0) - (0) = 6/9\\\\ x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3 = 5/4 - 3/4(6/9) - 1/4(0) = 3/4\\\\ x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(6/9) - 1/28(3/4) = 13/112\\\\ \\end{align*}\\] Then, we go through the second iteration with the following values: \\(x_1=6/9, x_2=3/4, x_3=13/112\\) \\[\\begin{align*} x1 {}&amp;= 6/9 - 1/9x_2 - 2/9x_3 = 6/9 - (3/4) - (13/112) = 0.73765\\\\ x2 &amp;= 5/4 - 3/4x_1 - 1/4x_3 = 5/4 - 3/4(0.73765) - 1/4(13/112) = 0.66774\\\\ x3 &amp;= 6/28 - 3/28x_1 - 1/28x_2 = 6/28 - 3/28(0.73765) - 1/28(0.66774) = 0.11140\\\\ \\end{align*}\\] We then continue with the iteration using the new x values until convergence. Notice that the Jacobi method uses the values of the previous iteration while the Gauss-Seidel method uses new values immediately within the iteration. Here are the algorithms for the two methods: \\[ \\begin{array}{l} \\mathbf{Jacobi}\\\\ ===============\\\\ loop \\\\ \\ \\ \\ \\ \\phi = x\\\\ \\ \\ \\ \\ loop\\ i\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ s = 0\\\\ \\ \\ \\ \\ \\ \\ \\ \\ loop\\ k\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if ( i \\ne k):\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ s = s + A_{ik} * x_k \\\\ \\ \\ \\ \\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\phi_i = (b_i - s) / A_{ii}\\\\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ x = \\phi \\\\ \\ \\ \\ \\ if\\ \\|Ax - b \\| &lt; tol\\ then\\ break\\\\ end\\ loop \\end{array} \\left| \\begin{array}{l} \\mathbf{\\text{Gauss-Seidel}}\\\\ ===============\\\\ loop \\\\ \\ \\ \\ \\ loop\\ i\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ s = 0\\\\ \\ \\ \\ \\ \\ \\ \\ \\ loop\\ k\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if ( i \\ne k):\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ s = s + A_{ik} * x_k \\\\ \\ \\ \\ \\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ \\ \\ \\ \\ x_i = (b_i - s) / A_{ii}\\\\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ if\\ \\|Ax - b \\| &lt; tol\\ then\\ break\\\\ end\\ loop\\\\ \\\\ \\\\ \\end{array} \\right. \\] Note that both methods do not always result in convergence. There are matrices that do not converge using the methods. One way to identify them is using spectral radius - denoted as \\(\\rho(A)\\) - the maximum absolute eigenvalue of a matrix (system of linear equations). The approximate solution has a higher chance of convergence if the spectral radius is less than 0.70. \\[ \\rho(R/D) = \\rho(D^{-1}R)&lt; 0.70 \\] where D is the diagonal of A, and \\(\\mathbf{R=(L + U)}\\) is the non-diagonal of A. We normalize A using its diagonal entries to test if the matrix is diagonally dominant. Note that we choose to use 0.70 as our level of tolerance instead of 1.0 for smaller iterations for the sake of illustration. One may use 1.00, resulting in more iterations but may still converge. For other convergence criteria, also investigate Stein-Rosenberg theorem (e.g., convergence condition for rectangular matrices). Here is a naive implementation of the Jacobi and Gauss-Seidel method with the convergence condition (Note that we include a new method called SOR which we discuss in the next section): # maximum absolute eigenvalue of matrix spectral_radius &lt;- function(A) { max ( abs( eigen(A)$values ) ) } convergence_condition &lt;- function(A) { n = ncol(A) d = diag(A) D = d * diag(n) R = A - D max ( abs( eigen(solve(D) %*% R)$values ) ) } jacobi &lt;-function(x0, A, b) { x = x0 n = length(x) limit = 50 tol = 1e-5; err = 0 sequence = matrix(0, 0, n + 2) for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, x, err)) } else { x_ = x for (i in 1:n) { s = 0 for (k in 1:n) { if (i != k) { s = s + A[i,k] * x[k] } } x_[i] = (b[i] - s) / A[i,i] } x = x_ err = sqrt(sum((A %*% x - b)^2)) sequence = rbind(sequence, c(j, x, err)) if (err &lt; tol ) break } } colnames(sequence) = c(&quot;N&quot;, paste(&quot;x&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Jacobi Iteration&quot;=sequence, &quot;initial&quot;=x0, &quot;x&quot;=x) } gauss_seidel &lt;-function(x0, A, b) { x = x0 n = length(x) limit = 50 tol = 1e-5; err = 0 sequence = matrix(0, 0, n + 2) for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, x, err)) } else { for (i in 1:n) { s = 0 for (k in 1:n) { if (i != k) { s = s + A[i,k] * x[k] } } x[i] = (b[i] - s) / A[i,i] } err = sqrt(sum((A %*% x - b)^2)) sequence = rbind(sequence, c(j, x, err)) if (err &lt; tol ) break } } colnames(sequence) = c(&quot;N&quot;, paste(&quot;x&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Gauss-Seidel Iteration&quot;=sequence, &quot;initial&quot;=x0, &quot;x&quot;=x) } sor &lt;-function(x0, A, b,w) { x = x0 n = length(x) limit = 150 tol = 1e-5; err = 0 sequence = matrix(0, 0, n + 2) for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, x, err)) } else { for (i in 1:n) { s = 0 for (k in 1:n) { if (i != k) { s = s + A[i,k] * x[k] } } x[i] = ( 1 - w ) * x[i] + (b[i] - s) * ( w / A[i,i] ) } err = sqrt(sum((A %*% x - b)^2)) sequence = rbind(sequence, c(j, x, err)) if (err &lt; tol ) break } } colnames(sequence) = c(&quot;N&quot;, paste(&quot;x&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;SOR Iteration&quot;=sequence, &quot;initial&quot;=x0, &quot;x&quot;=x) } A = matrix(c(9,1,2, 3,4,1, 3,1,28),3, byrow=TRUE) b = c(1,2,3) if ( convergence_condition(A) &lt; 0.70 ) { J = jacobi(c(0,0,0), A, b) G = gauss_seidel(c(0,0,0), A, b) S = sor(c(0,0,0), A, b, 1) # becomes gauss_seidel if w=1 print(J) print(G) print(S) } else { print(&quot;Spectral Radius of Matrix &gt; 0.70&quot;) } ## $`Jacobi Iteration` ## N x1 x2 x3 error ## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00 ## [2,] 1 0.11111111 0.5000000 0.10714286 1.182653e+00 ## [3,] 2 0.03174603 0.3898810 0.07738095 4.709345e-01 ## [4,] 3 0.05059524 0.4568452 0.08981718 1.686652e-01 ## [5,] 4 0.04039116 0.4395993 0.08540604 6.478116e-02 ## [6,] 5 0.04328763 0.4483551 0.08711526 2.367872e-02 ## [7,] 6 0.04193493 0.4457555 0.08649221 9.001587e-03 ## [8,] 7 0.04236223 0.4469257 0.08672999 3.321377e-03 ## [9,] 8 0.04217936 0.4465458 0.08664241 1.255004e-03 ## [10,] 9 0.04224104 0.4467049 0.08667557 4.656032e-04 ## [11,] 10 0.04221600 0.4466503 0.08666329 1.752602e-04 ## [12,] 11 0.04222479 0.4466722 0.08666792 6.523372e-05 ## [13,] 12 0.04222133 0.4466644 0.08666619 2.449648e-05 ## [14,] 13 0.04222258 0.4466675 0.08666684 9.135815e-06 ## ## $initial ## [1] 0 0 0 ## ## $x ## [1] 0.04222258 0.44666745 0.08666684 ## ## $`Gauss-Seidel Iteration` ## N x1 x2 x3 error ## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00 ## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01 ## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02 ## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03 ## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04 ## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05 ## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07 ## ## $initial ## [1] 0 0 0 ## ## $x ## [1] 0.04222231 0.44666663 0.08666666 ## ## $`SOR Iteration` ## N x1 x2 x3 error ## [1,] 0 0.00000000 0.0000000 0.00000000 0.000000e+00 ## [2,] 1 0.11111111 0.4166667 0.08035714 5.829460e-01 ## [3,] 2 0.04695767 0.4446925 0.08622980 4.020236e-02 ## [4,] 3 0.04253866 0.4465386 0.08663734 2.692186e-03 ## [5,] 4 0.04224297 0.4466584 0.08666474 1.768140e-04 ## [6,] 5 0.04222357 0.4466661 0.08666654 1.145822e-05 ## [7,] 6 0.04222231 0.4466666 0.08666666 7.355387e-07 ## ## $initial ## [1] 0 0 0 ## ## $x ## [1] 0.04222231 0.44666663 0.08666666 3.4.5 Successive Over-Relaxation (SOR) Method The Successive Over-relaxation Method (SOR) improves over the Gauss-Seidel method (Dr. S. Karunanithi et al. 2018; Saha M. and Chakrabarty J. 2018). In SOR, we introduce a relaxation factor (w) used to slow down or speed up convergence. If \\(\\mathbf{w=1}\\), then SOR is reduced to Gauss-Seidel method. Here, we make a slight change in the equation used by the Gauss-Seidel method: For example: \\[\\begin{align*} x_1 {}&amp;= (1 - \\omega) x_1 + ( b_1 - a_{1,2}x_2 - a_{1,3}x_3 -\\ ...\\ - a_{1,n}x_n) \\frac{\\omega}{a_{1,1}}\\\\ x_2 &amp;= (1 - \\omega) x_2 + ( b_2 - a_{2,1}x_1 - a_{2,3}x_3 -\\ ...\\ - a_{2,n}x_n)\\frac{\\omega}{a_{2,2}}\\\\ \\vdots \\\\ x_3 &amp;= (1 - \\omega) x_3 + (b_n - a_{n,1}x_1 - a_{n,2}x_2 -\\ ...\\ - a_{n,n}x_n)\\frac{\\omega}{a_{n,3}}\\\\ \\end{align*}\\] And we use the same algorithm as the Gauss-Seidel method with slight modification in the equations used to approximate \\(\\mathbf{x_i}\\): \\[ \\begin{array}{l} \\mathbf{\\text{Successive Over-Relaxation (SOR)}}\\\\ ======================\\\\ loop \\\\ \\ \\ \\ \\ loop\\ i\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ s = 0\\\\ \\ \\ \\ \\ \\ \\ \\ \\ loop\\ k\\ in\\ 1..n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ if ( i \\ne k):\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ s = s + A_{ik} * x_k \\\\ \\ \\ \\ \\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ \\ \\ \\ \\ x_i = (1-\\omega) * x_i + (b_i - s) * (w / A_{ii})\\\\ \\ \\ \\ \\ end\\ loop\\\\ \\ \\ \\ \\ if\\ \\|Ax - b \\| &lt; tol\\ then\\ break\\\\ end\\ loop\\\\ \\\\ \\end{array} \\] See also the R implementation covered in previous section. 3.4.6 Newton’s Method Like Gauss-Seidel method, the Newton’s Method is also an iterative approximation. We use the following iterative equation (where x is a multivariate, e.g. \\(\\mathbf{x} \\in (x_1, x_2)\\)): \\[\\begin{align} x_{k+1} = x_k - J_k^{-1} f(x_k) \\end{align}\\] The equation is derived by approximation using first-order Taylor Series given a partial differentiable function where we force the function to zero to extract the roots, \\(f(x) = 0\\) (note that we can also use \\(f(x) \\approx 0\\) since here we are approximating the roots): \\[\\begin{align} f(x) \\approx f(x_k) + J_k(x_{k+1} - x_k) = 0,\\ \\ \\ \\ where\\ \\Delta x = \\hat{x} - x\\ and\\ \\hat{x}\\ =\\ x_{k+1} \\end{align}\\] Later, we show how Jacobian matrix, J, can be illustrated using two non-linear differentiable functions: \\[\\begin{align} f(\\mathbf{\\vec{x}}) = \\{\\ \\ \\ f_1(\\mathbf{\\vec{x}}),\\ \\ f_2(\\mathbf{\\vec{x}})\\ \\ \\} \\end{align}\\] where \\(\\mathbf{\\vec{x}}\\) is a multivariate vector. Now, when it comes to approximation, we can readily say that \\(\\mathbf{\\hat{x}}\\) is the approximation for the actual value of x and that \\(\\Delta x\\) is the difference (or delta change) between the approximate and the true value. Therefore, we can denote this as such: \\[\\begin{align} \\hat{x} = x + \\Delta x, \\ \\ \\ \\ \\ where\\ \\Delta x\\ \\text{is the delta change} \\end{align}\\] As we point out, the vector \\(\\mathbf{\\hat{x}}\\) can otherwise be represented as multivariate \\(\\{ \\mathbf{x_1, x_2, x_3, ..., x_n}\\}\\) and given we deal with approximation, as an example, we, therefore, can denote the equation this way (with two-variable multivariate): \\[\\begin{align} f(\\mathbf{\\vec{x}} + \\Delta \\vec{x}) = \\{\\ f_1(x_1 + \\Delta x_1, x_2 + \\Delta x_2), \\ f_2(x_1 + \\Delta x_1, x_2 + \\Delta x_2)\\ \\} \\end{align}\\] We can use Taylor series to approximate two-variable multivariate function \\(f_i(\\mathbf{\\vec{x}})\\), performing partial derivatives with respect to each variable: \\[\\begin{align} f_i(\\mathbf{\\vec{x}} + \\Delta \\vec{x}) {}&amp;= f_i(x_1 + \\Delta x_1,x_2 + \\Delta x_2 ) \\nonumber \\\\ &amp;+ \\frac{\\partial f_i}{\\partial x_1}(x_1 + \\Delta x_1,x_2 + \\Delta x_2) \\Delta x_1 \\nonumber \\\\ &amp;+ \\frac{\\partial f_i}{\\partial x_2}(x_1 + \\Delta x_1,x_2 + \\Delta x_2) \\Delta x_2 + R_n(\\vec{x} + \\Delta \\vec{x}) \\end{align}\\] It can also be written this way: \\[\\begin{align} f_i(\\mathbf{\\hat{x}}) = f_i(\\hat{x_1},\\hat{x_2} ) + \\frac{\\partial f_i}{\\partial x_1}(\\hat{x_1},\\hat{x_2} ) \\Delta x_1 + \\frac{\\partial f_i}{\\partial x_2}(\\hat{x_1},\\hat{x_2} ) \\Delta x_2 + R_n(\\hat{x}) \\end{align}\\] or this way: \\[\\begin{align} f_i(\\mathbf{\\hat{x}}) = f_i(\\hat{x_1},\\hat{x_2} ) + \\frac{\\partial f_i}{\\partial x_1}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_1} - x) + \\frac{\\partial f_i}{\\partial x_2}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_2} - x) + R_n(\\hat{x}) \\end{align}\\] where the remainder term is denoted as: \\[\\begin{align} R_n(\\hat{x}) = \\frac{\\partial f_i}{\\partial x_n}(\\xi ) (\\hat{x_n} - x) \\end{align}\\] For two non-linear equations, e.g. two differential functions, we expand \\(f(\\mathbf{\\hat{x}}) = \\{f_1(\\mathbf{\\hat{x}}), f_2(\\mathbf{\\hat{x}}) \\}\\) to show the two functions (as an approximate given we removed the remainder terms): \\[\\begin{align} f_1(\\mathbf{\\hat{x}}) \\approx f_1(\\hat{x_1},\\hat{x_2} ) + \\frac{\\partial f_1}{\\partial x_1}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_1} - x) + \\frac{\\partial f_1}{\\partial x_2}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_2} - x) \\\\ f_2(\\mathbf{\\hat{x}}) \\approx f_2(\\hat{x_1},\\hat{x_2} ) + \\frac{\\partial f_2}{\\partial x_1}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_1} - x) + \\frac{\\partial f_2}{\\partial x_2}(\\hat{x_1},\\hat{x_2} ) (\\hat{x_2} - x) \\end{align}\\] We can translate the partial derivatives into a Jacobian matrix. Note that we could have also considered including the second-order in the Taylor Series and hence be able to translate the derivatives into a Hessian matrix. Now, to illustrate Newton’s method, we settle with first-order. Let us illustrate the Newton’s method. First, perform partial derivatives with respect to each unknown variable and build the Jacobian matrix: \\[\\begin{align*} f_1(\\mathbf{\\vec{x}}) {}&amp;= 3x_1^2 + x_2^2 - 4x_1 &amp; \\ \\ \\frac{\\partial f_1}{\\partial x_1} {}&amp;= 6x_1 - 4,&amp; \\ \\ \\frac{\\partial f_1}{\\partial x_2} {}&amp;= 2x_2 + 0\\\\ f_2(\\mathbf{\\vec{x}}) &amp;= 9x_1^2 + x_2^2 - 2x_2 &amp; \\ \\ \\frac{\\partial f_2}{\\partial x_1} &amp;= 18x_1 + 0, &amp; \\ \\ \\frac{\\partial f_2}{\\partial x_2} &amp;= 2x_2 - 2 \\\\ \\end{align*}\\] We translate the partial derivatives into Jacobian matrix form: \\[ J(\\mathbf{\\vec{x}}) = \\left[ \\begin{array}{ccc} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2}\\ \\end{array} \\right] = \\left[ \\begin{array}{rr} 6x_1 - 4 &amp; 2x_2 + 0\\\\ 18x_1 + 0 &amp; 2x_2 - 2 \\end{array} \\right] \\] Second: compute for the functions and their partial derivatives given an initial arbitrary nonzero vector: Assume initial values for vector \\(\\mathbf{\\vec{x}}^0\\) (note that we are using a superscript for the vector to not confuse with the x variable indexes: \\[ \\mathbf{\\vec{x}}^0 = \\left[\\begin{array}{rrr} 1 &amp; 1 \\end{array}\\right]^T \\] We get result for \\(f(\\mathbf{\\vec{x}}^0)\\) and \\(J(\\mathbf{\\vec{x}}^0)\\): \\[ f(\\mathbf{\\vec{x}}^0) = \\left[ \\begin{array}{ccc} 3x_1^2 + x_2^2 - 4x_1 \\\\ 9x_1^2 + x_2^2 - 2x_2 \\end{array} \\right] = \\left[ \\begin{array}{ccc} 0 \\\\ 8 \\end{array} \\right]\\ \\ \\ \\ \\ \\ \\ \\ J(\\mathbf{\\vec{x}}^0) = \\left[ \\begin{array}{ccc} 6x_1 - 4 &amp; 2x_2 + 0 \\\\ 18x_1 + 0 &amp; 2x_2 - 2 \\end{array} \\right] = \\left[ \\begin{array}{ccc} 2 &amp; 2 \\\\ 18 &amp; 0 \\end{array} \\right] \\] Third, solve for the next \\(\\mathbf{\\vec{x}}^{k+1}\\) using the below equation: \\[\\begin{align} \\mathbf{\\vec{x}}^{k+1} {}&amp;= \\mathbf{\\vec{x}}^k - J_f(\\mathbf{\\vec{x}}^k)^{-1} f(\\mathbf{\\vec{x}}^k) \\label{eqn:eqnnumber200} \\\\ &amp;= \\mathbf{\\vec{x}}^0 - J(\\mathbf{\\vec{x}}^0)^{-1}f(\\mathbf{\\vec{x}}^0) \\label{eqn:eqnnumber201} \\\\ &amp;= \\left[ \\begin{array}{ccc} 1 \\\\ 1 \\end{array} \\right] - \\left[ \\begin{array}{ccc} 2 &amp; 2 \\\\ 18 &amp; 0 \\end{array} \\right]^{-1} \\left[ \\begin{array}{ccc} 0 \\\\ 8 \\end{array} \\right] \\nonumber \\\\ \\mathbf{\\vec{x}}^{1} &amp;= \\left[ \\begin{array}{r} 0.556 \\\\ 1.444 \\end{array} \\right] \\nonumber \\end{align}\\] Fourth, \\(\\mathbf{\\vec{x}}^{1}\\) becomes the next \\(\\mathbf{\\vec{x}}^k\\). From there, iterate until convergence (e.g., tolerance is reached): \\[\\begin{align} \\mathbf{\\vec{x}}^{k+1} {}&amp;= \\mathbf{\\vec{x}}^k - J_f(\\mathbf{\\vec{x}}^k)^{-1} f(\\mathbf{\\vec{x}}^k) \\\\ &amp;= \\mathbf{\\vec{x}}^1 - J(\\mathbf{\\vec{x}}^1)^{-1}f(\\mathbf{\\vec{x}}^1) \\end{align}\\] We use the following tolerance and convergence criterion: \\[\\begin{align} \\left|\\sqrt{\\sum(x^k)^2} - \\sqrt{\\sum(x^{k+1})^2}\\right| &lt; 1e{-5} \\end{align}\\] Convergence is reached with the following result: \\[ x_1 \\approx \\frac{1}{3}, \\ \\ \\ \\ \\ \\ x_2 \\approx 1\\ \\ \\ \\ \\ \\ \\text {if } x^0 = [1,1] \\] and: \\[ x_1 \\approx 0, \\ \\ \\ \\ \\ \\ x_2 \\approx 0\\ \\ \\ \\ \\ \\ \\text {if } x^0 = [-1,1] \\] The steps above illustrates the Newton’s Method algorithm: \\[ \\begin{array}{l} x^0 \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ \\Delta x^k = -J(x^k)^{-1} f(x^k) \\ \\ \\ \\ use\\ LU\\ decomposition\\ for\\ J^{-1}\\\\ \\ \\ \\ \\ x^{k+1} = x^k + \\Delta x^k \\\\ end\\ loop \\end{array} \\] We show the naive implementation of the Newton’s Method in R code: f1 &lt;- function(x) { 3*x[1]^2 + x[2]^2 - 4*x[1] } f2 &lt;- function(x) { 9*x[1]^2 + x[2]^2 - 2*x[2] } J &lt;- function(x) { matrix( c( 6*x[1] - 4, 2*x[2], 18*x[1] , 2*x[2] - 2), 2, byrow=TRUE ) } F &lt;- function(x) { c( f1(x), f2(x) ) } newton &lt;-function(x) { x0 = x n = length(x) limit = 50 tol = 1e-5; err = 0 sequence = matrix(0, 0, n + 2) for (j in 0:limit) { if (j==0) { sequence = rbind(sequence, c(j, x, err)) } else { x_ = x LU = lu_decomposition_by_doolittle(J(x)) uy = forward_sub(LU$lower, F(x)) delta_x = -backward_sub(LU$upper, uy) x = x + delta_x #x = x - solve(J(x)) %*% F(x) # alternative a = sqrt(sum(x_^2)) b = sqrt(sum(x^2)) err = abs(a-b) sequence = rbind(sequence, c(j, x, err)) if (err &lt; tol ) break } } colnames(sequence) = c(&quot;N&quot;, paste(&quot;x&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;initial&quot;=x0, &quot;x&quot;=x) } newton(c(1,1)) ## $Iteration ## N x1 x2 error ## [1,] 0 1.0000000 1.000000 0.000000e+00 ## [2,] 1 0.5555556 1.444444 1.333851e-01 ## [3,] 2 0.3858180 1.131770 3.518735e-01 ## [4,] 3 0.3388188 1.015405 1.252831e-01 ## [5,] 4 0.3334154 1.000241 1.609513e-02 ## [6,] 5 0.3333334 1.000000 2.543819e-04 ## [7,] 6 0.3333333 1.000000 6.209382e-08 ## ## $initial ## [1] 1 1 ## ## $x ## [1] 0.3333333 1.0000000 The implementation of Newton’s Method gives us the result of multivariate x: \\[ x_1 = 1/3\\ \\ \\ \\ \\ \\ \\ \\ \\ x_2 = 1\\ \\ \\ \\ \\ \\ where\\ x^{(0)} = (1,1) \\] Here is another example: newton(c(-1,1)) ## $Iteration ## N x1 x2 error ## [1,] 0 -1.000000e+00 1.000000e+00 0.000000e+00 ## [2,] 1 -5.555556e-01 -7.777778e-01 4.583996e-01 ## [3,] 2 -1.721440e-02 -9.029734e-01 5.267645e-02 ## [4,] 3 -1.082059e-01 -2.061246e-01 6.703374e-01 ## [5,] 4 -1.212666e-02 -5.150582e-02 1.798860e-01 ## [6,] 5 -7.137358e-04 -1.816708e-03 5.096225e-02 ## [7,] 6 -1.202316e-06 -3.927739e-06 1.947775e-03 ## [8,] 7 -4.940920e-12 -1.421849e-11 4.107624e-06 ## ## $initial ## [1] -1 1 ## ## $x ## [1] -4.940920e-12 -1.421849e-11 The result of multivariate x gives us: \\[ x_1 = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ x_2 = 0\\ \\ \\ \\ \\ \\ where\\ x^{(0)} = (-1,1) \\] 3.4.7 Broyden’s Method The Broyden’s Method is a Quasi-Newton method - it is an enhancement to the Newton method by avoiding the repeated evaluation of the Jacobian matrix at every iteration (Jarlebring E. 2018). Given the following equation: \\[\\begin{align} f(x) \\approx A_{k+1} \\Delta x = f(x_{k+1}) - f(x), \\ \\ \\ \\ where\\ \\Delta x = x_{k+1} - x, \\end{align}\\] we derive the Broyden’s formula: \\[\\begin{align} A_{k+1} = A_x + \\frac{(y_k - A_k \\Delta x)(\\Delta x)^T}{\\| \\Delta x \\|_{L2}^2},\\ \\ \\ \\ where\\ y_k = f(x_{k+1}) - f(x). \\end{align}\\] The steps above illustrates the Broyden’s method algorithm: \\[ \\begin{array}{l} x^0 \\leftarrow \\text{initial arbitrary nonzero vector} \\\\ A^0 = J_f(x^0) \\\\ loop\\ j\\ in\\ 1:\\ ... \\\\ \\ \\ \\ \\ \\Delta x = -(A^k)^{-1} f(x^k) \\ \\ \\ \\ use\\ LU\\ decomposition\\ for\\ A^{-1}\\\\ \\ \\ \\ \\ x_{k+1} = x^k + \\Delta x \\\\ \\ \\ \\ \\ y^k = f(x^{k+1}) - f(x^k)\\\\ \\ \\ \\ \\ r^k = y^k - A^k\\Delta x \\\\ \\ \\ \\ \\ A^{k+1} = A^k + r^k(\\Delta x )^T/\\| \\Delta x \\|_{L2}^2 \\\\ end\\ loop \\end{array} \\] We show the naive implementation of Broyden’s Method in R code: f1 &lt;- function(x) { 3*x[1]^2 + x[2]^2 - 4*x[1] } f2 &lt;- function(x) { 9*x[1]^2 + x[2]^2 - 2*x[2] } J &lt;- function(x) { matrix( c( 6*x[1] - 4, 2*x[2],18*x[1] , 2*x[2] - 2), 2, byrow=TRUE ) } F &lt;- function(x) { c( f1(x), f2(x) ) } broyden &lt;-function(x) { x0 = x n = length(x) limit = 50 tol = 1e-5; err = 0 sequence = matrix(0, 0, n + 2) A = J(x) for (i in 0:limit) { if (i==0) { sequence = rbind(sequence, c(i, x, err)) } else { x_ = x LU = lu_decomposition_by_doolittle(A) uy = forward_sub(LU$lower, F(x)) delta_x = - backward_sub(LU$upper, uy) x = x + delta_x y = F(x) - F(x_) r = y - A %*% delta_x A = A + (r %*% t(delta_x)) /( sqrt(sum(delta_x^2)) )^2 a = sqrt(sum(x_^2)) b = sqrt(sum(x^2)) err = abs(a-b) sequence = rbind(sequence, c(i, x, err)) if (err &lt; tol ) break } } colnames(sequence) = c(&quot;N&quot;, paste(&quot;x&quot;,seq(1,n), sep=&quot;&quot;), &quot;error&quot;) list(&quot;Iteration&quot;=sequence, &quot;initial&quot;=x0, &quot;x&quot;=x) } broyden(c(1,1)) ## $Iteration ## N x1 x2 error ## [1,] 0 1.0000000 1.0000000 0.000000e+00 ## [2,] 1 0.5555556 1.4444444 1.333851e-01 ## [3,] 2 0.4639175 1.2061856 2.552740e-01 ## [4,] 3 0.3870957 1.1129019 1.140235e-01 ## [5,] 4 0.3215272 0.9879433 1.393538e-01 ## [6,] 5 0.3269287 0.9877312 1.482875e-03 ## [7,] 6 0.4564969 1.2256526 2.674740e-01 ## [8,] 7 0.3320245 0.9966953 2.573608e-01 ## [9,] 8 0.3330608 0.9992606 2.761418e-03 ## [10,] 9 0.3333392 1.0000118 8.006343e-04 ## [11,] 10 0.3333336 1.0000005 1.248732e-05 ## [12,] 11 0.3333333 1.0000000 5.371434e-07 ## ## $initial ## [1] 1 1 ## ## $x ## [1] 0.3333333 1.0000000 broyden(c(-1,1)) ## $Iteration ## N x1 x2 error ## [1,] 0 -1.000000e+00 1.000000e+00 0.000000e+00 ## [2,] 1 -5.555556e-01 -7.777778e-01 4.583996e-01 ## [3,] 2 -1.616267e-01 -1.502607e+00 5.554606e-01 ## [4,] 3 2.120387e-01 -1.478334e+00 1.781104e-02 ## [5,] 4 4.681942e-02 -7.533436e-01 7.386664e-01 ## [6,] 5 -1.218198e-01 -3.135692e-01 4.183959e-01 ## [7,] 6 -4.877114e-02 -4.873996e-02 2.674504e-01 ## [8,] 7 -4.399305e-03 -1.188260e-02 5.627993e-02 ## [9,] 8 -5.441345e-04 -2.080117e-03 1.052073e-02 ## [10,] 9 -1.332127e-05 -1.809338e-05 2.127641e-03 ## [11,] 10 1.432844e-07 -3.547711e-07 2.208574e-05 ## [12,] 11 -5.747636e-09 1.149681e-08 3.697599e-07 ## ## $initial ## [1] -1 1 ## ## $x ## [1] -5.747636e-09 1.149681e-08 3.4.8 BFGS (Broyden-Fletcher-Goldfarb-Shanno) method The BFGS method takes the Broyden’s Method even further with the following equation: \\[\\begin{align} A_{k+1} = A_x + \\frac{y_k(y_k)^T}{(y_k)^T \\Delta x} - \\frac{A_k \\Delta x(\\Delta x)^T A_k}{(\\Delta x)^TA_k \\Delta x},\\ \\ \\ \\ where\\ y_k = f(x_{k+1}) - f(x) \\end{align}\\] We leave the method and equation to the readers to investigate. 3.5 Approximating Polynomial Functions by Regression We use Figure 3.10 to illustrate a point in this section. Note that while our discussion in this section is about regression - fitting a line, the line being fitted can serve as a model for prediction, which we show in the left graph. We defer the subject of inference and cover that in the Statistical Computation chapter and Computational Learning chapter. Figure 3.10: Linear Regression In the previous section, we deal with linear systems of first-degree polynomials. Geometrically, each equation represents a line in a cartesian plane. Therefore, solving the system means searching for the intersections. In other words, the common unknown is a set of coordinates of (x,y) at the intersection - that is, the solution set (roots). In this section, however, it is the opposite. Geometrically, we are given a bunch of dots (points) instead of a set of lines (equations) in the cartesian plane. For example, in Figure 3.10, the first graph illustrates a set of arbitrary points in the system. These data points are usually sourced from a table like so: \\[ \\left[\\begin{array}{r}x \\\\ y \\end{array}\\left|\\begin{array}{rrrrrrrrrrr} 0.6 &amp; 0.7 &amp; 1.3 &amp; 1.6 &amp; 1.8 &amp; 1.8 &amp; 2.2 &amp; 2.5 &amp; 2.5 &amp; 3.2 &amp; 3.6\\\\ 1.5 &amp; 1.9 &amp; 2.3 &amp; 2.2 &amp; 2.8 &amp; 3.2 &amp; 3.1 &amp; 3.4 &amp; 3.8 &amp; 4.3 &amp; 4.5 \\end{array}\\right.\\right] \\] Note that these data points do not represent the solution set; instead, they are merely arbitrary points, perhaps a set of observations sampled from a population. Therefore, instead of looking for intersections (roots) of linear equations, we are looking for a good linear model represented as a line that fits through a given set of data points. Geometrically, we look for a linear pattern that we can draw through the center of a scattered plot. To do that, we use Least-Squares. 3.5.1 Least-Squares In Figure 3.11, we use the vertical absolute distance instead of the distance orthogonal to the line to compute for the Least-Squares. That is because it may be more convenient to use the vertical distance given it is, after all, proportional to the orthogonal distance when computed; and at the same time, because the difference between \\(\\hat{y}\\) and \\(y\\) fall vertically at a given x-axis. Figure 3.11: Least Squares The vertical distance represents the error, \\(\\epsilon_i\\), which is computed based on the following: A simple linear equation (where i in 1..n and n is number of data points): \\[\\begin{align} y_i {}&amp;= \\beta_0 + \\beta_i x_i \\\\ \\hat{y_i} {}&amp;= \\beta_0 + \\beta_i x_i + \\epsilon_i\\ \\rightarrow\\ \\ \\ \\ \\epsilon_i = \\left| \\hat{y_i} - \\left(\\beta_0 + \\beta_i x_i \\right) \\right| \\\\ \\nonumber \\\\ &amp;therefore\\ \\ \\ \\epsilon_i = |y_i - \\hat{y}_i | \\nonumber \\end{align}\\] A multilinear equation (where m is number of independent variables): \\[\\begin{align} y_i {}&amp;= \\beta_0 + \\sum_{j=1}^m \\beta_ix_{i,j} \\\\ \\hat{y_i} &amp;= \\beta_0 + \\sum_{j=1}^m \\beta_i x_{i,j} + \\epsilon_i\\ \\rightarrow\\ \\ \\ \\ \\epsilon_i = \\left| \\hat{y_i} - \\left(\\beta_0 + \\beta_{i} x_{i,j}\\right) \\right| \\\\ \\nonumber \\\\ &amp;therefore \\ \\ \\ \\epsilon_i = |y_i - \\hat{y}_i| \\nonumber \\end{align}\\] We sum the square of the errors by using the following function - we call this the Ordinary Least Squares (OLS). \\[\\begin{align} RSS(y, \\hat{y}) = \\text{residual sum square} {}&amp;= \\sum_{i=1}^n | y_i - \\hat{y}_i |^2 = \\sum_{i=1}^n |\\epsilon_i|^2 \\\\ \\text{1st degree multilinear} &amp;= \\sum_{i=1}^n \\left| y_i - \\left(\\beta_0 + \\sum_{j=1}^m \\beta_i x_{i,j} \\right) \\right| ^2 \\\\ \\text{general equation} &amp;= \\left| y - \\hat{A}x \\right| ^2 \\end{align}\\] where A is a matrix of coefficients (\\(\\beta\\)s). It can be a Vandermonde matrix for ordered higher degree polynomials or multivariate. If we randomly draw ten lines across a cartesian plane, we end up with a list of ten RSS. We then choose the line with the smallest RSS and conclude that our chosen line is our model. Such a model may not necessarily reflect the best fit, however. We might need to draw more lines - a hundred times perhaps - to be more confident that we get an even smaller RSS. In other words, we aim to find the most minimum RSS - we call this minimizing the loss function. We minimize the function with the following equation: \\[\\begin{align} \\hat{\\beta} = \\underset{\\beta}{\\mathrm{argmin}}\\ RSS(y, \\hat{y}) \\ \\ \\ \\ \\ where\\ \\hat{y} = \\hat{A}x \\end{align}\\] To minimize the objective function, we can get a simpler normalized equation by applying partial derivatives of the objective function with respect to the individual \\(\\beta\\) coefficients and solve for systems of equation (the derivatives). We do not cover the derivative step here, but it leads to the following equations that help us generate the \\(\\beta\\) coefficients corresponding to the Least-Square. For first-degree polynomials, there are a few derived equations to generate the coefficients, but we can choose one of the two formulas we cover here. Here is the first option: \\[\\begin{align} \\hat{\\beta}_1 &amp;= \\frac{n\\sum_{i=1}^n{(x_iy_i)} - \\sum_{i=1}^n{x_i}\\sum_{i=1}^n{y_i}}{n\\sum_{i=1}^n{(x_i^2)} - (\\sum_{i=1}^n{x_i})^2} \\\\ \\hat{\\beta}_0 &amp;= \\frac{\\sum_{i=1}^n{y_i} - \\hat{\\beta}_1\\sum_{i=1}^n{x_i}}{n} \\end{align}\\] where: n is the size of the data (e.g. number of data points) x = c(0.6,0.7,1.3,1.6,1.8,1.8,2.2,2.5,2.5,3.2,3.6) y = c(1.5,1.9,2.3,2.2,2.8,3.2,3.1,3.4,3.8,4.3,4.5) n=length(x) b1 &lt;- function(x,y) { ( n*sum(x * y) - sum(x) * sum(y) ) / (n*sum(x^2) - sum(x)^2) } b0 &lt;- function(x,y) { (sum(y) - b1(x,y) * sum(x)) / n } c(&quot;beta_hat_0&quot; = b0(x,y), &quot;beta_hat_1&quot; = b1(x,y)) ## beta_hat_0 beta_hat_1 ## 1.017374 1.000408 or, alternatively, we can use the second option: \\[\\begin{align} \\hat{\\beta}_1 &amp;= \\frac{\\sum_{i=1}^n(x_i - \\bar{x})(y_i - \\bar{y})}{\\sum_{i=1}^n(x_i - \\bar{x})^2} \\\\ \\hat{\\beta}_0 &amp;= \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{align}\\] where: \\(\\bar{x}\\) is the mean of all x \\(\\bar{y}\\) is the mean of all y b1 &lt;- function(x,y) { sum( (x - mean(x)) * ( y - mean(y))) / sum ((x-mean(x))^2) } b0 &lt;- function(x,y) { mean(y) - b1(x,y) * mean(x) } c(&quot;beta_hat_0&quot; = b0(x,y), &quot;beta_hat_1&quot; = b1(x,y)) ## beta_hat_0 beta_hat_1 ## 1.017374 1.000408 For multilinear equations, e.g. \\(\\{\\ x_1,\\ x_2,\\ x_3,\\ ...,\\ x_m\\ \\}\\), or higher order polynomials, e.g. \\(\\{\\ x^1,\\ x^2, ...,\\ \\ x^n\\ \\}\\), we use a more general matrix equation (where A is the matrix). See linear algebra chapter: \\[\\begin{align} \\hat{\\beta} = (A^T \\cdot A)^{-1} \\cdot A^T \\cdot y\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ y = A\\hat{\\beta} \\label{eqn:eqnnumber3} \\end{align}\\] The formula offers a mathematically convenient way to compute the coefficients using matrices and is derived from the following: \\[\\begin{align} A\\hat{\\beta} {}&amp;= y \\\\ A^T \\cdot A \\hat{\\beta} &amp;= A^T \\cdot y \\\\ \\hat{\\beta} &amp;= (A^T \\cdot A)^{-1} \\cdot A^T \\cdot y \\end{align}\\] Both equations lead to the same set of \\(\\hat{\\beta}\\)s - at least for the simple equation. 3.5.2 Linear Regression Let us illustrate linear regression by using Ordinary Least Square (OLS). First, we translate our data points above into a set of linear equations using the following formula: \\[\\begin{align} y = \\hat{\\beta}_0 + \\hat{\\beta}_1 x\\ \\ \\ \\ \\leftarrow \\ \\ y = mx + c\\ \\ \\ \\ \\text{(from the slope formula)} \\end{align}\\] We build a system of linear equations using the slope formula and the data points like so: \\[ \\begin{array}{lll} 1.5 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 0.6 &amp; &amp; 3.1 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 2.2 \\\\ 1.9 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 0.7 &amp; &amp; 3.4 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 2.5 \\\\ 2.3 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 1.3 &amp; &amp; 3.8 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 2.5 \\\\ 2.2 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 1.6 &amp; &amp; 4.3 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 3.2 \\\\ 2.8 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 1.8 &amp; &amp; 4.5 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 3.6 \\\\ 3.2 = \\hat{\\beta}_0 + \\hat{\\beta}_1 \\times 1.8 \\end{array} \\] Second, we construct the A matrix based on the above linear of equations. \\[ Vandermonde(A) = \\left[\\begin{array}{rr} 1 &amp; x_1 \\\\ 1 &amp; x_2 \\\\ 1 &amp; x_3 \\\\ 1 &amp; ...\\\\ 1 &amp; x_{11} \\\\ \\end{array}\\right] = \\left[\\begin{array}{rr} 1 &amp; 0.6 \\\\ 1 &amp; 0.7 \\\\ 1 &amp; 1.3 \\\\ 1 &amp; ...\\\\ 1 &amp; 3.6 \\\\ \\end{array}\\right] \\] Third, we also construct our vector of coefficients, \\(\\beta_0\\) and \\(\\beta_1\\). Here, we are looking for the unknown coefficients denoted as \\(\\beta = \\{m, c\\}\\) where m=unknown slope and c=unknown intercept: \\[\\begin{align} \\hat{\\beta} = \\left[\\begin{array}{c} c\\\\m \\end{array}\\right] = \\left[\\begin{array}{c} \\hat{\\beta}_0\\\\ \\hat{\\beta}_1 \\end{array}\\right] \\label{eqn:eqnnumber4} \\end{align}\\] Fourth, now let us solve for the coefficients using the following equation: \\[\\begin{align} A\\hat{\\beta} = y \\end{align}\\] Let us replace the equation with the vandermonde matrix (A) and coefficient vector (\\(\\beta\\)). \\[ A\\hat{\\beta} = y \\rightarrow \\ \\ \\ \\ \\ \\left[\\begin{array}{c} \\begin{array}{rr} 1 &amp; 0.6 \\\\ 1 &amp; 0.7 \\\\ 1 &amp; 1.3 \\\\ 1 &amp; 1.6 \\\\ 1&amp; 1.8 \\\\ 1 &amp; 1.8 \\\\ 1 &amp; 2.2 \\\\ 1 &amp; 2.5 \\\\ 1 &amp; 2.5 \\\\ 1 &amp; 3.2 \\\\ 1 &amp; 3.6 \\end{array} \\end{array}\\right]_A \\left[\\begin{array}{c} \\hat{\\beta}_0 \\\\ \\hat{\\beta}_1 \\end{array}\\right]_{\\hat{\\beta}} = \\left[\\begin{array}{c} \\begin{array}{r} 1.5 \\\\ 1.9 \\\\ 2.3 \\\\ 2.2 \\\\ 2.8 \\\\ 3.2 \\\\ 3.1 \\\\ 3.4 \\\\ 3.8 \\\\ 4.3 \\\\ 4.5 \\end{array} \\end{array}\\right]_y \\] Finally, let us implement the following equation in R code using Equation \\(\\ref{eqn:eqnnumber3}\\). intercept = rep(1, length(x)) # Our A matrix A = matrix( data = c(intercept, x), nrow = 11, ncol = 2, byrow=FALSE ) # Our coefficients B = solve(t(A) %*% A) %*% t(A) %*% y c(&quot;beta_hat_0&quot; = B[1], &quot;beta_hat_1&quot; = B[2]) ## beta_hat_0 beta_hat_1 ## 1.017374 1.000408 Our coefficients, \\(\\mathbf{\\beta}s\\), for the observed data are: \\(\\beta_0\\) = 1.0173736 and \\(\\beta_1\\) = 1.0004078. A plot of the linear regression model is shown in Figure 3.12. Figure 3.12: Simple Linear Regression Our regression model is expressed as: y = 1.0173736 + 1.0004078x We can use the equation as a model to solve for values of y given any new values of x. 3.5.3 Higher Degree Polynomials We now cover curves in this section. We know that the equation of a straight line is expressed by a polynomial with 1st-degree terms. This section introduces polynomials having higher degree terms that describe a curve. For example, the following equations describe Legendre polynomials - one of the orthogonal polynomials: Linear: \\(P_1(x)\\) = x Quadratic: \\(P_2(x)\\) = \\(\\frac{1}{2}(3x^2 -1)\\) Cubic: \\(P_3(x)\\) = \\(\\frac{1}{2}(5x^3 -3x)\\) Quartic: \\(P_4(x)\\) = \\(\\frac{1}{8}(35x^4 -30x^2 + 3)\\) Quintic: \\(P_5(x)\\) = \\(\\frac{1}{8}(63x^5 -70x^3 + 15x)\\) Sextic: \\(P_6(x)\\) = \\(\\frac{1}{16}(231x^6 -315x^4 + 105x^2 - 5)\\) Septic: \\(P_7(x)\\) = \\(\\frac{1}{16}(429x^7 -693x^5 + 315x^3 -35x)\\) We demonstrate Legendre polynomials in R to give us the following Figure 3.13: x = seq(-1, 1, .2) linear &lt;- function(t) { t * 1 } quadratic &lt;- function(t) { 1/2 * ( 3*t^2 - 1 ) } cubic &lt;- function(t) { 1/2 * (5*t^3 - 3*t ) } quartic &lt;- function(t) { 1/8 * (35*t^4 - 30*t^2 + 3) } quintic &lt;- function(t) { 1/8 * (63*t^5 - 70*t^3 + 15*t ) } sextic &lt;- function(t) { 1/16 * (231*t^6 - 315*t^4 + 105*t^2 - 5 ) } septic &lt;- function(t) { 1/16 * (429*t^7 - 693*t^5 + 315*t^3 - 35*t ) } par(pty=&quot;m&quot;) curve( linear(x), -1, 1, col=&quot;brown&quot;, ylim=range(-1,1), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) curve( quadratic(x), -1, 1, col=&quot;darksalmon&quot;,add=TRUE) curve( cubic(x), -1, 1, col=&quot;green&quot;, add=TRUE) curve( quartic(x), -1, 1, col=&quot;darksalmon&quot;, add=TRUE) curve( quintic(x), -1, 1, col=&quot;navyblue&quot;, add=TRUE) #curve( sextic(x), -1, 1, col=&quot;green&quot;, add=TRUE) #curve( septic(x), -1, 1, col=&quot;brown&quot;, add=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) x_loc = c(0.75, 0.00, -0.45, 0.00, 0.55) y_loc = c(0.9, -0.6, 0.52, 0.45, 0.22) labels= c(&quot;line&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;quartic&quot;, &quot;quintic&quot;) text(x_loc, y_loc, labels=labels, offset=0.5, col=&quot;darkgreen&quot; ) Figure 3.13: Legendre Polynomial See more discussion of Legendre polynomials under Section Gaussian Quadrature in Chapter 4 (Numerical Calculus), including the formula used to derive the polynomials. We leave readers to investigate Chebyshev polynomials in the first and second kinds. For example: First kind: \\[\\begin{align*} T_0(x) {}&amp;= 1 \\\\ T_1(x) &amp;= x \\\\ T_2(x) &amp;= 2x^2 - 1 \\\\ T_3(x) &amp;= 4x^3 - 3x \\\\ T_4(x) &amp;= 8x^4 - 8x^2 + 1 \\\\ T_5(x) &amp;= 16x^5 - 20x^3 + 5x \\\\ \\end{align*}\\] Second kind: \\[\\begin{align*} U_0(x) {}&amp;= 1 \\\\ U_1(x) &amp;= 2x \\\\ U_2(x) &amp;= 4x^2 - 1 \\\\ U_3(x) &amp;= 8x^3 - 4x \\\\ U_4(x) &amp;= 16x^4 - 12x^2 + 1 \\\\ U_5(x) &amp;= 32x^5 - 32x^3 + 6x \\\\ \\end{align*}\\] Both Legendre polynomials and Chebyshev polynomials belong to a class of polynomials called Orthogonal polynomials, which are expressed using the following generalized equation: \\[\\begin{align} \\int_{a}^{b} P_m(x)P_n(x)W(x)dx = 0,\\ where\\ n=0,1,2,..,m-1 \\end{align}\\] Orthogonal polynomials require that \\(P_m(x)\\) and \\(P_n(x)\\) are orthogonal, \\(\\langle P_m,P_n \\rangle = 0\\), with respect to a weight function, \\(W(x) \\geq 0\\). This book does not cover properties of Orthogonal polynomials; however, it helps to be familiar with other orthogonal polynomials such as: Hermite Polynomials Jacobi Polynomials Laguerre Polynomials One crucial point to emphasize is the difference of slope between a curve and a line. The slope of a line is constant anywhere - any point in a line. On the other hand, the slope of a curve can vary from point to point depending on the curvature. That obviously makes curves non-linear. The importance of a slope in a curvature becomes more apparent once we cover piecewise polynomials. In the previous section, the coordinates of (x,y) are given to us. In Figure 3.14, the first graph illustrates a set of arbitrary points in the system. Such random points form a collinear pattern resembling a straight line. However, this is not always the case. Some datasets paint all sorts of patterns, including curve lines. Such datasets do not come short of unique problems and, at the same time, possibly unique solutions. Let us use Figure 3.14 to illustrate a point in this section and introduce a few solutions to polynomial equations. Figure 3.14: Polynomial and Interpolation 3.5.4 Non-Linear Regression Performing regression to fit a line - a linear regression - is no different a concept from a curve. Generally, we extend the concept of a linear regression, for which a line is in the order of a linear 1st-degree polynomial, into polynomial regression, for which a curve is in the order of a non-linear higher-degree polynomial. Note here that we emphasize higher-degree polynomial equations as non-linear. In this section, we do not cover other more complex non-linear equations such as ODEs/PDEs or equations with interactions and a combination of logarithms or exponentials. We cover ODE and PDE in the next chapter. We have already discussed line fitting in Linear 1st-degree polynomials. We extend that concept and use the term curve fitting or polynomial fitting in both linear first-degree polynomials and non-linear higher-degree polynomials. Here, we approximate the placement of a curve through a given set of data points. Depending on the dataset, there may be a need to employ a 2nd-degree polynomial (a quadratic line) or a 3rd-degree polynomial (a cubic line) to fit against the data. In a case where a dataset cannot sufficiently accommodate a 2nd-degree or a 3rd-degree polynomial, a higher degree terms such as 4th-degree (quartic), 5th-degree (quintic), and so on to construct the polynomial may be required at times. However, there are consequences to using higher-degree polynomials. One of them is losing the generality of a model because the more the curve fits perfectly against a given dataset, the further away we can adapt the model to fit against any other datasets. This is called over-fitting. Now, fitting a curve calls for the same matrix equation but is more generalized: \\[ Ax = b \\] Recall a simple slope formula, (\\(y = mx + c\\)), and its generalized form: \\[\\begin{align} \\hat{y} = \\sum_{i=0}^n c_ix_i + e\\ \\rightarrow\\ \\ \\ \\ \\hat{y} = c_0 + c_1x_1 + c_2x_2 +,...+, c_{n-1}x_{n-1} + c_{n}x_n + \\epsilon \\end{align}\\] Here, we extend that to show that polynomial terms are now in higher order: \\(O(x^{n})\\). \\[\\begin{align} \\hat{y} = \\sum_{i=0}^\\infty c_ix_i + e\\ \\rightarrow\\ \\ \\ \\ \\hat{y} = c_0 + c_1x^1 + c_2x^2 + c_3x^3 ,...+, c_{n-2}x^{n-1}+ O(x^{n}) + \\epsilon \\end{align}\\] The benefit is higher precision. Because higher order of x implies increasing the degree of precision, graphically, the line starts to curve towards every data point, as we see shortly. Let us illustrate polynomial regression by first introducing an arbitrary set of data points as shown below: \\[ \\left[\\begin{array}{r}x \\\\ y \\end{array}\\left|\\begin{array}{rrrrrrrrrrr} 1.1 &amp; 1.5 &amp; 2.5 &amp; 2.0 &amp; 3.0 &amp; 3.5 &amp; 4.0 &amp; 4.3 &amp; 5.0 &amp; 5.2\\\\ 6.8 &amp; 5.0 &amp; 3.2 &amp; 4.0 &amp; 5.0 &amp; 6.0 &amp; 6.3 &amp; 6.0 &amp; 3.0 &amp; 3.5 \\end{array}\\right.\\right] \\] Now, our intent is to fit a curve, which requires us to find all the coefficients that we can use to construct the polynomial that best fit the dataset. This is similar to finding the m slope and c intercept for a simple line-fitting, \\(y = mx + c\\), which we illustrated previously. Using the same formula, \\(A\\beta = y\\), let us first construct a Vandermonde matrix. To do that, we start with a system of polynomial equations using the data points like so: \\[ \\begin{array}{lll} 6.8 = c_0 + c_1\\times 1.1 + c_2\\times 1.1^2 + c_3\\times 1.1^3 + c_4\\times 1.1^4 + ... + O(x^k)\\\\ 5.0 = c_0 + c_1\\times 1.5 + c_2\\times 1.5^2 + c_3\\times 1.5^3 + c_4\\times 1.5^4 + ... + O(x^k) \\\\ 3.2 = c_0 + c_1\\times 2.5 + c_2\\times 2.5^2 + c_3\\times 2.5^3 + c_4\\times 2.5^4 + ... + O(x^k) \\\\ 4.0 = c_0 + c_1\\times 2.0 + c_2\\times 2.0^2 + c_3\\times 2.0^3 + c_4\\times 2.0^4 + ... + O(x^k) \\\\ 5.0 = c_0 + c_1\\times 3.0 + c_2\\times 3.0^2 + c_3\\times 3.0^3 + c_4\\times 3.0^4 + ... + O(x^k) \\\\ 6.0 = c_0 + c_1\\times 3.5 + c_2\\times 3.5^2 + c_3\\times 3.5^3 + c_4\\times 3.5^4 + ... + O(x^k)\\\\ 6.3 = c_0 + c_1\\times 4.0 + c_2\\times 4.0^2 + c_3\\times 4.0^3 + c_4\\times 4.0^4 + ... + O(x^k) \\\\ 6.0 = c_0 + c_1\\times 4.3 + c_2\\times 4.3^2 + c_3\\times 4.3^3 + c_4\\times 4.3^4 + ... + O(x^k) \\\\ 3.0 = c_0 + c_1\\times 5.0 + c_2\\times 5.0^2 + c_3\\times 5.0^3 + c_4\\times 5.0^4 + ... + O(x^k) \\\\ 3.5 = c_0 + c_1\\times 5.2 + c_2\\times 5.2^2 + c_3\\times 5.2^3 + c_4\\times 5.2^4 + ... + O(x^k) \\\\ \\end{array} \\] We then translate that stack of equations into a Vandermonde matrix of the kth-order. In this case, let us assume 4th order. \\[ Vandermonde(A) = \\left[\\begin{array}{rrrrrrr} 1 &amp; x_1 &amp; x_1^2 &amp; x_1^3 &amp; x_1^4\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; x_2^3 &amp; x_2^4 \\\\ 1 &amp; x_3 &amp; x_3^2 &amp; x_3^3 &amp; x_3^4 \\\\ 1 &amp; ... &amp; ... &amp; ... &amp; ...\\\\ 1 &amp; x_{10} &amp; x_{10}^2 &amp; x_{10}^3 &amp; x_{10}^4 \\\\ \\end{array}\\right] = \\left[\\begin{array}{rrrrrr} 1 &amp; 1.1 &amp; 1.1^2 &amp; 1.1^3 &amp; 1.1^4\\\\ 1 &amp; 1.5 &amp; 1.5^2 &amp; 1.5^3 &amp; 1.5^4 \\\\ 1 &amp; 2.5 &amp; 2.5^2 &amp; 2.5^3 &amp; 2.5^4 \\\\ 1 &amp; ... &amp; ... &amp; ... &amp; ...\\\\ 1 &amp; 5.2 &amp; 5.2^2 &amp; 5.2^3 &amp; 5.2^4\\\\ \\end{array}\\right] \\] Here, we are looking for the unknown coefficients denoted as \\(\\hat{\\beta} = \\{c_0, c_1, c_2, c_3, c_4\\}\\): \\[ \\hat{\\beta} \\approx \\left[ \\begin{array}{rrrr} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\end{array}\\right] \\] So what we get is the following: \\[ A\\hat{\\beta} \\approx y \\rightarrow A\\left[\\begin{array}{ccc} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\end{array}\\right] \\approx y \\ \\ \\rightarrow \\ \\ \\left[\\begin{array}{c} \\begin{array}{rrrrrr} 1 &amp; 1.1 &amp; 1.1^2 &amp; 1.1^3 &amp; 1.1^4 \\\\ 1 &amp; 1.5 &amp; 1.5^2 &amp; 1.5^3 &amp; 1.5^4 \\\\ 1 &amp; 2.5 &amp; 2.5^2 &amp; 2.5^3 &amp; 2.5^4 \\\\ 1 &amp; 2.0 &amp; 2.0^2 &amp; 2.0^3 &amp; 2.0^4 \\\\ 1 &amp; 3.0 &amp; 3.0^2 &amp; 3.0^3 &amp; 3.0^4 \\\\ 1 &amp; 3.5 &amp; 3.5^2 &amp; 3.5^3 &amp; 3.5^4 \\\\ 1 &amp; 4.0 &amp; 4.0^2 &amp; 4.0^3 &amp; 4.0^4 \\\\ 1 &amp; 4.3 &amp; 4.3^2 &amp; 4.3^3 &amp; 4.3^4 \\\\ 1 &amp; 5.0 &amp; 5.0^2 &amp; 5.0^3 &amp; 5.0^4 \\\\ 1 &amp; 5.2 &amp; 5.2^2 &amp; 5.2^3 &amp; 5.2^4 \\\\ \\end{array} \\end{array}\\right]_A \\left[\\begin{array}{c} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\end{array}\\right] \\approx \\left[\\begin{array}{c} \\begin{array}{r} 6.8 \\\\ 5.0 \\\\ 3.2 \\\\ 4.0 \\\\ 5.0 \\\\ 6.0 \\\\ 6.3 \\\\ 6.0 \\\\ 3.0 \\\\ 3.5 \\end{array} \\end{array}\\right]_y \\] To compute for the \\(\\hat{\\beta}\\) with the \\(c_i\\) coefficients, we use Equation \\(\\ref{eqn:eqnnumber3}\\) as before. That gives us the following approximate coefficients, \\(\\beta\\): \\[ \\hat{\\beta} \\approx \\left[\\begin{array}{c} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\\\ c_4 \\end{array}\\right] = \\left[\\begin{array}{r} 25.190 \\\\ -26.955 \\\\ 11.366 \\\\ -1.813 \\\\ 0.090 \\end{array}\\right] \\] With that approximate coefficients, we can finally construct our fit. \\[ P(x) = 25.190 -26.955x + 11.366x^2 -1.813x^3 + 0.090x^4 \\] That is our polynomial that best fits the given dataset at 4th degree. We implement the case above in R code to show other higher-order polynomials and how each one fits the dataset. It turns out that there are four polynomials, 3rd-order, 4th-order, 5th-order, and 6th order, that seem to fit the dataset. They all seem to follow the same pattern inside the range between the first and last data points but tend to be diverse towards other patterns outside the range. For example, the 5th-order polynomial assumes an upward start but then predicts that the next data point is in the upward direction. The 3rd-order and 4th-order polynomials, however, tend towards a downward direction. We somehow need to validate which one may best fit, but it requires extra new unforeseen datasets. In Chapter 9 (Computational Learning I), we will be covering cross-validation and other means to validate a model. Before that, our polynomial regression works only against the currently available dataset. Here is a naive implementation of the entire case in R code. We introduce different orders of polynomials and fit each one. It can be seen that the polynomial of the 6th degree seems to fit better than the others: # arbitrary data x = c(1.1, 1.5, 2.5, 2.0, 3.0, 3.5, 4.0, 4.3, 5.0, 5.2) y = c(6.8, 5.0, 3.2, 4.0, 5.0, 6.0, 6.3, 6.0, 3.0, 3.5) polynomial &lt;- function(x, beta) { n=length(beta) y = beta[1] for (i in 2:n) { y = y + beta[i] * x^(i-1) } y } lu_factoring &lt;- function(A, y) { LU = lu_decomposition_by_doolittle(A) uy = forward_sub(LU$lower, y) x = backward_sub(LU$upper, uy) } show_curve &lt;- function(x, y, order, ilwd, col) { n = length(x) lty = 2; lwd=1 for (i in 1:length(order)) { p = rep(1,n) for (k in 1:order[i]) { p = c(p, x^k) } A = matrix(p, n, byrow=FALSE) # build vandermonde matrix beta = lu_factoring(t(A) %*% A,t(A) %*% y ) if (ilwd[i] == TRUE) { lty = 1; lwd=2 } curve( polynomial(x, beta), 0, 6, col=col[i], add=TRUE, lty=lty, lwd=lwd) } } par(pty=&quot;m&quot;) plot(NULL, xlim=range(0,6), ylim=range(0,8), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) show_curve(x, y, c(1,2,3,4,5,6), c(FALSE, FALSE, FALSE, FALSE, FALSE, TRUE), c(&quot;navyblue&quot;, &quot;lightgreen&quot;, &quot;green&quot;, &quot;dodgerblue&quot;, &quot;orange&quot;, &quot;red&quot;)) points(x,y, col=&quot;dodgerblue&quot;, pch=16, lwd=2) x_loc = c(3.8, 2.5, 5.2, 5.7, 0.75, 4.0) y_loc = c(4.5, 5.5, 0.52, 2.0, 2.5, 6.8) labels= c(&quot;line&quot;, &quot;quadratic&quot;, &quot;cubic&quot;, &quot;quartic&quot;, &quot;quintic&quot;, &quot;sextic&quot;) text(x_loc, y_loc, labels=labels, offset=0.5, col=&quot;darkgreen&quot; ) Figure 3.15: Polynomial Regression Additional Polynomial regression techniques are available in the Polynomial Smoothing section. Also, see the Local Linear Regression section that uses smoothing techniques. 3.6 Approximating Polynomial Functions by Series Expansion There is a difference between Polynomial Regression and Polynomial Approximation. The former performs curve fitting against an arbitrary dataset. The latter performs curve fitting against an arbitrary polynomial function. While both are considered an approximation, they differ in what they are approximating. This section covers three known series that we can use to perform curve fitting to a function - otherwise also regarded as Polynomial approximation. It can also be considered Function Optimization. Here, we approximate a given polynomial function by: using Maclaurin series \\[\\begin{align} p(x) = \\sum_{n=0}^\\infty \\left( \\frac{f^{(n)}(0)x^n}{n!}\\right) \\end{align}\\] which can be expanded into this: \\[\\begin{align} p(x) = f(0) + f&#39;(0)\\frac{x^1}{1!} + f&#39;&#39;(0)\\frac{x^2}{2!} + f&#39;&#39;&#39;(0)\\frac{x^3}{3!} +\\ ...\\ + f^{(n)}(0)\\frac{x^n}{n!} +\\ ... \\end{align}\\] using Taylor series \\[\\begin{align} p(a) = \\sum_{n=0}^\\infty \\left( \\frac{f^{(n)}(a)(z)^n}{n!}\\right) \\end{align}\\] which can be expanded into this: \\[\\begin{align} {}&amp;Given:\\ z\\ =\\ x\\ -\\ a \\nonumber \\\\ \\nonumber \\\\ &amp;p(a) = f(a) + f&#39;(a)\\frac{z^1}{1!} + f&#39;&#39;(a)\\frac{z^2}{2!} + f&#39;&#39;&#39;(a)\\frac{z^3}{3!} +\\ ...\\ + f^{(n)}(a)\\frac{z^n}{n!} +\\ ... \\end{align}\\] using Fourier series \\[\\begin{align} p(a) = \\sum_{n=0}^\\infty \\left( \\frac{f^{(n)}(a)(z)^n}{n!}\\right) \\end{align}\\] which can be expanded into this: \\[\\begin{align} {}&amp;Given:\\ z\\ = e^{i\\theta},\\ \\ \\ \\ where\\ e^{i\\theta} = cos\\theta + i\\cdotp sin\\theta \\nonumber \\\\ \\nonumber \\\\ &amp;p(a) = f(a) + f&#39;(a)\\frac{z^1}{1!} + f&#39;&#39;(a)\\frac{z^2}{2!} + f&#39;&#39;&#39;(a)\\frac{z^3}{3!} +\\ ...\\ + f^{(n)}(a)\\frac{z^n}{n!} +\\ ... \\end{align}\\] We illustrate curve-fitting using one of the series above. Let us try to approximate the following curve using Taylor Series: \\[ f(x) = 2x^3 + 5x^2 - 9x + 3,\\ \\ \\ \\ \\ where\\ \\ x=2 \\] First, let us compute for the derivatives of the function, f(x): \\[\\begin{align*} f(x) {}&amp;= 2x^3 + 5x^2 - 9x + 3 &amp;\\rightarrow f(2) {}&amp;= 21\\\\ f&#39;(x) &amp;= 6x^2 + 10x - 9 &amp;\\rightarrow f&#39;(2) &amp;= 35\\\\ f&#39;&#39;(x) &amp;= 12x + 10 &amp;\\rightarrow f&#39;&#39;(2) &amp;= 34 \\\\ f&#39;&#39;&#39;(x) &amp;= 12 &amp;\\rightarrow f&#39;&#39;&#39;(2) &amp;= 12\\\\ f^{(4)}(x) &amp;= 0 &amp;\\rightarrow f^{(4)}(2) &amp;= 0 \\\\ \\end{align*}\\] Second, using Taylor Series, we get: \\[\\begin{align} p(x) {}&amp;= \\sum_{n=0}^\\infty \\left( \\frac{f^{(n)}(2)(x-2)^n}{n!}\\right) \\\\ p(x) &amp;= f(2) + f&#39;(2)\\frac{z^1}{1!} + f&#39;&#39;(2)\\frac{z^2}{2!} + f&#39;&#39;&#39;(2)\\frac{z^3}{3!}, \\ \\ where \\ z = x - 2 \\\\ p(x) &amp;= 21 + 35\\frac{z^1}{1} + 34\\frac{z^2}{2} + 12\\frac{z^3}{6} \\nonumber \\\\ p(x) &amp;= 21 + 35z + 17z^2 + 2z^3 \\nonumber \\\\ p(x) &amp;= 21 + 35(x-2) + 17(x-2)^2 + 2(x-2)^3 \\end{align}\\] We then can use the derived polynomial, \\(p(x)\\), to approximate the actual curve. We discuss more of Fourier Series to derive methods in Chapter 4 (Numerical Calculus). Taylor series is covered more under Laplace Approximation Section in Chapter 8 (Bayesian Computation II). 3.7 Approximating Polynomial Functions by Interpolation Interpolation is a method used to numerically approximate an unknown data point that is either missing or hidden in a range of a given dataset such that it allows us to refurbish the missing data back into the range using the approximated data point. In some cases, this allows us to smoothen the curvature of a curve; in other cases, it allows us to merely fill the gap in a dataset - especially when completing data in tabular form. In hindsight, it is essential to understand the difference between Polynomial Regression and Polynomial Interpolation. If a dataset resembles a curvy pattern but is scattered in a way that we cannot get a curve to pass through the data points, what we can do is approximate the closest curve that we can fit through the data points - this is fitting a curve - this is regression. And if a dataset resembles a curvy pattern and we can get a curve to pass through all the data points, then we can use Interpolation to add more points through the curve. Both methods yield a polynomial expression that we can use as a model to predict the value of y given the value of x in any context that represents both y and x. For example, predicting the weather (y) given the condition of air (x). See Figure 3.16. Figure 3.16: Regression vs Interpolation Additionally, in this section, we will be discussing in detail spline regression (or curve fitting) and spline interpolation as a natural next topic for polynomial discussion. There are a few methods of interpolation we introduce in this section: 3.7.1 Polynomial interpolation The goal is to be able to construct a polynomial equation that can help us to obtain the unknown data points within the dataset. Given a set of data points (the initial range of dataset), determine the coefficients required to formulate the following polynomial equation: \\[\\begin{align} P(x) = a_o + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^{n-1} \\end{align}\\] First, the set of data points (dataset) could be arbitrary (mostly derived from an observation of an event). Here, a dataset is as simple as a list of x-y coordinates like so: (1,1), (3,4), (3,14). Second, to explain the case, we use Runge function to generate our sample data points. Our strategy is to build a uniform sequence of x values between [-1,1]. In R code, here is the sequence of x values with a fixed interval of 0.25: n = 8 seq(-1,1, length.out = n + 1) ## [1] -1.00 -0.75 -0.50 -0.25 0.00 0.25 0.50 0.75 1.00 Third, we feed the generated sequence of x values to the Runge function to give us our y values: n = 8 x = seq(-1,1, length.out = n + 1) runge &lt;- function(x) { 1/(1 + 25*x^2) } (y = round(runge(x),2)) ## [1] 0.04 0.07 0.14 0.39 1.00 0.39 0.14 0.07 0.04 So now we are able to obtain our data set: \\[ \\left[\\begin{array}{r}x \\\\ y\\end{array}\\left|\\begin{array}{rrrrrrrrrrr} -1.00 &amp; -0.75 &amp; -0.50 &amp; -0.25 &amp; 0.00 &amp; 0.25 &amp; 0.50 &amp; 0.75 &amp; 1.00\\\\ 0.04 &amp; 0.07 &amp; 0.14 &amp; 0.39 &amp; 1.00 &amp; 0.39 &amp; 0.14 &amp; 0.07 &amp; 0.04 \\end{array}\\right.\\right] \\] Our goal here is to formulate a polynomial using the dataset we obtained from the Runge function that will eventually help us perform an interpolation. Figure 3.17 shows a list of polynomial interpolations with different degrees. It can be observed that those interpolations do not seem to fit the Runge curve perfectly. For example, the polynomial with degree 10 perfectly fits the peak of the Runge curve, but then the oscillation gets larger (worse) at the edges, suggesting an extremely inaccurate interpolation. On the other hand, the polynomial of degree 9 seems to demonstrate being closer to the Runge curve, but still not quite. runge &lt;- function(x) { 1/(1 + 25*x^2) } uniform &lt;- function(n) { x = seq(0, n, 1); 2*x / n - 1 } chebyshev &lt;- function(n) { x = seq(0, pi, length.out=n + 1); -cos(x) } vandermonde &lt;- function(xi, n) { m &lt;- matrix(, nrow=0, ncol=n + 1) for (i in 0:n) { v = c(1) for (p in 0:n) { v[p + 1] = xi[i + 1]^p } m &lt;- rbind(m, v) } m } polynomial &lt;- function(coeffs, t, degree) { p = 0 for (d in 0:degree) { p = p + coeffs[d + 1] * t^d } p } interpolate &lt;- function(degree, col, plot=FALSE) { n=degree x = uniform(n) A = vandermonde(x, n) coeffs = solve(A, runge(x)) yi = c() n = 200 x = seq(-1, 1, length.out=n) for (i in 1:n) { yi[i] = polynomial(coeffs, x[i], degree) } if (plot == TRUE) { lines(x, yi , col=col, lwd=1) } yi } plot(NULL, xlim=range(-1,1), ylim=range(-1,2), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) abline(h=0, col=&quot;darksalmon&quot;) grid(lty=3, col=&quot;lightgrey&quot;) y = interpolate(degree=5, &quot;dodgerblue&quot;, TRUE) y = interpolate(degree=9, &quot;lightgreen&quot;, TRUE) y = interpolate(degree=10, &quot;lightblue&quot;, TRUE) curve(runge(x), -1, 1, col=&quot;red&quot;, lwd=2, add=TRUE) legend(-0.4, 2, c( &quot;runge function&quot;, &quot;polynomial interpolation (degree=5)&quot;, &quot;polynomial interpolation (degree=9)&quot;, &quot;polynomial interpolation (degree=10)&quot; ), fill=c(&quot;red&quot;, &quot;dodgerblue&quot;, &quot;lightgreen&quot;, &quot;lightblue&quot;, &quot;lightgrey&quot;), horiz=FALSE, cex=0.8) Figure 3.17: Polynomial Interpolation At a higher degree of 10, the oscillation gets larger (and worse), suggesting an inaccurate interpolation. For example, below, the interpolated data point at the oscillation peak is at (-0.9296482, 1.9258452). And it is way beyond the Runge curve. n = 200 x = seq(-1, 1, length.out=n) y = interpolate( degree=10, &quot;lightblue&quot;) (x_y = list( &quot;x&quot; = x[1:10], &quot;y&quot; = y[1:10])) ## $x ## [1] -1.0000000 -0.9899497 -0.9798995 -0.9698492 -0.9597990 -0.9497487 ## [7] -0.9396985 -0.9296482 -0.9195980 -0.9095477 ## ## $y ## [1] 0.03846154 0.72903685 1.23455801 1.58523834 1.80773572 1.92546758 ## [7] 1.95890471 1.92584519 1.84166913 1.71957548 c(x_y$x[8], x_y$y[8]) ## [1] -0.9296482 1.9258452 Try to run the R code using the Chebyshev interval and see what happens. Now let us look at other interpolation methods to see if we obtain a better result. 3.7.2 Lagrange interpolation This interpolation uses the following equation: \\[\\begin{align} L(x) = \\sum_{j=0}^k y_j l_j(x) \\end{align}\\] with the Lagrange polynomial formula as our basis function (Berrut JP and Trefethen L. N. 2012; Das B. and Chakrabarty D. 2016): \\[\\begin{align} l_j(x) = \\prod_{i = 0,\\ i \\ne j}^k \\frac{x - x_i}{x_j - x_i} = \\frac{(x - x_0)}{(x_j - x_0)} \\frac{(x - x_1)}{(x_j - x_1)} ... \\frac{(x - x_k)}{(x_j - x_k)} \\end{align}\\] where k = number of data points. Similar to Polynomial interpolation, the Lagrange interpolation demonstrates an also observed oscillation at the end of the curves. In expanded form, we get the following equation (where \\(i \\ne j\\)): \\[\\begin{align} L(x) &amp;= y_0 + y_{j=1}\\left(\\frac{(x-x_{i=0})}{(x_{j=1}-x_{i=0})} ...\\frac{(x-x_{i=k})}{(x_{j=1}-x_{i=k})}\\right) + ... \\nonumber \\\\ &amp; + y_{j=k}\\left(\\frac{(x-x_{i=0})}{(x_{j=k}-x_{i=0})} ...\\frac{(x-x_{i=k})}{(x_{j=k}-x_{i=k})}\\right) \\end{align}\\] As long as \\(i \\ne j\\) is a constraint, then \\((x_{j=k}-x_{i=k})\\) will not happen. The R code and result of Lagrange interpolation is demonstrated in Figure 3.18 using Chebyshev interval. Try to run the R code but using uniform interval. Compare the result against Figure 1.1. Also, for other functions, try to use \\(exp(x)\\) instead of \\(runge(x)\\) for experiment. runge &lt;- function(x) { 1/(1 + 25*x^2) } uniform &lt;- function(n) { x = seq(0, n , 1 ); 2*x / n - 1 } chebyshev &lt;- function(n) { x = seq(0, pi, length.out=n + 1 ); -cos(x)} lagrange &lt;- function(x, y, new_x) { p = rbind(x, y) k = ncol(p) Lx = 0 for (j in 1:k) { xj = p[,j][&quot;x&quot;] yj = p[,j][&quot;y&quot;] lj_x = 1 for (i in 1:k) { if (j != i) { xi = p[,i][&quot;x&quot;] lj_x = lj_x * (new_x - xi) / (xj - xi) } } Lx = Lx + yj * lj_x } Lx } interpolate &lt;- function(x, y) { x_n = seq(-1, 1, length.out=200) order = c( 11, 13, 15) col = c(&quot;dodgerblue&quot;, &quot;lightgreen&quot;, &quot;lightblue&quot;, &quot;lightgrey&quot;) l = length(order) xn = c() for (n in 1:l) { #x = uniform(order[n] ) x = chebyshev(order[n]) y_n = lagrange(x, runge(x), x_n) lines(x_n, y_n, col=col[n], lwd=1) if (n==1) { xn = x } } xn } x = seq(-1, 1, length.out=20) y = runge(x) plot(NULL, xlim=range(-1,1), ylim=range(-1,2), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) abline(h=0, col=&quot;darksalmon&quot;) grid(lty=3, col=&quot;lightgrey&quot;) xn = interpolate(x, y) curve(runge(x), -1, 1, col=&quot;red&quot;, lwd=2, add=TRUE) points(xn, runge(xn), col=&quot;dodgerblue&quot;, pch=16, lwd=2) legend(-0.4, 2, c( &quot;runge function&quot;, &quot;lagrange interpolation (n=12)&quot;, &quot;lagrange interpolation (n=14)&quot;, &quot;lagrange interpolation (n=16)&quot;), fill=c(&quot;red&quot;, &quot;dodgerblue&quot;, &quot;lightgreen&quot;, &quot;lightblue&quot;, &quot;lightgrey&quot;), horiz=FALSE, cex=0.8) Figure 3.18: Lagrange Interpolation (Chebyshev Interval) We leave readers to also investigate a variant formulation of the Lagrange interpolation using Barycentric formula (Berrut JP and Trefethen L. N. 2012; Das B. and Chakrabarty D. 2016). 3.7.3 Newton interpolation Newton interpolation uses the following equation with a divide difference component, \\(f[x_j]\\), and with newton basis polynomials (Berrut JP and Trefethen L. N. 2012). \\[\\begin{align} N(x) = \\sum_{j=0}^k f[x_j] n_j(x) \\label{eqn:eqnnumber5} \\end{align}\\] with the Newton polynomial formula as our basis function: \\[\\begin{align} n_j(x) = \\prod_{i = 0}^{j-1} (x - x_i) = (x - x_0) (x - x_1) ... (x - x_{j - 1}) \\end{align}\\] Divide Difference expression: \\[ f[x_j] = [y_0, ..., y_j] \\] Suppose in the expression that j = 5th order, then we can reference Figure 3.19. The table in the Figure 3.19 is a divide-difference table only up to the 5th order. Note that we are using a triangular scheme in this discussion. Figure 3.19: Divide Difference Table The following formula for divide-difference applies to the 4th order: \\[\\begin{align} \\text{1st order difference} = f[x0, x1] {} &amp;= \\frac{f(x1) - f(x0)}{x1 - x0} \\\\ \\text{2nd order difference} = f[x0, x1,x2] &amp;= \\frac{f[x1, x2] - f[x0, x1]}{x2 - x0}\\\\ \\text{3rd order difference} = f[x0, x1, x2, x3] &amp;= \\frac{f[x1, x2, x3] - f[x0, x1, x2]}{x3 - x0} \\\\ \\text{4th order difference} = f[x0, x1, x2, x3, x4] &amp;= \\frac{f[x1, x2, x3, x4] - f[x0, x1, x2, x3]}{x4 - x0} \\end{align}\\] To give an example, suppose we use Runge function for the divide-difference. Let us initialize the x-y value pair in R code: runge &lt;- function(x) { 1/(1 + 25*x^2) } n = 4 (xi = seq(-1,1, length.out = n + 1)) # uniform interval ## [1] -1.0 -0.5 0.0 0.5 1.0 (yi = round(runge(xi),2)) # yi = f[xi] ## [1] 0.04 0.14 1.00 0.14 0.04 Using the generated x-y pair (xi, f[xi]), we can then construct the triangular divide-difference table like so: \\[ \\left[ \\begin{array}{lr} xi \\\\ .\\\\ x_0 =&amp; -1.00 \\\\ x_1 =&amp; -0.50 \\\\ x_2 =&amp; 0.00 \\\\ x_3 =&amp; 0.50 \\\\ x_4 =&amp; 1.00 \\end{array} \\left|\\begin{array}{r} yi = f(xi) \\\\ .\\\\ 0.04 \\\\ 0.14\\\\ 1.00 \\\\ 0.14 \\\\ 0.04 \\end{array}\\right. \\left|\\begin{array}{r} 1st\\ order \\\\\\Delta^1y_1 \\\\ 0.20 \\\\ 1.72 \\\\ -1.72 \\\\ -0.20\\\\ .\\end{array}\\right. \\left|\\begin{array}{r} 2nd\\ order \\\\\\Delta^2y_2 \\\\ 1.52 \\\\ -3.44 \\\\ 1.52 \\\\ . \\\\ .\\end{array}\\right. \\left|\\begin{array}{r} 3rd\\ order \\\\ \\Delta^3y_3 \\\\ -3.31 \\\\ 3.31 \\\\ . \\\\ .\\\\ .\\end{array}\\right. \\left|\\begin{array}{r} 4th\\ order \\\\\\Delta^4y_4 \\\\ 3.31 \\\\ . \\\\ .\\\\ .\\\\. \\end{array}\\right. \\right] \\] For the First Order: \\[\\begin{align*} f[x0, x1] &amp;= \\frac{0.14-0.04}{-0.50 +1.00} = 0.20\\\\ f[x1, x2] &amp;= \\frac{1.00-0.14}{-0.00 +0.50} = 1.72\\\\ f[x2, x3] &amp;= \\frac{0.14-1.00}{0.50 +0.00} = -1.72\\\\ f[x3, x4] &amp;= \\frac{0.04-0.14}{1.00 -0.50} = -0.20 \\end{align*}\\] For the Second Order: \\[\\begin{align*} f[x0, x1, x2] &amp;= \\frac{1.72-0.20}{-0.00 +1.00} = 1.52 \\\\ f[x1, x2, x3] &amp;= \\frac{-1.72-1.72}{0.50 +0.50} = -3.44 \\\\ f[x2, x3, x4] &amp;= \\frac{-0.20+1.72}{1.00 +0.00} = 1.52 \\end{align*}\\] For the Third Order: \\[\\begin{align*} f[x0, x1, x2, x3] &amp;= \\frac{-3.44-1.52}{0.50 +1.00} = -3.31 \\\\ f[x1, x2, x3, x4] &amp;= \\frac{1.52+3.44}{1.00 +0.50} = 3.31 \\end{align*}\\] For the Fourth Order: \\[\\begin{align*} f[x0, x1, x2, x3, x4] &amp;= \\frac{3.31+3.31}{1.00 +1.00} = 3.31 \\end{align*}\\] And to show that in R code: runge &lt;- function(x) { 1/(1 + 25*x^2) } divide_difference &lt;- function(x, y) { n = length(y) o = matrix (rep(0, n^2), n) o[,1] = y for (i in 1:(n-1)) { for (j in 1:(n-i)) { o[j,i+1] = (o[j+1, i] - o[j, i]) / (x[j+i] - x[j]) } } colnames(o) &lt;- c(&quot;f(xi)&quot;, &quot;1st order&quot;, &quot;2nd order&quot;, &quot;3rd order&quot;, &quot;4th order&quot;) o } n = 4 xi = seq(-1,1, length.out = n + 1) # uniform interval yi = round(runge(xi),2) # yi = f[xi] round(divide_difference(xi, yi),2) ## f(xi) 1st order 2nd order 3rd order 4th order ## [1,] 0.04 0.20 1.52 -3.31 3.31 ## [2,] 0.14 1.72 -3.44 3.31 0.00 ## [3,] 1.00 -1.72 1.52 0.00 0.00 ## [4,] 0.14 -0.20 0.00 0.00 0.00 ## [5,] 0.04 0.00 0.00 0.00 0.00 Therefore, the divide difference method yields: \\[ f[x_j] = f[x_0, ... x_4] = 3.31\\ \\ \\ \\sim \\text{3.32, if 3-decimal precision} \\] We can solve for N(x) from Equation (\\(\\ref{eqn:eqnnumber5}\\)) using R code. The expanded version is written as such: \\[\\begin{align*} N(x) {}&amp;= f[x_0] + \\Delta^1y_1(x-x_0) + \\Delta^2y_2(x-x_0)(x-x_1) + ... \\\\ &amp;+ \\Delta^ky_k(x-x_0) ...(x - x_k) \\end{align*}\\] which is also the same as: \\[\\begin{align*} N(x) {}&amp;= f[x_0] + f[x_0, x_1](x-x_0) + f[x_0, x_1, x_2](x-x_0)(x-x_1) + ... \\\\ &amp;+ f[x_0, ... , x_k](x-x_0) ...(x - x_k) \\end{align*}\\] The R code and result of Newton interpolation is demonstrated in Figure 3.20 using Chebyshev interval. Try to run the R code but using uniform interval. Compare the result against Figure 1.1. Also, for other functions, try to use \\(exp(x)\\) instead of \\(runge(x)\\) for experiment. Figure 3.20: Newton Interpolation (Chebyshev Interval) 3.7.4 Newton Forward interpolation Newton Forward interpolation is another version of Newton interpolation. Below is the formula for the interpolation. \\[\\begin{align} N_{fw}(x) = f(x) + \\Delta y_n u + \\Delta^2y_n\\frac{u(u-1)}{2!} + \\Delta^3y_n\\frac{u(u-1)(u-2)}{3!} + ... \\end{align}\\] where: \\[ u = \\frac{(x - x_n)}{h}, \\text{derived from } x = xn + uh \\] The method also requires the divide difference table to solve for the polynomial. Similar to the Newton interpolation, once a dataset (e.g., Runge function dataset) is provided, it is run against the divide difference algorithm to get the differences, \\(\\Delta^n y_n\\), which will be used to construct the polynomial needed to solve for \\(N_{fw}(x)\\). 3.7.5 Newton Backward interpolation Newton Backward interpolation is identical to the Newton Forward interpolation except the basis uses addition instead of subtraction. \\[\\begin{align} N_{bw}(x) = f(x) + \\nabla y_n u + \\nabla^2y_n\\frac{u(u+1)}{2!} + \\nabla^3y_n\\frac{u(u+1)(u+2)}{3!} + ... \\end{align}\\] where: \\[ u = \\frac{(x - x_n)}{h}, \\text{derived from } x = xn + uh \\] Here, similar to Newton Forward interpolation, we also run against the divide difference algorithm to get the differences, \\(\\nabla^n y_n\\), which will be used to construct the polynomial needed to solve for \\(N_{bw}(x)\\). 3.7.6 Interpolation Considerations Seeing that some interpolations tend to show instability, it is not a wonder to have many other numerical methods of interpolations today. Though this book does not cover them all in detail, there are other interpolations that are discussed in other literature; and it helps to be familiar with other interpolation methods and understand both the advantages and disadvantages. Just a few are listed below: Gauss Interpolation Stirling Interpolation Bessel Interpolation Hermite Interpolation Aitken Interpolation Also, instead of using other interpolation methods, one technique that can mitigate the oscillation is by avoiding Uniform intervals; instead, use Chebyshev intervals as reflected in the R code samples provided for Lagrange interpolation and Newton interpolation. While this helps reduce oscillation, there are also other considerations to take when interpolating. For example, will the derived polynomial being used for interpolation have to be re-constructed for every new data point? 3.7.7 Lebesque Constant Suppose we have decided to use a Chebyshev cubic polynomial of the first kind. \\[ T_3(x) = 4x^3 - 3x \\] Suppose also that we can construct an interpolant while trying to approximate the Chebyshev polynomial. \\[ P(x) = 5x^3 - 4x \\] Here is an R code to show the outcome. See Figure 3.21. chebyshev &lt;- function(x) { 4*x^3 - 3*x } interpolant &lt;- function(x) { 5*x^3 - 4*x } x = seq(-1, 1, length.out=20) plot(NULL, xlim=range(-1,1), ylim=range(-2,2), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(chebyshev(x), -1, 1, col=&quot;navyblue&quot;, lwd=2, add=TRUE) curve(interpolant(x), -1, 1, col=&quot;darksalmon&quot;, lwd=2, add=TRUE) legend(0, 2, c( &quot;chebyshev&quot;, &quot;interpolant&quot;), fill=c(&quot;navyblue&quot;, &quot;darksalmon&quot;), horiz=FALSE, cex=0.8) Figure 3.21: Lebesgue Constant Notice that the difference between the two polynomial is in the coefficients. For example: \\[\\begin{align*} \\text{ chebyshev polynomial } = (4, 3) \\\\ \\text{ interpolant } = (5,4) \\end{align*}\\] Early in this chapter, we described the sensitivity of values and the condition number of a system in terms of the ratio of change between input and output. Considering those, let us then come back to put some intution into the relative goodness of an interpolant, computationally (or mathematically). If this is about linear regression, we can say that the Residual may be a good starting point to determine the goodness of a fit; similarly, there is a constant we can use to effectively measure the sensitivity or conditioning of an interpolant. This constant is the Lebesgue constants. We know that the conditioning of a system is measured using the following formula: \\[\\begin{align} \\text{Condition Number} = \\frac{\\text{Relative Change in Output}}{\\text{Relative Change in Input}} \\end{align}\\] Using the formula, let us compute for the relative change of polynomial. \\[\\begin{align} \\text{Relative Change of Polynomial} = \\frac{P(x) - T_3(x)}{T_3(x)} \\end{align}\\] We can also compute for the relative change of the coefficients. Let v be a vector (4,3) and \\(\\hat{v}\\) be a vactor of change (5,4). We get. \\[\\begin{align} \\text{Relative Change of Coefficients} = \\frac{\\hat{v} - v}{v} \\end{align}\\] Here is the R code: chebyshev &lt;- function(x) { 4*x^3 - 3*x } interpolant &lt;- function(x) { 5*x^3 - 4*x } relative_change &lt;- function(a, b) { mean((a-b)/b)} x = seq(-1, 1, length.out=20) # Start from a polynomial with no change and # compute for the relative error. T_x = P_x = chebyshev(x) (relative_error = relative_change(P_x, T_x)) ## [1] 0 # Now introduce the interpolant function and # compute for the relative error. P_x = interpolant(x) (relative_error = relative_change(P_x, T_x)) ## [1] 0.2361586 # Next compute for the relative error of a change in coefficients. v = c(4,3) v_hat = c(5,4) (relative_error = relative_change(v_hat, v)) ## [1] 0.2916667 To now compute for the condition number, we use the following formula: \\[\\begin{align} \\text{Condition Number} (\\Lambda) = \\frac{\\text{Relative Change of Polynomial}}{\\text{Relative Change of Coefficients}} \\end{align}\\] # Compute for the condition number (condition_number = relative_change(P_x, T_x) / relative_change(v_hat, v) ) ## [1] 0.8096866 Here, \\(\\Lambda\\) is the Lebesgue constant used as a conditioning number and in our computation, the Lebesgue constant equals 0.8096866. 3.7.8 Horner’s method One way to evaluate an interpolant (its polynomial expression) is by using the Horner’s method. Let us use Chebyshev polynomial of degree 5 in the first kind to explain the method: \\[ P_5(x) = 16x^5 - 20x^3 + 5x \\] This Chebyshev polynomial expands into the following Horner’s equation: \\[ H(x) = 0 + x(5 + x(0 + x(-20 + x(0 + x(16))))) \\] Without loss of generality, the Horner’s method is expressed as: \\[\\begin{align} p(x) = a_o + x(a_1 + x(a_2 + x (a_3 + ... + x(a_{n-1} + a_nx)))) \\end{align}\\] for the following equation: \\[ p(x) = a_o + a_1x + a_2x^2 + a_3x^3 + ... + a_nx^n \\] To show the method implemented in R code, let us evaluate \\(P_5(X)\\) and \\(H(x)\\): polynomial &lt;- function(A,x) { p = 0 for (i in 1:length(A)) { p = p + A[i] * x^(i-1) } p } horner_rule &lt;- function(A,x) { p = 0 for (i in length(A):1) { p = A[i] + x * p } p } A = c(16, 0, -20, 0, 6, 0) x = 1 # evaluate if polynomial result is identical to horner rule result identical( polynomial(A,x) , horner_rule(A,x) ) ## [1] TRUE There are other methods to be familiar with when evaluating other forms of polynomials: Cleanshaw Method - to evaluate Chebyshev polynomials. De Boor’s Method - to evaluate Spline polynomials. In the next section, we will begin to cover Splines. 3.7.9 Piecewise Polynomial Interpolation So far, we have covered Lagrange interpolation and Newton interpolation. In our examples, we have used Runge’s function to sample our dataset for interpolation. We can observe that the two interpolations, including the Monomial interpolation, do manifest oscillation behavior at the end of the curves when using uniform intervals, manifesting the Runge’s phenomenon in Figure 1.1. Also, for interpolation, we have only introduced polynomials in 2nd, 3rd, 4th, or even 5th-degree terms - so-called the power-series polynomials. One point to emphasize in Figure 3.13 is that the rendered polynomials are rather regular curves. In some, if not most, situations, however, datasets tend to characterize complex, irregular patterns or curves. See Figure 3.22. Figure 3.22: Irregular Curves In a previous discussion in linear equations, we deal with polynomials of 1st degree, e.g. \\[ y = mx + b \\] Polynomials of higher degree however are non-linear and curvy and have the following equation: \\[\\begin{align} y = c_1x^0 + c_2x^1 + c_3x^2 + ... + c_{n}x^{n-1} \\end{align}\\] But even such polynomial equations may have limitations in terms of flexibility in handling irregular curves such as those in Figure 3.22. One way to handle such irregularity is not to take the curves in one fitting. Rather, cut the curves into pieces and deal with each piece individually - we call this method piecewise polynomial interpolation. The entire curve is called the spline. The slices of the spline are called spline segments or spline curves. The points where spline segments are connected to form two continuous curves and eventually make the whole spline continuous are called interior knots or just knots. Natural Cubic Spline A slice described as a 3rd degree polynomial - e.g. \\(ax^3 + bx^2 + cx + d\\) - is called a cubic spline segment. A function that holds an expression for each of the cubic spline segment is a spline segment function. Because each segment describes a 3rd-degree polynomial, we also can say that the segments are cubic polynomials. Figure 3.23: A spline In Figure 3.23, the spline runs through four knots, {\\(t_0=1, t_1=2, t_2=3, t_n=4\\)} where n = 3, and sliced into three spline segments, S = {\\(s_0, s_1, s_k\\)}, where k is the degree or order of the segment (in this case, k = n - 1), corresponding to three intervals or three spline functions, \\(\\phi_i(t)\\) = {\\(\\phi_0(t), ..., \\phi_2(t)\\)}. Each segment has two points at a given abscissa t that can be expressed in terms of time - equivalently, positioned at the x location in the x-axis. The spline segments ( n = k + 1 ) : \\[\\begin{align} s_0 {}&amp;= \\phi_0(t) \\leftarrow \\text{1st interval} \\\\ s_1 &amp;= \\phi_1(t) \\leftarrow \\text{2nd interval} \\\\ s_2 &amp;= \\phi_2(t) \\leftarrow \\text{3rd interval} \\end{align}\\] \\[ \\phi_i(t)\\ \\ \\ \\ \\ \\ \\text{where }i = 0\\ ...\\ n\\ \\ \\ and\\ n = \\text{number of splines or intervals} \\] The knots ( n + 1): \\[\\begin{align} \\phi_0(t_0) {}&amp; \\leftarrow \\text{1st endpoint knot} \\\\ \\phi_0(t_1) &amp; \\leftarrow \\text{1st interior knot} \\rightarrow \\text{connecting to } \\phi_1(t_1) \\\\ \\phi_1(t_2) &amp; \\leftarrow \\text{2nd interior knot} \\rightarrow \\text{connecting to } \\phi_2(t_2) \\\\ \\phi_3(t_3) &amp; \\leftarrow \\text{2nd endpoint knot} \\end{align}\\] Constraining a Natural Cubic Spline: First, for equations describing the cubic polynomials at the knots: We know that the 1st spline segment s0 starts at t0 and ends at t1, the 2nd spline segment s1 starts at t1 and ends at t2, and so on. Therefore, we have six equations for all four points of the three cubic spline segments. \\[\\begin{align} \\phi_0(t_0) {}&amp;= y0 = a_0t_0^3 + a_1t_0^2 + a_2t_0 + a_3 &amp; \\phi_0(t_1) {}&amp;= y1 = a_0t_1^3 + a_1t_1^2 + a_2t_1 + a_3 \\\\ \\phi_1(t_1) &amp;= y1 = b_0t_1^3 + b_1t_1^2 + b_2t_1 + b_3 &amp; \\phi_1(t_2) &amp;= y2 = b_0t_2^3 + b_1t_2^2 + b_2t_2 + b_3 \\\\ \\phi_2(t_2) &amp;= y2 = c_0t_2^3 + c_1t_2^2 + c_2t_2 + c_3 &amp; \\phi_2(t_3) &amp;= y3 = c_0t_3^3 + c_1t_3^2 + c_2t_3 + c_3 \\end{align}\\] Looking closely at each of the equations, we have four unknowns, e.g. {\\(a_0, a_1, a_2, a_3\\)}, for each of our spline functions. We have twelve unknowns (four unknowns times three segments). The equations above represent rules (or constraints) that require us to ensure our segments go through the points. So that gives us 2n constraints equivalent to six equations. Second, for equations describing the slopes of the three connecting knots: The fact that we see in Figure 3.23 a continuous curve means that the three spline segments have to be connected smoothly. To re-iterate, each knot between two segments has to be smoothly connected - or twice continuously differentiable. To show this smoothness in an equation, we need to show that the slope of the curve at the ending knot - that is located at abscissa t1 - of the first spline s0 is the same slope at the starting knot - that is still at abscissa t1 - of the second spline s1. And because a slope can be computed as the 1st derivative of the spline functions of s0 and s1 at a given point (t), we then can express the equality of those slopes this way: \\[\\begin{align} \\phi_0^{&#39;}(t_1) = \\phi_1^{&#39;}(t_1) \\end{align}\\] The equation is read as the slopes of s1 and s2 at the connecting ends (knots) are equal. Let us then compute for the equations of the 1st derivatives of the spline functions - Hermite cubic interpolation. Additionally, to get a solid smooth joint, let us also perform the 2nd derivative - Cubic Spline interpolation. That gives us four additional equations by computing for the 1st and 2nd derivatives of the spline functions. \\[\\begin{align} \\phi_0^{&#39;}(t_1) = \\phi_1^{&#39;}(t_1) {}&amp; &amp; &amp; \\phi_0^{&#39;&#39;}(t_1) = \\phi_1^{&#39;&#39;}(t_1) \\\\ \\phi_1^{&#39;}(t_2) = \\phi_2^{&#39;}(t_2) {}&amp; &amp; &amp; \\phi_1^{&#39;&#39;}(t_2) = \\phi_2^{&#39;&#39;}(t_2) \\end{align}\\] In other words, the equations above represent rules (or constraints) that require us to ensure each spline segment goes through the knots smoothly connecting through to the next spline segment. Given that we have n-1 constraints for the 1st derivative and n-1 constraints for the 2nd derivative, we, therefore, have 2(n-1) constraints corresponding to four equations as above in our case. Third, for equations describing the slopes of the two endpoints: Finally, for natural cubic spline, we need two more equations computing for the 2nd derivative at the beginning of the first spline s1 and at the end of the last spline s4. Our constraint is to have the 2nd derivatives result to zero. \\[\\begin{align} \\phi_0^{&#39;&#39;}(t_0) = 0 {}&amp; &amp; &amp; \\phi_2^{&#39;&#39;}(t_3) = 0 \\end{align}\\] Note here that we only deal with Natural Cubic endpoints. We leave readers to investigate Clamped endpoints and Not-a-knot endpoints. Overall, we have a system of twelve equations to solve: 4n unknowns and 4n equations. We now can solve the unknowns using the matrix formula: \\(Ax = b\\). \\[ \\left[ \\begin{array}{rrrrrrrrrrrr} t_0^3 &amp; t_0^2 &amp; t_0 &amp; 1 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ t_1^3 &amp; t_1^2 &amp; t_1 &amp; 1 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; t_1^3 &amp; t_1^2 &amp; t_1 &amp; 1 &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; t_2^3 &amp; t_2^2 &amp; t_2 &amp; 1 &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; t_2^3 &amp; t_2^2 &amp; t_2 &amp; 1 \\\\ . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; t_3^3 &amp; t_3^2 &amp; t_3 &amp; 1 \\\\ 3t_1^2 &amp; 2t_1 &amp; 1 &amp; . &amp; -3t_1^2 &amp; -2t_1 &amp; -1 &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ 6t_1 &amp; 2 &amp; . &amp; . &amp; -6t_1 &amp; -2 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 3t_2^2 &amp; 2t_2 &amp; 1 &amp; . &amp; -3t_2^2 &amp; -2t_2 &amp; -1 &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; 6t_2 &amp; 2 &amp; . &amp; . &amp; -6t_2 &amp; -2 &amp; . &amp; . \\\\ 6t_0 &amp; 2 &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; . &amp; 6t_3 &amp; 2 &amp; . &amp; . \\end{array} \\right] \\left[\\begin{array}{c} a_0 \\\\ a_1 \\\\ a_2 \\\\ a_3 \\\\ b_0 \\\\ b_1 \\\\ b_2 \\\\ b_3 \\\\ c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{array}\\right]= \\left[\\begin{array}{c} y_0 \\\\ y_1 \\\\ y_1 \\\\ y_2 \\\\ y_2 \\\\ y_3 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\end{array}\\right] \\] For example, given the dataset, (1,2), (2,4),(3,1),(4,3.5), in Figure 3.23, we then get the initial set of equations: \\[\\begin{align} \\phi_0(t_0) {}&amp;= \\phi_0(1) = y0 = 2 &amp; \\phi_0(t_1) {}&amp;= \\phi_0(2) = y1 = 4 \\\\ \\phi_1(t_1) &amp;= \\phi_1(2) = y1 = 4 &amp; \\phi_1(t_2) {}&amp;= \\phi_1(3) = y2 = 1 \\\\ \\phi_2(t_2) &amp;= \\phi_2(3) = y2 = 1 &amp; \\phi_2(t_3) {}&amp;= \\phi_2(4) = y3 = 3.5 \\end{align}\\] Solving for the matrix above, e.g. \\(x = solve(A,y)\\) in R where x = {\\(a_0, a_1, a_2\\), \\(a_3, b_0, b_1, b_2, b_3, c_0, c_1, c_2, c_3\\)}, we get the following: \\[\\begin{align*} a_0 {}&amp;= -1.7 &amp; b_0 {}&amp;= 3.5 &amp; c_0 {}&amp;= -1.8 \\\\ a_1 &amp;= +5.1 &amp; b_1 &amp;= -26.10 &amp; c_1 &amp;= 21.6 \\\\ a_2 &amp;= -1.4 &amp; b_2 &amp;= 61.0 &amp; c_2 &amp;= -82.1 \\\\ a_3 &amp;= +0.0 &amp; b_3 &amp;= -41.6 &amp; c_3 &amp;= 101.5 \\end{align*}\\] along with the corresponding cubic polynomials: \\[\\begin{align*} \\phi_0(t) {}&amp;= -1.7t^3 + 5.1t^2 - 1.4t + 0 \\\\ \\phi_1(t) &amp;= 3.5t^3 - 26.1t^2 + 61t - 41.6 \\\\ \\phi_2(t) &amp;= -1.8t^3 + 21.6t^2 -82.1t + 101.5 \\end{align*}\\] Now let us use R code to show how we stitch the three cubic polynomials - the pieces. See Figure 3.24. phi0 &lt;- function(t) { -1.7*t^3 + 5.1*t^2 -1.4*t + 0 } phi1 &lt;- function(t) { 3.5*t^3 - 26.1*t^2 + 61*t - 41.6 } phi2 &lt;- function(t) { -1.8*t^3 + 21.6*t^2 -82.1*t + 101.5 } x1 = seq(1,2, length.out=50) x2 = seq(2,3, length.out=50) x3 = seq(3,4, length.out=50) y1 = phi0(x1) # 1st piece y2 = phi1(x2) # 2nd piece y3 = phi2(x3) # 3rd piece # plot plot(NULL, xlim=range(0,5), ylim=range(0,5), xlab=&quot;t (knot vector)&quot;, ylab=expression(paste(coeffs[i]))) abline(h=0, col=&quot;darksalmon&quot;, lty=2) abline(v=c(2,3), col=&quot;red&quot;, lty=2) grid(lty=3, col=&quot;lightgrey&quot;) lines(x1, y1, col=&quot;dodgerblue&quot;, lwd=2) lines(x2, y2, col=&quot;green&quot;, lwd=2) lines(x3, y3, col=&quot;darksalmon&quot;, lwd=2) points(c(1,2,3,4), c(2,4,1,3.5), pch=16, lwd=2, col=&quot;darkblue&quot;) text( 1.2, 3.5, expression(phi[0](t)) ) text( 2.2, 2.5, expression(phi[1](t)) ) text( 3.5, 2.5, expression(phi[2](t)) ) Figure 3.24: Natural Cubic Spline in R Next, let us discuss B-spline interpolation as a method to deal with a higher degree of polynomials. 3.7.10 B-Spline interpolation B-Spline Curves: B-Spline is a linear combination of coefficients and basis functions that describe a curve. Like a linear combination such as below: \\[\\begin{align} f(x) = c_0x_0 + c_1x_1 + c_2x_2 + ... + c_nx_n \\end{align}\\] we take that equation and convert the x variables into basis functions: \\[\\begin{align} f(t) = c_0\\phi_0(t) + c_1\\phi_1(t) + c_2\\phi_2(t) + ... + c_n\\phi_n(t) \\end{align}\\] We then have a general formula for B-spline: \\[\\begin{align} B(t) = \\sum_{i=0}^n c_i \\phi_{i,k}(t) \\end{align}\\] \\[\\begin{align*} \\text{where }i {}&amp;= \\{0\\ ...\\ n\\} \\\\ \\\\ n + 1 &amp;= \\text{number of spline segments, also number of control points} \\\\ \\\\ k &amp;= \\text{derivative order or degree of basis functions}, \\\\ &amp;\\ \\ \\ \\ \\text{e.g. 1 - linear, 2 - quadratic, 3 - cubic; with index=0} \\\\ \\end{align*}\\] Here, we have a set of corresponding coefficients (or control points): where n + 1 = number of control points (coefficients) \\[ c_i = \\{c_0, c_1, ..., c_n\\}. \\] We also have a set of knots called knot vector, T = {\\(t_0, t_1, t_3, ..., t_{n + k + 1}\\)}, with the Cox-de Boor recursion formula, \\(\\phi_{i,k}(t)\\), as our basis function (Carl De Boor 2002): If indices start with 1: \\[\\begin{align} \\phi_{i,j}(t) {}&amp;= \\left(\\frac{t - t_i}{t_{i+j-1} - t_i}\\right) \\phi_{i,j-1}(t) \\ +\\ \\left(\\frac{t_{i+j} - t}{t_{i+j} - t_{i+1}}\\right) \\phi_{i+1,j-1}(t) \\label{eqn:eqnnumber300}\\\\ \\nonumber \\\\ \\phi_{i,1}(t) &amp;= \\begin{cases} 1 &amp; if\\ t_i \\leq t &lt; t_{i+1} \\\\ 0 &amp; otherwise \\end{cases} \\leftarrow\\ \\ \\ \\ \\text{indicator function} \\label{eqn:eqnnumber702} \\end{align}\\] If indices start with 0: \\[\\begin{align} \\phi_{i,j}(t) {}&amp;= \\left(\\frac{t - t_i}{t_{i+j} - t_i}\\right) \\phi_{i,j-1}(t) \\ +\\ \\left(\\frac{t_{i+j+1} - t}{t_{i+j + 1} - t_{i+1}}\\right) \\phi_{i+1,j-1}(t) \\label{eqn:eqnnumber301} \\\\ \\nonumber \\\\ \\phi_{i,0}(t) &amp;= \\begin{cases} 1 &amp; if\\ t_i \\leq t &lt; t_{i+1} \\\\ 0 &amp; otherwise \\end{cases} \\leftarrow\\ \\ \\ \\ \\text{indicator function} \\label{eqn:eqnnumber703} \\end{align}\\] Note that our index starts with zero throughout our discussion (except in our R code which begins with one ). Other literature may begin with one. See Figure 3.25 for how we compute the basis functions in a recursive triangular scheme: Figure 3.25: B-Spline Basis Functions We use R code to demonstrate the Cox-De Boor Recursion Formula. See Figure 3.26. Here we graph the basis functions, \\(\\phi_{1,4}(t), \\phi_{2,4}(t), \\phi_{3,4}(t), \\phi_{4,4}(t)\\) - indices starting at one. # indices for R starts at 1 start_index = 1 # implementing Cox-de Boor recursion Formula cooxdeboor_basis &lt;- function(x, i, j, t) { b = 0 if (j == start_index) { if (t[i] &lt;= x &amp;&amp; x &lt; t[i+1]) { b = 1 } } else { b = ((x - t[i])/( t[i+j-1] - t[i] )) * cooxdeboor_basis (x, i, j-1, t) + ((t[i+j] - x) / (t[i+j] - t[i+1])) * cooxdeboor_basis(x, i+1, j-1, t) } b } # Using Uniform Cubic B-spline N = 4 K = 4 t = seq(start_index, N+K, 1) # Our Dataset x = seq(0,8, 0.01) b1 = c(); b2 = c(); b3 = c(); b4 = c() for (n in 1:length(x)) { b1[n] = cooxdeboor_basis(x[n], start_index, start_index, t) b2[n] = cooxdeboor_basis(x[n], start_index, start_index + 1, t) b3[n] = cooxdeboor_basis(x[n], start_index, start_index + 2, t) b4[n] = cooxdeboor_basis(x[n], start_index, start_index + 3, t) } plot(NULL, xlim=range(-0.1,7), ylim=range(-0.1,1), xlab=&quot;t (knot vector)&quot;, ylab= expression(paste( phi[&quot;i,j&quot;](t)))) abline(h=0, col=&quot;black&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x,b1, col=&quot;darksalmon&quot;) lines(x,b2, col=&quot;green&quot;) lines(x,b3, col=&quot;dodgerblue&quot;) lines(x,b4, col=&quot;blue&quot;) legend(4, 0.80, c( expression(paste(&quot;1st Order: &quot;, phi[&quot;1,4&quot;](t))), expression(paste(&quot;2nd Order: &quot;, phi[&quot;2,4&quot;](t))), expression(paste(&quot;3rd Order: &quot;, phi[&quot;3,4&quot;](t))), expression(paste(&quot;4th Order: &quot;, phi[&quot;4,4&quot;](t))) ), fill=c(&quot;darksalmon&quot;,&quot;green&quot;, &quot;dodgerblue&quot;, &quot;blue&quot;), horiz=FALSE, cex=0.8) Figure 3.26: Cox-De Boor Recursion Let us tackle a Uniform Cubic B-Spline to demonstrate B-Spline. First, the formula to use is the following: \\[\\begin{align} B(t) = \\sum_{i=0}^{n=3} c_i \\phi_{i,k=3}(t) \\end{align}\\] which expands into a 4th-order (k + 1) B-spline formula: \\[\\begin{align} B(t) = c_0\\phi_{0,3}(t) + c_1\\phi_{1,3}(t) + c_2\\phi_{2,3}(t) + c_3\\phi_{3,3}(t) \\end{align}\\] with four coefficients (control points) and a knot vector: \\[\\begin{align*} c {}&amp;= \\{c_0, c_1, c_2, c_{k=3}\\} \\\\ \\\\ T &amp;= \\{t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_{n + k + 1 = 7}\\} \\end{align*}\\] Secondly, for \\(\\phi_{0,3}(t)\\), we use the following triangular table for our basis functions: \\[ \\left[\\begin{array}{rrrrr} i/k &amp; k=0 &amp; k=1 &amp; k=2 &amp; k=3 \\\\ \\\\ i=0 &amp; \\phi_{0,0}(t) &amp; \\phi_{0,1}(t) &amp; \\phi_{0,2}(t) &amp; \\phi_{0,3}(t) \\\\ i=1 &amp; \\phi_{1,0}(t) &amp; \\phi_{1,1}(t) &amp; \\phi_{1,2}(t) \\\\ i=2 &amp; \\phi_{2,0}(t) &amp; \\phi_{2,1}(t) \\\\ i=3 &amp; \\phi_{3,0}(t) \\\\ \\end{array}\\right] \\] and along with the Cox-de Boor recursion formula, we get the following equations: \\[\\begin{align*} \\phi_{0,3}(t) {}&amp;= \\left( \\frac{t - 0}{3 - 0} \\right) \\phi_{0,2}(t) + \\left( \\frac{4 - t}{4 - 1} \\right) \\phi_{1,2}(t) &amp; \\phi_{0,1}(t) &amp;= \\left( \\frac{t - 0}{1 - 0} \\right) \\phi_{0,0}(t) + \\left( \\frac{2 - t}{2 - 1} \\right) \\phi_{1,0}(t) \\\\ \\phi_{0,2}(t) &amp;= \\left( \\frac{t - 0}{2 - 0} \\right) \\phi_{0,1}(t) + \\left( \\frac{3 - t}{3 - 1} \\right) \\phi_{1,1}(t) &amp; \\phi_{1,1}(t) &amp;= \\left( \\frac{t - 1}{2 - 1} \\right) \\phi_{1,0}(t) + \\left( \\frac{3 - t}{3 - 2} \\right) \\phi_{2,0}(t) \\\\ \\phi_{1,2}(t) &amp;= \\left( \\frac{t - 1}{3 - 1} \\right) \\phi_{1,1}(t) + \\left( \\frac{4 - t}{4 - 2} \\right) \\phi_{2,1}(t) &amp; \\phi_{2,1}(t) &amp;= \\left( \\frac{t - 2}{3 - 2} \\right) \\phi_{2,0}(t) + \\left( \\frac{4 - t}{4 - 3} \\right) \\phi_{3,0}(t) \\end{align*}\\] Next, let us substitute the equations for \\(\\phi_{0,2}(t)\\) and \\(\\phi_{1,2}(t)\\) into \\(\\phi_{0,3}(t)\\): \\[\\begin{align*} \\phi_{0,3}(t) {}&amp;= \\left( \\frac{t - 0}{3 - 0} \\right) \\left[ \\left( \\frac{t - 0}{2 - 0} \\right) \\phi_{0,1}(t) + \\left( \\frac{3 - t}{3 - 1} \\right) \\phi_{1,1}(t) \\right] \\\\ &amp;+ \\left( \\frac{4 - t}{4 - 1} \\right) \\left[ \\left( \\frac{t - 1}{3 - 1} \\right) \\phi_{1,1}(t) + \\left( \\frac{4 - t}{4 - 2} \\right) \\phi_{2,1}(t) \\right] \\end{align*}\\] Next, let us substitute the equations for \\(\\phi_{0,1}(t)\\), \\(\\phi_{1,1}(t)\\), and \\(\\phi_{2,1}(t)\\) into \\(\\phi_{0,3}(t)\\); simplifying, we get the equation below: \\[\\begin{align*} \\phi_{0,3}(t) {}&amp;= \\frac{(t)^3}{6} \\phi_{0,0}(t) \\\\ &amp;+ \\frac{(t)^2(2-t)}{6} \\phi_{1,0}(t) + \\frac{(t)(3-t)(t-1)}{6} \\phi_{1,0}(t) + \\frac{(4-t)(t-1)^2}{6} \\phi_{1,0}(t) \\\\ &amp;+ \\frac{(t)(3-t)^2}{6} \\phi_{2,0}(t) + \\frac{(4-t)(t-1)(3-t)}{6} \\phi_{2,0}(t) + \\frac{(4-t)^2(t-2)}{6} \\phi_{2,0}(t) \\\\ &amp;+ \\frac{(4-t)^3}{6} \\phi_{3,0}(t) \\\\ \\\\ \\phi_{0,3}(t) &amp;= \\frac{(t)^3}{6} \\phi_{0,0}(t) \\\\ &amp;+ \\frac{(-3t^3 + 12t^2 -12^t + 4)}{6} \\phi_{1,0}(t) + \\frac{(3t^3 +-24t^2 +60^t - 44)}{6} \\phi_{2,0}(t) \\\\ &amp;+ \\frac{(4-t)^3}{6} \\phi_{3,0}(t) \\end{align*}\\] Then, recalling the Cox-de Boor recursion constraint: \\[ \\phi_{i,0}(t) = \\begin{cases} 1 &amp; if\\ t_i \\leq t &lt; t_{i+1} \\\\ 0 &amp; otherwise \\end{cases} \\] For 0 &lt;= t &lt; 1: \\[ \\phi_{0,0}(t) = 1,\\ \\ \\ \\phi_{1,0}(t) = 0,\\ \\ \\ \\phi_{2,0}(t) = 0,\\ \\ \\ \\phi_{3,0}(t) = 0 \\] the basis function, \\(\\phi_{0,3}(t)\\), will be: \\[ \\phi_{0,3}(t) = \\frac{t^3}{6} \\] For 1 &lt;= t &lt; 2:, \\[ \\phi_{0,0}(t) = 0,\\ \\ \\ \\phi_{1,0}(t) = 1,\\ \\ \\ \\phi_{2,0}(t) = 0,\\ \\ \\ \\phi_{3,0}(t) = 0 \\] the basis function, \\(\\phi_{0,3}(t)\\), will be: \\[ \\phi_{0,3}(t) = \\frac{-3t^3+12t^2-12t+4}{6} \\] For 2 &lt;= t &lt; 3: \\[ \\phi_{0,0}(t) = 0,\\ \\ \\ \\phi_{1,0}(t) = 0,\\ \\ \\ \\phi_{2,0}(t) = 1,\\ \\ \\ \\phi_{3,0}(t) = 0 \\] the basis function, \\(\\phi_{0,3}(t)\\), will be: \\[ \\phi_{0,3}(t) = \\frac{3t^3 + 24t^2+60t-44}{6} \\] For 3 &lt;= t &lt; 4: \\[ \\phi_{0,0}(t) = 0,\\ \\ \\ \\phi_{1,0}(t) = 0,\\ \\ \\ \\phi_{2,0}(t) = 0,\\ \\ \\ \\phi_{3,0}(t) = 1 \\] the basis function, \\(\\phi_{0,3}(t)\\), will be: \\[ \\phi_{0,3}(t) = \\frac{(4-t)^3}{6} \\] Secondly, for \\(\\phi_{1,3}(t)\\), we use the following triangular table for our basis functions: \\[ \\left[\\begin{array}{crrrr} i/k &amp; k=0 &amp; k=1 &amp; k=2 &amp; k=3 \\\\ \\vdots \\\\ i=1 &amp; \\phi_{1,0}(t) &amp; \\phi_{1,1}(t) &amp; \\phi_{1,2}(t) &amp; \\phi_{1,3}(t) \\\\ i=2 &amp; \\phi_{2,0}(t) &amp; \\phi_{2,1}(t) &amp; \\phi_{2,2}(t) \\\\ i=3 &amp; \\phi_{3,0}(t) &amp; \\phi_{3,1}(t) \\\\ i=4 &amp; \\phi_{4,0}(t) \\end{array}\\right] \\] and along with the Cox-de Boor recursion formula, we get the following equations: \\[\\begin{align*} \\phi_{1,3}(t) {}&amp;= \\left( \\frac{t - 1}{4 - 1} \\right) \\phi_{1,2}(t) + \\left( \\frac{5 - t}{5 - 2} \\right) \\phi_{2,2}(t) &amp; \\phi_{1,1}(t) &amp;= \\left( \\frac{t - 1}{2 - 1} \\right) \\phi_{1,0}(t) + \\left( \\frac{3 - t}{3 - 2} \\right) \\phi_{2,0}(t) \\\\ \\phi_{1,2}(t) &amp;= \\left( \\frac{t - 1}{3 - 1} \\right) \\phi_{1,1}(t) + \\left( \\frac{4 - t}{4 - 2} \\right) \\phi_{2,1}(t) &amp; \\phi_{2,1}(t) &amp;= \\left( \\frac{t - 2}{3 - 2} \\right) \\phi_{2,0}(t) + \\left( \\frac{4 - t}{4 - 3} \\right) \\phi_{3,0}(t) \\\\ \\phi_{2,2}(t) &amp;= \\left( \\frac{t - 2}{4 - 2} \\right) \\phi_{2,1}(t) + \\left( \\frac{5 - t}{5 - 3} \\right) \\phi_{3,1}(t) &amp; \\phi_{3,1}(t) &amp;= \\left( \\frac{t - 3}{4 - 3} \\right) \\phi_{3,0}(t) + \\left( \\frac{5 - t}{5 - 4} \\right) \\phi_{4,0}(t) \\end{align*}\\] Next, let us perform substitution into \\(\\phi_{1,3}(t)\\), skipping some algebraic operations: \\[\\begin{align*} \\phi_{1,3}(t) {}&amp;= \\frac{(t-1)^3}{6}\\phi_{1,0}(t) \\\\ &amp; + \\frac{(t-1)^2(3-t)}{6}\\phi_{2,0}(t) + \\frac{(t-1)(4-t)(t-2)}{6}\\phi_{2,0}(t) + \\frac{(5-t)(t-2)^2}{6}\\phi_{2,0}(t) \\\\ &amp; + \\frac{(t-1)(4-t)^2}{6}\\phi_{3,0}(t) + \\frac{(5-t)(t-2)(4-t)}{6}\\phi_{3,0}(t) + \\frac{(5-t)^2(t-3)}{6}\\phi_{3,0}(t)\\\\ &amp; + \\frac{(5-t)^3}{6}\\phi_{4,0}(t) \\\\ \\\\ \\phi_{1,3}(t) &amp; = \\frac{(t-1)^3}{6}\\phi_{1,0}(t) \\\\ &amp; + \\frac{(-3t^3 + 21t^2 -45t + 31)}{6} \\phi_{2,0}(t) + \\frac{(3t^3 -33t^2 + 117t - 131)}{6} \\phi_{3,0}(t) \\\\ &amp; + \\frac{(5-t)^3}{6}\\phi_{4,0}(t) \\end{align*}\\] Thirdly, for \\(\\phi_{2,3}(t)\\), we use the following triangular table for our basis functions: \\[ \\left[\\begin{array}{crrrr} i/k &amp; k=0 &amp; k=1 &amp; k=2 &amp; k=3 \\\\ \\vdots \\\\ i=2 &amp; \\phi_{2,0}(t) &amp; \\phi_{2,1}(t) &amp; \\phi_{2,2}(t) &amp; \\phi_{2,3}(t) \\\\ i=3 &amp; \\phi_{3,0}(t) &amp; \\phi_{3,1}(t) &amp; \\phi_{3,2}(t) \\\\ i=4 &amp; \\phi_{4,0}(t) &amp; \\phi_{4,1}(t) \\\\ i=5 &amp; \\phi_{5,0}(t) \\\\ \\end{array}\\right] \\] and along with the Cox-de Boor recursion formula, we get the following equations: \\[\\begin{align*} \\phi_{2,3}(t) {}&amp;= \\left( \\frac{t - 2}{5 - 2} \\right) \\phi_{2,2}(t) + \\left( \\frac{6 - t}{6 - 3} \\right) \\phi_{3,2}(t) &amp; \\phi_{2,1}(t) &amp;= \\left( \\frac{t - 2}{3 - 2} \\right) \\phi_{2,0}(t) + \\left( \\frac{4 - t}{4 - 3} \\right) \\phi_{3,0}(t) \\\\ \\phi_{2,2}(t) &amp;= \\left( \\frac{t - 2}{4 - 2} \\right) \\phi_{2,1}(t) + \\left( \\frac{5 - t}{5 - 3} \\right) \\phi_{3,1}(t) &amp; \\phi_{3,1}(t) &amp;= \\left( \\frac{t - 3}{4 - 3} \\right) \\phi_{3,0}(t) + \\left( \\frac{5 - t}{5 - 4} \\right) \\phi_{4,0}(t) \\\\ \\phi_{3,2}(t) &amp;= \\left( \\frac{t - 3}{5 - 3} \\right) \\phi_{3,1}(t) + \\left( \\frac{6 - t}{6 - 4} \\right) \\phi_{4,1}(t) &amp; \\phi_{4,1}(t) &amp;= \\left( \\frac{t - 4}{5 - 4} \\right) \\phi_{4,0}(t) + \\left( \\frac{6 - t}{6 - 5} \\right) \\phi_{5,0}(t) \\end{align*}\\] Next, let us perform substitution into \\(\\phi_{2,3}(t)\\), skipping some algebraic operations: \\[\\begin{align*} \\phi_{2,3}(t) {}&amp; = \\frac{(t - 2)^3}{6} \\phi_{2,0}(t) \\\\ &amp; + \\frac{(t - 2)^2(4-t)}{6} \\phi_{3,0}(t) + \\frac{(t - 2)(5-t)(t-3)}{6} \\phi_{3,0}(t) + \\frac{(6-t)(t - 3)^2}{6} \\phi_{3,0}(t) \\\\ &amp;+ \\frac{(t - 2)(5-t)^2}{6} \\phi_{4,0}(t) + \\frac{(6-t)(t-3)(5-t)}{6} \\phi_{4,0}(t) + \\frac{(6 - t)^2(t-4)}{6} \\phi_{4,0}(t) \\\\ &amp;+ \\frac{(6 - t)^3}{6} \\phi_{5,0}(t) \\\\ \\\\ \\phi_{2,3}(t) {}&amp;= \\frac{(t - 2)^3}{6} \\phi_{2,0}(t) \\\\ &amp;+ \\frac{(-3t^3 + 30t^2 -96t + 100)}{6} \\phi_{3,0}(t) + \\frac{(3t^3 - 42t^2 + 192t - 284)}{6} \\phi_{4,0}(t) \\\\ &amp;+ \\frac{(6 - t)^3}{6} \\phi_{5,0}(t) \\end{align*}\\] Lastly, for \\(\\phi_{3,3}(t)\\), we use the following triangular table for our basis functions: \\[ \\left[\\begin{array}{crrrr} i/k &amp; k=0 &amp; k=1 &amp; k=2 &amp; k=3 \\\\ \\vdots \\\\ i=3 &amp; \\phi_{3,0}(t) &amp; \\phi_{3,1}(t) &amp; \\phi_{3,2}(t) &amp; \\phi_{3,3}(t) \\\\ i=4 &amp; \\phi_{4,0}(t) &amp; \\phi_{4,1}(t) &amp; \\phi_{4,2}(t) \\\\ i=5 &amp; \\phi_{5,0}(t) &amp; \\phi_{5,1}(t) \\\\ i=6 &amp; \\phi_{6,0}(t) \\\\ \\end{array}\\right] \\] and along with the Cox-de Boor recursion formula, we get the following equations: \\[\\begin{align*} \\phi_{3,3}(t) {}&amp;= \\left( \\frac{t - 3}{6 - 3} \\right) \\phi_{3,2}(t) + \\left( \\frac{7 - t}{7 - 4} \\right) \\phi_{4,2}(t) &amp; \\phi_{3,1}(t) &amp;= \\left( \\frac{t - 3}{4 - 3} \\right) \\phi_{3,0}(t) + \\left( \\frac{5 - t}{6 - 4} \\right) \\phi_{4,0}(t) \\\\ \\phi_{3,2}(t) &amp;= \\left( \\frac{t - 3}{5 - 3} \\right) \\phi_{3,1}(t) + \\left( \\frac{6 - t}{6 - 4} \\right) \\phi_{4,1}(t) &amp; \\phi_{4,1}(t) &amp;= \\left( \\frac{t - 4}{5 - 4} \\right) \\phi_{4,0}(t) + \\left( \\frac{6 - t}{6 - 5} \\right) \\phi_{5,0}(t) \\\\ \\phi_{4,2}(t) &amp;= \\left( \\frac{t - 4}{5 - 4} \\right) \\phi_{4,1}(t) + \\left( \\frac{7 - t}{7 - 5} \\right) \\phi_{5,1}(t) &amp; \\phi_{5,1}(t) &amp;= \\left( \\frac{t - 5}{6 - 5} \\right) \\phi_{5,0}(t) + \\left( \\frac{7 - t}{7 - 6} \\right) \\phi_{6,0}(t) \\end{align*}\\] Next, let us perform substitution into \\(\\phi_{3,3}(t)\\), skipping some algebraic operations: \\[\\begin{align*} \\phi_{3,3}(t) {}&amp;= \\frac{(t - 3)^3}{6} \\phi_{3,0}(t) \\\\ &amp;+ \\frac{(t - 3)^2(5-t)}{6} \\phi_{4,0}(t) + \\frac{(t - 3)(6-t)(t-4)}{6} \\phi_{4,0}(t) + \\frac{(7-t)(t-4)^2}{6} \\phi_{4,0}(t) \\\\ &amp;+ \\frac{(t - 3)(6-t)^2}{6} \\phi_{5,0}(t) + \\frac{(7-t)(t-4)(6-t)}{6} \\phi_{5,0}(t) + \\frac{(7-t)^2(t-5)}{6} \\phi_{5,0}(t) \\\\ &amp;+ \\frac{(7-t)^3}{6} \\phi_{6,0}(t) \\\\ \\\\ \\phi_{3,3}(t) {}&amp;= \\frac{(t - 3)^3}{6} \\phi_{3,0}(t) \\\\ &amp;+ \\frac{(-3t^3 + 39t^2 - 165t + 229)}{6} \\phi_{4,0}(t) + \\frac{(3^t - 51t^2 + 285t - 521)}{6} \\phi_{5,0}(t) \\\\ &amp;+ \\frac{(7-t)^3}{6} \\phi_{6,0}(t) \\end{align*}\\] Now, recalling the Cox-de Boor recursion constraint again, we get a complete list of basis functions for \\(\\phi_{0,3}(t)\\), \\(\\phi_{1,3}(t)\\), \\(\\phi_{2,3}(t)\\), \\(\\phi_{3,3}(t)\\): \\[ \\left[\\begin{array}{cll} t &amp; \\phi_{0,3}(t) &amp; \\phi_{1,3}(t) \\\\ ---&amp;----------- &amp;------------- \\\\ 0 \\leq t &lt; 1 &amp; \\frac{1}{6}(t)^3 &amp; 0 \\\\ 1 \\leq t &lt; 2 &amp; \\frac{1}{6}(-3t^3 + 12t^2 -12^t + 4) &amp; \\frac{1}{6}(t-1)^3 \\\\ 2 \\leq t &lt; 3 &amp; \\frac{1}{6}(3t^3 +-24t^2 +60^t - 44) &amp; \\frac{1}{6}(-3t^3 + 21t^2 -45t + 31) \\\\ 3 \\leq t &lt; 4 &amp; \\frac{1}{6}(4-t)^3 &amp; \\frac{1}{6}(3t^3 -33t^2 + 117t - 131) \\\\ 4 \\leq t &lt; 5 &amp; 0 &amp; \\frac{1}{6}(5-t)^3 \\\\ \\\\ t &amp; \\phi_{2,3}(t) &amp; \\phi_{3,3}(t) \\\\ ---&amp;----------- &amp;------------- \\\\ 2 \\leq t &lt; 3 &amp; \\frac{1}{6}(t - 2)^3 &amp; 0\\\\ 3 \\leq t &lt; 4 &amp; \\frac{1}{6}(-3t^3 + 30t^2 -96t + 100) &amp; \\frac{1}{6}(t - 3)^3 \\\\ 4 \\leq t &lt; 5 &amp; \\frac{1}{6}(3t^3 - 42t^2 + 192t - 284) &amp; \\frac{1}{6}(-3t^3 + 39t^2 - 165t + 229) \\\\ 5 \\leq t &lt; 6 &amp; \\frac{1}{6}(6 - t)^3 &amp; \\frac{1}{6}(3^t - 51t^2 + 285t - 521)\\\\ 6 \\leq t &lt; 7 &amp; 0 &amp; \\frac{1}{6}(7-t)^3 \\end{array} \\right] \\] Let us use an R code to implement the functions and corresponding constraints as above (we will use a dataset of 141 data points with 0.05 interval (e.g. seq(0,7,0.05) in R). See Figure 3.27. # indices start with zero # most naive way of implementing Cox-de Boor formulas phi03 &lt;-function(t, phi) { (t^3/6) * phi[1] + (-3*t^3 + 12*t^2 -12 * t + 4)/6 * phi[2] + (3*t^3 - 24*t^2 + 60*t -44)/6 * phi[3] + ((4-t)^3)/6 * phi[4] } phi13 &lt;-function(t, phi) { ((t-1)^3)/6 * phi[1] + (-3*t^3 + 21*t^2 -45 * t + 31)/6 * phi[2] + (3*t^3 - 33*t^2 + 117*t -131)/6 * phi[3] + ((5-t)^3)/6 * phi[4] } phi23 &lt;-function(t, phi) { ((t-2)^3)/6 * phi[1] + (-3*t^3 + 30*t^2 -96 * t + 100)/6 * phi[2] + (3*t^3 - 42*t^2 + 192*t -284)/6 * phi[3] + ((6-t)^3)/6 * phi[4] } phi33 &lt;-function(t, phi) { ((t-3)^3)/6 * phi[1] + (-3*t^3 + 39*t^2 -165 * t + 229)/6 * phi[2] + (3*t^3 - 51*t^2 + 285*t -521)/6 * phi[3] + ((7-t)^3)/6 * phi[4] } coxdeboor_constraint &lt;- function(t, i, k) { phi0 = c(0,0,0,0) l = 0 for (j in i:(i+k)) { l = l + 1 if (j &lt;= t &amp;&amp; t &lt; j+1) { phi0[l] = 1 } } phi0 } x = seq(0,7, 0.05) s1 = c(); s2 = c(); s3 = c(); s4 = c() for (t in 1:length(x)) { s1[t] = phi03(x[t], coxdeboor_constraint(x[t], 0, 3)) s2[t] = phi13(x[t], coxdeboor_constraint(x[t], 1, 3)) s3[t] = phi23(x[t], coxdeboor_constraint(x[t], 2, 3)) s4[t] = phi33(x[t], coxdeboor_constraint(x[t], 3, 3)) } plot(NULL, xlim=range(0,7), ylim=range(-0.1,0.9), xlab=&quot;t (knot vector)&quot;, ylab=expression(paste(phi[&quot;0,i&quot;],&quot;(t)&quot;))) abline(h=0, col=&quot;black&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x,s1, col=&quot;darksalmon&quot;) lines(x,s2, col=&quot;navyblue&quot;) lines(x,s3, col=&quot;lightgreen&quot;) lines(x,s4, col=&quot;brown&quot;) text(2, 0.70, expression(paste(phi[&quot;0,3&quot;],&quot;(t)&quot;))) text(3, 0.70, expression(paste(phi[&quot;1,3&quot;],&quot;(t)&quot;))) text(4, 0.70, expression(paste(phi[&quot;2,3&quot;],&quot;(t)&quot;))) text(5, 0.70, expression(paste(phi[&quot;3,3&quot;],&quot;(t)&quot;))) Figure 3.27: 4th Order Basis Functions Alternatively, we can use the following more simplified version of the R code, invoking cooxdeboor_basis function: # indices start with zero start_index = 1 # Using Uniform Cubic B-spline N = 4 K = 4 t = seq(1, N+K, 1) x = seq(0,7, 0.05) s1 = c(); s2 = c(); s3 = c(); s4 = c() for (n in 1:length(x)) { s1[n] = cooxdeboor_basis(x[n], start_index, N-1, t) s2[n] = cooxdeboor_basis(x[n], start_index + 1, N-1, t) s3[n] = cooxdeboor_basis(x[n], start_index + 2, N-1, t) s4[n] = cooxdeboor_basis(x[n], start_index + 3, N-1, t) } plot(NULL, xlim=range(0,7), ylim=range(-0.1,0.9), xlab=&quot;t (knot vector)&quot;, ylab=expression(paste(phi[&quot;0,i&quot;],&quot;(t)&quot;))) abline(h=0, col=&quot;black&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x,s1, col=&quot;darksalmon&quot;) lines(x,s2, col=&quot;navyblue&quot;) lines(x,s3, col=&quot;lightgreen&quot;) lines(x,s4, col=&quot;brown&quot;) text(2.5, 0.80, expression(paste(phi[&quot;1,4&quot;],&quot;(t)&quot;))) text(3.5, 0.80, expression(paste(phi[&quot;2,4&quot;],&quot;(t)&quot;))) text(4.5, 0.80, expression(paste(phi[&quot;3,4&quot;],&quot;(t)&quot;))) text(5.5, 0.80, expression(paste(phi[&quot;4,4&quot;],&quot;(t)&quot;))) Figure 3.28: 4th Order Basis Functions Note that the code uses indices that start with 1, so the graph is shifted by 1. Finally, we go back to the Cox-de Boor recursion formula: \\[ B(t) = \\sum_{i=0}^{n=3} c_i \\phi_{i,k=3}(t) \\] Let us expand the formula using the basis functions generated as shown in Figure 3.27: \\[\\begin{align} B(t) = c_0\\phi_{0,3}(t) + c_1\\phi_{1,3}(t) + c_2\\phi_{1,3}(t) + c_3\\phi_{2,3}(t). \\end{align}\\] Suppose that the following control points (coefficients) are given as such: \\[ c_0 = -1,\\ \\ c_1 = 2,\\ \\ c_2 = -2,\\ \\ c_3 = 1 \\] in this case, our Uniform Cubic B-spline expression then looks like this: \\[\\begin{align} B(t) = -1 \\phi_{0,3}(t) + 2 \\phi_{1,3}(t) - 2 \\phi_{1,3}(t) + 1\\phi_{2,3}(t). \\end{align}\\] Using R code with the generated results from the basis functions in Figure 3.27, we get: Cubic_B_Spline &lt;- function(x, y1, y2, y3, y4, coeffs, legend) { knots = c(2,3,4,5) # Can also narrow down to 2 &lt;= t &lt;= 5 i = which(0 &lt;= x &amp; x &lt;= 7) t = x[i] # basis function output s1 = y1[i]; s2 = y2[i]; s3 = y3[i]; s4 = y4[i] # linear combination Bt = coeffs[1] * s1 + coeffs[2] * s2 + coeffs[3] * s3 + coeffs[4] * s4 # plot plot(NULL, xlim=range(0,7), ylim=range(-3,3.5), xlab=&quot;t (knot vector)&quot;, ylab=expression(paste(coeffs[i]))) abline(h=0, col=&quot;darksalmon&quot;, lty=2) abline(v=c(2,5), col=&quot;red&quot;, lty=2) grid(lty=3, col=&quot;lightgrey&quot;) lines(t,Bt, col=&quot;blue&quot;) legend(0,3.5, bg=&quot;white&quot;, legend) # control points path lines(knots, coeffs, col=&quot;darksalmon&quot;) points(knots, coeffs, col=&quot;navyblue&quot;, pch=16, lwd=2) coef_text = paste(c(&quot;c0=&quot;,&quot;c1=&quot;,&quot;c2=&quot;,&quot;c3=&quot;), coeffs) text(knots + 0.4, coeffs - 0.2, coef_text ) } # legend legend = expression(paste( &quot;B(t) = &quot;, &quot; - 1&quot;, phi[&quot;0,3&quot;],&quot;(t)&quot;, &quot; + 2&quot;, phi[&quot;1,3&quot;],&quot;(t)&quot;, &quot; - 2&quot;, phi[&quot;2,3&quot;],&quot;(t)&quot;, &quot; + 1&quot;, phi[&quot;3,3&quot;],&quot;(t)&quot;)) # control points (coefficients) coeffs = c(-1.0, 2.0, -2.0, 1.0) Bt = Cubic_B_Spline(x, s1, s2, s3, s4, coeffs, legend) Figure 3.29: Uniform Cubic B-Spline Note that we used Uniform interval for the Cubic B-spline in the example. In other literature, we may also encounter the term “Cardinal” for “Uniform”. That is because the Knot Vector is sequenced in descending order of integers at fix intervals, e.g. \\(T_i = \\{0, 1, 2, ... N + K + 1\\}\\). The same Cox-de Boor recursion formula also applies to Non-Uniform B-splines if choosing Non-Cardinal intervals. Also, we showed a Uniform Cubic B-spline with N=3 and K=3 expressed in the 4th order: \\[ B(t) = c_0\\phi_{0,3}(t) + c_0\\phi_{1,3}(t) + c_0\\phi_{2,3}(t) + c_0\\phi_{3,3}(t) \\] As another example, a Uniform Quartic B-Spline with N=6 and K=4 parameters is expressed as such: \\[\\begin{align} B(t) = \\sum_{i=0}^{n=6} c_i \\phi_{i,k=4}(t) \\end{align}\\] which expands into a 5th-order (k+1) B-spline formula: \\[ B(t) = c_0\\phi_{0,4}(t) + c_1\\phi_{1,4}(t) + c_2\\phi_{2,4}(t) + c_3\\phi_{3,4}(t) + c_4\\phi_{4,4}(t) + c_5\\phi_{5,4}(t) + c_6\\phi_{6,4}(t) \\] with seven coefficients (control points) and a knot vector: \\[\\begin{align*} c_{i} {}&amp;= \\{c_0, c_1, c_2, c_3, c_4, c_5, c_{k=6}\\} \\\\ \\\\ T &amp;= \\{t_0, t_1, t_2, t_3, t_4, t_5, t_6, t_7, t_8, t_9, t_{n+k+1 = 10}\\} \\end{align*}\\] For this, it is best to use computer software libraries in a language of choice (e.g., R, python, java, etc.) Bézier Curve: A Bézier curve is a special B-spline curve with restriction. The number of control points (N) matches the degree of the polynomial (K). For example: \\[\\begin{align} B(t) = \\sum_{i=0}^{n=3} c_i \\phi_{i, k=3}(t) \\end{align}\\] with the Bernstein polynomial formula as our basis function: \\[\\begin{align} \\phi_{i,k}(t) = \\binom{n}{k} t^i(1 - t)^{n-i} \\end{align}\\] In Figure 3.29, there are four coefficients in the B-spline of order four. Adjusting any one of the coefficients does not greatly affect the neighboring curves - this makes the coefficients a local control point. Let us change the coefficient of the 2nd-order (2nd term) from +2 to 0 and observe the outcome in Figure 3.30, comparing that with Figure 3.29. Figure 3.30: B-Spline (Local Control) On the other hand, Bézier curves are controlled by global control points, making those control points somehow restricted. This book does not cover Bézier curves in detail, given that we have covered B-spline curves as a generalization of Bézier curves. However, for reference, it helps to study De Casteljau’s algorithm and Bernstein polynomial (Prautzsch H. et al. 2002). 3.7.11 B-Spline Regression Based on our discussions about B-splines, we now recognize four knobs we use to influence the curvature of B-splines when interpolating through all the data points. the coefficients the number of knots the placements of the knots the basis function In B-Spline Regression, we deal with approximation. Here we perform curve fitting. We also need to consider how to tune the knobs above to fit well. As for the number of knots, a large number of knots tend to overfit the curve, and a small number of knots tend to underfit it. Now, recall residual sum of squares(RSS) in linear regression. Similarly, our method is to look for RSS amongst a system of b-spline equations, then minimize the objective - that is, determine the minimum RSS to find the best fit for a curve. For a uniform cubic b-spline, we assume our spline to be approximate by considering some noise, \\(\\epsilon\\). For example: \\[\\begin{align} B(t) {}&amp;=c_0\\phi_{0,3}(t) + c_1\\phi_{1,3}(t) + c_2\\phi_{2,3}(t) + c_3\\phi_{3,3}(t) \\\\ \\nonumber \\\\ \\hat{B}(t) &amp;= B(t) + \\epsilon \\end{align}\\] Therefore, our residual looks like so: \\[ \\epsilon = \\hat{B}(t) - B(t) \\] and our objective function to minimize becomes: \\[\\begin{align} RSS = \\sum_{i=1}^k |\\hat{B}(t) - B(t)|^2 = \\sum_{i=1}^k |\\epsilon |^2 \\end{align}\\] Another way to look at residual is the delta change between the actual value of coefficients and the approximate value of coefficients (in this case, excluding noise, \\(\\epsilon\\)). where: \\[\\begin{align*} \\hat{B}(t) {}&amp;= \\hat{c_0}\\phi_{0,3}(t) + \\hat{c_1}\\phi_{1,3}(t) + \\hat{c_2}\\phi_{2,3}(t) + \\hat{c_3}\\phi_{3,3}(t) \\\\ \\\\ \\hat{c} &amp;= \\{ \\hat{c_0}, \\hat{c_1}, \\hat{c_2}, ..., \\hat{c_n} \\} \\leftarrow\\ \\text{find optimal coefficients} \\end{align*}\\] Recall the formula below (See Equation \\(\\ref{eqn:eqnnumber3}\\)): \\[ \\hat{x} = (A^T \\cdotp A)^{-1} \\cdotp A^T \\cdotp y \\] where, this time, A is a matrix of basis functions and \\(\\hat{x}\\) is a vector of optimal coefficients, both form part in the below equation: \\[\\begin{align} \\hat{c} = (\\phi{i,j}(t)^T \\cdotp \\phi{i,j}(t))^{-1} \\cdotp \\phi{i,j}(t)^T \\cdotp S \\rightarrow\\ \\ \\ \\ \\hat{c} = (B^T \\cdotp B)^{-1} \\cdotp B^T \\cdotp S \\end{align}\\] In solving for the system of equations using matrix computation for \\(\\hat{c}\\), we end up with the optimal spline for the best fit: \\[\\begin{align} \\hat{S}(t) = \\sum_{i=0}^k \\hat{c_i} \\phi_{i,k}(t) \\end{align}\\] Interpretability of coefficients Recall that B-spline is a curve function consisting of a linear combination of coefficients and polynomials (basis functions). The critical point for a “B-spline” is the coefficients’ context. For these control points to be interpretable, it matters most to know that the coefficients can be treated as flexible control points. For example, being scalar to basis functions may not be enough. Some simple weight against the basis functions may be required to serve as regularizers that enforce penalty or reward. Either way, they add extra weight to the coefficients and add some nudge of constraint against the basis functions. Here, suppose we replace the coefficients with the weight function so that the formula below: \\[\\begin{align} B(t) = \\sum_{i=0}^{n} c_i \\phi_{i,k}(t) \\end{align}\\] is described into the following more flexible form: \\[\\begin{align} B(t) = \\sum_{i=0}^{n} \\omega_{i}(c_i) \\phi_{i,k}(t) \\end{align}\\] where: \\[ \\omega_{i}(c_i)\\ \\text{is a weight function of any kind} \\] From here, we can describe the weight functions in ways more contextual (or more interpretable) for the coefficients, and thus the implementation then varies. For example, when dealing with weights, we can impose a brute force approach or trial and error approach to approximate the weight and then estimate the Least-Square between the actual values and approximate values of the weights, both of which may be costly. Another approach is to approximate the likelihood of the weights by random sampling. This is where we talk about Bayesian methods, which we cover in Volume II of this book. 3.7.12 P-Spline Regression P-spline regression is a “Penalized B-spline regression”. Here, penalty is a way to smoothen the roughness of spline curves. Smoothness of splines If we look carefully at our optimal coefficients in previous section, we have the following (See Equation \\(\\ref{eqn:eqnnumber3}\\)): \\[\\begin{align} \\hat{c} = (B^T \\cdotp B)^{-1} \\cdotp B^T \\cdotp S \\end{align}\\] One way to optimize the smoothness of curves is to add penalty, \\(\\lambda\\), into the equation as such: \\[\\begin{align} \\hat{c}_{penalized} = (B^T \\cdotp B + \\lambda D_k^TD_k)^{-1} \\cdotp B^T \\cdotp S \\end{align}\\] where \\(D\\) is the matrix representation of the difference, \\(\\Delta_k\\). Here, we mention two ways to add a penalty, \\(\\lambda\\), to the objective function in terms of residuals: The Eilers-Marx method (Eilers and Marx - 1996): \\[\\begin{align} RSS_{penalty} = \\sum_{i=1}^m |\\hat{B}(t) - B(t)|^2 + \\lambda \\sum_{j=k+1}^m \\left( \\Delta^k c_j \\right)^2 \\end{align}\\] where \\(\\lambda &gt; 0\\) and \\(\\Delta^kc_i\\) is backward difference for \\(\\Delta{ci} = c_i - c_{i-1}\\). The Wand-Ormerod method (Wand and Ormerod - 2008): \\[\\begin{align} RSS_{penalty} = \\sum_{i=1}^m |\\hat{B}(t) - B(t)|^2 + \\lambda \\int_{min\\ t}^{ma\\ t} \\left( \\hat{B}&#39;&#39;(t;k) \\right)^2 dx \\end{align}\\] where k in \\(\\hat{B}&#39;&#39;(t;k)\\) is the k-th order of the B-spline. We leave readers to investigate more on those penalty approaches. 3.8 Approximating Polynomial Functions by Smoothing We have partially covered polynomial smoothing under the P-spline regression section. In this section, we explain polynomial smoothing methods. The main focus of our discussion is around LOWESS and LOESS; however, it helps to have a prior understanding of concepts that complement them. 3.8.1 Bin Smoothing Similar to piecewise techniques used in B-spline interpolation, the idea of bin smoothing is to discretize data into buckets called bins. There are different methods to perform Bin Smoothing. First, let us start with the following polynomial data: set.seed(2020) sample_size = 200 e = rnorm(n=sample_size, mean=0, sd=1) / 30 sample.poly = poly(1:sample_size, degree=3, simple=TRUE) data = sample.poly[,3] + e # add Gauss. residual using 3rd degree poly names(data ) = seq(1, sample_size) summary(data) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -0.2326270 -0.0663426 0.0068862 -0.0001126 0.0624455 0.2035865 Second, choose a window size. The following formulas are available to determine the window size: \\[\\begin{align*} W_{size}{}&amp;= 1 + 3.322 log N\\ \\ &amp; \\text{Sturge&#39;s Rule} \\\\ W_{size} &amp;= 2(IQR)n^{-1/3}\\ \\ &amp; \\text{Freedman-Diaconis&#39; Rule} \\\\ W_{size} &amp;= 3.49\\sigma n^{-1/3}\\ \\ &amp; \\text{Scott&#39;s Rule} \\\\ W_{size} &amp;= (\\text{sample size})^{1/3} \\times 2\\ \\ &amp; \\text{Rice&#39;s Rule} \\end{align*}\\] Here, we use Rice’s rule. (window_size = sample_size^(1/3)*2) ## [1] 11.69607 Third, choose a partitioning method. Partitioning methods: By equal width (bandwidth) - note here that width is an interval and also can be called window size or span. This works best for ordered (sorted data); thus, it may not be ideal for the smoothing we need. Nonetheless, here is an example naive implementation of binning by width. sample.data =c (4,8,9,12,18,20,23,50,61,70) bin_count = floor(length(sample.data)^(1/3)*2) # Rice&#39;s rule smallest = min(sample.data); largest = max(sample.data) interval = ceiling((largest - smallest) / bin_count) ordered.data = sort(sample.data, decreasing=FALSE) start = smallest bins = ordered.data intervals = ordered.data for (i in 1:bin_count) { end = start + interval idx = which(ordered.data &gt;= start &amp; ordered.data &lt;= end) intervals[idx] = paste0(start,&quot;-&quot;,end) bins[idx] = i start = end + 1 } binned.data = rbind(intervals , ordered.data) binned.data = rbind(binned.data , bins) binned.data[,1:8] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## intervals &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;4-21&quot; &quot;22-39&quot; &quot;40-57&quot; ## ordered.data &quot;4&quot; &quot;8&quot; &quot;9&quot; &quot;12&quot; &quot;18&quot; &quot;20&quot; &quot;23&quot; &quot;50&quot; ## bins &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;1&quot; &quot;2&quot; &quot;3&quot; By equal depth (frequency) - note that each window contains an equal number of data points. The idea is to slice the dataset into an equal number of data points. Let us use this partition for our illustration. bin_count = ceiling(length(data) / window_size) bins = rep(1:bin_count, rep(window_size, bin_count))[1:sample_size] binned.data = rbind(data, bins) binned.data[,1:5] ## 1 2 3 4 5 ## data -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627 ## bins 1.0000000 1.0000000 1.0000000 1.0000000 1.000000 Fourth, choose a smoothing method. Before that, let us add three rows to accommodate the three smoothing methods. mean = rep(0,sample_size) median = rep(0,sample_size) boundary = rep(0,sample_size) smooth.data = rbind(binned.data, mean ) smooth.data = rbind(smooth.data, median ) smooth.data = rbind(smooth.data, boundary ) smooth.data[,1:5] ## 1 2 3 4 5 ## data -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.232627 ## bins 1.0000000 1.0000000 1.0000000 1.0000000 1.000000 ## mean 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 ## median 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 ## boundary 0.0000000 0.0000000 0.0000000 0.0000000 0.000000 Smoothing methods: By median, mean - values in a bin are replaced with the mean (or median) of the bin. bin.means = aggregate(binned.data[1,], by=list(binned.data[2,]), mean) bin.median = aggregate(binned.data[1,], by=list(binned.data[2,]), median) By boundary - observations in a bin are replaced with the min or max value of the bin, depending on which boundary is closest. boundary &lt;- function(x) { n = length(x) a = min(x); b = max(x) for (i in 1:n) { dist_a = abs(a - x[i]) dist_b = abs(b - x[i]) if (dist_a &lt; dist_b ) { x[i] = a } else {x[i] = b} } x } bin.boundary = aggregate(binned.data[1,], by=list(binned.data[2,]), boundary) Fifth, populate bins with new values. for (i in 1:bin_count) { idx = which(smooth.data[2,] == i) smooth.data[3, idx] = bin.means[i,2] smooth.data[4, idx] = bin.median[i,2] smooth.data[5, idx] = bin.boundary[i,2][[1]] } smooth.data[,1:5] ## 1 2 3 4 5 ## data -0.1689877 -0.1605539 -0.1965348 -0.1872164 -0.2326270 ## bins 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 ## mean -0.1366121 -0.1366121 -0.1366121 -0.1366121 -0.1366121 ## median -0.1182712 -0.1182712 -0.1182712 -0.1182712 -0.1182712 ## boundary -0.2326270 -0.2326270 -0.2326270 -0.2326270 -0.2326270 Sixth, create a separate curve line for the groups (use the average). group.means = length(bin.means[,1]) curve.fit = c() location = 0; step = 0 for (i in 1:(group.means)) { idx = which(smooth.data[2,] == i) bin = smooth.data[3, idx] center = round( length(bin) / 2 ) location = step + center step = step + length(bin) curve.fit = cbind(curve.fit, location) } smooth.fit = as.numeric(curve.fit) Finally, let us plot. See 3.31. plot(NULL, xlim=range(1,sample_size), ylim=range(-0.3,0.2), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Bin Smoothing&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(data, col=&quot;grey&quot;) #lines(smooth.data[4,], col=&quot;magenta&quot;, pch=16) # median lines(smooth.data[5,], col=&quot;dodgerblue&quot;, pch=16) # boundary lines(smooth.data[3,], col=&quot;darksalmon&quot;, pch=16, lwd=2) # mean points(smooth.fit, bin.means[,2], col=&quot;black&quot;, pch=16) lines(smooth.fit, bin.means[,2], col=&quot;navyblue&quot;, pch=16) legend(40, -0.1, legend=c( &quot;boundary&quot;, &quot;mean&quot;, &quot;smooth.fit&quot;), col=c(&quot;dodgerblue&quot;, &quot;darksalmon&quot;, &quot;navyblue&quot;), , lty=1, cex=0.8) Figure 3.31: Bin Smoothing As an extension to our previous discussion around polynomial regression and our recent discussion around bin smoothing, here, we introduce two smoothing techniques: LOWESS and LOESS, which use local weighted regression techniques (Cleveland, W. S. et al. 1988). LOWESS and LOESS are polynomial smoothers that use locally weighted (linear) functions to fit a smooth model to each bin. LOWESS stands for locally weighted scatterplot smoothing and LOESS stands for locally estimated scatterplot smoothing. LOWESS uses the following two weighing functions: A tricube weight function for width (distance amongst neighboring points) (Cleveland, W. S. et al. 1988) (See also Kernel Smoothing for the tricube kernel): \\[\\begin{align} W_{(width)}(w) = \\begin{cases} \\left[1 - |w|^3 \\right]^3 &amp; |w| &lt; 1\\\\ 0 &amp; |w| \\ge 1 \\end{cases}\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ w = w_k(x_i) = \\frac{x_i - x_k}{d_k},\\ \\ \\ \\label{eqn:eqnnumber704} \\end{align}\\] Here, \\(\\mathbf{d_k}\\) is kth nearest neighbor (kth smallest distance) derived from a list of x distances arranged in ascending order, e.g. \\(d_i = \\{ |x_i - x_1|, |x_i - x_2|,|x_i - x_3|,...,|x_i - x_n| \\}_{sort}\\). \\[\\begin{align} d_k = knn(d_i^{(sort)}, kth),\\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ kth = f\\times n \\end{align}\\] where: n is the number of data points f is a proportion of n. Note that by setting the weight to zero when \\(|w| \\ge 1\\), it eliminates the influence of distant neighbors. An optional bisquare weight function for depth (to control the influence of outliers) - this is also called robust weighing: \\[\\begin{align} W_{(depth)}(w) = \\begin{cases} \\left[1 - |w|^2 \\right]^2 &amp; |w| &lt; 1\\\\ 0 &amp; |w| \\ge 1 \\end{cases} \\label{eqn:eqnnumber705} \\end{align}\\] \\[ where\\ \\ w = w_k(x_i) = \\frac{e_k}{6 \\times median(|e_1|, ...,|e_n|)} \\] The local weight becomes: \\[\\begin{align} W^{local}_k = W_{(width)}(w_k) W_{(depth)}(w_k) \\end{align}\\] Note that by setting the weight to zero when \\(|w| \\ge 1\\), it eliminates the influence of outliers. Now, recall our discussion around OLS in the polynomial regression section. \\[\\begin{align} RSS(\\beta) &amp;= \\sum_{k=1}^n | y_k - \\hat{y}_k |^2 = \\sum_{k=1}^n |\\epsilon_k|^2 \\\\ \\text{1st degree or multivariate} &amp;= \\sum_{k=1}^n \\left| y_k - \\left(\\beta_0 + \\sum_{j=1}^m \\beta_j x_{j,k} \\right) \\right| ^2 \\\\ \\text{higher degree} &amp;= \\sum_{k=1}^n \\left| y_k - \\left(\\beta_0 + \\sum_{j=1}^m \\beta_j x_k^j \\right) \\right| ^2 \\end{align}\\] where RSS is residual sum square. Here, we inject the local weight into RSS. The equation becomes: \\[\\begin{align} WSS(\\beta, w_k) &amp;= \\sum_{i=1}^n w_k | y_k - \\hat{y}_k |^2 = \\sum_{k=1}^n w_k |\\epsilon_k|^2 \\\\ \\text{1st degree or multivariate} &amp;= \\sum_{k=1}^n w_k \\left| y_k - \\left(\\beta_0 + \\sum_{j=1}^m \\beta_j x_{j,k} \\right) \\right| ^2 \\\\ \\text{higher degree} &amp;= \\sum_{k=1}^n w_k \\left| y_k - \\left(\\beta_0 + \\sum_{j=1}^m \\beta_j x_k^j \\right) \\right| ^2 \\end{align}\\] where WSS is weighted residual sum square and \\(\\mathbf{w_k}\\) is our local weight, \\(W^{(local)}_k\\). By performing weighted least squares, we generate a vector of coefficients that are weighted - we call them beta hats, \\(\\hat{\\beta}\\): For first degree equations, we derive the coefficients (beta hats) using the following formulation: \\[\\begin{align} \\hat{\\beta}_1 = \\frac{n\\sum_{k=1}^n{(x_ky_k)} - \\sum_{k=1}^n{x_k}\\sum_{k=1}^n{y_k}}{n\\sum_{k=1}^n{(x_k^2)} - (\\sum_{k=1}^n{x_k})^2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{align}\\] We then modify the \\(\\beta\\) formulae to inject the weighted function: \\[\\begin{align} \\hat{\\beta}_1 = \\frac{\\sum_{k=1}^n (w_k x_k y_k) - \\bar{x} \\cdot \\bar{y}\\sum_{k=1}^n w_k} {\\sum_{k=1} (w_k x_k^2) - \\bar{x}^2 \\cdot \\sum_{k=1}^n w_k} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} \\end{align}\\] where: \\[\\begin{align} \\bar{x} = \\frac{\\sum_{k=1}^n(w_k x_k)}{\\sum_{k=1}^n w_k} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\bar{y} = \\frac{\\sum_{k=1}^n(w_k y_k)}{\\sum_{k=1}^n w_k} \\end{align}\\] For multivariate equations and non-linear (e.g., quadratic) equations, recall the use of a Vandermonde matrix - for our matrix equation - under the polynomial regression section. \\[\\begin{align} \\hat{\\beta} \\approx (A^T \\cdot A)^{-1} \\cdot A^T \\cdot y\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ y = \\hat{\\beta}^TA \\end{align}\\] We modify the equation to include the weighted function: \\[\\begin{align} \\hat{\\beta} \\approx (A^T \\cdot W \\cdot A)^{-1} \\cdot A^T \\cdot W \\cdot y \\end{align}\\] Finally, we can perform local weighted linear regression to our data points. For LOWESS, we use a simple linear equation for regression: \\(y_k = \\beta_0 + \\beta_1 x_k\\). \\[\\begin{align} WSS(\\beta, W_k^{(local)}) = \\sum_{k=1}^n W_k^{local}(y_k - (\\beta_0 + \\beta_1 x_k)) \\end{align}\\] For LOESS, we also can use a quadratic (parabolic) equation for regression: \\(y_k = \\beta_0 + \\beta_1 x_k + \\beta_2 x_k^2\\). \\[\\begin{align} WSS(\\beta, W_k^{(local)}) = \\sum_{k=1}^n W_k^{local}(y_k - (\\beta_0 + \\beta_1 x_k + \\beta_2 x_k^2)) \\end{align}\\] Note that LOESS is a generalized polynomial version of LOWESS. For that reason, we use the normal matrix equation. Let us illustrate: First, let us create a distance table using the built-in R function called dist(): set.seed(2020) sample_size = n = 50 e = rnorm(n=sample_size, mean=0, sd=1) / 30 sample.poly = poly(1:sample_size, degree=3, simple=TRUE) y = sample.poly[,3] + e # add Gausian residual using 3rd degree poly x = sort( sample(seq(1,80), size=sample_size, replace=FALSE)) ymin = min(y); ymax = max(y); xmax = max(x) D = as.matrix( abs( dist(x, upper=TRUE))) colnames(D) = paste0(seq(1:length(x)), &quot;k&quot;) rownames(D) = paste0(seq(1:length(x)), &quot;i&quot;) D[1:10,1:10] # limit display to 10 ## 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k ## 1i 0 1 2 4 5 9 11 13 16 17 ## 2i 1 0 1 3 4 8 10 12 15 16 ## 3i 2 1 0 2 3 7 9 11 14 15 ## 4i 4 3 2 0 1 5 7 9 12 13 ## 5i 5 4 3 1 0 4 6 8 11 12 ## 6i 9 8 7 5 4 0 2 4 7 8 ## 7i 11 10 9 7 6 2 0 2 5 6 ## 8i 13 12 11 9 8 4 2 0 3 4 ## 9i 16 15 14 12 11 7 5 3 0 1 ## 10i 17 16 15 13 12 8 6 4 1 0 Second, now use a fraction number for our range of nearest neighbors: f = 0.40 n = length(x) # kth nearest neighbor (for our kth smallest distance) (kth = round( f * n )) ## [1] 20 Third, let us compute for kth nearest distance: knn &lt;- function(D, kth) { dk = c() for (i in 1:n) { sorted_dist = sort(D[i,], decreasing=FALSE) # kth nearest neighbor (kth smallest distance) dk [i] = sorted_dist[kth] } names(dk) = paste0(seq(1:length(x)), &quot;i&quot;) dk } (dk = knn(D,kth))[1:10] # limit display to 10 ## 1i 2i 3i 4i 5i 6i 7i 8i 9i 10i ## 31 30 29 27 26 22 20 18 16 16 Fourth, compute for the weight (for width): weight_width &lt;- function(D, dk) { w = abs(D) / dk # by assigning 1, this is equivalent to 0 for ( 1 - W^3)^3 if W &gt;=1 w[w &gt;= 1] = 1 # relaxed version of tricube # without the 70/81 scale (1 - w^3)^3 # tricube smoother } # limit display to 10x10 (W = round( weight_width(D, dk), 3))[1:10,1:10] ## 1k 2k 3k 4k 5k 6k 7k 8k 9k 10k ## 1i 1.000 1.000 0.999 0.994 0.987 0.928 0.872 0.795 0.642 0.582 ## 2i 1.000 1.000 1.000 0.997 0.993 0.944 0.893 0.820 0.670 0.610 ## 3i 0.999 1.000 1.000 0.999 0.997 0.958 0.913 0.845 0.699 0.640 ## 4i 0.990 0.996 0.999 1.000 1.000 0.981 0.949 0.893 0.759 0.701 ## 5i 0.979 0.989 0.995 1.000 1.000 0.989 0.964 0.915 0.790 0.733 ## 6i 0.808 0.863 0.906 0.965 0.982 1.000 0.998 0.982 0.906 0.863 ## 7i 0.579 0.670 0.751 0.877 0.921 0.997 1.000 0.997 0.954 0.921 ## 8i 0.242 0.348 0.460 0.670 0.759 0.967 0.996 1.000 0.986 0.967 ## 9i 0.000 0.005 0.036 0.193 0.308 0.769 0.911 0.980 1.000 0.999 ## 10i 0.000 0.000 0.005 0.100 0.193 0.670 0.850 0.954 0.999 1.000 Fifth, perform local regression using the computed weights for width (close neighbors). Generate our fitted y: A = matrix(c(rep(1, n), x), n, 2, byrow=FALSE) B = list() y.hat = c() residual = c() for (k in 1:n) { Wk = as.numeric( W[,k] ) Wk = diag(Wk) # least square B[[k]] = solve(t(A) %*% Wk %*% A) %*% t(A) %*% Wk %*% y beta = B[[k]] y.fit = beta[1] + beta[2] * x[k] # linear fit y.hat = c(y.hat, y.fit) residual = c(residual, abs(y[k] - y.fit)) } Sixth, let us work on the robust weight for outliers by computing for the residuals, e: weight_depth &lt;- function(residual) { n = length(residual) s = median(residual) w = c() for (k in 1:n) { e_k = residual[k] w = c(w, e_k / (6*s) ) } # by assigning 1, this is equivalent to 0 for ( 1 - W^2)^2 if W &gt;=1 w[ w &gt;= 1] = 1 (1 - w^2)^2 # bisquare smoother } w.depth = weight_depth(residual) round(w.depth,4)[1:8] # limit display to 8 ## [1] 0.9516 0.9835 0.9932 0.9891 0.9911 0.8109 0.7614 0.9652 Finally, perform smoothing using both width and depth weights: A = matrix(c(rep(1, n), x), n, 2, byrow=FALSE) B= list() y.hat2 = c() for (k in 1:n) { w.width = as.numeric( W[,k] ) w = diag(w.width * w.depth[k]) if ( length( which(w != 0) ) == 0 ) { w = diag(w.width) } B[[k]] = solve(t(A) %*% w %*% A) %*% t(A) %*% w %*% y beta = B[[k]] y.fit = beta[1] + beta[2] * x[k] y.hat2 = c(y.hat2, y.fit) } Here is a naive implementation of scatterplot smoothing for LOESS and LOWESS in R (emphasizing LOWESS). Also, note that the robustness of fit can be processed iteratively against the fit itself, which we skip implementing here: scatterplot_smoothing &lt;- function(x,y, f=0.40, span=0.75, smooth=&quot;lowess&quot;, degree=1) { n = length(x) weight_width &lt;- function(d, h) { w = d / h w[w &gt;= 1] = 1 # relaxed version of tricube # without the 70/81 scale (1 - w^3)^3 # tricube smoother } weight_depth &lt;- function(e) { s = median(e) w = c() for (k in 1:n) { e_k = e[k] w = c(w, e_k / (6*s) ) } w[ w &gt;= 1] = 1 (1 - w^2)^2 # bisquare smoother } knn &lt;- function(d_i, kth) { sort(d_i, decreasing=FALSE)[kth] } coeffs &lt;- function(A, y, W = 1) { n = nrow(A) if (length(W) == 1) { W = diag(W,n) } else { W = diag(W) } solve(t(A) %*% W %*% A) %*% t(A) %*% W %*% y } fit.lowess &lt;- function(B, x, degree=1) { B[1] + B[2] * x^degree } fit.loess &lt;- function(B, x, degree=2) { if (degree == 0) { B[1] } else if (degree == 1) { B[1] + B[2] * x } else if (degree == 2) { B[1] + B[2] * x + B[3] * x^2 } } fit &lt;- function(A, x, y, kth, fit.smooth, degree = 2) { # Generate distance of data points D = as.matrix( abs( dist(x, upper=TRUE))) y.hat = c() y.res = c() y.width = list() for (k in 1:n) { # kth nearest neighbor (kth smallest distance) dk = knn(D[k,], kth) w = weight_width(D[,k], dk) B = coeffs(A, y, w) y.fit = fit.smooth(B, x[k], degree) y.hat = c(y.hat, y.fit) y.res = c(y.res, abs(y[k] - y.hat)) y.width[[k]] = w } y.hat = c() for (k in 1:n) { w.width = y.width[[k]] w.depth = weight_depth(y.res) w = w.width * w.depth if (length(which(w != 0)) == 0) { w = w.width } B = coeffs(A, y, w) y.fit = fit.smooth(B, x[k], degree) y.hat = c(y.hat, y.fit) } list(&quot;x&quot;=x, &quot;y&quot;=y.hat) } lowess &lt;- function(x, y, f) { A = matrix(c(rep(1, n), x), n, 2, byrow=FALSE) # kth nearest neighbor kth = round(f * n) fit(A, x, y, kth, fit.lowess, 1) } loess &lt;- function(x, y, span, degree) { if (degree == 0) { A = matrix(c(rep(1, n)), n, 1, byrow=FALSE) } else if (degree == 1) { A = matrix(c(rep(1, n), x), n, 2, byrow=FALSE) } else if (degree == 2) { A = matrix(c(rep(1, n), x, x^2), n, 3, byrow=FALSE) } # kth nearest neighbor kth = round(span * n) fit(A, x, y, kth, fit.loess, degree) } if (smooth == &quot;lowess&quot;) { lowess(x, y, f) } else if (smooth == &quot;loess&quot;) { loess(x, y, span, degree) } } Let us plot the outcome of scatterplot smoothing function: set.seed(2020) sample_size = n = 100 e = rnorm(n=sample_size, mean=0, sd=1) / 30 sample.poly = poly(1:n, degree=3, simple=TRUE) y = sample.poly[,3] + e # add Gausian residual using 3rd degree poly x = sort( sample(seq(1,n), size=n, replace=TRUE)) ymin = min(y); ymax = max(y); xmin = min(x); xmax = max(x) D = as.matrix( abs( dist(x, upper=TRUE))) our.lowess.fit = scatterplot_smoothing(x, y, f=0.40, smooth=&quot;lowess&quot;) our.loess.fit = scatterplot_smoothing(x, y, span=0.40, smooth=&quot;loess&quot;, degree=2) lowess.fit = lowess(x, y, f=0.40 ) loess.model = loess(y ~ x, span=0.75, degree=2, family=c(&quot;gaussian&quot;)) loess.fit = stats::predict(loess.model) plot(NULL, xlim = range(xmin, xmax), ylim=range(ymin,ymax ), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Scatterplot Smoothing (Local Linear Regression)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, y, col=&quot;black&quot;) lines(our.lowess.fit, col=&quot;dodgerblue&quot;, lwd=1) lines(our.loess.fit, col=&quot;green&quot;, lwd=1) lines(lowess.fit, col=&quot;brown&quot;, lwd=1, lty=2) lines(loess.fit, col=&quot;magenta&quot;, lwd=1, lty=2) Figure 3.32: Scatterplot Smoothing 3.8.2 Kernel Smoothing Kernel Smoothing, also considered as Kernel Regression, is another smoothing or regression technique similar to LOESS and LOWESS smoothers. There are a few kernel functions that can be used for Kernel regression (estimation) (See Table 3.1) (Jośe E. Chacón J E. et al 2018; Cheruiyot L. R. et al 2020): Table 3.1: Kernel Functions Kernel — Formula: \\(I(x) = 1_{( \\mid x \\mid \\le 1)}\\) \\(\\mu_2(K)\\) R(K) Efficiency Epanechnikov \\(K(x) = \\frac{3}{4}(1 - x^2)\\cdot I(x)\\) \\(\\frac{1}{5}\\) \\(\\frac{3}{5}\\) 1.0000 Cosine \\(K(x) = \\frac{\\pi}{4}cos\\left(\\frac{\\pi}{2}x\\right)\\cdot I(x)\\) \\(1 - \\frac{8}{\\pi^2}\\) \\(\\frac{\\pi^2}{16}\\) 0.9995 Tricube \\(K(x) = \\frac{70}{81}(1 - \\mid x \\mid^3)^3\\cdot I(x)\\) \\(\\frac{35}{243}\\) \\(\\frac{175}{247}\\) 0.9979 Quartic \\(K(x) = \\frac{15}{16}(1 - x^2)^2\\cdot I(x)\\) \\(\\frac{1}{7}\\) \\(\\frac{5}{7}\\) 0.9939 Triweight \\(K(x) = \\frac{35}{32}(1 - x^2)^3\\cdot I(x)\\) \\(\\frac{1}{9}\\) \\(\\frac{350}{429}\\) 0.9867 Triangular \\(K(x) = (1 - \\mid x \\mid)\\cdot I(x)\\) \\(\\frac{1}{6}\\) \\(\\frac{2}{3}\\) 0.9859 Gaussian \\(K(x) = \\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}x^2}\\) \\(1\\) \\(\\frac{1}{2\\sqrt{\\pi}}\\) 0.9512 Uniform \\(K(x) = \\frac{1}{2}\\cdot I(x)\\) \\(\\frac{1}{3}\\) \\(\\frac{1}{2}\\) 0.9295 Logistic \\(K(x) = (e^x +_ 2 + e^{-x})^{-1}\\) \\(\\frac{\\pi^2}{3}\\) \\(\\frac{1}{6}\\) 0.8876 As shown in the table, the performance of the kernel functions in terms of efficiency is calculated based on the following equation and comparably measured against Epanechnikov Kernel, which effectively gets 100% efficiency: \\[\\begin{align} Efficiency = \\sqrt{\\mu_2(K)}\\cdot R(K) \\end{align}\\] \\[\\begin{align} where \\ \\ \\ \\ \\mu_2(K) = \\int x^2K(x)dx, \\ \\ \\ \\ \\ R(K) = \\int K^2(x) dx \\end{align}\\] For example, the efficiency of Epanechnikov Kernel is computed like so (let A = \\(\\mu_2(K)\\) and B = \\(R(K)\\)): \\[ A = \\mu_2(K) = \\frac{1}{5},\\ \\ \\ \\ \\ \\ B = R(K) = \\frac{3}{5}\\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ \\ Efficiency = \\sqrt{\\frac{1}{5}} \\cdot \\frac{3}{5} = 0.2683282. \\] epanechnikov_kernel &lt;-function(x) 3 / 4 * (1-x^2) f1 &lt;- function(x) x^2 * epanechnikov_kernel(x) f2 &lt;- function(x) (epanechnikov_kernel(x))^2 A = integrate(f = f1, lower = -1, upper = 1)$value B = integrate(f = f2, lower = -1, upper = 1)$value E_epanechnikov = sqrt(A) * B c(&quot;A&quot; = A, &quot;B&quot; = B, &quot;Efficiency&quot; = E_epanechnikov) ## A B Efficiency ## 0.2000000 0.6000000 0.2683282 The efficiency of Gaussian Kernel is computed like so: \\[ A = \\mu_2(K) = 1,\\ \\ \\ \\ \\ \\ B = R(K) = \\frac{1}{2\\sqrt{\\pi}}\\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ Efficiency = \\sqrt{1} \\cdot \\frac{1}{2\\sqrt{\\pi}} = 0.2820948. \\] gaussian_kernel &lt;-function(x) 1 / (sqrt(2 * pi)) * exp( -1/2 * x^2) f1 &lt;- function(x) x^2 * gaussian_kernel(x) f2 &lt;- function(x) (gaussian_kernel(x))^2 A = integrate(f = f1, lower = -Inf, upper = Inf)$value B = integrate(f = f2, lower = -Inf, upper = Inf)$value E_gaussian = sqrt(A) * B c(&quot;A&quot; = A, &quot;B&quot; = B, &quot;Efficiency&quot; = E_gaussian) ## A B Efficiency ## 1.0000000 0.2820948 0.2820948 We then compare each efficiency against the calculated efficiency of Epanechnikov efficiency: \\[\\begin{align*} E_{epanechnikov} {}&amp;= 0.2683282 / 0.2683282 = 1.0000 \\\\ E_{gaussian} &amp;= 0.2683282 / 0.2820948 = 0.9511987 = 0.9512 \\end{align*}\\] E.epanechnikov = E_epanechnikov / E_epanechnikov E.gaussian = E_epanechnikov / E_gaussian c(&quot;Efficiency (epanechnikov)&quot; = round(E.epanechnikov, 4), &quot;Efficiency (gaussian)&quot; = round(E.gaussian, 4)) ## Efficiency (epanechnikov) Efficiency (gaussian) ## 1.0000 0.9512 Any function can be used as a kernel as long as the following properties are satisfied (Zucchini W. 2003): \\[ \\int K(x)dx = 1,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\int xK(x)dx = 0,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_2(K) := \\int x^2K(x)dx &lt; \\infty \\] We can see the different functions plotted in Figure 3.33. Figure 3.33: Kernel Functions Now, to illustrate how Kernel Smoothing works, we start with a simple linear equation: \\[\\begin{align} y_i = \\beta_0 + \\beta_1 x_i + \\epsilon \\end{align}\\] We focus on a linear equation with the goal of estimating the following: \\[\\begin{align} \\hat{m}(x_i) \\approx \\beta_0 + \\beta_1 x_i \\end{align}\\] so that we end up with the following linear equation: \\[\\begin{align} \\hat{y}_i = \\hat{m}(x_i) + \\epsilon \\end{align}\\] The function \\(\\hat{m}(x)\\) is derived like so: \\[\\begin{align} \\mathbb{E}(Y) \\rightarrow \\mathbb{E}(Y|X = x) {}&amp;= \\int y \\cdot f(y|x) dy \\\\ &amp;= \\int y \\cdot \\frac{f(x,y)}{f(x)} dy\\\\ &amp;= \\hat{m}(x). \\end{align}\\] From there, we obtain an equation for \\(\\hat{m}(x)\\). For (NWKE, PCKE, GMKE), we have: \\[\\begin{align} \\hat{m}(x) = \\sum_{i=1}^n \\omega_i(x)y_i \\end{align}\\] For KDE, we have: \\[\\begin{align} \\hat{m}(x) = \\sum_{i=1}^n \\omega_i(x) \\end{align}\\] We then perform non-parametric estimation using a Kernel Estimator, \\(\\omega_i(x)\\). There are choices for \\(\\omega_i(x)\\): Nadaraya-Watson Kernel Estimator (NWKE): \\[\\begin{align} \\omega_i(x) {}&amp;= \\frac{K \\left(\\frac{x - x_i}{h}\\right)} {\\sum_{j=1}^n K \\left(\\frac{x - x_j}{h}\\right)}\\ \\ \\ \\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ \\ \\ \\\\ \\omega_i(x) &amp;= \\frac{K \\left(\\frac{\\|x - x_i\\|_p}{h}\\right)} {\\sum_{j=1}^n K \\left(\\frac{\\|x - x_j\\|_p}{h}\\right)}\\ \\ \\ \\text{for multi-dimension} \\end{align}\\] Priestley-Chao Kernel Estimator (PCKE): \\[\\begin{align} \\omega_i(x) {}&amp;= \\frac{\\psi}{h} K\\left(\\frac{x - x_{(i+1)}}{h}\\right) \\ \\ where\\ \\psi = \\frac{(b - a)}{n}\\ \\ \\leftarrow\\ \\ \\ \\text{equally spaced}\\\\ \\omega_i(x) {}&amp;= \\frac{1}{h}(x_{(i+1)} - x_i) K\\left(\\frac{x - x_{(i+1)}}{h}\\right) \\ \\ \\leftarrow\\ \\ \\ \\text{not equally spaced} \\end{align}\\] Gasser-Muller Kernel Estimator (GMKE): \\[\\begin{align} \\omega_i(x) = \\frac{1}{h}\\left|\\int_{s_{i-1}}^{s_i} K \\left(\\frac{ x - t}{h}\\right)dt \\right|\\ \\ \\ \\ \\ \\leftarrow \\ where\\ \\ s_i = \\frac{x_i + x_{(i+1)}}{2} \\end{align}\\] Parzen-Rosenblatt window - Kernel Density estimator (KDE): \\[\\begin{align} \\omega_i(x) {}&amp;= \\frac{1}{n}\\sum_{j=1}^n \\frac{1}{h}K\\left(\\frac{x - x_i}{h}\\right) \\ \\ \\ \\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ \\ \\ \\\\ \\omega_i(x) &amp;= \\frac{1}{n}\\sum_{j=1}^n \\frac{1}{h^d}K\\left(\\frac{\\|x - x_i\\|_p}{h}\\right) \\ \\ \\ \\text{for multi-dimension} \\end{align}\\] where: d - d-dimensions p - p-norm, e.g. 2 for euclidean The kernel estimators rely on: K - a choice of kernel as above (e.g. Gaussian, Box, Epanechnikov). h - the bandwidth. Let us now illustrate kernel smoothing by choosing one combination to implement in R. For our kernel function, let us choose Gaussian Kernel. For our kernel estimator, let us choose Nadaraya-Watson (NWKE). Here is a naive implementation of our kernel regression (smoothing) in R code using the chosen combination: K &lt;- function(x, kernel = &quot;normal&quot;) { I &lt;- function(x) { one = which(x&lt;= 1); zero = which(x&gt; 1) x[one] = 1; x[zero] = 0 x } switch (kernel, &quot;epanechnikov&quot; = 3/4 * (1 - x^2) * I(x), &quot;normal&quot; = 1/sqrt(2 * pi) * exp(-1/2 * x ^2), &quot;tricube&quot; = 70/81 *( 1 - abs(x)^3)^3 * I(x), &quot;rectangular&quot; = 1 / 2 * I(x), &quot;triangular&quot; = (1 - abs(x)) * I(x), &quot;quartic&quot; = 15/16 * (1 - x^2)^2 * I(x) ) } estimator &lt;- function(X, x, y, h, etype = &quot;nwke&quot;, kernel = &quot;normal&quot;) { nwke &lt;- function(X, x, y, h, kernel) { h = 0.25 * h # see ksmooth for adjustment of bandwidth n = length(x); m = length(X) mx = c() for (k in 1:m) { w = c() for (i in 1:n) { numer = K ( ( X[k] - x[i] ) / h , kernel ) denom = sum ( K ( ( X[k] - x ) / h , kernel ) ) w = c(w, numer / denom ) } mx = c(mx, sum( w * y ) ) } mx } if (etype == &quot;nwke&quot;) { nwke(X, x, y, h, kernel) } } kernel_smoothing &lt;- function( x, y, kernel=&quot;normal&quot;, h = 1) { m &lt;- estimator xmin = min(x); xmax = max(x) X = seq(xmin, xmax, length.out=100) Y = m(X, x, y, h, etype=&quot;nwke&quot;, kernel) list(&quot;x&quot;=X, &quot;y&quot;=Y) } Let us plot and introduce a built-in R function called ksmooth() (see Figure 3.34). set.seed(2020) sample_size = n = 100 e = rnorm(n=sample_size, mean=0, sd=1) / 30 sample.poly = poly(1:n, degree=4, simple=TRUE) y = sample.poly[,4] + e # add Gaussian residual using 3rd degree poly x = sort( sample(seq(1,n), size=n, replace=FALSE)) ymin = min(y); ymax = max(y); xmin = min(x); xmax = max(x) # Run our own NW implementation nwke.fit = kernel_smoothing(x, y, kernel=&quot;normal&quot;, h = 7 ) # Use ksmooth ksmooth.fit = ksmooth(x, y, kernel = c( &quot;normal&quot;), bandwidth = 5 ) plot(NULL, xlim = range(xmin, xmax), ylim=range(ymin,ymax ), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Kernel Smoothing&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, y, col=&quot;black&quot;) lines(ksmooth.fit, col=&quot;navyblue&quot;) lines(nwke.fit, col=&quot;brown&quot;) legend(3, ymax , legend=c( &quot;our nwke(h=7)&quot;, &quot;built-in ksmooth(bandwidth=5)&quot;), col=c( &quot;brown&quot;, &quot;navyblue&quot;), lty=1, cex=0.8) Figure 3.34: Kernel Smoothing Note that our curve overlaps right on top of the curve fit produced by the ksmooth() function. It is an exact match; however, we can accomplish this only because we arbitrarily choose a bandwidth (\\(h=7\\)), which is practically not optimal by hand. Choosing an optimal bandwidth and the correct Kernel is a good exercise and study in Kernel regression or Kernel Smoothing. For bandwidth selection, we have a few choices: First, we can use cross-validation techniques. LOOCV (Leave One Out Cross-Validation) K-Fold CV (K-Fold Cross-Validation) The idea is to have an initial list of random bandwidths as data points for cross-validation. We then use mean squared error (MSE) to evaluate the cross-validation result. The result with the least MSE serves as the optimal bandwidth. The equation for MSE is shown below: \\[\\begin{align} MSE {}&amp;= \\mathbb{E}\\left[(f(x) - \\hat{f}(x)^2\\right] \\\\ &amp;= Bias(\\hat{f}(x))^2 + Var(\\hat{f}(x)) \\\\ &amp;= \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{y}_i\\right)^2 \\end{align}\\] Note that MSE is measured based on its two components and how the trade-off plays along: Bias and Variance. We can find more discussion around adjusting the components in Chapter 6 (Statistical Computation) under the Significance of Regression Section and Chapter 9 (Computational Learning I) under the Regularization Section. For least MSE equation, we have: \\[\\begin{align} CV(bandwidth) = \\underset{h}{\\mathrm{argmin}}\\ MSE(h) \\end{align}\\] We cover more of cross-validation topic in Chapter 6 (Statistical Computation) under Model Selection Section and Chapter 9 (Computational Learning I). There are also other measurements adapted from MSE: MISE - mean integrated squared error (Jones 1990). We simply integrate MSE like so: \\[\\begin{align} MISE(h) {}&amp;= \\int MSE(h) dx \\\\ &amp;= \\int Bias(\\hat{f}(x))^2 dx + \\int Var(\\hat{f}(x)) dx \\\\ &amp;= \\int(f(x) - \\hat{f}(x))^2 dx + + \\int Var(\\hat{f}(x)) dx \\end{align}\\] AMISE - asymptotic limit of mean integrated squared error (Scott 1992 and Wand and Jones 1995): \\[\\begin{align} AMISE(h) = \\frac{1}{Nh} R(K) + \\frac{1}{4}h^4 u_2(K)^2 R(f&#39;&#39;) \\end{align}\\] A simplification (or derivation) of the above AMISE equation is as follows: \\[\\begin{align} h_{(AMISE)} = \\left(\\frac{R(K)^{\\frac{1}{5}}}{\\mu_2(K)^\\frac{2}{5}R(f&#39;&#39;)^{\\frac{1}{5}}}\\right) n^{-\\frac{1}{5}} = \\left(\\frac{R(K)}{\\mu_2(K)^2R(f&#39;&#39;)}\\right)^{\\frac{1}{5}} n^{-\\frac{1}{5}} \\end{align}\\] where: \\[\\begin{align} R(K) = \\int K(x)^2 dx,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_2 (K) = \\int x^2K(x) dx,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ R(f&#39;&#39;) = \\int f&#39;&#39;(y)^2 dy \\end{align}\\] Second, for optimal bandwidth calculation, we can use the Silverman Rule of Thumb, also called Normal Rule of Thumb. For example, when using Gaussian kernel, we use the below equation (Silverman 1986, Scott 1992, Jones et al. 1996, Venables-Ripley 2002): \\[\\begin{align} h_{(optimal)} = \\left(\\frac{4 \\hat{\\sigma} ^5}{3n}\\right)^{\\frac{1}{5}} = 1.06 \\hat{\\sigma} n^{-\\frac{1}{5}} \\end{align}\\] A variant version was proposed (Silverman and Scott 1992) to avoid oversmoothing: \\[\\begin{align} h_{(optimal)} = 0.9 A n^{-\\frac{1}{5}}\\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ A = min\\left(\\hat{\\sigma}, \\frac{IQR(x)}{1.349}\\right) \\end{align}\\] where: \\(\\hat{\\sigma}\\) is the sample standard deviation IQR is inter-quartile range The implementation of KDE and sample bandwidth choices are discussed in Chapter 5 (Probability and Distribution) under the Non-parametric distribution Section, in which we cover KDE in detail as part of estimating probability density. 3.9 Polynomial Optimization This section introduces the basic concept of Linear Programming and Optimization. The importance of such a subject becomes apparent in Computational Learning Chapters (Volume III). We start with the following two examples of a convex 2-polytope graphs from Figure 3.35. The first graph has solutions within the feasible region bounded by four linear equations. The second graph has solutions within the feasible region bounded by a linear equation and a convex quadratic equation. Figure 3.35: Linear Programming Our goal is to find the optimal value (a maximum and a minimum) of a given objective function - also called linear cost function. Our objective function is subject to inequality constraintsin our case. Let us start with an objective function for the first graph subject to four inequality constraints corresponding to the four equations. \\[ \\begin{array}{rl} \\mathbf{\\text{objective:}} &amp; 3x + 4y = z\\\\ \\mathbf{\\text{subject to:}} &amp; y \\le -\\frac{1}{3}x + 3\\\\ &amp; y \\ge -3x + 3\\\\ &amp; x \\le 3\\\\ &amp; y \\ge 0 \\end{array} \\] Note that the first graph shows four intersections (corners) around the region. To solve the problem, let us use those intersecting points. \\[ \\begin{array}{rrl} (x, y): &amp; 3x + 4y &amp; \\text{optimal value} \\\\ ====== &amp; ========= &amp; ======== \\\\ (0,3) :&amp; 3(0) + 4(3) =12 &amp; \\\\ (3, 2) :&amp; 3(3) + 4(2) =17 &amp; \\text{(maximum)} \\\\ (1,0) :&amp; 3(1) + 4(0) =\\ \\ 3 &amp; \\text{(minimum)} \\\\ (3,0) :&amp; 3( 3) + 4(0) =\\ \\ 9 &amp; \\end{array} \\] The maximum value for z is 17 at (3,2) and the minimum value is 3 at (1, 0). 3.9.1 Simplex Method Alternatively, rather than relying on a graph to visualize the feasible region, we can use the Simplex Method to find the optimal solution - maximum or minimum value of z. First, transform inequality equations into equality equations. To do this, we use the following guide: For \\(\\mathbf{\\le}\\) inequality, we add a slack variable (\\(s_i\\)) to the left hand side (LHS) of the inequality. For \\(\\mathbf{\\ge}\\) inequality, we add an excess or surplus variable (\\(e_i\\)) and an artificial variable (\\(a_i\\)) to the LHS of the inequality. For \\(\\mathbf{=}\\) equality, we add an artificial variable (\\(a_i\\)) to the LHS of the equality. For each artificial variable added to the constraints, subtract a term from the objective function, namely (\\(M \\times a_i\\)), where M is an arbitrarily large number. If the objective function is for minimization, then we add the term instead. If the right-hand side (RHS) constant is negative, multiply the inequality with \\(\\mathbf{(-1)}\\). Second, let us re-arrange our equations such that we have the variables located at the left-hand side (LHS) and the constant located at the right-hand side (RHS). \\[ \\left\\{ \\begin{array}{lrrrrrrrr} y &amp;\\le&amp; -\\frac{1}{3}x &amp;+&amp; 3\\\\ y &amp;\\ge&amp; -3x &amp;+&amp; 3\\\\ x &amp;\\le&amp; 3\\\\ 3x &amp;+&amp; 4y &amp;=&amp; z \\end{array} \\right\\}\\rightarrow \\left\\{ \\begin{array}{rrrrrrrrrrrrrr} \\frac{1}{3}x &amp;+&amp; 1y &amp;\\le&amp; 3\\\\ 3x &amp;+&amp; 1y &amp;\\ge&amp; 3\\\\ 1x &amp;+&amp; 0y &amp;\\le&amp; 3\\\\ 3x &amp;+&amp; 4y &amp;=&amp; z \\end{array} \\right\\} \\] Third, let us start adding variables. The slack, surplus, and artificial variables serve as placeholder buffers that indicate how much difference the left-hand side is from the right-hand side constant. \\[ \\left\\{ \\begin{array}{llllllllllllllllllllr} \\frac{1}{3}x &amp;+&amp; 1y &amp;+&amp; s_1 &amp;=&amp; 3\\\\ 3x &amp;+&amp; 1y &amp;-&amp; e_1 &amp;=&amp; 3\\\\ 1x &amp;+&amp; 0y &amp;+&amp; s_2 &amp;=&amp; 3\\\\ &amp;&amp; 3x &amp;+&amp; 4y &amp;=&amp; z \\end{array} \\right\\} \\rightarrow \\left\\{ \\begin{array}{lllllllllllllllllllllr} 1x &amp;+&amp; 3y &amp;+&amp; s_1 &amp;&amp;&amp;=&amp; 9\\\\ 3x &amp;+&amp; 1y &amp;-&amp; e_1 &amp;+&amp; a_1 &amp;=&amp; 3\\\\ 1x &amp;+&amp; 0y &amp;+&amp; s_2 &amp;&amp; &amp;=&amp; 3\\\\ 3x &amp;+&amp; 4y &amp;-&amp; Ma_1 &amp;&amp; &amp;=&amp; z \\end{array} \\right\\} \\] There are a few notes to mention here. The first note is that the value of \\(\\mathbf{a_1}\\) equals \\(-3x - 1y + e_1 + 3\\). Therefore, z can be expanded as such (note the use of Big M): \\[ \\begin{array}{lll} 3x + 4y - M(-3x - 1y + e_1 + 3) &amp;= z \\\\ 3x + 4y + 3Mx + My - Me_1 - 3M &amp;= z\\\\ (3+3M)x + (4+M)y - Me_1 - 3M &amp;= z\\\\ z -(3+3M)x -(4+M)y + Me_1 &amp;= -3M \\end{array} \\] The second note is that we have added four new constraints to cover the four basic variables: \\[ s_1, s_2, e_1, a_1 \\ge 0 \\] Lastly, introducing an artificial variable to the equations avoids us from ending into an infeasible solution, for example when we add surplus variables. Because of the existence of artificial variables in the problem, let us use the Big M Method and avoid a two-phase approach. The M indicates a large number that acts as a buffer variable to avoid an infeasible solution. Fourth, we can now create an initial augmented matrix called simplex tableau based on the transformed equalities. See Figure 3.36. Figure 3.36: Simplex Method (Maximization) In the figure, the initial tableau shows that column x has the most negative number, which becomes our entering variable. \\(\\mathbf{A_1}\\) becomes our departing variable (based on the ratio test - divide b by x to get the row with the smallest positive quotient). Here, the pivot element is 3 - meaning all values in the column need to be zeroed out except the pivot element. We perform transformation using the equations under the Oper column for rows, namely S1, S2, and Z. Doing so gets us the 1st transformation tableau. Then, the pivot element is transformed to 1 with the rest of the rows. Then, under the Base column of the 1st transformed tableau, we see \\(\\mathbf{X}\\) row taking the place of \\(\\mathbf{A_1}\\). We then repeat the process. We see that column Y has the most negative number, and the smallest positive quotient falls under row \\(\\mathbf{S_1}\\) - the departing variable. The pivot element is also 8/3. With that, we run the iteration until we reach the final transformation, giving the following conclusion: The optimal (maximal) solution is \\(\\mathbf{z} = 17\\) at (3,2) with the following base values: \\[ x = 3\\ \\ \\ \\ \\ \\ \\ y = 2\\ \\ \\ \\ \\ \\ \\ s_1 = 0\\ \\ \\ \\ \\ \\ \\ s_2 = 0\\ \\ \\ \\ \\ \\ \\ e_1 = 8\\ \\ \\ \\ \\ \\ \\ a_1 = 0 \\] If our objective function is set for a minimizaton problem, then we add \\(Ma_i\\) rather than subtract. \\[ \\begin{array}{lll} 3x + 4y + M(-3x - 1y + e_1 + 3) &amp;= z \\\\ 3x + 4y - 3Mx - My + Me_1 + 3M &amp;= z\\\\ (3-3M)x + (4-M)y + Me_1 + 3M &amp;= z\\\\ z -(3-3M)x -(4-M)y - Me_1 &amp;= 3M \\end{array} \\] From there, we follow the same Big M Method. See Figure 3.37. Figure 3.37: Simplex Method (Minimization) The optimal (minimal) solution is \\(\\mathbf{z} = 3\\) at (1,0) with the following base values: \\[ x = 1\\ \\ \\ \\ \\ \\ \\ y = 0\\ \\ \\ \\ \\ \\ \\ s_1 = 8\\ \\ \\ \\ \\ \\ \\ s_2 = 2\\ \\ \\ \\ \\ \\ \\ e_1 = 0\\ \\ \\ \\ \\ \\ \\ a_1 = 0 \\] The same approach applies to the second graph, which has a convex quadratic equation. We just need to analyze a given optimization problem statement and see if there is a solution based on the given two constraints. \\[ \\mathbf{\\text{subject to: }} y \\le 2\\ \\text{and}\\ y \\ge \\frac{1}{2}x^2 \\] 3.9.2 Dual Simplex In this section, let us get more familiar with the Theory of Duality in which our original problem is in Primal form, which can be transformed into its Dual form. For constrained minimization problem, we have the following Canonical-Primal formulation: \\[ \\begin{array}{llll} \\mathbf{\\text{minimize:}} &amp; \\mathbf{c}^T\\mathbf{x} = z &amp;\\leftarrow\\ \\ \\ \\sum_{j=1}^n c_j x_j = z\\\\ \\mathbf{\\text{subject to:}} &amp; A \\mathbf{x} \\ge \\mathbf{b} &amp;\\leftarrow\\ \\ \\ \\sum_{j=1}^n a_{ij} x_j \\ge \\mathbf{b}_i &amp;\\forall i = 1,..,m\\\\ &amp; \\mathbf{x} \\ge \\mathbf{0} &amp;\\leftarrow\\ \\ \\ \\mathbf{x}_j \\ge \\mathbf{0} &amp; \\forall j = 1,..,n \\end{array} \\] where m is the number of constraints, and n is the number of decision variables. For constrained maximization problem, we have the following Canonical-Dual formulation: \\[ \\begin{array}{llll} \\mathbf{\\text{maximize:}} &amp; \\mathbf{b}^T\\mathbf{y} = z &amp;\\leftarrow\\ \\ \\ \\sum_{i=1}^m b_i y_i = z\\\\ \\mathbf{\\text{subject to:}} &amp; A^T \\mathbf{y} \\le \\mathbf{c} &amp;\\leftarrow\\ \\ \\ \\sum_{i=1}^m a_{ij} y_i \\le \\mathbf{c}_j &amp;\\forall j = 1,..,n\\\\ &amp; \\mathbf{y} \\ge \\mathbf{0} &amp;\\leftarrow\\ \\ \\ \\mathbf{y}_i \\ge \\mathbf{0} &amp; \\forall i = 1,..,m \\end{array} \\] A more general and comprehensive guide is expressed in Figure 3.38 (Ekeocha R.J. et al. 2018). Figure 3.38: Duality Theory To illustrate, let us use the same example as we demonstrated recently. Here, we maximize the objective function - in our case, this characterizes the Dual formulation of our problem. Let us ensure our inequalities are transformed into \\(\\le\\) constraints. \\[ \\begin{array}{rlllll} \\mathbf{\\text{objective function}}&amp;:\\ 3y_1 + 4y_2 = z\\ \\ \\ \\text{(maximize)}\\\\ \\mathbf{\\text{subject to}}&amp;:\\ +1y_1 + +3y_2 \\le +9\\\\ &amp;:\\ -3y_1 + - 1y_2 \\le -3\\\\ &amp;:\\ +1y_1 + +0y_2 \\le +3\\\\ \\end{array} \\] The constraints above are re-arranged from their original form. Note that we also modified the variable names to follow the rules mentioned above - so not to confuse, we have y1 for x and y2 for y: \\[ \\left\\{ \\begin{array}{lrrrrrrrr} y &amp;\\le&amp; -\\frac{1}{3}x &amp;+&amp; 3\\\\ y &amp;\\ge&amp; -3x &amp;+&amp; 3\\\\ x &amp;\\le&amp; 3\\\\ \\end{array} \\right\\}\\rightarrow \\left\\{ \\begin{array}{rrrrrrrrrrrrrr} 1y_1 &amp;+&amp; 3y_2 &amp;\\le&amp; 9\\\\ -3y_1 &amp;+&amp; -1y_2 &amp;\\le&amp; -3\\\\ 1y_1 &amp;+&amp; 0y_2 &amp;\\le&amp; 3\\\\ \\end{array} \\right\\} \\] Based on the rule above, let us discover the Primal formulation of the problem. Then, to write our Dual Problem to its Primal formulation, we perform the following transposition of the coefficients: \\[ \\left\\{ \\begin{array}{rrrrrrrrrrrrrr} 1 &amp;+&amp; 3 &amp;\\le&amp; 9\\\\ -3 &amp;+&amp; -1 &amp;\\le&amp; -3\\\\ 1 &amp;+&amp; 0 &amp;\\le&amp; 3\\\\ 3 &amp;+&amp; + 4 &amp;=&amp; 0 \\end{array} \\right\\}\\rightarrow \\left\\{ \\begin{array}{rrrrrrrrrrrrrr} 1 &amp;+&amp; -3 &amp;+&amp; 1 &amp;\\ge&amp; 3\\\\ 3 &amp;+&amp; -1 &amp;+&amp; 0 &amp;\\ge&amp; 4\\\\ 9 &amp;+&amp; -3 &amp;+&amp; 3 &amp;\\ge&amp; 0\\\\ \\end{array} \\right\\} \\] From here, we can now express the Primal formulation like so: \\[ \\begin{array}{rlllll} \\mathbf{\\text{objective function}}&amp;:\\ 9x_1 + -3x_2 + 3x_3 = z\\ \\ \\ \\text{(minimize)}\\\\ \\mathbf{\\text{subject to}}&amp;:\\ 1x_1 + -3x_2 + 1x_3 \\ge 3\\\\ &amp;:\\ 3x_1 + -1x_2 + 0x_3 \\ge 4\\\\ \\end{array} \\] We can then use Simplex Method to solve for both maximal and minimal values. A good case of an optimization problem has to do with blending pastry ingredients, which is a typical toy example. Our goal is to minimize cost (the primal problem) subject to a minimum amount of ingredients. On the other hand, the dual problem is to maximize the number of ingredients subject to a maximum allowable cost. We leave readers to investigate the standard forms of an optimization problem that involves one of the optimal forms to be unrestricted. Refer to fig 3.38. 3.9.3 Primal-Dual Formulation Let us now contemplate a case in which our equations are non-linear. As we move to Lagrangian Multiplier methods, this becomes apparent in the next section. For intuition, let us review Figure 3.39. Figure 3.39: Primal-Dual Problem A weak duality theorem is subject to the following such that the objective function being maximized is less or equal to the objective function being minimized.: \\[\\begin{align} \\underbrace{\\text{max:}\\ \\mathbf{b}^T \\mathbf{x}}_{D(\\alpha)} \\le \\underbrace{\\text{min:}\\ \\mathbf{c}^T\\mathbf{y}}_{P(w)} \\end{align}\\] A strong duality happens when the duality gap equates to zero such that, by optimizing \\(\\mathbf{w}^*\\) and \\(\\mathbf{\\alpha}^*\\), we arrive at the objective functions being equal. \\[\\begin{align} P(w^*) = D(\\alpha^*) \\end{align}\\] We cover more of this topic in Chapter 10 (Computational Learning II) under the Non-Linear SVM section. 3.9.4 Lagrange Multiplier Before switching context to a new area, let us introduce Lagrange Multiplier, denoted by the symbol lambda (\\(\\lambda\\)). For intuition, let us use Figure 3.40. Figure 3.40: Lagrange Multiplier The goal is to find the point called extremum at which a function, namely \\(f(x)\\), is maximized up to (but barely touching) a level curve (contour) of a constraining function, namely \\(g(x)\\). We deal with vectors of the functions called gradients denoted as \\(\\nabla\\) as these gradients are perpendicular (or normal) to their respective tangent lines and are scaled using the Lagrange multiplier \\(\\lambda\\). We have a maximizing/minimizing function and a constraining function, scaled using Lagrange multiplier. \\[ \\underbrace{f(x)}_\\text{max/minimizing function}\\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{g(x)}_\\text{inequality constraint}\\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{h(x)}_\\text{equality constraint} \\] Mathematically, we are merely solving systems of equations. Here, there is only one constraint. \\[\\begin{align} \\nabla f(X) = \\lambda_1 \\times \\nabla g(X) ,\\ \\ \\ \\ \\ \\ g(X) = c \\end{align}\\] The equation for multiple constraints is written as: \\[\\begin{align} \\nabla f(X) = \\sum_{i=1}^n \\lambda^{(1)}_i \\times \\nabla g_i(X) + \\sum_{j=1}^m \\lambda^{(2)}_j \\times \\nabla h_j(X), \\end{align}\\] \\[ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{array}{ll} \\forall i: \\ g_i(X) = c^{(a)}_i\\\\ \\forall j: \\ h_j(X) = c^{(b)}_j \\end{array} \\] The Lagrange multiplier allows us to find the extremum or critical point as it allows the function f(X) to be in multiples of the constraining function g(X). To find the extremum, we use the following Lagrangian function: \\[\\begin{align} \\mathcal{L}(X, \\lambda) = f(X) - \\sum_{i=1}^n \\lambda_i \\times \\left( g_i(X) - c_i\\right) \\end{align}\\] From there, we perform derivatives on the lagrangian function: \\[\\begin{align} \\nabla_{X,Y,Z} \\mathcal{L}(X, Y, Z) = 0 \\end{align}\\] That is to say, perform partial derivatives with respect to each of the variables, namely, \\(x, y, z\\), setting each equation to zero: \\[ \\frac{\\partial}{\\partial X} \\mathcal{L}(X, Y,Z) = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial}{\\partial Y} \\mathcal{L}(X, Y, Z) = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial}{\\partial Z} \\mathcal{L}(X, Y, Z) = 0 \\] To illustrate, suppose we need to maximize \\(f(x,y,z)\\) subject to two constraints \\(g_1(x,y,z) = 0\\) and \\(g_2(x,y,z) = 0\\): \\[ f(x, y, z) = x - y^2 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ g_1(x, y, z) \\equiv y - z^2 = 0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ g_2(x, y, z) \\equiv z - x^2 = 0 \\] The gradient of \\(f(x,y, z)\\) is derived as. \\[\\begin{align*} \\frac{\\partial}{\\partial x} f(x,y,z) {}&amp;= \\frac{\\partial}{\\partial x} \\left(x - y^2 \\right) = 1\\\\ \\frac{\\partial}{\\partial y} f(x,y,z) &amp;= \\frac{\\partial}{\\partial y} \\left(x - y^2 \\right) = -2y\\\\ \\frac{\\partial}{\\partial z} f(x,y,z) &amp;= \\frac{\\partial}{\\partial z} \\left(x - y^2 \\right) = 0\\\\ \\nabla f(x,y,z) &amp;= \\left[\\begin{array}{r} 1 \\\\ -2y \\\\ 0 \\end{array} \\right] \\end{align*}\\] The gradient of \\(g_1(x,y, z)\\) is derived as. \\[\\begin{align*} \\frac{\\partial}{\\partial x} g_1(x,y,z) {}&amp;= \\frac{\\partial}{\\partial x} (y - z^2) = 0\\\\ \\frac{\\partial}{\\partial y} g_1(x,y,z) &amp;= \\frac{\\partial}{\\partial x} (y - z^2) = 1\\\\ \\frac{\\partial}{\\partial z} g_1(x,y,z) &amp;= \\frac{\\partial}{\\partial x} (y - z^2) = -2z\\\\ \\nabla g_1(x,y, z) &amp;= \\left[\\begin{array}{r} 0 \\\\1\\\\-2z \\end{array}\\right] \\end{align*}\\] The gradient of \\(g_2(x,y, z)\\) is derived as. \\[\\begin{align*} \\frac{\\partial}{\\partial x} g_2(x,y,z) {}&amp;= \\frac{\\partial}{\\partial x} \\left(z - x^2\\right) = -2x\\\\ \\frac{\\partial}{\\partial y} g_2(x,y,z) &amp;= \\frac{\\partial}{\\partial y} \\left(z - x^2\\ \\right) = 0\\\\ \\frac{\\partial}{\\partial z} g_2(x,y,z) &amp;= \\frac{\\partial}{\\partial z} \\left(z - x^2\\right) = 1\\\\ \\nabla g_2(x,y, z) &amp;= \\left[\\begin{array}{r} -2x \\\\ 0 \\\\ 1 \\end{array} \\right] \\end{align*}\\] To validate, we can derive the functions, e.g. f(x,y,z), in R like so: dx = D(expression(x - y^2), &quot;x&quot;) dy = D(expression(x - y^2), &quot;y&quot;) dz = D(expression(x - y^2), &quot;z&quot;) as.data.frame(t(c(&quot;dx&quot; = dx, &quot;dy&quot; = paste0(dy, collapse=&quot;&quot;), &quot;dz&quot; = dz))) ## dx dy dz ## 1 1 -2 * y 0 Plugging in the gradients into our Lagrangian Function: \\[\\begin{align*} \\nabla\\mathcal{L}(x,y,z) {}&amp;\\equiv \\nabla f(x,y,z) - ( \\lambda_1 \\nabla g_1(x,y,z) + \\lambda_2 \\nabla g_2(x,y,z)) = 0 \\\\ &amp;\\equiv \\left[\\begin{array}{r} 1 \\\\ -2y\\\\ 0 \\end{array}\\right] - \\lambda_1 \\left[\\begin{array}{r} 0 \\\\ 1 \\\\ -2z \\end{array} \\right] - \\lambda_2 \\left[\\begin{array}{r} -2x \\\\ 0 \\\\ 1 \\end{array} \\right] = \\left[\\begin{array}{rrr} 0 \\\\ 0 \\\\ 0 \\end{array}\\right],\\\\ \\end{align*}\\] we get the following: \\[ x = \\frac{-1}{2\\lambda_2}\\ \\ \\ \\ \\ \\ \\ \\ y = \\frac{-\\lambda_1}{2}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ z = \\frac{\\lambda_2}{2\\lambda_1} \\] We then formulate system of equations to solve for \\(\\lambda_1\\) and \\(\\lambda_2\\): \\[\\begin{align*} g_1(x,y,z) = y - z^2 {}&amp;= \\frac{-\\lambda_1}{2} - \\left(\\frac{\\lambda_2}{2\\lambda_1}\\right)^2 \\\\ g_2(x,y,z) = z - x^2 &amp;= \\frac{\\lambda_2}{2\\lambda_1} - \\left(\\frac{-1}{2\\lambda_2}\\right)^2 \\end{align*}\\] Using a scientific calculator, the result would be: \\[ \\lambda_1 = -0.6095068\\ \\ \\ \\ \\ \\ \\ \\ \\lambda_2 = -0.6729501 \\] From there, we can solve for \\(x, y, z\\): \\[\\begin{align*} x {}&amp;= \\frac{-1}{2\\lambda_2} = \\frac{-1}{2 \\times (-0.6729501)} = 0.7429971\\\\ y &amp;= \\frac{-\\lambda_1}{2} = \\frac{-(-0.6095068)}{2} = 0.3047534\\\\ z &amp;= \\frac{\\lambda_2}{2\\lambda_1} = \\frac{-0.6729501}{2\\times (-0.6095068)} = 0.5520448 \\end{align*}\\] Therefore, the maximum value of our function f(x,y,z) subject to the two constraints \\(g_1(x,y,z)=0\\) and \\(g_2(x,y,z) = 0\\) is 0.6501225 at the critical point (0.7429971, 0.3047534, 0.5520448). As complementary or prerequisite, we leave readers to investigate Fenchel Conjugate, Conjugate Duality, and Convex Analysis. 3.9.5 Karush-Khun-Tucker Conditions In the previous section, we gave a taste of optimization by illustrating the use of Lagrange multipliers with two constraints. In this section, we introduce Karush-Khun-Tucker (KKT) conditions. The idea is to optimize (e.g. minimize / maximize) a continuously differentiable objective function with KKT constraints. For example, we have an objective function, namely \\(f(x^*)\\), and two constraints, namely \\(h(x^*) = 0\\) and \\(g(x^*) \\le 0\\) where \\(x^*\\) is the optimal solution of the problem so that we have the following: \\[\\begin{align} min\\ f(x^*)\\ \\ \\ \\ \\ such\\ that\\ \\ \\ \\ h(x^*) = 0\\ \\ and\\ \\ \\ g(x^*) \\le 0 \\end{align}\\] Similar to our previous discussion around Lagrange Multipliers, we write the objective function and constraints into a single Lagrangian equation like so: \\[\\begin{align} L(x^*, \\lambda_1, \\lambda_2) = f(x^*) + {\\lambda_1}^T h(x^*) + {\\lambda_2}^T g(x^*) \\end{align}\\] where: \\[ x^*, \\lambda_1, \\lambda_2\\ \\ \\ \\text{are vectors and } \\lambda_1, \\lambda_2 \\text{ are dual variables} \\] additionally, we have the following sum of weighted functions: \\[\\begin{align} {\\lambda_1}^T h(x^*) {}&amp;= \\sum_{j=1}^m \\lambda_{1_j} h_j(x^*)\\\\ {\\lambda_2}^T g(x^*) &amp;= \\sum_{i=1}^n \\lambda_{2_i} g_i(x^*) \\end{align}\\] We then optimize by differentiation: \\[\\begin{align} \\nabla L(x^*, \\lambda_1, \\lambda_2) = \\nabla f(x^*) + {\\lambda_1}^T \\nabla h(x^*) + {\\lambda_2}^T \\nabla g(x^*) {}&amp;= 0 \\\\ \\nonumber \\\\ \\frac{\\partial L}{\\partial x^*} = \\frac{\\partial f}{\\partial x^*} + \\lambda_1\\frac{\\partial h}{\\partial x^*} + \\lambda_2\\frac{\\partial g}{\\partial x^*} &amp;= 0 \\end{align}\\] In the specific case above, the KKT conditions are: \\[ \\begin{array}{lrl} \\text{* Optimality (Stationarity) condition} &amp; \\nabla L(x_i^*, \\lambda1, \\lambda_2) = 0, &amp; i=1,...,n\\\\ \\text{* Primal Feasibility conditions} &amp; h_j(x^*) = 0, &amp; j=1,...,m \\\\ &amp; g_i(x^*) \\le 0, &amp; i=1,...,n \\\\ \\text{* Dual Feasibility condition} &amp; \\lambda_i \\ge 0, &amp; i=1,...,l \\\\ \\text{* Complementary Slackness condition} &amp; {\\lambda_{2_i}}^Tg_i(x^*) = 0, &amp; i=1,...,n \\\\ \\end{array} \\] We cover the case of SVM in applying KKT constraints in Chapter 9 (Computational Learning I), including topics around Primal and Dual formulation. 3.10 Summary In dealing with approximations by way of iteration, it helps to pay close attention to convergence speed. There are optimization techniques that can be used to improve convergence speed. We will explore these optimization solutions in the Machine Learning and Deep Learning chapters. We have shown the mechanics of approximation by showing different stationary iterative solutions for each type of problem, dealing specifically with systems of equations that do not depend on change or time. In the next chapter, we review methods used for dynamic memory-less systems. After which, we also review methods used for probabilistic memory-based systems. Systems that are considered dynamic are dependent on changes in time. Systems that are considered memory-based are dependent on state changes. Dynamic memory-less (stateless) systems are discussed in the Calculus chapter. Probabilistic memory-based (stateless) systems are discussed in the Bayesian Chapter. "],["numericalcalculus.html", "Chapter 4 Numerical Calculus 4.1 Introductory Calculus 4.2 Approximation by Numerical Integration 4.3 Approximation by Numerical Differentiation 4.4 Approximation using Ordinary Differential Equations 4.5 Approximation using Functional Differential Equations 4.6 Approximation using Partial Differential Equations 4.7 Approximation using Fourier Series And Transform 4.8 Summary", " Chapter 4 Numerical Calculus Following the natural order of things, we have shown how we went from Direct to Indirect Methods. Next, as a continuation of the Numerical Approximation, we introduce several fundamental numerical methods relevant to Integration and Differentiation. Additionally, we will cover solutions to problems in Dynamic Systems based on approximation in the latter part of this chapter. In this chapter, we focus on Numerical Analysis in the context of Calculus as we reference the great works of Atkinson K. E.(1989), Adams R. (1995), Heath M.T. (2002), Burden R.L. et al. (2005), Larson R. et al. (2006), Press W.H et al. (2007), and Strauss W. A. (2008) along with other additional references for consistency. For a summary, let us review Figure 4.1. Figure 4.1: ODE and PDE Let us first have a quick refresh of Calculus as a starting point; though, readers with a full grasp of Calculus can skip the next few sections. 4.1 Introductory Calculus One of the hallmark achievements of the 17th century by Sir Isaac Newton and Gottfried Leibnitz was the discovery of Calculus. This branch of mathematics deals with differentiation and integration which we briefly cover in this chapter. This section is only a review of Calculus. There are many textbooks that explain Calculus in detail. Our goal here is to recall standard rules in derivation and integration such as chain rules, power rules, product rules, trigonometric rules, etc. without too much of an explanation of how those rules are formed. The main emphasis in subsequent sections is to present the numerical methods available when specific integrations cannot be achieved by analytical means. Let us start by explaining about functions. 4.1.1 Function In its most general term, a function represents simple to complex expressions that take X number of input variables and produce Y number of output values. This definition becomes more apparent in computer programming, in which program functions can put complex logic into code based on what we intend the computer to accomplish. Mathematically, we can simply say that a function, in its simplest form, takes an input and produces an output. Here is an example: f(x) = x + 1 The example function f(x) accepts a value and stores it into a variable x. Then, the x+1 expression performs an addition. So if the x variable takes a value of 200, then the expression adds up to 201, and the f(x) function outputs the result as 201. If f(x) = y and ‘y’ is the output variable, then f(x) = y = x + 1; therefore f(200) = 200 + 1 = 201. Therefore, y = 201. Let us introduce three terms associated with functions. Domain is a set of input values defined for functions. The set of input values can also be termed arguments or argument values. In computer programming, one will usually encounter such terms, particularly when coding a function. Range is a set of output values for functions. Support is a subset of the domain that supports the function. We will use such terms when discussing improper integrals and statistical probabilities. Figure 4.2 shows a plot of two functions: A quadratic function and a linear function. Figure 4.2: Functions The convex curve is drawn by the following quadratic function: \\(f(x) = x^2 + 50\\). The slanted line is drawn by the following linear function: \\(f(x) = x + 50\\). Each of the two functions accepts a value of X as input and produces an output value - let us call it a value of Y. See Table 4.1. Table 4.1: X and Y Coordinates 1 2 3 4 5 6 7 8 9 10 X Input -15 -14 -13 -12 -11 -10 -9 -8 -7 -6 Quadratic Ouput 275 246 219 194 171 150 131 114 99 86 Linear Output 35 36 37 38 39 40 41 42 43 44 So, if we choose the first coordinates where \\(x = -15\\), then we have the following: \\[\\begin{align*} Quadratic\\ function &amp; = f(x) = x^2 + 50 = (-15)^2 + 50 = 275 \\\\ Linear\\ function &amp; = f(x) = x + 50 = (-15) + 50 = 35 \\end{align*}\\] Here, we provide (-15) as an input value of x, and the quadratic function f(x) produces (275) as an output value of y. Similarly, we provide (-15) as an input value of x, and the linear function f(x) produces (35) as an output value of y, given that \\(y = f(x)\\). 4.1.2 Slopes Slope is described as the rise over the run. It is expressed as such: \\[\\begin{align} slope = \\frac{rise}{run} = \\frac{y2 - y1}{x2 - x1} \\end{align}\\] We will use a line using a linear function to explain a slope as plotted in Figure 4.3. As the horizontal arrow (the run) increases, so does the vertical arrow (the rise). Mathematically, as illustrated in Figure 4.3, for every delta change in x - denoted by the symbol (\\(\\delta\\)), there is a corresponding delta change in y - denoted by the symbol (\\(\\Delta\\)). \\[\\exists x\\in \\mathbb{R} (\\delta x \\to \\Delta y)\\] Figure 4.3: Slope The two points correspond to the (x1, y1) and the (x2, y2) coordinates. They are represented as: \\[\\begin{align} x1 {} &amp; = x \\\\ x2 &amp; = x + \\delta \\\\ y1 &amp; = y = f(x) \\\\ y2 &amp; = y + \\Delta = f(x + \\delta) \\end{align}\\] The change from point (x1, y1) to point (x2, y2) is represented as: \\[\\begin{align*} \\Delta y {} &amp; = y2 -y1 = f(x + \\delta) - f(x) \\\\ \\delta x &amp; = x2 - x1 = (x + \\delta) - x = \\delta \\end{align*}\\] Expanding our slope, we get: \\[\\begin{align} slope = \\frac{rise}{run} = \\frac{y2 - y1}{x2 - x1} = \\frac{\\Delta y}{\\delta x} = \\frac{f(x + \\delta)-{f(x)}}{\\delta} \\end{align}\\] For now, let us replace \\(\\delta\\) with ‘h’: \\[\\delta = h\\] With that, here is the final equation for the slope: \\[\\begin{align} slope = \\frac{f(x + h)-{f(x)}}{h} \\end{align}\\] An essential concept about slopes is that it represents the average rate of the change of a line (Larson R. et al. 2006, 105–10). Just by the use of averages, the following terms are interchangeable: average rate of change of a line slope of a line Another essential fact is that every point in the curve does not have the same average rate; the slope changes at every point. That is explained in the next section. 4.1.3 Limits Noting that curves have curvatures - no linearity - one can put an imaginary line at a curvature point, and it balances itself perpendicular to the curve at that point. This line is called a tangent line - and is always perpendicular to the curve at a point. For example, in Figure 4.4, there are three tangent lines (red lines) that intersect with the curve at three different points (points a, b, and c). Figure 4.4: Slopes in a Curve If we have to walk along the curve, every point we step into has an imaginary tangent line with a slope that reflects the average range of change at that point in the curve. Therefore, unlike a straight (linear) line; in a curve, slopes may be different each time at a given point as we walk along the curve. As for limits, the goal here is to find the limit of a function. A limit is the output value of a curve function as a given input approaches a particular point, p. \\[ \\lim_{x\\to p}f(x) = output\\ value = L \\] Let us use Figure 4.5 to explain further. Figure 4.5: Limits As h approaches 0, the secant line gets closer and closer to the tangent line in the chart. It gets closer that the slope of the secant line is almost equal to the slope of the tangent line at a point. Using the slope formula, we can now formulate the limit of the function. The limit of a function - f(x) - is written as f’(x). \\[\\begin{align} f&#39;(x) =\\lim_{h\\to 0}\\frac{f(x + h) - f(x)}{h} = L \\end{align}\\] Example 1: Find the limit of \\(f(x) = x^2 + 1\\) as h approaches 0. \\[\\begin{align*}{lll} f&#39;(x) &amp;= \\lim_{h\\to 0}\\frac{f(x + h) - f(x)}{h} \\\\ &amp;= \\lim_{h\\to 0}\\frac{((x + h)^2 + 1 ) - (x^2+1)}{h} \\\\ {} &amp;= \\lim_{h\\to 0}\\frac{(x^2 + 2xh + h^2 + 1) - (x^2+1)}{h} \\\\ &amp;= \\lim_{h\\to 0}\\frac{x^2 + 2xh + h^2 + 1 - x^2 - 1}{h} \\\\ {} &amp;= \\lim_{h\\to 0}\\frac{2xh + h^2}{h} \\\\ &amp;= \\lim_{h\\to 0}\\frac{h(2x + h)}{h} \\\\ {} &amp;= \\lim_{h\\to 0} ( 2x + h ) \\\\ &amp;= \\lim_{h\\to 0} ( 2x + 0 )\\ ;\\ where\\ h \\to 0\\\\ {} &amp;= (2x + 0) &amp; {}\\\\ {} &amp;= 2x &amp; {} \\end{align*}\\] Example 2: Find the limit of \\(f(x) = x^3 + x^2 + 1\\) as h approaches 0. \\[\\begin{align*} f&#39;(x) &amp; = \\lim_{h\\to 0}\\frac{f(x + h) - f(x)}{h} \\\\ &amp; = \\lim_{h\\to 0}\\frac{((x + h)^3 + (x + h)^2 + 1 ) - (x^3 + x^2+1)}{h} \\\\ &amp; = \\lim_{h\\to 0}\\frac{(x + h)^3 + (x + h)^2 + 1 - x^3 - x^2 - 1}{h} \\\\ &amp; = \\lim_{h\\to 0}\\frac{(x + h)^3 + (x + h)^2 - x^3 - x^2}{h} \\\\ &amp; \\text{(use difference of cube and square)}\\\\ &amp; = \\lim_{h\\to 0}\\frac{((x + h)^3 - x^3) + ( (x + h)^2 - x^2) }{h}\\\\ &amp; = \\lim_{h\\to 0}\\frac{((x + h)^2 + (x + h)x + x^2)((x + h) - x) + ((x + h) + x)((x + h) - x) }{h} \\\\ &amp; \\text{(common factor)}\\\\ &amp; = \\lim_{h\\to 0}\\frac{(((x + h)^2 + (x + h)x + x^2) + ((x + h) + x))((x + h) - x) }{h}\\\\ \\end{align*}\\] \\[\\begin{align*} &amp; = \\lim_{h\\to 0}\\frac{((x + h)^2 + (x + h)x + x^2 + 2x + h)(h)}{h} \\\\ &amp; = \\lim_{h\\to 0}((x + h)^2 + (x + h)x + x^2 + 2x + h) \\\\ &amp; = \\lim_{h\\to 0}((x^2 + xh + h^2) + x^2 + xh + x^2 + 2x + h) \\\\ &amp; = \\lim_{h\\to 0}(x^2 + xh + h^2 + x^2 + xh + x^2 + 2x + h) \\\\ &amp; = \\lim_{h\\to 0}(3x^2 + 2x + xh + h^2 + xh + h) \\\\ &amp; = \\lim_{h\\to 0} ( 3x^2 + 2x + x0 + 0^2 + x0 + 0)\\ ;\\ where\\ h \\to 0\\\\ &amp; = (3x^2 + 2x + 0) \\\\ &amp; = 3x^2 + 2x \\end{align*}\\] Notice that we have to go through a long computation for the limit of \\[f(x) = x^3 + x^2 + 1\\] It can get complex as we deal with a higher order of polynomials. Example: Find the limit of \\(f(x) = x^6 + x^5 + x^4 + x^3 + x^2 + 1\\) as h approaches 0. \\[\\begin{align*} f&#39;(x) {} &amp; = \\lim_{h\\to 0}\\frac{f(x + h) - f(x)}{h} \\\\ &amp; = \\lim_{h\\to 0}\\frac{(x + h)^6 + (x + h)^5 + ((x + h)^4 + (x + h)^3 + (x + h)^2 + 1 ) - (x^4 + x^3 + x^2+1)}{h} \\\\ &amp; \\vdots \\\\ &amp; = 6x^5 + 5x^4 + 4x^3 + 3x^2 + 2x \\end{align*}\\] The answer to the following expression \\(f(x) = x^6 + x^5 + x^4 + x^3 + x^2 + 1\\) for the limit is: \\[6x^5 + 5x^4 + 4x^3 + 3x^2 + 2x\\] How did we arrive at the answer? Foremost, there is a short-cut method governed by rules, which is by way of derivatives. 4.1.4 Derivatives Let us define Derivatives in terms of limits the same way we define limits in the previous discussion. Looking for the derivative of a given function is the same as looking for the limit of a function. That is similar to finding the slope of a tangent line at a point. A slope of a tangent line at a point is defined as the change of y over the change of x. In other words, we are computing the average rate of change. \\[\\begin{align} \\text{average rate of change} = \\frac{\\text{change of a condition (y)}}{\\text{change of another condition (x)}} \\end{align}\\] In general terms, the average rate of change can be applied to any scenario that has properties of change. Derivative notation The derivative of a function - f(x) - is written as: \\[\\begin{align} \\frac{dy}{dx} = f&#39;(x) = \\lim_{x\\to0} \\frac{f(x + h) - f(x)}{h} \\end{align}\\] where f’(x) is a Newton notation, and where \\(\\frac{dy}{dx}\\) is a Leibnitz notation and is read as the derivative of y with respect to x. \\[\\begin{align} Since {}&amp; : y = f(x) \\nonumber \\\\ \\nonumber \\\\ \\text{It follows that} &amp; : \\frac{dy}{dx} = \\frac{d f(x)}{dx} = \\frac{df}{dx} (x) = \\frac{d}{dx} f(x) \\end{align}\\] Now, instead of following the way we compute for limits, we will use a different method called differentiation using a set of rules. Let us now differentiate (or find the derivative of a function). Constant Rule \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx}(au) = a\\frac{d}{dx}(u),\\ \\ \\ where\\ a\\ \\to constant \\end{align}\\] Example: Find the derivative of f(x) = 25x. \\[ \\frac{dy}{dx} = \\frac{d}{dx}(25x) = 25\\frac{d}{dx}{x} = 25 \\] Find the derivative of f(x) = 25. \\[ \\frac{dy}{dx} = \\frac{d}{dx}(25) = 0 \\] Power Rule \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx}(x^n) = nx^{n-1} \\end{align}\\] Example: Find the derivative of f(x) = \\(2x^3\\cdot3x^2 + 10x^4 + 2x^3 + x^2 + x + 1\\). \\[\\begin{align*} \\frac{d}{dx}f(x) {} &amp; = \\frac{d}{dx}( 2x^3\\cdot 3x^2 + 10x^4 + 2x^3 + x^2 + x + 1 )\\\\ &amp; = \\frac{d}{dx}( 6x^5 + 10x^4 + 2x^3 + x^2 + x + 1 )\\\\ &amp; = (5)6x^{5-1} + (4)10x^{4-1} + (3)2x^{3-1} + (2)x^{2-1} + (1)x^{1-1} \\\\ &amp; = 30x^4 + 40x^3 + 6x^2 + 2x \\\\ &amp; = 2x( 15x^3 + 20x^2 + 3x + 1) \\end{align*}\\] Sum and Difference Rule \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx}(f\\pm g) = \\frac{d}{dx} f + \\frac{d}{dx} g = (f\\pm g)&#39; = f&#39;\\pm g&#39; \\end{align}\\] Example: \\[\\begin{align*} \\frac{dy}{dx} = \\frac{d}{dx}(4x^3 + x^2) {}&amp; = \\frac{d}{dx} (4x^3) + \\frac{d}{dx} (x^2)\\\\ &amp; = (3)4x^{3-1} + (2)x^{2-1}\\\\ &amp; = 12x^2 + 2x^1\\\\ &amp;=2x(6x+1) \\end{align*}\\] Trigonometric Rule Table 4.2: Trigonometric Derivatives Derivative Functions Derivatives \\(\\frac{d}{dx}sin(x)\\) \\(cos(x)\\) \\(\\frac{d}{dx}cos(x)\\) \\(\\ -sin(x)\\) \\(\\frac{d}{dx}tan(x)\\) \\(\\ sec^2(x)\\) \\(\\frac{d}{dx}cot(x)\\) \\(\\ -csc^2(x)\\) \\(\\frac{d}{dx}sec(x)\\) \\(\\ sec(x)tan(x)\\) \\(\\frac{d}{dx}csc(x)\\) \\(\\ -csc(x)cot(x)\\) \\(\\frac{d}{dx}sin^{-1}x\\ or\\ arcsin(x)\\) \\(\\ \\frac{1}{\\sqrt{1 - x^2}}\\) \\(\\frac{d}{dx}cos^{-1}x\\ or\\ arccos(x)\\) \\(\\ -\\frac{1}{\\sqrt{1 - x^2}}\\) \\(\\frac{d}{dx}tan^{-1}x\\ or\\ arctan(x)\\) \\(\\ \\frac{1}{x^2 + 1}\\) \\(\\frac{d}{dx}cot^{-1}x\\ or\\ arccot(x)\\) \\(\\ -\\frac{1}{x^2 + 1}\\) \\(\\frac{d}{dx}sec^{-1}x\\ or\\ arcsec(x)\\) \\(\\ \\frac{1}{\\mid x \\mid \\sqrt{x^2 + 1}}\\) \\(\\frac{d}{dx}csc^{-1}x\\ or\\ arcscs(x)\\) \\(\\ -\\frac{1}{\\mid x \\mid \\sqrt{x^2 + 1}}\\) Example: Find the derivative of f(x) = \\(2x^3\\cdot3x^2 + cos(x)\\). \\[\\begin{align*} f&#39;(x) {}&amp; = \\frac{d}{dx}(2x^3\\cdot3x^2 + cos(x) )\\\\ &amp; = \\frac{d}{dx}(6x^5 + cos(x))\\\\ &amp; = (5)6x^{5-1} + (-sin(x))\\\\ &amp; = 30x^4 - sin(x) \\end{align*}\\] Logarithmic Rule Table 4.3: Logarithmic Derivatives Derivative Functions Derivatives \\(\\frac{d}{dx}(log_{a}\\ x) = \\frac{d}{dx}(\\frac{ln\\ x}{ln\\ a})\\) \\(\\frac{1}{x\\ ln\\ a}\\) \\(\\frac{d}{dx}(log_{a}\\ x) =\\frac{1}{ln\\ a}\\frac{d}{dx}(ln\\ x)\\) \\(\\frac{1}{x\\ ln\\ a}\\) \\(\\frac{d}{dx}(ln\\ x)\\) \\(\\frac{1}{x}\\ or\\ x^{-1}\\) \\(\\frac{d}{dx}(ln\\ \\mid x \\mid)\\) \\(\\frac{1}{x}\\ or\\ x^{-1}\\) \\(\\frac{d}{dx}(e^n)\\) \\(e^n\\) \\(\\frac{d}{dx}(e^2)\\) \\(e^2\\) \\(\\frac{d}{dx}(x^n)\\) \\(x^nln(x)\\) \\(\\frac{d}{dx}(log_{a})x\\) \\(\\frac{1}{x\\cdot ln(a)}\\) Example: Find the derivative of f(x) = \\(2x^3\\cdot3x^2 + ln(x)\\). \\[\\begin{align*} f&#39;(x) {}&amp; = \\frac{d}{dx}(2x^3\\cdot3x^2 + ln(x))\\\\ &amp; = \\frac{d}{dx}(6x^5 + ln(x))\\\\ &amp; = (5)6x^{5-1} + \\frac{1}{x}\\\\ &amp; = 30x^4 + \\frac{1}{x} \\end{align*}\\] Product Rule \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx}( u \\cdot v ) = \\frac{du}{dx} \\cdot v + u \\cdot \\frac{dv}{dx} = (f\\cdot g)&#39;(x) = f&#39;g + g&#39;f \\end{align}\\] Example 1: Find the derivative of f(x) = \\(5x\\cdot 2x\\). \\[\\begin{align*} (f\\cdot g)&#39;(x) {} &amp; = \\frac{d}{dx}(5x\\cdot 2x) \\\\ &amp; = \\frac{d}{dx}(5x)\\cdot2x\\ +\\ \\frac{d}{dx}(2x)\\cdot 5x \\\\ &amp; = 5\\frac{d}{dx}(x)\\cdot2x\\ +\\ 2\\frac{d}{dx}(x)\\cdot 5x \\\\ &amp; = 5\\cdot(x^{1-1})\\cdot2x\\ +\\ 2\\cdot(x^{1-1})\\cdot 5x \\\\ &amp; = 5\\cdot(1)\\cdot2x\\ +\\ 2\\cdot(1)\\cdot 5x \\\\ &amp; = 10x + 10x\\\\ &amp; = 20x \\end{align*}\\] Example 2: Find the derivative of f(x) = \\(x^3\\cdot cos(x)\\). \\[\\begin{align*} (f\\cdot g)&#39;(x) {} &amp; = \\frac{d}{dx}(x^3\\cdot cos(x)) \\\\ &amp; = (3)x^{3-1}\\cdot cos(x) + -sin(x)\\cdot x^3\\\\ &amp; = 3x^2cos(x) - x^3sin(x) \\\\ &amp; = x^2( 3cos(x) - xsin(x)) \\end{align*}\\] Quotient Rule \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx}( \\frac{u}{v} ) = (\\frac{f}{g})&#39;(x) = \\frac{f&#39;g - g&#39;f}{g^2} \\end{align}\\] Example: Find the derivative of f(x) = \\(\\frac{3x^2+1}{2x+1}\\). \\[\\begin{align*} (\\frac{f}{g})&#39;(x) {} &amp; = \\frac{d}{dx}\\left( \\frac{3x^2+1}{2x+1} \\right)\\\\ &amp; = \\frac{(2)3x^{2-1} (2x+1) - (1)2x^{1-1}(3x^2+1)}{(2x+1)^2}\\\\ &amp; = \\frac{6x (2x+1) - 2(3x^2+1)}{(2x+1)^2}\\\\ &amp; = \\frac{12x^2+6x - 6x^2 - 2}{(2x+1)^2}\\\\ &amp; = \\frac{6x^2 + 6x -2}{(2x+1)^2} \\end{align*}\\] Chain Rule (General Power Rule) \\[\\begin{align} \\frac{dy}{dx} = \\frac{dy}{du}\\frac{du}{dx} = (f\\circ g(x))&#39; = (f(g(x))&#39; = f&#39;(g(x))g&#39;(x) \\end{align}\\] \\[\\begin{align} (f(g(x))&#39; = \\frac{d}{dx}[\\ (f(x))^n\\ ] = n\\cdot (f(x))^{n-1}\\cdot\\frac{d}{dx}(f(x)) \\end{align}\\] Example: Find the derivative of f(x) = \\((2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)^7\\) . \\[\\begin{align*} Let\\ u {}&amp; = 2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1\\\\ \\\\ (f(g(x))&#39; &amp; = \\frac{d}{dx} (u^7) \\\\ &amp; = (7)u^{7-1}\\cdot\\frac{d}{dx} u\\\\ &amp; = 7u^6 \\cdot \\frac{d}{dx} u \\\\ &amp; = 7(2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\cdot \\frac{d}{dx}(2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\\\ &amp; = 7(2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\cdot \\frac{d}{dx}(6x^5 + 4x^4 + x^3 + x^2 + 1)\\\\ &amp; = 7(2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\cdot ((5)6x^{5-1} + (4)4x^{4-1} + (3)x^{3-1} + (2)x^{2-1} )\\\\ &amp; = 7(2x^3\\cdot3x^2 + 4x^4 + x^3 + x^2 + 1)\\cdot (30x^4 + 16x^3 + 3x^2 + 2x )\\\\ \\end{align*}\\] Order Rule What is the derivative of a derivative of a derivative of a function? \\[\\begin{align} first\\ derivative {}&amp; : f&#39;(x) = \\frac{dy}{dx}\\\\ second\\ derivative &amp; : f&#39;&#39;(x) = (f&#39;(x))&#39; = \\frac{d}{dx} \\left(\\frac{dy}{dx}\\right) = \\frac{d^2y}{dx^2} \\\\ third\\ derivative &amp; : f&#39;&#39;&#39;(x) = (f&#39;&#39;(x))&#39; = ((f&#39;(x))&#39;)&#39; = \\frac{d}{dx} \\left(\\frac{d^2y}{dx^2}\\right) = \\frac{d^3y}{dx^3} \\end{align}\\] Example: Find the first derivative, second derivative, third derivative, and fourth derivative of f(x) = \\(x^4 + x^3 + x^2 + 1\\). \\[\\begin{align*} f(x) &amp; = x^4 + x^3 + x^2 + 1 \\\\ (speed) f&#39;(x) &amp; = 4x^3 + 3x^2 + 2x\\\\ (acceleration) f&#39;&#39;(x) &amp; = 12x^2 + 6x + 2\\\\ (torque/force) f&#39;&#39;&#39;(x) &amp; = 24x + 6 \\\\ (horsepower/power) f&#39;&#39;&#39;&#39;(x) &amp; = 24 \\\\ \\end{align*}\\] Multivariate Functions (multivariables) In univariate (one-variable) calculus, we use the following expression: \\[ y = f(x) \\] Here is an example of a univariat function: \\[ y = f(x) = 4x^2 - 1 = (2x+1)(2x - 1) \\] In multivariate (multivariable) calculus, we use the following expression: \\[\\begin{align} z = f(x,y) \\end{align}\\] Here is an example of a multivariate function: \\[ z = f(x,y) = 4x^2 - y^2 = (2x+y)(2x - y) \\] So how do we differentiate a multivariate function? The answer is to use partial derivatives. Partial Derivatives Partial derivative is finding the derivative of functions with respect to individual variables. Example: Find the derivative of \\(f(x,y) = (4x^2-y^2)\\) with respect to x, where z = f(x,y). \\[\\begin{align*} \\frac{\\partial z}{\\partial x} {}&amp; = \\frac{\\partial}{\\partial x} (4x^2-y^2)\\\\ &amp; = \\frac{\\partial}{\\partial x} (4x^2) - \\frac{\\partial}{\\partial x}(y^2)\\\\ &amp; = (2)4x^{2-1} - y^2 \\frac{\\partial}{\\partial x}(1),\\ \\ \\ \\ \\text{where y is a constant and } \\frac{\\partial}{\\partial x}(1) = 0\\\\ &amp; = (2)4x^1 - y^2\\cdot 0 \\\\ &amp; = 8x \\end{align*}\\] Now find the derivative of \\(f(x,y) = (4x^2-y^2)\\) with respect to y, where z = f(x,y). \\[\\begin{align*} \\frac{\\partial z}{\\partial y} {}&amp; = \\frac{\\partial}{\\partial y} (4x^2-y^2)\\\\ &amp; = \\frac{\\partial}{\\partial y} (4x^2) - \\frac{\\partial}{\\partial y}(y^2)\\\\ &amp; = x^2 \\frac{\\partial}{\\partial y}(4)- (2)y^{2-1} \\,\\ \\ \\ \\ \\text{where x is a constant and } \\frac{\\partial}{\\partial x}(4) = 0\\\\ &amp; = x^2\\cdot 0 - 2y^1 \\\\ &amp; = -2y \\end{align*}\\] To validate, we use R’s derivative function called D(): f = expression( 4 * x^2 - y^2) ( x = D( f, &quot;x&quot;) ) ## 4 * (2 * x) ( y = D( f, &quot;y&quot;) ) ## -(2 * y) Gradient of a function Gradient is another word for derivatives (or slope) so that given the same function as above, namely \\(f(x,y) = (4x^2-y^2)\\), the gradient is written as: \\[ \\nabla f(x,y) = \\left(\\frac{\\partial z}{\\partial x}, \\frac{\\partial z}{\\partial y}\\right) = \\left(8x, -2y\\right) \\] Example: \\[ \\nabla f(2,3) = \\left(8x, -2y\\right) = \\left(8(2), -2(3)\\right) = \\left(16, -6\\right) \\] 4.1.5 Integrals Integration is the reverse method of Differentiation - the antiderivative of a function (or a derivative if in higher orders). Integral Notations Note that the antiderivative of a derivative is written as F. \\[\\begin{align*} if{}&amp;: f&#39;(x)\\ \\text{is the derivative}\\\\ then&amp;: F(f&#39;(x))\\ \\text{ is the antiderivative} \\\\ therefore&amp;: F(f&#39;(x)) = f(x) \\end{align*}\\] Similar to Differentiation, we also follow rules when dealing with Integration. Let us integrate (or find the integral of a derivative). Constant Rule \\[\\begin{align} \\int (k) dx = (k)x + c, \\text{where both c and k are constants} \\end{align}\\] Example: Find the antiderivative of f’(x) = 5. \\[\\begin{align*} \\int (5) dx {}&amp; = (5)x + c\\\\ &amp; = 5x + c \\end{align*}\\] Trigonometric Rule Table 4.4: Trigonometric Integrals Integral Functions Integrals \\(\\int sin(x) dx\\) \\(-cos(x)\\ +\\ c\\) \\(\\int cos(x) dx\\) \\(sin(x)\\ +\\ c\\) \\(\\int sec^2(x) dx\\) \\(\\ tan2(x)\\ +\\ c\\) \\(\\int csc^2(x) dx\\) \\(\\ -cot^2(x)\\ +\\ c\\) \\(\\int sec(x)tan(x) dx\\) \\(\\ sec(x)\\ +\\ c\\) \\(\\int csc(x)cot(x) dx\\) \\(\\ -csc(x) + \\ c\\) \\(\\int \\frac{1}{\\sqrt{1 - x^2}} dx\\) \\(sin^{-1}x\\) or arcsin(x) + c \\(\\int -\\frac{1}{\\sqrt{1 - x^2}} dx\\) \\(cos^{-1}x\\) or arccos(x) + c \\(\\int \\frac{1}{x^2 + 1}dx\\) \\(tan^{-1}x dx\\) or arctan(x) + c \\(\\int -\\frac{1}{x^2 + 1}dx\\) \\(cot^{-1}x dx\\) or arccot(x) + c \\(\\int \\frac{1}{\\mid x \\mid \\sqrt{x^2 + 1}}dx\\) \\(sec^{-1}x\\) or arcsec(x) + c \\(\\int -\\frac{1}{\\mid x \\mid \\sqrt{x^2 + 1}}dx\\) \\(csc^{-1}x\\) or arcscs(x) + c Example: Find the antiderivative of f’(x) = \\(2x^3\\cdot3x^2 + cos(x)\\). \\[ \\int (2x^3\\cdot3x^2 + cos(x)) dx = x^6 + sin(x) + c \\] where: \\[ \\int (2x^3\\cdot3x^2 ) dx = x^6\\ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\ \\int cos(x) dx = sin(x) \\] Logarithmic Rule Table 4.5: Logarithmic Derivatives Functions Integrals \\(\\int \\frac{1}{x\\ ln\\ a}dx\\) \\((log_{a}\\ x)\\ +\\ c = (\\frac{ln\\ x}{ln\\ a})\\ +\\ c\\) \\(\\int \\frac{1}{x\\ ln\\ a}dx\\) \\((log_{a}\\ x)\\ +\\ c =\\frac{1}{ln\\ a}(ln\\ x)\\ +\\ c\\) \\(\\int (\\frac{1}{x}\\ or\\ x^{-1})dx\\) \\((ln\\ x)\\ +\\ c\\) \\(\\int (\\frac{1}{x}\\ or\\ x^{-1})dx\\) \\((ln\\ \\mid x \\mid)\\ +\\ c\\) \\(\\int e^n dx\\) \\((e^n)\\ +\\ c\\) \\(\\int e^2 dx\\) \\((e^2)\\ +\\ c\\) \\(\\int x^nln(x) dx\\) \\((x^n)\\ +\\ c\\) \\(\\int \\frac{1}{x\\cdot ln(a)} dx\\) \\((log_{a})x\\ +\\ c\\) Power Rule \\[\\begin{align} \\int (k)x^n dx = (k)\\frac{x^{n+1}}{n+1} + c, \\text{where n }\\ne 1 \\end{align}\\] Example: Find the antiderivative of \\(f&#39;(x) = 20x^3\\) \\[\\begin{align*} \\int (20x^3) dx {}&amp; = (20)\\frac{x^{3+1}}{3+1} + c \\\\ &amp; = (20) \\frac{x^4}{4} + c\\\\ &amp; = \\frac {20x^4}{4} + c\\\\ &amp; = 5x^4 + c \\end{align*}\\] Sum and Difference Rule \\[\\begin{align} \\int (f\\pm g)\\ dx = \\int (f)dx\\ \\pm \\int (g)dx \\end{align}\\] Example: Find the antiderivative of \\(f&#39;(x) = 12x^2 + 2x\\). \\[\\begin{align*} \\int (12x^2 + 2x) dx {}&amp; = \\int (12x^2)dx\\ + \\int (2x)dx \\\\ &amp; = 12\\frac{x^{2+1}}{2+1} + 2\\frac{x^{1+1}}{1+1} + c\\\\ &amp; = 12\\frac{x^3}{3} + 2\\frac{x^2}{2} + c\\\\ &amp; = \\frac{12x^3}{3} + \\frac{2x^2}{2} + c\\\\ &amp; = 4x^3 + x^2 + c\\\\ \\end{align*}\\] Product Rule Unlike Differentiation, there is no product rule for Integration. But we can derive a rule out of it from Differentiation. Let us start with the following two functions that we intend to perform product calculation. Given two functions: \\(u\\cdot v\\) \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx} (u\\cdot v) \\end{align}\\] By product rule, \\[\\begin{align} \\frac{dy}{dx} = \\frac{d}{dx} u\\cdot v = \\frac{du}{dx}\\cdot v + u\\cdot \\frac{dv}{dx} \\end{align}\\] We know that, from our Differentiation discussion, we have the following derivative formula. \\[\\begin{align} (f\\cdot g)&#39;(x) = (u\\cdot v)&#39;(x) = \\frac{du}{dx}\\cdot v + u\\cdot \\frac{dv}{dx} \\end{align}\\] We can get the antiderivative therefore: \\[ F((f\\cdot g)&#39;(x)) = (u\\cdot v)(x) = \\int \\left( \\frac{du}{dx}\\cdot v + u\\cdot \\frac{dv}{dx} \\right) dx = uv \\] By sum rule, we get: \\[\\begin{align} uv {}&amp; = \\int \\left( \\frac{du}{dx}\\cdot v \\right)dx + \\int \\left(u\\cdot \\frac{dv}{dx} \\right) dx \\\\ uv &amp; = \\int du\\cdot v + \\int u\\cdot dv \\end{align}\\] Based on the formula above, let us talk about integration by parts. Integration by parts \\[\\begin{align} \\int u\\cdot dv = uv -\\int v\\cdot du \\end{align}\\] Example 1: Find the antiderivative of \\(f&#39;(x) = xe^x dx\\). \\[\\begin{align*} let:\\\\ u {}&amp; = x\\\\ dv &amp; = e^x\\\\ therefore:\\\\ du &amp; = dx\\\\ v &amp; = e^x\\\\ \\\\ now:\\\\ \\int (xe^x)dx &amp; = \\int u\\ dv\\\\ &amp; = uv -\\int v\\ du\\\\ &amp; = x\\cdot e^x - \\int e^x dx\\\\ &amp; = x\\cdot e^x - e^x + c \\end{align*}\\] Example 2: Find the antiderivative of \\(f&#39;(x) = x\\cdot sin(x)\\). \\[\\begin{align*} let:\\\\ \\ u {}&amp; = x\\\\ dv &amp; = sin(x)dx\\\\ therefore:\\\\ du &amp; = dx\\\\ v &amp; = -cos(x) \\\\ now:\\\\ \\int (x\\cdot sin(x)) dx &amp; = \\int u\\ dv\\\\ &amp; = uv\\ -\\int v\\ du\\\\ &amp; = -x\\cdot cos(x) -\\int -cos(x)dx\\\\ &amp; = -x\\cdot cos(x) -(-sin(x) ) + c\\\\ &amp; = -x\\cdot cos(x) + sin(x) + c \\end{align*}\\] Chain Rule Unlike Differentiation, there is no chain rule for Integration. What can be available to use is Integration by Substitution. Integration by Substitution This is also known as reverse chain rule. \\[\\begin{align} \\int f(g(x))g&#39;(x)dx \\end{align}\\] For this to work, we perform substitution using ‘u’: \\[\\begin{align} u = g(x)\\\\ du = g(x)dx \\end{align}\\] That gives us: \\[\\begin{align} \\int f(g(x))g&#39;(x)dx = \\int f(u)du \\end{align}\\] Example: Find the antiderivative of \\(f&#39;(x) = e^{3x}\\) \\[\\begin{align*} let:\\\\ \\ u {}&amp; = 3x\\\\ du &amp; = 3dx\\\\ where:\\\\ dx &amp; = \\frac{1}{3}du \\end{align*}\\] \\[\\begin{align*} therefore:\\\\ &amp; = \\int (e^{3x}) dx\\\\ &amp; = \\int (e^u)\\frac{1}{3} du\\\\ &amp; = \\frac{1}{3}\\int {e^u} du\\\\ &amp; = \\frac{1}{3}\\cdot e^u + c\\\\ \\text{now substitute back u:}\\\\ &amp; = \\frac{1}{3}\\cdot e^{3x} + c \\end{align*}\\] Definite Integrals Let us study Figure 4.6. In the figure, we construct a chart using the R code below. Figure 4.6: Limits Using Figure 4.6, one of the strengths of Integration is computing for the area bounded by the curve and the line. Areas that have curvatures around the edges are candidates for integral computation. We highlight the area in the figure by using many small lines of width 1. We use lines to explain the regions filling up the whole area. The gaps between the lines are the regions of the whole area, and those regions are divided by the lines. As we reduce the width of the region, more lines are added to the area. It means there are more regions (though thinner) to fill the whole area. Imagine now, if we try to reduce the width of the region to an extremely small value, at that point, the total size of the area spanning all the regions will equal the size of the whole area bounded by the curve and the line. Mathematically, we know that \\(\\Delta x\\) is the size (base) of each region. We also know that each region will have a height that touches the curve: \\[\\begin{align} \\Delta y = f(\\Delta x) =f(x + \\Delta x) - f(x) \\end{align}\\] Therefore, the area of each individual rectangular region is: \\[\\begin{align} A_{R} {}&amp; = \\Delta x \\cdot \\Delta y\\\\ where &amp;: \\nonumber \\\\ \\Delta y &amp; = [ f(x + \\Delta x) - f(x) ] \\\\ \\Delta y &amp; = f(\\Delta x)\\\\ therefore &amp;: \\nonumber\\\\ A_{R} {}&amp; = \\Delta x \\cdot [ f(x + \\Delta x) - f(x) ] \\\\ &amp; = \\Delta x \\cdot f(\\Delta x) \\end{align}\\] Therefore, the whole area bounded by the curve and line where they intersect at points a and b is: \\[\\begin{align} A_{ab} = \\sum_{i=1}^{n} A_{R} = \\sum_{i=1}^{n} f(\\Delta x) \\cdot \\Delta x \\end{align}\\] For this to work, we need to make sure we set a limit to the number of regions created. The number of regions is limited to the whole area itself, bounded by points a and b. \\[\\begin{align} A_{ab} {}&amp; = \\lim_{n\\to \\infty} \\sum_{i=1}^{n} A_{R} &amp;on [a,b]\\\\ &amp; = \\lim_{n\\to \\infty} \\sum_{i=1}^{n} f(\\Delta x) \\cdot \\Delta x &amp;on\\ [a,b] \\end{align}\\] To convert this to integral, we write: \\[\\begin{align} A_{ab} {}&amp; = \\lim_{n\\to \\infty} \\int_{a}^{b} f(\\Delta x)\\ dx &amp;where\\ \\Delta x = dx \\end{align}\\] That is what we call definite integral: \\[\\begin{align} \\int_{a}^{b} f(x)\\ dx \\end{align}\\] We will cover this more in statistics and probabilities as we deal with data distribution. Improper Integrals So far we have been dealing with Indefinite Integral. One with no limits. On the other hand, Definite Integral is one with limits. We express the equation for Definite Integrals this way: \\[\\begin{align} \\int_{a}^{b}f(x) dx \\end{align}\\] Here, we define an integral with an interval between a and b - in other words, having a support or domain [a,b]. Figure 4.7: Definite Integral To evaluate the definite integral, we use the following equation: \\[\\begin{align} \\int_{a}^{b}f(x) dx = \\left[ f(x)\\ \\right]_{a}^{b} = f(b) - g(a) \\end{align}\\] Example: Find the integral of f(x) = x^2 +1 with domain [0,1]. \\[\\begin{align*} \\int (x^2 +1)dx {}&amp; = \\int x^2\\ dx + \\int (1)dx\\\\ &amp;= \\left[ \\frac{1}{3} x^3 + x \\right]_{0}^{1} \\\\ &amp; = \\left( \\frac{1}{3} (1)^3 + (1) \\right) - \\left( \\frac{1}{3} (0)^3 + (0) \\right) \\\\ &amp; = \\left( \\frac{1}{3} + 1 \\right) - 0 \\\\ &amp; = \\frac{1}{3} + 1 \\\\ &amp; = \\frac{4}{3} \\end{align*}\\] For improper integrals, we have to account for infinite limits of integration. What that is, has to do with the existence of limits. Let us review each one. First, we have a closure in which left and right brackets enclose the support: \\([a,b]\\). The domain is continuous, and both a and b are existing limits. Second, we have a closure in which a left parenthesis and a right bracket enclose the support: \\((-\\infty,b]\\). The domain is continuous from b but reaches no limit towards the negative direction. Third, we have a closure in which a left bracket and a right parenthesis enclose the support: \\([b,\\infty)\\). The domain is continuous and b but reaches no limit towards the positive direction. Fourth, we have a closure in which left and right parentheses enclose the support: \\((-\\infty,\\infty)\\). The domain is continuous but reaches no limits towards either end. Here, for improper integrals, we deal with enclosures with no limits (the ones with \\(\\infty\\)) \\[\\begin{align} \\int_{-\\infty}^{\\infty}f(x) dx \\end{align}\\] To solve the equation, we can split the integral into two: \\[\\begin{align} \\int_{-\\infty}^{\\infty}f(x) dx = \\int_{-\\infty}^{b}f(x) dx + \\int_{b}^{\\infty}f(x) dx \\end{align}\\] Notice how we introduce point b to cut the interval into two. The first part has an interval between \\(-\\infty\\) and b, and the second part has an interval between b and \\(\\infty\\). We call this method improper integral. So, let us find the improper integral of the following equation: \\[\\begin{align} \\int_{-\\infty}^{\\infty} x^2 dx \\end{align}\\] It can be assumed that between \\(-\\infty\\) and \\(\\infty\\) is a zero which we will use to perform improper integrals. \\[\\begin{align} \\int_{-\\infty}^{\\infty} x^2 dx = \\int_{-\\infty}^{0} x^2 dx + \\int_{0}^{\\infty} x^2 dx \\end{align}\\] Let us apply limits of definite integrals to the equation: \\[\\begin{align} \\int_{-\\infty}^{\\infty} x^2 dx {} &amp;= \\lim_{a\\to-\\infty} \\int_{-\\infty}^{0} x^2 dx + \\lim_{a\\to \\infty} \\int_{0}^{\\infty} x^2 dx\\\\ &amp;= \\lim_{a\\to-\\infty} |\\frac{1}{3} x^3 |_{a}^{0} + \\lim_{a\\to-\\infty} |\\frac{1}{3} x^3 |_{0}^{a} \\\\ &amp;= \\left( \\frac{1}{3}(0)^3 - \\frac{1}{3}(\\infty)^3 \\right) + \\left( \\frac{1}{3}(\\infty)^3 - \\frac{1}{3}(0)^3 \\right) \\nonumber \\\\ &amp;= \\left( 0 - \\infty \\right) + ( \\infty - 0) \\nonumber \\\\ &amp;= -\\infty + \\infty \\nonumber \\\\ &amp;= (-1+1)\\cdot\\infty \\to \\text{factor out common term} \\nonumber \\\\ &amp; = 0\\cdot\\infty \\to \\text{indeterminate outcome} \\nonumber \\end{align}\\] The L’Hôpital’s Rule It helps to recall that the following forms are indeterminate: \\[ \\frac{\\pm\\infty}{\\pm\\infty},\\ \\ \\ \\frac{0}{0},\\ \\ \\ \\pm\\infty \\cdot\\ 0,\\ \\ \\ \\infty - \\infty,\\ \\ \\ \\infty^0,\\ \\ \\ 1^\\infty,\\ \\ \\ 0^0 \\] It also helps to recall that the following forms are considered to have finite non-zero limit: \\[ \\frac{\\mathbb{R}}{\\pm0},\\ \\ \\ \\ \\pm\\infty^1\\ or \\pm\\infty \\] If the outcome of a limit ends with an indeterminate form, then we can apply the \\(\\mathbf{L&#39;H\\hat{o}pital&#39;s}\\) rule; which states that we can differentiate the numerator and denominator separately like so: \\[\\begin{align} \\lim_{a\\to p}\\frac{f(x)}{g(x)} = \\lim_{a\\to p}\\frac{f&#39;(x)}{g&#39;(x)} \\end{align}\\] Example: Find the limit of \\(f(x) = \\frac{ln\\ x}{x}\\) as x approaches \\(\\infty\\). \\[ f&#39;(x) = \\lim_{x\\to\\infty}\\frac{ln\\ x}{x} \\] Test for indiscriminate form: \\[\\begin{align*} ln\\ x {}&amp; = ln\\ \\infty {}&amp; \\to \\infty \\\\ x &amp; = \\infty &amp; \\to \\infty \\end{align*}\\] Therefore: \\[ f&#39;(x) = \\lim_{x\\to\\infty}\\frac{ln\\ x}{x} \\to \\frac{\\infty}{\\infty} \\] Let us apply \\(\\mathbf{L&#39;H\\hat{o}pital&#39;s}\\) rule. Here, we first differentiate the numerator and then differentiate the denominator: \\[\\begin{align*} f&#39;(x) {}&amp; = \\lim_{x\\to\\infty}\\frac{ln\\ x}{x}\\\\ &amp; = \\lim_{x\\to\\infty}\\left(\\frac{\\frac{1}{x}}{1}\\right) \\\\ &amp; = \\lim_{x\\to\\infty}\\left(\\frac{\\frac{1}{\\infty}}{1}\\right)\\\\ &amp; = \\lim_{x\\to\\infty}\\left(\\frac{0}{1}\\right) \\to \\frac{1}{\\infty} = 0 \\\\ &amp; = 0 &amp; \\end{align*}\\] Order Rule What is the antiderivative of an antiderivative of an antiderivative of a function? To answer the question, let us recall the following order-rule formula for differentiation: \\[\\begin{align} first\\ derivative {}&amp; : f&#39;(x) = \\frac{dy}{dx}\\\\ second\\ derivative &amp; : f&#39;&#39;(x) = (f&#39;(x))&#39; = \\frac{d}{dx} \\left(\\frac{dy}{dx}\\right) = \\frac{d^2y}{dx^2} \\\\ third\\ derivative &amp; : f&#39;&#39;&#39;(x) = (f&#39;&#39;(x))&#39; = ((f&#39;(x))&#39;)&#39; = \\frac{d}{dx} \\left(\\frac{d^2y}{dx^2}\\right) = \\frac{d^3y}{dx^3} \\end{align}\\] Therefore: The antiderivative of first derivative: \\[\\begin{align} F(f&#39;(x)) = f(x) \\to \\text{the function itself} \\to y = f(x) \\end{align}\\] The antiderivative of second derivative: \\[\\begin{align} F(f&#39;&#39;(x)) = f&#39;(x) = \\frac{dy}{dx} \\to \\text{the first derivative} \\end{align}\\] The antiderivative of third derivative: \\[\\begin{align} F(f&#39;&#39;&#39;(x)) {} = (f&#39;(x))&#39; = \\frac{d}{dx} \\left(\\frac{dy}{dx}\\right) = \\frac{d^2y}{dx^2} \\to \\text{the second derivative} \\end{align}\\] Example: Find the antiderivatives of the first, second, third, and fourth derivatives of f(x) = \\(x^4 + x^3 + x^2 + 1\\). Here are the order of derivatives for the function: f(x) = \\(x^4 + x^3 + x^2 + 1\\). \\[\\begin{align*} f(x) &amp; = x^4 + x^3 + x^2 + 1 \\\\ (speed) f&#39;(x) &amp; = 4x^3 + 3x^2 + 2x\\\\ (acceleration) f&#39;&#39;(x) &amp; = 12x^2 + 6x + 2\\\\ (torque/force) f&#39;&#39;&#39;(x) &amp; = 24x + 6 \\\\ (horsepower/power) f&#39;&#39;&#39;&#39;(x) &amp; = 24 \\\\ \\end{align*}\\] Therefore: The antiderivative of the first derivative: \\[\\begin{align*} F(f&#39;(x)) {}&amp; = \\int (4x^3 + 3x^2 + 2x) dx \\\\ &amp; = \\frac{4x^{3+1}}{3+1} + \\frac{3x^{2+1}}{2+1} + \\frac{2x^{1+1}}{1+1} + c \\\\ &amp; = \\frac{4x^{4}}{4} + \\frac{3x^{3}}{3} + \\frac{2x^{2}}{2} + c \\\\ &amp; = x^4 + x^3 + x^2 + c &amp;\\to f(x) \\end{align*}\\] The antiderivative of the second derivative: \\[\\begin{align*} F(f&#39;&#39;(x)) {}&amp; = \\int (12x^2 + 6x + 2) dx \\\\ &amp; = \\frac{12x^{2+1}}{2+1} + \\frac{6x^{1+1}}{1+1} + \\frac{2x^{0+1}}{0+1} + c \\\\ &amp; = \\frac{12x^{3}}{3} + \\frac{6x^{2}}{2} + \\frac{2x^{1}}{1} + c \\\\ &amp; = 4x^3 + 3x^2 + 2x + c &amp;\\to f&#39;(x) \\end{align*}\\] The antiderivative of the third derivative: \\[\\begin{align*} F(f&#39;&#39;&#39;(x)) {}&amp; = \\int (24x + 6) dx \\\\ &amp; = \\frac{24x^{1+1}}{1+1} + \\frac{6x^{0+1}}{0+1} + c \\\\ &amp; = \\frac{24x^{2}}{2} + \\frac{6x^{1}}{1} + c \\\\ &amp; = 12x^2 + 6x + c &amp;\\to f&#39;&#39;(x) \\end{align*}\\] The antiderivative of the fourth derivative: \\[\\begin{align*} F(f&#39;&#39;&#39;&#39;(x)) {}&amp; = \\int (24) dx \\\\ &amp; = \\frac{24x^{0+1}}{0+1} + c \\\\ &amp; = \\frac{24x^{1}}{1} + c \\\\ &amp; = 24x + c &amp;\\to f&#39;&#39;&#39;(x) \\end{align*}\\] Find the antiderivative of the antiderivative of the second derivative: \\[\\begin{align} F(F(f&#39;&#39;(x))) = \\iint f&#39;&#39;(x) dxdx \\end{align}\\] Example: \\[\\begin{align*} F(F(f&#39;&#39;(x))) {}&amp; = \\iint (12x^2 + 6x) dx dx \\\\ &amp; = 4x^3 + 3x^2 + 2x + c \\end{align*}\\] Find the antiderivative of the antiderivative of the third derivative: \\[\\begin{align} F(F(F(f&#39;&#39;&#39;(x)))) = \\iiint f&#39;&#39;&#39;(x) dxdxdx \\end{align}\\] Example: \\[\\begin{align*} F(F(f&#39;&#39;(x))) {}&amp; = \\iiint (24x + 6) dxdxdx \\\\ &amp; = 4x^3 + 3x^2 + 2x + c \\end{align*}\\] 4.2 Approximation by Numerical Integration In Chapters 2 and 3 (Numerical Linear Algebra), we covered Numerical Methods to solve for systems of lines and curves. This section introduces Numerical Methods of Integration to calculate areas of shapes and volumes. As part of this section, the Integration (or Quadrature) - or put it, Summation - of smaller squares to fit the area of a circle is what we are now going to discuss as a starting point. Instead of showing how to approximate an area of a full circle, let us perhaps use a quarter of a circle to illustrate a few quadratic techniques. Let us first get some basic intuition. Consider the following Figure 4.8. Figure 4.8: Quadrature Here, Figure 4.8 shows a cosine function bounded by limits [a,b] - the interval - divided into partitions that are spaced equally, denoted as \\(\\Delta x = h\\), between nodes (points), denoted as n. That said, we should emphasize a few things: First, the polynomial function (cosine in this case) is bounded by an interval [ a=0.0, b=1.0 ]. \\[ a = 0.0\\ \\ \\ \\ \\ \\ \\ \\ b = 1.0 \\] Second, the distance (or Delta) is equally spaced between nodes (along the x-axis): \\[ \\Delta x = h = x_1 - x_0 = x_2 - x_1 =\\ ...\\ = \\frac{b - a}{n-1} = \\frac{1.0 - 0.0}{5} = 0.2 \\] We call the equal distance as stepsize \\(\\Delta x = h = \\frac{b - a}{n-1} = 0.2\\). Third, our integration can be computed analytically as such (using cosine function in this case): \\[\\begin{align} \\int_{a}^{b} f(x)dx = \\int_{a=0.0}^{b=1.0} cos(x)dx \\end{align}\\] But numerically, we can use approximation: \\[\\begin{align} \\int_{a}^{b} f(x)dx \\approx \\Delta x\\sum_{i=0}^{n-1} f(x_i)\\ \\ \\ \\ where\\ \\ \\ x_i = a + i \\Delta x \\end{align}\\] That can be expanded this way: \\[\\begin{align} I(f_x) \\approx \\Delta x (\\ f(x_0) + f(x_1) + f(x_2) + f(x_3) + f(x_4) \\ ) \\label{eqn:eqnnumber10} \\end{align}\\] Note how we use a simple quadrature approximation by stacking rectangles horizontally and then adding the area (e.g. Area = width x height = \\(\\Delta x \\times f(x_i)\\)) of each rectangles - this is what we call Reimann sums method. That can be seen in Figure 4.9: Figure 4.9: Approximate Integration Therefore, by adding all the rectangular areas, we get the following approximation (given n-1 = 5 ): \\[\\begin{align} I(f_x) {}&amp;\\approx w \\times h_1 + w \\times h_2 + w \\times h_3 + w \\times h_4 + w \\times h_5 \\\\ &amp;\\approx \\Delta x f(x_0) + \\Delta x f(x_1) + \\Delta x f(x_2) + \\Delta x f(x_3) + \\Delta x f(x_4) \\\\ &amp;\\approx 0.2cos(0.0) + 0.2cos(0.2) + 0.2cos(0.4) + 0.2cos(0.6) + 0.2cos(0.8) \\nonumber \\\\ &amp;\\approx 0.884634 \\nonumber \\end{align}\\] We then compare the numerical approximation with the analytical result: \\[ \\int_{a=0.0}^{b=1.0} cos(x)dx = sin(1) = 0.841471. \\] And we get a large absolute error: \\[ absolute\\ err = 0.884634 - 0.841471 = 0.043163. \\] One way to reduce the error is to add more partitions. For example, instead of only five partitions, consider a larger number of smaller partitions, e.g., n-1=1000. That ends up with equally distant spacing (width), \\(\\Delta x = 0.01\\). See the following: \\[\\begin{align} I(f) \\approx \\sum_{i=0}^{n-1=1000} \\Delta x f(x_i) = 0.8417008 \\end{align}\\] We get a much smaller absolute error: \\[ absolute\\ err = 0.8417008 - 0.841471 = 0.043163 = 0.0002298 \\] Now, we can see in Figure 4.9 that each rectangle has an extra area above the cosine curve. Also, we can see the large space above the cosine curve, especially the rectangle starting at point &lt;\\(x_4, f(x_4)\\)&gt; in the upper left corner. Those extra regions are added to the total area below the curve and thus account for the total absolute error. One way to solve this is to reduce the height of the rectangle so that it just falls in the middle between the x-points, e.g. \\[\\begin{align*} y_0 {}&amp;= cos ((x_0 + x_1)/2) \\\\ y_1 &amp;= cos ((x_1 + x_2)/2) \\\\ &amp;\\vdots \\end{align*}\\] The new Figure 4.10 now looks like so (compare that with Figure 4.9): Figure 4.10: Midpoint Rule By doing so, we get a numerical value 0.841471 equal to the analytical value 0.841471 which renders the absolute error to zero for cos(x) Quadrature with n=1000 and \\(\\Delta x = 0.001\\). The method we used is called Midpoint rule - one of the Newton-Cotes Quadrature rules. Let us illustrate other common Numerical Integration starting with Newton-Cotes Quadrature. 4.2.1 Newton-Cotes Quadrature The Newton-Cotes Quadrature shares rules governing the numerical integration of interpolating polynomial functions - the integrands \\(f(x)\\) - divided into equal spaces to fit a given order of points. Each rule is characterized as either a closed type or an open type. For a closed type rule, our formula tackles the quadrature by including both boundary points (x-axis) into the computation where the integrand stretches from \\(x_0\\) and \\(x_n\\) such that \\(x_0 = a\\) and \\(x_n = b\\). On the other hand, the rule is open otherwise. \\[\\begin{align} x_i = a + \\delta \\Delta x \\end{align}\\] \\[\\begin{align} where\\ \\ \\ \\ \\delta\\Delta x = \\begin{cases} \\delta=i,\\Delta x = \\left(\\frac{b-a}{n-1}\\right) &amp; if\\ closed \\\\ \\\\ \\delta=(i-1),\\Delta x = \\left(\\frac{b-a}{n+1}\\right) &amp; if\\ open \\end{cases} \\label{eqn:eqnnumber11} \\end{align}\\] Recall the following Quadrature equation we derived previously (See Equation \\(\\ref{eqn:eqnnumber10}\\)): \\[\\begin{align} I(f_x) \\approx \\Delta x (\\ f(x_0) + f(x_1) + f(x_2) + f(x_3) + f(x_4) \\ ) \\end{align}\\] In Table 4.6, we show Newton-Cotes quadrature rules and the formulas used to construct the interpolating polynomials1 : \\(f_x = P_n = \\Delta x f(x_0) + \\Delta x f(x_1) + ... + \\Delta x f(x_{n-1})\\). Table 4.6: Newton-Cotes Formula n Rule stepsize: \\(\\Delta x\\) Type Formula: \\(I(f_x)\\) 1 Midpoint \\(\\frac{1}{1}(b-a)\\) open \\(\\Delta x f(\\frac{a + b}{2})\\) 2 Midpoint \\(\\frac{1}{2}(b-a)\\) open \\(2 \\Delta x f(x_1)\\) 3 Trapezoidal \\(\\frac{1}{3}(b-a)\\) open \\(\\frac{3}{2} \\Delta x \\left( f(x_1)+f(x_2) \\right)\\) 4 Milne’s \\(\\frac{1}{4}(b-a)\\) open \\(\\frac{4}{3}\\Delta x\\left( 2f(x_1)-f(x_2)+2f(x_3) \\right)\\) 2 Trapezoidal \\(\\frac{1}{1}(b-a)\\) closed \\(\\frac{1}{2}\\Delta x \\left( f(x_0)+f(x_1) \\right)\\) 2 Simpson’s 1/3 \\(\\frac{1}{2}(b-a)\\) closed \\(\\frac{1}{3}\\Delta x\\left( f(x_0)+4f(x_1)+f(x_2) \\right)\\) 3 Simpson’s 3/8 \\(\\frac{1}{3}(b-a)\\) closed \\(\\frac{3}{8}\\Delta x\\left( f(x_0)+3f(x_1)+3f(x_2)+f(x_3) \\right)\\) 5 Boole’s \\(\\frac{1}{5}(b-a)\\) closed \\(\\frac{2}{45}\\Delta x ( 7f(x_0) +32(x_1)+12f(x_2)+\\) \\(32f(x_3)+7f(x_4) )\\) Note in the table that the formula is categorized as open or closed. It is closed if the interval [a,b] (or endpoints a and b) are used; otherwise it is open. We leave readers to investigate other combinations of the open or closed types (Zhao W. et al. 2013; Ramachandran T. et al. 2017; Zafar F. et al. 2014). Previously, we approximated the cosine integral, \\(\\int cos(x) dx\\), by introducing the use of Reimann rule and newton-cote midpoint rule. Let us explore two more rules based on Table 4.6. Given the same cosine integral: \\[ \\int_{a=0.0}^{b=1.0} cos(x) dx \\] we approximate using two of the newton-cote rules: Trapezoidal rule with n=2 and type=closed: \\[\\begin{align} I(f) \\approx \\Delta x(f(x_0) + f(x_1))\\ \\ \\ where \\ \\ \\ \\Delta x = \\frac{1}{n-1}(b - a) \\end{align}\\] we get: \\[\\begin{align*} \\Delta x {}&amp;= (b - a) = 1 (1 - 0) = 1 \\\\ x_0 &amp;= a + i \\Delta x = 0.0 + 0 \\times 1 = 0.0 \\\\ x_1 &amp;= a + i \\Delta x = 0.0 + 1 \\times 1 = 1.0 \\\\ I(f) &amp;\\approx \\frac{1}{2} \\Delta x(f(x_0) + f(x_1)) \\\\ &amp;\\approx 0.5 * ( cos(0.0) + cos(1.0) ) = 0.7701512 \\end{align*}\\] Simpson’s 1/3 rule with n=2 and type=closed: \\[\\begin{align} I(f) \\approx \\frac{1}{3} \\Delta x(f(x_0) + 4f(x_1) + f(x_2))\\ \\ \\ where \\ \\ \\ \\Delta x = \\frac{1}{2}(b - a) \\end{align}\\] we get: \\[\\begin{align*} \\Delta x {}&amp;= \\frac{1}{2}(b - a) = 0.5 (1 - 0) = 0.5 = 1/2 \\\\ x_0 &amp;= a + i \\Delta x = 0.0 + 0 \\times 0.5 = 0.0 \\\\ x_1 &amp;= a + i \\Delta x = 0.0 + 1 \\times 0.5 = 0.5 \\\\ x_2 &amp;= a + i \\Delta x = 0.0 + 2 \\times 0.5 = 1.0 \\\\ I(f) &amp;\\approx \\frac{1}{3}\\Delta x(f(x_0) + 4f(x_1) + f(x_2)) \\\\ &amp;\\approx 1/3*1/2( cos(0.0) + 4cos(0.5) + cos(1.0) ) = 0.8417721 \\end{align*}\\] Simpson’s 1/3 rule renders a numerical integral of 0.8417721, which is closer to the analytical integral of 0.841471, having an absolute error of 0.0003011, which is quite accurate. 4.2.2 Composite and Adaptive Quadrature Notice that the Trapezoidal rule performs an approximation to the integral of cos(x) but does not quite get there for the expected accuracy. \\[ I(f) \\approx 0.7701512\\ \\ \\ \\ \\ \\text{with an absolute error of } \\mathbf{0.0713198} \\] That is because we use only one trapezoid. We can improve upon that by applying a Composite Trapezoidal rule. It means plainly dividing the interval [ a, b ] into more partitions (or sub-intervals) and then using the Trapezoidal rule for each partition. For that, the Trapezoidal rule equation changes to: \\[\\begin{align} I(f) \\approx \\frac{1}{2}\\Delta x(f(a=x_0) + 2f(x_1) + 2f(x_2)+\\ ...\\ + 2f(x_{n-1}) + f(b=x_n)) \\end{align}\\] where \\(\\Delta x = \\frac{1}{n-1}(b - a)\\). For example, let us try to partition the interval into ten trapezoids (n=11 nodes): \\[ \\Delta x = \\frac{(b - a)}{n-1} = 1/10 = 0.1 \\] where: \\[ x_i = x_0 + i\\Delta x\\ \\ \\ \\ where\\ i = 0,2,\\ ...\\ ,\\ n-1 \\] That gives us: \\[\\begin{align*} x_0 &amp;= a + i \\Delta x = 0.0 + 0 \\times 0.1 = 0.0 \\\\ x_1 &amp;= a + i \\Delta x = 0.0 + 1 \\times 0.1 = 0.1 \\\\ x_2 &amp;= a + i \\Delta x = 0.0 + 2 \\times 0.1 = 0.2 \\\\ &amp;\\vdots \\\\ x_{n-1} &amp;= a + (n-1) \\Delta x = 0.0 + (n-1) \\times 0.1 = 1.0\\\\ I(f) &amp;\\approx \\frac{1}{2}\\Delta x(f(x_0) + 2f(x_1) +\\ ...\\ + 2f(x_9) + f(x_{10})) \\\\ &amp;\\approx 0.05( cos(0.0) + 2cos(0.1) + 2cos(0.2) + \\ ...\\ + 2cos(0.9) + cos(1) ) \\\\ &amp;\\approx 0.840770 \\end{align*}\\] This time the absolute error becomes: 0.841471 - 0.840770 = 0.000701 To illustrate further, let us also tackle Composite Simpson’s 1/3 rule using ten partitions (n=11 nodes): \\[\\begin{align} \\Delta x = \\frac{(b - a)}{n-1} = 1/10 = 0.1 \\end{align}\\] For that, the simpson’s 1/3 rule equation changes to: \\[\\begin{align} I(f) \\approx \\frac{1}{3}\\Delta x(f(a=x_0) + 4f(x_1) + 2f(x_2)+\\ ...\\ + 2f(x_{n-2}) + 4f(x_{n-1}) + f(b=x_n)) \\end{align}\\] Note that the equation works only on condition that the number of partitions is even. That gives us: \\[\\begin{align*} x_0 &amp;= a + i \\Delta x = 0.0 + 0 \\times 0.1 = 0.0 \\\\ x_1 &amp;= a + i \\Delta x = 0.0 + 1 \\times 0.1 = 0.1 \\\\ x_2 &amp;= a + i \\Delta x = 0.0 + 2 \\times 0.1 = 0.2 \\\\ &amp;\\vdots \\\\ x_{n-1} &amp;= a + (n-1) \\Delta x = 0.0 + (n-1) \\times 0.1 = 1.0\\\\ I(f) &amp;\\approx \\frac{1}{3}\\Delta x(f(x_0) + 4f(x_1) + 2f(x_2) +\\ ...\\ + 2f(x_8) + 4f(x_9) + f(x_{10})) \\\\ &amp;\\approx 0.1/3( cos(0.0) + 4cos(0.1) + 2cos(0.2) + \\ ...\\ + 2cos(0.8) + 4cos(0.9) + cos(1) ) \\\\ &amp;\\approx 0.841471 \\end{align*}\\] This time the absolute error becomes: 0.841471 - 0.841471 = 0 The use of the Composite Trapezoidal rule is illustrated in the Romberg integration section later in this chapter. As for the Adaptive Quadrature, the motivation of the Adaptive Quadrature is the same as the Composite Quadrature. The idea is to adjust the number of intervals to optimize accuracy. In other pieces of literature, the choice of tolerance plays a role in optimizing accuracy. 4.2.3 Gaussian Quadrature Note that we can optimize the accuracy of a Numerical Integral based on the correct number of nodes (points), the placements of their abscissas \\(\\mathbf{x_i}\\), and the corresponding optimal value of weights (coefficients) to use. In newton-cotes rule, the placement of abscissas \\(\\mathbf{x_i}\\) are fixed. In the Gaussian Quadrature rule, we can vary the placement of abscissas \\(\\mathbf{x_i}\\) apart from the number of points and corresponding weights; and they can be pre-determined. See Figure 4.11. Figure 4.11: Gaussian Placement of Abscissas To perform Gaussian Quadrature, we use one of the following common Orthogonal polynomials such as: Legendre polynomials Chebyshev polynomials Laguerre polynomials Hermite polynomials Jacobi polynomials For our purpose, we use Legendre polynomials to illustrate Gaussian Quadrature. See Figure 3.13. We may call this Gauss-Legendre Quadrature in the same way we can call Gauss-Chebyshev if we choose to associate Chebyshev polynomials for our Gaussian Quadrature. To proceed with Gauss-Legendre quadrature, we first derive (or pre-determine) the abscissas \\(\\mathbf{x_i}\\). Recall the equation for Legendre polynomials under Higher Degree Polynomials in Chapter 3 (Numerical Linear Algebra II). To illustrate, we reference three Legendre polynomials from the Numerical Linear Algebra II chapter. We can validate the results of the abscissas and weights in our calculations below can using Table 25.4 from USC, Engr CE 108 (n.d.) in reference to Davis P. et al. (1956). Also, in terms of Abscissas and Weights, a good reference to use is Holoborodko P. (2012). For a quadratic Legendre polynomial, we get the abscissas \\(\\mathbf{x_i}\\) using the following: \\[\\begin{align*} P_2(x) {}&amp;= \\frac{1}{2}(3x^2 -1) \\\\ 0 &amp;= \\frac{1}{2}(3x^2 -1)\\ \\ \\ \\ \\text{apply root-finding} \\\\ x_0 &amp;= -0.5773503,\\ \\ \\ x_1 = 0.5773503 \\end{align*}\\] For a Cubic Legendre polynomial, we get the abscissas \\(\\mathbf{x_i}\\): \\[\\begin{align*} P_3(x) {}&amp;= \\frac{1}{2}(5x^3 -3x)\\\\ 0 &amp;=\\frac{1}{2}(5x^3 -3x)\\ \\ \\ \\ \\text{apply root-finding} \\\\ x_0 &amp;= 0,\\ \\ \\ x_1 = -0.7745967,\\ \\ \\ x_2 = 0.7745967 \\end{align*}\\] For a Quartic legendre polynomial, we get: \\[\\begin{align*} P_4(x) {}&amp;= \\frac{1}{8}(35x^4 -30x^2 + 3)\\\\ 0 &amp;=\\frac{1}{8}(35x^4 -30x^2 + 3)\\ \\ \\ \\ \\text{apply root-finding} \\\\ x_0 &amp;= -0.339981,\\ \\ \\ x_1 = 0.339981,\\ \\ \\ x_2 = -0.8611363,\\ \\ \\ x_3 = 0.8611363 \\end{align*}\\] We have a choice to continue computing more roots for other higher-order Legendre polynomials, but for our case, we settle only up to Quartic Legendre polynomial. Also, we leave readers to further investigate the other Legendre polynomials along with how to derive them using the following closed-form [ -1, 1 ] formula: \\[\\begin{align} P_n(x) = \\frac{1}{2\\pi i}\\oint(1-2tx+t^2)^{-1/2}t^{-n-1}dt \\end{align}\\] The open-form ( -1, 1 ) can also be investigated using Gram-Schmidt orthonormalization which is introduced in Numerical Linear algebra under QR factorization. Second, we compute for the weights \\(\\mathbf{w_i}\\) using the derived abscissas \\(\\mathbf{x_i}\\). We use the formula below: \\[\\begin{align} w_i = \\frac{2}{(1-x_i^2)(P&#39;_n(x_i))^2} \\end{align}\\] For example, for a Quadratic Legendre polynomial, with \\(x_0 = -0.5773503\\) and \\(x_1 = 0.5773503\\), and a first derivative of the polynomial \\(P_2(x)\\): \\[ P_2(x) = \\frac{1}{2}(3x^2 -1)\\ \\rightarrow P&#39;_2(x) = 3x \\] we get the following weights: \\[\\begin{align*} w_0 {}&amp;= \\frac{2}{(1-x_0^2)(3x_0)^2} = \\frac{2}{(1-(-0.5773503)^2)(3\\times-0.5773503)^2} = 1 \\\\ \\\\ w_1 &amp;= \\frac{2}{(1-x_1^2)(3x_1)^2} = \\frac{2}{(1-(0.5773503)^2)(3\\times 0.5773503)^2} = 1 \\end{align*}\\] For a Cubic Legendre polynomial, with \\(x_0 = 0\\), \\(x_1 = -0.7745967\\) and \\(x_2 = 0.7745967\\), and a first derivative of the polynomial \\(P_3(x)\\): \\[ P_3(x) = \\frac{1}{2}(5x^3 - 3x)\\ \\rightarrow P&#39;_3(x) = 7.5x^2 - 1.5 \\] where: \\[ w_0 = \\frac{2}{(1-x_0^2)(7.5x_0^2 - 1.5)^2} \\] we get the following weights: \\[ w_0 = 0.8888889,\\ \\ \\ w_1 = 0.5555556,\\ \\ \\ w_2 = 0.5555556 \\] For a Quartic Legendre polynomial, with \\(x_0 = -0.339981\\), \\(x_1 = 0.339981\\), \\(x_2 = -0.8611363\\), and \\(x_3 = 0.8611363\\), and a first derivative of the polynomial \\(P_4(x)\\): \\[ P_4(x) = \\frac{1}{8}(35x^4 -30x^2 + 3)\\ \\rightarrow P&#39;_4(x) = 17.5x^3 - 7.5x \\] where: \\[ w_i = \\frac{2}{(1-x_i^2)(17.5x_i^3 - 7.5x_i)^2} \\] we get the following weights: \\[ w_0 = 0.6521452,\\ \\ \\ w_1 = 0.6521452,\\ \\ \\ w_2 = 0.3478549,\\ \\ \\ w_3 = 0.3478549 \\] Now, we are ready to perform an N-point Gauss-Legendre Quadrature. But let us first use the following integral to get the actual value that we can use to validate against the result of our quadrature approximation later: \\[ \\int_{a=0.0}^{b=1.0} cos(x) dx \\] We scale the integral to form the following: \\[\\begin{align} \\int_a^b f(x) dx = \\frac{(b-a)}{2} \\int_{-1}^1 f \\left( \\frac{b-a}{2}x + \\frac{b+a}{2} \\right) dx \\end{align}\\] Therefore: \\[ \\int_{a=0.0}^{b=1.0} cos(x) dx = \\frac{(b-a)}{2} \\int_{-1}^1 cos \\left( \\frac{b-a}{2}x + \\frac{b+a}{2} \\right) dx = \\frac{1}{2} \\int_{-1}^1cos \\left( \\frac{1}{2}x + \\frac{1}{2} \\right) dx \\] Proof: \\[\\begin{align*} \\frac{1}{2} \\int_{-1}^1cos\\left(\\frac{1}{2}x + \\frac{1}{2}\\right) dx {}&amp;= \\frac{1}{2} \\int_{-1}^1cos\\left(\\frac{1}{2}x\\right) cos\\left(\\frac{1}{2}\\right) - sin\\left(\\frac{1}{2}x\\right) sin\\left(\\frac{1}{2}\\right)\\ dx \\\\ &amp;= \\frac{1}{2} \\left[\\int_{-1}^1cos\\left(\\frac{1}{2}x\\right)cos \\left(\\frac{1}{2}\\right)\\ dx - \\int_{-1}^1sin\\left(\\frac{1}{2}x\\right)sin\\left(\\frac{1}{2}\\right)\\ dx\\right] \\\\ &amp;= \\frac{1}{2} \\left[cos\\left(\\frac{1}{2}\\right)\\int_{-1}^1cos\\left(\\frac{1}{2}x\\right)\\ dx - sin\\left(\\frac{1}{2}\\right)\\int_{-1}^1 sin\\left(\\frac{1}{2}x\\right)\\ dx\\right] \\\\ \\end{align*}\\] \\[\\begin{align*} &amp;\\rightarrow \\int_{-1}^1 cos\\left(\\frac{1}{2}x\\right)\\ dx = 4\\ sin\\left(\\frac{1}{2}\\right) \\ \\ \\ \\ \\text{using u-sub, e.g. cos(u)2du} \\\\ &amp;\\rightarrow\\int_{-1}^1 sin\\left(\\frac{1}{2}x\\right)\\ dx = 0\\ \\ \\ \\ \\ \\text{odd parity} \\\\ &amp;= \\frac{1}{2} \\left[cos\\left(\\frac{1}{2}\\right)4\\ sin\\left(\\frac{1}{2}\\right) - sin\\left(\\frac{1}{2}\\right) \\times 0\\right] \\\\ &amp;= \\frac{1}{2} \\left[cos\\left(\\frac{1}{2}\\right)4\\ sin\\left(\\frac{1}{2}\\right)\\right] \\\\ &amp;= 0.841471 \\end{align*}\\] And now, to perform numerical integration, let us use the following equation: \\[\\begin{align} \\int_{a=0.0}^{b=1.0} f(x) dx \\approx \\frac{1}{2} I_n(f) = \\frac{1}{2}\\sum_{i=0}^{n-1} w_i f(x_i) \\end{align}\\] For a Two-point Gauss-Legendre Quadrature using the derived abscissas \\(\\mathbf{x_i}\\) and weights \\(\\mathbf{w_i}\\) from a Quadratic Legendre polynomial, we evaluate using the formula below: \\[\\begin{align} I_2(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) \\\\ I_2(f) &amp;= (1)cos(-0.5773503) + (1)cos(0.5773503) \\nonumber \\\\ I_2(f) &amp;= (1)(-0.5773503) + (1)(0.5403023) = 1.675824 \\nonumber \\end{align}\\] For a Three-point Gauss-Legendre Quadrature using the derived abscissas \\(\\mathbf{x_i}\\) and weights \\(\\mathbf{w_i}\\) from a Cubic Legendre polynomial, we evaluate using the formula below: \\[\\begin{align} I_3(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) + w_2 f(x_2)\\\\ I_3(f) &amp;= (0.8888889)cos(0) + (0.5555556)cos(-0.7745967) + (0.5555556)cos(0.7745967) \\nonumber \\\\ I_3(f) &amp;= 1.683004 \\nonumber \\end{align}\\] For a Four-point Gauss-Legendre Quadrature using the derived abscissas \\(\\mathbf{x_i}\\) and weights \\(\\mathbf{w_i}\\) from a Quartic Legendre polynomial, we evaluate using the formula below: \\[\\begin{align} I_4(f) {}&amp;= w_0 f(x_0) + w_1 f(x_1) + w_2 f(x_2) + w_3 f(x_3)\\\\ I_4(f) &amp;= (0.6521452)cos(-0.339981) + (0.6521452)cos(0.339981) \\nonumber \\\\ &amp;+ (0.3478549)cos(-0.8611363) + (0.3478549)cos(0.8611363) \\nonumber \\\\ I_4(f) &amp;= 1.682942 \\nonumber \\end{align}\\] Therefore: \\[ \\int_{a=0.0}^{b=1.0} cos(x) dx \\approx \\frac{1}{2} I_4(f) = \\frac{1}{2}(1.682942) = 0.841471 \\] It turns out that the Four-point Gauss-Legendre Quadrature has the most precise result compared to the other two in our sample case of integrating cosine(x). We have used Gauss-Legendre to demonstrate Gaussian Quadrature. It helps to read about other Orthogonal polynomials used. Also, we leave readers to investigate the Gauss-Kronrod Quadrature. 4.2.4 Romberg integration Romberg Integration combines Composite Trapezoidal rule and Richardson Extrapolation to approximate the integral of a function. The idea is to construct the following lower-triangular matrix based on two algorithms (Burden R.L. et al. 2005). \\[ \\begin{array}{lllllllll} R_{0,0} \\\\ &amp; \\searrow \\\\ R_{1,0} &amp; \\rightarrow &amp; R_{1,1}\\\\ &amp; \\searrow &amp; &amp; \\searrow \\\\ R_{2,0} &amp; \\rightarrow &amp; R_{2,1} &amp; \\rightarrow &amp; R_{2,2}\\\\ &amp; \\searrow &amp; &amp; \\searrow &amp; &amp; \\searrow \\\\ R_{3,0} &amp; \\rightarrow &amp; R_{3,1} &amp; \\rightarrow &amp; R_{3,2} &amp; \\rightarrow &amp; R_{3,3}\\\\ \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots &amp; &amp; \\vdots &amp; \\ddots \\\\ R_{n,0} &amp; \\rightarrow &amp; R_{n,1} &amp; \\rightarrow &amp; R_{n,2} &amp; \\rightarrow &amp; R_{n,3} &amp; ... &amp; R_{n,n}\\\\ \\end{array} \\] The first algorithm uses Composite Trapezoidal rule CTR) to construct the first column of the triangle, yielding a column whose components are approximate integral based on trapezoidal rule. The first column is built using a series of intervals (or trapezoids) based on \\(2^n\\) given that \\(k = 1\\). It will be apparent once we illustrate the Romberg integration steps. The second algorithm uses the General Romberg formula where (\\(k \\neq 1\\)): \\[\\begin{align} R_{j,k} = \\frac{4^{k-1}R_{j,k-1} - R_{j-1,k-1}}{4^{k-1}-1} \\end{align}\\] where (for example): \\[\\begin{align} R_{1,1} = \\frac{4^1R_{1,0} - R_{0,0}}{4^1 - 1} = \\frac{4R_{1,0} - R_{0,0}}{3} \\\\ R_{3,2} = \\frac{4^2R_{3,1} - R_{2,1}}{4^2 - 1} = \\frac{16R_{3,1} - R_{2,1}}{15} \\end{align}\\] To illustrate Romberg Integration, as usual, we start with the sample cosine(x) integral: \\[ \\int_{a=0.0}^{b=1.0} cos(x) dx \\] First, let us apply Composite Trapezoidal rule to the above integral by constructing a list of approximate integrals in the order of \\(N=2^n\\), e.g. (n=2, n=4, n=8): n = 2 \\(\\rightarrow\\) \\(\\Delta x = 0.5\\): \\[\\begin{align*} 1/2 I_2(f) {}&amp;= 1/2( (0.5)cos(0) + 2(0.5)cos(0.5) + (0.5)cos(1) ) = 0.8238669 \\\\ \\\\ R_{0,0} &amp;= 0.8238669 \\end{align*}\\] n = 4 \\(\\rightarrow\\) \\(\\Delta x = 0.25\\): \\[\\begin{align*} 1/2 I_4(f) {}&amp;= 1/2( (0.25)cos(0) + 2(0.25)cos(0.25) + 2(0.25)cos(0.50) \\\\ &amp;+ 2(0.25)cos(0.75) + (0.25)cos(1) ) = 0.8370838 \\\\ \\\\ R_{1,0} &amp;= 0.8370838 \\end{align*}\\] n = 8 \\(\\rightarrow\\) \\(\\Delta x = 0.125\\): \\[\\begin{align*} 1/2 I_8(f) {}&amp;= 1/2( 0.125cos(0) + 2(0.125)cos(0.125) + ... \\\\ &amp;+ 2(0.125)cos(0.875)+(0.125)cos(1) ) = 0.8403750 \\\\ \\\\ R_{2,0} &amp;= 0.8403750 \\end{align*}\\] Second, let us apply Richardson Extrapolation using the derived approximation: \\[\\begin{align*} R_{0,0} = 1/2 I_2(f) = 0.8238669\\\\ R_{1,0} = 1/2 I_3(f) = 0.8370838\\\\ R_{2,0} = 1/2 I_4(f) = 0.8403750\\\\ \\end{align*}\\] Solve for \\(R_{1,1}\\): \\[ R_{1,1} = \\frac{ 4^1 R_{1,0} - R_{0,0}}{4^1-1} = \\frac{4(0.8370838) - 0.8238669}{3} = 0.8414894 \\] Solve for \\(R_{2,1}\\): \\[ R_{2,1} = \\frac{ 4^1 R_{2,0} - R_{1,0}}{4^1-1} = \\frac{4(0.8403750) - 0.8370838}{3} = 0.8414721 \\] Solve for \\(R_{2,2}\\): \\[ R_{2,2} = \\frac{ 4^2 R_{2,1} - R_{1,1}}{4^2-1} = \\frac{16(0.8414721) - 0.8414894}{15} = 0.841471 \\] We get a lower triangular matrix with the following Romberg values: \\[ \\begin{array}{lll} R_{0,0} = 0.8238669\\\\ R_{1,0} = 0.8370838 &amp; R_{1,1} = 0.8414894\\\\ R_{2,0} = 0.8403750 &amp; R_{2,1} = 0.8414721 &amp; R_{2,2} = 0.841471\\\\ \\end{array} \\] Here, Romberg Iteration uses n=2, n=4, n=8 in composite trapezoidal rule to yield three initial approximate values for the cosine(x) integral. Then those approximates are further processed using the General Romberg Integration formula. From there, we arrive at an approximate value \\(\\mathbf{R_{2,2}=0.841471}\\) which is remarkably accurate. Here is the Romberg integration algorithm: \\[ \\begin{array}{l} R = matrix(n,n) \\\\ loop\\ j\\ in\\ 1:n \\\\ \\ \\ \\ \\ loop\\ k\\ in\\ 1:j \\\\ \\ \\ \\ \\ \\ \\ \\ \\ if\\ k==1:\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ R_{j,1} = trapezoidal\\_rule(2^j, lower\\_bound, upper\\_bound, func) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ else: \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ R_{j,k} = \\frac{4^{k-1}R_{j,k-1} - R_{j-1,k-1}}{4^{k-1}-1} \\leftarrow \\mathbf{general \\ Romberg\\ formula} \\\\ \\ \\ \\ \\ end\\ loop \\\\ end\\ loop \\end{array} \\] Let us look at our naive implementation of Romberg integration in R code: f &lt;- function(x) { cos(x) } # where t = number of trapezoid # where t + 1 = number of points (including end points) trapezoidal_rule &lt;- function(t, a, b, f) { h = (b-a) / t x = rep(0, (t+1)) s = 0 for (i in 1:(t+1)) { x[i] = a + (i-1) * h if (x[i] != a &amp;&amp; x[i] != b) { s = s + 2 * f( x[i] ) } else { s = s + f( x[i] ) } } 1/2 * ( h * s ) } romberg_integration &lt;- function(n, lower, upper) { R = matrix(rep(0, n*n), n, byrow=TRUE) tol = 1e-5 actual = integrate(f=f, lower=lower, upper=upper)[1]$value approx = 0; err = 0; N_ = 0 for (j in 1:n) { for (k in 1:j) { if (k == 1) { # Trapezoidal rule (1st column) R[j, 1] = trapezoidal_rule(2^j, 0, 1, f) } else { # General Richardson Extrapolation R[j,k] = ( 4^(k-1) * R[j,k-1] - R[j-1, k-1] ) / ( 4^(k-1) - 1) } } # Check error N_ = j approx=R[N_,N_] err = abs( actual - approx ) if (err &lt; tol ) break } A = R[1:N_, 1:N_] colnames(A) = paste(&quot;k&quot;, seq(1,N_,1), sep=&quot;=&quot;) rownames(A) = paste(&quot;j&quot;, seq(1,N_,1), sep=&quot;=&quot;) list(&quot;matrix&quot;=A, &quot;actual_value&quot;=actual, &quot;approx_value&quot;=A[N_,N_], &quot;absolute_error&quot;=err) } romberg_integration(n=5, lower=0, upper=1) ## $matrix ## k=1 k=2 k=3 ## j=1 0.8238669 0.0000000 0.000000 ## j=2 0.8370838 0.8414894 0.000000 ## j=3 0.8403750 0.8414721 0.841471 ## ## $actual_value ## [1] 0.841471 ## ## $approx_value ## [1] 0.841471 ## ## $absolute_error ## [1] 6.849664e-09 4.3 Approximation by Numerical Differentiation Differentiation is the reverse of the Integration process. Similar to Numerical Integration, the idea here is to approximate the value of a derivative (or slope): \\(f&#39;(x)\\). And we use the Finite Difference method to do just that. 4.3.1 Order of Accuracy It may help to explain order of error and accuracy by using Taylor Series. We know that if \\(\\mathbf{\\hat{x}}\\) is an approximate value we are aiming to solve, then \\(h\\) must be the difference between the actual value \\(\\mathbf{x}\\) and the approximate value: \\[\\begin{align} \\mathbf{x} = \\mathbf{\\hat{x}} + h\\ \\ \\ \\ \\ \\ where\\ h = \\mathbf{x} - \\mathbf{\\hat{x}} \\end{align}\\] The question is to know if we can quantify the error. After all, an approximation can come in many forms. To quantify the order of error we use Taylor series to expand a function \\(f(\\hat{x} + h)\\). \\[\\begin{align} f(x) = f(\\hat{x} + h) = f(\\hat{x}) + f&#39;(\\hat{x})h + \\frac{f&#39;&#39;(\\hat{x})h^2}{2!} + \\frac{f&#39;&#39;&#39;(\\hat{x})h^3}{3!}\\ +\\ ... \\end{align}\\] An approximation of a derivative is in the first-order of accuracy if the formula includes only up to the first derivative of the Taylor series expansion; thus, \\(\\mathbf{O(h^2)}\\) gets truncated (sort of trimming the error): \\[\\begin{align} f(\\hat{x} + h) {}&amp;= f(\\hat{x}) + f&#39;(\\hat{x})h + \\mathbf{O(h^2)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\mathbf{O(h^2)} = \\frac{f&#39;&#39;(\\xi)h^2}{2!}\\ +\\ ... \\\\ \\nonumber \\\\ f(\\hat{x} + h) &amp;\\approx f(\\hat{x}) + f&#39;(\\hat{x})h \\end{align}\\] An approximation of a derivative is in the second-order of accuracy if the formula includes up to the second derivative of the Taylor series expansion; thus, \\(\\mathbf{O(h^3)}\\) gets truncated: \\[\\begin{align} f(\\hat{x} + h) {}&amp;= f(\\hat{x}) + f&#39;(\\hat{x})h + \\frac{f&#39;&#39;(\\hat{x})h^2}{2!} + \\mathbf{O(h^3)}\\ \\ \\ where\\ \\mathbf{O(h^3)} = \\frac{f&#39;&#39;(\\xi)h^3}{3!}\\ +\\ ... \\\\ \\nonumber \\\\ f(\\hat{x} + h) &amp;\\approx f(\\hat{x}) + f&#39;(\\hat{x})h + \\frac{f&#39;&#39;(\\hat{x})h^2}{2!} \\end{align}\\] Higher-order of accuracy for approximation follows the same. The minute we truncate the Taylor series expansion of a function, the result becomes just an approximation, and therefore there is a Finite Difference. 4.3.2 Finite Difference Recall the following formula in Calculus section under Limits subsection: \\[\\begin{align} f&#39;(x) = \\lim_{h\\rightarrow 0}\\frac{f(x+h) - f(x)}{h} \\end{align}\\] Recall also that we introduced the concept of slope in Calculus section under Slopes subsection. We describe slope as the average rate of change. In other words, the derivative f’(fx) represents the slope. Here, with both the terms Limits and Slopes, let us introduce another common phrase, Difference Quotient, which is the formula derived from above: \\[\\begin{align} \\frac{f(x+h) - f(x)}{h} \\end{align}\\] The derivation of the formula is explained - geometrically - in the Calculus section under the Slopes subsection. Here, we derive \\(\\mathbf{f(x+h)}\\) using Taylor Series expansion (LeVeque R. J. 2007, ch.1). As long as we have a finite h &gt; 0, then the Taylor series expansion below becomes useful in deriving a formula for \\(\\mathbf{f(x+h)}\\). \\[\\begin{align} f(x + h) = f(x) + \\frac{f&#39;(x)h^1}{1!} + \\frac{f&#39;&#39;(x)h^2}{2!}\\ +\\ \\frac{f&#39;&#39;&#39;(x)h^3}{3!}\\ + \\ ... \\end{align}\\] If we limit the order of accuracy to first-order such that: \\[\\begin{align} f(x + h) \\approx f(x) + \\frac{f&#39;(x)h^1}{1!} \\end{align}\\] then we get the finite forward difference formula: \\[\\begin{align} f&#39;(x) \\approx \\frac{f(x+h) - f(x)}{h} \\leftarrow \\mathbf{\\text{finite forward difference}} \\end{align}\\] Another formula to consider is the Finite Backward Difference: \\[\\begin{align} f&#39;(x) \\approx \\frac{f(x) - f(x -h)}{h} \\leftarrow \\mathbf{\\text{finite backward difference}} \\end{align}\\] which is derived out of the same first-order accuracy: \\[\\begin{align} f(x - h) \\approx f(x) - \\frac{f&#39;(x)h^1}{1!} \\end{align}\\] using the following Taylor series expansion to derive \\(\\mathbf{f(x - h)}\\): \\[\\begin{align} f(x - h) = f(x) + \\frac{f&#39;(x)(-h)^1}{1!} + \\frac{f&#39;&#39;(x)(-h)^2}{2!}\\ +\\ \\frac{f&#39;&#39;&#39;(x)(-h)^3}{3!}\\ + \\ ... \\end{align}\\] Combining the Finite Forward Difference and Finite Backward Difference formulae gives us the Finite Centered Difference formula of first-order accuracy: \\[\\begin{align} f&#39;(x) {}&amp;\\approx \\frac{f(x+h) - f(x)}{h} + \\frac{f(x) - f(x -h)}{h} \\\\ \\nonumber \\\\ f&#39;(x) &amp;\\approx \\frac{f(x+h) -f(x - h)}{2h} \\leftarrow \\mathbf{\\text{finite centered difference}} \\end{align}\\] To illustrate, suppose we have the following problem statement and we are looking for the differential (derivative) of \\(\\mathbf{cos(6x)}\\): \\[ f(x) = cos(6x), \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ x = 0.5,\\ \\ \\ \\ \\ h=0.1 \\] where exact solution is: \\[ f&#39;(x) = -6sin(6x) = -6sin(6 \\times 0.5) = -0.84672 \\] To approximate the value \\(f&#39;(x)\\), let us use the three finite difference formulas: \\[\\begin{align*} FD \\rightarrow f&#39;(x) {}&amp;\\approx \\frac{f(x+h) - f(x)}{h} = \\frac{cos(0.5+ 0.1) - cos(0.5)}{0.1} \\\\ &amp;\\approx \\frac{-0.896758 - -0.989992}{0.1} = 0.932341 \\\\ \\\\ BD \\rightarrow f&#39;(x) &amp;\\approx \\frac{f(x) - f(x -h)}{h} = \\frac{cos(0.5) - cos(0.5-0.1)}{0.1} \\\\ &amp;\\approx \\frac{-0.989992 - -0.737394}{0.1} = -2.525988 \\\\ \\\\ CD \\rightarrow f&#39;(x) &amp;\\approx \\frac{f(x+h) -f(x - h)}{2h} = \\frac{cos(0.5+0.1) - cos(0.5-0.1)}{0.2} \\\\ &amp;\\approx \\frac{-0.896758 - -0.737394}{0.2} = -0.796824 \\end{align*}\\] The Centered Difference formula yields an absolute error of -0.049896 compared to the other two. Here is an implementation of the Finite Difference methods in R code: Figure 4.12: Finite Difference For a higher-order Numerical Differentiation, we can use the same Taylor Series. For example, to approximate the second derivative of a function, we first can combine the following: \\[\\begin{align} f(x + h) + f(x -h) {}&amp;= \\left[ f(x) + \\frac{f&#39;(x)(h)^1}{1!} + \\frac{f&#39;&#39;(x)(h)^2}{2!}\\ +\\ \\frac{f&#39;&#39;&#39;(x)(h)^3}{3!} + O(h^4) \\right] \\nonumber \\\\ &amp;+ \\left[ f(x) + \\frac{f&#39;(x)(-h)^1}{1!} + \\frac{f&#39;&#39;(x)(-h)^2}{2!}\\ +\\ \\frac{f&#39;&#39;&#39;(x)(-h)^3}{3!} + O(h^4) \\right] \\nonumber \\\\ &amp;= \\left[ 2f(x) + 2\\frac{f&#39;&#39;(x)(h)^2}{2!}\\ + 2\\frac{f^{(4)}(x)(h)^4}{4!}\\ + O(h^6) \\right] \\\\ &amp;= 2f(x) + f&#39;&#39;(x)(h)^2\\ + O(h^4) + ... \\end{align}\\] Note that terms with odd stepsize such as \\(\\pm h^1, \\pm h^3, \\pm h^5, ...\\) are cancelled when added, e.g.: \\[\\begin{align} f&#39;(x)(h)^1 + f&#39;(x)(-h)^1 = f&#39;(x)(h)^1 - f&#39;(x)(h)^1 = 0 \\end{align}\\] Now, re-arrange the terms to get the approximate second derivative of a function: \\[\\begin{align} f(x + h) + f(x -h) {}&amp;= 2f(x) + f&#39;&#39;(x)(h)^2\\ + O(h^4) \\\\ f&#39;&#39;(x)(h)^2\\ + O(h^4) &amp;= f(x + h) - 2f(x) + f(x -h) \\\\ f&#39;&#39;(x)\\ &amp;= \\frac{f(x + h) - 2f(x) + f(x -h) }{h^2} + O(h^2)\\ \\ \\leftarrow \\frac{O(h^4)}{h^2} = O(h^2) \\\\ f&#39;&#39;(x)\\ &amp;\\approx \\frac{f(x + h) - 2f(x) + f(x -h) }{h^2} \\end{align}\\] We drop the order of accuracy \\(\\mathbf{O(h^2)}\\) since we are approximating. From there, we get the second-order Finite Centered Difference. We then compare that with the exact solution based on the second derivative: \\[ f&#39;(x) = -6sin(6x)\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ f&#39;&#39;(x) = -36cos(6x) \\] 4.4 Approximation using Ordinary Differential Equations Change happens from moment to moment and place to place. It happens from the movement of celestial bodies down to the motion of a pendulum. There is Change where there is growing, aging, decaying, cooling, and heating. There is also Change as water flows downstream or as the wind blows in all directions. In this section, we deal with the rate of Change with respect to time - we introduce Differential Equations. Differential Equations, in one of its most basic examples, is expressed in one of the following general forms: \\[\\begin{align} y&#39; = y\\ \\ \\ \\ \\ \\ or\\ \\ \\ \\ \\ \\ \\ y&#39;&#39; = y\\ \\ \\ \\ \\ \\ or\\ \\ \\ \\ \\ \\ \\ y&#39;&#39; + y&#39; = y\\ \\ \\ \\ \\ or\\ \\ \\ \\ \\ \\ \\ y&#39;&#39;&#39; + y&#39;&#39; + y&#39; = y \\end{align}\\] It simply means that the equation comes with a function y and its corresponding one or more derivatives y’, y’’, etc. Note that what we refer to as function y can also be expressed in this form: \\[\\begin{align} y&#39; = y\\ \\ \\ \\rightarrow\\ \\ \\ f&#39;(x) = f(x)\\ \\ \\ \\ \\ \\ where\\ y = f(x)\\ \\ and\\ \\ y&#39; = f&#39;(x) \\end{align}\\] For most of the methods of differential equations covered in this chapter, we use the simplest form: \\(y&#39; = y\\ or\\ y&#39;&#39;=y\\) to explain a point. The idea of differential equations is to solve for function y. For that, we use a method called separation of variables. As long as a differential equation can be separable, then a separable differential equation can yield us a solution to a function. For example, we know that: \\[\\begin{align} \\frac{dy}{dx} = y\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ y&#39; = y \\end{align}\\] Here we separate x and y like so, then we integrate: \\[ \\frac{dy}{dx} = y\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\frac{dy}{y} = dx\\ \\ \\rightarrow\\ \\ \\ \\int\\frac{dy}{y} = \\int dx\\ \\ \\ \\rightarrow\\ \\ \\ \\ln|y|+c = x + c\\ \\ \\ \\ \\rightarrow\\ \\ \\ y = e^{x} \\] Here, we illustrate how to solve a function which can be expressed this way: \\[ y = e^{x}\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ f(x) = e^{x} \\] Plotting \\(\\mathbf{f(x) = e^{x}}\\) gives us the following Figure 4.13. Figure 4.13: Separable Differential Equations One pointer to note is that not all Differential Equations are separable or that solving the function y may post an extreme challenge. For that, we need to approximate. We investigate a few approximation methods to solve function y in ODE following the rest of the sections. Note that other literature denotes the approximate solution y as u to distinguish exact solution vs. approximate solution, e.g.: \\[ \\mathbf{y_{(exact)}\\ vs\\ y_{(approx)}}\\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ \\ \\ y=e^x\\ \\ \\ \\ \\ \\ vs\\ \\ \\ \\ u\\approx c_0 + c_1x + c_2 x^2 \\] In most of our discussions, we use the notation y to denote the approximate solution. And we express the exact solution more explicitly. First, we introduce two types of Differential equations: the first type is the Ordinary Differential Equations (ODE) with one input. The second type is the Partial Differential Equations (PDE) with multiple inputs. With ODE, there are two problems we try to solve: the initial value problem (IVP) and the boundary value problem (BVP). In many cases, one of the challenges we deal with when it comes to Change is determining the initial state of the problem before the first Change - what was the initial value of the problem? If the initial value is unknown, we face multiple solutions. But suppose a problem has a given initial value, e.g., \\(y(0) = 1\\), and has a derivative of an unknown function. In that case, this is an initial value problem (IVP), so then we can approximate a target value. We introduce a few methods that deal with solving IVPs in the following sections, using an initial value as a starting point for an iterative method. We start with one of the classic methods called Euler’s method. 4.4.1 Euler’s Method (Explicit) All Change starts from an initial value, then Change happens one step at a time over the course of a period. If we are to express the statement into an Euler equation, we get: \\[\\begin{align} y_{1} = y_0 + \\Delta t y&#39;_0\\ \\ \\ \\leftarrow y_1 = y_0 + \\Delta t\\ f(t_0, y_0) \\ \\ \\ \\ \\ \\leftarrow \\text{recall}\\ y = mx + b \\end{align}\\] where: \\[ \\begin{array}{lll} y_0 &amp;\\leftarrow &amp; \\text{initial value} \\\\ y_1 &amp;\\leftarrow &amp;\\text{next value} \\\\ t &amp;\\leftarrow &amp; \\text{time}\\ \\ \\ \\text{t is the abscissa x in x-axis }\\\\ \\Delta t &amp;\\leftarrow &amp;\\text{one step at a time (stepsize of time)} \\\\ y_0&#39; &amp;\\leftarrow &amp;\\text{course of time (change with respect to time)}\\ \\ \\leftarrow \\frac{dy_0}{dt_0} \\end{array} \\] Here, we are after approximating an unknown target point given an initial point and a first-order derivative of a function. In general, the formula is as follows: \\[\\begin{align} y_{k+1} = y_{k} + \\Delta t y&#39;_k \\ \\ \\ \\leftarrow y_{k+1} = y_k + \\Delta t\\ f(t_k, y_k) \\end{align}\\] and where: \\[\\begin{align} y_k&#39; = f(t_k, y_k) \\end{align}\\] To illustrate, we use basic iterative method called Euler’s method. For example, use Euler’s method to approximate a target point (\\(\\mathbf{x_1, y_1}\\)) given the following: \\[\\begin{align} y_0(0) = 1,\\ \\ \\ \\ y_1(1) = &lt;unknown\\ target&gt;\\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39; = y \\end{align}\\] Here, we have: \\[\\begin{align} f(t_k, y_k) = y_k&#39; = y\\ \\ \\ \\ \\ \\ \\ \\ \\text{where our exact solution is }\\ \\mathbf{y = e^t} \\end{align}\\] See Table 4.7. Table 4.7: Euler Table t \\(y_k\\) \\(y_k + \\Delta t y&#39;_k\\) approx:\\(\\ y_{k+1}\\) exact:\\(\\ e^{t+1}\\) Err 0.0 1.000 \\(1.000 + (0.2)(1.000)\\) 1.200 1.221 0.021 0.2 1.200 \\(1.200 + (0.2)(1.200)\\) 1.440 1.492 0.052 0.4 1.440 \\(1.440 + (0.2)(1.440)\\) 1.728 1.822 0.094 0.6 1.728 \\(1.728 + (0.2)(1.728)\\) 2.074 2.226 0.152 0.8 2.074 \\(2.074 + (0.2)(2.074)\\) 2.488 2.718 0.230 \\(\\mathbf{1.0}\\) \\(\\mathbf{2.488}\\) – – \\(\\mathbf{2.718}\\) \\(\\mathbf{0.230}\\) Here is a naive implementation of Euler’s method in R code using a more general IVP method function (see Figure 4.14): f_true &lt;- function(x) { exp( x) } f_forward &lt;- function(tk, yk) { yk } f_backward &lt;- function(delta_tk, yk) { yk / ( 1 - delta_tk )} ivp_method &lt;- function(init_val, fun, iteration, target, type=&quot;rungekutta&quot;) { x0 = init_val[1] y0 = yk = init_val[2] k = seq(0, target, length.out=iteration ) stepsize = k[2] - k[1] sequence = matrix(0, 0, 6) cnt = 0 for (tk in k) { if (type == &quot;explicit&quot;) { # euler forward yk = yk + stepsize * f_forward(tk, yk) } else if (type == &quot;implicit&quot;) { # euler backward yk = f_backward( stepsize, yk) } else if (type == &quot;heun&quot;) { k1 = f_forward(tk, yk) k2 = f_forward(tk + stepsize, yk + stepsize * k1 ) yk = heun = yk + stepsize / 2 * (k1 + k2) } else if (type == &quot;rungekutta&quot;) { k1 = f_forward(tk, yk) k2 = f_forward(tk + stepsize / 2, yk + stepsize / 2 * k1 ) k3 = f_forward(tk + stepsize / 2, yk + stepsize / 2 * k2 ) k4 = f_forward(tk + stepsize, yk + stepsize * k3) yk = yk + stepsize/6 * ( k1 + 2*k2 + 2*k3 + k4) } true_y = exp(tk ) err = abs( true_y - y0 ) cnt = cnt + 1 sequence = rbind(sequence, c(cnt, stepsize, tk, y0, true_y, err)) y0 = yk } colnames(sequence) = c(&quot;iteration&quot;, &quot;stepsize&quot;, &quot;tk&quot;, &quot;y-approx&quot;, &quot;y-exact&quot;, &quot;absolute error&quot;) list(&quot;Iteration&quot;= sequence, &quot;count&quot;=nrow(sequence), &quot;type&quot;=type ) } ivp_plot &lt;- function(ivp, base_delta, target, label=TRUE) { iteration = ivp$Iteration iteration = iteration[ which( round(iteration[,3],3) %in% round(seq(0, target, base_delta),3) ), ] n = nrow(iteration) for (k in 1:(n-1)) { h = iteration[k+1,3] - iteration[k,3] x1 = iteration[k,3] x2 = x1 + h y1_true = f_true(x1) y2_true = f_true(x2) y1 = iteration[k,4] y2 = iteration[k+1,4] if (k == n - 1) { a = round( iteration[k+1,4],5) stepsize = iteration[k, 2] text(1.2, y2, cex=0.7, label=substitute( paste(Delta,&quot;t=&quot;, stepsize, &quot;, y(1)=&quot;, a, sep=&quot;&quot;))) } lines(c(x1,x2), c(y1,y2), lty=2, col=&quot;darksalmon&quot;) points(c(x1, x2), c(y1, y2), pch=16, col=&quot;darksalmon&quot;) points(c(x1, x2), c(y1_true, y2_true), pch=16, col=&quot;navyblue&quot;) } if (ivp$type == &quot;explicit&quot; &amp;&amp; label==TRUE) { text(1,2.3, cex=0.7, label=&quot;tangent&quot;) } if (label==TRUE) { iteration } } n = 15 x = seq(0,1, length.out=n) ## iteration stepsize tk y-approx y-exact absolute error ## [1,] 1 0.2 0.0 1.00000 1.000000 0.00000000 ## [2,] 2 0.2 0.2 1.20000 1.221403 0.02140276 ## [3,] 3 0.2 0.4 1.44000 1.491825 0.05182470 ## [4,] 4 0.2 0.6 1.72800 1.822119 0.09411880 ## [5,] 5 0.2 0.8 2.07360 2.225541 0.15194093 ## [6,] 6 0.2 1.0 2.48832 2.718282 0.22996183 ## iteration stepsize tk y-approx y-exact absolute error ## [1,] 1 0.05 0.0 1.000000 1.000000 0.000000000 ## [2,] 5 0.05 0.2 1.215506 1.221403 0.005896508 ## [3,] 9 0.05 0.4 1.477455 1.491825 0.014369254 ## [4,] 13 0.05 0.6 1.795856 1.822119 0.026262474 ## [5,] 17 0.05 0.8 2.182875 2.225541 0.042666340 ## [6,] 21 0.05 1.0 2.653298 2.718282 0.064984123 ## iteration stepsize tk y-approx y-exact absolute error ## [1,] 1 0.002 0.0 1.000000 1.000000 0.0000000000 ## [2,] 101 0.002 0.2 1.221159 1.221403 0.0002439310 ## [3,] 201 0.002 0.4 1.491229 1.491825 0.0005958164 ## [4,] 301 0.002 0.6 1.821027 1.822119 0.0010914887 ## [5,] 401 0.002 0.8 2.223764 2.225541 0.0017773523 ## [6,] 501 0.002 1.0 2.715569 2.718282 0.0027133078 Figure 4.14: IVP Methods Note that the dotted lines in the figure represent tangent lines for which the corresponding slope is based on the approximated point (\\(\\mathbf{t_{k+1}, y_{k+1}}\\)). We sample three iterations in the code towards an approximate value. The one with the smallest step size (0.002) results in better accuracy. It yields an approximate value of \\(\\mathbf{2.71557}\\) with an absolute error of \\(\\mathbf{0.0027}\\) compared to the other two. As the plot shows in Figure 4.14, we can observe that the smaller the step size becomes, the tangent line gets more aligned to the actual function curve. Also, we excluded all other nodes (or tangent lines) in the plot to show a fixed set of select but fixed nodes. Also, recall that if we do not specify an initial value, we end up with an infinite number of solutions. Figure 4.15 shows an example of multiple solutions. In the figure, we show three possible initial points, \\(y(0) \\in (0.5,1,5)\\), arriving at three separate unique solutions. Figure 4.15: Initial Value Problem - Euler Method Indeed, our goal in solving IVPs is to set a unique solution by having an initial value and a corresponding derivative of a function. 4.4.2 Euler’s Method (Implicit) The previous Euler’s method is an explicit method which uses conditions under the current state to advance to the next state. This method is called Forward Euler’s method with the following general formula: \\[\\begin{align} y_{k+1} = y_{k} + \\Delta t\\ y&#39;_{k} \\ \\ \\ \\leftarrow y_{k+1} = y_k + \\Delta t\\ f(t_{k}, y_{k}) \\ \\ \\leftarrow\\ \\ \\ \\frac{y_{k+1} - y_{k-1}}{\\Delta t} = f(t_{k}, y_{k}) \\end{align}\\] which can also be derived from a truncated Taylor series expansion: \\[\\begin{align} y_{k+1} \\approx f(t_k + \\Delta t) = f(t_k) + \\frac{f&#39;(t_k)\\Delta t^1}{1!} + O(\\Delta t^2) \\end{align}\\] Here, we explore Euler’s method further by illustrating implicit method called Backward Euler’s method. The method uses conditions of the next state, which may appear counter-intuitive, but the method becomes more apparent using the following general formula and an illustration. \\[\\begin{align} y_{k+1} &amp;= y_{k} + \\Delta t y&#39;_{k+1} \\\\ &amp;= y_k + \\Delta t\\ f(t_{k+1}, y_{k+1}) \\ \\ \\leftarrow\\ \\ \\ \\frac{y_{k+1} - y_{k}}{\\Delta t} = f(t_{k+1}, y_{k+1}) \\end{align}\\] which can also be derived from a truncated Taylor series expansion: \\[\\begin{align} y_{k} \\approx f(t_{k+1} - \\Delta t) = f(t_{k+1}) - \\frac{f&#39;(t_{k+1})\\Delta t^1}{1!} + O(\\Delta t^2) \\end{align}\\] The implicit method improves upon the stability that otherwise, in certain cases, is unattainable using the explicit method. To illustrate, we use implicit method to approximate a target point (\\(\\mathbf{x_1, y_1}\\)) given the following: \\[ y_0(0) = 1,\\ \\ \\ \\ y_1(1) = &lt;unknown\\ target&gt;\\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39; = y, \\] and where our exact solution is \\(\\mathbf{y=e^t}\\). Let us re-arrange our implicit method equation to adapt to our specific problem: \\[\\begin{align} y_{k+1} {}&amp;= y_k + \\Delta t\\ f(t_{k+1}, y_{k+1}) \\\\ y_{k+1} &amp;= y_k + \\Delta t\\ y_{k+1}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ f(t_{k+1}, y_{k+1}) = y&#39;_{k+1} = y_{k+1} \\\\ y_{k+1} - \\Delta t\\ y_{k+1} &amp;= y_k \\\\ y_{k+1}( 1 - \\Delta t) &amp;= y_k \\\\ y_{k+1} &amp;= \\frac{y_k}{(1 - \\Delta t)} \\end{align}\\] See Table 4.8. Table 4.8: Backward Euler Table t \\(y_k\\) \\(y_k / (1 - \\Delta t )\\) approx:\\(\\ y_{k+1}\\) exact:\\(\\ e^{t+1}\\) Err 0.0 1.000 \\(1.000 / (0.8)\\) 1.250 1.221 0.029 0.2 1.250 \\(1.250 / (0.8)\\) 1.562 1.492 0.070 0.4 1.562 \\(1.562 / (0.8)\\) 1.953 1.822 0.131 0.6 1.953 \\(1.953 / (0.8)\\) 2.441 2.226 0.215 0.8 2.441 \\(2.441 / (0.8)\\) 3.052 2.718 0.334 \\(\\mathbf{1.0}\\) \\(\\mathbf{3.052}\\) – – \\(\\mathbf{2.718}\\) \\(\\mathbf{0.334}\\) As we can see, Backward Euler’s method requires extra algebraic preparation to arrive at a new equation such as: \\(y_{k+1} = y_k / (1 - \\Delta t)\\). Such preparation is only possible because we use a simple differential equation: \\(y&#39; = y\\). It could have been a lot more complicated. But even after simplifying the equation, Backward Euler’s method does not always yield better accuracy than the explicit method though perhaps more stable. As for applying a naive implementation of the implicit method in R code, just set the type to implicit. # invoke ivp_methods target = 1.0 base_delta = 0.2 ivp = ivp_method(c(0,1), iteration=6, target = target, type=&quot;implicit&quot;) ivp_plot(ivp, base_delta, target, label=FALSE) 4.4.3 Heun’s Method Heun’s method is called an improved Euler’s method. It is also a second-order implicit method that combines both the Euler’s explicit and implicit equations to arrive at the following: \\[\\begin{align} y_{k+1} = y_k + \\frac{\\Delta t}{2}\\left( \\mathbf{k_1} + \\mathbf{k_2} \\right) \\end{align}\\] where: \\[\\begin{align*} \\mathbf{k_1} {}&amp;= f(t_{k}, y_{k}) \\\\ \\mathbf{k_2} &amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \\Delta t, y_k + \\Delta t k_1) \\end{align*}\\] To illustrate, we use Heun method to approximate a target point (\\(\\mathbf{x_1, y_1}\\)) given the following: \\[ y_0(0) = 1,\\ \\ \\ \\ y_1(1) = &lt;unknown\\ target&gt;\\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39; = y, \\] and where our exact solution is \\(\\mathbf{y=e^t}\\). Note that: \\[\\begin{align} y_k&#39; = f(t_k, y_k) \\end{align}\\] First, we solve for \\(\\mathbf{k_1}\\). \\[\\begin{align} \\mathbf{k_1} = f(t_{k}, y_{k}) = y&#39;_k = y_k = 1 \\end{align}\\] Second, we solve for \\(\\mathbf{k_2}\\). \\[\\begin{align} \\mathbf{k_2} {}&amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \\Delta t, y_k + \\Delta t k_1) \\\\ &amp;= f( 0.0 + 0.2, 1 + 0.2(1)) = f(0.2, 1.2) \\nonumber \\\\ &amp;= 1.2\\ \\ \\ \\leftarrow\\ \\ \\ \\ f(t_k, y_k) = y_k \\nonumber \\end{align}\\] Third, we solve for \\(\\mathbf{y_{k+1}}\\): \\[\\begin{align} y_{k+1} {}&amp;= y_k + \\frac{\\Delta t}{2}\\left( \\mathbf{k_1} + \\mathbf{k_2} \\right) \\\\ &amp;= 1 + \\frac{0.2}{2}(1 + 1.2) \\nonumber \\\\ &amp;= 1 + 0.1 (2.2) = 1.22 \\nonumber \\end{align}\\] Finally, we repeat the process until we hit the target, substituting \\(\\mathbf{y_{k}}\\) with \\(\\mathbf{y_{k+1}}\\). See Table 4.9. Table 4.9: Heun Table t \\(y_k\\) \\(f(t_k, y_k)\\) \\(f(t_{k+1}, y_{k+1})\\) approx:\\(\\ y_{k+1}\\) exact:\\(\\ e^{t+1}\\) Err 0.0 1.000 1.000 1.220 1.220 1.221 0.001 0.2 1.220 1.220 1.464 1.488 1.492 0.003 0.4 1.488 1.488 1.786 1.816 1.822 0.006 0.6 1.816 1.816 2.179 2.215 2.223 0.010 0.8 2.215 2.215 2.658 2.702 2.718 0.015 \\(\\mathbf{1.0}\\) \\(\\mathbf{2.702}\\) – – – \\(\\mathbf{2.718}\\) \\(\\mathbf{0.015}\\) As for applying a naive implementation of the Heun’s method in R code, just set type to heun. # invoke ivp_methods target = 1.0 base_delta = 0.2 ivp = ivp_method(c(0,1), iteration=6, target = target, type=&quot;heun&quot;) ivp_plot(ivp, base_delta, target, label=FALSE) 4.4.4 Runge-Kutta Method The Runge-Kutta method is an implicit method and comes with different formulas of N-order (Heath M.T. 2002; Burden R.L. et al. 2005). Below are two examples of the Runge-Kutta method of the 3rd-order and 4th-order. \\[ \\begin{array}{llll} \\begin{array}{l} 3rd-Order \\\\ ========\\\\ \\mathbf{k_1} = f(t_k, y_k) \\\\ \\mathbf{k_2} = f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + \\frac{\\Delta t}{2} \\mathbf{k_1} \\right) \\\\ \\mathbf{k_3} = f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + 2 \\mathbf{k_2} - \\mathbf{k_1} \\right) \\\\ \\\\ \\\\ \\mathbf{y_{k+1}} = y_k + \\frac{\\Delta t}{6}\\ ( \\mathbf{k_1} + 4\\mathbf{k_2} + \\mathbf{k_3} ) \\end{array} &amp; &amp; &amp; \\begin{array}{l} 4th-Order \\\\ ========\\\\ \\mathbf{k_1} = f(t_k, y_k) \\\\ \\mathbf{k_2} = f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + \\frac{\\Delta t}{2} \\mathbf{k_1} \\right) \\\\ \\mathbf{k_3} = f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + \\frac{\\Delta t}{2} \\mathbf{k_2} \\right) \\\\ \\mathbf{k_4} = f(t_k + \\Delta t,\\ y_k + \\Delta t \\mathbf{k_3})\\\\ \\\\ \\mathbf{y_{k+1}} = y_k + \\frac{\\Delta t}{6}\\ ( \\mathbf{k_1} + 2\\mathbf{k_2} + 2\\mathbf{k_3} + \\mathbf{k_4}) \\end{array} \\end{array} \\] In fact, Heun’s method is a 2nd-order Runge-Kutta method: \\[\\begin{align} y_{k+1} = y_k + \\frac{\\Delta t}{2}(\\mathbf{k_1} + \\mathbf{k_2}) \\end{align}\\] where: \\[\\begin{align} \\mathbf{k_1} {}&amp;= f(t_{k}, y_{k}) \\\\ \\mathbf{k_2} &amp;= f\\left(t_k + \\Delta t,\\ y_k + \\Delta t \\mathbf{k_1} \\right) \\end{align}\\] We leave the readers to investigate other orders such as 5th-order and 6th-order. Here, we illustrate Runge-Kutta method using the 4th-order. Note that, with the 4th order, the parameters \\(\\mathbf{k_1, k_2, k_3, k_4}\\) are all slopes of tangent line to an estimated curve function. Those four slopes are averaged to arrive at an optimal slope, e.g. \\(y_{k+1} = y&#39;_1\\). See Figure 4.16. Figure 4.16: Runge-Kutta Method To illustrate, we use Runge-Kutta method to approximate a target point (\\(\\mathbf{x_1, y_1}\\)) given the following: \\[ y_0(0) = 1,\\ \\ \\ \\ y_1(1) = &lt;unknown\\ target&gt;\\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39; = y, \\] and where our exact solution is \\(\\mathbf{y=e^t}\\). First, solve for \\(\\mathbf{k_1}\\): \\[\\begin{align} k_1 = f(t_k, y_k) = y_k&#39; = y_k = 1 \\end{align}\\] Notice \\(\\mathbf{k_1}\\) is similar to forward Euler’s method Second, solve for \\(\\mathbf{k_2}\\): \\[\\begin{align} k_2 {}&amp;= f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + \\frac{\\Delta t k_1}{2} \\right) \\\\ &amp;= f \\left(0.0 + \\frac{0.2}{2},\\ 1.0 + \\frac{0.2 (1)}{2} \\right) \\nonumber \\\\ &amp;= f \\left(0.1,\\ 1.1 \\right)\\ \\ \\ \\leftarrow\\ \\ \\ \\ f(t_k, y_k) = y_k \\nonumber \\\\ &amp;=1.1 \\nonumber \\end{align}\\] Third, solve for \\(\\mathbf{k_3}\\): \\[\\begin{align} k_3 {}&amp;= f\\left(t_k + \\frac{\\Delta t}{2},\\ y_k + \\frac{\\Delta t k_2}{2}\\right) \\\\ &amp;= f\\left(0.0 + 0.1,\\ 1.0 + 0.1(1.1)\\right) \\nonumber \\\\ &amp;= f\\left(0.1, 1.11\\right)\\ \\ \\ \\leftarrow\\ \\ \\ \\ f(t_k, y_k) = y_k \\nonumber \\\\ &amp;=1.11 \\nonumber \\end{align}\\] Fourth, solve for \\(\\mathbf{k_4}\\): \\[\\begin{align} k_4 {}&amp;= f\\left(t_k + \\Delta t,\\ y_k + \\Delta t k_3\\right) \\\\ &amp;= f\\left(0.0 + 0.2,\\ 1.0 + 0.2 (1.11)\\right) \\nonumber \\\\ &amp;= f\\left(0.2, 1.222\\right)\\ \\ \\ \\leftarrow\\ \\ \\ \\ f(t_k, y_k) = y_k \\nonumber \\\\ &amp;= 1.222 \\nonumber \\end{align}\\] Fifth, solve for \\(\\mathbf{y_{k+1}}\\): \\[\\begin{align} y_{k+1} {}&amp;= y_k + \\frac{\\Delta t}{6}\\ ( k_1 + 2k_2 + 2k_3 + k_4) \\\\ &amp;= 1.0 + \\frac{0.2}{6}\\ ( 1.0 + 2(1.1) + 2(1.11) + 1.222) \\nonumber \\\\ &amp;= 1.0 + \\frac{0.2}{6}\\ (6.642) \\nonumber \\\\ &amp;= 1.2214 \\nonumber \\end{align}\\] Finally, repeat the process until we hit the target, substituting \\(\\mathbf{y_{k}}\\) with \\(\\mathbf{y_{k+1}}\\). See Table 4.10. In the table, \\(\\mathbf{y_{k+1}}\\) represents the approximate solution and \\(\\mathbf{e^{t+1}}\\) represents the exact target. Table 4.10: Runge-Kutta Table t \\(y_k\\) \\(k_1\\) \\(k_2\\) \\(k_=3\\) \\(k_4\\) \\(\\ y_{k+1}\\) \\(\\ e^{t+1}\\) Err 0.0 1.000 1.000 1.100 1.110 1.222 1.221 1.221 2.8e-6 0.2 1.221 1.221 1.344 1.356 1.493 1.492 1.492 6.7e-6 0.4 1.492 1.492 1.641 1.660 1.823 1.822 1.822 1.2e-5 0.6 1.822 1.822 2.004 2.023 2.227 2.226 2.226 2.0e-5 0.8 2.226 2.226 2.448 2.470 2.720 2.718 2.718 3.1e-5 \\(\\mathbf{1.0}\\) \\(\\mathbf{2.718}\\) – – – – – \\(\\mathbf{2.718}\\) \\(\\mathbf{3.1e-5}\\) As for applying a naive implementation of the Runge-Kutta method in R code, just set the type to rungekutta. # invoke ivp_methods target = 1.0 base_delta = 0.2 ivp = ivp_method(c(0,1), iteration=6, target = target, type=&quot;rungekutta&quot;) ivp_plot(ivp, base_delta, target, label=FALSE) Before we move on to Boundary Value Problems, we leave readers to investigate two enhancements to Runge-Kutta methods that use Adaptive Stepsize, which can yield errors in the order of \\(O(h^9)\\) or even \\(O(h^{10})\\). Dormand-Prince Method (1980) Bogacki-Shampine Method (1989) We also leave readers to investigate Multi-Step Methods such as: Adams-Bashforth Method Adams-Moulton Method 4.4.5 Shooting Method We have methods that deal with Initial Value Problems (IVP) in the previous sections. This section covers the discussion around Boundary Value Problems (BVP). Here, we shall see that we can reduce BVP into IVP to simplify the process. As a start, we use the Shooting method. To explain the method, we use Figure 4.17. Figure 4.17: Shooting Method As observed in the figure, our target is represented as a slope given by \\(y&#39;(b) = \\beta\\) at a horizontal distance \\(b\\). We first try to aim at the target by adjusting our angle in the form of a slope given by \\(y&#39;(a) = \\alpha_1\\). This first attempt to shoot an arrow forms a trajectory that eventually lands at \\(b\\) with an angle represented as a slope given by \\(y&#39;(b) = \\sigma_1\\). Similarly, we make a second attempt, shooting at an angle, \\(y&#39;(a) = \\alpha_2\\), and hits a slope, \\(y&#39;(b) = \\sigma_2\\). It does show that we miss the target twice. Ideally, if we shoot multiple attempts, we can use interpolation using the approximate targets to find one unique solution closer to the actual target. We explain that later in this section. Now, when it comes to boundaries, notice that there seems to be a boundary condition in place for both the starting point and ending point ( a Dirichlet type of boundary condition): \\[ a &lt; x &lt; b \\] There are a few common boundary conditions we can use depending on the situation. See Table 4.11. Table 4.11: Shooting Method Boundaries Boundary Conditions Starting Point (a) Ending Point (b) Neumann \\(y&#39;(a) = \\alpha\\) \\(y&#39;(b) = \\beta\\) Dirichlet \\(y(a) = \\alpha\\) \\(y(b) = \\beta\\) Cauchy \\(y(a) = \\alpha\\) \\(y&#39;(a) = \\beta\\) Robin \\(c_1 y(a) + c_2 y&#39;(a) = \\alpha\\) \\(c_1 y(b) + c_2 y&#39;(b) = \\beta\\) Periodic \\(y(a) = y(b)\\) \\(y&#39;(b) = y&#39;(b)\\) For our illustration, we use the Dirichlet boundary condition against a nonlinear second-order BVP equation: \\[\\begin{align} y&#39;&#39; = f(t, y, y&#39;)\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ a \\leq t \\leq b \\end{align}\\] with boundary conditions, \\[\\begin{align} y(a) = \\alpha,\\ \\ \\ \\ \\ \\ \\ \\ \\ y(b) = \\beta \\end{align}\\] One way to solve for two-point BVP is to resort back to any IVP methods; in particular, we arbitrarily choose the simple first-order forward Euler method. We know that IVP is satisfied with the following conditions (which is a Cauchy type of boundary condition): \\[ y(a) = \\alpha,\\ \\ \\ \\ \\ \\ y&#39;(a) = \\sigma,\\ \\ \\ \\ where\\ \\sigma =\\ &lt;guess&gt; \\] To use IVP method, we perform some mathematical manipulation against the following equation: \\[\\begin{align} y&#39;&#39;(t) = \\frac{d^2y}{dt^2}\\ \\ \\ \\rightarrow (y&#39;(t))&#39; = \\frac{d}{dt}\\left(\\frac{dy}{dt}\\right) \\end{align}\\] by extracting such first-order derivative from the second-order derivative of the below Leibnitz notation: \\[\\begin{align} \\frac{d^2y}{dt^2} = \\frac{d}{dt}\\left(\\frac{dy}{dt}\\right) \\ \\ \\ \\rightarrow\\ \\ \\ \\ (1)\\ \\ \\ \\ \\frac{dy}{dt} = x = f(x,y,t),\\ \\ \\ \\ (2)\\ \\ \\ \\ \\frac{dx}{dt} = g(x,y,t) \\end{align}\\] and then use Euler’s forward method formula (e.g. \\(y_{k+1} = y_k + \\Delta t\\ f\\)): \\[\\begin{align} (1)\\ \\ \\ \\ \\frac{dy}{dt} &amp;= f(x,y,t)\\ \\ \\rightarrow y_{k+1} = y_k + f(x_k, y_k, t_k) \\Delta t \\\\ (2)\\ \\ \\ \\ \\frac{dx}{dt} &amp;= g(x,y,t)\\ \\ \\rightarrow x_{k+1} = x_k + g(x_k, y_k, t_k) \\Delta t \\end{align}\\] So in general, solving BVP using shooting method given: \\[\\begin{align} \\frac{d^2y}{dt^2} = \\frac{dx}{dt} =g(x, y, t) \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ where\\ \\ \\ y(a) = \\alpha,\\ \\ \\ y(b) = \\beta \\end{align}\\] can be solved using IVP following the below equation and condition: \\[\\begin{align} \\frac{dy}{dt} = f(x,y,t) \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ where\\ \\ y(a) = \\alpha,\\ \\ \\ y&#39;(a) = \\sigma \\end{align}\\] Our goal then is to guess for: \\[\\begin{align} \\frac{dy}{dt}(0) = y&#39;(0) = x(0) = &lt;unknown&gt; \\end{align}\\] To illustrate, suppose we have the following: \\[ y_0(0) = 1,\\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39;&#39; = y, \\] and where our exact solution is \\(\\mathbf{y=e^t}\\). First, let us aim for our target. We know that aiming for the target can be a miss or a hit. We assume that our first attempt can be a miss (a close one but still a miss). Our approximate target (guess) is: \\[ y_0&#39;(0) = x_0 = 1.10 \\] Solve for \\(\\mathbf{y_1}\\): \\[\\begin{align} y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \\Delta t \\\\ y_1 &amp;= y_0 + f(x_0, y_0, t_0) \\Delta t \\\\ &amp;= 1 + f(1.100, 1.00, 0.00) 0.2 \\nonumber \\\\ &amp;= 1 + 1.100 * 0.2 \\ \\ \\leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \\nonumber \\\\ &amp;= 1.220 \\nonumber \\end{align}\\] Solve for \\(\\mathbf{x_1}\\): \\[\\begin{align} x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \\Delta t \\\\ x_1 &amp;= x_0 + g(x_0, y_0, t_0) \\Delta t \\\\ &amp;= 1.100 + g(1.100, 1.00, 0.00) 0.2 \\nonumber \\\\ &amp;= 1.100 + 1.00 * 0.2 \\ \\ \\leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \\nonumber \\\\ &amp;= 1.300 \\nonumber \\end{align}\\] Second, let us iterate one more time: Solve for \\(\\mathbf{y_2}\\): \\[\\begin{align} y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \\Delta t \\\\ y_2 &amp;= y_1 + f(x_1, y_1, t_1) \\Delta t \\\\ &amp;= 1.220 + f(1.300, 1.220, 0.20) 0.2 \\nonumber \\\\ &amp;= 1.220 + 1.300 * 0.2 \\ \\ \\leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \\nonumber \\\\ &amp;= 1.480 \\nonumber \\end{align}\\] Solve for \\(\\mathbf{x_2}\\): \\[\\begin{align} x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \\Delta t \\\\ x_2 &amp;= x_1 + g(x_1, y_1, t_1) \\Delta t \\\\ &amp;= 1.300 + g(1.300, 1.220, 0.20) 0.2 \\nonumber \\\\ &amp;= 1.300 + 1.220 * 0.2 \\ \\ \\leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \\nonumber \\\\ &amp;= 1.544 \\nonumber \\end{align}\\] We repeat a few times until we reach \\(y_k(1)\\). See Table 4.12. Table 4.12: Shooting Method Table t \\(y_k\\) \\(x_1\\) exact:\\(\\ e^{t+1}\\) Err 0.0 1.000 1.100 1.100 0.000 0.2 1.220 1.300 1.221 0.001 0.4 1.480 1.544 1.492 0.012 0.6 1.789 1.840 1.822 0.033 0.8 2.157 2.198 2.226 0.069 1.0 2.596 2.630 2.718 0.122 Here is a naive implementation of Shooting method in R code using a more general BVP method function and also, this time, using the Runge-Kutta method for IVP: f_true &lt;- function(x) { exp( x) } f_forward &lt;- function(xk, yk, tk) { xk } g_forward &lt;- function(xk, yk, tk) { yk } f_rungekutta &lt;- function(f, xk, yk, tk, stepsize) { k1 = f(xk, yk, tk) k2 = f(xk + stepsize / 2 * k1, yk + stepsize / 2 * k1, tk + stepsize / 2 ) k3 = f(xk + stepsize / 2 * k2, yk + stepsize / 2 * k2, tk + stepsize / 2) k4 = f(xk + stepsize * k3, yk + stepsize * k3, tk + stepsize) yk = yk + stepsize/6 * ( k1 + 2*k2 + 2*k3 + k4) } bvp_method &lt;- function(init_val, fun, iteration, target, type=&quot;shooting:rkutta&quot;) { t0 = init_val[1] y0 = yk = init_val[2] x0 = xk = guess = init_val[3] k = seq(0, target, length.out=iteration ) stepsize = k[2] - k[1] sequence = matrix(0, 0, 7) cnt = 0 for (tk in k) { if (type == &quot;shooting:euler&quot;) { yk = yk + stepsize * f_forward(x0, y0, tk) # euler forward xk = xk + stepsize * g_forward(x0, y0, tk) # euler forward } else if (type == &quot;shooting:rkutta&quot;) { yk = f_rungekutta(f_forward, x0, y0, tk, stepsize) xk = f_rungekutta(g_forward, x0, y0, tk, stepsize) } true_y = exp(tk ) err = abs( true_y - y0 ) cnt = cnt + 1 sequence = rbind(sequence, c(cnt, stepsize, tk, y0, x0, true_y, err)) y0 = yk x0 = xk } colnames(sequence) = c(&quot;iteration&quot;, &quot;stepsize&quot;, &quot;tk&quot;, &quot;yk&quot;, &quot;xk&quot;, &quot;y-exact&quot;, &quot;absolute error&quot;) list(&quot;Iteration&quot;= sequence, &quot;count&quot;=nrow(sequence), &quot;type&quot;=type, &quot;guess&quot;=guess ) } bvp = bvp_method(c(0,1, 1.1), iteration=6, target = target, type=&quot;shooting:rkutta&quot;) bvp ## $Iteration ## iteration stepsize tk yk xk y-exact absolute error ## [1,] 1 0.2 0.0 1.000000 1.100000 1.000000 0.00000000 ## [2,] 2 0.2 0.2 1.243540 1.221400 1.221403 0.02213724 ## [3,] 3 0.2 0.4 1.513958 1.518860 1.491825 0.02213326 ## [4,] 4 0.2 0.6 1.850234 1.849148 1.822119 0.02811471 ## [5,] 5 0.2 0.8 2.259635 2.259875 2.225541 0.03409400 ## [6,] 6 0.2 1.0 2.759971 2.759918 2.718282 0.04168948 ## ## $count ## [1] 6 ## ## $type ## [1] &quot;shooting:rkutta&quot; ## ## $guess ## [1] 1.1 bvp = bvp_method(c(0,1, 1.3), iteration=6, target = target, type=&quot;shooting:rkutta&quot;) bvp ## $Iteration ## iteration stepsize tk yk xk y-exact absolute error ## [1,] 1 0.2 0.0 1.000000 1.300000 1.000000 0.00000000 ## [2,] 2 0.2 0.2 1.287820 1.221400 1.221403 0.06641724 ## [3,] 3 0.2 0.4 1.558238 1.572943 1.491825 0.06641326 ## [4,] 4 0.2 0.6 1.906488 1.903232 1.822119 0.08436882 ## [5,] 5 0.2 0.8 2.327863 2.328584 2.225541 0.10232222 ## [6,] 6 0.2 1.0 2.843412 2.843252 2.718282 0.12512981 ## ## $count ## [1] 6 ## ## $type ## [1] &quot;shooting:rkutta&quot; ## ## $guess ## [1] 1.3 Instead of going for a third shot, let us perform linear interpolation using the results of the two attempts of shooting for the target (where superscripts indicate 1st and 2nd attempts) : \\[\\begin{align} y_0&#39;(0) &amp;= x_0^{(1)} + \\frac { x_0^{(2)} - x_0^{(1)}}{y_0^{(2)} - y_0^{(1)}}( y - y_0^{(1)}) \\\\ y_0&#39;(0) &amp;= 1.10 + \\frac{1.30 - 1.10}{2.843412 - 2.759971} (2.718282 - 2.759971) \\nonumber \\\\ y_0&#39;(0) &amp;= 1.00 \\nonumber \\end{align}\\] where \\(y = f(1) = 2.718282\\) which is our exact solution. We now go back and attempt shooting another arrow given a guess of \\(y&#39;(0) = 1.00\\). We use the interpolation result to become our next 3rd attempt to guess the target. \\[ y_0&#39;(0) = x_0^{(3)} = 1.00 \\] (bvp = bvp_method(c(0,1, 1.0), iteration=6, target = target, type=&quot;shooting:rkutta&quot;)) ## $Iteration ## iteration stepsize tk yk xk y-exact absolute error ## [1,] 1 0.2 0.0 1.000000 1.000000 1.000000 0.000000e+00 ## [2,] 2 0.2 0.2 1.221400 1.221400 1.221403 2.758160e-06 ## [3,] 3 0.2 0.4 1.491818 1.491818 1.491825 6.737641e-06 ## [4,] 4 0.2 0.6 1.822106 1.822106 1.822119 1.234405e-05 ## [5,] 5 0.2 0.8 2.225521 2.225521 2.225541 2.010271e-05 ## [6,] 6 0.2 1.0 2.718251 2.718251 2.718282 3.069185e-05 ## ## $count ## [1] 6 ## ## $type ## [1] &quot;shooting:rkutta&quot; ## ## $guess ## [1] 1 The solution we get out of our interpolated guess, \\(x_0^{(3)} = 1.0\\), ends up with the following: \\[ y_0 = 1.000,\\ \\ \\ \\ y_1 = 1.221,\\ \\ \\ \\ y_2 = 1.492,\\ \\ \\ \\ y_3 = 1.822,\\ \\ \\ \\ y_4 = 2.226,\\ \\ \\ \\ y_5 = 2.718 \\] If we graph the trajectory, we get the Figure 4.18. Figure 4.18: BVP Trajectory For ODE Note that the unique solution we seek is a set of \\(\\mathbf{y_k} = f(x_k)\\) with their corresponding slopes, \\(\\mathbf{x_k}\\), forming a trajectory that lands close to the target, \\(y(1) = 2.718\\). The next method we cover, Finite Difference, illustrates how we solve for Systems of Linear Equations to derive the solution for our BVP. 4.4.6 Finite Difference Method In this section, we solve BVPs using Finite Difference. Recall second-order Centered Finite Difference formula: \\[\\begin{align} f&#39;&#39;(x)\\ \\approx \\frac{f(x + h) - 2f(x) + f(x -h) }{h^2}\\ \\ \\ \\rightarrow\\ \\ \\ \\ y&#39;&#39; \\approx \\frac{ y_{k+1} -2y_k + y_{k-1}}{(\\Delta t)^2} \\end{align}\\] Recall also first-order Forward Finite Difference formula: \\[\\begin{align} f&#39;(x) \\approx \\frac{f(x+h) - f(x)}{h} \\ \\ \\ \\rightarrow\\ \\ \\ \\ y&#39; \\approx \\frac{ y_{k+1} -y_k}{\\Delta t} \\end{align}\\] To illustrate the use of those formulas, suppose we have the following: \\[ y_0(0) = 1,\\ \\ \\ \\ y_0(1) = 2.718, \\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39;&#39; = y, \\] and where our exact solution is \\(\\mathbf{y=e^t}\\). Here, because \\(y&#39;&#39; = f&#39;&#39;(x)\\), we simply substitute: \\[\\begin{align} y&#39;&#39; = y\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\frac{ y_{k+1} -2y_k + y_{k-1}}{(\\Delta t)^2} = y_k \\end{align}\\] Note that we are not substituting \\(y&#39;\\) for anything because our problem statement does not include the first derivative (e.g., \\(y&#39;&#39; = y\\) ). Now, let us simplify: \\[\\begin{align} \\frac{ y_{k+1} -2y_k + y_{k-1}}{(\\Delta t)^2} {}&amp;= y_k\\\\ \\nonumber \\\\ y_{k+1} - 2y_k + y_{k-1} &amp;= y_k(\\Delta t)^2 \\\\ y_{k+1} - 2y_k - y_k(\\Delta t)^2 + y_{k-1} &amp;= 0\\\\ y_{k+1} - (2 + (\\Delta t)^2)y_k + y_{k-1} &amp;= 0\\\\ y_{k-1} - (2 + (\\Delta t)^2)y_k + y_{k+1} &amp;= 0 \\end{align}\\] We now have an equation we can use to solve our BVP (note that this is a discretization of the domain of our general function, \\(\\mathbf{f(x)}\\)): \\[\\begin{align} y_{k-1} - (2 + (\\Delta t)^2)y_k + y_{k+1} = 0 \\end{align}\\] To be able to use the equation, let us review Figure 4.19 which illustrates our problem statement; where our Dirichlet boundary is \\(t \\in [a,b]\\). Notice that our general function, \\(f(x)\\), is discretized into nodes of individual functions, \\(f(x) \\in \\{f(0), f(0.2), ...,f(0.8), f(1)\\}\\). Figure 4.19: Discretized Nodes For ODE In the figure, we see six nodes of functions that we now can use to form our system of equations: \\[\\begin{align} f(0.0)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_0 = 1.0\\ \\ \\leftarrow boundary\\ a\\\\ f(0.2)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_0 - (2 + (\\Delta t)^2)y_1 + y_2 = 0 \\\\ f(0.4)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_1 - (2 + (\\Delta t)^2)y_2 + y_3 = 0 \\\\ f(0.6)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_2 - (2 + (\\Delta t)^2)y_3 + y_4 = 0 \\\\ f(0.8)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_3 - (2 + (\\Delta t)^2)y_4 + y_5 = 0 \\\\ f(1.0)\\ \\ \\ &amp;\\rightarrow\\ \\ \\ \\ y_5 = 2.718\\ \\ \\leftarrow boundary\\ b \\end{align}\\] and then to translate into tridiagonal matrix form where: \\[ -(2 + (\\Delta t)^2) = -2.04. \\] We have: \\[ \\left[ \\begin{array}{rrrrrr} 1 &amp; . &amp; . &amp; . &amp; . &amp; . \\\\ 1 &amp; -2.04 &amp; 1 &amp; . &amp; . &amp; . \\\\ . &amp; 1 &amp; -2.04 &amp; 1 &amp; . &amp; . \\\\ . &amp; . &amp; 1 &amp; -2.04 &amp; 1 &amp; . \\\\ . &amp; . &amp; . &amp; 1 &amp; -2.04 &amp; 1 \\\\ . &amp; . &amp; . &amp; . &amp; . &amp; 1 \\\\ \\end{array} \\right] \\left[\\begin{array}{r}y_0 \\\\ y_1 \\\\ y_2 \\\\ y_3 \\\\ y_4 \\\\ y_5 \\end{array}\\right] = \\left[\\begin{array}{r}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 2.718 \\end{array}\\right] \\] We can then use any numerical methods (e.g., Newton, Broyden, etc.) discussed in Chapter 3 (Numerical Linear Algebra II) to solve for Systems of Linear Equations, forming a domain with the following set of y values. \\[ y_0 = 1.000,\\ \\ \\ \\ y_1 = 1.222,\\ \\ \\ \\ y_2 = 1.492,\\ \\ \\ \\ y_3 = 1.823,\\ \\ \\ \\ y_4 = 2.226,\\ \\ \\ \\ y_5 = 2.718 \\] And if we graph the trajectory of the approximate solution, it will show a geometric match for the Shooting method shown in Figure 4.18. Note that the figure shows the trajectory of our exact solution itself, e.g. \\(\\mathbf{y = e^x}\\). 4.4.7 Finite Element Method (based on WRM and VM) Finite Element method has been used across many fields, from mechanical and aerospace engineering to material science. It is also used in structural (solid) mechanics, fluid (liquid and gas) mechanics, quantum mechanics, and other fields of physics. Computer simulations assist in Finite Element analysis to understand such dynamic systems (Burden R.L. et al. 2005). We leave readers to investigate a few simulation software starting with Ansys, OpenFOAM, and SimScale. In structural engineering, construction materials are gauged based on strain when stress is applied or displacement when force is applied. Similar to piecewise polynomial interpolation, a well-known approach to determine the overall state of the entire material is to first partition the material into pieces (or elements) and compute the local state of each element; hence, finite element. And then aggregate the individual (local) states to form the overall (global) state. There are two common ways to partition the material: rectangular or triangular. See Figure 4.20. Note that the figure shows the most rudimentary way of generating the mesh. Mesh generation can be done precisely by computers today using graphics software following Delaunay Refinement algorithm for Triangular Mesh and adjusting vertices following Ruppert’s algorithm. Figure 4.20: Finite Element Mesh In this section, instead of discussing complex structures involving constructing a complex mesh of elements, let us first get the intuition of FEM using a simple one-dimension two-point BVP. Then, we use a stencil to show that. Discussion about complex structures is left to readers pursuing structural or mechanical engineering or material science. Moreover, in other literature, FEM offers two approaches, namely Weighted Residual Method (WRM) and Variational Method (VM). In this section, we only focus on WRM. Similar to Finite Difference Methods (FDM), to solve BVPs using Finite Element methods (FEM), we discretize the domain of the function into equally-spaced intervals forming a set of discretized nodes - each node representing a function. The difference, however, is that in FEM, we build a linear combination (as an approximate solution) constructed from a standard series or a polynomial of choice associated with a set of discretized nodes. These nodes are considered basis functions. These basis functions denoted as \\(\\phi(t)\\) form an approximation to the solution. The approximate solution is a weighted solution using the following choice of polynomials or series for its basis functions: Polynomial Interpolation (e.g. Lagrange, Newton, B-spline, Legendre, Chebyshev, …) Fourier Series (e.g. Sinusoidal, …) Wavelets (e.g. Harmonic, …) In here, we use the generalized Taylor series for our trial function, \\(y(x)\\). \\[\\begin{align} y(x) = \\sum_{n=0}^{\\infty} c_n x^n = c_0 + c_1x + c_2 x^2 + c_3 x^3 +\\ . . . \\ \\ \\ \\ where\\ c_n = \\frac{f(n)(a)}{n!} \\end{align}\\] One may prefer to use Sinusoidal series instead (depending on study or use): \\[\\begin{align} y(x) = \\sum_{n=1}^{\\infty} c_n sin\\frac{n\\pi x}{L} = c_1 sin \\frac{\\pi x}{L} + c_2 sin \\frac{2\\pi x}{L} + c_3 sin \\frac{3\\pi x}{L} +\\ . . . \\end{align}\\] Note that \\(c_i\\) are the unknown coefficients that we need to compute. From physical connotation, the number of unknown coefficients is based on the characteristic of the material. In other words, the number of properties of each element determines the order of the polynomial we use. For example, we may account for stress, strain, and displacement in solid dynamics. In this case, we may need a cubic polynomial for each element of the elastic material. And in fluid dynamics, we may account for viscosity, temperature, density, pressure, and velocity. As for simple illustrations, we narrow down our case to a one-dimension two-point BVP covering a few Weighted Residual approaches under Finite Element methods and using the usual problem statement below: \\[ y_0(0) = 1,\\ \\ \\ \\ y_0(1) = 2.718, \\ \\ \\ \\ where \\ \\ \\ \\ \\ \\Delta t = 0.2,\\ \\ \\ \\ y&#39;&#39; = y, \\] and just as the same, we use the below exact solution as a baseline to validate our choice of trial function - this is our governing equation: \\[ y = e^t\\ \\ \\ \\ \\ \\leftarrow \\text{exact function as our baseline} \\] Note that governing equations are used for the more practical cases for Finite Element analysis. One governing equation is the Navier-Stokes equation for Fluid Dynamics. We base this equation on the law of conservation (e.g., mass, momentum, energy) around the physical properties of fluids such as viscosity, density, pressure, temperature, and velocity. In our case, we use the most simple governing equation to illustrate the fundamental idea of the Finite Element method and may not necessarily reflect any physical phenomenon other than to show our approximate geometric representation of the actual equation: \\[ y = e^t \\] Our first step is to generate a set of trial functions using the Taylor series as our choice. Given the Dirichlet boundary condition above, it shows that we have six nodes based on \\(\\Delta t = 0.2\\), which is just our way of illustrating the partition of a global domain (the geometric curve) into five sub-domains (or elements). Here is the stencil for that which is shown in Figure 4.21: Figure 4.21: Stencil for Trial Function For our trial function, we use a simple cubic polynomial - a truncated taylor series - for each element: \\[\\begin{align} y(x) = c_0 + c_1x + c_2 x^2 + c_3 x^3 \\end{align}\\] One may choose other polynomials as basis (trial) function as long as it satisfies three conditions: it satisfies the essential boundary conditions it is continuous and as such, if it is continuous, it is differentiable in which the square of its derivative is integrable and bounded. One point to make is that the basis function is regarded as a weighted function. In variational formulation, where we require to solve for the roots of the equation (the properties of the elements), we relax the third condition above, which means that we do not require a second-order derivation of continuity - which makes it, therefore, a weak formulation. Otherwise, the weighted function requires it if we do not reduce (meaning, we weaken the requirement of) the second-order of continuity (of the approximate solution) using integration (but not by parts) and thus is considered a strong formulation. In the next few sections, we focus on Weighted Residual methods (WRM) to solve for Bounded Value Problems. We start with the Least-Square Method. 4.4.8 Least-Square Method (using WRM) Let us introduce a WRM under FEM called the Least-Square method. Foremost, note that WRM is an integral approach that integrates weighted residuals of trial functions to form an approximation function which is a weaker formulation of the actual function (Salih A., n.d.; Mohammed A. S. et al. 2021). To illustrate, let us form a system of equations starting with the basis function, which we derived from Taylor Series as pointed out previously: \\[\\begin{align} y(x) = c_0 + c_1x + c_2 x^2 + c_3 x^3 \\end{align}\\] Our first goal is to find the values of the unknown coefficients that minimize \\(y(x)\\), which we require to fulfill our ultimate goal of formulating the approximation function that we can use to determine the values of each element in the domain - the curve. First, for the first boundary, a, where \\(\\mathbf{y(0) = 1}\\), we compute for \\(\\mathbf{c_0}\\): \\[\\begin{align*} y(x) {}&amp;= c_0 + c_1x + c_2 x^2 + c_3 x^3 \\\\ y(0) &amp;= c_0 + c_1 (0) + c_2 (0)^2 + c_3 (0)^3 \\\\ y(0) &amp;= c_0 \\\\ \\therefore c_0 &amp;= 1 \\end{align*}\\] Second, for the second boundary, b, where \\(\\mathbf{y(1) = 2.718}\\), we compute for \\(\\mathbf{c_1}\\) (note that we also substitute \\(c_0\\) in the equation): \\[\\begin{align*} y(x) {}&amp;= c_0 + c_1x + c_2 x^2 + c_3 x^3\\\\ y(1) &amp;= 1 + c_1 (1) + c_2 (1)^2 + c_3 (1)^3\\\\ y(1) &amp;= 1 + c_1 + c_2 + c_3 \\\\ c_1 &amp;= y(1) - 1 - c_2 - c_3 \\\\ c_1 &amp;= 2.718 - 1 - c_2 - c_3 \\\\ \\therefore c_1 &amp;= 1.718 - c_2 - c_3 \\end{align*}\\] Third, let us simplify our trial function: \\[\\begin{align*} y(x) {}&amp;= c_0 + c_1x + c_2 x^2 + c_3 x^3 \\\\ y(x) &amp;= 1 + (1.718 - c_2 - c_3)x + c_2 x^2 + c_3 x^3 \\\\ y(x) &amp;= 1 + 1.718x - c_2x - c_3x + c_2 x^2 + c_3 x^3 \\\\ y(x) &amp;= 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) \\\\ \\end{align*}\\] Fourth, let us get the first-order and-second order of the simplified trial function: \\[\\begin{align*} y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\\\ y&#39;&#39;(x) &amp;= 2c_2 + 6c_3x \\\\ \\end{align*}\\] Fifth, let us now plug our derivatives and trial functions into our problem statement to get the Residual equation: \\[\\begin{align*} y&#39;&#39; = y\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ R = y&#39;&#39; - y {}&amp;= 0\\\\ \\\\ 2c_2 + 6c_3x - ( 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x)) &amp;= 0 \\\\ 2c_2 + 6c_3x - 1 - 1.718x - c_2(x^2 - x) - c_3(x^3 - x) &amp;= 0 \\\\ 2c_2 + 6c_3x - 1 - 1.718x - c_2 x^2 + c_2 x - c_3x^3 + c_3 x &amp;= 0 \\\\ c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1 &amp; = 0 \\\\ \\\\ \\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1 &amp;= 0 \\end{align*}\\] Sixth, given the following integral form of our weighted residual, let us compute for other equations (this is the core of WRM): \\[\\begin{align} \\int_{i=0}^{n} W_i R\\ dx = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{cases} W_1 = 2 + x - x^2 \\\\ W_2 = 6x - x^3 + x \\end{cases} \\label{eqn:eqnnumber12} \\end{align}\\] For \\(\\mathbf{W_1}\\): \\[ \\int_0^1 (2 + x - x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 4.7c_2+7.05c_3-4.0278 = 0 \\] For \\(\\mathbf{W_2}\\): \\[ \\int_0^1 (6x - x^3 + x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 7.050c_2+13.676 c_3-6.9150 = 0 \\] Based on the four equations we generated, we form our matrix of system of equations: \\[ \\begin{array}{r} c_0 = 1.0000\\\\ 4.7c_2+7.05c_3 = 4.0278 \\\\ 7.05c_2+13.676 c_3 = 6.9150 \\\\ c_1 + c_2 + c_3 = 1.7182 \\end{array}\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\left[ \\begin{array}{rrrr} 1 &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; 4.700 &amp; 7.050 \\\\ . &amp; . &amp; 7.050 &amp; 13.676 \\\\ . &amp; 1 &amp; 1 &amp; 1 \\end{array} \\right] \\left[\\begin{array}{r} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{array}\\right] = \\left[\\begin{array}{r} 1.0000 \\\\ 4.0278 \\\\ 6.9150 \\\\ 1.7182 \\end{array}\\right] \\] Therefore, our unknown coefficients are: \\[ c_0 = 1\\ \\ \\ \\ \\ c_1 = 1.002031 \\ \\ \\ \\ \\ c_2 = 0.4345505 \\ \\ \\ \\ \\ c_3 = 0.2816188 \\] Giving us the weak formulation of our approximate solution ( note that we chose cubic polynomial): \\[ y(x) = 1 + 1.002031 x + 0.4345505 x^2 + 0.2816188 x^3 \\] With that, we get a geometric match of the exact solution, \\(y=e^x\\), in Figure 4.22: Figure 4.22: Finite Element Method (WRM) Also, in terms of error, we get the following reasonable result: f_exact &lt;- function(x) { exp(x) } f_approx &lt;- function(x) { 1 + 1.002031 * x + 0.4345505 * x^2 + 0.2816188 * x^3 } n = 6 x = seq(0,1, length.out=n) y_exact = f_exact(x) y_approx = f_approx(x) err = y_exact - y_approx m = cbind( y_exact, cbind( y_approx, err)) colnames(m) = c(&quot;Exact&quot;, &quot;Approximate&quot;, &quot;Error&quot; ) knitr::kable( m, caption = &#39;Least-Square Method&#39;, booktabs = TRUE, escape=FALSE) Table 4.13: Least-Square Method Exact Approximate Error 1.000000 1.000000 0.0000000 1.221403 1.220041 0.0013616 1.491825 1.488364 0.0034606 1.822119 1.818486 0.0036324 2.225541 2.223926 0.0016150 2.718282 2.718200 0.0000815 4.4.9 Galerkin Method (using WRM) Let us introduce another WRM under FEM called the Galerkin method (Salih A., n.d.; Mohammed A. S. et al. 2021). Similar to the Least Square method, we use the same trial function as below (note that we can use other polynomials instead): \\[ y(x) = c_0 + c_1x + c_2 x^2 + c_3 x^3 \\] with its simplified form as illustrated in the Least-Square method: \\[ y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) \\] Now, the Weighing function is different in the Sixth step in the Least Square method for the Galerkin method. The difference this time is that we use the simplified form to extract our weighing functions: So given the following integral form of our weighted residual, let us compute for other equations: \\[ \\int_{i=0}^{n} W_i R\\ dx = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{cases} W_1 = x^2 - x \\\\ W_2 = x^3 - x \\end{cases} \\] where \\(\\mathbf{R}\\) remains to be (the same as Least Squares method): \\[\\begin{align} \\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1 \\label{eqn:eqnnumber13} \\end{align}\\] So that, for \\(\\mathbf{W_1}\\): \\[ \\int_0^1 (x^2 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 0.3098-0.3667c_2-0.5500c_3 = 0 \\] And for \\(\\mathbf{W_2}\\): \\[ \\int_0^1 (x^3 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 0.4791-0.5500c_2-0.8762c_3 = 0 \\] Based on the four equations we generated, we form our matrix of system of equations: \\[ \\begin{array}{r} c_0 = 1.0000\\\\ 0.3667c_2+0.5500c_3 = 0.3098 \\\\ 0.5500c_2+0.8762 c_3 = 0.4791 \\\\ c_1 + c_2 + c_3 = 1.7182 \\end{array}\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\left[ \\begin{array}{rrrr} 1 &amp; . &amp; . &amp; . \\\\ . &amp; . &amp; 0.3667 &amp; 0.5500 \\\\ . &amp; . &amp; 0.5500 &amp; 0.8762 \\\\ . &amp; 1 &amp; 1 &amp; 1 \\end{array} \\right] \\left[\\begin{array}{r} c_0 \\\\ c_1 \\\\ c_2 \\\\ c_3 \\end{array}\\right] = \\left[\\begin{array}{r} 1.0000 \\\\ 0.3098 \\\\ 0.4791 \\\\ 1.7182 \\end{array}\\right] \\] Therefore, our unknown coefficients are: \\[ c_0 = 1\\ \\ \\ \\ \\ c_1 = 1.014161 \\ \\ \\ \\ \\ c_2 = 0.422377 \\ \\ \\ \\ \\ c_3 = 0.2816625 \\] and our complete approximate solution becomes: \\[ y(x) = 1 + 1.014161x + 0.422377 x^2 + 0.2816625 x^3 \\] Also, in terms of error, we get the following reasonable result: f_exact &lt;- function(x) { exp(x) } f_approx &lt;- function(x) { 1 + 1.014161 * x + 0.422377 * x^2 + 0.2816625 * x^3 } n = 6 x = seq(0,1, length.out=n) y_exact = f_exact(x) y_approx = f_approx(x) err = y_exact - y_approx m = cbind( y_exact, cbind( y_approx, err)) colnames(m) = c(&quot;Exact&quot;, &quot;Approximate&quot;, &quot;Error&quot; ) knitr::kable( m, caption = &#39;Galerkin Method&#39;, booktabs = TRUE, escape=FALSE) Table 4.14: Galerkin Method Exact Approximate Error 1.000000 1.000000 0.0000000 1.221403 1.221981 -0.0005778 1.491825 1.491271 0.0005536 1.822119 1.821391 0.0007274 2.225541 2.225861 -0.0003204 2.718282 2.718201 0.0000813 4.4.10 Petrov-Galerkin Method (using WRM) In the Petrov-Galerkin method, our Weighing functions are such that given the following integral form of our weighted residual, we just compute for the equations using x and raise the order until we meet the system of equations we require: \\[\\begin{align} \\int_{i=0}^{n} W_i R\\ dx = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{cases} W_1 = x \\\\ W_2 = x^2 \\end{cases}\\ or \\begin{cases} W_1 = x^2 \\\\ W_2 = x^3 \\end{cases} \\label{eqn:eqnnumber14} \\end{align}\\] For \\(\\mathbf{W_1}\\): \\[ \\int_0^1 (x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 3.25c_2+6.4c_3-3.218 = 0 \\] For \\(\\mathbf{W_2}\\): \\[ \\int_0^1 (x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 2.15c_2+4.75c_3-2.2885 = 0 \\] Our complete approximate solution becomes: \\[ y(x) = 1 + 1.027872 x + 0.3093443 x^2 + 0.3809836 x^3 \\] 4.4.11 Rayleigh-Ritz Method (using WRM) Finally, we introduce the Rayleigh-Ritz method using WRM under FEM (Papadopoulos P. 2015). Using the same approach as Galerkin, we use the same trial function as before: \\[ y(x) = c_0 + c_1x + c_2 x^2 + c_3 x^3 \\] with its simplified form as illustrated in the Least Square: \\[ y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) \\] and with its first-order derivative: \\[\\begin{align*} y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\\\ \\end{align*}\\] Now, similar to Galerkin method, we use the simplified form to extract our weighing functions: \\[\\begin{align} \\int_{i=0}^{n} W_i R\\ dx = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{cases} W_1 = x^2 - x,\\ \\ \\ W&#39;_1 = 2x - 1 \\\\ W_2 = x^3 - x,\\ \\ \\ W&#39;_2 = 3x^2 - 1 \\\\ \\end{cases} \\label{eqn:eqnnumber8} \\end{align}\\] where this time, we use integration by parts starting with \\(\\mathbf{R}\\): \\[\\begin{align} R = y&#39;&#39; - y = 0\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ R = \\frac{d^2y}{dx^2} - \\frac{dy}{dx} = 0 \\end{align}\\] So then, that brings our integral equation into the following form: \\[\\begin{align} \\int_{i=0}^{n} W_i R\\ dx = 0 \\ \\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ \\ \\int_{i=0}^{n} W_i \\left(\\frac{d^2y}{dx^2} - \\frac{dy}{dx}\\right)\\ dx = 0 \\end{align}\\] And by integration by parts: \\[\\begin{align} \\int_{i=0}^{n} W_i \\left(\\frac{d^2y}{dx^2}\\right) dx - \\int_{i=0}^{n} W_i \\left(\\frac{dy}{dx}\\right)\\ dx = 0 \\end{align}\\] We continue solving for each part: \\[\\begin{align} \\left[w\\left(\\frac{dy}{dx}\\right)\\right]_0^1 - \\int_{0}^{1} \\left(\\frac{dw}{dx}\\right)\\left(\\frac{dy}{dx}\\right) dx - \\int_{0}^{1} W_i \\left(\\frac{dy}{dx}\\right)\\ dx = 0 \\end{align}\\] giving us the final integral form: \\[ - \\int_{0}^{1} \\left(\\frac{dw}{dx}\\right)\\left(\\frac{dy}{dx}\\right) dx - \\int_{0}^{1} W_i \\left(\\frac{dy}{dx}\\right)\\ dx = 0, \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\left[w\\left(\\frac{dy}{dx}\\right)\\right]_0^1 is\\ cancelled \\] So that, for \\(\\mathbf{W_1}\\) and \\(\\mathbf{W_1&#39;}\\): \\[\\begin{align*} {}&amp;- \\int_{0}^{1} (2x-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\\\ &amp;\\ \\ \\ - \\int_{0}^{1} (x^2-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0 \\end{align*}\\] we get the following equation: \\[ -0.3333 c_2-0.5167 c_3+0.2863 = 0 \\] And for \\(\\mathbf{W_2}\\) and \\(\\mathbf{W_2&#39;}\\): \\[\\begin{align*} {}&amp;- \\int_{0}^{1} (3x^2-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\\\ &amp;\\ \\ \\ - \\int_{0}^{1} (x^3-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0 \\end{align*}\\] we get the following equation: \\[ 0.4295-0.48333 c_2-0.8c_3 = 0 \\] That gives us the following approximate solution: \\[ y(x) = 1 + 0.835589 x + 0.8733474 x^2 + 0.0092640 x^3 \\] 4.4.12 Subdomain Method (using subdomains) Subdomain Method does not require the Weighing function; however, it uses integration to subdivide the Residual equation (See Equation \\(\\ref{eqn:eqnnumber13}\\)) into subdomains, generating the required equations (Papadopoulos P. 2015; Salih A., n.d.). One may use as many subdomains as there are to fit a domain; however, because we only need a number of equations sufficient enough to form a system of equations, we only need two more equations in our case. \\[\\begin{align} \\int_a^b R\\ dx = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\begin{cases} S_1 \\rightarrow a = 0.0, b = 0.5 \\\\ S_2 \\rightarrow a = 0.5, b = 1.0 \\end{cases} \\label{eqn:eqnnumber9} \\end{align}\\] For \\(\\mathbf{S_1}\\): \\[ \\int_0^{0.5} ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 1.08334c_2+0.859375c_3-0.71475 = 0 \\] For \\(\\mathbf{S_2}\\): \\[ \\int_{0.5}^{1} ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0 \\] we get the following equation: \\[ 1.08333c_2+2.390625c_3-1.14425 = 0 \\] That gives us the following approximate solution: \\[ y(x) = 1 + 1.000445 x + 0.4372653 x^2 + 0.2804898 x^3 \\] 4.4.13 Collocation Method (using direct location points) Collocation Method does not require the Weighing function and the integration (Salih A., n.d.; Mohammed A. S. et al. 2021). Instead, we plug the location directly into the Residual equation: \\[ R = c_2 ( 2 - x_i^2 + x_i) + c_3 ( 6x_i - x_i^3 + x_i) - 1.718x_i - 1 = 0\\ \\ \\ \\ \\ where\\ \\ \\ x \\in \\{ 0.2, 0.4, 0.6, 0.8 \\} \\] Note that we need only two equations to complete the system of equations for the matrix and so we can just randomly choose two from the set of x. Here, we choose \\(x \\in \\{0.2, .06\\}\\): For \\(\\mathbf{x_i = 0.2}\\): \\[ R = c_2 ( 2 - (0.2)^2 + (0.2)) + c_3 ( 6(0.2) - (0.2)^3 + (0.2)) - 1.718(0.2) - 1 = 0 \\] we get the following equation: \\[ 2.16c_2+1.392c_3-1.3436 = 0 \\] For \\(\\mathbf{x_i = 0.6}\\): \\[ R = c_2 ( 2 - (0.6)^2 + (0.6)) + c_3 ( 6(0.6) - (0.6)^3 + (0.6)) - 1.718(0.6) - 1 = 0 \\] we get the following equation: \\[ 2.24c_2+3.984c_3-2.0308 = 0 \\] That gives us the following approximate solution: \\[ y(x) = 1 + 1.006949 x + 0.4603359 x^2 + 0.2509156 x^3 \\] 4.4.14 Weighted Residual Summary In summary, comparing the different Weighted Residual methods vs. Subdomain and Collocation, we get the following: Table 4.15: Weighted Residual Method \\(c_0\\) \\(c_1\\) \\(c_2\\) \\(c_3\\) y abs error Least Square 1 1.002031 0.4345505 0.2816188 2.7182 3e-07 Galerkin 1 1.014161 0.4223770 0.2816625 2.7182 5e-07 Petrov-Galerkin 1 1.027872 0.3093443 0.3809836 2.7182 1e-07 Rayleigh-Ritz 1 0.835589 0.8733474 0.0092640 2.7182 4e-07 Subdomain 1 1.000445 0.4372653 0.2804898 2.7182 1e-07 Collocation 1 1.006949 0.4603359 0.2509156 2.7182 5e-07 4.5 Approximation using Functional Differential Equations Before we cover Partial Differential Equations, it helps introduce the concept of Calculus of Variations. We start with a discussion around Variational functions. 4.5.1 Variational Functions The idea is to operate on functions instead of variables. Suppose we have the following function that accepts two variables: x = 5 and y = 5. \\[\\begin{align} f(x, y) = x + y = 10 \\end{align}\\] The function gives us a fixed outcome of 10. Depending on the problem statement, we can use a Functional denoted by \\(\\mathbf{F}[f(.)] \\equiv \\mathbf{F}[u(.)]\\) that accepts an arbitrary function (or a set of arbitrary functions), namely u, instead of a variable (or set of variables). See the example below: \\[ F\\left[u(x,y)\\right] = \\begin{cases} 10 &amp; u(x,y) = x + y \\\\ 20 &amp; u(x,y) = 2(x + y) \\\\ 50 &amp; u(x,y) = x^2 + y^2 \\\\ 100 &amp; u(x,y) = (x + y)^2 \\\\ \\end{cases}\\ \\ \\ \\ F\\left[u&#39;(x,y)\\right] = \\begin{cases} 1 &amp; u&#39;(x,y) = 1 \\\\ 2 &amp; u&#39;(x,y) = 2 \\\\ 10 &amp; u&#39;(x,y) = 2x \\\\ 20 &amp; u&#39;(x,y) = 2(x + y) \\\\ \\end{cases} \\] where: \\[\\begin{align*} {}&amp;F(u)\\ \\rightarrow \\text{(passing function u)}\\\\ &amp;F(u&#39;)\\ \\rightarrow \\text{(passing 1st derivative of function u)}\\\\ &amp;F(u&#39;&#39;)\\ \\rightarrow \\text{(passing 2nd derivative of function u)} \\end{align*}\\] One popular equation that calculates for the length of an arc uses a functional like so: \\[ S[u(x)] = \\int_a^b \\sqrt{1+(u&#39;)^2} dx \\] For example, find the length of an arc given \\(u = u(x) = x^{\\frac{3}{2}}\\) and \\(u&#39; = u&#39;(x) = \\frac{3x^{\\frac{1}{2}}}{2}\\). Our equation becomes: \\[ S[u(x)] = \\int_0^1 \\sqrt{1+\\left(\\frac{3x^{\\frac{1}{2}}}{2}\\right)^2} dx \\] Here is a sample R code: u.deriv = deriv(expression(x^(3/2)), &quot;x&quot;, func=TRUE) u &lt;- function(x) { sqrt(1 + u.deriv(x)^2) } S.functional &lt;- function(u) { integrate(u, 0, 1) } S.functional(u) ## 1.111448 with absolute error &lt; 1.2e-14 Euler-Lagrange Equation is a popular functional derivative equation. We leave readers to investigate the derivation: \\[\\begin{align} \\frac{\\partial F}{\\partial y} = \\frac{d}{dx}\\left(\\frac{\\partial F}{\\partial y&#39;}\\right) \\end{align}\\] 4.5.2 Variational Methods In the previous sub-section, we summarize the different Weighted Residual methods along with the choice of polynomials as basis functions to solve for ODE. Variational method is an alternative FEM method to solve for ODE. In our case, we briefly introduce the concept of configuration space using a string such as illustrated in Figure 4.23. In the figure, the string is displaced in the shape of half of a sine function. We focus on six points in the displaced string. Each point corresponds to functions {f(0), f(0.2), f(0.4), f(0.6), f(0.8), f(1)}. There are three configurations (or unique paths) formed by the string. We can assume that the three different displacements or configurations are consecutive at three-time intervals caused by an initial perturbation into the system. Figure 4.23: Finite Element Method (Variational) This book does not cover classical mechanics or even quantum mechanics. However, it also helps investigate different principles and variational equations governing stress levels of bendable or stretchable materials. Tall building structures, cranes, pulleys, and bridges exemplify how those principles and equations are applied. As shown in Figure 4.23, a simple bendable (or stretchable) string already follows three principles: Hamiltonian Principle is expressed as the sum of kinetic energy (T) and potential energy (V). \\[\\begin{align} H = T + V \\end{align}\\] The Hamiltonian principle also covers functional and extremitizing functions in which we minimize or maximize a function. For example, in FEM, we look for the extremum of each function (minimum or maximum points) to determine the subsequent displacement of a string at each marching time. Lagrange’s equation is expressed as the difference between the kinetic energy (T) and potential energy (V). \\[\\begin{align} L = T - V \\end{align}\\] D’Alembert’s Principle expresses an alternate form of Newton’s second law in which force (F) minus mass (m) and acceleration (a) equals zero. \\[\\begin{align} F - ma = 0 \\end{align}\\] 4.6 Approximation using Partial Differential Equations Partial Differential Equations (PDE) deal with more than one input (independent variables). The intuition becomes more apparent as we go through three common PDE equations: Heat, Wave, and Laplace. In the previous sections, we introduce Finite Element Methods (FEM) using several Weighted Residual methods (WRM) to solve for ODE and briefly cover Variational methods. In this section, we use Finite Difference Methods (FDM) to solve for PDE, although we can use FEM also. ### The Poisson Equation Many dynamic systems are governed by PDEs. For example, an equation used to describe the dynamics of newton’s gravity is expressed as such: \\[\\begin{align} \\nabla \\cdot \\mathbf{g} = -4 \\pi G \\rho \\end{align}\\] A general equation called Poisson’s general equation governs such well-studied dynamic systems. \\[\\begin{align} \\Delta u = f \\end{align}\\] where u can be numerically replaced with an approximating function (rather than an analytical function such as \\(\\mathbf{g} = -\\nabla \\phi\\)). The \\(\\Delta\\) is the Laplace operator and the notation \\(\\Delta u\\) is represented in different forms: \\[\\begin{align} \\Delta u = \\nabla^2 u = \\nabla \\cdot \\nabla u\\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ \\ \\ \\nabla = \\left(\\frac{\\partial}{\\partial_{x1}}, \\frac{\\partial}{\\partial_{x2}},\\ ...\\ \\frac{\\partial}{\\partial_{xn}} \\right) \\end{align}\\] In a one-dimension system, we see notations of the equation such as: \\[\\begin{align} \\Delta u(x) = f(x) \\ \\ \\ \\ {}&amp;\\rightarrow \\ \\ \\ \\ \\left( \\frac{\\partial^2}{\\partial x^2} \\right)u(x) = f(x)\\\\ \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ \\frac{\\partial^2u}{\\partial x^2} = f\\\\ \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ u_{xx} = f \\end{align}\\] In a multi-dimension system, we see notations of the equation such as: \\[\\begin{align} \\Delta u(x, y,\\ ...) = f(x,y,\\ ...) \\ \\ \\ \\ {}&amp;\\rightarrow \\ \\ \\ \\ \\left( \\frac{\\partial^2}{\\partial x^2} + \\frac{\\partial^2}{\\partial y^2} +\\ ...\\right)u(x,y,\\ ...) = f(x,y,\\ ...)\\\\ \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ \\frac{\\partial^2u}{\\partial x^2} + \\frac{\\partial^2u}{\\partial y^2} +\\ ... = f \\\\ \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ u_{xx}\\ +\\ u_{yy} + ... = f \\end{align}\\] Notice the change of notation for simplicity: \\[\\begin{align} u_x \\rightarrow \\frac{\\partial u}{\\partial x} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ u_{xx} \\rightarrow \\frac{\\partial^2 u}{\\partial x^2} \\end{align}\\] A two-dimensional system governed by a general Poisson equation becomes: \\[\\begin{align} \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = f \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{xx}\\ +\\ u_{yy} = f \\end{align}\\] There are dynamic systems that have unknown functions \\(\\mathbf{u(x,...)}\\). Others have known (analytical) functions but are computationally impractical. Our goal is to use numerical methods that provide approximate solutions. In subsequent sections, we use the Finite Difference methods to perform PDE approximation to the solution of the three equations we mentioned. Note that in PDE, our solution definition is a function solution rather than a value solution, which means that we are looking for a function that can best describe a domain (e.g., See Figure 4.24). 4.6.1 The Laplace Equation (Elliptic PDE) If a particular dynamic system reaches the steady-state or equilibrium, e.g. no heat, then we have the following Laplace equation: \\[\\begin{align} \\Delta u = 0 \\end{align}\\] The intuition behind the Laplace equation can be explained with heat diffusion. In other words, we can interpret the notation \\(\\Delta u\\) in the equation as the gradient divergence where the gradient (heat) vector field diverges outward or inward. Consider the lower domain (heat segment) in Figure 4.24 which closely characterizes the heat diffusion of the metallic rod. If we are to apply heat at the center of the metallic rod, the heat starts to resemble a tall narrower gaussian shape. As we stop the source of heat, the gaussian shape starts to get shorter and wider, indicating that the heat is diffusing towards the left and right ends of the metallic rod until such that the temperature reaches equilibrium - or in laplacian terms, until the Laplace equation is zero. See Figure 4.24. Figure 4.24: 1D heat and wave graph The Laplace equation for a one-dimensional dynamic system is expressed as: \\[\\begin{align} \\frac{\\partial^2 u}{\\partial x^2} = 0 \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{xx} = 0 \\end{align}\\] The Laplace equation for a two-dimensional dynamic system is expressed as: \\[\\begin{align} \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0 \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{xx} + u_{yy} = 0 \\end{align}\\] and it can serve as means for us to determine equilibrium. To illustrate the use of Laplace equation for a two-dimensional dynamic system, we use discretization using second-order centered finite difference like so: \\[\\begin{align} \\frac{\\partial^2 u}{\\partial x^2} + \\frac{\\partial^2 u}{\\partial y^2} = 0 \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\\Delta x)^2} + \\frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\\Delta y)^2} = 0 \\end{align}\\] Let the deltas be equal distant, \\(\\Delta x = \\Delta y\\). That gives us the following nodal equation: \\[\\begin{align} 0 {}&amp;= u_{k+1}^t + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1} - 4u_k^t \\\\ u_k^t &amp;= \\frac{1}{4} \\left( u_{k+1}^t + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1}\\right) \\end{align}\\] Figure 4.25 shows a grid that allows us to construct our system of linear equations. Each circle in the grid represents the temperature of a node, \\(\\mathbf{u_{ij}}\\), which is expressed and can be solved using the nodal equation above. In our case, we assume that the temperature of the external nodes is already known, and the temperature of the interior nodes is not known. If we think that there are no practical analytical means of solving for the internal temperatures, we use approximation. Figure 4.25: Laplace Equation using Gauss-Seidel Method We start with the given boundary conditions (e.g. temperatures at the exterior nodes are known): \\[ u_k^0 = 20^\\circ C,\\ \\ \\ \\ \\ \\ u_0^t = 40^\\circ C,\\ \\ \\ \\ \\ \\ u_5^t = 60^\\circ C,\\ \\ \\ \\ \\ \\ u_k^5 = 80^\\circ C \\] where: \\[ t=\\{0\\ ..\\ 5\\},\\ \\ \\ \\ \\ \\ \\ k=\\{1\\ ..\\ 4\\},\\ \\ \\ \\ \\ \\ \\mathbf{u_k^t} = 0^\\circ C \\] Our goal is to approximate the temperature of the interior nodes. We use the Gauss-Seidel method to perform an iteration using an initial temperature of zero for the interior nodes; we can use other methods to solve a system of linear equations. First, let us start by choosing an interior node in the matrix, say \\(\\mathbf{u_1^1}\\), from which to construct our first equation. The nodal equation for the node is easier shown using a stencil. See Figure 4.26. Figure 4.26: 2D Stencil We use the nodal equation to solve for \\(\\mathbf{u_1^1}\\): \\[\\begin{align} u_k^t {}&amp;= \\frac{1}{4} \\left( u_{k+1}^t + u_{k-1}^{t} + u_{k}^{t+1} + u_{k}^{t-1}\\right)\\\\ u_1^1 &amp;= \\frac{1}{4} \\left( u_2^1 + u_0^1 + u_1^2 + u_1^0 \\right) \\\\ &amp;= \\frac{1}{4} \\left( 0 + 40.00 + 0 + 20.00 \\right), \\ \\ \\ \\ where\\ interior\\ nodes\\ are\\ zero \\nonumber \\\\ &amp;= \\frac{1}{4} (60) = 15.00 \\nonumber \\end{align}\\] Second, let us choose the next interior node in the matrix, say \\(\\mathbf{u_1^2}\\): \\[\\begin{align} u_1^2 {}&amp;= \\frac{1}{4} \\left( u_2^2 + u_0^2 + u_1^3 + u_1^1 \\right)\\\\ u_1^2 &amp;= \\frac{1}{4} \\left( 0 + 40.00 + 0 + 15.00 \\right) \\nonumber\\\\ &amp;= \\frac{1}{4} (55) = 13.75 \\nonumber \\end{align}\\] Third, we proceed with \\(\\mathbf{u_1^3}\\) and \\(\\mathbf{u_1^4}\\): \\[\\begin{align} u_3^1 {}&amp;= \\frac{1}{4} \\left( u_3^2 + u_0^3 + u_1^4 + u_1^2 \\right)\\\\ u_3^1 &amp;= \\frac{1}{4} \\left( 0 + 40.00 + 0 + 13.75 \\right) \\nonumber\\\\ &amp;= \\frac{1}{4} (28.75) = 13.44 \\nonumber\\\\ \\nonumber\\\\ u_4^1 {}&amp;= \\frac{1}{4} \\left( u_4^2 + u_0^4 + u_1^5 + u_1^3 \\right)\\\\ u_4^1 &amp;= \\frac{1}{4} \\left( 0 + 40.00 + 80.00 + 13.44 \\right) \\nonumber\\\\ &amp;= \\frac{1}{4} (87.188) = 33.36 \\nonumber \\end{align}\\] Fourth, let us proceed with the next column, \\(\\mathbf{u_2^1}\\): \\[\\begin{align} u_2^1 {}&amp;= \\frac{1}{4} \\left( u_3^1 + u_1^1 + u_2^2 + u_2^0 \\right)\\\\ u_2^1 &amp;= \\frac{1}{4} \\left( 0 + 15.00 + 0 + 20.00 \\right) \\nonumber\\\\ &amp;= \\frac{1}{4} (45) = 8.75 \\nonumber \\end{align}\\] Fifth, we proceed with the rest of the interior nodes. After computing the temperature of the interior nodes, we see the following result of the first iteration: \\[ \\left[ \\begin{array}{rrrrrr} u_0^5 &amp; u_1^5 &amp; u_2^5 &amp; u_3^5 &amp; u_4^5 &amp; u_5^5\\\\ u_0^4 &amp; u_1^4 &amp; u_2^4 &amp; u_3^4 &amp; u_4^4 &amp; u_5^4 \\\\ u_0^3 &amp; u_1^3 &amp; u_2^3 &amp; u_3^3 &amp; u_4^3 &amp; u_5^3 \\\\ u_0^2 &amp; u_1^2 &amp; u_2^2 &amp; u_3^2 &amp; u_4^2 &amp; u_5^2 \\\\ u_0^1 &amp; u_1^1 &amp; u_2^1 &amp; u_3^1 &amp; u_4^1 &amp; u_5^1 \\\\ u_0^0 &amp; u_1^0 &amp; u_2^0 &amp; u_3^0 &amp; u_4^0 &amp; u_5^0 \\\\ \\end{array} \\right]\\rightarrow \\left[ \\begin{array}{rrrrrr} 40.00 &amp; 80.00 &amp; 80.00 &amp; 80.00 &amp; 80.00 &amp; 60.00\\\\ 40.00 &amp; 33.36 &amp; 29.53 &amp; 27.88 &amp; 47.17 &amp; 60.00 \\\\ 40.00 &amp; 13.44 &amp; 4.77 &amp; 1.99 &amp; 20.81 &amp; 60.00 \\\\ 40.00 &amp; 13.75 &amp; 5.62 &amp; 3.20 &amp; 21.25 &amp; 60.00 \\\\ 40.00 &amp; 15.00 &amp; 8.75 &amp; 7.19 &amp; 21.80 &amp; 60.00 \\\\ 40.00 &amp; 20.00 &amp; 20.00 &amp; 20.00 &amp; 20.00 &amp; 60.00 \\end{array} \\right] \\] We repeat the process starting with \\(\\mathbf{u_1^1}\\) again and completing all interior nodes for the next iteration. We iterate until convergence. Convergence happens when the temperature of all of the individual interior nodes (the nodal temperature) reaches a tolerable relevant error. \\[ \\left|\\frac{ u_{ij}^{current} - u_{ij}^{previous}} {u_{ij}^{current}}\\right| &lt; tol \\] Here is a naive R implementation of solving for the initial temperature distribution across the plate using the Laplacian equation and Gauss-Seidel method: #laplatian plate with initial temperature of zero for interior nodes # 36 nodes A = matrix(0, 6, 6) #boundary conditions for exterior nodes A[1,] = 80; A[6,] = 20; A[,1]=40; A[,6]=60 laplace &lt;- function(A, iter ) { err = A converge = TRUE tol = 1e-1 M = nrow(A) N = ncol(A) # Gauss-Seidel for (K in seq(1,(N-2),1)) { for (T in seq(1,(M-2),1)) { k=K+1; t=M-T A[t,k] = (A[t,k+1] + A[t,k-1] + A[t-1,k] + A[t+1,k] ) / 4 err[t,k] = abs( ( A[t,k] - err[t,k] ) / A[t,k] ) * 100 if (err[t,k] &gt; tol) converge = FALSE } } list(&quot;converge&quot;=converge, &quot;rel_err&quot;=err, &quot;laplace_plate&quot;=round(A,2), &quot;iteration&quot;=iter ) } limit=50; sol = NULL for (iterate in 1:limit) { sol = laplace(A, iterate) if (sol$converge == TRUE) break A = sol$laplace_plate } sol ## $converge ## [1] TRUE ## ## $rel_err ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 40 80.00000000 80.00000000 80.00000000 80.000000000 60 ## [2,] 40 0.02377444 0.01943362 0.01486774 0.006828848 60 ## [3,] 40 0.03180476 0.04954826 0.03051368 0.013977502 60 ## [4,] 40 0.05142632 0.06883892 0.04793824 0.030091247 60 ## [5,] 40 0.04463621 0.05658029 0.05558349 0.023736587 60 ## [6,] 40 20.00000000 20.00000000 20.00000000 20.000000000 60 ## ## $laplace_plate ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 40 80.00 80.00 80.00 80.00 60 ## [2,] 40 58.16 64.52 66.80 66.35 60 ## [3,] 40 48.15 53.14 56.33 58.62 60 ## [4,] 40 41.32 43.58 46.77 51.80 60 ## [5,] 40 33.61 33.14 35.42 41.80 60 ## [6,] 40 20.00 20.00 20.00 20.00 60 ## ## $iteration ## [1] 18 After convergence, we see the final solution of the initial heat distribution across the plate. Note that this does not imply that the temperature across the plate has diffused in equilibrium. The solution is only for one specific point in time. Suppose we account for a time-dependent dynamic system. In that case, we have to capture each solution for every step over time until heat diffuses in equilibrium - meaning that the nodal temperature across the plate is the same. 4.6.2 The Heat equation (Parabolic PDE) It helps to understand PDE by introducing the dynamics of heat and wave on a simple one-dimension manifold such as a line - in our case, we use rod for our physical material. To illustrate heat, we assume a thin metallic rod tightly bolted onto two heat sink materials or ice blocks. See Figure 4.24. We may need to assume a few things also. First, the heat in the cross-section of the rod is constant as we heat the metallic rod in the middle. Second, we assume that the rod is insulated to disallow heat flux from escaping the rod. Then we assume that the metallic rod has already heated to an initial temperature. Our goal is to demonstrate that as we apply heat in the middle of the rod, the heat diffuses towards the left and right edges of the rod until the temperature across the rod meets equilibrium. For illustration, we use a one-dimension homogeneous heat equation, which is expressed as such: \\[\\begin{align} \\frac{\\partial u}{\\partial t} = \\alpha\\frac{\\partial^2 u}{\\partial x^2} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{t} = \\alpha\\ u_{xx} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{t}(x,t) = \\alpha\\ u_{xx}(x,t) \\end{align}\\] where \\[\\begin{align} \\alpha = \\frac{k}{c \\rho}\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ k=conductivity,\\ c=capacity,\\ \\rho=density \\end{align}\\] and where \\(\\mathbf{\\alpha}\\) is a constant (e.g. diffusivity) and our dirichlet boundary condition applies to space: \\[\\begin{align} u(0, t) = 0,\\ \\ \\ \\ \\ \\ \\ \\ u(1, t) = 0 \\end{align}\\] Our initial condition (initial temperature) is as follows: \\[\\begin{align} u(x, 0) = f(x) = \\frac{1}{\\sqrt{0.4\\pi}} e^{-\\frac{x^2}{0.4^2}} \\end{align}\\] We use Finite Difference method to approximate the solution to the heat equation. Our approximation approach uses a fully discretized method called Forward Time, Centered Space (FTCS). The idea is to use the first-order Forward Finite Difference for time ( the t independent variable): \\[\\begin{align} f&#39;(x) \\approx \\frac{f(x + h) - f(x)}{h}\\ \\rightarrow\\ u_t \\approx \\frac{u_k^{t+1} - u_k^t}{\\Delta t} \\end{align}\\] where our h is change in time denoted as \\(\\mathbf{\\Delta t}\\). Additionally, the idea is also to use the second-order Centered Finite Difference for space (the x independent variable): \\[\\begin{align} f&#39;&#39;(x) \\approx \\frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\\ \\rightarrow\\ u_{xx} \\approx \\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\\Delta x)^2} \\end{align}\\] We substitute with the approximate finite difference equations: \\[\\begin{align} u_t = \\alpha u_{xx}\\ \\ \\ \\rightarrow \\frac{u_k^{t+1} - u_k^t}{\\Delta t} {}&amp;= \\alpha \\left(\\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\\Delta x)^2}\\right) \\\\ u_k^{t+1} - u_k^t &amp;= \\left(\\frac{\\alpha \\Delta t}{(\\Delta x)^2}\\right) \\left( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\\right) \\end{align}\\] and then perform simplification to arrive at the following equation: \\[\\begin{align} u_k^{t+1} - u_k^t {}&amp;= \\lambda ( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}) \\\\ u_k^{t+1} &amp;= u_k^t + \\lambda ( u_{k+1}^t - 2u_k^t + u_{k-1}^{t}) \\\\ u_k^{t+1} &amp;= \\lambda u_{k+1}^t + ( 1 - 2 \\lambda )u_k^t + \\lambda u_{k-1}^{t} \\end{align}\\] where we have the CFL condition (Courant, Friedrichs, Lewy 1928) as: \\[\\begin{align} \\lambda = \\left(\\frac{\\alpha \\Delta t}{(\\Delta x)^2}\\right) &lt;= 0.5 \\end{align}\\] In matrix form, we get: \\[ \\left[\\begin{array}{r} u_{0}^{t+1} \\\\ u_{1}^{t+1} \\\\ u_{2}^{t+1} \\\\ \\vdots \\\\ u_{n}^{t+1} \\end{array}\\right] = \\left[ \\begin{array}{rrrrrr} (1-2\\lambda ) &amp; \\lambda &amp; 0 &amp; ... &amp; 0 \\\\ \\lambda&amp; (1-2\\lambda) &amp; \\lambda &amp; ... &amp; 0 \\\\ 0 &amp; \\lambda &amp; (1-2\\lambda) &amp; ... &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\lambda \\\\ 0 &amp; ... &amp; 0 &amp; \\lambda&amp; (1-2\\lambda) \\\\ \\end{array} \\right] \\left[\\begin{array}{r} u_{0}^t \\\\ u_{1}^t \\\\ u_{2}^t \\\\ \\vdots \\\\ u_n^t \\end{array}\\right] \\] Here is a naive implementation of Heat Diffusion (non-smooth) in R code without using matrix or methods such as Gauss-Seidel or SOR to solve the matrix. See Figure 4.27. f &lt;- function(x, t) { 1/sqrt( 0.4 * pi ) * exp(-x^2 /(0.4)^2 ) } initial &lt;- function(x) { x_ = f(x, 0) x_[1] = 0; x_[N] = 0 x_ } # finite difference (ftcs) heat_ftcs &lt;- function(x, lambda) { u = matrix(0, ncol=N, nrow=M) u[1,] = initial(x) for (t in 1:(M-1)) { u_t = u[t,] x_ = rep(0, N) for (k in 2:(N-1)) { x_[k] = lambda * u_t[k+1] + (1 - 2 * lambda) * u_t[k] + lambda * u_t[k-1] } u[t+1,] = x_ } u } # finite difference (crank-nicolson) heat_crank_nicolson &lt;- function(x, lambda) { u = matrix(0, ncol=N, nrow=M) u[1,] = initial(x) for (t in 1:(M-1)) { u_t = u[t,] x_forward = rep(0, N) x_backward = rep(0, N) for (k in 2:(N-1)) { x_forward[k] = lambda * u_t[k+1] + (1 - 2 * lambda) * u_t[k] + lambda * u_t[k-1] } for (k in 2:(N-1)) { x_backward[k] = lambda * x_forward[k+1] + (1 - 2 * lambda) * x_forward[k] + lambda * x_forward[k-1] } u[t+1,] = 1/2 * (x_forward + x_backward ) } u } space = x = seq(-1, 1, length.out=31) time = seq(0.1, 1, length.out=100) N = length(space); M = length(time) u = heat_ftcs(space, 0.5) plot(NULL, xlim=range(-1,1), ylim=range(0,1), xlab=&quot;metalic rod&quot;, ylab=&quot;temperature&quot;, main=&quot;2D view of heat diffusion&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(v=c(-1,0,1), lty=2, col=&quot;darksalmon&quot;) for (t in 1:M) { lines(x, u[t,], col=&quot;navyblue&quot;) } Figure 4.27: Heat Diffusion library(&#39;plot3D&#39;) x=1:N;y=1:M;z=u persp3D(y, x, z, xlab=&quot;time&quot;, ylab=&quot;rod length&quot;, zlab=&quot;temperature&quot;, main=&quot;3D view of heat diffusion&quot;, axes=TRUE, ticktype=&quot;detailed&quot;) Figure 4.28: 3D view of heat diffusion Notice in the FTCS implementation of the heat equation that the \\(\\lambda\\) affects the stability of the system. A value less than 0.5 renders stability. 4.6.3 The Wave equation (Hyperbolic PDE) Hyperbolic systems deal with vibrations (oscillations) and waves. Unlike Heat diffusion where temperature settles down to equilibrium immediately, the behavior of vibration in particular starts from an initial state (say high altitude). But instead of slowing down to equilibrium, it tends to overshoot to the other side, eventually reaching a pulling resistance, causing the system to oscillate back and forth like a pendulum or a vibrating string. Each oscillation, however, exponentially manifests a slow-down decay until such that it eventually reaches equilibrium. The resistance - restoring force - encountered by the oscillation is the inertia that conserves energy. The Wave equation we use is slightly different from the Heat equation by raising the left side of the equation to the power of 2. To illustrate the use of Wave equation, we likewise use the Finite Difference method to approximate the one-dimension homogeneous Wave equation below for a vibrating string: \\[\\begin{align} \\frac{\\partial^2 u}{\\partial t^2} = \\alpha \\frac{\\partial^2 u}{\\partial x^2} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{tt} = \\alpha u_{xx} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{tt}(x,t) = \\alpha\\ u_{xx}(x,t) \\end{align}\\] where \\[\\begin{align} \\alpha = \\frac{\\tau}{\\rho}\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\tau=tension,\\ \\rho=density \\end{align}\\] and where \\(\\mathbf{\\alpha}\\) is a constant (e.g. speed of wave propagation) and our Dirichlet boundary condition applies to space: \\[\\begin{align} u(0, t) = 0,\\ \\ \\ \\ \\ \\ \\ \\ u(1, t) = 0 \\end{align}\\] Our initial conditions for displacement and velocity are: \\[\\begin{align} u(x, 0) = f(x)\\ \\leftarrow displacement,\\ \\ \\ \\ u_t(x, 0) = \\frac{\\partial u}{\\partial t}(x,0) = g(x)\\ \\leftarrow velocity,\\ \\ \\ \\ 0 \\leq x \\leq 1 \\end{align}\\] Here, the idea is to compute the trajectory of the waves by knowing the displacement and velocity. In the case of our illustration, we have a vibrating string. Our approximation approach is to use the second-order Centered Finite Difference: \\[\\begin{align} f&#39;&#39;(x) \\approx \\frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\\ \\rightarrow\\ u_{tt} \\approx \\frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\\Delta t)^2} \\end{align}\\] where our h is change in time denoted as \\(\\mathbf{\\Delta t}\\), Additionally, we also use the second-order Centered Finite Difference for x: \\[\\begin{align} f&#39;&#39;(x) \\approx \\frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\\ \\rightarrow\\ u_{xx} \\approx \\frac{u_{k+1}^{t} - 2u_k^t + u_{k-1}^t}{(\\Delta x)^2} \\end{align}\\] We substitute with the approximate Finite Difference equations: \\[\\begin{align} u_{tt} = \\alpha u_{xx}\\ \\ \\ \\rightarrow {}&amp;\\frac{u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1}}{(\\Delta t)^2} = \\alpha\\left(\\frac{u_{k+1}^{t} - 2u_k^t + u_{k-1}^t}{(\\Delta x)^2}\\right) \\\\ &amp;u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1} = \\alpha \\left( \\frac{\\Delta t}{\\Delta x}\\right)^2(u_{k+1}^{t} - 2u_k^t + u_{k-1}^t) \\end{align}\\] and then perform simplification to arrive at the following equation: \\[\\begin{align} {}&amp;u_{k}^{t+1} - 2u_k^t + u_{k}^{t-1} = \\lambda (u_{k+1}^{t} - 2u_k^t + u_{k-1}^t)\\\\ &amp;u_{k}^{t+1} = 2u_k^t - u_{k}^{t-1} + \\lambda (u_{k+1}^{t} - 2u_k^t + u_{k-1}^t)\\\\ &amp;u_{k}^{t+1} = \\lambda(u_{k+1}^t + u_{k-1}^t) + 2(1 - \\lambda)u_k^t - u_k^{t-1} \\end{align}\\] where: \\[\\begin{align} \\lambda = \\alpha \\left( \\frac{\\Delta t}{\\Delta x}\\right)^2 &lt;= 0.5 \\end{align}\\] Here is a naive way of implementing the Wave equation in R code: f &lt;- function(x,t) { 1/sqrt( 0.4 * pi ) * exp(-x^2 /(0.4)^2 ) } initial &lt;- function(x) { x_ = f(x, 0) x_[1] = 0; x_[N] = 0 x_ } wave &lt;- function(x, lambda) { # finite difference u = matrix(0, ncol=N, nrow=M) u[1,] = u_t_old = initial(x) for (t in 1:(M-1)) { u_t = u[t,] u_t_new = rep(0, N) for (k in 2:(N-1)) { u_t_new[k] = lambda*(u_t[k+1] + u_t[k-1]) + 2 *(1 - lambda)*u_t[k] - u_t_old[k] } u_t_old = u_t u[t+1,] = u_t_new } u } space = x = seq(-1, 1, length.out=41) time = seq(0.1, 1, length.out=100) N = length(space); M = length(time) u = wave(space, 0.4) plot(NULL, xlim=range(-1,1), ylim=range(-1,1), xlab=&quot;vibrating string&quot;, ylab=&quot;oscillation&quot;, main=&quot;2D view of Oscillation&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(v=c(-1,0,1), lty=2, col=&quot;darksalmon&quot;) for (t in 1:M) { lines(x, u[t,], col=&quot;navyblue&quot;) } Figure 4.29: Wave Propagation library(&#39;plot3D&#39;) x=1:N;y=1:M;z=u persp3D(y, x, z, xlab=&quot;time&quot;, ylab=&quot;string length&quot;, zlab=&quot;oscillation&quot;, main=&quot;3D view of Oscillation&quot;, axes=TRUE, ticktype=&quot;detailed&quot;) Figure 4.30: 3D view of Wave Propagation Note that Figure 4.29 shows a sample of an oscillating string. We can also simulate a wave that propagates, such as a lightwave, by adjusting the boundary conditions and possibly the initial condition while retaining the computed Finite Difference equation: \\[\\begin{align} u_{k}^{t+1} = \\lambda(u_{k+1}^t + u_{k-1}^t) + 2(1 - \\lambda)u_k^t - u_k^{t-1} \\end{align}\\] Taking into account stability and accuracy, we leave readers to investigate other explicit finite difference schemes such as Forward Euler, Leapfrog, Lax-Wendroff and also explore other implicit finite difference schemes such as Backward Euler, and Crank-Nicolson. We also leave readers to investigate solving heat equation and wave equations using Finite Element Methods. Lastly, for a more accurate (or precise) result, we leave readers to also investigate Modern Tailor Series Method (MTSM). 4.6.4 The Crank-Nicolson Equation An alternate approximation for the heat equation is the Crank-Nicolson Equation which is unconditionally stable. Our approximation approach account for the average between two time-intervals using explicit and implicit finite difference, a.l.a, forward Euler and backward Euler). Figure 4.31: Crank Nicolson Stencil The idea is still to use the first-order Forward Finite Difference for time ( the t independent variable): \\[\\begin{align} f&#39;(x) \\approx \\frac{f(x + h) - f(x)}{h}\\ \\rightarrow\\ u_t \\approx \\frac{u_k^{t+1} - u_k^t}{\\Delta t} \\end{align}\\] where our h is change in time denoted as \\(\\mathbf{\\Delta t}\\). Additionally, the idea is to also use the average of the explicit and implicit second-order Centered Finite Difference for space (the x independent variable): \\[\\begin{align} f&#39;&#39;(x) {}&amp;\\approx \\frac{f(x + h) - 2f(x) + f(x-h)}{h^2}\\\\ u_{xx} &amp;\\approx \\frac{1}{2} \\left( \\frac{u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1}}{(\\Delta x)^2} + \\frac{u_{k+1}^t - 2u_k^t + u_{k-1}^{t}}{(\\Delta x)^2} \\right) \\\\ &amp;\\approx \\frac{1}{2(\\Delta x)^2} \\left( u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t} \\right) \\end{align}\\] We substitute with the approximate finite difference equations: \\[\\begin{align} u_t = \\alpha u_{xx}\\ \\ \\ \\rightarrow \\frac{u_k^{t+1} - u_k^t}{\\Delta t} {}&amp;= \\alpha \\left[ \\frac{1}{2(\\Delta x)^2} \\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\\right) \\right]\\\\ u_k^{t+1} - u_k^t &amp;= \\frac{\\lambda}{2} \\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\\right) \\end{align}\\] where: \\[\\begin{align} \\lambda = \\left(\\frac{\\alpha \\Delta t}{(\\Delta x)^2}\\right) &lt;= 0.7 \\end{align}\\] Simplifying further, we then get the following: \\[\\begin{align} {}&amp;u_k^{t+1} - u_k^t = \\frac{\\lambda}{2} \\left(u_{k+1}^{t+1} - 2u_k^{t+1} + u_{k-1}^{t+1} + u_{k+1}^t - 2u_k^t + u_{k-1}^{t}\\right) \\\\ &amp;- \\lambda u_{k+1}^{t+1} + 2(1 + \\lambda)u_k^{t+1} - \\lambda u_{k-1}^{t+1} = \\lambda u_{k+1}^t + 2(1 - \\lambda)u_k^t + \\lambda u_{k-1}^{t} \\end{align}\\] The equation above can be used to form a matrix of system of equations. \\[ \\left[ \\begin{array}{rrrrr} 2(1+\\lambda) &amp; -\\lambda &amp; 0 &amp; ... &amp; 0 \\\\ -\\lambda&amp; 2(1+\\lambda) &amp; -\\lambda &amp; ... &amp; 0 \\\\ 0 &amp; -\\lambda &amp; 2(1+\\lambda) &amp; ... &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; -\\lambda \\\\ 0 &amp; ... &amp; 0 &amp; -\\lambda&amp; 2(1+\\lambda) \\\\ \\end{array} \\right] \\left[\\begin{array}{r} u_{0}^{t+1} \\\\ u_{1}^{t+1} \\\\ u_{2}^{t+1} \\\\ \\vdots \\\\ u_n^{t+1} \\end{array}\\right] = \\left[\\begin{array}{r} 2(1-\\lambda)u_{0}^t + \\lambda u_{1}^t \\\\ \\lambda u_{0}^t + 2(1-\\lambda)u_{1}^t + \\lambda u_{2}^t \\\\ \\lambda u_{1}^t + 2(1-\\lambda)u_{2}^t + \\lambda u_{3}^t \\\\ \\vdots \\\\ \\lambda u_{n-1}^t + 2(1-\\lambda)u_{n}^t \\end{array}\\right] \\] Approximation methods such as Gauss-Seidel, SOR, etc., can then be used to solve the unknown functions for each time interval. Also, our naive implementation in R for the heat equation shows the Crank-Nicolson method of solving the solution using a greedy loop rather than matrix manipulation methods. 4.6.5 The Burger’s Equation It helps to introduce Burger’s equation because this PDE has essential applications in areas such as fluid dynamics and traffic flow. \\[\\begin{align} \\frac{\\partial u}{\\partial t} + u\\frac{\\partial u}{\\partial x} = \\alpha\\frac{\\partial^2 u}{\\partial x^2} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ u_{t} + uu_x= \\alpha u_{xx} \\end{align}\\] We leave readers to further investigate this equation. In subsequent sections, we focus on harmonic or periodic systems used for signal processing. Along with the discussions, we illustrate how to identify and work on individual components of complex systems. 4.7 Approximation using Fourier Series And Transform In Chapter 3 (Numerical Linear Algebra II), we introduced Polynomial Approximation and Polynomial Interpolation to deal with irregular curves. We also introduced methods based on the Taylor Series, representing a *Polynomial Expansion** - a linear combination of polynomials (Press W.H. et al. 2007). Here, we introduce Fourier Series which represents a sinusoidal expansion - a linear combination of sine and cosine functions. We present curves that demonstrate periodic or harmonic patterns such that each period (T) repeats: \\[\\begin{align} f(x) = f(x + T) \\end{align}\\] Sine and Cosine functions give us an example of periodic wave patterns. Apart from such patterns, we introduce composite periodic patterns generated by a combination of periodic functions instead of just being individual Sine or Cosine patterns. Figure 4.32 shows a signal generated by a simple summation of sine and cosine, e.g.: \\[\\begin{align} f(x) = sin(x) + cos(x) \\end{align}\\] The period used in the graph is of length \\(\\mathbf{2\\pi}\\). In the graph, we see three periods (or three waves) overlapping with an extra square wave having the following properties: \\[ f(x) \\rightarrow \\begin{cases} -1 &amp; -\\pi &lt; x &lt; 0 \\\\ \\ \\ \\ 0 &amp; x = 0,\\pm \\pi, \\pm 2 \\pi \\\\ \\ \\ \\ 1 &amp; 0 &lt; x &lt; \\pi \\end{cases} \\] Figure 4.32: Periodic Signal This section introduces Fourier Series, expressed as a function that handles curves with harmonic or periodic patterns. In this particular case, we intend to derive a Fourier Series function that can closely represent the square wave in Figure 4.32. We can express such periodic series using the following equation: \\[\\begin{align} F(x) = \\sum_{k=1}^{\\infty} e^{kx} = a_0 + \\sum_{k=1}^{\\infty} (a_k cos(kx) + b_k sin(kx)) \\end{align}\\] The three coefficients in the formula expand into the following (note that we do not cover the derivation of the coefficients here): \\[\\begin{align} a_0 {}&amp;= \\frac{1}{2L} \\int_{-L}^L f(x) dx \\\\ \\nonumber \\\\ a_k &amp;= \\frac{1}{L} \\int_{-L}^L f(x) cos(kx) dx \\\\ \\nonumber \\\\ b_k &amp;= \\frac{1}{L} \\int_{-L}^L f(x) sin(kx) dx \\end{align}\\] where \\(L = (period)/2\\). To illustrate the use of the Fourier series equation, let us solve for the coefficients; given a period of \\(\\mathbf{2\\pi}\\), along with the graph in Figure 4.32, and including the properties of the square wave. Note that the period, \\(\\mathbf{2\\pi}\\), results in \\(L=(2\\pi)/2 = \\pi\\). First, let us solve for \\(\\mathbf{a_0}\\) by splitting the improper integral into two integrals: \\[\\begin{align} a_0 {}&amp;= \\frac{1}{2\\pi} \\int_{-\\pi}^\\pi f(x) dx \\\\ &amp;= \\frac{1}{2\\pi} \\left[ \\int_{-\\pi}^0 (-1)dx + \\int_0^{\\pi} (1)dx \\right] \\\\ &amp;= 0 \\nonumber \\end{align}\\] Second, we solve for \\(\\mathbf{a_k}\\). Here, it helps to be aware of the concept of odd or even functions. It allows us to immediately determine the ending result of the function without having to go through the integration. A function is even if \\(\\mathbf{f(x) = f(-x)}\\) for all \\(\\mathbf{x}\\). And a function is odd if \\(\\mathbf{f(x) = -f(-x)}\\) for all \\(\\mathbf{x}\\). This gives us the following formulas: \\[\\begin{align} f_{odd}(x) = \\int_{-L}^{L} f(x) dx = 0,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ f_{even}(x) = \\int_{-L}^{L} f(x) dx = 2 \\int_{0}^{L} f(x) dx \\end{align}\\] Additionally, an odd function multiplied by an even function results in an odd function. Otherwise, it is even function. For example, below is an odd function because it is a multiplication of even and odd. Therefore, the result is zero. To illustrate: \\[ f(x) = 1 \\rightarrow odd,\\ \\ \\ \\ \\ cos(kx) \\rightarrow even,\\ \\ \\ \\ \\ \\therefore\\ \\text{we get odd}. \\] Therefore, \\[\\begin{align*} a_k &amp;= \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) cos(kx) dx,\\ \\ \\leftarrow \\text{odd function} \\\\ &amp;= 0. \\end{align*}\\] Third, we solve for \\(\\mathbf{b_k}\\). Here, we have the following \\[ f(x) = 1 \\rightarrow odd,\\ \\ \\ \\ \\ sin(kx) \\rightarrow odd,\\ \\ \\ \\ \\ \\therefore\\ \\text{we get even}. \\] Therefore, \\[\\begin{align*} b_k &amp;= \\frac{1}{\\pi} \\int_{-\\pi}^\\pi f(x) sin(kx) dx \\\\ &amp;= \\frac{2}{\\pi} \\int_{0}^\\pi f(x) sin(kx) dx,\\ \\ \\leftarrow \\text{even function} \\\\ &amp;= \\frac{2}{\\pi} \\int_{0}^\\pi (1) sin(kx) dx\\\\ &amp;= \\frac{2}{\\pi} \\left[\\ -cos(kx)\\ \\right]_0^{\\pi}\\\\ &amp;= \\begin{cases} 0 &amp; \\text{k is even} \\\\ \\frac{4}{k\\pi} &amp; \\text{k is odd} \\end{cases} \\end{align*}\\] Finally, we compose our Fourier Series function for the square wave function in Figure 4.32 using the coefficients, and we end up with the following square-wave equation: \\[ F(x) = \\frac{4}{\\pi}\\left(sin(x) + \\frac{1}{3}sin(3x) + \\frac{1}{5}sin(5x)\\ +\\ ... \\right) \\] One of the widely used examples of Fourier series is shown in Figure 4.33 which shows a simple wave function transforming into a square-wave periodic function. The figure shows how a simple sinusoidal curve can evolve into a square-wave periodic pattern by extending the terms of a sine-wave Fourier series function. Figure 4.33: Sinusoids in Time (Periodic) Domain The phenomena can be explained by observing two waves, which, when combined, form a convolution of a larger wave. The total amplitude is the sum of the individual amplitude of the individual waves. The larger wave and its total amplitude is the physical manifestation of the superposition of two waves (two wave functions). If we review the individual signals in Figure 4.33 with the fixed-frequency waves in Figure 4.34, we can see that each signal is a linear combination of their corresponding set of frequencies. For example, signal 6 renders a periodic pattern that composes of six terms (or six frequencies) based on the following linear combination: \\[ F(x) = \\frac{4}{\\pi} \\left( sin(x) + \\frac{sin(3x)}{3} + \\frac{sin(5x)}{5} + \\frac{sin(7x)}{7} + \\frac{sin(9x)}{9} + \\frac{sin(11x)}{11} \\right) \\] If we are to graph each of those six terms, we see the following Figure 4.34: Figure 4.34: Sinusoidal Components in Frequency Domain The sixth sinusoid - signal 6 - is decomposed into six unique frequencies (or sinusoidal components). We see that the sinusoid has a frequency spectrum of six unique frequencies. Notice also in Figure 4.33 that each of the signals flows within its own time-domain in that each signal is broken into individual frequencies spread across its frequency-domain as shown in Figure 4.34. Each of the unique frequencies is held into a frequency bin. Let us further illustrate this by using Figure 4.35 which shows a 2D view of a periodic signal and Figure 4.36 which shows its 3D view. Figure 4.35: Time and Frequency Domain Figure 4.36: Time and Frequency Domain In Figure 4.36, the first ribbon (left in the figure) represents the composition of the next three ribbons which respectively represent the three frequencies in the frequency-domain . The first ribbon is expressed as: \\[ F(x) = sin(x) + \\frac{sin(3x)}{3} + \\frac{sin(5x)}{5} \\] whereas the other three frequencies are as follows: \\[ F1(x) = sin(x)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ F2(x) = \\frac{sin(3x)}{3} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ F3(x) = \\frac{sin(5x)}{5} \\] So far, we have shown one property of the Fourier Series, e.g., Linearity. However, other properties carry relatively advanced concepts around the Fourier Series and Transform. We leave readers to further investigate the other properties (including Linearity): linearity time-shifting (in time-domain) frequency shifting (in frequency-domain) conjugation convolution multiplication It is notable to mention that Fourier series is a periodic function that deals with continuous signals in continuous-time (Collings, I. 2021). Literatures may sometimes refer to this as Continuous-Time Fourier Series (CTFS). Another function that deals with continuous signal is Fourier transform; however, this function is aperiodic - the signal does not repeat (Collings, I. 2021). Other pieces of literature may sometimes refer to this as Continuous-Time Fourier Transform (CTFT). This type of transform applies around signal compression or decompression. Now, similar to periodic CTFS and aperiodic CTFT in continuous-time, we have the counter-part signals in discrete-time in the form of periodic Discrete-Time Fourier Series (DTFS) and aperiodic Discrete-Time Fourier Transform (DTFT) (Collings, I. 2021). We leave readers to investigate the differences between the signals mentioned above. Now, in terms of signals in discrete time, our focus in the next section is on computing discrete signals in the form of Discrete Fourier Transform (DFT). Instead of dealing with analog signal, we sample signal in finite discrete-time. By discretizing this way, we approximate the function that may closely represent the actual analog signal. We perform transformationduring the process, which deals with transforming signals from their time-domain representation into their frequency-domain representation and vice-versa. 4.7.1 Discrete Fourier Transform (DFT) We showed that complex signals may come as an aggregation (superposition) of individual simple frequencies. If we decompose signals into their frequencies, we require some function or operation to do so - or the mathematical equation it represents. DFT operation can help here. It describes the correlation between the signal and some oscillating sinusoidal frequency. A signal-processing equation can represent a DFT operation which approximates frequencies given a signal. The function decomposes complex signals into simpler finite individual frequencies. Take Figure 4.37 as an example with a given unknown digital signal. Figure 4.37: Unknown Digital Signal The unknown digital signal is generated from the below equation: \\[ F(x) = sin(x) + \\frac{sin(3x)}{3} + \\frac{sin(5x)}{5} \\] But if we only receive the signal as a dataset of samples, we need to approximate the solution that can closely match the signal. We do need to decipher the signal and approximate its frequency contents. To do that, we need to be familiar with the following DFT equation (\\(\\ref{eqn:eqnnumber6111}\\)): From infinite continuous-time form (e.g., an analog signal) - in the time-domain: \\[\\begin{align} X(f) = \\int_{-\\infty}^{\\infty} f(t) e^{-j2\\pi ft} dt \\end{align}\\] To finite discrete-time form (e.g. digital signal) - in the frequency-domain (see Figure 4.39): \\[\\begin{align} X_m = \\sum_{k=0}^{N-1} x_k e^{\\frac{1}{N}(-j 2\\pi km)} = \\sum_{k=0}^{N-1} x_k \\omega_N^{km} \\label{eqn:eqnnumber6111} \\end{align}\\] where \\(\\omega_N\\) signifies the fundamental frequency. \\[\\begin{align} \\omega_N = e^{\\frac{1}{N}(-j2\\pi)}\\ \\ \\ \\ or\\ \\ \\ \\rightarrow \\omega_N^{km} = e^{\\frac{1}{N}(-j2\\pi km)} \\end{align}\\] and the frequency interval (space between samples) is \\(\\frac{2\\pi}{N}\\). Note that \\(2\\pi\\) represents one period, which is one cycle (assume sine wave) before the value repeats, and N represents the number of sample points to capture within that period. We recognize that if we do not sample the signal correctly in terms of spacing, the correlation between our signal and frequency may not correctly align. We encourage readers to investigate the sampling rate in terms of spacing using Nyquist rate and Nyquist frequency. Note that \\(\\omega_N\\) is an analyzing function or basis function that handles the sinusoid pattern, denoted by the below Euler’s identity: \\[\\begin{align} e^{j\\theta } = (cos(\\theta) + j\\ sin(\\theta))\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ where: \\theta = \\frac{-2\\pi k m}{N} \\end{align}\\] Figure 4.38: Euler’s Identity Figure 4.38 shows a geometric view of Euler’s identity (image reference: Rick Lyons (2013)). Also, if we use Sine and Cosine functions, it removes us from dealing with complex exponentials. No matter what the case, however, DFT translates (or transforms) the signal into a spectrum of frequencies - this is for us to perform further spectral analysis of the frequencies. Figure 4.39 shows the frequency-domain that contains frequencies of our unknown signals in Figure 4.37. Figure 4.39: Frequency Domain To illustrate, let us work on a sampling of four real numbers from a digital signal: \\[ x \\in \\{\\ 0.00,\\ 0.10,\\ 0.40,\\ 0.10\\ \\}\\ \\ \\ \\ \\ where\\ k = \\{0,1,2,3\\} \\] Using the DFT equation (\\(\\ref{eqn:eqnnumber6111}\\)) where N is the number of samples in the time-domain, e.g. N=4, our goal here is to solve for all the DFT coefficients, \\(\\mathbf{\\{ X_1\\ ... \\ X_m \\}}\\). Here, we start by first solving \\(\\mathbf{X_1}\\). Then iteratively, solving for the other coefficients. For that, it does help to show the equation one more time: \\[\\begin{align} X_m = \\sum_{k=0}^{N-1} x_k e^{\\frac{1}{N}(-j 2\\pi km)}, \\end{align}\\] First, we solve for \\({X_1}\\): \\[\\begin{align*} X_1 {}&amp;= 0.00 \\cdotp e^{\\frac{1}{4}(-j2\\pi (0) (0))} + 0.10 \\cdotp e^{\\frac{1}{4}(-j2\\pi (1) (0))} + \\\\ &amp;\\ 0.40 \\cdotp e^{\\frac{1}{4}(-j2\\pi (2) (0))} + 0.10 \\cdotp e^{\\frac{1}{4}(-j2\\pi (3) (0))} \\\\ \\end{align*}\\] Next, we use Euler’s identity to further solve \\(\\mathbf{X_1}\\). Perform substitution for \\(e^{j\\theta}\\). \\[\\begin{align*} X_1 {}&amp;= 0.00 \\left[cos\\left(-\\frac{\\pi(0)(0)}{2}\\right) + j\\ sin\\left(-\\frac{\\pi(0)(0)}{2}\\right) \\right] \\\\ &amp;+ 0.10 \\left[cos\\left(-\\frac{\\pi(1)(0)}{2}\\right) + j\\ sin\\left(-\\frac{\\pi(1)(0)}{2}\\right) \\right] \\\\ &amp;+ 0.40 \\left[cos\\left(-\\frac{\\pi(2)(0)}{2}\\right) + j\\ sin\\left(-\\frac{\\pi(2)(0)}{2}\\right) \\right] \\\\ &amp;+ 0.10 \\left[cos\\left(-\\frac{\\pi(3)(0)}{2}\\right) + j\\ sin\\left(-\\frac{\\pi(3)(0)}{2}\\right) \\right] \\\\ \\\\ &amp;=0.00(1+0i) + 0.10(1+0i) + 0.40(1+0i) + 0.10(1+0i) \\\\ &amp;= 0.60 + 0j \\end{align*}\\] Second, we repeat the process for the rest of the frequency bins. In doing so, we get the following DFT coefficients: \\[ X_1 = 0.60+0j,\\ \\ \\ \\ X_2 = -0.40-0j,\\ \\ \\ \\ \\ X_3 = 0.20+0j,\\ \\ \\ \\ X_4 = -0.40-0j \\] To save time from the long process, as illustrated in the first step, we can rely on a DFT matrix form ( using the equation with exponential terms rather than the cosine-sine terms): \\[ \\left[ \\begin{array}{llllll} \\omega_n^0 &amp; \\omega_n^0 &amp; \\omega_n^0 &amp; \\omega_n^0 &amp; \\cdots &amp; \\omega_n^0\\\\ \\omega_n^0 &amp; \\omega_n^1 &amp; \\omega_n^2 &amp; \\omega_n^3 &amp; \\cdots &amp; \\omega_n^{N-1} \\\\ \\omega_n^0 &amp; \\omega_n^2 &amp; \\omega_n^4 &amp; \\omega_n^6 &amp; \\cdots &amp; \\omega_n^{N-2} \\\\ \\omega_n^0 &amp; \\omega_n^3 &amp; \\omega_n^6 &amp; \\omega_n^9 &amp; \\cdots &amp; \\omega_n^{N-3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\omega_n^0 &amp; \\omega_n^{N-1} &amp; \\omega_n^{N-2} &amp; \\omega_n^{N-3} &amp; \\cdots &amp; \\omega_n^1 \\\\ \\end{array} \\right] \\left[\\begin{array}{c} x_0 \\\\ x_1 \\\\ x_2 \\\\ x_3 \\\\ \\vdots \\\\ x_{N-1} \\end{array}\\right] = \\left[ \\begin{array}{c} X_1 \\\\ X_2 \\\\ X_3 \\\\ X_4 \\\\ \\vdots \\\\ X_m \\end{array}\\right] \\] Using our sample dataset, we fill up our matrix equation with the following: \\[ \\left[ \\begin{array}{rrrr} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; \\omega_4^1 &amp; \\omega_4^2 &amp; \\omega_4^3 \\\\ 1 &amp; \\omega_4^2 &amp; \\omega_4^4 &amp; \\omega_4^6 \\\\ 1 &amp; \\omega_4^3 &amp; \\omega_4^6 &amp; \\omega_4^9 \\\\ \\end{array} \\right] \\left[ \\begin{array}{r} 0.00 \\\\ 0.10 \\\\ 0.40 \\\\ 0.10 \\end{array}\\right] = \\left[ \\begin{array}{c} X_1 \\\\ X_2 \\\\ X_3 \\\\ X_4 \\end{array}\\right] \\] \\[ \\downarrow \\] \\[ \\left[ \\begin{array}{rrrr} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; -j &amp; -1 &amp; j \\\\ 1 &amp; -1 &amp; 1 &amp; -1 \\\\ 1 &amp; j &amp; -1 &amp; -j \\\\ \\end{array} \\right] \\left[ \\begin{array}{r} 0.00 \\\\ 0.10 \\\\ 0.40 \\\\ 0.10 \\end{array}\\right] = \\left[ \\begin{array}{r} 0.60+0j \\\\ -0.40-0j \\\\ 0.20+0j \\\\ -0.40-0j \\end{array}\\right] \\] Note that the imaginary part in complex numbers is represented as either j or i. Other pieces of Engineering literature may use j. However, we may interchangeably use j or i. For example, in R code, we may have to use i, but in this book, we use j. Here is a naive implementation of DFT in R code: dft_exponential &lt;-function(x, M) { N = length(x) X = matrix(rep(0, M)) for (m in 0:(M-1)) { X[m+1] = 0 for (k in 0:(N-1)) { w = ( -2*pi*k*m ) / N X[m+1] = X[m+1] + x[k+1] * ( exp(1i*w) ) } } list(&quot;DFT_Exponential&quot;=X) } dft_cosine_sine &lt;-function(x, M) { N = length(x) X = matrix(rep(0, M)) for (m in 0:(M-1)) { X[m+1] = 0 for (k in 0:(N-1)) { w = ( -2*pi*k*m ) / N X[m+1] = X[m+1] + x[k+1] * ( cos(w) + 1i*sin(w)) } } list(&quot;DFT_cosine_sine&quot;=X) } x = c(0,0.10,0.40,0.10) dft_exponential(x, length(x)) ## $DFT_Exponential ## [,1] ## [1,] 0.6+0i ## [2,] -0.4-0i ## [3,] 0.2+0i ## [4,] -0.4-0i (X=dft_cosine_sine(x, length(x))) ## $DFT_cosine_sine ## [,1] ## [1,] 0.6+0i ## [2,] -0.4-0i ## [3,] 0.2+0i ## [4,] -0.4-0i Figure 4.40: DFT Finally, given the DFT coefficients, let us approximate a sinusoidal equation that can match the original signal. Here, we use the following inverse equation (Inverse Discrete Fourier Transform (IDFT)): \\[\\begin{align} x_k = \\frac{1}{L} \\sum_{m=1}^{L} X_m e^{\\frac{1}{L}(j 2\\pi km)} \\end{align}\\] where L is the number of frequencies in the frequency-domain. Assume, we have \\(\\mathbf{L=4}\\). \\[\\begin{align*} x_n {}&amp;= \\frac{1}{4} \\left[] X_1\\cdotp e^{\\frac{1}{4}j2\\pi (1)(n)} + X_2\\cdotp e^{\\frac{1}{4}j2\\pi (2)(n)} + X_3\\cdotp e^{\\frac{1}{4}j2\\pi (3)(n)} + X_4\\cdotp e^{\\frac{1}{4}j2\\pi (4)(n)} \\right] \\\\ &amp;= \\frac{1}{4} \\left[ X_1\\cdotp e^{\\frac{1}{2}j\\pi n} + X_2\\cdotp e^{j\\pi n} + X_3\\cdotp e^{\\frac{3}{2}j\\pi n} + X_4\\cdotp e^{j2\\pi n} \\right] \\\\ &amp;= \\frac{1}{4} \\left[ 0.60 \\cdotp e^{\\frac{1}{2}j\\pi n} + -0.40 \\cdotp e^{j\\pi n} + 0.20 \\cdotp e^{\\frac{3}{2}j\\pi n} + -0.40 \\cdotp e^{j2\\pi n} \\right] \\\\ &amp;= 0.15 \\cdotp e^{\\frac{1}{2}j\\pi n} + -0.10 \\cdotp e^{j\\pi n} + 0.05 \\cdotp e^{\\frac{3}{2}j\\pi n} + -0.10 \\cdotp e^{j2\\pi n} \\\\ \\end{align*}\\] \\[\\begin{align*} x_n &amp;= 0.15 \\left(cos(\\frac{1}{2}\\pi n) + j sin(\\frac{1}{2}\\pi n)\\right) + -0.10 \\left(cos(\\pi n) + j sin(\\pi n)\\right) \\\\ &amp;+ 0.05 \\left(cos(\\frac{3}{2}\\pi n) + j sin(\\frac{3}{2}\\pi n)\\right) + -0.10 \\left(cos(2\\pi n) + j sin(2\\pi n)\\right) \\end{align*}\\] There are cases when the DFT coefficients may not correctly approximate the frequencies of the signal if we do not choose the correct Nyquist frequency and rate. Here, both parameters can determine the critical points. We can then take a signal sampling to represent the frequencies better given these critical points; otherwise, the basis function may render a different sinusoidal wave (oscillation) not representative of the frequency that correlates to the signal. We leave the topic on Nyquist for the readers to explore further. 4.7.2 Inverse Discrete Fourier Transformation (IDFT) As a reference, this section is a quick outline of the equations for DFT and its inverse which we have already covered in the previous section. We use the following equations for the inverse of DFT. From continuous form (e.g. analog signal) - in time-domain: \\[\\begin{align} X(f) = \\int_{-\\infty}^{\\infty} f(t) e^{-j2\\pi ft} dt\\ \\ \\rightarrow inverse\\ \\ \\rightarrow f(t) = \\int_{-\\infty}^{\\infty} X(f) e^{-j2\\pi ft} df \\end{align}\\] To discrete form (e.g. digital signal) - in the frequency-domain: \\[\\begin{align} X_m = \\sum_{k=0}^{N-1} x_k e^{\\frac{1}{N}(-j 2\\pi km)} \\ \\ \\rightarrow inverse\\ \\ \\rightarrow x_k = \\frac{1}{N} \\sum_{m=0}^{N-1} X_m e^{\\frac{1}{N}(j 2\\pi km)} \\end{align}\\] 4.7.3 Fast Fourier Transform (FFT) FFT is a numerically faster and more efficient way of calculating DFT, rendering \\(O(N log_2 N)\\) operations compared to \\(O(N^2)\\) operations for the DFT function. While there are FFT algorithms introduced in other pieces of literature such as Prime-factor, Rader, and Bruun, here in this section, we use the Cooley-Tukey algorithm (James Cooley and John Tukey [1965]) which implements a divide-and-conquer approach. Recall the following equation: \\[\\begin{align} X_m = \\sum_{k=0}^{N-1} x_k e^{\\frac{1}{N}(-j 2\\pi km)} = \\sum_{k=0}^{N-1} x_k \\omega_N^{km} \\end{align}\\] where \\(\\omega_N\\) signifies the fundamental frequency (the twiddle factor). \\[\\begin{align} \\omega_N = e^{\\frac{1}{N}(-j2\\pi)}\\ \\ \\ \\ or\\ \\ \\ \\rightarrow \\omega_N^{km} = e^{\\frac{1}{N}(-j2\\pi km)} \\end{align}\\] The idea in FFT is to divide the signal data into two groups: the even group and the odd group. For example, given the following signal data: \\[ x = \\{0.00, 0.10, 0.40, 0.10 \\}, \\] we separate the data that fall under odd indices from the data that fall under the even indices. Therefore, we get: \\[ odd = \\{0.00, 0.40\\},\\ \\ \\ \\ \\ \\ \\ \\ \\ even = \\{ 0.10, 0.10\\} \\] We first perform the first recursive step. We split into even groups and odd groups, then compute. \\[\\begin{align*} \\hat{X_0} {}&amp;= x_1 + x_3 \\cdot\\omega^0 = \\ \\ 0.2+0j\\ \\leftarrow \\text{odd} \\\\ \\hat{X_1} &amp;= x_1 - x_3 \\cdot\\omega^1 = \\ \\ 0.0+0j\\ \\leftarrow \\text{odd} \\\\ \\hat{X_2} &amp;= x_0 + x_2 \\cdot\\omega^0 = \\ \\ 0.4+0j\\ \\leftarrow \\text{even} \\\\ \\hat{X_3} &amp;= x_0 - x_2 \\cdot\\omega^1 = -0.4+0j\\ \\leftarrow \\text{even} \\\\ \\end{align*}\\] Then we perform the second recursive step: \\[\\begin{align*} \\hat{X_0} + \\hat{X_2} {}&amp;= X_0 \\\\ \\hat{X_1} + \\hat{X_3} &amp;= X_1\\\\ \\hat{X_0} - \\hat{X_2} &amp;= X_2\\\\ \\hat{X_1} - \\hat{X_3} &amp;= X_3 \\end{align*}\\] If we simplify, we get the following equations: \\[\\begin{align*} X_0 {}&amp;= (x_1 + x_3 \\cdot\\omega^0) + (x_0 + x_2 \\cdot\\omega^0) = (x_0 + x_1) + \\omega^0(x_2 + x_3) \\\\ X_1 &amp;= (x_1 - x_3 \\cdot\\omega^1) + (x_0 - x_2 \\cdot\\omega^1) = (x_0 + x_1) - \\omega^1(x_2 + x_3) \\\\ X_2 &amp;= (x_1 + x_3 \\cdot\\omega^0) - (x_0 + x_2 \\cdot\\omega^0) = (x_1 - x_0) + \\omega^0(x_3 - x_2) \\\\ X_3 &amp;= (x_1 - x_3 \\cdot\\omega^1) - (x_0 - x_2 \\cdot\\omega^1) = (x_1 - x_0) - \\omega^1(x_3 - x_2) \\\\ \\end{align*}\\] Here is a naive implementation of FFT in R code using Cooley-Tukey algorithm: fft &lt;- function(x) { # Signal Vector N = length(x) if (N == 1) return(x) if (N %% 2) { x = append(x, 0); N = N + 1 } X = seq(1, N, 1) odd = x[c(FALSE, TRUE)] even = x[c(TRUE,FALSE)] X_e = fft(even) X_o = fft(odd) for (k in 1:(N/2)) { omega = exp(-2i*pi*(k-1)/N) X[k] = X_e[k] + X_o[k] * omega X[k + (N/2)] = X_e[k] - X_o[k] * omega } return( X ) } x = c(0.00, 0.10, 0.40, 0.10) fft(x) ## [1] 0.6+0i -0.4+0i 0.2+0i -0.4+0i The R code implementation of the Cooley-Tukey algorithm shows a recursive way of dividing the signal data and computing for the frequency. We can also represent the algorithm using a Butterfly Diagram. See Figure 4.41. Figure 4.41: Cooley-Tukey (FFT) Algorithm There are other Fourier Transforms published in other pieces of literature that are notable for investigation. We leave three of them for readers to investigate: Sparse Fourier Transform (SFT) Discrete Tchebichef (Chebyshev) Transform (DTT) Fractional Fourier Transform (FrFT) 4.8 Summary This chapter discussed Integration, Ordinary Differential Equations, Partial Differential Equations, and Fourier Series. These essential topics have become a classic standard part of most, if not all, Calculus syllabi. As the next step, we leave readers to take a further reading of such topics in detail. Other great references include Olsen-Kettle L. (2011), Lewis A.D (2017), Jakobsen P.K. (2019), and Ivrii V. (2021). As shown, Numerical methods deal with approximations. We illustrated some of the classic approaches to approximate the behavior of dynamic systems. We also demonstrated how to arrive at a steady state of systems from an initial dynamic state and vice-versa. In volume II of this book, we begin to discuss approximations in the context of Stochastic methods, covering three major topics, namely Probability, Statistical Computation, and Bayesian Computation. wikipedia: https://en.wikipedia.org/wiki/Newton%E2%80%93Cotes_formulas↩ "],["numericalprobability.html", "Chapter 5 Probability and Distribution 5.1 Approximation based on Random Chances 5.2 Distribution 5.3 Mass and Density 5.4 Probability 5.5 Probability Density Function (PDF) 5.6 Probability Mass function (PMF) 5.7 Cumulative Distribution Function (CDF) 5.8 Special Functions 5.9 Types of Distribution 5.10 Summary", " Chapter 5 Probability and Distribution We covered many Numerical methods in previous chapters of Volume I, providing approximations to solutions that otherwise tend to be impractical or impossible to attain. In the chapters ahead in Volume II, we begin to review approximations of random chances (or random events). Unlike approximating linear or non-linear functions, we focus on approximate functions that follow the distribution of data. Note that there are cases when we only have a subset of the whole data. In that respect, we analyze the sample and produce an estimate or approximate analysis that may closely explain the complete set. Particularly, in this chapter, we continue to focus on approximations in the context of Probabilities and Distribution as we reference the great works of Vapnik V. (2000), Press W.H et al. (2007), Stewart W.J. (2009), Murphy K.P. (2012), Forsyth D. (2018), and Lambert B. (2018), along with other additional references for consistency. 5.1 Approximation based on Random Chances Essentially, when we speak about data, we rely upon a measured value (whether in quantity or quality). Given some sample data, \\(\\Omega = \\{0.11, 0.14, 0.09, 0.11, 0.15, ... \\}\\), we may be dealing with a set of random measurements of observations. Thus, we have to ask the following questions: How many observations have we sampled? Here, we take the number of observations. How many of those observations are unique? Here, we take the cardinality of the observation - a measure of uniqueness. The higher the uniqueness, the higher the cardinality. Can we group non-unique observations? If so, how many groups would there be? Do these groups have the same number of observations? Here, we now look at central tendencies such as mean, median, and mode. How many groups do we have? What group(s) have the highest number of similar observations, and what group(s) have the lowest number of similar observations? Here, we begin to measure the spread or variance. Finally, can we get a picture of the groupings? Here, we want to plot the groupings - the distribution. In other words, we want to know all about how data is distributed. We also want to use charts to visualize the distribution. However, before doing so, let us first review a few terms: Random variables hold numerical values resulting from a random outcome or random chance, e.g., tossing a coin five times. What is the chance of getting a tail each time - could it be 0.50 (50% chance)? Orthogonal random variables are independent random variables. A change in one random variable does not affect the other; in other words, one is completely orthogonal to the other. Covariates or Variates hold values of random variables (one being for independent variables and the other being for dependent variables). So if we toss a coin and get a tail, the outcome of having a tail is considered a random outcome or, to be more specific, a random variate. On the other hand, a covariate is regarded as a random outcome that is statistically dependent on the independent variable. Factor variables are categorical variables that hold categorical values called levels. So if we are talking about gender as a factor (or categorical) variable, then male is a level, and female is also a level. As for random variables, because of the absence of concrete and precise measurements, the distribution of values of random variables requires analysis. We commonly use two distributions to explain data distribution in other literature: Uniform and Normal distribution. Uniform distribution of random variables tends to follow a rectangular shape as the size of sampled data increases. Figure 5.1 shows chart of a uniform distribution. Below is the R code that plots the chart. set.seed(142) x = runif(n=5000, min=-1.5, max=1.5) hist(x, breaks=20, prob=TRUE, main=&#39;Uniform Distribution&#39;, xlab=&#39;Random Variable&#39;, xlim=range(-1.5,1.5)) Figure 5.1: Uniform Distribution Normal distribution of random variables tends to follow a bell shape as the size of sampled data increases. set.seed(142) x = rnorm(n=5000,mean=0,sd=1) hist(x, breaks=20, prob=TRUE, main=&#39;Standard Normal Distribution&#39;, xlab=&#39;Standard Deviation&#39;, xlim=range(-4,4)) curve(dnorm(x, mean=0, sd=1),add=TRUE, lwd=2, col=&quot;darksalmon&quot;) Figure 5.2: Standard Normal Distribution In the rest of this book, we use uniform and normal distribution to simulate most of our data distribution. We emphasize, however, that in practice, not all data are distributed uniformly or normally. We will encounter different kinds of distribution. To name a few, we cover Logistic distribution, Gamma distribution, Beta distribution, Poisson distribution, and Power-law distribution. Furthermore, some distributions do not follow the characteristics of uniform or normal distribution. Because of that, we use some clever means to simulate the distribution. We will discuss more on that in the next chapter. Also, note that there are cases when we need to take multiple samples of a population. Instead of dealing with one group of sampled data, we deal with numerous groups of sampled data. This grouping (or sampling) of sampled data can form some distribution. We regard these samples as sampling distribution, which we will cover later. Also, note that we are literal when referring to data distribution as the distribution of data. As we mostly deal with statistics, each data attribute is assumed to be already measured. Each computed measure of one attribute, such as the average temperature in Fahrenheit, is what we regard as a statistic. In terms of the number of variables and number of outcomes per variable, we can categorize them into the following: Univariate - refers to dealing with one variable. Multivariate - refers to dealing with multiple variables. Bivariate - refers to dealing with two variables. Binomial - refers to a response variable with only two possible outcomes (dichotomous), e.g., 1 or 0, true or false. Multinomial - refers to a response variable with more than two possible outcomes, e.g., nominal values 5.2 Distribution To understand data distribution, we need to be familiar with a couple of terms. We start with the term stochastic, which is a property commonly associated with randomness of events. Then, we have stochastic process, which is the process that generates the randomness of events, leading to an observed outcome in the form of distribution, e.g., tossing a coin renders a binomial distribution. Here, we are interested to know how data is distributed, such as below: set.seed(142) h = rnorm(n=5000,mean=5.5,sd=1) hist(x=h, breaks=20, prob=FALSE, main=&#39;Normal Distribution&#39;, xlab=&#39;Spread&#39;, ylab=&quot;Number of Individuals&quot;) Figure 5.3: Height Distribution Intuitively, in Figure 5.3, we see a peak (the highest point) centered in the middle of the chart in the distribution. Here, the peak indicates the highest concentration of data in the distribution. To put that into perspective, suppose we sample the height of a given population and use a histogram similar to Figure 5.3. We should notice that the highest point in the histogram seem to be between 5 and 6 feet in height. In actuality, the average is at 5.5 ft - that is the mean denoted as \\(\\mu_{\\vec{x}}\\). Note that the mean is also often referred to as the expected value or expectation denoted as \\(\\mathbb{E}(\\vec{x})\\). However, expectation has another representation especially when random variables are weighted - see Probability Rules in Chapter 7 (Bayesian Computation I). If we then try to determine the next highest peak after the mean, we may see a 5.0 ft and a 6.0 ft. And as we look for the next highest peak after 5.0 ft or 6.0 ft, we then see 2.0 ft and 8.0 ft. The count is much less than the highest peak and the second-highest peak. It gets lesser as we keep counting for the next highest count. The spread of the distribution spans from 2ft through 8 ft. - that is the variance denoted as \\(Var(\\vec{x}) = \\mathbb{E}(\\vec{x}^2) - \\mathbb{E}(\\vec{x})^2\\). The illustration above characterizes a normal distribution. We start from a peak - in the center - and then the bars taper off, gradually decreasing in height almost symmetrically on each side. Two parameters characterize our normal distribution: mean and variance. Here, mean is given the symbol \\(\\mu\\) and variance is given the symbol \\(\\sigma^2\\). The notation for a normal distribution is then expressed this way: \\[\\begin{align} X \\sim N(\\mu, \\sigma^2 ) \\end{align}\\] It reads: X is distributed as a normal distribution with mean and variance as the parameters. It also can be read as: the random variable X follows a normal distribution with mean and variance parameters. Here, the mean parameter for a normal distribution is needed to see the most popular item in the data. We need the variance parameter to understand the popularity of items by ranking them according to popularity. In some way, this gives us a picture of how items are spread out based on rank. Graphically, if we use a bar chart, most popular items have taller ranks (taller bars) than low-ranking items. Figure 5.4: Height Distribution Figure 5.4 shows four un-normalized distributions. Notice that the un-normalized distribution with \\(\\mu=50\\) has a peak around 50 in the chart. Notice that the un-normalized distribution with \\(\\sigma^2=50\\) has a wider spread. In many cases, when we deal with data, we may prefer to normalize the distribution. Furthermore, on top of that, we may also prefer to standardize the normalized distribution. Figure 5.5: Height Distribution As shown in figure 5.5, there is one particular kind of normal distribution called standard normal distribution. A Standard Normal Distribution is a distribution that is both scaled down and centered so that its mean is forced down to zero, e.g. \\((x - \\mu)/\\sigma^2\\). set.seed(142) x = rnorm(n=5000,mean=0,sd=1) hist(x, breaks=20, prob=TRUE, main=&#39;Standard Normal Distribution&#39;, xlab=&#39;Standard Deviation&#39;, xlim=range(-4,4)) curve(dnorm(x, mean=0, sd=1),add=TRUE, lwd=2, col=&quot;darksalmon&quot;) Figure 5.6: Standard Normal Distribution 5.3 Mass and Density Mass refers to the data distribution formed by discrete random variables. Density refers to the data distribution formed by continuous random variables. Geometrically, a Density represents a curve formed by a Density function, which we discuss later; e.g., the curve that forms the bell shape in a normal distribution. See Figure 5.8. Here, we use the greek symbol (\\(\\rho\\)) for Density, and it reads as rho: \\[ \\rho(x) \\] Although from time to time, we may also use the notation: \\(f(x)\\). Recall that when talking about distribution, mathematically, we use the following notation (e.g., for normal distribution): \\[\\begin{align} X \\sim \\mathcal{N}(\\mu, \\sigma^2 ) \\end{align}\\] We can use a density notation like so: \\[\\begin{align} \\rho(x) \\sim \\mathcal{N}(\\mu, \\sigma^2 ) \\end{align}\\] We can also use a parameterized notation to be more complete: \\[\\begin{align} \\rho(x|\\mu,\\sigma^2) \\sim \\mathcal{N}(\\mu, \\sigma^2 ) \\end{align}\\] which reads: the density of x is distributed as normal distribution - the first notation having the parameters for \\(\\mu\\ and\\ \\sigma^2\\) can be implicit; meaning, we do not have to show the parameters in the notation. 5.4 Probability Probability is a measure of the number of occurrences of a random event. That is the non-Bayesian definition. Bayesian definition puts a different perspective; instead, that Probability measures the degree of uncertainty (the likelihood) of information. After all, when we say that there is a possible chance that a certain event will occur, we claim that we are uncertain about the event to occur. Therefore, the higher the chance an event would occur, the lower our uncertainty of the information. When dealing with all probabilities of a random event, we know that the totality comprises the whole. In that respect, we express Probability in terms of Proportionality. For example, in the world population, what is the proportion of vegans vs. non-vegans if there are 1 billion vegans out of 7 billion? The proportion is about 14.29% (1/7). One of the better exercises when working on proportions (and probability) is developing our familiarity with the different distributions (densities) of data. It is simply about how distributed our data can be. Then, we estimate the probability that a given event (or proportion) falls somewhere in the distribution. To plot this, let us recall the description of a function in Calculus in which we show how a function drives the slope of a line or the curvature of a curve (or shape of hyperplanes in higher dimensional space). Let us start with a curve using a simple quadratic function: \\(f(x) = x^2 \\text{, where } 0 \\leq x \\leq 1\\). See figure 5.7. suppressWarnings(suppressMessages(library(scales))) slope = 6 # Generate X-Axis x = seq(0, 1, .01) # Formulate the functions for a curve with 0 intercept so that the # curve touches the y-axis at zero. intercept = 0 quadratic_function = x^2 + intercept # Plot the curve, the secant, and the tangent plot(x, quadratic_function, lwd=1, pch=16, col=alpha(&quot;navyblue&quot;, 0.0), main=&quot;Area bounded by a curve and a line&quot;, xlab=&quot;X-Axis&quot;, ylab=&quot;Y-Axis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) # The Curve lines(x, quadratic_function, col=alpha(&quot;navyblue&quot;,1), lwd=1) text(x=.8, y=.5, label=&quot;f(x) = x^2&quot;) # The Area Limits a=.2; b=.4; c=.6 abline(v=c(a,c), lty=3, col=&quot;darksalmon&quot;) # The Area for P(a &lt; x &lt; c) p = which(x &gt; a &amp; x &lt; c) y1 = rep(0, length(quadratic_function[p])) y2 = quadratic_function[p] segments(x[p], y1, x[p], y2, lwd=1, col=&quot;darksalmon&quot; ) text(x=c, y=.65, label=&quot;c&quot;) text(x=.4, y=.65, label=expression(&quot;P(a &lt;= X &lt;= c)&quot;), lty=.5, cex=.9) segments(a, 0, a, .6, lwd=2, col=&quot;steelblue&quot; ) segments(c, 0, c, .6, lwd=2, col=&quot;steelblue&quot; ) # The Area for P(a &lt; x &lt; b) p = which(x &gt;= a &amp; x &lt; b) y1 = rep(0, length(quadratic_function[p])) y2 = quadratic_function[p] segments(x[p], y1, x[p], y2, lwd=2, col=&quot;steelblue&quot; ) text(x=a, y=.65, label=&quot;a&quot;) text(x=b, y=.45, label=&quot;b&quot;) segments(b, 0, b, .4, lwd=2, col=&quot;steelblue&quot; ) text(x=.3, y = .25, label=&quot;P(a &lt;= X &lt;= b)&quot;, lty=.5, cex=.9) Figure 5.7: Area under Density Curve The totality of a continuous density in terms of proportion equates to 1 given the following formula: \\[\\begin{align} \\int_{-\\infty}^{\\infty} f(x) dx = 1 \\end{align}\\] We can also say that the total proportion or probability (all probable outcome) of a density unites into 1. There are no other outcomes after accounting for all other probable outcomes; hence, the 1. Similarly, the probability of a discrete distribution of data equates to 1 given the following formula: \\[\\begin{align} \\sum_{i\\to0}^{1} P(x) = 1 \\end{align}\\] As an example, if we claim that vegans represent the 14.3 proportion of the world population, that is 0.143 of the total sum of 1. Therefore, 0.857 are non-vegans. In figure 5.7, we are only interested in the proportion between limit a and b. The probability (or area) bounded by a and b is: \\[\\begin{align} \\int_{a}^{b} f(x) dx {}&amp;= \\int_{0.2}^{0.4} x^2 dx \\\\ &amp;= F(f&#39;(b)) - F(f&#39;(a))\\\\ &amp;=\\lim_{x\\to b} F(x)\\ - \\lim_{x\\to a} F(x)\\\\ &amp;= \\left[\\frac{x^3}{3} + c\\right]_{0.2}^{0.4} = \\left(\\frac{0.4^3}{3}\\right)\\ - \\left(\\frac{0.2^3}{3}\\right) \\nonumber \\\\ &amp;= 0.0187 \\nonumber \\end{align}\\] Similarly, the probability (or area) bounded by a and c is calculated as such: \\[\\begin{align} \\int_{a}^{c} f(x) dx {}&amp;= \\int_{0.2}^{0.6} x^2 dx \\\\ &amp;= F(f&#39;(c)) - F(f&#39;(a))\\\\ &amp;=\\lim_{x\\to c} F(x)\\ - \\lim_{x\\to a} F(x)\\\\ &amp;= \\left[\\frac{x^3}{3} + c\\right]_{0.2}^{0.6} = \\left(\\frac{0.6^3}{3}\\right)\\ - \\left(\\frac{0.2^3}{3}\\right) \\nonumber\\\\ &amp;= 0.0693 \\nonumber \\end{align}\\] The plot shows a synthetic (only made up) quadratic curve for a density and does not reflect one of the typical distributions, as we shall see in the next sections. Nevertheless, this is to demonstrate probability and proportionality. We regard a bounded region as a proportion (or probable outcome) sliced between boundaries. The region or area between boundaries represents the range of probable outcomes. We see more of this topic when discussing cumulative distribution. 5.5 Probability Density Function (PDF) Geometrically, the probability density function is a continuous function that describes the shape (or curvature) of a distribution. For example, in Figure 5.8, we see a bell shape density curve. Our probability density function generates this; hereafter, we use the term density function. A density function is expressed in the below general form using a lower-case notation f(x): \\[\\begin{align} f_X(x) = P(X = x) \\end{align}\\] An example of a density function is the normal (Gaussian) PDF expressed in the below equation: \\[\\begin{align} f(x; \\mu, \\sigma) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right] \\end{align}\\] with the following sample R code implementation: normal.pdf &lt;- function(x, mean, sd ) { # Gaussian / Normal Distribution variance = sd^2 (1 / (sqrt( 2 * pi * variance ))) * exp(-(x - mean)^2/(2 * variance)) } plot(NULL, xlim=range(-3,3), ylim=range(0,0.5), xlab=&quot;spread (variance)&quot;, ylab=&quot;probability density&quot;, main=&quot;Gaussian PDF&quot;, axes=FALSE, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;red&quot;, lty=2, lwd=2) curve(normal.pdf(x, mean=0, sd=1), col=&quot;darksalmon&quot;, add=TRUE, lwd=2 ) axis(1, -3:3, c(expression(-3*sigma), expression(-2*sigma), expression(sigma),0,expression(sigma), expression(2*sigma),expression(3*sigma))) axis(2) y = normal.pdf(x=0, mean=0, sd=1) segments(0, 0, 0, y, col=&quot;navyblue&quot;, lwd=2) text(0, y+0.02, label=paste(&quot;probability density =&quot;, round(y,7)) ) Figure 5.8: Gaussian PDF Figure 5.8 helps to visualize the probability density of observing data in a normal distribution in which x = 0. Using our implementation of the density function, we get the following result: normal.pdf(x=0, mean=0, sd=1) ## [1] 0.3989423 Alternatively, using a built-in R package dnorm(), we can obtain the same result: dnorm(x=0, mean=0, sd=1) ## [1] 0.3989423 5.6 Probability Mass function (PMF) Geometrically, the probability mass function is a discrete function that describes the height of a distribution with respect to a discrete location (X-value). In an x-y coordinate system, given an X value, one can identify the corresponding Y value using the function similar to PDF, but only this time, we deal with discrete random variable, X. A probability mass function is expressed in the below general form using a lower-case notation f(x): \\[\\begin{align} f_X(x) = P(X = x) \\end{align}\\] Hereafter, we use the term mass function. A mass function is illustrated using a table or a histogram. For example, let us suppose we have the following discrete support, S: \\[ s \\in S\\ \\ \\ \\rightarrow S = \\{\\ -3,-2,-1,\\ 0,\\ 1,\\ 2,\\ 3\\ \\} \\] In R code, we have the following graph: x = -3:3 y = normal.pdf(x, mean=0, sd=1) plot(NULL, xlim=range(-3,3), ylim=range(0,0.5), xlab=&quot;support&quot;, ylab=&quot;probability mass&quot;, main=&quot;Probability Mass Function (Histogram)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;red&quot;, lty=2) curve(normal.pdf(x, mean=0, sd=1), col=&quot;grey&quot;, add=TRUE, lty=2, lwd=2 ) for (i in 1:7) { segments(x[i], 0, x[i], y[i], col=&quot;navyblue&quot;, lwd=7) } y = normal.pdf(x=0, mean=0, sd=1) text(0, y+0.02, label=paste(&quot;probability mass =&quot;, round(y,7)) ) Figure 5.9: Pobability Mass Function To illustrate, let us calculate the probability mass of a normal distribution in which x = 0. Note that our x can only be within the range of our support, X. \\[\\begin{align} f_X(x) = P(X = x),\\ \\ \\ \\ \\ where\\ x \\in \\{\\ -3,-2,-1,\\ 0,\\ 1,\\ 2,\\ 3\\ \\}. \\end{align}\\] We get the following result: normal.pdf(x=0, mean=0, sd=1) ## [1] 0.3989423 Alternatively, using the same built-in R package dnorm(), we can obtain the same result: dnorm(x=0, mean=0, sd=1) ## [1] 0.3989423 5.7 Cumulative Distribution Function (CDF) A cumulative distribution function computes the cumulative probability that a continuous random variable X falls under a value less than or equal to x; hereafter, we use the term distribution function. A distribution function is expressed in the below general form using an upper-case notation F(x): \\[\\begin{align} \\mathcal{F}_X(x) = P(X \\le x) \\end{align}\\] The cumulative distribution describes a monotonic distribution curve. See figure 5.10. normal.cdf &lt;- function(x) { round( pnorm(x, mean=0, sd=1), 2) } q = x = 1 # one unit of standard deviation p = normal.cdf(q) plot(NULL, xlim=range(-3,3), ylim=range(0,1), xlab=&quot;spread/variance (qnorm)&quot;, ylab=&quot;cumulative probability (pnorm)&quot;, main=&quot;Gaussian CDF&quot;, axes=FALSE, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) curve(pnorm(x, mean=0, sd=1), col=&quot;darksalmon&quot;, add=TRUE, lwd=2 ) segments(c(0,-1,1) , c(0, 0, 0), c(0,-1,1), c(normal.cdf(0), normal.cdf(-1), p), col=&quot;red&quot;, lty=2) abline(h=0, col=&quot;red&quot;, lty=2) axis(1, -3:3, c(expression(-3*sigma), expression(-2*sigma), expression(-1*sigma),0,expression(1*sigma), expression(2*sigma),expression(3*sigma))) axis(2) text(2.5, 0.95, label=&quot;cdf curve&quot;, ce=1, col=&quot;blue&quot;) text(0 - 0.65, normal.cdf(0), label=paste0(&quot;(pnorm) = &quot;, normal.cdf(0)), ce=1, col=&quot;black&quot;) text(-1 - 0.65, normal.cdf(-1), label=paste0(&quot;(pnorm) = &quot;, normal.cdf(-1)), ce=1, col=&quot;black&quot;) text(q - 0.65, normal.cdf(q), label=paste0(&quot;(pnorm) = &quot;, normal.cdf(q)), ce=1, col=&quot;black&quot;) text(x + 0.5, 0.03, label=paste0(&quot;(qnorm) = &quot;, q), ce=1, col=&quot;black&quot;) Figure 5.10: Gaussian CDF Base on figure 5.10, we notice three characteristics of a distribution curve: The curve is monotonically increasing to the right. If we increase the support infinitely, we see that the curve flattens infinitely towards 1: \\[\\begin{align} \\underset{x \\rightarrow \\infty}{\\mathrm{lim}}(\\text{CDF}) = 1 \\end{align}\\] If we decrease the support, we see that the curve flattens infinitely towards 0: \\[\\begin{align} \\underset{x \\rightarrow -\\infty}{\\mathrm{lim}}(\\text{CDF}) = 0 \\end{align}\\] Alternatively, a distribution function also computes for the area under the density curve. See figure 5.11. normal.cdf &lt;- function(prob) { qnorm(prob) } area &lt;- function(prob) { # boundaries a = -3; b = normal.cdf(prob) # (quantile function) # area area = seq(a, b, length.out=50) x = c(a, area , b) y = c(0, normal.pdf(area, 0, 1), 0) polygon(x, y, col=&quot;grey&quot;) } plot(NULL, xlim=range(-3,3), ylim=range(0,0.5), xlab=&quot;spread/variance (qnorm)&quot;, ylab=&quot;probability density (dnorm)&quot;, main=&quot;Gaussian CDF&quot;, axes=FALSE, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;red&quot;, lty=2) curve(normal.pdf(x, mean=0, sd=1), col=&quot;darksalmon&quot;, add=TRUE, lwd=2 ) axis(1, -3:3, c(expression(-3*sigma), expression(-2*sigma), expression(-1*sigma),0,expression(1*sigma), expression(2*sigma),expression(3*sigma))) axis(2) area(prob=0.5) text(0, 0.42, label=&quot;pdf curve&quot;, ce=1, col=&quot;blue&quot;) text(-0.6, 0.14, label=&quot;cdf area = 0.5&quot;, ce=1, col=&quot;black&quot;) text(-0.6, 0.09, label=&quot;(pnorm)&quot;, ce=1, col=&quot;black&quot;) text(0.50, 0.02, label=&quot;(qnorm) q=0&quot;) Figure 5.11: Gaussian CDF To illustrate, let us calculate the cumulative density (or cumulative distribution or probability distribution) of a normal distribution in which \\(x = 0\\): \\[\\begin{align} \\mathcal{F}_X(x) = P(X \\le x) = P(X \\le 0) \\end{align}\\] In R code, cumulative density can be computed using pnorm: (p = pnorm(q=0, mean=0, sd=1)) # area under the curve, given quantile=0 ## [1] 0.5 In reverse, get the quantile value given the result of cdf: (q = qnorm(p=0.5, mean=0, sd=1)) # inverse of cdf (or pnorm) ## [1] 0 5.8 Special Functions Before we discuss the different types of distribution in this section, let us first introduce some special functions that are often used in numerical computing for PDFs and CDFs. Here, we exclude the derivation of the functions. We leave readers to investigate those derivations. Other references include Mathai A. M. (1993)] and Gautschi W. (1983). 5.8.1 Gamma function Gamma function extends the scope of factorial function from non-negative integer to real numbers. \\[\\begin{align} \\Gamma(z) = \\int_0^\\infty x^{z-1} e^{-x} dx = (z-1)!\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\Gamma(z+1) = z\\Gamma(z) = z(z-1)! \\end{align}\\] The Gamma function can be split into two regions (see next section). \\[\\begin{align} \\Gamma(z) = \\gamma(z, \\alpha) + \\Gamma(z, \\alpha) \\end{align}\\] 5.8.2 Incomplete Gamma function The term Incomplete refers to the bounded (only partial) region covered by the integration with respect to the parameter, \\(\\alpha\\). There are two regions: The lower region defined by the following equation (where x &gt;=0): \\[\\begin{align} \\gamma(z, \\alpha) = \\int_0^\\alpha x^{z-1}e^{-x} dx,\\ \\ \\ \\ \\ \\ \\ where\\ z &gt; 0 \\end{align}\\] The upper region defined by the following equation (where x &gt;= 0): \\[\\begin{align} \\Gamma(z, \\alpha) = \\int_\\alpha^\\infty x^{z-1}e^{-x} dx,\\ \\ \\ \\ \\ \\ \\ where\\ z &gt; 0 \\end{align}\\] Here, let us use one of a few numerical variations such as the power series to compute for the lower incomplete gamma function (Wikipedia - Incomplete Gamma Function): \\[\\begin{align} \\gamma(z, \\alpha) = \\alpha^z e^{-\\alpha} \\sum_{k=0}^\\infty \\frac{\\alpha^k}{\\Gamma(z+k+1)} \\end{align}\\] With that, we have the following regularized Incomplete Gamma functions: lower tail in a distribution: \\[\\begin{align} P(z, \\alpha) = \\frac{\\gamma(z, \\alpha)}{\\Gamma(z)} = \\frac{1}{\\Gamma(\\alpha)}\\int_0^\\alpha x^{z-1}e^{-x}dx = \\frac{ \\alpha^z e^{-\\alpha} }{\\Gamma(z)} \\sum_{k=0}^\\infty \\frac{\\alpha^k}{\\Gamma(z+k+1)}, \\end{align}\\] and upper tail in a distribution: \\[\\begin{align} Q(z,\\alpha) = \\frac{\\Gamma(z,\\alpha)}{\\Gamma(z)} = \\frac{1}{\\Gamma(\\alpha)}\\int_\\alpha^\\infty x^{z-1}e^{-x}dx = 1 - P(z, \\alpha) \\end{align}\\] Here is a naive implementation of Incomplete Gamma functions in R code: library(pracma) Gamma &lt;- function(n) { factorial(n-1)} GammaInc &lt;- function(alpha, z) { s = 0 limit = 300 flimit = 172 # R&#39;s gamma limit for (k in 0:limit) { if (z + k + 1 &gt; 171) { break } s = s + alpha^k / Gamma( z + k + 1) } lower = alpha^z * Gamma(z) * exp(-alpha) * s upper = Gamma(z) - lower P = lower / Gamma(z) # regular inc gamma Q = upper / Gamma(z) list(&quot;lower&quot;= lower, &quot;upper&quot;=upper, &quot;P&quot;=P, &quot;Q&quot;=Q ) } # Naive implementation t(GammaInc(2,1)) ## lower upper P Q ## [1,] 0.8646647 0.1353353 0.8646647 0.1353353 # Built-in R package (pracma) gammainc(2,1) ## lowinc uppinc reginc ## 0.8646647 0.1353353 0.8646647 Note that in R, the gamma function may not allow numbers over the range limit of 172. A way to work around that is using log gamma (lgamma) with additional adjustments in the expressions. 5.8.3 Digamma Function The Digamma function is the derivative of the log of Gamma function and is written as: \\[\\begin{align} \\Psi(x) = \\frac{d}{dx} \\log_e \\Gamma(x) = \\frac{\\Gamma&#39;(x)}{\\Gamma(x)} \\end{align}\\] We also have Polygamma function as a general extension of Digamma function and written like so: \\[\\begin{align} \\Psi(n,x) = \\frac{d^n}{dx^n} \\log_e \\Gamma(x) = \\frac{\\Gamma^{(n)}(x)}{\\Gamma(x)} \\end{align}\\] 5.8.4 Beta function Beta function is named by Legendre for Euler’s integral of the first kind which relies on Gamma function. \\[\\begin{align} \\mathcal{B}(\\alpha, \\beta) \\equiv \\mathcal{B}(1; \\alpha, \\beta) = \\int_0^1 x^{\\alpha-1}(1-x)^{\\beta-1} dx = \\frac{\\Gamma(\\alpha)\\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)} \\end{align}\\] where \\(\\alpha &gt; 0\\ and\\ \\beta &gt; 0\\). Note that if function is symmetric, then we have: \\[ \\mathcal{B}(\\alpha, \\beta) = \\mathcal{B}(\\beta, \\alpha) \\] 5.8.5 Incomplete Beta function Given the support as \\(0 \\le x \\le 1\\), we have the following function: \\[\\begin{align} \\mathcal{B}_x(\\alpha, \\beta) \\equiv \\mathcal{B}(x; \\alpha, \\beta) = \\int_0^x x^{\\alpha-1}(1-x)^{\\beta-1} dx \\end{align}\\] 5.8.6 Regularized Beta function Regularized Beta function is also called the Normalized Incomplete Beta function and is written as. \\[\\begin{align} I_x(\\alpha, \\beta) \\equiv I(x; \\alpha, \\beta) = 1 - I_{1-x}(\\beta, \\alpha) = \\frac{\\mathcal{B}_x(\\alpha, \\beta)}{\\mathcal{B}(\\alpha, \\beta)} \\end{align}\\] Equivalently, we have: \\[\\begin{align} I_x(\\alpha, \\beta) + I_{1-x}(\\beta, \\alpha) = 1 \\end{align}\\] Also, for the derivations, we have: \\[\\begin{align} I_x(\\alpha, \\beta) \\equiv I(x; \\alpha, \\beta) = \\frac{1}{\\mathcal{B}(\\alpha, \\beta) } \\int_0^x x^{\\alpha-1}(1-x)^{\\beta-1} dx \\end{align}\\] and \\[\\begin{align} I_{1-x}(\\beta, \\alpha) \\equiv I(1-x; \\beta, \\alpha) = \\frac{1}{\\mathcal{B}(\\alpha, \\beta) } \\int_{x}^1 x^{\\alpha-1}(1-x)^{\\beta-1} dx \\end{align}\\] For further discussion of Incomplete Beta functions, see CDF for T distribution in Chapter 6 (Statistical Computation). 5.8.7 Hypergeometric function Hypergeometric function is a special function that computes for the sum of hypergeometric series. Generalized form: \\[\\begin{align} {}_pF_q(\\alpha_1,...,\\alpha_p; \\beta_1,...,\\beta_q; x) = \\sum_{n=0}^\\infty \\frac{(\\alpha_1)_n\\cdots(\\alpha_p)_n}{\\beta_1)_n\\cdots(\\beta_q)_n}\\frac{x^n}{n!} \\end{align}\\] Gauss form: \\[\\begin{align} {}_2F_1(a,b;c; x) = \\sum_{n=0}^\\infty \\frac{(a)_n(b)_n}{(c)_n}\\frac{x^n}{n!} \\end{align}\\] Note that \\((x)_n\\) is a Pochhammer symbol which is expressed as: Falling Factorial (where \\(x \\ge 0\\) and \\(x \\in \\mathbb{R}\\)): \\[\\begin{align} (x)_n = \\frac{\\Gamma(x+n)}{\\Gamma(x - n + 1)} = x(x-1)(x-2) ... (x- n +1) = \\prod_{k=0}^{n-1} (x-k) \\end{align}\\] Rising Factorial (where \\(x \\ge 0\\) and \\(x \\in \\mathbb{R}\\)): \\[\\begin{align} (x)^n = \\frac{\\Gamma(x+n)}{\\Gamma(x)} = x(x+1)(x+2) ... (x+n-1) = \\prod_{k=0}^{n-1}(x+k) \\end{align}\\] A rather expansion form of Hypergeometric function without the Pochhammer symbol is the following: \\[\\begin{align} {}_pF_q(\\alpha_1,...,\\alpha_p; \\beta_1,...,\\beta_q; x) = \\sum_{k=0}^\\infty \\prod_{i=1}^p \\frac{\\Gamma(k+\\alpha_i)}{\\Gamma(\\alpha_i)} \\prod_{j=1}^p \\frac{\\Gamma(\\beta_j)}{\\Gamma(k+\\beta_j)} \\frac{x^k}{k!} \\end{align}\\] and using that to form our Gauss Hypergeometric function: \\[\\begin{align} {}_2F_1(\\alpha_1,\\alpha_2; \\beta_1; x) = \\sum_{k=0}^\\infty \\prod_{i=1}^2 \\frac{\\Gamma(k+\\alpha_i)}{\\Gamma(\\alpha_i)} \\prod_{j=1}^1 \\frac{\\Gamma(\\beta_j)}{\\Gamma(k+\\beta_j)} \\frac{x^k}{k!} \\end{align}\\] The Gauss Hypergeometric function has other transformation formulas (Abramowitz and Stegun 1964): \\[\\begin{align} {}_2F_1(\\alpha,\\beta,c; x) &amp;= (1-x)^{-\\beta} {}_2F_1\\left(c-\\alpha,\\beta;c; \\frac{x}{x-1}\\right)\\ \\ \\ \\leftarrow\\ \\ \\ Pfaff\\\\ &amp;= (1-x)^{-\\alpha} {}_2F_1\\left(\\alpha,c-\\beta;c; \\frac{x}{x-1}\\right)\\ \\ \\ \\leftarrow\\ \\ \\ Pfaff\\\\ &amp;= (1-x)^{c-\\alpha-\\beta} {}_2F_1(c-\\alpha,c-\\beta;c; x)\\ \\ \\ \\leftarrow\\ \\ \\ Euler \\end{align}\\] To illustrate the use of Gamma, Beta, and Hypergeometric functions, see the discussion of CDF in Chapter 6 (Statistical Computation) under t-Distribution. 5.8.8 Continued Fraction Continued Fraction is a finite representation of a rational number in the form of a fraction characterized by a repeating quotient. It has the following format (with \\(\\mathcal{K}\\) indicating a series of continued fractions): \\[\\begin{align} CF(a,b) = b_0 + \\mathcal{K}_{n=1}^\\infty \\frac{a_n}{b_n} = b_0 + \\frac{a_1}{b_1 + \\frac{a_2}{b_2 + \\frac{a_3}{b_3 + ...}}} \\end{align}\\] There are two notations we can use to represent Continued Fraction: \\[\\begin{align} CF(a,b) = ( b_0; b_1, b_2, b_3, ... )\\ \\ \\ \\ and\\ \\ \\ \\ CF(a,b) = \\left[ b_0 + \\frac{a_1}{b_1 +}\\frac{a_2}{b_2 +}\\frac{a_3}{b_3 +} ... \\right] \\end{align}\\] Only for illustration purpose, here is a sample naive (recursive) implementation of Continued Fraction in R code: continued_fraction &lt;- function(n, a, b) { # recursive if (n == 0) return(a/b) b + a / continued_fraction(n-1, a, b) } continued_fraction(n=20,a=2,b=1) ## [1] 2 \\[\\begin{align*} CF(a,b) {}&amp;= b_0 + \\mathcal{K}_{n=1}^\\infty \\frac{a_n}{b_n} = 1 + \\mathcal{K}_{n=1}^{20} \\frac{2}{1} &amp;= b_0 + \\frac{a_1}{b_1 + \\frac{a_2}{b_2 + \\frac{a_3}{b_3 + ...}}} &amp;= 1 + \\frac{2}{1+ \\frac{2}{1 + \\frac{2}{1 + ...}}} = 2 \\end{align*}\\] We leave readers to investigate Lentz’s algorithm used to compute for continued fractions. The next two functions may not necessarily be used in distribution functions; however, it helps to be familiar with them. 5.8.9 Dirac Delta Function The Dirac Delta function is also called the Impulse function. It characterizes a behavior such that it develops into a sudden spike from a prolonged constant state and immediately drops back to a constant state. For example, a sudden change of momentum because of an applied force creates such an impulse. \\[\\begin{align} \\delta (x) = \\begin{cases} 0 &amp; x \\ne 0\\\\ \\infty &amp; x = 0 \\end{cases} \\label{eqn:eqnnumber15} \\end{align}\\] 5.8.10 Kronecker Delta Function The Kronecker delta has the likes of an indicator function in that it outputs one if a variable equals some given value; otherwise, it outputs zero. \\[\\begin{align} \\delta (x) = \\begin{cases} 0 &amp; x \\ne 0\\\\ 1 &amp; x = 0 \\end{cases} \\ \\ \\ \\ \\ \\ \\ \\ \\delta (x) = \\begin{cases} 0 &amp; x \\ne m\\\\ 1 &amp; x = m \\end{cases} \\label{eqn:eqnnumber16} \\end{align}\\] 5.9 Types of Distribution Apart from summarizing data based on quantifying its moments (mean, variance, skewness, kurtosis), it helps characterize the distribution of data based on the geometric shape, size, and thickness of its tails. This section shows the PDF and CDF to describe the distribution. Here, we reference the great works of Walck C. (2007), Press W.H et al. (2007), pp. 321-339, Ross S. (2010), Murphy K.P. (2012), pp. 34-43, and McLaughlin M.P. (2016). Let us start with Bernoulli distribution - a single trial Binomial distribution. 5.9.1 Bernoulli distribution A Bernoulli distribution, named after Jacob Bernoulli, is a discrete probability distribution of a random variable taking a binary outcome which can be 0 or 1, true or false, head or tail of a coin, and is written as: \\[\\begin{align} X \\sim Bernoulli(\\rho)\\ \\ \\ or\\ \\ \\ \\ X \\sim Ber(\\rho) \\end{align}\\] where: \\(\\rho\\) is the approximate probability of success X is the data (random variable). Here, we continue to emphasize the idea of approximation. We give two cases where we show approximation for random events. For example, suppose we make a single attempt to toss a coin with an approximate 50% chance that the coin lands on heads. Here, we have a probability outcome denoted as \\(\\rho\\). If \\(\\rho\\) is the probable outcome of the head, then \\(q = 1 - \\rho\\) is the probable outcome of tails. Being a discrete distribution, the PMF, probability mass function, where \\(\\rho=0.50\\) is written as: \\[\\begin{align} f(x) = \\begin{cases} \\rho &amp; x = 1 \\\\ q = 1 - \\rho &amp; x = 0 \\end{cases} \\label{eqn:eqnnumber17} \\end{align}\\] which can also be written as: \\[\\begin{align} f(x; \\rho) = P(X=x|\\rho) = \\rho^xq^{1-x} = \\rho^x(1-\\rho)^{1-x} \\end{align}\\] For another example, suppose we make just one attempt to roll a six-sided die with a 16% probability that it lands on one side with the number four. Here, let us use \\(\\mathbf{x=1}\\) if the die lands on number four, and \\(\\mathbf{x=0}\\) if the die lands on other numbers. Also, let us consider the following: \\[ \\rho_{\\{4\\}} = 0.16\\ \\ \\ \\ \\ \\ \\ \\ q_{\\{1,2,3,5,6\\}} = 0.84 \\] Using the equation, we get the following: \\[\\begin{align*} f(x; \\rho) {}&amp;= P(X=x|\\rho) = \\rho^xq^{1-x} = \\rho^x(1-\\rho)^{1-x}\\\\ f(x=1; \\rho=0.16) &amp;= P(X=1|\\rho=0.16) = (0.16)^{1}(0.84)^{1-1} = 0.16\\\\ f(x=0; \\rho=0.16) &amp;= P(X=0|\\rho=0.16) = (0.16)^{0}(0.84)^{1-0} = 0.84\\\\ \\end{align*}\\] 5.9.2 Binomial distribution On the other hand, a Binomial distribution is the sum of Bernoulli trials and is written as: \\[\\begin{align} X \\sim Bin(n, \\rho) \\end{align}\\] Note that a Bernoulli distribution is characterized by a single trial (n=1) and a Binomial distribution is characterized by multiple trials (n &gt; 1). The PMF - probability mass function - for a discrete Binomial distribution is expressed as: \\[\\begin{align} f(x; n, \\rho) = P(X = x|n,\\rho) = \\binom{n}{x} \\rho^xq^{n-x} = \\binom{n}{x} \\rho^x(1-\\rho)^{n-x} \\end{align}\\] where: n is the number of independent trials (independent observations) \\(\\rho\\) is the probability of a state, e.g. probability of a coin landing on heads. X is data (random variable), where X \\(\\in\\) {0,1}. and: \\[ \\binom{n}{x} = \\frac{n!}{(n-x)!x!}\\ \\ \\ \\ \\text{(this is a constant)} \\] and: \\[ \\rho^x(1-\\rho)^{n-x}\\ \\ \\ \\ \\ \\text{(this describes the shape of curve)} \\] The PMF is read as a function of random variable x with n and \\(\\rho\\) parameters. The CMF - cumulative mass function - for a discrete Binomial distribution is expressed as: \\[\\begin{align} \\mathcal{F}(x; n, \\rho) = P( X \\le x| n, \\rho) = \\sum_{k=0}^x \\binom{n}{k} \\rho^k(1-\\rho)^{n-k} \\end{align}\\] Because Binomial distribution is discrete, we use summation instead of integration. Note that we use the lowercase f function to indicate PDF/PMF for the rest of this section, and we use the uppercase F function to indicate CDF/CMF. Now to illustrate, suppose we toss a coin ten times. Each time we toss, we record the outcome. In the end, we will be able to record ten outcomes of mixed tails and heads. However, there are two possibilities for each outcome: either a T or an H. Therefore, if we toss a coin ten times, the possible outcome would end up 2^10 = 1024 possible outcomes. That are many possibilities. Let us try to visualize some of the combinations and see what the possible outcome of flipping an H is: Possibility that all ten flips end up to be Tails = 1 count out of 1024 possibilities \\[ T T T T T T T T T T \\] Possibility that all 10 flips end up to be Heads = 1 count out of 1024 possibilities \\[ H H H H H H H H H H \\] Possibility that all 10 flips end up to be Tails except the first = 1 count / 1024 \\[ H T T T T T T T T T \\] Possibility that all 10 flips end up to be Tails except the second = 1 count / 1024 \\[ T H T T T T T T T T \\] If we continue this, we will have to do it for all 1024 possibilities. Let us use combination formula: \\[\\begin{align*} P(Outcome = \\ \\ 0\\ H) {}&amp; = nCr / 1024 = {}_{10}C_0 / 1024 = 1 / 1024\\\\ P(Outcome = \\ \\ 1\\ H) &amp;= nCr / 1024 = {}_{10}C_1 / 1024 = 10 / 1024\\\\ P(Outcome = \\ \\ 2\\ H) &amp;= nCr / 1024 = {}_{10}C_2 / 1024 = 45 / 1024\\\\ P(Outcome = \\ \\ 3\\ H) &amp;= nCr / 1024 = {}_{10}C_3 / 1024 = 120 / 1024\\\\ P(Outcome = \\ \\ 4\\ H) &amp;= nCr / 1024 = {}_{10}C_4 / 1024 = 210 / 1024\\\\ P(Outcome = \\ \\ 5\\ H) &amp;= nCr / 1024 = {}_{10}C_5 / 1024 = 252 / 1024\\\\ P(Outcome = \\ \\ 6\\ H) &amp;= nCr / 1024 = {}_{10}C_6 / 1024 = 210 / 1024\\\\ P(Outcome = \\ \\ 7\\ H) &amp;= nCr / 1024 = {}_{10}C_7 / 1024 = 120 / 1024\\\\ P(Outcome = \\ \\ 8\\ H) &amp;= nCr / 1024 = {}_{10}C_8 / 1024 = 45 / 1024\\\\ P(Outcome = \\ \\ 9\\ H) &amp;= nCr / 1024 = {}_{10}C_9 / 1024 = 10 / 1024\\\\ P(Outcome = 10\\ H) &amp;= nCr / 1024 = {}_{10}C_{10} / 1024 = 1 / 1024 \\end{align*}\\] Now let us plot the distribution of these probable outcomes … random_x &lt;- c(1,10,45,120,210,252,210,120,45,10,1) / 1024 names(random_x) &lt;- seq(0,10,1) barplot(random_x, density=T, col=2, xlab=&quot;H Outcome&quot;, ylim=range(0, 0.25), xlim=range(0,12), ylab=&quot;Probability&quot;, main=&quot;Flip A Fair Coin&quot;) Figure 5.12: Statistics We can derive the same probability of a binomial case using the dbinom(.) function. For example, we can write the probability of getting two heads successfully out of 10 trials given a 0.20 probability threshold as: dbinom(x = 2, size=10, prob=0.20) ## [1] 0.3019899 For the expected value and variance of Binomial distribution, we use the following equations: Expected value: \\[\\begin{align} \\mathbb{E}(X) {}&amp;= \\sum_{x=0}^{n} x(fx) = x \\binom{n}{x} p^n(1-p)^{n-x} = np\\\\ \\mathbb{E}(X^2) &amp;= \\sum_{x=0}^{n} x^2(fx) = x^2 \\binom{n}{x} p^n(1-p)^{n-x} = n(n-1)p^2 + np \\end{align}\\] Variance: \\[\\begin{align} Var(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2 = \\left[ n(n-1)p^2 + np \\right] - (np)^2 = np(1-p) = npq \\end{align}\\] 5.9.3 Multinomial distribution A Multinomial distribution, also called Categorical distribution, models a discrete categorical probability distribution of a random variable taking an outcome with multiple categories which can be 0 to K states, e.g., a die has six possible states (or outcomes), and is written as: \\[\\begin{align} X \\sim Multi(n,\\rho) \\end{align}\\] The PMF for a Multinomial distribution is expressed as: \\[\\begin{align} f(x; n, \\rho) = P(X= x|n, \\rho) {}&amp;= \\frac{n!}{x_1! \\times ... \\times x_k!} \\rho_1^{x_1} \\times ... \\times \\rho_k^{x_k}\\\\ &amp;= \\frac{n!}{\\prod_{i=1}^k x_i!} \\prod_{i=1}^k \\rho_i^{x_i} \\end{align}\\] where: n is the number of independent trials (independent observations) \\(\\rho\\) is the probability of a state, e.g., the probability of a die landing on 4. \\(k\\) is the number of possible states (or outcomes), e.g., a die has six sides. X is a random variable with list of occurrences, e.g. X = ( \\(x_1, x_2,..., x_k\\) ) \\(x_i\\) is the number of occurrences of a state (i); e.g. ‘x_1 = 3’ means there are three occurrences of drawing the first marble from an urn (assuming each marble is labeled with a number). \\(\\rho_i\\) is the probability of state (i), e.g., \\(\\rho_4 = 0.60\\) means a 60% probability of drawing the fourth marble from an urn (assuming each marble is labeled with a number). and: \\[ \\frac{n!}{\\prod_{i=1}^k x_i!}\\ \\ \\ \\ \\ \\ \\text{( the number of possible arrangements)} \\] Here are a few examples that allow a stochastic process to generate multinomial distribution: Probability of getting number 6 after rolling a 6-sided dice. Probability of drawing a red marble out of 6 marbles from an urn. Probability of drawing a blood type of ‘AB’ out of four blood types (e.g., O, A, B, AB) from a list of patients. To illustrate, let us use a typical example. Suppose we draw five marbles - with replacement - from an urn with four marbles - one red marble, two green marbles, and one blue marble. Let us calculate the probability of selecting two red and three green marbles. We have the following: n = 5 draws (trials) k = 3 states (red, green, blue) X = (2 red marbles, 3 green marbles, 0 blue marbles); \\(x_1\\) = 2, \\(x_2\\) = 3, \\(x_3\\) = 0 \\(\\rho\\) = probabilities: (\\(1/4\\) red, \\(2/4\\) green, \\(1/4\\) blue); \\(\\rho_1\\) = 0.25, \\(\\rho_2\\) = 0.50, \\(\\rho_3\\) = 0.25 Using the Multinomial distribution formula: \\[ f(x; n,p) = P(X= x|n,\\rho) = \\frac{5!}{2! \\times 3! \\times 0!} \\times \\left( 0.25^2 \\times 0.50^3 \\times 0.25^0 \\right) \\] let us implement in R code: multinomial.pdf &lt;- function(x, n, p) { ( factorial(n) / prod(factorial(x)) ) * prod ( p^x ) } n = 5 x = c(2, 3, 0) p = c(0.25, 0.50, 0.25) c(&quot;probability&quot;=multinomial.pdf(x, n, p)) ## probability ## 0.078125 We can validate using a built-in R function called dmultinom(). dmultinom(x, size=n, prob=p) ## [1] 0.078125 5.9.4 Geometric distribution Geometric distribution models a discrete distribution of a random variable X, considering the number of failed Bernoulli attempts prior to a successful one. It is written as: \\[\\begin{align} X \\sim Geo(\\rho) \\end{align}\\] The PMF for a Geometric distribution is expressed as: \\[\\begin{align} f(x; \\rho) = P(X=x|\\rho) = q^{(x-1)}\\rho = \\rho(1-\\rho)^{x-1} \\end{align}\\] where: \\(\\rho\\) is the probability of success q is the probability of failure (1-p) The CMF for a Geometric distribution is expressed as: \\[\\begin{align} \\mathcal{F}(x; \\rho) = P(X \\le x|\\rho) = \\begin{cases} 1 - (1-\\rho)^{x} &amp; x \\ge 0\\\\ 0 &amp; x &lt; 0 \\end{cases} \\label{eqn:eqnnumber18} \\end{align}\\] Note that other literature may have the following equations for geometric PDF and CDF respectively instead: \\[\\begin{align} f(x; \\rho) = \\rho(1-\\rho)^{x}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{F}(x; \\rho) = 1 - (1-\\rho)^{x+1} \\end{align}\\] To illustrate, in tossing a coin, compute for the probability that we miss the first four attempts before a successful fifth attempt, granting the probability of a successful attempt is 0.60. \\[ P(X = 5) = P(X \\le 5)^{4}P(X=5) = (0.40)^3(0.60) = 0.01536 \\] where: \\[\\begin{align*} {}&amp;P(X \\le 5)^4 \\ \\ \\ \\leftarrow\\ \\text{first four failed attempts}\\\\ &amp;P(X = 5) \\ \\ \\ \\leftarrow\\ \\text{fifth successful attempt}\\\\ \\end{align*}\\] The expected value and variance is written respectively as: \\[\\begin{align} \\mathbb{E}(X) = 1/\\rho\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Var(X) = \\frac{q}{\\rho^2} \\end{align}\\] 5.9.5 Beta distribution Beta distribution models a continuous distribution which is a special kind of Binomial distribution written as: \\[\\begin{align} X \\sim Beta(\\alpha, \\beta) \\end{align}\\] with the following Beta PDF where support is \\(0 \\le x \\le 1\\): \\[\\begin{align} f(x;\\alpha,\\beta) = P(X = x|\\alpha,\\beta) = \\frac{1}{\\mathcal{B}(\\alpha,\\beta)} x ^{\\alpha-1}(1-x)^{\\beta - 1} \\end{align}\\] where Beta function has the following: \\[\\begin{align} \\mathcal{B}(\\alpha,\\beta) {}&amp;= \\int_0^1 x^{\\alpha-1}(1-x)^{\\beta-1}dx \\\\ \\mathcal{B}(\\alpha,\\beta) &amp;= \\frac{\\Gamma(\\alpha)\\Gamma(\\beta) }{\\Gamma(\\alpha + \\beta) } \\end{align}\\] and where the Gamma function is as follows: \\[\\begin{align} \\Gamma(n) = (n-1)!\\ \\ \\ \\ \\ \\ \\ \\ \\Gamma(n+1) = n\\Gamma(n) = n(n-1)! \\end{align}\\] On the other hand, the CDF of Beta distribution is expressed as: \\[\\begin{align} f(x;\\alpha,\\beta) {}&amp;= P(X \\le x|\\alpha,\\beta) = \\frac{1}{\\mathcal{B}(\\alpha,\\beta)} \\int_0^x x^{\\alpha-1}(1-x)^{\\beta - 1} dx\\\\ \\nonumber \\\\ &amp;= I_x(\\alpha,\\beta) = \\frac{B_x(x; \\alpha, \\beta)}{\\mathcal{B}(\\alpha,\\beta)}. \\end{align}\\] The CDF is a regularized beta function as introduced in the Special functions section. Because Beta distribution is continuous, we use integration instead of summation (such as CMF for Binomial distribution). Here is a naive implementation of PDF and CDF for Beta distribution with the different shapes, \\(\\{\\alpha, \\beta\\}\\) (See also T-distribution in Chapter 6 (Statistical Computation) for an alternative implementation of \\(\\mathbf{Ix(\\alpha,\\beta)}\\)): Gamma &lt;- function(n) { factorial(n-1)} B &lt;- function(alpha, beta) { # also can use built-in, beta(a,b) (Gamma(alpha) * Gamma(beta)) / Gamma(alpha + beta ) } incomplete_beta &lt;- function(x,a,b) { pbeta(x,a,b) * B(a,b) } Ix &lt;-function(x, a, b) { #regulrized beta function, Ix(x; a, b) incomplete_beta(x,a,b) / B(a,b) } beta_pdf &lt;- function(x, alpha, beta ) { 1/B(alpha,beta) * ( x^(alpha-1) * (1-x)^(beta-1) ) } beta_cdf &lt;- function(x, alpha, beta ) { Ix(x, alpha, beta) } # probability density plot(NULL, xlim=range(0:1), ylim=range(0,3), xlab=&quot;support&quot;, ylab=&quot;probability density&quot;, main=&quot;PDF (Beta Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,1, length.out=20) curve(beta_pdf(x, alpha=1, beta=4), col=&quot;purple&quot;, add=TRUE ) curve(beta_pdf(x, alpha=2, beta=2), col=&quot;darksalmon&quot;, add=TRUE ) curve(beta_pdf(x, alpha=2, beta=4), col=&quot;navyblue&quot;, add=TRUE ) curve(beta_pdf(x, alpha=5, beta=1), col=&quot;red&quot;, add=TRUE ) curve(beta_pdf(x, alpha=5, beta=3), col=&quot;brown&quot;, add=TRUE ) Figure 5.13: Beta Distribution (Probability Density) # cumulative density plot(NULL, xlim=range(0:1), ylim=range(0,1), xlab=&quot;support&quot;, ylab=&quot;cumulative probability&quot;, main=&quot;CDF (Beta Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,1, length.out=20) curve(beta_cdf(x, alpha=1, beta=4), col=&quot;purple&quot;, add=TRUE ) curve(beta_cdf(x, alpha=2, beta=2), col=&quot;darksalmon&quot;, add=TRUE ) curve(beta_cdf(x, alpha=2, beta=4), col=&quot;navyblue&quot;, add=TRUE ) curve(beta_cdf(x, alpha=5, beta=1), col=&quot;red&quot;, add=TRUE ) curve(beta_cdf(x, alpha=5, beta=3), col=&quot;brown&quot;, add=TRUE ) Figure 5.14: Beta Distribution (Cumulative Density) Let us save further discussion of Beta distribution until we get to the Bayesian Computation to cover Conjugate and Joint distributions in which one distribution is chained to another. \\[\\begin{align} X_{Pr} \\sim Beta(\\alpha, \\beta)\\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ X \\sim Bin(n, X_{Pr}) \\end{align}\\] Also, we leave readers to investigate on Pert distribution which requires a minimum and a maximum parameter for Beta distribution: \\[\\begin{align} X \\sim Beta(min, max, \\alpha, \\beta) \\equiv Pert(min, max, \\alpha, \\beta) \\end{align}\\] 5.9.6 Dirichlet distribution Dirichlet distribution models a continuous distribution and is a special kind of Multinomial distribution written as: \\[\\begin{align} X_{Pr} \\sim Dir(\\alpha) \\end{align}\\] Like dealing with Beta distribution, which is related to Binomial distribution, the Dirichlet distribution is related to Multinomial distribution. The PDF for Dirichlet distribution with support \\(\\{x_1,..., x_k\\}\\) and \\(0 \\le x_i \\le 1\\) and \\(\\sum(X) = 1\\) is expressed as: \\[\\begin{align} f(X_k;\\alpha_k) = P(x_1,...,x_k|\\alpha_1, ... \\alpha_k) = \\frac{1}{\\mathcal{B}(\\alpha)} \\prod_{i=1}^k {x_i}^{\\alpha_i-1} \\end{align}\\] where the Beta function, \\(\\mathbf{\\mathcal{B}(\\vec{\\alpha})}\\), is equivalent to that of Beta distribution : \\[\\begin{align} \\mathcal{B}(\\alpha) &amp;= \\frac{\\prod_{i=1}^k \\Gamma(\\alpha_i)} {\\Gamma(\\sum_{i=1}^k \\alpha_i)} \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{B}(\\alpha_1, \\alpha_2) = \\frac{\\Gamma(\\alpha_1)\\Gamma(\\alpha_2)}{\\Gamma(\\alpha_1 + \\alpha_2)} \\ \\ \\ \\ \\text{if k=2} \\end{align}\\] and where the Gamma function is as follows: \\[\\begin{align} \\Gamma(n) = (n-1)!\\ \\ \\ \\ \\ \\ \\ \\ \\Gamma(n+1) = n\\Gamma(n) = n(n-1)! \\end{align}\\] Note that Dirichlet distribution is a generalization of Beta distribution. To illustrate, we can continue to use Dirichlet PDF against Binomial distribution where \\(\\{\\alpha,\\beta\\} = \\{ \\alpha, \\alpha \\} = \\{\\vec{ \\alpha} \\}\\) Figure 5.13 illustrates graphs of the different shapes, \\(\\{\\vec{ \\alpha} \\}\\), of Dirichlet distribution. Gamma &lt;- function(n) { factorial(n-1)} dirichlet_B &lt;- function(alpha) { prod(Gamma(alpha)) / Gamma( sum (alpha)) } dirichlet_pdf &lt;- function(x, alpha) { # naive implementation # using binomial distribution, e.g. x (%success), 1-x (%fail) 1/dirichlet_B(alpha) * ( x^(alpha[1]-1) * (1-x)^(alpha[2]-1) ) } plot(NULL, xlim=range(0:1), ylim=range(0,3), xlab=&quot;support&quot;, ylab=&quot;probability density&quot;, main=&quot;PDF (Dirichlet Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,1, length.out=20) curve(dirichlet_pdf(x, c(1,4)), col=&quot;purple&quot;, add=TRUE ) curve(dirichlet_pdf(x, c(2,2)), col=&quot;darksalmon&quot;, add=TRUE ) curve(dirichlet_pdf(x, c(2,4)), col=&quot;navyblue&quot;, add=TRUE ) curve(dirichlet_pdf(x, c(5,1)), col=&quot;red&quot;, add=TRUE ) curve(dirichlet_pdf(x, c(5,3)), col=&quot;brown&quot;, add=TRUE ) Figure 5.15: Dirichlet Distribution Similarly, we further cover Dirichlet distribution in Chapter 7 (Bayesian Computation I) when discussing Conjugate distribution. Also, the idea is about one distribution in which the parameters are based on the outcome of another distribution. \\[\\begin{align} X \\sim Mult(n, \\rho)\\ \\ \\ \\leftarrow \\ \\ \\ \\ \\ \\rho \\sim Dir(\\alpha) \\end{align}\\] 5.9.7 Exponential distribution Exponential distribution models a continuous distribution of a random variable X, taking into account the waiting (or elapsed) time between events. It is written as: \\[\\begin{align} X \\sim Expo(\\lambda) \\end{align}\\] The PDF of an Exponential distribution has the support condition: \\[\\begin{align} f(x;\\lambda) = \\begin{cases} \\lambda e^{-\\lambda x} &amp; x \\ge 0\\\\ 0 &amp; x &lt; 0 \\end{cases}. \\label{eqn:eqnnumber19} \\end{align}\\] Therefore, with support \\(0 \\le x \\le \\infty\\), we get: \\[\\begin{align} P(X = x|\\lambda) = \\lambda e^{-\\lambda x} \\end{align}\\] where: \\(\\lambda\\) is a shape parameter describing event rate, \\(\\lambda = 1/t\\), e.g. 1 event per avg. time. t is the average wait time (or average elapsed time prior to an event occurring) and the CDF of an Exponential distribution is expressed as: \\[\\begin{align} \\mathcal{F}(x; \\lambda) = P(X \\le x|\\lambda) = 1 - e^{-\\lambda x} \\end{align}\\] Exponential growth and decay are two typical events in which we can use Exponential distribution to measure the expected time. To illustrate, here are three examples of events in which we can form an Exponential distribution: Suppose we plant a pumpkin seed about an inch into fertile soil. Then, we compute the time it takes for the seed to germinate. Hint: does a pumpkin seed germinate in a week? Suppose we procure a piece of enterprise-grade computer equipment. Then, we compute the time it takes before the equipment starts to fail. Hint: does it take three to five years for equipment support to expire? Suppose we arrive at a gas station but have to wait for our turn to fill up gas. Then, we compute the time it takes to wait. Here, \\(\\lambda\\) (lambda) is the expected time for events to occur. It answers the question: How long? Below is a naive implementation of PDF and CDF of Exponential distribution in R code. Here we use \\(\\lambda = 0.5\\) and \\(x = 4\\) to show a larger area. See Figure 5.17. exp_pdf &lt;- function(x, lambda ) { lambda * exp(-lambda * x) } exp_cdf &lt;- function(x, lambda) { 1 - exp(-lambda * x) } exp_area &lt;- function(x, lambda) { a = 0; b = x # boundaries area = seq(a, b, length.out=50) # area x = c(a, area , b) y = c(0, exp_pdf(area, lambda), 0) polygon(x, y, col=&quot;lightgrey&quot;) } #Plotting PDF and CDF (Area for lambda=0.5) plot(NULL, xlim=range(0:5), ylim=range(0,2.0), xlab=&quot;support&quot;, ylab=&quot;probability density&quot;, main=&quot;PDF (Exponential Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) p = exp_cdf(x=4, lambda=0.5) exp_area(x=4, lambda=0.5) curve(exp_pdf(x, lambda=0.5), col=&quot;navyblue&quot;, add=TRUE ) curve(exp_pdf(x, lambda=1.0), col=&quot;red&quot;, lty=2, add=TRUE ) curve(exp_pdf(x, lambda=1.5), col=&quot;brown&quot;, lty=2, add=TRUE ) curve(exp_pdf(x, lambda=2.0), col=&quot;darksalmon&quot;, lty=2, add=TRUE ) text(3,0.3, label=expression(paste(&quot;pdf=&quot;, lambda * e^(-lambda * x)))) text(0.6, 0.2, label=&quot;(cdf)&quot;, col=&quot;black&quot;) text(0.7, 0.1, label=paste(&quot;Area = &quot;, round(p,5), sep=&quot;&quot;), ce=1, col=&quot;black&quot;) text(4.2, 0.12, label=&quot;x=4&quot;) Figure 5.16: Exponential Distribution (PDF) #Plotting CDF plot(NULL, xlim=range(0:5), ylim=range(0,1.0), xlab=&quot;support&quot;, ylab=&quot;cumulative density&quot;, main=&quot;CDF (Exponential Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,5, length.out=500) curve(exp_cdf(x, lambda=0.5), col=&quot;brown&quot;, add=TRUE ) curve(exp_cdf(x, lambda=1.0), col=&quot;red&quot;, add=TRUE ) curve(exp_cdf(x, lambda=1.5), col=&quot;darksalmon&quot;, add=TRUE ) curve(exp_cdf(x, lambda=2.0), col=&quot;navyblue&quot;, add=TRUE ) Figure 5.17: Exponential Distribution (CDF) To illustrate further, let us use one of the examples given. Suppose we wait to fill up gas at a gas station. Let us compute the probability of waiting for less than 5 minutes given that \\(\\lambda = 1/2\\). That gives us the following problem statement: \\[\\begin{align} P(X \\le x) = P(X \\le 5) = 1 - e^{-\\lambda x} \\end{align}\\] were average time to wait = 2 minutes. Here is the implementation of CDF in R code: sum ( exp_cdf(x=c(4),lambda=0.5) ) # our code ## [1] 0.8646647 (p = sum(pexp(q=c(4), rate=0.5))) # R&#39;s built-in package ## [1] 0.8646647 Both outcomes give around 86.47% probability for us to wait for 4 minutes only. On the other hand, if the average wait time is 10 minutes instead. Then we get: sum ( exp_cdf(x=c(4),lambda=0.2) ) # our code ## [1] 0.550671 (p = sum(pexp(q=c(4), rate=0.2))) # R&#39;s built-in package ## [1] 0.550671 Both outcomes give around 55.07% probability for us to wait for 4 minutes only. In terms of expected value and variance, we have the following expression: Expected value: \\[\\begin{align} \\mathbb{E}(X) = \\int_0^\\infty x\\lambda e^{-\\lambda x} dx = \\left[\\frac{e^{-\\lambda x}}{\\lambda}\\right]_0^\\infty = \\frac{1}{\\lambda} \\end{align}\\] Variance: \\[\\begin{align} Var(X) = \\mathbb{E}({X}^2) - \\mathbb{E}(X)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2} \\end{align}\\] We now discuss the next type of distribution - the Gamma distribution. It is notable to mention that Exponential distribution and Gamma distribution are somewhat related. While Exponential distribution is about waiting time between events of interest, Gamma distribution is waiting time taken for number of events. 5.9.8 Gamma distribution Gamma distribution models a continuous distribution of a random variable X taking into account the number of events that occurred after wait (or elapse) time and is written as: \\[\\begin{align} X \\sim Gamma(\\alpha, \\beta)\\ \\ \\ \\ or\\ \\ \\ \\ \\ X \\sim \\Gamma(\\alpha, \\beta) \\end{align}\\] Any queueing system involving wait times or any events that can be measured in terms of elapsed time are two common examples in which using Gamma distribution is helpful. The PDF of a Gamma distribution with support \\(x \\ge 0\\) is expressed as: \\[\\begin{align} f(x; \\alpha,\\beta) = P(X = x|\\alpha, \\beta) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\frac{x}{\\beta}} = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\beta x} \\end{align}\\] where the Gamma function is expressed as: \\[\\begin{align} \\Gamma(\\alpha) {}&amp;= \\int_0^\\infty x^{\\alpha-1} e^{-x} dx \\\\ \\Gamma(n) &amp;= (n-1)! \\\\ \\Gamma(n+1) &amp;= n\\Gamma(n) = n(n-1)! \\end{align}\\] also where: \\(\\alpha\\) is the number of events that occurred \\(\\beta\\) is the average number of events per time. It is equivalent to \\(1/\\lambda\\) in which \\(\\lambda\\) denotes the average time between events. The inverse of Gamma distribution is expressed as: \\[\\begin{align} g(x; \\alpha,\\beta) = P(X = x|\\alpha, \\beta) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)}x^{-(\\alpha+1)}e^{-\\frac{\\beta}{ x}}\\ \\ \\text{(inverse)} \\end{align}\\] Note that if the average time between events (e.g. bathroom breaks) is two hours \\(\\rightarrow \\lambda = 2\\ hrs\\), then \\(\\beta = 1/2 = 0.5\\). The CDF of a Gamma distribution where \\(x \\ge 0\\) is expressed as: \\[\\begin{align} \\mathcal{F}(x; \\alpha, \\beta) = P(X \\le x|\\alpha, \\beta) = 1 - \\sum_{i=0}^{\\alpha-1} \\frac{(\\lambda x)^i}{i!}e^{-\\lambda x} \\end{align}\\] Below is a naive implementation of Gamma Distribution in R code: Gamma &lt;- function(n) { factorial(n-1)} gamma_pdf &lt;- function(x, alpha, beta ) { 1 / (beta ^alpha * Gamma(alpha)) * x^(alpha - 1) * exp(-x/beta) } gamma_cdf &lt;- function(x, alpha, beta) { constant = 0 lambda = 1/beta for (i in 0:(alpha-1)) { constant = constant + ((lambda*x)^i)/factorial(i)*exp(-lambda*x) } 1 - constant } gamma_area &lt;- function(x, alpha, beta) { a = 0; b = x # boundaries area = seq(a, b, length.out=50) # area x = c(a, area , b) y = c(0, gamma_pdf(area, alpha, beta), 0) polygon(x, y, col=&quot;lightgrey&quot;) } #Plotting PDF and CDF(Area for alpha=5, beta=1) plot(NULL, xlim=range(0:15), ylim=range(0,0.5), xlab=&quot;support&quot;, ylab=&quot;probability density&quot;, main=&quot;PDF and CDF (Gamma Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) p = gamma_cdf(x=6, alpha=5, beta=1) gamma_area(x=6, alpha=5, beta=1) curve(gamma_pdf(x, alpha=1, beta=2), col=&quot;brown&quot;, lty=2, add=TRUE) curve(gamma_pdf(x, alpha=2, beta=2), col=&quot;red&quot;, lty=2, add=TRUE) curve(gamma_pdf(x, alpha=5, beta=1), col=&quot;navyblue&quot;, add=TRUE ) curve(gamma_pdf(x, alpha=10, beta=0.5), col=&quot;darksalmon&quot;, lty=2, add=TRUE) text(4,0.06, label=&quot;(cdf)&quot;, col=&quot;black&quot;) text(4, 0.02, label=paste(&quot;Area = &quot;, round(p,5), sep=&quot;&quot;), ce=0.80, col=&quot;black&quot;) text(6.5, 0.04, label=&quot;x=6&quot;) #Plotting CDF plot(NULL, xlim=range(0:15), ylim=range(0,1), xlab=&quot;support&quot;, ylab=&quot;cumulative density&quot;, main=&quot;CDF (Gamma Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,15, length.out=500) curve(gamma_cdf(x, alpha=1, beta=2), col=&quot;brown&quot;, add=TRUE ) curve(gamma_cdf(x, alpha=2, beta=2), col=&quot;red&quot;, add=TRUE ) curve(gamma_cdf(x, alpha=5, beta=1), col=&quot;darksalmon&quot;, add=TRUE ) curve(gamma_cdf(x, alpha=10, beta=0.5), col=&quot;navyblue&quot;, add=TRUE ) Figure 5.18: Gamma Distribution Figure 5.19: Gamma Distribution Note that if \\(\\alpha = 1\\), the inverse of \\(\\beta\\) makes the Gamma distribution equivalent to Exponential distribution. Meaning, use \\(\\alpha=1, \\beta=1/\\lambda\\) to mimic Exponential distribution using Gamma PDF. One way to illustrate the relation between Gamma and Exponential distribution is shown in figure 5.20. Figure 5.20: Gamma and Exponential Distribution In Figure 5.20, there are four events, \\(\\{ e1, e2, e3, e4\\}\\). Event e1 happens between times 0 and 1. The time taken is one second, x1. Event e2 happens between times 1 and 2. The time taken is also one second, x2. Event e3 happens between times 2 and 4. The time taken is two seconds, x3. Moreover, event e4 happens between times 4 and 7. The time taken is three seconds, x4. Here, Exponential distribution focuses on the time taken between events. On the other hand, at time 1, event e1 happens after waiting for one second, G1. There is only one event that happens after one second. At time 2, events e1, e2 happen after two seconds elapsed, G2. Two events happen after two seconds. At time 4, after four seconds ,G3, events e1, e2, e3 happen. Three events happen after four seconds. Finally, at time 7, seven seconds, G4, events e1, e2, e3, e4 happen. Four events happen after seven seconds. Here, Gamma distribution focuses on the number of events after some elapsed time. To illustrate practically, suppose a shuttle bus in an airport’s long-term parking lot arrives every 30 minutes at the airport to pick up travelers. Let us compute the probability of expecting three buses to arrive after waiting between 1 hour and 2 hours. A shuttle bus arriving every 30 minutes means we expect to see \\(\\beta = 1/0.5= 2\\) buses arriving every hour on average. Using \\(\\alpha = 3\\) and \\(\\beta = 2\\), we can compute this as follows: \\[\\begin{align} P(1 \\le X \\le 2) = \\sum_{x=1}^{\\alpha-1} \\frac {1}{\\beta^\\alpha\\Gamma(\\alpha)}x^{(\\alpha-1)}e^{-\\frac{x}{\\beta}} = \\sum_{x=1}^{3-1} \\frac {1}{\\Gamma(3)2^3}x^{(3-1)}e^{-\\frac{x}{2}} = 0.129878 \\end{align}\\] Here is the implementation of PDF in R code: gamma_pdf &lt;- function(x, alpha, beta) { 1/( factorial(alpha-1)* beta^alpha ) * x^(alpha-1) * exp(-x/beta) } sum ( gamma_pdf(x=c(1,2), alpha=3, beta=2) ) # our code ## [1] 0.129878 sum(dgamma(x=c(1,2), shape=3, rate=0.5)) # R&#39;s built-in package ## [1] 0.129878 Note that \\(\\alpha = shape\\) and \\(\\beta = 1 / rate\\). In terms of expected value and variance, we have the following formulas: Expected value: \\[\\begin{align} \\mathbb{E}(X) = \\alpha \\beta \\end{align}\\] Variance: \\[\\begin{align} Var(X) = \\mathbb{E}({X}^2) - \\mathbb{E}(X)^2 = \\alpha \\beta^2 \\end{align}\\] 5.9.9 Inverse Gamma distribution Inverse Gamma distribution models a continuous distribution whose PDF is inverse of Gamma distribution where support \\(x \\ge 0\\) and is written as: \\[\\begin{align} f(x; \\alpha,\\beta) = P(X = x|\\alpha, \\beta) = \\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}x^{-(\\alpha+1)}e^{-\\frac{1}{x\\beta}} \\end{align}\\] and its CDF is: \\[\\begin{align} \\mathcal{F}(x; \\alpha,\\beta) = P(X &lt;= x|\\alpha, \\beta) = \\frac{\\Gamma(\\alpha, \\frac{\\beta}{x})}{\\Gamma(\\alpha)} \\end{align}\\] where the upper incomplete Gamma function is written as: \\[\\begin{align} \\Gamma(\\alpha, \\frac{\\beta}{x}) = \\int_0^x t^{\\alpha - 1}e^{-t}dt \\end{align}\\] Expected value: \\[\\begin{align} \\mathbb{E}(X) = \\frac{\\beta}{(\\alpha - 1 )} \\end{align}\\] Variance: \\[\\begin{align} Var(X) = \\mathbb{E}({X}^2) - \\mathbb{E}(X)^2 = \\frac{\\beta^2}{(\\alpha-1)^2(\\alpha - 2)} \\end{align}\\] 5.9.10 Weibull distribution For an alternative to Gamma distribution, we leave readers to investigate Weibull distribution, which offers simplicity and reliability and is written as: \\[\\begin{align} X \\sim Weib(\\alpha, \\beta) \\end{align}\\] with the following 2-parameter PDF for Weibull distribution, where support \\(x \\ge 0\\) : \\[\\begin{align} f(x; \\tau, \\lambda) = P(X = x|\\tau, \\lambda) = \\frac{\\tau}{\\lambda}\\left(\\frac{x}{\\lambda}\\right)^{\\tau - 1} e^{ -\\left(\\frac{x}{\\lambda}\\right)^{\\tau}} \\end{align}\\] and with the 2-parameter CDF for Weibull distribution: \\[\\begin{align} \\mathcal{F}(x; \\tau, \\lambda) = P(X \\le x| \\tau, \\lambda) = 1 - e^{ -\\left(\\frac{x}{\\lambda}\\right)^{\\tau}} \\end{align}\\] We also note exploring the 3-parameter Weibull PDF and the 1-parameter Weibull PDF. We now discuss the next type of distribution - the Poisson distribution. It is notable to mention that Gamma distribution and Poisson distribution are also somewhat interrelated. While Gamma distribution is about the number of events after wait time, Poisson distribution is about the number of events between fixed times. 5.9.11 Poisson distribution Poisson distribution is also called Count distribution as it describes a discrete distribution of data based on the number of times events are occurring in some given fixed time-intervals and is expressed as: \\[\\begin{align} X \\sim Pois(\\lambda )\\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ \\ X \\sim Po(\\lambda ) \\end{align}\\] As examples: How many words are typed every minute? How many drops of rainfall on a basin every second? How many cars pass by the highway every minute? Here, the expected number of occurrences is \\(\\lambda\\) (lambda). It answers the question, How many?. The PMF of a discrete Poisson distribution is expressed as: \\[\\begin{align} f(x; \\lambda) = P(X = x|\\lambda) = \\frac{\\lambda^xe^{-\\lambda}}{x!} \\label{eqn:eqnnumber6001} \\end{align}\\] where: k is the number of events occurring successfully \\(\\lambda\\) is a shape parameter describing event rate, \\(\\lambda = rt\\), e.g., number of events per fixed interval of time. r is the number of occurrences. t is the fixed interval time. The CMF of a discrete Poisson distribution is expressed as: \\[\\begin{align} \\mathcal{F}(x; \\lambda) = P(X \\le x|\\lambda) = \\sum_{k=0}^{x} \\frac{\\lambda^k}{k!}e^{-\\lambda} \\end{align}\\] Below is a naive implementation of Poisson Distribution in R code: poisson_pmf &lt;- function(x, lambda ) { lambda^x * exp(-lambda) / factorial(x) } poisson_cmf &lt;- function(x, lambda) { poisson = 0 for (k in 0:x) { poisson = poisson + (lambda^k)/factorial(k) * exp(-lambda) } poisson } poisson_area &lt;- function(x, lambda) { a = 0; b = x # boundaries area = seq(a, b, length.out=50) # area x = c(a, area , b) y = c(0, poisson_pmf(area, lambda), 0) polygon(x, y, col=&quot;lightgrey&quot;) bars = seq(0, 20, length.out=21) y = poisson_pmf(bars, lambda) } plot(NULL, xlim=range(0:20), ylim=range(-0.01,0.6), xlab=&quot;support&quot;, ylab=&quot;probability mass&quot;, main=&quot;PMF and CMF (Poisson Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) p = poisson_cmf(x=10, lambda=9) poisson_area(x=10, lambda=9) # use n=550 to smoothen curves curve(poisson_pmf(x, lambda=0.5), n=550, col=&quot;brown&quot;, lty=2, add=TRUE ) curve(poisson_pmf(x, lambda=1), n=550, col=&quot;red&quot;, lty=2, add=TRUE ) curve(poisson_pmf(x, lambda=5), n=550, col=&quot;darksalmon&quot;, lty=2, add=TRUE ) curve(poisson_pmf(x, lambda=9), n=550, col=&quot;navyblue&quot;, add=TRUE ) x = seq(0, 20, length=21) y = poisson_pmf(x, lambda=9) points(x,y, col=&quot;navyblue&quot;, pch=16, cex=0.8) text(7,0.06, label=&quot;(cmf)&quot;, col=&quot;black&quot;) text(7, 0.02, label=paste(&quot;Area = &quot;, round(p,5), sep=&quot;&quot;), ce=0.80, col=&quot;black&quot;) text(10, -0.01, label=&quot;x=10&quot;, cex=0.8) Figure 5.21: Poisson (PMF and CMF) Our CMF for the Poisson distribution in figure 5.21 is 0.70599. Note that the PMF and CMF may appear continuous in figure 5.21. However, we focus more on the discrete points along the curves generated by PMF. Equivalently, CMF generates a set of discrete bars up to \\(x\\) instead of a continuously filled region. To illustrate further, suppose that a software developer types an average of 40 words per minute on a computer keyboard. Calculate the probability of k = (0,1,2,3,..6) in an interval of 1-minute. \\[ P(X\\ \\in\\ \\{0,1,2,3,4,5,6\\}) = \\frac{\\lambda^x e^{-\\lambda}}{x!} \\] where: r is 40 words typed on the average. t is 1 minute interval. \\(\\lambda\\) is 40 words / minute Here is the implementation of PMF in R code: k = c(30,35,40,45,50) round(poisson_pmf(x=k,lambda=40), 5) # our code ## [1] 0.01847 0.04854 0.06295 0.04397 0.01771 (p = round(dpois(x=k, lambda=40), 5)) # R&#39;s built-in package ## [1] 0.01847 0.04854 0.06295 0.04397 0.01771 names(p) &lt;- seq(0,4,1) barplot(p, density=T, col=2, xlab=&quot;Events&quot;, ylab=&quot;Probability&quot;, main=&quot;Rate of Typed Words&quot;) Figure 5.22: (PMF) Poisson Distribution Here, the probability of typing 30 words per minute with an average of 40 words per minute is 1.85%. The probability of typing 40 words per minute is 6.30% if the average is 40 words per minute. For the expected value and variance, respectively, we have: \\[\\begin{align} \\mathbb{E}(X) = Var(X) = \\lambda = rt \\end{align}\\] 5.9.12 Pareto distribution Pareto distribution models a skewed and heavy-tailed continuous distribution written as: \\[\\begin{align} X \\sim Pareto(\\lambda, \\alpha) \\end{align}\\] This distribution is commonly known to model the distribution of incomes. The Pareto PDF of a Pareto distribution with support, \\(x &gt; \\lambda\\), is expressed as: \\[\\begin{align} f(x; \\lambda, \\alpha) = P(X = x) = \\frac{\\alpha \\cdot \\lambda^\\alpha}{X^{\\alpha+1}} \\end{align}\\] where: \\(\\lambda\\) represents the minimum wage \\(\\alpha\\) is the shape parameter modeling an income distribution The Pareto CDF is expressed in the below equation, where \\(x &gt; \\lambda\\): \\[\\begin{align} f(x; \\lambda, \\alpha) = P(X \\le x) = 1 - \\left(\\frac{\\lambda}{x}^\\alpha\\right) \\end{align}\\] The mean and variance are expressed as such: \\[\\begin{align} \\mathbb{E}(X) = \\frac{\\alpha\\lambda}{\\alpha - 1}, \\alpha &gt; 1 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ VAR(X) = \\frac{\\alpha\\lambda^2}{(\\alpha - 1)^2(\\alpha - 2)}, \\alpha &gt; 2 \\end{align}\\] Below is a naive implementation of Pareto Distribution in R code (See Figures 5.23 and 5.24): pareto_pdf &lt;- function(x, lambda, alpha ) { (alpha * lambda^alpha) / (x^(alpha + 1)) } pareto_cdf &lt;- function(x, lambda, alpha) { 1 - (lambda^alpha / x) } pareto_area &lt;- function(x, lambda, alpha) { a = 1; b = x # boundaries area = seq(a, b, length.out=50) # area x = c(a, area , b) y = c(0, pareto_pdf(area, lambda, alpha), 0) polygon(x, y, col=&quot;lightgrey&quot;) } #Plotting PDF and CDF (Area for alpha=1, beta=1) plot(NULL, xlim=range(1:8), ylim=range(0,2), xlab=&quot;support&quot;, ylab=&quot;probability density&quot;, main=&quot;PDF (Pareto Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) p = pareto_cdf(x=2, lambda=0.5, alpha=1) pareto_area(x=2, lambda=0.5, alpha=1) curve(pareto_pdf(x, lambda=0.5, alpha=1), col=&quot;navyblue&quot;, add=TRUE ) curve(pareto_pdf(x, lambda=1, alpha=1), col=&quot;darksalmon&quot;, lty=2, add=TRUE ) curve(pareto_pdf(x, lambda=1.5, alpha=1), col=&quot;brown&quot;, lty=2, add=TRUE ) curve(pareto_pdf(x, lambda=2, alpha=1), col=&quot;red&quot;, lty=2, add=TRUE ) text(1.2,0.2, label=&quot;(cdf)&quot;, col=&quot;black&quot;) text(1.5, 0.1, label=paste(&quot;Area = &quot;, round(p,5), sep=&quot;&quot;), ce=0.80, col=&quot;black&quot;) Figure 5.23: Pareto Distribution (PDF) #Plotting CDF plot(NULL, xlim=range(2:30), ylim=range(0,1), xlab=&quot;support&quot;, ylab=&quot;cumulative density&quot;, main=&quot;CDF (Pareto Distribution)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lty=2) x = seq(0,15, length.out=500) curve(pareto_cdf(x, lambda=0.5, alpha=1), col=&quot;brown&quot;, add=TRUE) curve(pareto_cdf(x, lambda=1, alpha=1), col=&quot;darksalmon&quot;, add=TRUE) curve(pareto_cdf(x, lambda=1.5, alpha=1), col=&quot;navyblue&quot;, add=TRUE) curve(pareto_cdf(x, lambda=2, alpha=1), col=&quot;red&quot;, add=TRUE ) Figure 5.24: Pareto Distribution (CDF) 5.9.13 Normal distribution Normal distribution, also called Gaussian distribution, models a continuous distribution written as: \\[\\begin{align} X \\sim \\mathcal{N}(\\mu, \\sigma^2) \\end{align}\\] The Normal PDF of a Normal distribution with support \\(x \\in \\mathbb{R}\\) is expressed as: \\[\\begin{align} f(x; \\mu, \\sigma) = P(X = x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right] \\end{align}\\] where: \\(\\mu\\) is the average or mean of the distribution, where \\(-\\infty &lt; \\mu &lt; \\infty\\) \\(\\sigma^2\\) is the variance \\(\\sigma\\) is the standard deviation. Note that, geometrically, \\(\\mu\\) controls the location of the bell-shaped curve, and \\(\\sigma\\) controls the shape or scale of the curve. We discuss this further in Chapter 7 (Bayesian Computation I) under Likelihood Subsection under Bayes Theorem Section. See Figure 5.8 for the bell-shaped curve of standard normal distribution. The \\(\\frac{1}{\\sqrt{2\\pi}}\\) is a normalizing constant that helps bring the probability equal to one. That is because if we remove the normalizing constant, then the probability area will not integrate into one; instead, we get (\\(\\sqrt{2\\pi}\\)): \\[\\begin{align} \\int_{-\\infty}^\\infty exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right] dx = \\sqrt{2\\pi} \\end{align}\\] To understand the derivation, investigate Gaussian integrals with polar coordinates. Now, if we add the normalizing constant based on the following integration (for continuous distribution), we get: \\[\\begin{align} \\frac{1}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty exp\\left[-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right] dx = \\frac{1}{\\sqrt{2\\pi}} \\left(\\sqrt{2\\pi}\\right) = 1 \\end{align}\\] On the other hand, the fraction \\(\\frac{1}{2}\\) in the exponent exists to transform the variance into a unit variance (and effectively into a unit standard deviation). Additionally, the negative sign in the exponent exists to flip the quadratic parabola so that its vertex points upwards geometrically, making the shape a bell shape. Finally, the exponent expression describes the shape of the curve (e.g., bell shape). If we drop the constant, this does not affect the shape or proportionality described by the exponent. The Normal CDF is expressed in the below equation (wikipedia 2020), where \\(x \\ge 0\\) and \\(\\sigma &gt; 0\\): \\[\\begin{align} \\mathcal{F}(x; \\mu, \\sigma^2) = P(X \\le x) = \\frac{1}{2} + \\frac{1}{2} erf\\left( \\frac{( x-\\mu)}{2\\sqrt{\\sigma}} \\right) \\end{align}\\] where erf, error function, is written as: \\[\\begin{align} erf(x) = \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2} dt \\end{align}\\] here we can use an approximation for erf: \\[\\begin{align} erf(x) \\approx tanh\\left(\\frac{x\\pi}{\\sqrt{6}}\\right) \\end{align}\\] See Figure 5.28 for Gauss error function. The R code below draws a scattered plot and normal distribution. See Figure 5.25. set.seed(142) # Generate random values for random variable x # using standard normal distribution. size=500 x = rnorm(n=size, mean=0, sd=1) # Draw the scattered plot. plot(x, lwd=1, pch=16, col=&quot;black&quot;, main=&quot;Scattered Plot&quot;, ylab=&quot;Response (Y)&quot;, xlab=&quot;Predictor (X)&quot;) Figure 5.25: Normal Distribution breaks=20 # Draw the standard normal distribution. hist(x, breaks=breaks, prob=TRUE, xlim=range(-4,4), main=&#39;Standard Normal Distribution&#39;, xlab=&#39;Standard Deviation&#39;) curve(dnorm(x, mean=0, sd=1), add=TRUE, lwd=2, col=&quot;navyblue&quot;) Figure 5.26: Normal Distribution Note that a normal distribution with mean = 0 and standard deviation = 1 is also called a unit normal. For multivariate normal distribution (MVN), we can use the following similar notation but with vectorized parameters: \\[\\begin{align} X_{(p)} \\sim \\mathcal{N}\\left(\\mu_{(p)}, \\Sigma_{(pxp)}\\right) \\end{align}\\] \\[\\begin{align*} \\mu_{(p)} = \\left(\\begin{array}{cc}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{array}\\right) \\ \\ \\ \\ \\ \\ \\ \\Sigma_{(pxp)} = \\left(\\begin{array}{cccc} \\sigma^2_{11} &amp; \\sigma^2_{12} &amp; \\cdots &amp;\\sigma^2_{1p} \\\\ \\sigma^2_{21} &amp; \\sigma^2_{22} &amp; \\cdots &amp;\\sigma^2_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma^2_{p1} &amp; \\sigma^2_{p2} &amp; \\cdots &amp;\\sigma^2_{pp} \\\\ \\end{array}\\right)_{pxp} \\end{align*}\\] where: \\(\\Sigma_{(pxp)}\\) is symmetric positive-definite. An MVN PDF is written as: \\[\\begin{align} f(x_{(p)}; \\mu_{(p)}, \\Sigma_{(pxp)}) &amp;= P(X = x_{(p)})\\\\ &amp;= \\frac{1}{|\\Sigma_{(pxp)}|^\\frac{1}{2} (2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(x-\\mu)^T\\Sigma^{-1}_{(pxp)}( x - \\mu)\\right] \\end{align}\\] For example, given a bivariate normal distribution: \\[\\begin{align} f(x_{(p)}; \\mu_{(p)}, \\Sigma_{(pxp)}) &amp;= \\frac{1}{2\\pi \\left[\\begin{array}{llll} \\sigma_{11}^2 &amp; \\sigma_{12}^2 \\\\ \\sigma_{21}^2 &amp; \\sigma_{22}^2 \\end{array}\\right]_{(2x1)}^{\\frac{1}{2}}} \\times \\nonumber \\\\ &amp;\\exp\\left[-\\frac{1}{2} \\left[\\begin{array}{l} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{array}\\right]^T_{(2x1)} \\left[\\begin{array}{ll} \\sigma_{11}^2 &amp; \\sigma_{12}^2 \\\\ \\sigma_{21}^2 &amp; \\sigma_{22}^2 \\end{array}\\right]^{-1}_{(2x2)} \\left[\\begin{array}{l} x_1 - \\mu_1 \\\\ x_2 - \\mu_2 \\end{array}\\right]_{(2x1)} \\right] \\nonumber \\end{align}\\] Note that a vector x may follow a set of independent Gaussian normal distributions. Thus, we also can write this way: \\[\\begin{align} f(x_{(p)}; \\mu_{(p)}, \\Sigma_{(pxp)}) = \\prod_{i=1}^n \\frac{1}{ \\sqrt{2\\pi\\sigma_i}} exp\\left[-\\frac{1}{2}\\frac{(x_1-\\mu_1)^2}{\\sigma_i^2}\\right] \\end{align}\\] Here is a simple implementation of multivariate normal (MVN) distribution (specifically, bivariate): library(mvtnorm) set.seed(142) sample_size = n = 40 x = seq(-5,5, length.out = 40) y = seq(-5,5, length.out = 40) mu = rep(0,2); sigma = diag(1,2) g = expand.grid(x, y) z &lt;- matrix ( dmvnorm(as.matrix(g), mean=mu, sigma=sigma), nrow = n ) persp(x,y,z, phi=30, theta=25, expand = 0.5, shade = 0.10, main=&quot;Bivariate Normal Distribution&quot;) Figure 5.27: Bivariate Normal Distribution 5.9.14 Wald Distribution Wald Distribution is also known as Inverse Gaussian Distribution and is written as: \\[\\begin{align} X \\sim \\mathcal{IG}(\\mu, \\lambda) \\end{align}\\] The Wald PDF of a Normal distribution with support \\((0, \\infty)\\) is expressed as: \\[\\begin{align} f(x; \\mu, \\lambda) = P(X = x) = \\sqrt{\\frac{\\lambda}{2\\pi x^3}} exp\\left(-\\frac{\\lambda(x-\\mu)^2}{2\\mu^2 x}\\right) \\end{align}\\] where: \\(\\mu\\) is the average or mean of the distribution (location), \\(\\lambda\\) is the standard deviation (shape). The Wald CDF is expressed in the below equation: \\[\\begin{align} \\mathcal{F}(x; \\mu, \\lambda) = \\Phi \\left(\\sqrt{\\frac{\\lambda}{x}}\\left(\\frac{x}{\\mu} - 1\\right) \\right) + exp\\left(\\frac{2\\lambda}{\\mu}\\right)\\Phi \\left( \\sqrt{\\frac{\\lambda}{x}}\\left(\\frac{x}{\\mu} + 1\\right) \\right) \\end{align}\\] Wald distribution tends to be a skewed Gaussian distribution, such that if \\(\\lambda\\) increases to infinity, the Wald distribution eventually becomes a Gaussian distribution. 5.9.15 Log-normal Distribution A Log-normal distribution models a continuous distribution in which the logarithm of its random variable models a Normal (Gaussian) distribution and is written as: \\[\\begin{align} X \\sim ln\\ \\mathcal{N}(\\mu, \\sigma^2) \\end{align}\\] The Log-normal PDF is expressed in the below equation, where \\(x \\ge 0\\): \\[\\begin{align} f(x; \\mu, \\sigma^2) = \\frac{1}{x\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(ln\\ x-\\mu)^2}{2\\sigma^2}} \\end{align}\\] The Log-normal CDF is expressed in the below equation: \\[\\begin{align} \\mathcal{F}(x; \\mu, \\sigma^2) = P(X \\le x) = \\frac{1}{2} + \\frac{1}{2} erf\\left( \\frac{ln\\ x-\\mu}{\\sigma\\sqrt{2}} \\right) \\end{align}\\] where erf, error function, is written as: \\[\\begin{align} erf(x) {}&amp;= \\frac{2}{\\sqrt{\\pi}}\\int_0^x e^{-t^2} dt \\\\ &amp;\\approx tanh\\left(\\frac{x\\pi}{\\sqrt{6}}\\right) \\end{align}\\] ERF is a sigmoid function as shown in Figure 5.28. Note that we use tanh as an approximation only to the integral equation. erf &lt;- function(x) { tanh(x*pi/sqrt(6)) } x = seq(0, 1, length.out=100) plot(NULL, xlim=range(-2,2), ylim=range(-1,1), xlab=&quot;x&quot;, ylab=&quot;y&quot;, main=&quot;Gauss Error Function&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(erf(x), col=&quot;navyblue&quot;, lwd=2,add=TRUE) Figure 5.28: Gauss Error Function Note that Log-normal distribution is more advantageous over Normal distribution for situations where the distribution cannot take a negative value. Below is a naive implementation of Log-normal distribution in R code: erf &lt;- function(x, a, b ) { tanh(x*pi/sqrt(6)) # an approximation. } logpdf &lt;- function(x, mean, sd ) { # Log-normal Distribution ( 1 / (x*sqrt(2*pi*sd))) * exp( - (log(x)-mean)^2/(2*sd)) } logcdf &lt;- function(x, mean, sd) { 1/2 + 1/2 * erf((log(x) - mean)/(sqrt(2*sd))) } logarea &lt;- function(x, mean, sd) { a = 0.01; b = x # boundaries area = seq(a, b, length.out=50) # area x = c(a, area , b) y = c(0, logpdf(area, 0, 1), 0) polygon(x, y, col=&quot;lightgrey&quot;) } And we plot the distribution as shown in Figure 5.29. Figure 5.29: Log-normal Distribution plot(NULL, xlim=range(0,10), ylim=range(-0.01,0.8), xlab=&quot;spread (variance)&quot;, ylab=&quot;probability density&quot;, main=&quot;Log-normal Distribution&quot; ) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;navyblue&quot;) p = logcdf(x=4, 0, 1) logarea(x=4, 0, 1) curve(logpdf(x, 0, 1.0), n=500, col=&quot;navyblue&quot;, add=TRUE) curve(logpdf(x, 1, 0.7), n=500, col=&quot;darksalmon&quot;, lty=2, add=TRUE) curve(logpdf(x, 2, 0.5), n=500, col=&quot;brown&quot;, lty=2, add=TRUE) text(1.5,0.09, label=&quot;(cdf)&quot;, col=&quot;black&quot;) text(1.5, 0.05, label=paste(&quot;Area = &quot;, round(p, 5), sep=&quot;&quot;), ce=1, col=&quot;black&quot;) text(4, -0.01, label=&quot;q=2&quot;) 5.9.16 Uniform Distribution A Uniform distribution models a continuous distribution and is written as: \\[\\begin{align} X \\sim U(a,b) \\end{align}\\] The PDF for a Uniform distribution with support \\(\\{a,b\\} \\in \\mathbb{R}\\) and \\(a &lt; b\\) is expressed as: \\[\\begin{align} f(x) = \\begin{cases} \\frac{1}{b - a} &amp; a \\le x \\le b \\\\ 0 &amp; otherwise \\end{cases} \\label{eqn:eqnnumber20} \\end{align}\\] The CDF for a Uniform distribution is expressed as: \\[\\begin{align} F(x) = \\begin{cases} 0 &amp; x &lt; a \\\\ 1 &amp; x &gt; b \\\\ \\frac{x-a}{b - a} &amp; otherwise \\end{cases} \\label{eqn:eqnnumber21} \\end{align}\\] For the expected value and variance respectively, we have: \\[\\begin{align} \\mu = \\frac{a+b}{2}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma = \\sqrt{\\frac{(b-a)^2}{12}} \\end{align}\\] Given the simple formula above, the PDF and CDF for Uniform distribution should be simple to implement in R code (we skip the implementation). The next few sections cover T-distribution, F-distribution, Chi-Square distribution, Wishart distribution, and Mixture distribution among a few others. We introduce the CDF of the subsequent distributions, which come with complexity because of special functions such as Gamma, Beta, Continued Fraction, and HyperGeometric functions. While these complex CDFs are known or used in practice, continued efforts may still be ongoing to explore better alternatives. 5.9.17 T-Distribution A T-distribution, also called Student’s distribution, models a continuous distribution sampled from a Normal distribution and is written as: \\[\\begin{align} X_s \\sim T(\\nu)\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ X_p \\sim iid\\ N(\\mu, \\sigma^2) \\end{align}\\] The T-distribution is used for T-statistic tests to analyze samples of data with smaller sample size - typical suggested size is 30 or less. We discuss the T-Test in later sections. It can be said that a T-distribution is independently and identically distributed as a normal distribution whose PDF follows a much shorter and fatter curve. The PDF for a T-distribution with support \\(x \\in \\mathbb{R}\\) is expressed as: \\[\\begin{align} f(x; \\nu) = \\frac{1} {\\Gamma(\\frac{\\nu}{2})\\sqrt{\\nu\\pi}} \\Gamma\\left(\\frac{\\nu+1}{2}\\right) \\left(1 + \\frac{x^2}{\\nu}\\right)^{-\\frac{\\nu+1}{2}} \\end{align}\\] where: \\(\\nu\\) is the degrees of freedom. Here is a naive implementation of PDF for a symmetric (central) T-Distribution in R code: set.seed(1) Gamma &lt;- function(n) { factorial(n-1) } t_pdf &lt;- function(x, df) { Gamma((df+1)/2) / ( Gamma(df/2) * sqrt(df * pi) ) * ( 1 + x^2/df )^(-(df+1)/2) } population = round(seq(4,7, length.out=10),1) x = sample(population, size=20, replace=TRUE) plot(NULL, xlim=range(-4,4), ylim=range(0,0.5), xlab=&quot;T value&quot;, ylab=&quot;probability density&quot;, main=&quot;T Distribution (PDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(dnorm(x, 0, 1), col=&quot;navyblue&quot;, add=TRUE) curve(t_pdf(x, 10), col=&quot;darksalmon&quot;, lty=2, add=TRUE) curve(t_pdf(x, 2), col=&quot;red&quot;, lty=2, add=TRUE) curve(t_pdf(x, 1), col=&quot;brown&quot;, lty=2, add=TRUE) Figure 5.30: T-Distribution (PDF) It can be noticed that as the degree of freedom, \\(\\mathbf{\\nu}\\), gets larger, the T-distribution gets closer to that of a Normal distribution based on PDF. The CDF for a symmetric (central) T-distribution is expressed as: \\[\\begin{align} F(x; \\nu) = \\frac{1}{2} + \\frac{1}{2} sign(x) \\left[ I\\left(1; \\frac{\\nu}{2},\\frac{1}{2}\\right) - I\\left(\\frac{\\nu}{\\nu + x^2}; \\frac{\\nu}{2},\\frac{1}{2}\\right) \\right] \\end{align}\\] where: \\[\\begin{align} I_x(\\alpha,\\beta) = \\frac{\\mathcal{B}_x(\\alpha,\\beta)}{\\mathcal{B}(\\alpha,\\beta)}\\ \\ \\leftarrow\\ \\ \\text{regularized beta function} \\end{align}\\] Moreover, as complementary, we can explore the use of continued fraction for regularized beta function instead of hypergeometric function. Example, if x &lt; (a+1) / (a+b+2): \\[\\begin{align} Bx(\\alpha,\\beta) = \\frac{ Kx(\\alpha,\\beta) }{a} \\left[1+\\frac{d_1}{1+}\\frac{d_2}{1+}\\frac{d_3}{1+}...\\right] \\end{align}\\] where: \\[\\begin{align} Kx(\\alpha,\\beta) = \\frac{1}{\\mathcal{B}(\\alpha,\\beta)}x^\\alpha(1-x)^\\beta \\end{align}\\] and \\[\\begin{align} d_{2m} = \\frac{m(\\beta-m)x}{(\\alpha+2m - 1)(\\alpha+2m)}\\ \\ \\ \\ \\ \\ \\ \\ \\ d_{2m+1} = -\\frac{(\\alpha+m)(\\alpha+\\beta+m)x}{(\\alpha+2m)(\\alpha+2m+1)} \\end{align}\\] Let us first show an implementation of continued fraction in R code (see Numerical Recipes (W.H. Press et al., 1992) in C code): Gamma &lt;- function(n) { factorial(n-1) } Beta &lt;- function(a,b) { Gamma(a)*Gamma(b) / Gamma(a+b)} even &lt;- function(x, a, b, m) { (m*(b-m)*x) / ((a + 2*m - 1)*(a+2*m)) } odd &lt;- function(x, a, b, m) { -((a+m)*(a+b+m)*x) / ((a+2*m)*(a+2*m+1)) } tiny &lt;- function(z) { eps = 1e-30 if (z &lt; eps) { return(eps) } z } betacf &lt;- function(x, a, b, m) { limit = 200 epsilon = 3e-14 c = 1 d = 1 / tiny(1-(a+b)*x/(a+1)) cf = d for (m in 1:limit) { num = even(x, a, b, m) d = 1 / tiny(1+num * d ) c = tiny(1 + num / c) cf = cf * d*c num = odd(x, a, b, m) d = 1 / tiny(1+num * d ) c = tiny(1 + num/c) cf = cf * d*c if (abs(d*c-1) &lt; epsilon) { return(cf) } } return (Inf) } Bx &lt;- function(x, a, b) { n = length(x) bx = rep(0, n) for (i in 1:n) { if (x[i] &lt; 0 || x[i] &gt; 1) {bx[i] = 0; next } k = 0 if ( 0 &lt; x[i] &amp;&amp; x[i] &lt; 1 ) { k = 1 / Beta(a,b) * (x[i]^a * (1-x[i])^(b) ) } if (x[i] &lt; (a+1) / (a+b+2)) { # For I(x, a, b) bx[i] = (k / a * betacf( x[i], a, b, 1))*Beta(a,b) } else { # For I(1-x, b, a) bx[i] = (1 - k / b * betacf( 1-x[i], b, a, 1))*Beta(a,b) } } return(bx) } # replaces original implementation from beta distribution section Ix &lt;- function(x, a, b) { Bx(x, a, b) / Beta(a, b) } incomplete_beta &lt;- function(x,a,b) { pbeta(x,a,b) * beta(a,b) # using built-in R package &quot;pbeta&quot;. } Ix_alt &lt;-function(x, a, b) { # see beta distribution chapter incomplete_beta(x,a,b) / Beta(a,b) } x = c(0.50, -0.50) list(&quot;Ix&quot;=Ix(x, 3, 2), &quot;pbeta&quot;=pbeta(x, 3, 2)) ## $Ix ## [1] 0.3125 0.0000 ## ## $pbeta ## [1] 0.3125 0.0000 list(&quot;Ix&quot;=Ix(x, 2, 3), &quot;pbeta&quot;=pbeta(x, 2, 3)) ## $Ix ## [1] 0.6875 0.0000 ## ## $pbeta ## [1] 0.6875 0.0000 Note that to avoid overflows or underflows, or to avoid using multiplication and division, we use exponential and logarithmic functions (as shown in the original C code from Numerical Recipes). So that for the constant, K, we have: \\[\\begin{align} Kx(\\alpha,\\beta) {}&amp;= exp( log(\\Gamma(\\alpha)) - log(\\Gamma(\\beta)) - log(\\Gamma(\\alpha +\\beta)) \\nonumber \\\\ &amp;+ a \\times log(x) + b \\times log(1-x)) \\end{align}\\] However, in our R code, we intentionally use the original beta function instead of the exponential and logarithmic function to focus on the notation. \\[\\begin{align} \\mathcal{K}x(\\alpha,\\beta) = \\frac{1}{\\mathcal{B}(\\alpha,\\beta)}x^\\alpha(1-x)^\\beta \\end{align}\\] Also, the regular beta function keeps the common notation in the R code: \\[\\begin{align} \\mathcal{I}_x(\\alpha,\\beta) = \\frac{ \\mathcal{B}_x(\\alpha,\\beta) }{ \\mathcal{B}(\\alpha, \\beta)} \\end{align}\\] Given all that, here is a naive implementation of CDF for T-Distribution in R code: set.seed(1) t_cdf &lt;- function(x, df) { 1/2 + 1/2 * sign(x) * ( Ix(1,df/2, 1/2) - Ix(df/(df+x^2), df/2, 1/2) ) } population = round(seq(4,7, length.out=10),1) x = sample(population, size=20, replace=TRUE) plot(NULL, xlim=range(-5,5), ylim=range(0,1), xlab=&quot;T value&quot;, ylab=&quot;cumulative probability&quot;, main=&quot;T Distribution (CDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(pnorm(x, 0, 1), col=&quot;navyblue&quot;, add=TRUE) curve(t_cdf(x, 10), col=&quot;darksalmon&quot;, lty=2, add=TRUE) curve(t_cdf(x, 2), col=&quot;red&quot;, lty=2, add=TRUE) curve(t_cdf(x, 1), col=&quot;brown&quot;, lty=2, add=TRUE) Figure 5.31: T-Distribution (CDF) An alternative solution to the CDF for the T-distribution uses the Hypergeometric function as follows: \\[\\begin{align} {}_2F_1(a,b;c; x) {}&amp;= (1-x)^{-b} {}_2F_1\\left(c-a,b;c; z\\right) \\ \\ \\ \\ \\ where\\ \\ \\ z = \\frac{x}{x-1}\\\\ &amp;= \\frac{1}{(1-x)^{b}}\\sum_{n=0}^\\infty\\frac{(c-a)_n (b)_n}{(c)_n} \\frac{z^n}{n!} \\end{align}\\] Note that we use the first transformation form of the Hypergeometric function as listed in Chapter 5 (Probability and Distribution) under the Special Functions Section: Also, note that we use the Rising Factorial for the symbol \\((...)_n\\). Here is a naive implementation of the Hypergeometric function for our t-distribution CDF (Note that in this implementation, the alternative function, t_cdf_alt, is limited to the t-distribution support range, \\(-4 \\le x \\le 4\\)): set.seed(1) Gamma &lt;- function(n) { factorial(n-1) } rise_factorial &lt;- function(x, n) { if (n==0) return(1) prod = 1 for (k in 0:(n-1)) { prod = prod * (x + k) } prod } hypergeometric &lt;- function(a, b, c, x) { # only for 2F1(a,b;c;x) hyperG_1st_form = 0 limit = 50 z = x / ( x - 1) for (n in 0:limit) { hyperG_1st_form = hyperG_1st_form + (( rise_factorial(c-a , n) * rise_factorial(b, n) ) / rise_factorial(c, n)) * ( z^n / factorial(n)) } hyperG_1st_form / (1-x)^b } t_cdf_alt &lt;- function(x, df) { 1/2 + x * Gamma((df+1)/2) / ( Gamma(df/2) * sqrt(df * pi) ) * hypergeometric(1/2, (df+1)/2, 3/2, -(x^2/df)) } Now in terms of the expected value and variance respectively, we have: \\[\\begin{align} \\mu = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma = \\frac{v}{v-2} \\end{align}\\] We leave readers to investigate the non-central (asymmetric) T-distribution. 5.9.18 F-Distribution An F-distribution, also known as Fisher-Snedecor distribution, models a continuous distribution and is written as: \\[\\begin{align} X \\sim F(\\nu_1, \\nu_2) \\end{align}\\] The distribution is used for F-statistic test by comparing two populations which we discuss in later section. The PDF for an F-distribution with support \\(x \\ge 0\\) is expressed as: \\[\\begin{align} f(x; \\nu_1, \\nu_2) {}&amp;= \\left[ \\Gamma\\left(\\frac{\\nu_1 + \\nu_2}{2}\\right) \\nu_1^{\\frac{\\nu_1}{2}} \\nu_2^{\\frac{\\nu_2}{2}} x^{\\frac{\\nu_1}{2} - 1} \\right] \\left[ \\Gamma\\left(\\frac{\\nu_1}{2}\\right) \\Gamma\\left(\\frac{\\nu_2}{2}\\right) ( \\nu_1 x + \\nu_2)^{\\frac{\\nu1 + \\nu_2}{2}} \\right]^{-1} \\\\ &amp;=\\left[ \\nu_1^{\\frac{\\nu_1}{2}} \\nu_2^{\\frac{\\nu_2}{2}} x^{\\frac{\\nu_1}{2} - 1} \\right] \\left[ B\\left(\\frac{\\nu_1}{2}, \\frac{\\nu_2}{2} \\right) ( \\nu_1 x + \\nu_2)^{\\frac{\\nu1 + \\nu_2}{2}} \\right]^{-1} \\end{align}\\] where: \\(\\nu_1\\ and\\ \\nu_2\\) are the degrees of freedom. The CDF for an F-distribution with support \\(x \\ge 0\\) is expressed as: \\[\\begin{align} F(x; \\nu_1, \\nu_2) = I_z(\\alpha, \\beta) = I(z; \\alpha, \\beta)\\ \\ \\rightarrow\\ \\ \\text{regularized beta function} \\end{align}\\] where: \\[\\begin{align} z = \\left(\\frac{\\nu_1 x}{\\nu_1 x + \\nu_2}\\right) \\end{align}\\] Here is a naive implementation of PDF and CDF for F-distribution in R code (Note that we use the built-in R package pbeta() for the regularized beta function): Gamma &lt;- function(n) {factorial(n-1)} Beta &lt;- function(a, b) { ( Gamma(a) * Gamma(b)) / Gamma(a+b) } f_pdf &lt;- function(x, df1, df2) { n = df1^(df1/2) * df2^(df2/2) * x^((df1/2) - 1) d = (df1*x + df2)^((df1+df2)/2) n / ( Beta(df1/2, df2/2) * d ) } f_cdf &lt;- function(x, df1, df2) { z = (df1 * x) / ( df1 * x + df2) pbeta ( z, df1/2, df2/2) } And we plot the distribution as shown in Figures 5.32 and 5.33. Figure 5.32: F-Distribution (PDF) Figure 5.33: F-Distribution (CDF) # Probability Density x = seq(0, 6) plot(NULL, xlim=range(0,6), ylim=range(0,1), xlab=&quot;F value&quot;, ylab=&quot;probability density&quot;, main=&quot;F Distribution (PDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(f_pdf(x, 2, 5), col=&quot;darksalmon&quot;, add=TRUE) curve(f_pdf(x, 10, 5), col=&quot;brown&quot;, add=TRUE) curve(f_pdf(x, 50, 10), col=&quot;red&quot;, add=TRUE) # Cumulative Density x = seq(0, 6) plot(NULL, xlim=range(0,6), ylim=range(0,1), xlab=&quot;F value&quot;, ylab=&quot;cumulative probability&quot;, main=&quot;F Distribution (CDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(f_cdf(x, 2, 5), col=&quot;darksalmon&quot;, add=TRUE) curve(f_cdf(x, 10, 5), col=&quot;brown&quot;, add=TRUE) curve(f_cdf(x, 50, 10), col=&quot;red&quot;, add=TRUE) 5.9.19 Chi-square Distribution A Chi-square distribution, also called \\(X^2\\)-distribution, models a continuous distribution formed by squaring and summing the standard normal deviation of \\(\\mathbf{\\nu}\\) independent variables that follow a standard normal distribution. The distribution can be expressed as follows: \\[\\begin{align} Q \\sim X^2(\\nu)\\ \\ \\ \\ \\ \\ \\ where\\ \\mathbf{\\nu}\\ = \\text{degrees of freedom} \\end{align}\\] The PDF for a Chi-squared distribution with support \\(x \\ge 0\\) is expressed as: \\[\\begin{align} f(x; \\nu) = \\frac{1}{2^{\\frac{\\nu}{2}}\\Gamma\\left(\\frac{\\nu}{2}\\right)} x^{\\frac{\\nu}{2}-1} e^{-\\frac{x}{2}} \\end{align}\\] The CDF for a Chi-square distribution with support \\(x \\ge 0\\) is expressed as: \\[\\begin{align} F(x; \\nu) = \\frac{1}{\\Gamma\\left(\\frac{\\nu}{2}\\right)} \\gamma \\left(\\frac{\\nu}{2},\\frac{x}{2}\\right) = P(\\frac{\\nu}{2},\\frac{x}{2}) \\end{align}\\] where \\(\\gamma(\\nu, x)\\) is the lower incomplete gamma function and \\(P(\\nu, x)\\) is the lower regularized gamma function. Here is a naive implementation of PDF and CDF for Chi-square distribution in R code: Gamma &lt;- function(n) {factorial(n-1)} Beta &lt;- function(a, b) { ( Gamma(a) * Gamma(b)) / Gamma(a+b) } chi_pdf &lt;- function(x, df) { 1 / ( 2^(df/2) * Gamma(df/2)) * x^(df/2-1) * exp(-x/2) } # Probability Density x = seq(0, 10) plot(NULL, xlim=range(0,10), ylim=range(0,1), xlab=&quot;Chi-square value&quot;, ylab=&quot;probability density&quot;, main=&quot;Chi-squared Distribution (PDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(chi_pdf(x, 1), col=&quot;darksalmon&quot;, add=TRUE) curve(chi_pdf(x, 2), col=&quot;brown&quot;, add=TRUE) curve(chi_pdf(x, 4), col=&quot;red&quot;, add=TRUE) curve(chi_pdf(x, 6), col=&quot;purple&quot;, add=TRUE) curve(chi_pdf(x, 9), col=&quot;navyblue&quot;, add=TRUE) GammaInc &lt;- function(alpha, z) { s = 0 limit = 300 flimit = 172 # R&#39;s gamma limit for (k in 0:limit) { if (z + k + 1 &gt; 171) { break } s = s + alpha^k / Gamma( z + k + 1) } lower = alpha^z * Gamma(z) * exp(-alpha) * s upper = Gamma(z) - lower P = lower / Gamma(z) # regular inc gamma Q = upper / Gamma(z) list(&quot;lower&quot;= lower, &quot;upper&quot;=upper, &quot;P&quot;=P, &quot;Q&quot;=Q ) } chi_cdf &lt;- function(x, v) { 1 / Gamma(v/2) * GammaInc(x/2, v/2)$lower } # Cumulative Density x = seq(0, 10) plot(NULL, xlim=range(0,20), ylim=range(0,1), xlab=&quot;Chi-square value&quot;, ylab=&quot;cumulative probability&quot;, main=&quot;Chi-squared Distribution (CDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) curve(chi_cdf(x, 1), col=&quot;darksalmon&quot;, add=TRUE) curve(chi_cdf(x, 2), col=&quot;brown&quot;, add=TRUE) curve(chi_cdf(x, 4), col=&quot;red&quot;, add=TRUE) curve(chi_cdf(x, 6), col=&quot;purple&quot;, add=TRUE) curve(chi_cdf(x, 9), col=&quot;navyblue&quot;, add=TRUE) Figure 5.34: Chi-square Distribution (PDF and CDF) Figure 5.35: Chi-square Distribution (PDF and CDF) It helps to also reference the Chi-square table in the Appendix. \\[ \\underbrace{P(x &gt; 2.7055, 1) = 0.100}_\\text{df=1}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{P(x &gt; 4.6052, 2) = 0.100}_\\text{df=2} \\] c(&quot;df=1&quot;=round( 1 - pchisq(2.7055,1),3), &quot;df=2&quot;=round( 1 - pchisq(4.6052,2),3)) ## df=1 df=2 ## 0.1 0.1 Now in terms of the expected value and variance respectively, we have: \\[\\begin{align} \\mathbb{E}(X) = n\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Var(X) = 2n \\end{align}\\] See Chi-square Test section for sample application of the distribution. 5.9.20 Wishart distribution Wishart distribution models a covariance continuous distribution drawn or sampled (as a precision or inverse covariance matrix, namely V ) from a multivariate normal distribution (MVN). It is both an extension of gamma distribution and a generalization of the Chi-Square \\(\\mathcal{X}^2\\) distribution. To compare Chi-Square and Wishart structure in terms of multivariate distribution, see below (Mathew T. 1997): \\[\\begin{align} \\underbrace{V = \\sum_{i=1}^n X_i^2}_{ \\begin{array}{c}\\text{chi-square}\\ (V \\in \\mathbb{R}^n)\\ dist\\\\ \\ V\\sim\\ \\mathcal{X}^2(\\ \\nu\\ )\\\\ from \\\\ \\text{univariate dist}\\\\ X \\sim\\ \\mathcal{N}(\\mu, \\sigma^2)\\end{array} }\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{V = \\sum_{i=1}^n X_i X_i^T}_{ \\begin{array}{c}\\text{wishart} (V \\in \\mathbb{R}^{pxp})\\ dist\\\\ \\ \\Sigma\\ \\sim\\ \\mathcal{W}(\\ \\nu, V)\\\\ from \\\\\\text{multivariate dist}\\\\ X \\sim\\ \\mathcal{N}_p(\\mu, \\Sigma)\\end{array} } \\label{eqn:eqnnumber22} \\end{align}\\] Below is the structure of a multivariate distribution, namely X, with a corresponding covariance matrix, namely \\(\\Sigma\\): \\[ X = \\left[\\begin{array}{rrrr} x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]_\\text{(nxp)} \\ \\ \\ \\mu = \\left[\\begin{array}{c}\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\vdots \\\\ \\bar{x}_p \\end{array}\\right] = \\left[\\begin{array}{c}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{array}\\right]_\\text{(1xp)} \\] \\[ \\Sigma_{(pxp)} = \\left[\\begin{array}{rrrr} \\sigma^2_{1} &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1p}\\\\ \\sigma_{21} &amp; \\sigma^2_{2} &amp; \\cdots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\cdots &amp; \\sigma^2_{p} \\end{array}\\right]_\\text{(pxp)} \\] Here, we cover three types of Wishart distributions using the following illustration: The first distribution type is the Central Wishart distribution which is written as: \\[\\begin{align} V \\sim W_p \\left(\\nu, \\Sigma_{(pxp)}\\right) \\equiv Wishart_p \\left(\\nu, \\Sigma_{(pxp)} \\right) \\end{align}\\] where: V is a precision or inverse covariance positive-definite matrix. \\(\\Sigma_{(pxp)}\\) is upper sigma describing a pxp non-inverse covariance positive-definite scale matrix p is number of random variables, e.g. p-variate distribution. \\(\\mathbf{\\nu}\\) is degrees of freedom, where \\(\\nu\\) &gt; p - 1. Also, \\(\\nu = n\\). The X is drawn from a multivariate normal distribution of which its covariance matrix is a positive-definite scale matrix with p random variables. We use a bivariate normal distribution for a shorter illustration where p = 2. \\[\\begin{align} X \\sim \\mathcal{N}_2(\\mu_{(2)}, \\Sigma_{(2x2)}),\\ \\ \\ \\ \\ \\ \\mu_{(2)} = \\left[\\begin{array}{cc}\\mu_1 \\\\ \\mu_2 \\end{array}\\right],\\ \\ \\ \\ \\ \\ \\Sigma_{(2x2)} = \\underbrace{\\left[\\begin{array}{cc} \\sigma^2_{11} &amp; \\sigma^2_{12} \\\\ \\sigma^2_{21} &amp; \\sigma^2_{22} \\\\ \\end{array}\\right]_{2x2}}_\\text{scale matrix} \\label{eqn:eqnnumber23} \\end{align}\\] Given X, we generate a sum square covariance V distribution written as: \\[\\begin{align} V = \\sum_{i=1}^n\\left(x_i -\\mu)(x_i - \\mu\\right)^T \\ \\ \\ \\ \\ where\\ \\mu = \\bar{x} \\end{align}\\] The PDF for a Central Wishart distribution is expressed as: \\[\\begin{align} f_p(V; \\nu, \\Sigma_{(pxp)}) = \\frac{|V|^{\\frac{\\nu-p-1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma^{-1}V)\\right]} {2^{\\frac{\\nu p}{2}}|\\Sigma|^{\\frac{\\nu}{2}}\\ \\Gamma_p\\left(\\frac{\\nu}{2}\\right)}\\ \\ \\ \\begin{array}{ll} \\text{(see matrix trace in Linear}\\\\ \\text{ Algebra chapter)}\\\\ \\end{array} \\label{eqn:eqnnumber24} \\end{align}\\] where multivariate Gamma function is: \\[\\begin{align} \\Gamma_p\\left(\\frac{\\nu}{2}\\right) = \\pi^{\\frac{p(p-1)}{4}} \\prod_{i=1}^p \\Gamma\\left[\\frac{\\nu - (i - 1)}{2}\\right] \\end{align}\\] and \\[\\begin{align} mean = n \\times \\Sigma_{(pxp)}\\ \\ \\ \\ \\ \\ \\ |V| = \\text{det}(V) \\end{align}\\] The second distribution type is the Non-Central Wishart distribution which is written as: \\[\\begin{align} U \\sim W_p(\\nu, \\Sigma_{(pxp)}, \\Upsilon) \\equiv Wishart_p(\\nu, \\Sigma_{(pxp)}, \\Upsilon) \\end{align}\\] where: U is a precision or inverse covariance positive-definite matrix. \\(\\Sigma_{(pxp)}\\) is a pxp non-inverse covariance positive-definite scale matrix p is the number of random variables, e.g. p-variate distribution. \\(\\mathbf{\\nu}\\) is degrees of freedom, where \\(\\nu\\) &gt; p - 1. The X is drawn from a multivariate normal distribution of which its covariance matrix is a positive-definite scale matrix with p random variables. For illustration, we use bivariate normal distribution where p = 2. \\[\\begin{align} X \\sim \\mathcal{N}(\\mu_{(2)}, \\Sigma_{(2x2)}),\\ \\ \\ \\ \\ \\ \\mu_{(2)} = \\left(\\begin{array}{cc}\\mu_1 \\\\ \\mu_2 \\end{array}\\right),\\ \\ \\ \\ \\ \\ \\Sigma_{(2x2)} = \\underbrace{\\left(\\begin{array}{cc} \\sigma^2_{11} &amp; \\sigma^2_{12} \\\\ \\sigma^2_{21} &amp; \\sigma^2_{22} \\\\ \\end{array}\\right)_{2x2}}_\\text{scale matrix} \\label{eqn:eqnnumber25} \\end{align}\\] Here we extend the Central Wishart equation with two other factors: \\[\\begin{align} U = \\sum_{i=1}^n X_i X_k^T\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\Upsilon = N\\Sigma^{-1}\\mu\\mu^T \\end{align}\\] The PDF for a Non-Central Wishart distribution is expressed as: \\[\\begin{align} f_p(U; \\nu, \\Sigma_{(pxp)}, \\Upsilon) = \\frac{|U|^{\\frac{\\nu-p-1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma^{-1}U)\\right]} {2^{\\frac{\\nu p}{2}}|\\Sigma|^{\\frac{\\nu}{2}}\\ \\Gamma_p\\left(\\frac{\\nu}{2}\\right)}exp\\left[-\\frac{1}{2}\\Upsilon\\right]{}_0F_1\\left(\\frac{\\nu}{1}; \\frac{1}{4}\\Upsilon \\Sigma^{-1} U\\right) \\end{align}\\] The third distribution type is the Inverse Wishart distribution which is written as: \\[\\begin{align} \\Sigma_{pxp} = \\sim \\mathcal{IW}_p(\\nu, V) \\equiv \\mathcal{W}_p^{-1}(\\nu, V) \\end{align}\\] where V is a inverse covariance positive-definite scale matrix. \\(\\Sigma_{(pxp)}\\) is a pxp non-inverse covariance positive-definite matrix p is the number of random variables, e.g. p-variate distribution. \\(\\mathbf{\\nu}\\) is degrees of freedom, where \\(\\nu\\) &gt; p - 1. The PDF for an Inverse Wishart distribution is expressed as: \\[\\begin{align} f_p(\\Sigma_{(pxp)}; \\nu, V) = \\frac{ |\\Sigma|^{-\\frac{\\nu +p+1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma^{-1}V)\\right]} {2^{\\frac{\\nu p}{2}}|V|^{-\\frac{\\nu}{2}}\\ \\Gamma_p\\left(\\frac{\\nu}{2}\\right)} \\end{align}\\] and \\[\\begin{align} mean = \\frac{V}{v - p - 1} \\end{align}\\] It may help to point out that to optimize matrix computation, recall Cholesky factorization in Chapter 2 (Numerical Linear Algebra I) for invertible positive definite square matrix using the upper Cholesky factor. We leave readers to investigate this topic. 5.9.21 LKJ distribution Lewandowski-Kurowicka-Joe (LKJ) distribution models a distribution around correlations of parameters, treated as random variables. In Bayesian inference, we often deal with events as uncertainty (and thus are treated as random). Such correlation is formed as a positive definite correlation matrix. We leave readers to investigate this distribution as a modern alternative to Wishart distribution. 5.9.22 Mixture distribution A Mixture distribution is a parent distribution formed from the weighted \\(\\lambda k\\) combination of more than one child distribution called components of the parent distribution. Note that it is possible to plot the individual components. Figure 5.36 illustrates 3 components (K=3). The overall mixing proportions, \\(\\pi_{k}\\), equates to 1, e.g., (\\(\\sum_{k}\\pi_k = 1\\)). A mixture model has the following PDF formula: \\[\\begin{align} f(x) = \\sum_k \\pi_k f(x|\\theta_k) \\end{align}\\] where \\(\\pi_k\\) is the mixing proportion. Below is a sample implementation of a Mixture distribution using a built-in KDE function called density() which we briefly introduce under Non-parametric distribution section (See Figure 5.36). set.seed(142) n=500 x = c( rnorm(n=n/2.5, 4, 1.5), rnorm(n=n/2.5, 7.5, 0.4), rnorm(n=n/2.5, 8, 1)) # Plotting the main (parent) mixture distribution plot(density(x), col=&quot;navyblue&quot;, lwd=2, main=&quot;Mixture Density&quot;, xlab=&quot;Three Independent Normal Distributions&quot;) grid() # Let us use scale for plotting convenience only # to fit the components along the mixture distribution scale=3.5 # Plotting the 1st component distribution x = rnorm(n=n/2.5, 4, 1.5) c1 = curve(dnorm(x, 4, 1.5)/scale, add=TRUE, col=&quot;green&quot; ) # Plotting the 2nd component distribution x = rnorm(n=n/2.5, 7.5 , 0.4) c2 =curve(dnorm(x, 7.5, 0.4)/scale, add=TRUE, col=&quot;darksalmon&quot; ) # Plotting the 3rd component distribution x = rnorm(n=n/2.5, 8, 1) c3 = curve(dnorm(x, 8, 1)/scale, add=TRUE, col=&quot;red&quot; ) rug(rnorm(n=15, 4, 1.5), col=&quot;brown&quot;) rug(rnorm(n=15, 7.5, 0.4), col=&quot;darksalmon&quot;) rug(rnorm(n=15, 8, 1), col=&quot;red&quot;) Figure 5.36: Mixture Distribution Note that mixture gaussian models are covered under Expectation-Maximization (EM) section in Chapter 7 (Bayesian Computation I) as extension to mixture distribution discussion. 5.9.23 Non-parametric distribution The previous section discusses about common PDFs and CDFs. For example, recall the following normal PDF function. \\[\\begin{align} f(x; \\mu, \\sigma) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2 }}e^{\\frac{(x-\\mu)^2}{2\\sigma^2}} \\end{align}\\] The function is parametric, e.g. for normal distribution, we use the mean and variance parameters. In a case in which our data set cannot follow any of the formal distributions that we discussed to represent specific PDFs and CDFs, we then resort to an estimation of a distribution. We want to simulate a non-parametric density function by which our data set can follow some estimated distribution. To do that, we use what we call Kernel Density Estimators (KDE). KDE and the math involved are introduced in Chapter 3 (Numerical Linear Algebra II) under Kernel Smoothing Subsection along with a list of kernel functions and kernel estimators. Our guide to being able to construct such an estimated PDF is by visualizing a histogram and perhaps drawing a curve that matches it. Here is a naive implementation of KDE (see Figure 5.37) to show the curve and histogram. For our function K() implementation, refer to Chapter 3 (Numerical Linear Algebra II) under Kernel Smoothing Subsection. kde &lt;- function( x, h, kernel) { X = seq(-10, 15, length.out = 200) m = length(X); n = length(x) y = c() for (k in 1:m) { w = c() for (i in 1:n) { x_ = (1 / h) * K ( ( X[k] - x[i] ) / h , kernel ) w = c(w, x_ ) } y = c(y, sum(w) / n ) } list(&quot;x&quot;=X, &quot;y&quot; = y) } x = c( -3.0, 1.0, 3, 3, 4, 5, 7, 9, 10 ) # Silverman Rule of Thumb (optimal bandwidth) h = bw.nrd(x) - 0.2 kde.our.model = kde( x, h, &quot;normal&quot;) kde.sj.model = density(x, bw=&quot;sj&quot;) # Sheather and Jones (1991) kde.nrd.model = density(x, bw=&quot;nrd&quot;) # silverman rule plot(NULL, xlim = range(-6, 12), ylim=range(0, 3.2 ), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Kernel Density Estimation&quot;) grid(lty=3, col=&quot;lightgrey&quot;) hist(x, add=TRUE ) scale = 28 lines(kde.our.model$x, kde.our.model$y * scale, col=&quot;navyblue&quot;, lwd=2) lines(kde.sj.model$x, kde.sj.model$y * scale, col=&quot;darksalmon&quot;, lwd=2) lines(kde.nrd.model$x, kde.nrd.model$y * scale, col=&quot;brown&quot;, lwd=2) n = length(x) for (i in 1:n) { mu = x[i] x_ = seq( mu - 3, mu + 3, length.out = 50) y_ = dnorm(x_, mean=mu, sd = 1) lines(x_, y_, col=&quot;brown&quot;, lty=2) } rug(x, col = &quot;blue&quot;) legend(-6, 3, legend=c( &quot;our kde (h=1.5)&quot;, &quot;built-in density(bw=sj)&quot;, &quot;build-in density(bw=nrd)&quot;), col=c( &quot;navyblue&quot;, &quot;darksalmon&quot;, &quot;brown&quot;), lty=1, cex=0.7) Figure 5.37: Kernel Density Estimation Notice that our implementation of KDE exactly matches the output of the built-in R function density(). The bandwidth is intentionally adjusted with an offset of 0.2 so that the curve produced by our KDE implementation is demonstratively visible and does not overlap with the built-in estimate. For bandwidth selection, we can use the built-in R function bw.&lt;choices&gt;(): (optimal_bandwidth = bw.nrd(x)) # Silverman&#39;s Rule of Thumb ## [1] 2.038978 Alternatively, we can use unbiased cross-validation with a list of random bandwidths between 2 and 5. With that, we use a tolerance scaled at 0.1 (using the sample provided in the documentation of the built-in R function bw.ucv()): # unbiased cross-validation lower = 2; upper = 5 (optimal_h = bw.ucv(x, nb = 1000, lower = 0.1 * upper, upper = upper, tol = 0.1 * lower)) ## [1] 4.730645 5.9.24 Multi-dimensional Density Most of the parametric distributions discussed are unimodal - a distribution with only one peak. In mixture distribution, we begin to show multimodal characteristics of the distribution in which we see multiple peaks. The individual peak follows an individual normal distribution. Then finally, in KDE, we continue to show multimodal behavior; however, high peaks represent stacks of weights (summation of weights) computed based on a choice of kernel functions. Whether the distribution is unimodal or multimodal, it represents a univariate distribution - meaning, we deal with only one random independent variable. In this section, we show two examples of multivariate distribution: First, we introduce multivariate unimodal distribution, which deals with multiple independent variables with one peak. We start by using a 3rd-party library called mvtnorm and then generate a data set that follows a multivariate normal distribution with noise: library(mvtnorm) set.seed(142) sample_size = n = 200 e = rnorm(n=sample_size, mean=0, sd=1) / 30 # Noise/Gaussian Residual x = sample(seq(-5,5), size=n, replace=TRUE) y = sample(seq(-5,5), size=n, replace=TRUE) mu = rep(0,2); sigma = diag(1,2) z = dmvnorm(cbind(x, y), mean = mu, sigma=sigma ) + e We then use an R function called loess() to fit a two-dimensional model to our data using a span of 0.5 and a polynomial degree of 2 - a parabolic polynomial. # Because we added noise, our goal is to demonstrate how to fit # a 2D loess model. loess.model = loess(z ~ x + y, degree=2, span=0.5) # Perform smooth fit by prediction x = seq(-5,5, length.out = 50) y = seq(-5,5, length.out = 50) xy.grid = as.matrix( expand.grid(x,y)) z.fit = stats::predict(loess.model, newdata = xy.grid) z.fit = matrix(data = z.fit, nrow=length(x), ncol=length(y)) Finally, we plot a 3D perspective view (See Figure 5.38). persp(x, y, z.fit, phi=30, theta=25, xlim=range(-5,5), ylim=range(-5,5), main=&quot;Fitting LOESS with Multi-Dimensional data&quot;) Figure 5.38: Fitting LOESS with Multi-Dimensional data In Statistical Computation, we extend our discussion on LOESS under Statistical Regression and Statistical Inference. Second, we introduce multivariate multimodal distribution, which deals with multiple independent variables with multiple peaks. We start by using a 3rd-party library called KernSmooth and then construct some random sample datasets. library(KernSmooth) ## KernSmooth 2.23 loaded ## Copyright M. P. Wand 1997-2009 set.seed(142) sample_size = n = 40 y = sample(seq(1,9), size=sample_size, replace=TRUE) x = sample(seq(1,9), size=sample_size, replace=TRUE) data = cbind(x, y) We then use an R function called bkde2d() to fit a two-dimensional KDE model to our data with an optimal bandwidth obtained from bw.nrd(). Note that bkde2d() uses a bivariate gaussian kernel. h = bw.nrd(x) kde.grid = bkde2D(x = data, bandwidth= h) Finally, let us plot a 3D perspective view of our multi-dimensional KDE distribution (See Figure 5.39). persp(kde.grid$fhat, phi=30, theta=25, zlab = &quot;z-axis&quot;, ylab=&quot;y-axis&quot;, xlab = &quot;x-axis&quot;, main=&quot;Multi-dimensional density estimate (3D perspective)&quot;) Figure 5.39: Multi-dimensional KDE (3D Perspective) Additionally, we also can show the contour of our 3D view (See Figure 5.40). contour(kde.grid$x1, kde.grid$x2, kde.grid$fhat, xlab=&quot;x-axis&quot;, ylab=&quot;y-axis&quot;, main=&quot;Multi-dimensional density estimate (Contour)&quot;) Figure 5.40: Multi-dimensional KDE (Contour) 5.10 Summary Let us review the distributions we covered in this chapter and the built-in R packages that we can use in practice later. Table 5.1: Stochastic Distribution (R functions) Distribution Sampling Density Cumulative Quartile Notation Binomial rbinom dbinom pbinom qbinom X\\(\\sim\\)Bin(n, \\(\\rho\\)) Multinomial rmultinom dmultinom pmultinom qmultinom X\\(\\sim\\)Multi(n, \\(\\rho\\)) Geometric rgeom dgeom pgeom qgeom X\\(\\sim\\)Geo(\\(\\rho\\)) Beta rbeta dbeta pbeta qbeta X\\(\\sim\\)Beta(\\(\\alpha\\), \\(\\beta\\)) Exponential rexp dexp pexp qexp X\\(\\sim\\)Expo(\\(\\lambda\\)) Gamma rgamma dgamma pgamma qgamma X\\(\\sim\\)Gamma(\\(\\alpha\\), \\(\\beta\\)) Weibull rweibull dweibull pweibull qweibull X\\(\\sim\\)Weib(\\(\\alpha\\), \\(\\beta\\)) Poisson rpois dpois ppois qpois X\\(\\sim\\)Pois(\\(\\lambda\\)) Normal rnorm dnorm pnorm qrnorm X\\(\\sim\\)N(\\(\\mu\\), \\(\\sigma^2\\)) Log-normal rlnorm dlnorm plnorm qlnorm X\\(\\sim\\)ln N(\\(\\mu\\), \\(\\sigma^2\\)) T rt dt pt qt X\\(\\sim\\)T(\\(\\nu\\)) F rf df pf qf X\\(\\sim\\)\\(F(\\nu_1, \\nu_2)\\) Chi-Squared rchisq dchisq pchisq qchisq X\\(\\sim\\)X^2(k)$ Uniform runif dunif punif qunif X\\(\\sim\\)U(a,b)$ Wilcoxon rwilcox dwilcox pwilcox qwilcox X\\(\\sim\\)RankSum(m,n) Wilcoxon rsignrank dsignrank psignrank qsignrank X\\(\\sim\\)SignRank(n) Logis rlogis dlogis plogis qlogis X\\(\\sim\\)Logis(l,s) The build-in R functions allow us to analyze the different distributions. Each distribution is prefixed with the following letters: r, d, p q. r stands for random values. d stands for density/mass. p stands for probability distribution corresponding to its quartile (q). q stands for quartile, corresponding to a probability distribution (p). Therefore, to generate random values for a Binomial distribution, we use rbinom. We use dbinom to get the PDF. We use pbinom to get the CDF. Moreover, we use qbinom to get the value x (or the quartile). Note that four distributions (T-distribution, F-distribution, Chi-Squared distribution, Uniform) are covered in Chapter 6 (Statistical Computation). "],["statistics.html", "Chapter 6 Statistical Computation 6.1 Descriptive Statistics 6.2 Inferential Statistics 6.3 The Significance of Difference 6.4 Post-HOC Analysis 6.5 Multiple Comparison Tests 6.6 Statistical Modeling 6.7 Regression Analysis 6.8 The Significance of Regression 6.9 Inference for Regression 6.10 Summary", " Chapter 6 Statistical Computation In this chapter, we cover the Computational side of Statistics. By computation, we may refer to the use of computers to perform computational statistics. Here, we reference the great works of Sternstein M. (1996), Vapnik V. (2000), Hastie T. et al. (2016), Agresti A. et al. (2017), Forsyth D. (2018), and Illowsky B., Dean S. et al. (2018), along with other additional references for consistency. First, let us mention two kinds of Statistics: descriptive statistics and inferential statistics. 6.1 Descriptive Statistics Descriptive Statistics is Statistics based on describing (in Summary) the characteristics of sampled data. By Summary, we describe data in Moments (e.g., mean, variance or dispersion, skewness, and kurtosis), Central Tendencies, and Quartile Ranks. These Statistics are then further described by visual representation. 6.1.1 Visual Representation The most common visual representation of data is as follows: Histogram and Bar Charts - data is grouped and is presented as bars. Tabular Form - data is tabulated. Pie Charts - data is grouped into slices of a pie chart. Line Graphs - data is presented as a line passing through data points in a graph. These charts are visual representations of specific measurements in terms of counts, percentages, or frequencies. 6.1.2 Central Tendency Central tendency is a measure based on common or distinct values of a distribution. There are three measurements: mean, median, and mode. For example, we expect the mean to be zero in a standard normal distribution. set.seed(142) x = rnorm(n=5000, mean=0, sd=1) hist(x, breaks=20, prob=TRUE, main=&#39;Standard Normal Distribution&#39;, xlab=&#39;Standard Deviation&#39;, xlim=range(-4,4)) curve(dnorm(x, mean=0, sd=1),add=TRUE, lwd=2, col=&quot;navyblue&quot;) abline(v=0, col=&quot;darksalmon&quot;, lwd=3) Figure 6.1: Standard Normal Distribution Let us briefly describe the three common central tendencies: Mean, Median, Mode (Sternstein M. 1996). The average of a normal distribution is called the Mean. Values: 1 3 4 5 5 5 6 7 9 where N = number of values = 9 Therefore, the mean of (1 + 3 + 4 + 5 + 5 + 5 + 6 + 7 + 9) / N = 5 The Median of a normal distribution represents the middle value(s) which is 5 in the example above. We take the average of the middle values in an even count of sampled data. Values: 1 2 3 4 Therefore, the median for the middle values (2,3) is (2 + 3) / 2 = 2.5 The Mode of a normal distribution is the value with the highest occurrence or highest frequency. Here, 5 is the mode because it repeats three times. If a sample of data does not have repeating values, then there is no mode. Values: 1 2 3 3 5 5 5 6 6 7 8 9 Therefore, the mode is 5 Values: 1 2 3 Therefore, the mode is 0 Additionally, it helps to be aware of the following terms, namely Mean-Centering or finding the Zero-Mean. These terms rely on looking for the central tendency. Let us study a dataset with 5 random variables ( X1, X2, X3, X4, X5 ) of 3 observations each using uniform distribution: options(scipen = 1, digits = 4, width = 80, fig.align = &quot;center&quot;) set.seed(142) dataset = data.frame( X1 = runif(3,0,1), X2 = runif(3,0,1), X3 = runif(3,0,1), X4 = runif(3,0,1), X5 = runif(3,0,1)) knitr::kable( head(dataset, 20), caption = &#39;Central Tendency&#39;, booktabs = TRUE ) Table 6.1: Central Tendency X1 X2 X3 X4 X5 0.8952 0.5615 0.2804 0.4323 0.7637 0.6990 0.8107 0.6833 0.8565 0.5609 0.9558 0.8678 0.4328 0.8814 0.5153 Using the dataset above, let us convert to a zero-mean matrix (in other words, let us normalize our data): dmatrix = as.matrix(dataset) dmatrix = sweep(dmatrix, 2, colMeans(dmatrix)) knitr::kable( head(dmatrix, 20), caption = &#39;Zero-mean Matrix&#39;, booktabs = TRUE ) Table 6.2: Zero-mean Matrix X1 X2 X3 X4 X5 0.0452 -0.1851 -0.1851 -0.2911 0.1505 -0.1510 0.0640 0.2177 0.1331 -0.0524 0.1058 0.1211 -0.0327 0.1580 -0.0980 To know if our matrix is normalized into zero-mean, we convert the matrix to zero-mean repeatedly. In doing so, we get the same set of values over and over because the mean of the matrix has settled to zero; hence zero-mean. dmatrix = as.matrix(dataset) dmatrix = sweep(dmatrix, 2, colMeans(dmatrix)) dmatrix = sweep(dmatrix, 2, colMeans(dmatrix)) dmatrix = sweep(dmatrix, 2, colMeans(dmatrix)) knitr::kable( head(dmatrix, 20), caption = &quot;Zero-mean Matrix (Repeated 3 times)&quot;, booktabs = TRUE ) Table 6.3: Zero-mean Matrix (Repeated 3 times) X1 X2 X3 X4 X5 0.0452 -0.1851 -0.1851 -0.2911 0.1505 -0.1510 0.0640 0.2177 0.1331 -0.0524 0.1058 0.1211 -0.0327 0.1580 -0.0980 We use the colMeans(.) function to see if the mean in Table 6.3 has settled down to zero. dmatrix = as.matrix(dataset) dmatrix = sweep(dmatrix, 2, colMeans(dmatrix)) colMeans(dmatrix) ## X1 X2 X3 X4 X5 ## 3.701e-17 0.000e+00 1.850e-17 -3.701e-17 0.000e+00 As it turns out, the results rendered mean values that are extremely small, close to -17 digits in precision - in essence, that is equivalent to zero. There are other ways to compute the means of complex datasets more optimally. One way is matrix manipulation which we cover in Chapter 2 (Numerical Linear Algebra I). 6.1.3 Variability Variability is the measure of the variance (or spread) of data. The idea is to measure how close data is to the mean. At times, the standard deviation is preferred over variance because of its interpretability. For example, based on Figure 6.1, a four-unit of standard deviation indicates that a data point is at the far tail-end of the distribution and could therefore be deemed as an outlier. 6.1.4 Kurtosis and Skewness Kurtosis is used to measure the height (peak) of a distribution. Additionally, we also measure the thickness of the tail of a distribution. For example, Figure 6.2 shows how the tail of a distribution gets thicker as its height tapers off. In other words, the more a set of observations crowd around the center, the higher the frequency, and the lesser observations are left away from the center. There are three types of kurtosis distribution: Leptokurtic distribution - reflects a slender shape distribution Platykurtic distribution - reflects a wider spread (broader shape) distribution Mesokurtic distribution - reflects a more normal-like distribution set.seed(142) n=3000 x = rt(n, df=25) hist(x, xlim = range(-4,4), ylim=range(0, 0.80), prob=TRUE, breaks=20, main=&#39;t-Distribution&#39;, xlab=&#39;Standard Deviation&#39;) curve(dnorm(x, mean=0, sd=0.60),add=TRUE, lwd=2, col=&quot;darksalmon&quot;) curve(dnorm(x, mean=0, sd=1),add=TRUE, lwd=2, col=&quot;navyblue&quot;) curve(dnorm(x, mean=0, sd=2),add=TRUE, lwd=2, col=&quot;brown&quot;) legend(&quot;topright&quot;, inset=.02, c(&quot;Leptokurtic&quot;,&quot;MesoKurtic&quot;,&quot;Platykurtic&quot;), fill=c(&quot;darksalmon&quot;, &quot;navyblue&quot;, &quot;brown&quot;), horiz=FALSE, cex=0.8) Figure 6.2: t-Distribution Skewness measures the distortion made against a symmetric distribution, e.g., a bell-shaped normal distribution. In other words, this measures the extent to which a normal distribution becomes asymmetric. To demonstrate, we simulate Skewness by using chi-square distribution. Figure 6.3 does show an interesting distribution with dramatic Skewness to the left side. We manipulated the shape of the distribution by using degrees of freedom to show Skewness. set.seed(142) n=1000 x = rchisq(n,df=5, ncp=0) hist(x, prob=TRUE, breaks=20, main=&#39;Chi-Square Distribution&#39;, xlab=&#39;Chi-Square&#39;) curve(dchisq(x, df=1),add=TRUE, lwd=2, col=&quot;darksalmon&quot;) curve(dchisq(x, df=5),add=TRUE, lwd=2, col=&quot;navyblue&quot;) curve(dchisq(x, df=10),add=TRUE, lwd=2, col=&quot;brown&quot;) legend(&quot;topright&quot;, inset=.02, horiz=FALSE, cex=0.8, c(&quot;df=1&quot;,&quot;df=5&quot;,&quot;df=10&quot;), fill=c(&quot;darksalmon&quot;, &quot;navyblue&quot;, &quot;brown&quot;)) Figure 6.3: Chi-Square Distribution We have seen just a few - the common ones - of the many kinds of distribution of data. The shape of the distribution leaves clues that help us decide how to proceed with our analysis. It helps to understand the nature of the data we deal with as part of our exploratory data analysis. 6.1.5 Five Number Summary Five Number Summary refers to the 5 quartiles of a continuous distribution (see Figure 6.4): Maximum - represents the upper whisker in a box plot. Third Quartile - represents the upper side of the box in a box plot. Median - represents the middle thicker line in a box plot. First Quartile - represents the lower side of the box in a box plot. Minimum - represents the lower whisker in a box plot. Figure 6.4: Measuring The vertical length of the box is called the inter-quartile range (IQR). The minimum and maximum distances from the upper or lower side of the box are computed using: 1.5 * IQR. The other distance for 1.5 * IQR starting from the minimum or maximum whiskers, going outwards, is where outliers lie. To explain further, Figure 6.5 illustrates three boxplots, each from one categorical group. The dataset contains three levels (0, 1, 2). The boxplot of each level shows the quartile distribution. set.seed(142) x1 = rnorm(n=500, mean=0, sd=1) x2 = as.factor(rbinom(n=500, size=2, prob=0.5) ) boxplot(x1 ~ x2, pch=16, outcol=&quot;deepskyblue&quot;, main=&quot;Box Plot (Whisker Plot)&quot;, xlab=&quot;X1&quot;, ylab=&quot;X2&quot;) Figure 6.5: Five number Summary Level 1 of the dataset shows three outliers. Level 2 shows only one outlier. The median, first quartile, and third quartiles of the three levels are all almost aligned. Similarly, the lower (minimum) whiskers are also aligned. Only the upper whiskers are a bit apart. Here, one can see the distribution of categories to be close. Figure 6.6 shows another set of boxplots. The boxplots show that Level 1 and Level 2 have values concentrated in the range between 30 and 40 and have a median within the same value range, while Level 0 concentrates around 10-30 in the boxplot with a median within the 10-20 value range. set.seed(142) x1 = c( runif(n=150, min=1, max=20), runif(n=150, min=25, max=35), runif(n=200, min=30, max=50) ) x2 = as.factor(c(rbinom(n=150, size=2, prob=0.1), rbinom(n=150, size=2, prob=0.45), rbinom(n=200, size=2, prob=0.45) ) ) boxplot(x1 ~ x2, pch=16, outcol=&quot;deepskyblue&quot;, main=&quot;Box Plot (Whisker Plot)&quot;, xlab=&quot;X1&quot;, ylab=&quot;X2&quot;) Figure 6.6: Five number Summary How do we then interpret the three boxplots? One thing to point out is that the random variable X2 has a broader number of observations with values between 10 and 30 correlated with the category 0 (or level 0) of random variable X1. It also emphasizes that as we deal with more random variables, we begin to understand how it can get challenging to plot every combination of every random variable. The so-called “Curse of Dimensionality” characterizes an increase of complexity for every random variable that gets included for analysis. Luckily, we have methods available to help us identify whether a random variable is significant (or relevant) or is merely a duplicate that can be excluded from the analysis. 6.2 Inferential Statistics Inferential Statistics is Statistics focusing on the interpretation of sampled data with the intent to generalize its representation. For example, we formulate hypothetical claims about the nature of data distribution using Descriptive Statistics by predicting its Mean or Variance. Let us first review the definition of a few terminologies in this section: Population refers to a complete set of data. Sample refers to a subset of data (or observation) taken randomly from a population. # Here is a population of ten male and female # marsupials from which we sample only 5 set.seed(1228) # set initial seed population = c(&quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;M&quot;, &quot;F&quot;, &quot;M&quot;, &quot;F&quot;) sample(population, 5) ## [1] &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot; Group refers to discrete categorical grouping of samples. set.seed(1228) # set initial seed treatments = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) population_size = 100 sample_size = 30 # simulating a population population = sample(x=treatments, size = population_size, replace=TRUE) sample = sample(x=population, size = sample_size) treatmentA = length(which(sample == &quot;A&quot;)) treatmentB = length(which(sample == &quot;B&quot;)) treatmentC = length(which(sample == &quot;C&quot;)) c(&quot;A&quot;=treatmentA, &quot;B&quot;=treatmentB, &quot;C&quot;=treatmentC) ## A B C ## 11 9 10 Statistical data refers to data with observable and measurable quality that typically can be quantitatively analyzed based on statistics. In this section, we take a population sample and perform inferences. There are many ways to infer the data represented in a sample. Our approach covers reviewing the distribution of the sample, analyzing the significance of the differences of samples, analyzing the significance of regression of samples, and possibly dealing with exploratory analysis (which is covered in Chapter 9 (Computational Learning I). Sufficient Statistic refers to a statistic such as a sample mean \\(\\bar{x}\\) from a sample distribution that contains enough information to represent a specific model parameter such as the population mean \\(\\mu\\) from a population distribution. Formally, the statistic \\(\\bar{x}\\) is sufficient if it can be independent of the model parameter \\(\\mu\\). Note that the notation of a sufficient statistic is \\(t = T(X)\\), and the model parameter is denoted as \\(\\theta\\) where T(X) is an estimating function. In the next several sections under this Chapter, our main focus is Inferential Statistics. 6.3 The Significance of Difference In this section, let us evaluate data that is sampled from a population. In some cases, it is virtually impractical to derive parameters such as averages, variances, and standard deviation from a large population. Therefore, in this circumstance, our approach is to sample the population instead and evaluate if the sampled data is a good representation of the entire population data. The phrase good representation can be quantified based on Statistical Significance. First, we compare the average (the mean) of the sample data against the average (the mean) of the population data. Then conclude to see if there is any significant difference between the two data sets in terms of a chosen set of statistics such as the mean, variance, and others. It is important to note that some literature tends to explain Statistical Significance in the context of a legal setting in which a defendant is assumed to be innocent until sufficient evidence is gathered to prove the defendant guilty beyond a reasonable doubt. Here, we show how that works using a simple example. We base our discussion on additional references, namely Sternstein M. (1996), Parkhurst D. F. (2006), De Groot A. D. (2006), Lempert R.O (2008), Sabri F. and Gyateng T. (2015). 6.3.1 Hypothesis In this section, we review the concept of Hypothesis. The term Hypothesis is typically associated with other familiar terms such as supposition, proposition, assumption, and claim. Here, we declare a statement based on observed data and make a claim. For example, we can derive a hypothetical estimate if we cannot possibly know the exact average IQ level of a large population. Suppose we are given a hypothetical IQ level average of a large population to be 100. Note that the average IQ level of 100 is just hypothetical, and thus the population mean is unknown. Nonetheless, let us use that to formulate our Hypothesis. There are two types of hypotheses (both need to be stated in our test): Null Hypothesis - this hypothesis is denoted by \\(\\mathbf{H_0}\\). In this Hypothesis, we claim that there is no significant difference between the sample and the population ( or between samples from the same population) based on a statistic. For example, we can make a claim that the average IQ level of our sample, \\(\\mu\\), is equal to the given hypothetical IQ level, \\(\\mu_0 = 100\\). Meaning we claim that there is no difference. \\[\\begin{align} \\mathbf{H_0: \\mu = \\mu_0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\mu_0 = 100 \\end{align}\\] Alternative Hypothesis - this hypothesis is denoted by \\(\\mathbf{H_1}\\). In this hypothesis, using statistics, we claim a significant difference between a sample and a population ( or between samples from the same population). For example, as an alternative hypothesis, we may believe instead that the average IQ level of our sample is greater than the given hypothetical IQ level of 100. That means we claim that there is a difference. \\[\\begin{align} \\mathbf{H_1: \\mu \\ne \\mu_0}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\mu_0 = 100 \\end{align}\\] Both null and alternative hypotheses are mutually exclusive, meaning that if one hypothesis is valid, then the other must be invalid. Note that alternative hypothesis can either be directional or non-directional. For example, if we claim that observation group A is greater than or lesser than observation group B, then this is a directional alternative hypothesis. On the other hand, if we compare group A and group B only to know if there is a difference between the two regardless of whether one is better or worse or greater than or lesser than the other, then this is a non-directional alternative hypothesis. In the case of our IQ level case above, our alternative hypothesis is directional because we claim that our sample mean is greater than the population mean in terms of IQ level. After formulating our null and alternative hypotheses, the next step is to disprove one of the claims, effectively proving the other. Our strategy is to test our claim on the alternative hypothesis, \\(\\mathbf{H_1}\\), in order to decide on what to do with the null hypothesis based on two outcomes: An outcome that rejects the claim made on the null hypothesis. It suggests that the sampled data, otherwise our approach, may need more investigation. In this case, the Alternative Hypothesis holds. An outcome that fails to reject the claim made on the null hypothesis. It suggests that the sampled data is a good representation of the population data. Here, if we fail to reject the claim, then the null Hypothesis holds. Also, it helps to be familiar with two types of errors in testing for the hypothesis: Type I error - We get this error if our test ends up rejecting a claim even though the null hypothesis - the claim - is actually true. We call this false positive. Type II error - We get this error if our test ends up failing to reject a claim even though the null hypothesis - the claim - is false. We call this false-negative. In the next sections, we discuss methods of testing our hypothetical estimates. Before we jump to the next sections, however, it may help to introduce three terms that will often describe the relationships of data points: Regression - When we sample a population, we tend to measure our observation based on estimates only. We may not know the exact value of the samples we have observed. Therefore, we can use as many available methods of measurements and perform adjustments if our goal is to get our estimated value as close to the actual value. We rely on Regression to see how close our estimated value is to the actual value - if our estimate regresses to the actual value. In other words, Regression measures the relationship between the dependent and independent variables. Our discussion will primarily focus on coefficients to find any significant effect or the lack thereof. Deviance - In contrast, a population sample with estimates deviating from the true values demonstrates a deviation. Deviation happens every time our estimate value gets farther away from the true value. Our discussion will also primarily focus on coefficients. Variance - On the other hand, Variance is not a measure of whether estimated values regress to or deviate from the true value. Instead, it characterizes the distance between estimates. That means that we are not comparing the estimated and true values. Here, we are comparing two estimate values to see if they are different or not, are related or not. Our discussion will primarily focus on averages to find any significant difference or the lack thereof. Note that in the following sections, we will deal with two major types of tests (of estimates): Test to determine the significance of difference - this deals mainly with Variance. Test to determine the significance of Regression - this deals mainly with Regression and Deviation. Let us now discuss some of the standard tests used in statistics to determine the significance of difference. 6.3.2 T-Test (True Variance unknown) We use a T-Test to test a given hypothesis by computing the t-value (or t-score) to evaluate our claim (Ugoni A. and F. Walker B.F. 1995; Abebe T. H. 2020). This t-value is a value of T statistic which follows a Student’s T-distribution (W.S. Gossett 1876-1937). Let us consider three types of samples for our T-Test: One-Sample T-Test: A One-Sample T-Test uses a sample from a population to determine the significance of the sample mean against a given known mean, but with an unknown variance. Here, we perform the following test statistic equation against the sample: \\[\\begin{align} t = \\frac{observed-expected}{standard\\ error} = \\frac{\\mu_s - \\mu_0} { \\sigma_s / \\sqrt{n_s}} \\end{align}\\] where: \\(\\mathbf{t}\\) is the test statistic, following a T distribution. \\(\\mathbf{\\mu_s}\\) is the sample mean - the average (mean) of the sample, \\(\\bar{x}_s\\). \\(\\mathbf{\\sigma}_s\\) is the sample standard deviation - the standard deviation of the sample. \\(\\mathbf{n_s}\\) is the size of the sample data. \\(\\mathbf{\\mu_0}\\) is a given hypothesized population mean. (Note that \\(\\mathbf{\\sigma}_p\\) - the population standard deviation - is unknown). See Conjugacy section in Bayesian Computation for unknown variance. The sample standard deviation (along with its corresponding sample variance) is expressed as: \\[\\begin{align} \\sigma_s = \\sqrt{\\frac{1}{n-1}\\left(\\sum_{i=1}^n (x_i - \\bar{x})^2\\right)} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\sigma_s^2\\ \\leftarrow\\ sample\\ variance \\end{align}\\] The standard error is expressed as: \\[\\begin{align} SE = \\frac{\\sigma_{s}}{\\sqrt{n}} \\end{align}\\] The margin of error is expressed as: \\[\\begin{align} me = t \\times SE \\end{align}\\] The confidence interval is expressed as: \\[\\begin{align} C.I. = \\mu_{s} \\pm me \\end{align}\\] To illustrate our example of IQ levels once again, suppose we take a sample of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Assume an average IQ level of 100. Here is what the t-value looks like in R-code: set.seed(1) statistic &lt;- function(x) { s = 0 m = mean(x) n = length(x) for (i in 1:n) { s = s + (x[i] - m)^2 } sd = sqrt( 1/ (n-1) * s) list(&quot;mean&quot;=m, &quot;sd&quot;=sd, &quot;sum_squared&quot;=s, &quot;n&quot;=n ) } t_test &lt;- function(x1, x2 = NULL, mu, paired=FALSE) { if (is.null(x2)) { # one sample t-test stat = statistic(x1) t = (stat$mean - mu) / (stat$sd / sqrt(stat$n)) } else if (paired == FALSE) { # two sample t-test stat1 = statistic(x1) stat2 = statistic(x2) pooled_variance = ( stat1$sum_squared + stat2$sum_squared ) / ( stat1$n + stat2$n - 2) t = (stat1$mean - stat2$mean) / (sqrt( pooled_variance * ( 1/stat1$n + 1/stat2$n))) } else { # paired t-test stat = statistic(x1 - x2) # get the difference t = (stat$mean - mu) / (stat$sd / sqrt(stat$n)) } list(&quot;statistic&quot;=t) } # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) population = sample(x = iq_range, size=100, replace=TRUE) # we take a sample of 25 individuals and record their IQ level x = sample(x = population, size=25, replace=FALSE) # we use a hypothesized average of 100 (tvalue = t_test(x1=x, mu=100)$statistic ) ## [1] 3.535 To validate if we get the same result, we use the built-in R function called “t.test()” from stats package: (tvalue = stats::t.test(x, y = NULL, mu = 100, alternative=&quot;greater&quot;)$statistic[[&quot;t&quot;]]) ## [1] 3.535 Two-Sample T-Test: A Two-Sample T-Test uses two samples from a population and performs the following test statistic equation against the two samples: \\[\\begin{align} t = \\frac{\\bar{x}_1 - \\bar{x}_2 } { \\sqrt{\\sigma_{ss}^2\\left(\\frac{1}{n_1}+\\frac{1}{n_2}\\right)}} \\end{align}\\] where: \\(\\mathbf{t}\\) is the test statistic. \\(\\bar{x}_2, \\bar{x}_2\\) are two sample means \\(\\sigma_{ss}^2\\) is the pooled sample variance. \\(n_1\\ and\\ n_2\\) are the sizes of the two samples respectively. The pooled sample variance is a merge of the individual variance computed as: \\[\\begin{align} \\sigma_{ss}^2 = \\frac{ \\sum_{i=0}^{n_1} (x_i - \\bar{x}_1)^2 + \\sum_{j=0}^{n_2} (x_j - \\bar{x}_2)^2 }{n_1 + n_2 - 2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\sigma_{ss}^2\\ \\leftarrow\\ pooled\\ variance \\end{align}\\] To illustrate, suppose we take two separate samples of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Here, we do not have a given “hypothesized” population mean, \\(\\mathbf{\\mu}\\), because we are comparing the mean of two separate samples. Here is what the t-value looks like: set.seed(1) # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) # assume a population (though we use the sample) population = sample(x = iq_range, size=100, replace=TRUE) # we take two samples of 25 individuals and record their IQ level x1 = sample(x = population, size=25, replace=FALSE) x2 = sample(x = population, size=25, replace=FALSE) (tvalue = t_test(x1=x1, x2=x2)$statistic) ## [1] 0.441 Let us use “t.test()” function to validate: (tvalue = stats::t.test(x = x1, y = x2, alternative=&quot;greater&quot;)$statistic[[&quot;t&quot;]]) ## [1] 0.441 Paired T-Test: A Paired T-Test uses two samples from a population. Here, the two samples are taken from the same population. However, instead of performing a Two-Sample T-Test, we perform a One-Sample T-Test. To do that, we generate new sample data using the difference between the two samples. For example, let us generate two samples of 25 individuals from a population and get the difference. Assume an average IQ level of 100. set.seed(1) # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) # hypothesized population population = sample(x = iq_range, size=100, replace=TRUE) # we take two samples of 25 individuals and record their IQ level x1 = sample(x = population, size=25, replace=FALSE) x2 = sample(x = population, size=25, replace=FALSE) # we use a hypothesized average of 100 (tvalue = t_test(x1=x1, x2=x2, mu=100, paired=TRUE)$statistic ) ## [1] -17.91 That is equivalent to: (tvalue = t_test(x1=(x1 - x2), mu=100, paired=FALSE)$statistic ) ## [1] -17.91 and using “t.test()” to validate, we get: (tvalue = stats::t.test(x = x1,y = x2, mu=100, paired=TRUE)$statistic[[&quot;t&quot;]]) ## [1] -17.91 So far, all we did in running the three types of T-tests is to get the t-value. The next step is to use the t-value to evaluate our Hypothesis. However, let us first take a careful look at Figure 6.7 to visualize a two-tail test and Figure 6.8 to visualize a one-tail test. Figure 6.7: Critical Value vs Significance Level - Two-Tail Test Figure 6.8: Critical Value vs Significance Level - One-Tail Test In the figures, we introduce five concepts. First, we introduce the concept of Confidence level. There are three commonly used levels of confidence: 99%, 95%, and 90%. For example, a confidence level of 90% means that we are 90% confident that repeated sampling of a population renders the same outcome. Second, we introduce the concept of Significance level which may complement confidence level only in so far as, for example, if the confidence level is 99%, then the significance level is 1% - or if the confidence level is 90%, then the significance level is 10%. In other words, there are also three commonly used levels of significance: 1%, 5%, 10%. Significance level is also denoted as the alpha - \\(\\mathbf{\\alpha}\\) - which corresponds to 0.01, 0.05, 0.10. For example, an alpha value - or significant level - of 0.10 means a 10% probability that the observed result is at least as extreme as the computed test statistic - e.g., the t-value - when the null hypothesis is true. Except just perhaps being a common practice, there is no rule to prevent us from using our own confidence level and significance level depending on our domain or area of expertise. We may, however, prefer a more stringent level. However, for illustration, let us continue to keep those three levels in our discussions. Third, we introduce the concept of a Critical value. A Critical value can be computed based on the Significance level. For example, a significance level of 0.01, 0.05, or 0.10 has the following corresponding computed critical value for a one-tail test: alpha=c(0.01, 0.05, 0.10) # compute for quartile lt = round( qt(alpha/2, df=Inf), 2) # reverse alpha then compute for quartile ut = round( qt(1-rev(alpha)/2, df=Inf),2) list(&quot;lower_tail&quot;= lt, &quot;upper_tail&quot;=ut) ## $lower_tail ## [1] -2.58 -1.96 -1.64 ## ## $upper_tail ## [1] 1.64 1.96 2.58 A significance level of 0.01, 0.05, and 0.10 respectively for a two-tail test have the following computed critical values: alpha=c(0.01, 0.05, 0.10) # compute for quartile lt = round( qt(alpha, df=Inf), 2) # reverse alpha then compute for quartile ut = round( qt(1-rev(alpha), df=Inf),2) list(&quot;lower_tail&quot;= lt, &quot;upper_tail&quot;=ut) ## $lower_tail ## [1] -2.33 -1.64 -1.28 ## ## $upper_tail ## [1] 1.28 1.64 2.33 Fourth, we introduce the concept of p-value. Let us recall the discussion around CDF - cumulative density function. Here, the p-value uses the CDF of a T-distribution. As an example, let us compute for the CDF using the \\(\\mathbf{H_1}\\) - our alternative hypothesis: \\[ P(\\mu &gt; 100 | \\mu_0 = 100) \\] We interpret that as the probability that the sample mean is greater than 100, given a true value (true mean) equal to 100. Let us use our T-distribution CDF function t_cdf(.) to compute for the p-value using the generated t-value (note that we sampled 25 individuals, n=25): set.seed(1) # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) population = sample(x = iq_range, size=5000, replace=TRUE) # we take a sample of 25 individuals and record their IQ level x = sample(x = population, size=25, replace=FALSE) tvalue = t_test(x1=x, mu=100)$statistic cdf = t_cdf(tvalue, df=25-1) pvalue_left = cdf pvalue_right = pvalue = 1 - cdf tvalue # T value ## [1] 4.519 pvalue_left # P value (area) towards left of T value ## [1] 0.9999 pvalue_right # P value (area) towards right of T value ## [1] 0.00007054 To validate, let us use the built-in R functions t.test() and pt() to compute for the p-value: tvalue = t.test(x=x, mu=100, alternative=&quot;less&quot;)$statistic[[&quot;t&quot;]] pvalue_left = pt(tvalue, df=25-1, lower.tail=TRUE) pvalue_right = pt(tvalue, df=25-1, lower.tail=FALSE) tvalue # T value ## [1] 4.519 pvalue_left # P value (area) towards left of T value ## [1] 0.9999 pvalue_right # P value (area) towards right of T value ## [1] 0.00007054 As we can see from the output, we have both t-value and p-value. Fifth and last, we re-introduce the directional and non-directional alternative hypothesis, which we evaluate based on whether we are performing a two-tail or one-tail test. A one-tail test corresponds to evaluating a one-directional hypothesis that tends to compare whether an observation is better or worse, greater or lesser than another observation. A two-tail test evaluates a non-directional alternative that tends to determine if there is a difference between the two observations regardless of direction. With all those five introductory concepts and to now evaluate our null hypothesis, we can use two methods: Use a t-value based on a chosen critical value as our comfortable threshold (establishing our rejection region). For a two-tail test, a common choice for critical value is based on: \\(\\pm 2.58, \\pm 1.96, \\pm 1.65\\). For a one-tail test, we use any of the following: \\(\\pm 2.33, \\pm 1.64, \\pm 1.28\\). Use a p-value based on a chosen significance level as our comfortable threshold (establishing the area - the alpha - under the curve as our rejection region). For a two-tail test, we can choose any of the given significance level: \\(0.005, 0.025, 0.05\\). For a one-tail test, we use any of the following: \\(0.01, 0.05, 0.10\\). To illustrate, let us continue to use our IQ level samples. Suppose that the average IQ level is 100. Now, let us make a claim, \\(\\mathbf{H_1}\\), that our sample mean is greater than the average IQ level, \\(\\mu\\). Here is what it looks like: \\[\\begin{align*} H_0 {}&amp;: \\mu= 100,\\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ \\text{null hypothesis}\\\\ H_1 &amp;: \\mu &gt; 100 \\end{align*}\\] Our computed t-value is 4.5191 and our computed p-value is 7.054310^{-5}. If our confidence level is 99% and we are performing a one-tail test, and our alternative hypothesis tends toward the right because we are claiming an IQ level greater than 100, then we have the following: 4.5191 &lt; 2.58 This means that our t-value is not in the extreme right-side rejection region (See Figure 6.8). The extreme right-side rejection region is the region in which we reject our null hypothesis. Moreover, because our t-value is not anywhere in that region, we can conclude that we fail to reject our claim that there is no difference given the null hypothesis is true. Our alternative hypothesis holds that the IQ level in the sample is greater than 100. On the other hand, we also see that our p-value has the following: 7.054310^{-5} &gt; 0.01 Because our p-value is greater than the significance level of 0.01, we get the same conclusion - that is, failing to reject our claim that there is no difference given the null hypothesis is true. Our three confidence levels for a right-side one-tail fail to reject our claim, given our null hypothesis is true. See Table 6.4. Table 6.4: Significance Level Conf Level T Value P Value Significance Critical Direction Analysis 99% 4.519 0 p &gt; 0.01 t &lt; 2.58 right-side Fail to Reject 95% 4.519 0 p &gt; 0.05 t &lt; 1.96 right-side Fail to Reject 90% 4.519 0 p &gt; 0.10 t &lt; 1.65 right-side Fail to Reject Using the built-in R function t.test(), we can implement the same IQ level case in many ways with a summary: # One Sample T-Test, with confidence level 99%, tends toward left # with (H0 = U0 vs H1 &lt; U0) t.test(x=x, mu=100, alternative=&quot;less&quot;, conf.level=0.99) ## ## One Sample t-test ## ## data: x ## t = 4.5, df = 24, p-value = 1 ## alternative hypothesis: true mean is less than 100 ## 99 percent confidence interval: ## -Inf 123.5 ## sample estimates: ## mean of x ## 115.1 # One Sample T-Test, with confidence level 99%, tends toward right # with (H0 = U0 vs H1 &gt; U0) t.test(x=x, mu=100, alternative=&quot;greater&quot;, conf.level=0.99) ## ## One Sample t-test ## ## data: x ## t = 4.5, df = 24, p-value = 7e-05 ## alternative hypothesis: true mean is greater than 100 ## 99 percent confidence interval: ## 106.8 Inf ## sample estimates: ## mean of x ## 115.1 # One Sample T-Test, with confidence level 99%, two-sided # with (H0 = U0 vs H1 &lt;&gt; U0) t.test(x=x, mu=100, alternative=&quot;two.sided&quot;, conf.level=0.99) ## ## One Sample t-test ## ## data: x ## t = 4.5, df = 24, p-value = 0.0001 ## alternative hypothesis: true mean is not equal to 100 ## 99 percent confidence interval: ## 105.8 124.5 ## sample estimates: ## mean of x ## 115.1 # Two Sample T-Test, tends toward the right # with (H0 = U0 vs H1 &gt; U0) t.test(x = x1, y = x2, alternative=&quot;greater&quot;) ## ## Welch Two Sample t-test ## ## data: x1 and x2 ## t = 0.44, df = 48, p-value = 0.3 ## alternative hypothesis: true difference in means is greater than 0 ## 95 percent confidence interval: ## -6.168 Inf ## sample estimates: ## mean of x mean of y ## 112.8 110.6 # Two Sample T-Test with pooled variances # with (H0 = U0 vs H1 &lt;&gt; U0) t.test(x = x1, y = x2, var.equal=TRUE) ## ## Two Sample t-test ## ## data: x1 and x2 ## t = 0.44, df = 48, p-value = 0.7 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.831 12.231 ## sample estimates: ## mean of x mean of y ## 112.8 110.6 # Two Sample T-Test, two-sided, with pooled variances # with (H0 = U0 vs H1 &lt;&gt; U0) t.test(x = x1, y = x2, alternative=&quot;two.sided&quot;, var.equal=TRUE) ## ## Two Sample t-test ## ## data: x1 and x2 ## t = 0.44, df = 48, p-value = 0.7 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -7.831 12.231 ## sample estimates: ## mean of x mean of y ## 112.8 110.6 A Student’s T table is provided (See Table 15.4) as a reference in the Appendix. Given degrees of freedom and significance level, one can cross-reference both parameters to arrive at a t-value. For example: alpha = 0.01 n = 5 # df = n - 1 qt(p=alpha, df=n-1, lower.tail=TRUE) # left-side one-tail crit value ## [1] -3.747 # We can use: 1 - qt(p = alpha, df=n-1, lower.tail=TRUE) # or we can use the following: qt(p=alpha, df=n-1, lower.tail=FALSE) # right-side one-tail crit value ## [1] 3.747 # As for two-tail, we split into two areas: 0.01/2 = 0.005 two_tail = qt(p = alpha / 2 , df=n-1, lower.tail=FALSE) c(-two_tail, two_tail) # left and right critical values ## [1] -4.604 4.604 For samples based on multivariate normal distribution, we leave readers to investigate Hotelling’s T^2 statistic. 6.3.3 Z-Test (True Variance known) We use a Z-Test to test a given hypothesis by computing for the z-score (or standard score) to evaluate our claim. In Z-Test, we assume that our z-score follows a normal distribution and that the population mean and variance are known. Our z-score statistic is therefore expressed as: \\[\\begin{align} z = \\frac{observed-expected}{standard\\ error} = \\frac{ (x - \\mu_p)}{\\sigma_p } \\end{align}\\] where: \\(\\mathbf{z}\\) is the test statistic, following a normal distribution, \\(\\mathbf{\\mu_p}\\) is a given hypothesized population mean, \\(\\mu_p\\) = \\(\\mu_0\\), \\(\\mathbf{\\sigma_p}\\) is a given hypothesized population standard deviation. Here, our statistic is based on an individual observation, \\(\\mathbf{x}\\). Equivalently, we can also use the following equation for a group of observations: \\[\\begin{align} z = \\frac{ (\\mu_s - \\mu_p)}{ SE(\\mu_s )} \\end{align}\\] where: \\(\\mathbf{z}\\) is the test statistic, following a normal distribution, \\(\\mathbf{\\bar{u}_s}\\) is the sample mean, \\(\\bar{x} = \\mu_s\\), \\(\\mathbf{\\mu_p}\\) is a given hypothesized population mean, \\(\\mu_p\\) = \\(\\mu_0\\). This alternative equation is based on the idea of the Central Limit Theorem which states that as the sample size increases (see Figure 5.30), it comes to a point where the sample mean follows a normal distribution (Kwak S. G., Kim J. H. 2016; Illowsky B, Dean S. 2018). That means that the sampling distribution starts to follow the population distribution closely. The difference is that the standard deviation of the sample is computed based on the standard error equation below: \\[\\begin{align} SE(\\bar{x}) = \\frac{\\sigma_p^2}{n_s} = \\frac{\\sigma_p}{\\sqrt{n_s}} \\end{align}\\] Figure 6.9 shows a standard normal distribution. The x-axis represents the standard deviation which is where the z-score is measured - it is a measure of the distance between our standard deviations and the mean, meaning how many standard deviations away we are from the mean. For example, if the z-score is positive two, then we are two standard deviations away from the right to the mean, \\(\\bar{x}_s = \\mu_s = 0\\). And if our z-score is negative two, then we are two standard deviations away from the left to the mean, \\(\\bar{x}_s = \\mu_s = 0\\). Therefore the unit of measurement for a z-score is in standard deviations. Figure 6.9: Standard Normal Distribution To illustrate, suppose we have a list of 40 observations sampled from a population. We can derive the z-score for each data in the sample as such: set.seed(1) z_score &lt;- function(x, mu, sd) { (x - mu) / sd } # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) # hypothesized mean and sd mu = 100; sd = 16.73 population = sample(x = iq_range, size=1000, replace=TRUE) # we take a sample of 40 individuals and record their IQ level # display only the first 10 items ( sample_data = sample(x = population, size=40, replace=FALSE) )[1:10] ## [1] 116 116 120 119 86 124 121 130 131 121 # The equivalent z-scores for each element in sample x. # display only the first 10 items (zscore = round( z_score(x=sample_data, mu=mu, sd = sd), 2))[1:10] ## [1] 0.96 0.96 1.20 1.14 -0.84 1.43 1.26 1.79 1.85 1.26 Similarly, we can derive back the raw data given a z-score using the following formula: \\[\\begin{align} x = z \\sigma + \\mu \\end{align}\\] For example, to validate the z-score we derived, we use the formula above to compare it with our sample data. # display only the first 10 items ( round( zscore * sd + mu, 0) == sample_data )[1:10] ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE Now for a better illustration, let us again use the IQ level case where we continue to assume an IQ level average of 100 and a standard deviation of 16.73. Let us determine the proportion of people with IQ levels greater than 90, lesser than 110, and between 90 and 110. First, let us get the z-scores. x = list(&quot;min&quot;=90, &quot;max&quot;=110) (zscore = round( z_score(x= c(x$min, x$max), mu=mu, sd = sd), 2)) ## [1] -0.6 0.6 Second, let us use the build-in R function pnorm to get the proportions (or percentage) of the population having an IQ level greater than 90. \\[ P(z &gt; 90) \\] set.seed(1) # Get proportions using standard normal distribution (z-score) mu = 0; sd = 1 proportion = pnorm(q = zscore[1], mean=mu, sd=sd, lower.tail=FALSE) list(&quot;proportion&quot; = paste( round((proportion)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;72.57%&quot; # Get proportions using normal distribution (raw data) mu = 100; sd = 16.73 proportion = pnorm(q = 90, mean=mu, sd=sd, lower.tail=FALSE) list(&quot;proportion&quot; = paste( round((proportion)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;72.5%&quot; Third, let us get the proportion (or percentage) of population having an IQ level lesser than 110. \\[ P(z &lt; 110) \\] set.seed(1) # Get proportions using standard normal distribution (z-score) mu = 0; sd = 1 # One way to do it proportion = pnorm(q = zscore[2], mean=mu, sd=sd, lower.tail=TRUE) # Another way to do it proportion = 1 - pnorm(q = zscore[2], mean=mu, sd=sd, lower.tail=FALSE) list(&quot;proportion&quot; = paste( round((proportion)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;72.57%&quot; # Get proportions using normal distribution (raw data) mu = 100; sd = 16.73 # One way to do it proportion = pnorm(q = 110, mean=mu, sd=sd, lower.tail=TRUE) # Another way to do it proportion = 1 - pnorm(q = 110, mean=mu, sd=sd, lower.tail=FALSE) list(&quot;proportion&quot; = paste( round((proportion)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;72.5%&quot; Lastly, let us get the proportion (or percentage) of population having IQ levels between 90 and 110. \\[ P( 90 &lt; z &lt; 110) \\] set.seed(1) # Get proportion using standard normal distribution (z-score) mu = 0; sd = 1 proportion1 = pnorm(q = zscore[2], mean=mu, sd=sd, lower.tail=TRUE) proportion2 = pnorm(q = zscore[1], mean=mu, sd=sd, lower.tail=TRUE) list(&quot;proportion&quot; = paste( round((proportion1 - proportion2)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;45.15%&quot; # Get proportion using = normal distribution (raw data) mu = 100; sd = 16.73 proportion1 = pnorm(q = 110, mean=mu, sd=sd, lower.tail=TRUE) proportion2 = pnorm(q = 90, mean=mu, sd=sd, lower.tail=TRUE) list(&quot;proportion&quot; = paste( round((proportion1 - proportion2)*100, 2) , &quot;%&quot;, sep=&quot;&quot; )) ## $proportion ## [1] &quot;45%&quot; Consequently, we can also derive the z-score and raw data using the built-in R function qnorm() given the probability (proportion). For example: # Get Z-Score using standard normal distribution mu = 0; sd = 1 round( qnorm(p = proportion2, mean=mu, sd=sd, lower.tail=TRUE), 1) ## [1] -0.6 round( qnorm(p = proportion1, mean=mu, sd=sd, lower.tail=TRUE), 1) ## [1] 0.6 # Get Raw Data using normal distribution mu = 100; sd = 16.73 qnorm(p = proportion2, mean=mu, sd=sd, lower.tail=TRUE) ## [1] 90 qnorm(p = proportion1, mean=mu, sd=sd, lower.tail=TRUE) ## [1] 110 There are two Z-tables for Z-Score that can be found in the Appendix. One table ranges from -3.9 to 0, and another table ranges from 0 to 3.9. The cross-section represents the probability. See Table 15.5 and Table 15.6 in the Appendix. So far, we have not discussed null hypothesis and alternative hypothesis in this section. Testing a hypothesis using Z-test also applies given a computation of z-score and computation of proportions. The Critical Values and Significance Level also apply. The z-score (similar to t-value) is compared against the chosen Critical Value and the proportion (similar to p-value) is compared against the chosen significance Level. From there, one can conclude whether to reject the claim, the null hypothesis - \\(\\mathbf{H_0}\\), stating that there is no difference between observations and population; otherwise, to indicate a failure to reject the claim, \\(\\mathbf{H_0}\\). Similar to T-test, we also can compute for margin of error, which is expressed as: \\[\\begin{align} me = Z \\times SE,\\ \\ \\ \\ \\ where\\ SE = \\frac{\\sigma_p}{\\sqrt{n}} \\end{align}\\] The confidence interval is expressed as: \\[\\begin{align} C.I. = \\bar{x}_{s} \\pm me \\end{align}\\] We leave readers to investigate the Two-Sample Z-Test and Paired Z-Test given the following equations: Two-Sample Z-Test \\[ z = \\frac{ (\\bar{x}_s - \\mu_p)}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_2^2}{N_2}}} \\] Paired Z-Test \\[ z = \\frac{ (\\bar{x}_{s1} - \\bar{x}_{s2}) - (\\mu_1 - \\mu_2)}{\\sqrt{\\frac{\\sigma_1^2}{N_1} + \\frac{\\sigma_2^2}{N_2}}} \\] 6.3.4 F-Test using F-ratio F-Test follows an F-distribution (Ardelean F. A. 2017; Illowsky B, Dean S. 2018). It is a test commonly used to compare variances of multiple groups independently sampled from a population (e.g., or from separate experiments) using an F statistic. To compute for the F statistic, we use the following formula (also known as ratio of variances or simply F-ratio): \\[\\begin{align} F = \\frac{\\sigma_1^2} {\\sigma_2^2}\\ \\ \\ \\ \\ where\\ \\sigma_1 &gt; \\sigma_2 \\end{align}\\] Note that we force a right-side tail by having the variance with greater value be the numerator, making it easier to interpret. For inference, our null hypothesis claims that there is no significant difference between the variance of the first sample and the variance of the second sample. That is expressed as: \\[\\begin{align*} H_0 {}&amp;: \\sigma_1^2 = \\sigma_2^2\\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ \\text{null hypothesis}\\\\ H_1 &amp;: \\sigma_1^2 \\ne \\sigma_2^2\\ \\ \\ \\ \\ \\leftarrow \\ \\ \\ \\text{alternative}\\\\ \\end{align*}\\] In terms of ratio, the null hypothesis is true only if F-ratio is 1. To illustrate, let us review the following naive implementation of F test in R code: f_test &lt;- function(x1, x2) { v1 = var(x1) v2 = var(x2) n1 = length(x1) n2 = length(x2) fratio = 0; p = 0 num = v1; denom = v2 df1 = n1 - 1; df2 = n2 - 1 greater = TRUE if (v2 &gt; v1) { num = v2; denom = v1 df1 = n2 - 1; df2 = n1 - 1 greater = FALSE } fratio = num / denom p = pf(fratio, df1, df2, lower.tail = FALSE) list(&quot;greater&quot;=greater, &quot;fratio&quot;=fratio, &quot;num df1&quot; = df1, &quot;denom df2&quot; = df2, &quot;pvalue&quot;=p) } # let us constrain IQ range between 80 and 140. iq_range = seq(80, 140) set.seed(1) population1 = sample(x = iq_range, size=100, replace=TRUE) set.seed(2) population2 = sample(x = iq_range, size=100, replace=TRUE) # we take two samples and record their IQ level set.seed(3) x1 = sample(x = population1, size=11, replace=FALSE) # group 1 x2 = sample(x = population2, size=26, replace=FALSE) # group 2 # get the F statistic fscore = f_test(x1, x2); t(fscore) ## greater fratio num df1 denom df2 pvalue ## [1,] TRUE 1.313 10 25 0.2768 Alternatively, we can use the built-in R function var.test(.). Note that the function always expects the first parameter to be the numerator. Our own f_test(.) function uses the greater sample as our numerator. Our result shows that the first sample is the lesser one; therefore, in using var.test(.), we pass the value less to the alternative parameter given we pass the second sample to the first parameter. # first parameter is the second sample, x2 # therefore, alternative=less v1 = var(x1); v2 = var(x2) if (v1 &gt; v2) { var.test(x1, x2, alternative=&quot;less&quot;, conf.level=0.90) } else { var.test(x2, x1, alternative=&quot;greater&quot;, conf.level=0.90) } ## ## F test to compare two variances ## ## data: x1 and x2 ## F = 1.3, num df = 10, denom df = 25, p-value = 0.7 ## alternative hypothesis: true ratio of variances is less than 1 ## 90 percent confidence interval: ## 0.000 2.854 ## sample estimates: ## ratio of variances ## 1.313 We can reference the F table, Table 15.7, in the Appendix to derive the critical value. Here, we see that x2 is the greater sample with degrees of freedom at 35, which becomes our numerator. Sample x1 has df at ten, and it becomes our denominator for our F ratio. Therefore, our critical value in the F table corresponds to 2.730 granting our confidence level is at 90% - equivalent to \\(\\alpha = 0.10\\). Figure 6.10: F Distribution Using Figure 6.10, we see that F is lesser than the critical value; meaning, f-score is outside the rejection region. Therefore, we fail to reject the claim, given the null hypothesis, \\(\\mathbf{H_0}\\), is true. Similarly, if we instead use a significance level of 0.05, e.g. \\(\\mathbf{\\alpha=0.05}\\), then we see that p-value 0.28 is greater than \\(\\alpha = 0.05\\). Therefore, we fail to reject the claim, given the null hypothesis, \\(\\mathbf{H_0}\\), is true. Let us now discuss a case in which we can derive the F ratio by using ANOVA. 6.3.5 F-Test with One-Way ANOVA Anova stands for analysis of variance (Ardelean F. A. 2017). There are two ways of performing ANOVA. One-Way Anova - we use this to analyze the variation of multiple groups involving one independent variable (one factor) and a dependent variable. The goal is to compare variations between groups (e.g., comparing means if they are closer or farther apart) and within groups. Two-Way Anova - we use this to analyze the variation of multiple groups involving two independent variables (two factors) and a dependent variable. The goal is to examine the effect of one factor, the other, or both factors (via interaction) against the dependent variable. Note that an F-Test using ANOVA is an Omnibus Test. To perform a one-way analysis of variance (One-Way ANOVA), we are to compute a few formulas: Sum of Squares Total (SST): \\[\\begin{align} SS_T = \\sum_{j=1}^m \\sum_{i=1}^{n_j} (x_{ji} - \\bar{x})^2 \\end{align}\\] where: m is the total number of groups sampled from a population. \\(\\mathbf{n_j}\\) is the sample size of sample j. \\(\\mathbf{x_{ji}}\\) is the ith observation in sample j. \\(\\mathbf{\\bar{x}}\\) is the overall grand mean. Sum of Squares between samples (SSB): \\[\\begin{align} SS_B = SS_{(treatment)} = \\sum_{j=1}^m \\sum_{i=1}^{n_j} (\\bar{x}_j - \\bar{x})^2 = \\sum_{j=1}^m n_j (\\bar{x}_j - \\bar{x})^2 \\end{align}\\] where: \\(\\mathbf{SS_B}\\) is also called the sum of squares treatment. m is the total number of samples. \\(\\mathbf{n_j}\\) is the sample size of sample j. \\(\\mathbf{\\bar{x}_j}\\) is the jth sample mean. \\(\\mathbf{\\bar{x}}\\) is the overall grand mean. Sum of Squares within samples (SSW): \\[\\begin{align} SS_W = SS_{error} = \\sum_{j=1}^m \\sum_{i=1}^{n_j} (x_{ji} - \\bar{x}_j)^2 \\end{align}\\] where: \\(\\mathbf{SS_W}\\) is also called the sum of squares error. m is the total number of samples. \\(\\mathbf{n_j}\\) is the sample size of sample j. \\(\\mathbf{x_{ji}}\\) is the ith observation in sample j. \\(\\mathbf{\\bar{x}_j}\\) is the jth sample mean. Note here that SST can also be computed as: \\[\\begin{align} SS_T = SS_B + SS_W \\end{align}\\] Mean Squares between samples (MSB): \\[\\begin{align} MS_B = s^2_B = \\frac{SS_B}{df_B},\\ \\ \\ \\ \\ \\ df_B = m - 1 \\end{align}\\] where: \\(\\mathbf{MS_B}\\) is also called the mean of squares treatment. m is the total number of samples. \\(\\mathbf{df_B}\\) is the degrees of freedom between samples. Mean Squares within samples (MSW): \\[\\begin{align} MS_W = MS_E = s^2_w = \\frac{SS_W}{df_W},\\ \\ \\ \\ \\ \\ df_W = \\sum_j^m (n_j - 1) \\end{align}\\] where: \\(\\mathbf{MS_W}\\) is also called the within mean square (error). N is the overall total observations, e.g., N = mn. m is the total number of samples. \\(\\mathbf{n_j}\\) is the sample size of sample j. \\(\\mathbf{df_W}\\) is the degrees of freedom within samples. Finally, we compute for the F statistic: \\[\\begin{align} F = \\frac{explained}{unexplained} = \\frac{s^2_B}{s^2_W} = \\frac{MS_B}{MS_W} \\end{align}\\] All the computation is reflected in the following One-Way ANOVA table (Larson M. G. 2008; Illowsky B, Dean S. 2018): Table 6.5: One-Way ANOVA Source of Variation Degrees of Freedom Sum of Squares Mean Squares F Between \\(df_{B}: m- 1\\) \\(SS_B\\) \\(MS_B: \\frac{SSB}{df_B}\\) \\(\\frac{MSB}{MSW}\\) Within (Error) \\(df_{W}: \\sum_j^m (n_j - 1)\\) \\(SS_W\\) \\(MS_W: \\frac{SS_W}{df_W}\\) Total \\(df_T: \\sum_j^m (n_j) - 1\\) \\(SS_T\\) Below is a naive implementation of One-Way ANOVA in R code: # Supports different group sizes one_way_anova &lt;- function(dependent, factor = NULL, size = 0) { if (is.null(factor)) { m = length(dependent) x = dependent } else { groups = levels(as.factor(factor)) m = length(groups) # number of groups x = list() # Group factors for (j in 1:m) { idx = which(factor == groups[j]) x[[j]] = dependent[idx] } } # Get the grand mean grand_mean = 0 r = 0 for (j in 1:m) { n = length(x[[j]]) # size of each group r = r + n for (i in 1:n) { grand_mean = grand_mean + x[[j]][i] } } grand_mean = grand_mean / r # sum squared total SST = 0 for (j in 1:m) { n = length(x[[j]]) # size of each group for (i in 1:n) { SST = SST + (x[[j]][i] - grand_mean)^2 } } # sum square between SSB = 0 for (j in 1:m) { grp_mean = mean(x[[j]]) n = length(x[[j]]) # size of each group SSB = SSB + n*(grp_mean - grand_mean)^2 } # sum square within SSW = 0 for (j in 1:m) { grp_mean = mean(x[[j]]) n = length(x[[j]]) # size of each group for (i in 1:n) { SSW = SSW + (x[[j]][i] - grp_mean)^2 } } # mean squared between dfB = m - 1 MSB = SSB / dfB # mean squared within dfW = 0 # assume different group sizes; otherwise, use m * (n - 1) for (j in 1:m) { dfW = dfW + ( length(x[[j]]) - 1 ) } MSW = SSW / dfW # F statistic F = MSB / MSW # return statistics c( &quot;SSB&quot;=SSB, &quot;SSW&quot;=SSW, &quot;dfB&quot;=dfB,&quot;dfW&quot;=dfW, &quot;MSB&quot;=MSB, &quot;MSW&quot;=MSW, &quot;F&quot;=F) } Let us consider here two cases. One case would be three nearly distinct populations - forcing three sample means to be farther apart. See below: # Now let us generate three groups (of population) # with different ranges to force different means. # The use of different random seed helps to # generate independent groups set.seed(10) iq_range = seq(70, 120) population1 = sample(x = iq_range, size=500, replace=TRUE) set.seed(20) iq_range = seq(80, 130) population2 = sample(x = iq_range, size=500, replace=TRUE) set.seed(30) iq_range = seq(90, 140) population3 = sample(x = iq_range, size=500, replace=TRUE) # let us now sample observations from each group # with same sample sizes set.seed(4) sample_size = 10 x1 = sample(x = population1, size=sample_size, replace=FALSE) x2 = sample(x = population2, size=sample_size, replace=FALSE) x3 = sample(x = population3, size=sample_size, replace=FALSE) # Generate One-Way Anova x = list() ; x[[1]] = x1 ; x[[2]] = x2; x[[3]] = x3 (anova = one_way_anova(x)) ## SSB SSW dfB dfW MSB MSW F ## 540.867 5124.500 2.000 27.000 270.433 189.796 1.425 And to see if we reject the null hypothesis, we evaluate our f-value: (Fcrit = qf(p = 0.05, df1 = anova[&quot;dfB&quot;], df2 = anova[&quot;dfW&quot;], lower.tail=FALSE)) ## [1] 3.354 (Fvalue = as.numeric(anova[&quot;F&quot;])) &gt; Fcrit ## [1] FALSE and our p-value: (Pvalue = pf(Fvalue, anova[&quot;dfB&quot;], anova[&quot;dfW&quot;], lower.tail=FALSE)) ## [1] 0.2581 Pvalue &lt; ( alpha = 0.05 ) ## [1] FALSE Both outcomes show as TRUE meaning, we reject the null hypothesis - there is a significant difference between the groups. We can also validate using the built-in function called aov(.): A= cbind(x1, rep(n=sample_size, 1)) B= cbind(x2, rep(n=sample_size, 2)) C= cbind(x3, rep(n=sample_size, 3)) data = A data = rbind(data, B) data = rbind(data, C) colnames(data) = c(&quot;y&quot;, &quot;x&quot;) aov.model = aov(y ~ as.factor(x), data = as.data.frame(data)) summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(x) 2 541 270 1.42 0.26 ## Residuals 27 5124 190 We can use df.residual(.) to get the degrees of freedom (dfW) for the within-group: df.residual(aov.model) ## [1] 27 We can use deviance(.) to get the sum square (SSW) for the within-group: deviance(aov.model) ## [1] 5124 Finally, we can get MSW by dividing deviance by degrees of freedom: deviance(aov.model)/df.residual(aov.model) ## [1] 189.8 The other case would be to take three samples from the same population - forcing sample means between samples to be closer together. An example of this scenario is when performing regression against a repeated sampling of the same population. set.seed(1) iq_range = seq(80, 140) # Now let us generate a single population ( a single group ) population = sample(x = iq_range, size=500, replace=TRUE) # let us now sample observations from each group # with same sample sizes set.seed(4) sample_size = 300 x1 = sample(x = population, size=sample_size, replace=FALSE) x2 = sample(x = population, size=sample_size, replace=FALSE) x3 = sample(x = population, size=sample_size, replace=FALSE) # Generate One-Way Anova x = list() ; x[[1]] = x1 ; x[[2]] = x2; x[[3]] = x3 (anova = round(one_way_anova(x),3)) ## SSB SSW dfB dfW MSB MSW F ## 422.909 266582.687 2.000 897.000 211.454 297.194 0.712 And to see if we reject the null hypothesis, we verify critical value first: (Fcrit = qf(p = 0.05, df1 = anova[&quot;dfB&quot;], df2 = anova[&quot;dfW&quot;], lower.tail=FALSE)) ## [1] 3.006 (Fvalue = as.numeric(anova[&quot;F&quot;])) &gt; Fcrit ## [1] FALSE and validate with P-value: (Pvalue = pf(Fvalue, anova[&quot;dfB&quot;], anova[&quot;dfW&quot;], lower.tail=FALSE)) ## [1] 0.4909 Pvalue &lt; ( alpha = 0.05 ) ## [1] FALSE Both outcomes show as FALSE meaning, we fail to reject the null hypothesis - there is no difference between the groups. As can be seen, the F statistic is a ratio that reflects the different distributions. Our case demonstrates that if the means are farther from each other between groups, meaning that MSB is greater than MSW, then our F ratio is greater than one. On the other hand, if the samples are almost overlapping, meaning that MSB and MSW are almost identical, we see F ratio to be closer to one. And if MSW is greater than MSB, it gets closer to zero. The differences in means are further discussed in the Post-HOC section. In terms of evaluation of hypothesis, it is clear that if the F ratio is farther away from one, then we reject the null hypothesis, \\(\\mathbf{H_0}\\). Otherwise, if F ratio is close to or equal to one, we fail to reject the null hypothesis, which claims that there is no difference between samples - that their means are somehow equal. Alternatively, the \\(\\mathbf{H_1}\\) has at least one mean to be different from the others. \\[\\begin{align} H_0: \\mu_1 = \\mu_2 = \\mu_3,\\ \\ \\ \\ \\ \\ \\ H_1: \\mu_1 \\ne \\mu_2 \\ne \\mu_3 \\end{align}\\] Let us revisit One-Way ANOVA when we cover Tukey’s method under the Post-Hoc Analysis section using a more practical dataset called mtcars. 6.3.6 F-Test with Two-Way ANOVA In One-Way ANOVA, we show how to perform a test with one independent nominal variable. In Two-Way ANOVA, we show how to analyze with two independent nominal variables (or factors) and a scale-level dependent variable. For example, suppose we are a dog food company, and we have invited dog owners across the country to participate in our marketing campaign to try our three new brands of dog food (call it A, B, C) specifically made for tiny, adorable cross-breed dogs of the following breed (ref: https://dogtime.com/dog-breeds/profiles): Cavachon (Cavalier King Charles Spaniel and Bichon Frise) Cavapoo (Cavalier King Charles Spaniel and Poodle) Maltipoo (Maltese and Poodle) Pomchi (Pomeranian and Chihuahua) Shichon (Shih Tzu and Bichon Frise) Shih-Poo (Shih Tzu and Toy Poodle) Shorkie (Shih Tzhu and Yorkshire Terrier) Westiepoo (West Highland White Terrier and Poodle) There are 24 combinations to test. We scheme to have four trials for each combination. In other words, we need four subjects from each cross-breed dog type to try one brand of dog food. That makes around 96 volunteered dogs. Each dog is given a 100-gram sample of the new brand (assume no bias - e.g., dog age, dog weight, dog mood, dog health, etc.). Each combination, therefore, makes around 400 grams. Here, we are to measure how many grams are consumed. Here is what we get (fictitious outcome): # let us use a fix range of consumed grams between 0 to 1000 grams. grams = seq(0, 100) a = 3; b = 8; r = 4; n = a * b * r # let us create three brands of food, with 4 replications for # each cross-breed per brand. set.seed(1); A = replicate(r, sample(x = grams, size= b, replace=TRUE)) set.seed(2); B = replicate(r, sample(x = grams, size= b, replace=TRUE)) set.seed(3); C = replicate(r, sample(x = grams, size= b, replace=TRUE)) subjects = matrix(0, 8, 3, byrow=FALSE) for (i in 1:b) { subjects[i,1] = paste(A[i,], collapse=&quot;,&quot;) subjects[i,2] = paste(B[i,], collapse=&quot;,&quot;) subjects[i,3] = paste(C[i,], collapse=&quot;,&quot;) } colnames(subjects) = c( &#39;Brand A&#39;, &#39;Brand B&#39;, &#39;Brand C&#39; ) rownames(subjects) = c( &#39;Cavachon&#39;, &#39;Cavapoo&#39;, &#39;Maltipoo&#39;, &#39;Pomchi&#39;, &#39;Shichon&#39;, &#39;Shih-Poo&#39;, &#39;Shorkie&#39;, &#39;Westiepoo&#39; ) knitr::kable( subjects, caption = &#39;Grams consumed by 4 dogs per cross-breed per brand&#39;, booktabs = TRUE, escape=FALSE) Table 6.6: Grams consumed by 4 dogs per cross-breed per brand Brand A Brand B Brand C Cavachon 26,63,72,26 18,47,98,35 16,58,11,23 Cavapoo 37,6,100,38 70,55,22,49 81,63,71,79 Maltipoo 57,20,38,1 57,55,44,15 38,51,90,60 Pomchi 91,17,78,38 16,24,7,36 33,51,28,91 Shichon 20,69,94,87 95,76,66,97 60,53,23,56 Shih-Poo 90,38,21,34 95,18,39,13 61,56,1,76 Shorkie 95,77,65,48 13,40,84,1 12,87,13,38 Westiepoo 66,50,12,60 84,86,15,16 29,83,9,37 Here, we deal with two factors which we label as A and B: A - Dog Food Type B - Dog Breed with the following size of each factor: a - number of food types, where a = 3 b - number of dog breeds, where b = 8 r - number of subjects (trials) per combination, where r = 4 n - grand total number of subjects, where n = abr = \\(3\\times 8 \\times 4 = 96\\). With that, let us step through the Two-Way ANOVA process. First, we need to solve for five sum of squared values: Solving sum of squared values for factor A \\[\\begin{align} S_A {}&amp;= \\frac{ \\sum^a (\\sum^b\\sum^r A)^2 }{b r} \\label{eqn:eqnnumber29} \\\\ &amp;= \\frac{(\\sum\\text{Brand A})^2 + (\\sum\\text{Brand B})^2 + (\\sum\\text{Brand C})^2}{8 \\times 4} \\nonumber \\\\ &amp;= \\frac{ \\left( \\begin{array}{c} (67 + 58 + 72 + 88 + ... + 81 + 6 + 33 + 73)^2 + \\\\ (84 + 80 + 95 + 42 + ... + 92 + 79 + 99 + 82)^2 + \\\\ ( 4 + 73 + 4 + 39 + ... + 19 + 28 + 11 + 73)^2 \\end{array} \\right) } {8 \\times 4} \\nonumber \\end{align}\\] (S_A = ( sum(A)^2 + sum(B)^2 + sum(C)^2 ) / (b * r)) ## [1] 226362 Solving sum of squared values for factor B \\[\\begin{align} S_B {}&amp;= \\frac{ \\sum^b (\\sum^a\\sum^r B)^2 }{a r} \\label{eqn:eqnnumber26} \\\\ &amp;= \\frac{(\\sum\\text{Cavachon})^2 + (\\sum\\text{Cavapoo})^2 + ... + (\\sum\\text{ Westiepoo})^2}{3 \\times 4} \\nonumber \\\\ &amp;= \\frac{ \\left( \\begin{array}{c} (67 + 58 + 72 + 88 + 84 + 80 + 95 + 42 + 4 + 73 + 4 + 39)^2 + \\\\ (38 + 50 + 78 + 43 + 78 + 75 + 49 + 37 + 57 + 54 + 36 + 21)^2 + \\\\ ... + \\\\ (81 + 6 + 33 + 73 + 92 + 79 + 99 + 82 + 19 + 28 + 11 + 73)^2 \\end{array} \\right) } {3 \\times 4} \\nonumber \\end{align}\\] S_B = 0 for (i in 1:b) { S_B = S_B + ( sum(A[i,]) + sum(B[i,]) + sum(C[i,]) )^2 } (S_B = S_B / (a * r)) ## [1] 232082 Solving sum of squared values for interaction of A and B \\[\\begin{align} S_{AB} {}&amp;= \\frac{ \\sum^b \\sum^a(\\sum^r AB)^2 }{r} \\label{eqn:eqnnumber27} \\\\ &amp;= \\frac{ \\left( \\begin{array}{c} (\\sum\\text{Brand A,Cavachon})^2 + (\\sum\\text{Brand B, Cavachon})^2 + \\\\ ... +\\\\ (\\sum\\text{Brand B,Westiepoo})^2 + (\\sum\\text{Brand C,Westiepoo})^2 \\end{array} \\right) }{4} \\nonumber \\\\ &amp;= \\frac{ \\left( \\begin{array}{c} (67 + 58 + 72 + 88)^2 + (84 + 80 + 95 + 42)^2 + \\\\ ... + \\\\ (92 + 79 + 99 + 82)^2 + (19 + 28 + 11 + 73)^2 \\end{array} \\right) } {4} \\nonumber \\end{align}\\] S_AB = 0 for (i in 1:b) { S_AB = S_AB + sum(A[i,])^2 + sum(B[i,])^2 + sum(C[i,])^2 } (S_AB = S_AB / r) ## [1] 246172 Solving sum of squared values for each subject \\[\\begin{align} S_W {}&amp;= \\sum^b \\sum^a \\sum^r (W)^2 \\\\ &amp;= (\\text{Brand A,Cavachon, Dog1})^2 + ... + (\\text{Brand C,Westiepoo,Dog4})^2 \\nonumber \\\\ &amp;= 67^2 + 58^2 + 72^2 + 88^2 + ... + 19^2 + 28^2 + 11^2 + 73^2 \\nonumber \\end{align}\\] S_W = 0 for (i in 1:b) { S_W = S_W + sum( A[i,]^2 ) + sum (B[i,]^2) + sum(C[i,]^2) } S_W ## [1] 303280 Solving sum of squared values for total \\[\\begin{align} S_T {}&amp;= (\\sum^b \\sum^a \\sum^r T)^2 \\\\ &amp;= \\frac{(\\text{Brand A,Cavachon, Dog1} + ... + \\text{Brand C,Westiepoo, Dog4})^2} { 3 \\times 8 \\times 4} \\nonumber \\\\ &amp;= \\frac{(67 + 58 + 72 + 88 + ... + 19 + 28 + 11 + 73)^2}{ 3 \\times 8 \\times 4 } \\nonumber \\end{align}\\] (S_T = ( sum(A) + sum(B) + sum(C) )^2 / (a * b * r)) ## [1] 226010 Second, let us use the Two-Way ANOVA Table (See Table 6.7) (Larson M. G. 2008; Natoli C. Cory 2017): Table 6.7: Two-Way ANOVA Source of Variation Degrees of Freedom Sum of Squares Mean Squares F Between Grps (A) \\(df_A: a - 1\\) \\(SS_A: S_A - S_T\\) \\(MS_A: \\frac{SS_A}{df_A}\\) \\(\\frac{MS_A}{MS_E}\\) Between Grps (B) \\(df_B: b - 1\\) \\(SS_B: S_B - S_T\\) \\(MS_B: \\frac{SS_B}{df_B}\\) \\(\\frac{MS_B}{MS_E}\\) Interaction (AB) \\(df_{AB}: (a-1)(b-1)\\) \\(SS_{AB}: S_{AB} - S_A\\) \\(MS_{AB}: \\frac{SS_{AB}}{df_{AB}}\\) \\(\\frac{MS_{AB}}{MS_E}\\) \\(\\text{ }\\)- \\(S_B + S_T\\) Within Grps (Err) \\(df_E: ab(r - 1)\\) \\(SS_E: S_W - S_{AB}\\) \\(MS_E: \\frac{SS_E}{df_E}\\) Total \\(df_T: abr - 1\\) \\(SS_T: S_W - S_T\\) With that, we can patch the values into the table (including the critical values at alpha=0.05): # Compute for Sum of Squares SSA = round(S_A - S_T,2); dfA = a - 1 SSB = round(S_B - S_T,2); dfB = b - 1 SSAB = round(S_AB - S_A - S_B + S_T,2); dfAB = (a - 1) * (b - 1) SSW = round(S_W - S_AB,2); dfW = a*b*(r - 1) SST = round(S_W - S_T,2); dfT = a*b*r - 1 # Compute for Mean Squares MSA = round(SSA/dfA,2) MSB = round(SSB/dfB,2) MSAB = round(SSAB/dfAB,2) MSE = round(SSW / dfW,2) # Compute for F-statistics F_A = round(MSA/MSE,4) F_B = round(MSB/MSE,4) F_AB = round(MSAB/MSE,4) # Compute for critical values at alpha=0.05 (95% confidence level). cv_A = round(qf(0.95, dfA, dfW),3) cv_B = round(qf(0.95, dfB, dfW),3) cv_AB = round(qf(0.95, dfAB, dfW),3) # Compute for p-value at alpha=0.05 (95% confidence level). pv_A = round(1 - pf(F_A, dfA, dfW),3) pv_B= round(1 - pf(F_B, dfB, dfW),3) pv_AB = round(1 - pf(F_AB, dfAB, dfW),3) listing = c( &quot;Between Groups (A)&quot;, dfA, SSA, MSA, F_A, cv_A, pv_A, &quot;Between Groups (B)&quot;, dfB, SSB, MSB, F_B, cv_B, pv_B, &quot;Interaction (AB)&quot;, dfAB, SSAB, MSAB, F_AB, cv_AB, pv_AB, &quot;Error (Within)&quot;, dfW, SSW, MSE, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;Total&quot;, dfT ,SST, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot; ) m = matrix(listing, nrow=length(listing)/7, ncol=7, byrow=TRUE) colnames(m) = c(&quot;Source of Variation&quot;, &quot;DF&quot;, &quot;SS&quot;, &quot;MS&quot;, &quot;F&quot;, &quot;Critical Value&quot;, &quot;P-value&quot; ) knitr::kable( m, caption = &#39;Two-Way ANOVA&#39;, booktabs = TRUE, align=c(rep(&#39;r&#39;,times=6)), digits=3, escape=FALSE) Table 6.8: Two-Way ANOVA Source of Variation DF SS MS F Critical Value P-value Between Groups (A) 2 352.33 176.16 0.2221 3.124 0.801 Between Groups (B) 7 6071.96 867.42 1.0936 2.14 0.377 Interaction (AB) 14 13738.17 981.3 1.2372 1.832 0.269 Error (Within) 72 57107.5 793.16 Total 95 77269.96 Finally, with the three computed F-statistics, let us now review our hypothesis. At an alpha value of 0.05, we have the following evaluation: The first F-statistic at 0.2221 is greater than 3.124, which is significant. Alternatively, our P-value at 0.801 is lesser than alpha=0.05; thus it is also significant. The other two F-statistic, at 1.0936 and 1.2372 respectively, are not significant as they are lesser than their corresponding critical values. That is also proven by their P-values, which are both greater than the alpha=0.05. Therefore, suppose our null hypotheses have the corresponding claims: \\(\\mathbf{H_0}\\) - There is no difference between brands (between groups in A), \\(\\mathbf{H_0}\\) - There is no difference between cross-breeds (between groups in B), \\(\\mathbf{H_0}\\) - There is no difference between the interaction of brands and cross-breeds. In that case, our Two-way ANOVA analysis shows that the first null hypothesis is statistically significant. That rejects the null hypothesis, which shows a difference between the brands. Also, there is no significant difference in the gram consumption between cross-breeds; neither is there any difference between the interaction of brand and cross-breeds. So to then know which of the brands all the dogs prefer, a starting point is to compare the means of each brand. The F distribution in Figure 6.11 shows that The f-value for “between groups” of A is greater than the critical value and therefore it falls within the rejection region. That means that the \\(\\mathbf{H_0}\\), null hypothesis, is rejected. On the other hand, we fail to reject the \\(\\mathbf{H_0}\\), null hypothesis, for both between-groups of B and intersection of A and B. Figure 6.11: F Distribution (for Two-Way ANOVA) A faster way to perform a Two-way ANOVA without the manual calculations we recently demonstrated is to use the built-in R aov(). Here, we use one of the standard datasets in R that comes with library MASS: mtcars; other datasets can be listed using the function data(). To get information about mtcars, we can use the “?” in R like so: &gt; ? mtcars Description: The data was extracted from the 1974 _Motor Trend_ US magazine, and comprises fuel consumption and 10 aspects of automobile design and performance for 32 automobiles (1973-74 models).} Format: A data frame with 32 observations on 11 (numeric) variables. [, 1] mpg Miles/(US) gallon [, 2] cyl Number of cylinders [, 3] disp Displacement (cu.in.) [, 4] hp Gross horsepower [, 5] drat Rear axle ratio [, 6] wt Weight (1000 lbs) [, 7] qsec 1/4 mile time [, 8] vs Engine (0 = V-shaped, 1 = straight) [, 9] am Transmission (0 = automatic, 1 = manual) [,10] gear Number of forward gears [,11] carb Number of carburetors … To view the first five records of mtcars, we can use the build-in R function, head(.): head(mtcars) ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 To view the internal structure: str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... The goal is to examine if each factor, such as the number of cylinders in the car (cyl) or the type of transmission (am), influences the rate of fuel consumption (mpg). Additionally, we examine interactions among factors. To do that, we use aov with an additive and an interaction formula. Below is an example of an additive formula (note that we are using only two factors here - cyl and am): (aov.model = aov(mpg ~ as.factor(cyl) + as.factor(am), data = mtcars)) ## Call: ## aov(formula = mpg ~ as.factor(cyl) + as.factor(am), data = mtcars) ## ## Terms: ## as.factor(cyl) as.factor(am) Residuals ## Sum of Squares 824.8 36.8 264.5 ## Deg. of Freedom 2 1 28 ## ## Residual standard error: 3.073 ## Estimated effects may be unbalanced It is notable to mention that independent variables have to be factor variables; otherwise, they are treated as continuous and not discrete. See below: (cyl_factor = as.factor(mtcars$cyl)) ## [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4 ## Levels: 4 6 8 We can use the built-in R function called levels() to show the list of discrete values of the category (the factor): levels(cyl_factor) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; Because the variable cyl is made into factor variable, there are three distinct levels (n=3), \\((4,\\ 6,\\ 8)\\) which renders a degree of freedom of 2 = (n - 1). The other variable has two discrete levels (n = 2): 0 and 1, with a degree of freedom of 1 = (n - 1). (am_factor = as.factor(mtcars$am)) ## [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1 ## Levels: 0 1 Using the ANOVA outcome, a way to summarize the statistic is with the use of the built-in R function, summary: summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## as.factor(cyl) 2 825 412 43.66 2.5e-09 *** ## as.factor(am) 1 37 37 3.89 0.058 . ## Residuals 28 264 9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The summary shows factor cyl to be significant with three asterisks &quot;***&quot; significant code only if alpha = 0.001 and that factor am is significant with “.” code only if alpha = 0.1. To also see if there is an interaction between the groups, we use the following interaction formula instead: aov.model = aov(mpg ~ cyl_factor * am_factor, data = mtcars) summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 44.85 3.7e-09 *** ## am_factor 1 37 37 4.00 0.056 . ## cyl_factor:am_factor 2 25 13 1.38 0.269 ## Residuals 26 239 9 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice a third entry in the outcome, cyl_factor:am_factor, representing the interaction. However, note that the P-value, Pr(&gt;F), shows no significance. Therefore, we can drop this model and use the additive formula instead. On the other hand, we see that if our alpha is 0.001, then only the cyl_factor is significant; therefore, we also can change our formula to only focus on the effect of that single factor variable: aov.model = aov(mpg ~ cyl_factor, data = mtcars) summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 39.7 5e-09 *** ## Residuals 29 301 10 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 If our assumption of a null hypothesis is that no factors affect the rate of fuel consumption, then we can use the following formula instead: (aov.model = aov(mpg ~ 1, data = mtcars)) ## Call: ## aov(formula = mpg ~ 1, data = mtcars) ## ## Terms: ## Residuals ## Sum of Squares 1126 ## Deg. of Freedom 31 ## ## Residual standard error: 6.027 summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 31 1126 36.3 6.3.7 Pearson’s Chi-square Test In Chi-square Test, we deal with multiple independent nominal or ordinal variables (or factors). We can use samples from Normal, Binomial, or Poisson distributions. We then compute for the Chi-Square to arrive at a resulting Chi-square distribution for analysis. Now, to illustrate, let us discuss two types of Chi-square Test, namely \\(\\text{One-Factor}\\ X^2\\ {Test}\\) and \\(\\text{Two-Factor}\\ X^2\\ {Test}\\). \\(\\mathbf{\\text{One-Factor}\\ X^2\\ {Test}}\\) (Goodness of Fit Test): A \\(\\mathbf{\\text{One-Factor}\\ X^2\\ {Test}}\\) performs test statistics using the following equation (McHugh M.L. 2013): \\[\\begin{align} X^2 = \\sum \\frac{(observed - expected)^2}{expected} = \\sum_i \\frac{(O_i - E_i)^2}{E_i} \\end{align}\\] Suppose we want to survey the cutest cross-breed dogs preferred by dog owners. So we go out across the country and invite dog owners to participate in our survey. We narrow down the cross-breed type based on the same list of cross-breed dogs as before, but this time with the following hypothetical datasets: Table 6.9: Observed Cross-Breed Preference Observation Probability Expected X^2 = (O - E)^2/E Cavachon 310 0.12 333.6 1.67 Cavapoo 400 0.13 361.4 4.123 Maltipoo 240 0.09 250.2 0.416 Pomchi 400 0.16 444.8 4.512 Shichon 420 0.16 444.8 1.383 Shih-Poo 430 0.14 389.2 4.277 Shorkie 300 0.11 305.8 0.11 Westiepoo 280 0.09 250.2 3.549 Total 2780 1 20.039 Note that we have assumed some prior probabilities in our calculation which we need to calculate the expected values. For example, the expected value for Cavachon dogs is calculated as such: E = Total \\(\\times\\) Probability = 2780 \\(\\times\\) 0.10 = 278. Relying on the Chi-square table (see the Appendix), we see that the Critical Value is 20.278 for a confidence level of 95% (alpha=0.05) with a degree of freedom at 7, (n-1). preference = c(310, 400, 240, 400, 420, 430, 300, 280) probability = c(0.12, 0.13, 0.09, 0.16, 0.16, 0.14, 0.11,0.09) chisq.test(preference, p=probability) ## ## Chi-squared test for given probabilities ## ## data: preference ## X-squared = 20, df = 7, p-value = 0.005 The P-value is also given. To validate, let us use pchisq(.) using a lower.tail=FALSE, which means that we are interested in the upper tail given the probability condition \\(P(X^2 &gt; CV)\\); meaning we are looking for the probability that the chi-square statistic is greater than the critical value (tabled value). (pvalue = pchisq(q = chisqr, df = 7, lower.tail=FALSE)) ## [1] 0.005486 Here, we reject the null hypothesis because our P-value is 0.0055 less than alpha=0.05 based on our confidence level. Alternatively, this conclusion is true because the Chi-square statistic is greater than the critical value (tabled value), \\(\\mathbf{X^2 &gt; CV}\\), which is 20.039 &gt; 14.067. That is more clear as we discuss the next type of test in which we go further with the analysis by using the Chi-square distribution. \\(\\mathbf{\\text{Two-Factor}\\ X^2\\ {Test}}\\) (Test of Independence): Let us now have \\(\\mathbf{\\text{Two-Factor}\\ X^2\\ {Test}}\\) and compute for the test statistics. Suppose we want to survey if there is a relationship between the gender of dog owners and the type of dogs preferred. That is to test if two samples are independent (IID). Again, we narrow down to the same list of cross-breed dogs as before, but this time with the following datasets in a contingency table: Table 6.10: Cross-Breed Preference Male Female Total Cavachon 140 170 310 Cavapoo 120 280 400 Maltipoo 108 132 240 Pomchi 200 200 400 Shichon 210 210 420 Shih-Poo 258 172 430 Shorkie 75 225 300 Westiepoo 14 266 280 Total 1125 1655 2780 Let us now tabulate the data to calculate our Chi-square. The expected column is calculated based on: \\[\\begin{align} E_{cell} = \\frac{O_j \\times O_i}{T} \\end{align}\\] where: \\(\\mathbf{O_j}\\) is the marginal row total \\(\\mathbf{O_i}\\) is the marginal column total \\(T\\) is the grand total For example, to compute the expected value for the Male-Cavachon combination, we perform the following: \\[ E_{male-cavachon} = \\frac{310 \\times 1125}{2780} = 125.4496 \\] Table 6.11: Cross-Breed Preference by Gender \\(Obs_{(M)}\\) \\(F-Obs_{(M)}\\) \\(Expected_{(M)}\\) \\(Expected_{(M)}\\) \\(X^2_{(M)}\\) \\(X^2_{(F)}\\) \\(X^2\\) 140 170 125.45 184.6 1.688 1.147 2.835 120 280 161.87 238.1 10.831 7.362 18.193 108 132 97.12 142.9 1.218 0.828 2.046 200 200 161.87 238.1 8.982 6.105 15.087 210 210 169.96 250.0 9.431 6.411 15.841 258 172 174.01 256.0 40.539 27.557 68.095 75 225 121.40 178.6 17.736 12.056 29.793 14 266 113.31 166.7 87.039 59.166 146.205 1125 1655 NA NA 177.463 120.632 298.095 Our calculation for the Degrees of Freedom is expressed as: \\(DF = (row - 1)\\times(col - 1) = (8-1)\\times(2-1) = 7\\) Relying on the Chi-square table (see the Appendix), the critical value is 14.067 for a confidence level of 95% with a degree of freedom at 7. Our Chi-square statistic is at 298.0947. We can use the built-in R function pchisq to derive the probability of finding \\(\\mathbf{X^2} \\ge CV\\). Here , we have 298.0947 &gt; 14.067: pchisq(q = ct[9,7], df = df, lower.tail=FALSE) ## $X^2$ ## 1.544e-60 The probability is at the extreme right side (right tail), which can also be shown using PDF of Chi-square distribution. area &lt;- function(df, a, b, col) { area = seq(a, b, length.out=50) x = c(a, area , b) y = c(0, chi_pdf(area, df), 0) polygon(x, y, col=col, border=&quot;navyblue&quot;, lwd=2) } x = seq(0, 40) plot(NULL, xlim=range(0,40), ylim=range(0,0.12), xlab=&quot;Chi-square value&quot;, ylab=&quot;probability density&quot;, main=&quot;Chi-square Distribution (PDF)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) # Using our CDF implementation of Chi-square PDF curve(chi_pdf(x, df), col=&quot;navyblue&quot;, lwd=2, add=TRUE) # Using built-in R function &quot;dchisq&quot; # curve(dchisq(x, df), col=&quot;navyblue&quot;, lwd=2, add=TRUE) # Chi-square value abline(v = x2, col=&quot;red&quot;, lty=3) text(x2, 0.06, label=paste(&quot;X^2=&quot;, x2), cex=0.8) # Critical Value abline(v = c(cv, 40), col=&quot;navyblue&quot;, lty=3) text(cv, 0.06, label=paste(&quot;cv=&quot;, cv), cex=0.8) # draw the alpha = 0.05 area(df, cv, 40, col=&quot;lightblue&quot;) # alpha text(7, 0.005, label=&quot;alpha=0.05&quot;, cex=0.8) arrows(9.5, 0.005, 12.5, 0.005, length=0.08) # rejection region arrows(cv, 0.02, 40, 0.02, lty=2, col=&quot;blue&quot;, code=3, length=0.08) text(31, 0.025, label=&quot;rejection region&quot;, cex=0.8) Figure 6.12: Chi-square Distribution (PDF) It shows that the Chi-square value is within the rejection region (\\(\\mathbf{X^2} \\ge\\) 14.067) past greater than the critical value, given a confidence level of 95%; therefore, we reject the null hypothesis. Therefore, the alternative hypothesis holds. Our null hypothesis as below is rejected: \\[ H_0\\ \\ \\rightarrow \\text{there is no association between gender of owner and cross-breed type} \\] Indeed, there is an association which is what the alternative hypothesis, \\(\\mathbf{H_1}\\), claims. 6.3.8 Wilcoxon Test In this section, we discuss non-parametric tests. The idea is that we do not rely on known parameters such as mean or variance; thus, we cannot assume that our data follows some normal distribution. Here, we do not assume any distribution. Instead, our test is based on rank. Here, we discuss two kinds of Wilcoxon tests, namely Wilcoxon Rank Sum Test and Wilcoxon Signed-Rank Test (Harris T and Hardin J.W. 2013): Wilcoxon Rank Sum Test: Wilcoxon Rank Sum Test is a non-parametric test. It is also called the Mann-Whitney test. To illustrate, let us use the following dataset: set.seed(1) range = seq(5, 20) sample_size = 10 groups = t( replicate(n=2, sample(range, size=sample_size, replace=TRUE)) ) rownames(groups) = c(&quot;Group A&quot;, &quot;Group B&quot;) groups ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Group A 9 10 14 19 8 19 20 15 15 5 ## Group B 8 7 15 11 17 12 16 20 11 17 We combine both groups, sort, and then rank each observation. rank = sort( c(groups[1,], groups[2,]) ) rank = rbind( rank, seq(1, sample_size*2)) rownames(rank) = c(&quot;Sorted&quot;, &quot;Ranked&quot; ) rank[,1:10] # display only first 10 columns ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Sorted 5 7 8 8 9 10 11 11 12 14 ## Ranked 1 2 3 4 5 6 7 8 9 10 Consecutively identical observations get to share the average of the rank. For example, the first two observations have identical values of 5 and are consecutively ranked 1 and 2; therefore, both will receive a rank of 1.5, derived from (1+2)/2. Moreover, there are three observations with values 11 ranked 10,11,12, respectively; therefore, they will be assigned a rank of 11, derived from (10+11+12)/3. We then list only the unique observations with their corresponding ranks (this is our rank template): adjusted_rank = as.matrix( aggregate(rank[2,], list(rank[1,]), mean)) colnames(adjusted_rank) = c(&quot;Sorted Observ.&quot;, &quot;Adjusted Rank&quot;) t(adjusted_rank)[,1:10] # display only first 10 columns ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Sorted Observ. 5 7 8.0 9 10 11.0 12 14 15 16 ## Adjusted Rank 1 2 3.5 5 6 7.5 9 10 12 14 We then assign the adjusted rank back to the original group. An easier way to rank is using the built-in R function rank() instead: builtin.rank = rank(c(groups[1,], groups[2,]), ties.method=&quot;average&quot;) ranks = matrix(builtin.rank, nrow=sample_size, ncol=2, byrow=FALSE) sum_rank1 = sum(ranks[,1]) sum_rank2 = sum(ranks[,2]) ranked_groups = matrix( c(groups[1,], ranks[,1], groups[2,], ranks[,2]), nrow=sample_size, ncol=4, byrow=FALSE) ranked_groups = rbind(ranked_groups, c(NA, sum_rank1, NA, sum_rank2)) colnames(ranked_groups) = c(&quot;Group A&quot;, &quot;Rank A&quot;, &quot;Group B&quot;, &quot;Rank B&quot;) rownames(ranked_groups) = c(seq(1, sample_size), &quot;Sum&quot;) ranked_groups ## Group A Rank A Group B Rank B ## 1 9 5.0 8 3.5 ## 2 10 6.0 7 2.0 ## 3 14 10.0 15 12.0 ## 4 19 17.5 11 7.5 ## 5 8 3.5 17 15.5 ## 6 19 17.5 12 9.0 ## 7 20 19.5 16 14.0 ## 8 15 12.0 20 19.5 ## 9 15 12.0 11 7.5 ## 10 5 1.0 17 15.5 ## Sum NA 104.0 NA 106.0 We compute for the U statistic for each group using the following formula: \\[\\begin{align} U_{grp} = R_{grp} - \\frac{n(n+1)}{2},\\ \\ \\ \\ \\ where\\ R_{grp} = \\sum^n_{i=1} rank(x_i) \\end{align}\\] n = sample_size U_a = sum_rank1 - n * (n + 1) / 2 U_b = sum_rank2 - n * (n + 1) / 2 c(&quot;U_a&quot; = U_a, &quot;U_b&quot; = U_b) ## U_a U_b ## 49 51 The final U-statistic is based on the group with lesser U-statistic. Therefore, \\(U_{stat}\\) = 49 As for the hypothesis, let us use a confidence level of 95%. Given an alternative hypothesis as below: \\[ H_1 : Group\\ A &gt; Group\\ B, \\] our critical value is computed using qwilcox(): (U_crit = U_upper_crit = qwilcox(p = 0.05, m = n, n = n, lower.tail=FALSE)) ## [1] 72 which shows that \\(U_{stat}\\) &lt; \\(U_{crit}\\), rejecting \\(H_0\\). On the other hand, with an alternative hypothesis as below: \\[ H_1 : Group\\ A &lt; Group\\ B, \\] our critical value using qwilcox() becomes: (U_crit = U_lower_crit = qwilcox(p = 0.05, m = n, n = n, lower.tail=TRUE)) ## [1] 28 which shows that \\(U_{stat}\\) &gt; \\(U_{crit}\\), rejecting \\(H_0\\). Lastly, with an alternative hypothesis as below: \\[ H_1 : Group\\ A \\ne Group\\ B, \\] our critical value using qwilcox() becomes: c(&quot;min crit&quot;=U_lower_crit, &quot;max crit&quot;=U_upper_crit) ## min crit max crit ## 28 72 which shows that \\(U_{min\\_crit} \\le U_{stat} \\le U_{max\\_crit}\\), rejecting \\(H_0\\). In terms of pvalue, for a one-sided upper-tail preference, we get: (pvalue = pwilcox(q = U_stat, m = n, n = n) * 2) ## [1] 0.9705 To validate, we can use the built-in R function wilcox.test(): wilcox.test(groups[1,], groups[2,], correct=FALSE, paired=FALSE, exact=FALSE) ## ## Wilcoxon rank sum test ## ## data: groups[1, ] and groups[2, ] ## W = 49, p-value = 0.9 ## alternative hypothesis: true location shift is not equal to 0 Note that we set exact = FALSE. That allows us to perform normal approximation for discrete observations less than 50. Additionally, the function wilcox.test() may show message about P-Value approximation if observations have ties. Wilcoxon Signed Rank Test: Wilcoxon Signed Rank Test is the second Wilcoxon test we discuss next, and it is also a non-parametric test equivalent to paired T-Test. This test utilizes signed-ranks to evaluate the U statistics. To illustrate, let us use the same dataset as before but compute for the difference. set.seed(1) range = seq(5, 20) sample_size = 10 groups = t( replicate(n=2, sample(range, size=sample_size, replace=TRUE)) ) diff = groups[2,] - groups[1,] groups = rbind(groups, diff) rownames(groups) = c(&quot;Group A&quot;, &quot;Group B&quot;, &quot;Difference&quot;) groups ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Group A 9 10 14 19 8 19 20 15 15 5 ## Group B 8 7 15 11 17 12 16 20 11 17 ## Difference -1 -3 1 -8 9 -7 -4 5 -4 12 We then sort and rank the absolute difference. The original groups are discarded. diff = sort( groups[1,] - groups[2,]) builtin.rank = rank( abs( diff ), ties.method=&quot;average&quot;) rank = rbind( diff, builtin.rank) rownames(rank) = c(&quot;Difference&quot;, &quot;Rank&quot; ) rank ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Difference -12 -9 -5 -1.0 1.0 3 4.0 4.0 7 8 ## Rank 10 9 6 1.5 1.5 3 4.5 4.5 7 8 Now, for the signed rank test, we sum negative groups and positive groups. mu = 0 rank_neg = rank [2, which( diff &lt; mu ) ] U_neg = sum(rank_neg) rank_pos = rank [2, which( diff &gt; mu ) ] U_pos = sum(rank_pos) c(&quot;U_neg&quot; = U_neg, &quot;U_pos&quot; = U_pos) ## U_neg U_pos ## 26.5 28.5 The U-statistic is based on the group with lesser U statistics. Therefore, \\(U_{stat}\\) = 26.5 As for the hypothesis, let us use a confidence level of 95% with the following claims for a two-tail test: \\[\\begin{align*} H_0 {}&amp;: \\text{there is no difference between Group A and B}\\\\ H_1 &amp;: \\text{there is a difference between Group A and B} \\end{align*}\\] Let us use qsignrank() to compute for critical value: U_min = qsignrank(p = 0.05/2, n = n, lower.tail = TRUE) U_max = qsignrank(p = 0.05/2, n = n, lower.tail = FALSE) c(&quot;lower&quot;=U_min, &quot;upper&quot;=U_max) ## lower upper ## 9 46 which shows that \\(U_{min\\_crit} \\le U_{stat} \\le U_{max\\_crit}\\), resulting to a rejection of \\(H_0\\). Let us now compute for the z-value by normalizing and standardizing our U statistic: \\[\\begin{align} Z = \\frac{U - E_{H_0}(U)}{\\sqrt{VAR_{H_0}(U)}} = \\frac{U - \\mu_u}{\\sigma_u} \\end{align}\\] where: \\[\\begin{align} \\mu_u = \\frac{n(n+1)}{4}\\ \\ \\ \\ \\ \\ \\ \\ \\sigma_u = \\frac{n(n+1)(2n+1)}{24} \\end{align}\\] n = sample_size mu = ( n * (n + 1) / 4 ) var = (n * (n + 1) * (2*n + 1))/ 24 (zvalue = (U_stat - mu) / sqrt(var)) ## [1] -0.1019 And for our P-value, we use pmon() instead with Z-value: pnorm( q = zvalue, mean=0, sd=1, lower.tail=TRUE ) * 2 ## [1] 0.9188 To validate, we can use the built-in R function wilcox.test(): wilcox.test(groups[1,], groups[2,], alternative=&quot;two.sided&quot;, correct=FALSE, paired=TRUE, exact=FALSE) ## ## Wilcoxon signed rank test ## ## data: groups[1, ] and groups[2, ] ## V = 28, p-value = 0.9 ## alternative hypothesis: true location shift is not equal to 0 We set the exact parameter of the function equal to FALSE to force approximation of the distribution to a normal distribution; though, we do not depend on the distribution. As we can see, the approximation is close to the result of pnorm(.). There are other Wilcoxon Tests available to use. We leave the rest for readers to investigate: Wilcoxon matched-pairs signed-rank test 6.3.9 Kruskal-Wallis Test Kruskal-Wallis test is also a non-parametric ranked sum test similar to Mann-Whitney in that we cannot assume about the distribution (for example, using mean and standard deviation). However, we can also compare two or more groups and rank each observation (Chan Y. and Walmsley R.P. 1997). To illustrate, let us use the same dataset as before. set.seed(2020) range = seq(5, 20) sample_size = 10 groups = t( replicate(n=3, sample(range, size=sample_size, replace=TRUE)) ) rownames(groups) = c(&quot;Group A&quot;, &quot;Group B&quot;, &quot;Group C&quot;) groups ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## Group A 15 11 14 12 7 6 7 11 5 14 ## Group B 17 16 18 11 11 13 20 15 13 9 ## Group C 8 6 18 20 19 7 10 16 12 13 We then rank the groups in the same manner as shown in Mann-Whitney test: builtin.rank = rank(c(groups[1,], groups[2,], groups[3,]), ties.method=&quot;average&quot;) ranks = matrix(builtin.rank, nrow=sample_size, ncol=3, byrow=FALSE) R1 = sum(ranks[,1]); R2 = sum(ranks[,2]); R3 = sum(ranks[,3]) ranked_groups = matrix( c(groups[1,], ranks[,1], groups[2,], ranks[,2], groups[3,], ranks[,3] ), nrow=sample_size, ncol=6, byrow=FALSE) ranked_groups = rbind(ranked_groups, c(NA, R1, NA, R2, NA, R3)) colnames(ranked_groups) = c(&quot;Group A&quot;, &quot;Rank A&quot;, &quot;Group B&quot;, &quot;Rank B&quot;, &quot;Group C&quot;, &quot;Rank C&quot;) rownames(ranked_groups) = c(seq(1, sample_size), &quot;Sum&quot;) ranked_groups ## Group A Rank A Group B Rank B Group C Rank C ## 1 15 21.5 17 25.0 8 7.0 ## 2 11 11.5 16 23.5 6 2.5 ## 3 14 19.5 18 26.5 18 26.5 ## 4 12 14.5 11 11.5 20 29.5 ## 5 7 5.0 11 11.5 19 28.0 ## 6 6 2.5 13 17.0 7 5.0 ## 7 7 5.0 20 29.5 10 9.0 ## 8 11 11.5 15 21.5 16 23.5 ## 9 5 1.0 13 17.0 12 14.5 ## 10 14 19.5 9 8.0 13 17.0 ## Sum NA 111.5 NA 191.0 NA 162.5 The formula to compute for the K-statistic is as follows: \\[\\begin{align} H = \\left(\\frac{12}{N(N+1)}\\sum^k_{i=1}{\\frac{R^2_i}{n_i}}\\right) - 3(N + 1) \\end{align}\\] Note that the K-statistic assumes a chi-square distribution as shown when using kruskal.test(.) function later. k = 3 # total number of groups n = sample_size # size per group N = n * k (H_stat = (12/(N * (N+1))) * (R1^2/n + R2^2/n + R3^2/n) - 3 * (N + 1)) ## [1] 4.186 We use Chi-Square functions to generate the Critical value and P-value: alpha=0.05 df = k - 1 (H_crit = qchisq(p=alpha, df = df, lower.tail=FALSE)) ## [1] 5.991 (p_value = pchisq(H_stat, df = df, lower.tail=FALSE)) ## [1] 0.1233 If our null hypothesis is expressed as below (with a confidence level of 95%): \\[ H_0: \\text{there is no difference between groups} \\] then, we reject \\(H_0\\) because the result shows that \\(H_{stat}\\) &lt; \\(H_{crit}\\). There is significant difference between groups. To validate, we can use the built-in R function kruskal.test(): # Perform Rank Sum Test n = length(groups[1,]) data = c(groups[1,], groups[2,], groups[3,]) grouping = as.factor (c(rep(1, n), rep(2, n), rep(3, n)) ) kruskal.test(data, grouping) ## ## Kruskal-Wallis rank sum test ## ## data: data and grouping ## Kruskal-Wallis chi-squared = 4.2, df = 2, p-value = 0.1 6.3.10 Friedman Test Friedman Test is non-parametric test with an F-statistic following a Chi-square distribution. We leave readers to investigate Friedman Test. 6.4 Post-HOC Analysis In our examples, a null hypothesis states that there is no difference or no association between groups. If we reject the null hypothesis, there is a difference or an association between groups. In certain circumstances, we may need to determine which groups have means that differ and by how much different (how significantly different). Additionally, we may need to run multiple comparison tests when dealing with multiple groups. However, we may come to know that as we run multiple comparisons, the type I error increases (or inflates). Therefore, there is a need to control the inflation while still being able to identify and find differences. That is where we come to the next process: Post-HOC analysis. Post-HOC refers to the Latin phrase ‘after this’ (Wikipedia). Statistically, we refer to Post-HOC analysis as the next step we take after the result of our experiment. As a starting point, it helps to be familiar with Figure 6.13. If our confidence level is at 95%, our threshold for mistakes is at 5% - that is our alpha or type I error - which means that we incorrectly reject a true null hypothesis in about 5 out of 100 observations. On the other hand, our goal is to lean towards a higher power - the probability of correctly rejecting a false null hypothesis. Figure 6.13: Statistical Error and Power It is essential to be able to measure and control the occurrences of false positives and lean towards achieving more true positives. In particular, we hope to achieve a result in which the power in significance has a higher probability than the probability of encountering type I errors. For that, let us also review Figure 6.14. Figure 6.14: Effect Size vs Power Figure 6.14 introduces the concept of effect size and power. As the effect size decreases toward the mean of \\(\\mathbf{H_0}\\), then the mean of \\(\\mathbf{H_1}\\) - the entire distribution - shifts toward the left, past the critical value; effectively decreasing the right-side tail of \\(\\mathbf{H_1}\\) - which also decreases the power (or probability). On the other hand, as the effect size increases farther away from the mean of \\(\\mathbf{H_0}\\), then the mean of \\(\\mathbf{H_1}\\) shifts farther away to the right, also past the critical value; effectively increasing the right-side tail of \\(\\mathbf{H_1}\\) - which also increases the power (or probability). Note that if the Power decreases, the probability of the Type II error, \\(\\beta\\), increases. What this means is that the probability of True Positive outcome, representing the Power, in which we claim that there is a difference in groups (\\(\\mathbf{H_1}\\)), increases as the difference between means, representing the effect size, becomes greater. Power of Test Power is defined as the probability that an alternative hypothesis holds, granting the alternative hypothesis is true. Moreover, it can be expressed as such: \\[\\begin{align} Power = 1 - \\beta = P(Reject\\ H_0\\ |\\ H_0\\text{ is False}) \\end{align}\\] Figure 6.13 is a good reference to show where power lies. Effect Size Effect size measures both the magnitude and direction of the difference or association between group means. To illustrate, let us introduce two types of groups: Experiment Group - a sample dataset where we expose the subject to treatments; thus, the independent variable is manipulated (adjusted or changed). Control Group - on the other hand, we expose the subjects to normal conditions; thus, the independent variable is constant and controlled. When running experiments, it is possible to have multiple experiment groups, each with varying treatments or exposures, while at the same time preserving the normal state of one group, being the control group. After the experiment, we compare the outcomes of each test based on the effect size (magnitude of effect) of the treatments on the subject vs. the subjects with no treatment (the control group). Furthermore, a control group may have two types: a positive group and a negative group. The positive group validates the effectiveness of the treatment, while the negative group validates the existence of external influences (e.g., contamination). We can measure effect size using Cohen’s d (Cohen 1969) formula: \\[\\begin{align} \\text{Effect Size} = d = \\frac{(\\mu_{experiment})-(\\mu_{control})}{\\text{standard deviation}} \\end{align}\\] Here is a table with the corresponding category (note that other literature may have other ranges for each size category): Table 6.12: Effect Size (Cohen 1969) Size (Category) Effect Size Large 0.8 Medium 0.5 Small 0.2 Here, effect size is a ratio of standard deviation. An effect size of 0.2 implies that the distance of the means of both groups is about 0.2 standard deviation apart. It deviates by that much, and per Cohen, it is categorized as a small deviation which is therefore trivial even if ANOVA may show some significance in difference. The effect size cannot easily be adjusted or controlled. On the other hand, perhaps, we can control the occurrence of Type I and type II errors instead. Error Rate There are three common ways to measure error rates and thus be able to control them (Benjamini Y. and Hochberg Y. 1994): Family-Wise Error Rate (FWER) measures the probability of encountering at least one Type I error. \\[\\begin{align} FWER = P(\\text{at least one type I error}) = 1 - P (type\\ I\\ error = 0) \\end{align}\\] False Discovery Rate (FDR) measures the proportion of incorrectly rejecting a true null hypothesis overall rejections. That is a measure of the rate of Type I error. See Figure 6.13. \\[\\begin{align} FDR = \\frac{\\text{false positive}}{\\text{false and true positive}} = \\frac{FP}{FP + TP} \\end{align}\\] True Discovery Rate (TDR) measures the proportion of correctly rejecting a false null hypothesis over all rejections. This is a measure of the rate of power in significance. See Figure 6.13. \\[\\begin{align} TDR = \\frac{\\text{true positive}}{\\text{false and true positive}} = \\frac{TP}{FP + TP} \\end{align}\\] When it comes to Control, two terms are being used to describe a method or process: conservative method - a method or process that tends toward enforcing stringent Control over Type I errors. We protect against the occurrence of false positives. While we prefer to have a probability (the p-value) of encountering Type I errors less than the nominal (significance) level (the alpha), too much protection, however, means that we may fail to discover occurrences of significant results. anticonservative method - a method or process that can relax the Control and accept a higher rate (or probability) of encountering at least one Type I error - that makes it towards more Power or higher probability of hitting true positives - that is discovering statistically significant results. One method that demonstrates a conservative approach is the Bonferroni Correction, discussed next. 6.4.1 Bonferroni Correction Bonferroni Correction is simply a correction to P-values, which relies on FWER. The corrected P-values are called the adjusted P-values (Armstrong R. A. 2014; Chen S., Feng Z., and Yi X. 2017)]. To understand the need for correction, let us use a fictitious dataset with one factor variable of five groups of size ten each: set.seed(1) m = 5 # number of groups p = 2 # pairwise comparison n = sample_size = 10 range = seq(100,300) # metrics groups = replicate(n=m, sample(x = range, size= sample_size, replace=TRUE)) colnames(groups) = c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;,&#39;D&#39;,&#39;E&#39;) groups ## A B C D E ## [1,] 153 141 287 196 265 ## [2,] 174 135 142 220 230 ## [3,] 215 238 230 199 257 ## [4,] 282 177 125 137 211 ## [5,] 140 254 153 266 206 ## [6,] 280 200 177 234 258 ## [7,] 289 244 102 259 104 ## [8,] 232 299 176 121 195 ## [9,] 226 176 274 245 247 ## [10,] 112 256 168 182 239 With five groups, we are looking at ten pairwise comparisons: \\[ AB\\ \\ AC\\ \\ AD\\ \\ AE\\ \\ BC\\ \\ BD\\ \\ BE\\ \\ CD\\ \\ CE\\ \\ DE \\] The number of comparisons can be calculated using the following: \\[\\begin{align} Comparisons = \\frac{m!}{p!(m-p)!} = \\frac{m (m-1)}{p} \\end{align}\\] where: m is the total number of groups p is the number of groups to compare. This is 2 for pairwise comparison. (comparisons = factorial(m) / ( factorial (p) * factorial(m-p))) ## [1] 10 We know that \\(\\alpha\\) represents the probability of making a Type I error: \\[ P(\\text{Type I error}) = \\alpha = 0.05 \\] And inversely, a confidence interval of 95%, \\(1 - \\alpha\\), is the probability of not making a Type I error: \\[ P(\\text{not a Type I error}) = 1 - \\alpha = 0.95 \\] Those two probabilities are actual if we are performing one pairwise comparison test. If we are performing more than one test, however, then the probability of making at least one Type I error is then expressed as: \\[\\begin{align} P(\\text{at least one Type I error}) = 1 - P_{(overall)}(\\text{not a Type I error}) \\end{align}\\] where: \\[\\begin{align} P_{(overall)}(\\text{not a Type I error}) = &amp;P_{AB}(\\text{not a Type I error}) \\times \\nonumber \\\\ &amp;P_{AC}(\\text{not a Type I error}) \\times \\nonumber \\\\ &amp;P_{AD}(\\text{not a Type I error}) \\times ...\\times \\nonumber \\\\ &amp;P_{DE}(\\text{not a Type I error}) \\end{align}\\] which is equivalent to: \\[\\begin{align} P_{(overall)}(\\text{not a Type I error}) = P_{(comparison)}(\\text{not a Type I error})^n \\end{align}\\] where: n is the number of pairwise comparisons. If we then use a confidence level of 95% for a factor variable with five groups (10 comparisons), we get this: \\[ P_{(overall)}(\\text{not a Type I error}) = (0.95)^{10} = 0.4012631 = 40.13\\% \\] That shows a 40.13% probability of not making a Type I error for ten comparison tests, which also means a 59.87% (1 - 40.13%) probability of making at least one Type I error. That is a very high probability and is problematic. \\[ P(\\text{at least one Type I error}) = 1 - 0.4012631 = 0.5987369 = 59.87\\% \\] To fix that, we apply Bonferroni’s correction. Assume we settle on \\(\\alpha=0.05\\), a Bonferroni adjustment is expressed as: \\[ \\alpha_c = \\frac{\\alpha}{\\text{no of tests}} = \\frac{0.05}{10} = 0.005 \\] or for our confidence level of 95%, we see the following adjustment for one comparison test: \\[ 1 - \\alpha_c = 1 - 0.005 = 0.995\\ \\ \\ \\rightarrow \\ 99.5\\%\\ \\text{confidence level} \\] Applying the adjustment for ten comparison tests, we get the following: \\[\\begin{align*} P(\\text{at least one Type I error}) {}&amp;= 1 - ( 1 - \\alpha_c)^{10}\\\\ &amp;= 1 - (1 - 0.005)^{10} = 0.04888987 = 4.89\\% \\end{align*}\\] To illustrate, let us use the same mtcars dataset to perform a pairwise T-test comparison with Bonferroni correction using a built-in R function called pairwise.t.test(): pairwise.t.test(mtcars$mpg, mtcars$cyl, p.adj=&quot;bonferroni&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: mtcars$mpg and mtcars$cyl ## ## 4 6 ## 6 4e-04 - ## 8 3e-09 0.01 ## ## P value adjustment method: bonferroni The result shows that the following pairs render the corresponding adjusted p-values: 6-4 has adj. p-value of 0.00036 8-4 has adj. p-value of 2.6e-09 8-6 has adj. p-value of 0.01246 Other adjustments used by the pairwise.t.test() includes Holm Adjustment: pairwise.t.test(mtcars$mpg, mtcars$cyl, p.adj=&quot;holm&quot;) ## ## Pairwise comparisons using t tests with pooled SD ## ## data: mtcars$mpg and mtcars$cyl ## ## 4 6 ## 6 2e-04 - ## 8 3e-09 0.004 ## ## P value adjustment method: holm To see other adjustments, we can use p.adjust.methods: p.adjust.methods ## [1] &quot;holm&quot; &quot;hochberg&quot; &quot;hommel&quot; &quot;bonferroni&quot; &quot;BH&quot; ## [6] &quot;BY&quot; &quot;fdr&quot; &quot;none&quot; 6.4.2 Benjamini-Hochberg Correction Benjamini-Hochberg (BH) method controls the false discovery rate (FDR). It limits the number of false positives. Similar to Bonferroni correction, the BH correction adjusts p-values (Chen S., Feng Z., and Yi X. 2017). To illustrate, let us directly assume a list of P-values corresponding to the number of experiments. no_of_experiments=7 pvalues = sort( round(runif(no_of_experiments, min=0.001, max=0.10),5)) pvalues = rbind(pvalues, seq(1, no_of_experiments)) rownames(pvalues) = c(&quot;Sorted Pvalues&quot;, &quot;Rank&quot;) colnames(pvalues) = paste0(&quot;R&quot;, seq(1, no_of_experiments)) pvalues ## R1 R2 R3 R4 R5 R6 R7 ## Sorted Pvalues 0.008 0.01085 0.02523 0.03231 0.04437 0.04828 0.08626 ## Rank 1.000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000 One way to adjust the P-value is to use an FDR threshold using this formula: \\[\\begin{align} Adj. p = p \\times \\frac{R_{(max)}}{R_{p}} \\end{align}\\] For example, to adjust the P-value 0.0863 with rank 7, we perform the following calculation: max_rank = max(pvalues[2,]) pvalue_rank = pvalues[2,6] (pvalues[1,6] * max_rank / pvalue_rank) ## [1] 0.05633 To adjust all the P-values, we use the following R code: p = pvalues[1,] rank = pvalues[2,] max_rank = max(rank) adj_pvalues = p * max_rank / rank pvalues = rbind(pvalues, adj_pvalues) rownames(pvalues) = c(&quot;Sorted Pvalues&quot;, &quot;Rank&quot;, &quot;Adj Pvalues&quot;) pvalues ## R1 R2 R3 R4 R5 R6 R7 ## Sorted Pvalues 0.008 0.01085 0.02523 0.03231 0.04437 0.04828 0.08626 ## Rank 1.000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000 ## Adj Pvalues 0.056 0.03798 0.05887 0.05654 0.06212 0.05633 0.08626 6.5 Multiple Comparison Tests We now discuss a few popular multiple comparison tests in statistics (Rodger R.S. and Roberts M. 2013; Lee S. and Lee D. K. 2018). The idea is essentially to compare groups and determine which groups contribute to a significant difference. Note that we can only use any of the Post-HOC tests if we determine from our previous analysis, e.g., ANOVA, that the null hypothesis does not hold. \\[\\begin{align} \\mathbf{H_0} {}&amp;: \\ \\ \\ \\mu_i = \\mu_j\\ \\ \\ \\leftarrow\\ \\ \\ rejected\\\\ \\mathbf{H_1} &amp;: \\ \\ \\ \\ \\mu_i \\ne \\mu_j\\ \\ \\ \\leftarrow\\ \\ \\ this\\ holds \\end{align}\\] A key point to emphasize here is that if we run our ANOVA test against hundred groups (supposition) and have determined that there are significant differences amongst groups, it may help to use one of the following multiple comparisons approaches: use a few groups for comparison instead of all of them or compare all of them or use one of them as a control group and compare it against the other experimental groups. or compare a subset of groups orthogonal to another subset of groups. A few of the methods that follow next in our discussion, such as Tukey’s Test and Dunnett’s Test, fall under any of those approaches. 6.5.1 Scheffe’s Test Scheffe’s Test is a Post-HOC multiple comparison test. We use the mtcars dataset as before to illustrate a pairwise comparison. First, let us determine the groups, e.g. \\((4,\\ 6,\\ 8)\\), to be compared from a factor variable, e.g. cyl (number of cylinders). (groups = levels(as.factor(mtcars$cyl))) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; Second, let us perform a One-Way ANOVA against the corresponding scaled metrics, mpg (fuel consumption), using our implementation: mtc = list() m = length(groups) for (i in 1:m) { mtc[[i]] = mtcars[mtcars$cyl == groups[i],]$mpg } (anova_outcome = anova = one_way_anova(mtc)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 Third, compute for the Critical value of the F-test (assume a 95% confidence level): (cv_anova = qf(0.95, anova[&quot;dfB&quot;], anova[&quot;dfW&quot;]) ) ## [1] 3.328 Fourth, compute for the Critical value of the Scheffe Test using: \\[\\begin{align} F&#39; = dfB \\times (\\text{Critical value of F-test}) \\end{align}\\] (cv_scheffe = as.numeric(anova[&quot;dfB&quot;]) * cv_anova ) ## [1] 6.655 Finally, with the ANOVA result, let us perform a Post-HOC analysis using Scheffe’s Method: \\[\\begin{align} F_{scheffe} = \\frac{(\\bar{x}_i - \\bar{x}_j)^2}{s^2_w\\left(\\frac{1}{n_i} + \\frac{1}{n_j}\\right)} \\end{align}\\] where: \\(\\mathbf{\\bar{x}_i}\\) is the mean of the first group being compared \\(\\mathbf{\\bar{x}_j}\\) is the mean of the second group being compared \\(\\mathbf{n_i}\\) is the group size of the first group being compared \\(\\mathbf{n_j}\\) is the group size of the second group being compared \\(\\mathbf{F_{scheffe}}\\) is the F statistic of Scheffe Test \\(\\mathbf{s^2_w}\\) is the within-group mean square (MSW or MSE) error, the outcome from ANOVA Below is a naive implementation of Scheffe’s comparison test in R code: scheffe.comparison &lt;- function(dependent, factor) { factors = levels(factor) groups = split( dependent, factor) anova = one_way_anova(groups) m = length(groups) # number of groups scheffe = c(); rname = c() # Critical value for the F-Test at %95 cv_anova = qf(0.95, anova[&quot;dfB&quot;], anova[&quot;dfW&quot;]) # Critical value for Scheffe Test cv_scheffe= as.numeric( anova[&quot;dfB&quot;] * cv_anova ) # Get the s2w s2w = anova[&quot;MSW&quot;] for (i in 1:m) { n.i = length(groups[[i]]); u.i = mean(groups[[i]]); g.i = factors[i] for (j in i:m) { if (i != j) { n.j = length(groups[[j]]); u.j = mean(groups[[j]]); g.j = factors[j] d = u.j - u.i # difference f_scheffe = (u.i - u.j)^2/ ( s2w * (1/n.i + 1/n.j)) signif = &#39;&#39; if (f_scheffe &gt; cv_scheffe) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i, signif)) scheffe = rbind(scheffe, c(round(d,5), f_scheffe, cv_scheffe )) } } } colnames(scheffe) = c(&quot;diff&quot;, &quot;F&quot;, &quot;critical value&quot; ) rownames(scheffe) = rname scheffe } scheffe.comparison(mtcars$mpg, as.factor( mtcars$cyl)) ## diff F critical value ## 6-4* -6.921 19.723 6.655 ## 8-4* -11.564 79.291 6.655 ## 8-6* -4.643 9.683 6.655 Here, an asterisk(*) implies a significant difference between the pair of means because the f-statistic is greater than the critical value. 6.5.2 Fisher’s Test Fisher’s Test, also called Tukey’s LSD (least significant difference), is another Post-HOC multiple comparison test. First, let us perform a One-Way ANOVA using our R implementation. Here, we use the mtcars dataset as before to illustrate a pairwise comparison: (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 We can validate the result using aov(). First and foremost, we convert one of the variables into a factor variable: number of cylinders (cyl). mtcars_ = mtcars mtcars_$cyl_factor = as.factor(mtcars_$cyl) aov.model = aov(mpg ~ cyl_factor, data = mtcars_) # One-Way Anova summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 39.7 5e-09 *** ## Residuals 29 301 10 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Second, compute for the standard error (SE) per comparison: \\[\\begin{align} SE_{(within\\ comparison)} {}&amp;= \\sqrt{\\frac{MS_W}{n}} = \\sqrt{\\frac{s^2_W}{n}} = \\frac{s_w}{\\sqrt{n}}\\\\ &amp;or \\nonumber \\\\ SE_{(within\\ comparison)} &amp;= \\sqrt{\\frac{MS_W}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)}\\ \\leftarrow \\text{if different group sizes}\\\\ &amp;= s_w \\sqrt{\\frac{1}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)} \\end{align}\\] where: \\(SE_{(within\\ comparison)}\\) is the standard error per within-comparison \\(\\mathbf{MS_W}\\) is the within-group mean square (error) n is the group size \\(\\mathbf{n_i}\\) is the size of the first group \\(\\mathbf{n_j}\\) is the size of the second group groups = split( mtcars$mpg, as.factor(mtcars$cyl)) n.1 = length(groups[[1]]); n.2 = length(groups[[2]]) (SE = sqrt(as.numeric(anova[&quot;MSW&quot;]) * ( 1 / n.1 + 1 / n.2))) ## [1] 1.558 Third, compute for Fisher’s LSD using the following formula: \\[\\begin{align} Fisher&#39;s\\ LSD_{i,j} = t_{\\alpha/2}{( df_w)} \\times SE \\end{align}\\] where: LSD is Fisher’s Significance range value, also called Fisher’s Criterion SE is the standard error t is the t-critical value (we can use built-in R function qt()) \\(\\mathbf{\\alpha}\\) is the alpha representing type I error \\(\\mathbf{df_w}\\) is degrees of freedom (residual or within-group df) Note: if group sizes are different, then \\(n_i\\) is the size of the first group in pair, \\(n_j\\) is the size of the second group in pair) Here, we use a built-in R function called qt() to calculate t-critical value. Then with an assumed 95% confidence level, we now compute for the LSD: t = qt(p = 0.05/2, df = anova[&quot;dfW&quot;] , lower.tail=FALSE ) (lsd = t * SE) ## [1] 3.187 Fourth, determine the significance. \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j) &gt; LSD_{i,j}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] Fifth, repeat the process for every pair. Here is a naive implementation of Fisher’s LSD test using R code: fisher.comparison &lt;- function(dependent, factor) { factors = levels(as.factor(factor)) groups = split( dependent, as.factor(factor)) anova = one_way_anova(groups) m = length(groups) # number of groups fisher = c(); rname = c() # (t - critical value) dfW = as.numeric(anova[&quot;dfW&quot;]) q = qt(p = 0.05/2, df = dfW , lower.tail=FALSE ) msw = as.numeric(anova[&quot;MSW&quot;]) for (i in 1:m) { n.i = length(groups[[i]]); u.i = mean(groups[[i]]); g.i = factors[i] for (j in i:m) { if (i != j) { n.j = length(groups[[j]]); u.j = mean(groups[[j]]); g.j = factors[j] d = abs(u.j - u.i) # difference se = sqrt( (msw / 2) * (1/n.i + 1/n.j)) # different size t = d / se # Fisher&#39;s LSD lsd = q * se # critical range p = pt( t * sqrt(2), df = dfW, lower.tail=FALSE) lower = d - lsd # lower boundary of conf. interval upper = d + lsd # upper boundary of conf. interval signif = &#39;&#39; if (d &gt; lsd) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i, signif)) fisher = rbind(fisher, c(se, dfW, round(d,5), lower, upper, lsd, q, p)) } } } colnames(fisher) = c(&quot;se&quot;, &quot;dfW&quot;, &quot;diff&quot;, &quot;lower&quot;, &quot;upper&quot;, &quot;Qlsd&quot;, &quot;Qcrit&quot;, &quot;pvalue&quot;) rownames(fisher) = rname fisher } fisher.comparison(mtcars$mpg, mtcars$cyl) ## se dfW diff lower upper Qlsd Qcrit pvalue ## 6-4* 1.1019 29 6.921 4.667 9.174 2.254 2.045 4.521e-10 ## 8-4* 0.9183 29 11.564 9.686 13.442 1.878 2.045 1.857e-17 ## 8-6* 1.0550 29 4.643 2.485 6.801 2.158 2.045 4.320e-07 In our result, the absolute difference of each pair of groups is larger than the Fisher’s LSD criterion (lsd), e.g., diff &gt; lsd. Additionally, we also show that the p-value is lesser than our alpha, e.g., \\(p &lt; \\alpha\\) (at 0.05); therefore, there is a significant difference for all pairs of means, as shown by the asterisk(*) in the table. 6.5.3 Tukey’s Test Tukey’s Test, also called Tukey’s HSD (honestly significant difference), is another Post-HOC multiple comparison test. It becomes a Tukey-Kramer’s Test if group sizes are not the same (See computation of standard error (SE) later). Here, we use the mtcars dataset as before to illustrate a pairwise comparison. Let us convert one of the variables into a factor variable: number of cylinders (cyl) while at the same time, view the outcome from aov() as a review: mtcars_ = mtcars mtcars_$cyl_factor = as.factor(mtcars_$cyl) aov.model = aov(mpg ~ cyl_factor, data = mtcars_) # One-Way Anova summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 39.7 5e-09 *** ## Residuals 29 301 10 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 First, there are a few assumptions we need to take note of when working with Tukey’s Method: assumption of independence - samples are independent of each other (no association). assumption of normality - samples follow a normal distribution. We can use Shapiro-Wilk test as an example to test normality. We can use the D’agostino-Pearson test for skewness and kurtosis. There are also other methods available to use. shapiro.test(residuals(aov.model)) ## ## Shapiro-Wilk normality test ## ## data: residuals(aov.model) ## W = 0.97, p-value = 0.5 assumption of homogeneity - samples have the same variance. We can use Bartlett’s test to test homogeneity of variance. See also Games-Howell Test in the next section. bartlett.test(mpg ~ cyl_factor, data = mtcars_) ## ## Bartlett test of homogeneity of variances ## ## data: mpg by cyl_factor ## Bartlett&#39;s K-squared = 8.4, df = 2, p-value = 0.02 Second, determine the groups, e.g. \\((4,\\ 6,\\ 8)\\), to be compared from the factor variable, e.g. cyl (number of cylinders). (groups = levels(as.factor(mtcars$cyl))) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; Third, perform a One-Way ANOVA using our R implementation: (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 Fourth, compute for the standard error (SE) per comparison: \\[\\begin{align} SE_{(within\\ comparison)} {}&amp;= \\sqrt{\\frac{MS_W}{n}} = \\sqrt{\\frac{s^2_W}{n}} = \\frac{s_w}{\\sqrt{n}}\\\\ &amp;or\\nonumber \\\\ SE_{(within\\ comparison)} &amp;= \\sqrt{\\frac{MS_W}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)}\\ \\leftarrow \\text{if different group sizes}\\\\ &amp;= s_w \\sqrt{\\frac{1}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)} \\label{eqn:eqnnumber28} \\end{align}\\] where: \\(SE_{(within\\ comparison)}\\) is the standard error per within-comparison \\(\\mathbf{MS_W}\\) is the within-group mean square (error) n is the group size \\(\\mathbf{n_i}\\) is the size of the first group \\(\\mathbf{n_j}\\) is the size of the second group n = sample_size (SE = sqrt(as.numeric(anova[&quot;MSW&quot;]) / n)) # using groups of same size ## [1] 1.019 Fifth, compute for Tukey’s HSD using the following formula: \\[\\begin{align} Q_{hsd} = q_{\\alpha}{( m, df_w)} \\times SE \\end{align}\\] where: \\(\\mathbf{Q_{hsd}}\\) is Tukey’s Significance range value, also called Tukey’s Criterion SE is the standard error q is the Tukey’s studentized range value (we can use built-in R function qtukey()) \\(\\mathbf{\\alpha}\\) is the alpha representing type I error m is total number of groups \\(\\mathbf{df_w}\\) is degrees of freedom (residual or within-group df) Note: if group sizes are different, then \\(n_i\\) is the size of the first group in pair, \\(n_j\\) is the size of the second group in pair) Here, we use a built-in R function called qtukey() to calculate Tukey’s studentized range value. Then with an assumed 95% confidence level, we now compute for the HSD: m = length(groups) # Number of Groups q = qtukey(p = 0.05, nmeans = m, df = anova[&quot;dfW&quot;] , lower.tail=FALSE ) (hsd = q * SE) # with same group sizes ## [1] 3.56 Sixth, determine the significance. \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j) &gt; Q_{hsd}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] Seventh, compute for the Tukey’s Test statistic (T) using the following formula: \\[\\begin{align} T = \\frac{(\\bar{x}_i - \\bar{x}_j)}{SE} \\end{align}\\] where: T is Tukey’s observed t statistic \\(\\bar{x}_{i}\\) is the larger of a pair of group means \\(\\bar{x}_{j}\\) is the smaller of a pair of group means SE is the standard error Eight, compute for the difference of means and the confidence interval: The confidence interval is computed using the following formula: \\[\\begin{align} C.I. = (\\bar{x}_{2} - \\bar{x}_1) \\pm HSD \\end{align}\\] where: \\(\\bar{x}_{2}\\) is the larger of a pair of group means \\(\\bar{x}_{1}\\) is the smaller of a pair of group means HSD is Tukey’s HSD (honestly significant difference) Here is a naive implementation of Tukey’s comparison test in R code: tukey.comparison &lt;- function(dependent, factor) { factors = levels(as.factor(factor)) groups = split( dependent, as.factor(factor)) anova = one_way_anova(groups) m = length(groups) # number of groups tukey = c(); rname = c() msw = as.numeric(anova[&quot;MSW&quot;]) dfW = as.numeric(anova[&quot;dfW&quot;]) # tukey&#39;s HSD (tukey criterion - critical value) q = qtukey(p = 0.05, nmeans = m, df = dfW , lower.tail=FALSE ) for (i in 1:m) { n.i = length(groups[[i]]); u.i = mean(groups[[i]]); g.i = factors[i] for (j in i:m) { if (i != j) { n.j = length(groups[[j]]); u.j = mean(groups[[j]]); g.j = factors[j] d = abs(u.j - u.i) # difference se = sqrt( (msw / 2) * (1/n.i + 1/n.j)) # different size t = d / se # Tukey&#39;s Observed q hsd = q * se # critical range p = ptukey( t * sqrt(2), nmeans = m, df = dfW, lower.tail=FALSE) lower = d - hsd # lower boundary of conf. interval upper = d + hsd # upper boundary of conf. interval signif = &#39;&#39; if (d &gt; hsd) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i, signif)) tukey = rbind(tukey, c(se, dfW, round(d,5), lower, upper, hsd, q, p)) } } } colnames(tukey) = c(&quot;se&quot;, &quot;dfW&quot;, &quot;diff&quot;, &quot;lower&quot;, &quot;upper&quot;, &quot;Qhsd&quot;, &quot;Qcrit&quot;, &quot;pvalue&quot;) rownames(tukey) = rname tukey } tukey.comparison(mtcars$mpg, mtcars$cyl) ## se dfW diff lower upper Qhsd Qcrit pvalue ## 6-4* 1.1019 29 6.921 3.0722 10.769 3.849 3.493 2.174e-06 ## 8-4* 0.9183 29 11.564 8.3565 14.771 3.207 3.493 8.404e-13 ## 8-6* 1.0550 29 4.643 0.9581 8.328 3.685 3.493 3.823e-04 In our result, the absolute difference of each pair of groups is larger than the tukey criterion (hsd), e.g., diff &gt; hsd. Additionally, we also show that the Tukey’s P-value is lesser than our alpha, e.g., \\(p &lt; \\alpha\\) (at 0.05); therefore, there is a significant difference for all pairs of means, as shown by the asterisk(*) in the table. Alternatively, we can also use the built-in R function TukeyHSD(). Let us first perform a One-Way ANOVA: mtcars_ = mtcars mtcars_$cyl_factor = as.factor(mtcars_$cyl) aov.model = aov(mpg ~ cyl_factor, data = mtcars_) # One-Way Anova summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 39.7 5e-09 *** ## Residuals 29 301 10 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Then, we can use TukeyHSD() to perform a Tukey multiple comparisons of means (assuming a confidence level of 95%): (tukey_outcome = TukeyHSD(aov.model, conf.level=0.95, ordered=FALSE)) ## Tukey multiple comparisons of means ## 95% family-wise confidence level ## ## Fit: aov(formula = mpg ~ cyl_factor, data = mtcars_) ## ## $cyl_factor ## diff lwr upr p adj ## 6-4 -6.921 -10.769 -3.0722 0.0003 ## 8-4 -11.564 -14.771 -8.3565 0.0000 ## 8-6 -4.643 -8.328 -0.9581 0.0112 To visualize the difference, we plot the Tukey outcome like so: Figure 6.15: Family-wise Confidence Level Note that we also want to compare adjusted P-values corresponding to each pairwise comparison and the difference and confidence intervals. We cover Bonferroni correction in a later section to introduce one way of handling adjusted P-values. 6.5.4 Newman-Keul Test Newman-Keul Test, also called Student-Newman-Keul (SNK) test, is another Post-HOC multiple comparison test similar to Tukey’s HSD done sequentially. First, because this is done sequentially, we list the means of groups in descending or ascending order: group_means = aggregate(x = mtcars$mpg, by = list(mtcars$cyl), FUN=&quot;mean&quot;) means_descend = sort(x = group_means$x, decreasing=TRUE, index.return = TRUE) names(means_descend$x) = groups[means_descend$ix] (means_descend = means_descend$x) ## 4 6 8 ## 26.66 19.74 15.10 We then get the difference between the greater means and the lesser means amongst all of them. Here, we use a table: n = length(means_descend) d = matrix(&#39;.&#39;, n, n) for (i in 1:n) { for (j in n:1) { if (means_descend[i] &gt; means_descend[j]) { d[i, n - j + 1] = means_descend[i] - means_descend[j] } } } rownames(d) = round(means_descend,3) colnames(d) = round(sort(means_descend, decreasing=FALSE),3) knitr::kable( d, caption = &quot;Newman-Keul Diff Table&quot; ) Table 6.13: Newman-Keul Diff Table 15.1 19.743 26.664 26.664 11.5636363636364 6.92077922077922 . 19.743 4.64285714285714 . . 15.1 . . . Second, perform a One-Way ANOVA using our R implementation (still using mtcars dataset): (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 Here we know that \\(\\mathbf{H_0}\\) is rejected, so we continue to investigate the difference. Third, calculate the standard error (SE) for each comparison with the same formula used for Tukey’s Test (See SE formula from Tukey’s test). Each comparison will have a different SE as we use mtcars with different group sizes. \\[\\begin{align} SE_{(within\\ comparison)} &amp;= \\sqrt{\\frac{MS_W}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)}\\ \\leftarrow \\text{if different group sizes}\\\\ &amp;= s_w \\sqrt{\\frac{1}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)} \\end{align}\\] Fourth, calculate the Newman-Keul’s criterion: \\[\\begin{align} Q_{nk} = q_{\\alpha}(r, df) \\times SE \\end{align}\\] where: \\(\\mathbf{Q_{nk}}\\) is Newman-Keul’s criterion q is the Tukey range value (for which we can use qtukey() function) \\(\\mathbf{\\alpha}\\) is the alpha (e.g. 0.05) r is the rank distance between pairs. Fifth, determine the significance. \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j)_r &gt; Q_{nk}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] Here is a naive implementation of the Newman-Keul’s comparison test in R code: newmankeul.comparison &lt;- function(group_means, anova) { means_descend = sort(x = group_means$x, decreasing=TRUE, index.return = TRUE) group_length = aggregate(x = mtcars$mpg, by = list(mtcars$cyl), FUN=&quot;length&quot;) colnames(group_length) = c(&quot;groups&quot;, &quot;n&quot;) n = length(means_descend$x) nkr = c(); rname = c() msw = as.numeric(anova[&quot;MSW&quot;]) dfw = as.numeric(anova[&quot;dfW&quot;]) for (i in 1:n) { for (j in n:1) { if (means_descend$x[i] &gt; means_descend$x[j]) { d = means_descend$x[i] - means_descend$x[j] i.index = means_descend$ix[i] j.index = means_descend$ix[j] n.i = group_length$n[i.index]; g.i = group_length$groups[i.index] n.j = group_length$n[j.index]; g.j = group_length$groups[j.index] rank = j.index - i.index + 1 se = sqrt( (msw / 2) * (1/n.i + 1/n.j)) q = qtukey(p = 0.05, nmeans = rank, df = dfw, lower.tail=FALSE) nk = q * se t = d / se # Tukey&#39;s Observed q p = ptukey( t * sqrt(2), nmeans = rank, df = dfw, lower.tail=FALSE) signif = &#39;&#39; if (d &gt; nk ) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i,signif)) nkr = rbind(nkr, c(se, d, rank, dfw, nk, q, p)) } } } colnames(nkr) = c(&quot;se&quot;, &quot;diff&quot;, &quot;rank&quot;, &quot;dfw&quot;, &quot;Qnk&quot;, &quot;Qcrit&quot;, &quot;adj. pval&quot;) rownames(nkr) = rname nkr } (nkr = newmankeul.comparison(group_means, anova)) ## se diff rank dfw Qnk Qcrit adj. pval ## 8-4* 0.9183 11.564 3 29 3.207 3.493 8.404e-13 ## 6-4* 1.1019 6.921 2 29 3.187 2.892 7.397e-07 ## 8-6* 1.0550 4.643 2 29 3.051 2.892 1.335e-04 We see that all the pairs (comparisons) have an asterisk(*), meaning that each of the mean differences is greater than \\(\\mathbf{Q_{(nk)}}\\), e.g., diff &gt; \\(Q_{nk}\\) - equivalent to being significant. For example, the difference of the pair 8-4 is greater than the computed Newman-Keul’s criterion, \\(\\mathbf{NK_{(3)}}\\) with a rank of 3, e.g. 11.5636 &gt; 3.4926. Therefore, this shows a significant difference between the pair of group means. 6.5.5 Games-Howell Test We use this test if there is a violation of homogeneity of variance, meaning variances across multiple groups are not constant (not equal). We leave readers to investigate different tests of homogeneity, e.g., Bartlett’s Test and Levene’s Test. Let us continue to use the mtcars dataset as before to illustrate a pairwise comparison. First, to solve for the violation of homogeneity - the unequal variances - we apply Welch’s correction using a t-test; hence, this is also called Welch’s t-test in which we use the following equation: \\[\\begin{align} t_{(welch)} = \\frac{\\bar{x}_i - \\bar{x}_j}{S_p}\\ \\ \\leftarrow \\ \\ S_p = {\\sqrt{\\frac{\\sigma^2_i}{n_i} + \\frac{\\sigma^2_j}{n_j}}} \\end{align}\\] where: t is Welch’s T statistic \\(S_p^2\\) is the pooled variance. \\(S_p\\) is the pooled standard deviation. \\(\\bar{x}_i\\ ,\\ \\bar{x}_j\\) are means of the first and second group in a pair \\(\\sigma_i\\ ,\\ \\sigma_j\\) are standard deviations of the first and second group \\(n_i\\ ,\\ n_j\\) are sizes of first and second group Second, compute for the degrees of freedom. Note that unlike Tukey’s Test in which we use the ANOVA’s within-group degrees of freedom (dfW), here in Games-Howell Test, we compute the degrees of freedom from each comparison (pair of group means): \\[\\begin{align} df = \\frac{(S_p^2)^2} {\\frac{\\left(\\frac{\\sigma^2_i}{n_i}\\right)^2}{n_i-1} + \\frac{\\left(\\frac{\\sigma^2_j}{n_j}\\right)^2}{n_j-1}}\\ \\ \\leftarrow \\ \\ S_p^2 = \\left(\\frac{\\sigma^2_i}{n_i} + \\frac{\\sigma^2_j}{n_j}\\right). \\end{align}\\] Third, compute for the pairwise standard error: \\[\\begin{align} SE = \\frac{S_p}{\\sqrt{2}} = \\sqrt{\\frac{S^2_p}{2}} = \\sqrt{\\frac{1}{2}\\left(\\frac{\\sigma^2_i}{n_i} + \\frac{\\sigma^2_j}{n_j}\\right)} \\end{align}\\] Fourth, compute for the Games-Howell Significance range value \\[\\begin{align} Q_{gh} = q_{\\alpha}(m, df) \\times SE \\end{align}\\] where: \\(\\mathbf{Q_{gh}}\\) is Games-Howell’s criterion q is the Tukey range value (for which we can use qtukey() function) \\(\\mathbf{\\alpha}\\) is the alpha (e.g. 0.05) m is the total number of groups Fifth, determine the significance. \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j) &gt; Q_{gh}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] Finally, similar to Tukey’s confidence interval, here is the lower and upper estimate of our confidence interval: \\[\\begin{align} C.I. = (\\bar{x}_{i} - \\bar{x}_j) \\pm R \\end{align}\\] where: \\(\\bar{x}_{i}\\) is the larger of a pair of group means \\(\\bar{x}_{j}\\) is the smaller of a pair of group means R is Games-Howell’s R (ghr) Here is a naive implementation of the Games-Howell test in R code: gameshowell.comparison &lt;- function(dependent, factor) { factors = levels(as.factor(factor)) groups = split( dependent, as.factor(factor)) anova = one_way_anova(groups) m = length(groups) # number of groups ghr = c(); rname = c() for (i in 1:m) { n.i = length(groups[[i]]); u.i = mean(groups[[i]]); s.i = var(groups[[i]]) g.i = factors[i] for (j in i:m) { if (i != j) { n.j = length(groups[[j]]); u.j = mean(groups[[j]]); s.j = var(groups[[j]]) g.j = factors[j] std.i = s.i / n.i; std.j = s.j / n.j d = abs( u.i - u.j ) s2p = ( std.i + std.j ) se = sqrt( s2p / 2 ) df = s2p^2 / ( (std.i^2 / (n.i - 1)) + (std.j^2 / (n.j - 1)) ) t = d / sqrt( s2p ) # welch&#39;s t q = qtukey(p = 0.05, nmeans = m, df = df, lower.tail = FALSE) p = ptukey( t * sqrt(2), nmeans = m, df = df, lower.tail=FALSE) gh = q * se lower = d - gh upper = d + gh signif = &#39;&#39; if (d &gt; gh ) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i,signif)) ghr = rbind(ghr, c(se, df, d, lower, upper, gh, q, p )) } } } colnames(ghr) = c(&quot;se&quot;, &quot;df&quot;, &quot;diff&quot;, &quot;lower&quot;, &quot;upper&quot;, &quot;Qgh&quot;, &quot;Qcrit&quot;, &quot;adj. pval&quot;) rownames(ghr) = rname ghr } round(gameshowell.comparison(mtcars$mpg, mtcars$cyl),3) ## se df diff lower upper Qgh Qcrit adj. pval ## 6-4* 1.037 12.96 6.921 3.047 10.795 3.874 3.736 0.001 ## 8-4* 1.076 14.97 11.564 7.609 15.518 3.955 3.674 0.000 ## 8-6* 0.620 18.50 4.643 2.409 6.877 2.234 3.601 0.000 6.5.6 Dunnett’s Test Dunnett’s Test is another Post-HOC multiple comparison test. It uses the mean of one fixed control group to compare against the means of other experiment groups. First, let us use the mtcars dataset as before with cyl as the factor variable of three groups: (groups = levels(as.factor(mtcars$cyl))) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; For illustration, let us use group 4 for our control group, while groups 6 and 8 are our experiment groups. That tells us that the group with cars having four cylinders is our model (or control group) for comparison of fuel consumption against cars that are not with four cylinders. Second, run One-Way ANOVA for analysis. (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 We know that \\(\\mathbf{H_0}\\) is rejected, so we continue to investigate the difference. Third, recall the standard error (SE) formula we used in Tukey’s Test (See Equation \\(\\ref{eqn:eqnnumber28}\\)). \\[\\begin{align} SE_{(within\\ comparison)} &amp;= \\sqrt{\\frac{MS_W}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_c}\\right)}\\ \\leftarrow \\text{if different group sizes}\\\\ &amp;= s_w \\sqrt{\\frac{1}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_c}\\right)} \\end{align}\\] where: \\(\\mathbf{n_c}\\) is sample size of the control group \\(\\mathbf{n_i}\\) is sample size of the ith experiment group Fourth, compute for Dunnett’s statistic: \\[\\begin{align} Q_{dunn} = q_{\\alpha}(n, df) \\times SE \\end{align}\\] where: \\(\\mathbf{Q_{dunn}}\\) is Dunnett’s criterion q is Dunnett’s Critical value ( we can use 3rd party function, cvSDDT() ) n is the nth experiment group Fifth, to determine significance, we use the following equation: \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j)_n &gt; Q_{dunn} \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] where: \\(\\bar{x}_{i}\\) is the larger of a pair of group means \\(\\bar{x}_{j}\\) is the smaller of a pair of group means Here is a naive implementation of Dunnett’s comparison in R code. We use a library called DunnettTests for the function cvsDDT() to determine Dunnett’s Critical value. library(DunnettTests) Note that the cvSDDT() uses pmvnorm() for a multivariate cumulative normal distribution. This is the function curve in which cvSDDT() also uses uniroot() for root finding. See Chapter 3 (Numerical Linear Algebra II) for Root Finding using Bisection method. The roots become the critical values used to construct the Dunnett’s table. dunnett.comparison &lt;- function(dependent, factor) { factors = levels(as.factor(factor)) groups = split( dependent, as.factor(factor)) anova = one_way_anova(groups) m = length(groups) # number of groups dunr = c(); rname = c() control = groups[[1]] # use first group as control group n.c = length(control); u.c = mean(control); s.c = var(control) std.c = s.c / n.c; g.c = factors[1] msw = as.numeric(anova[&quot;MSW&quot;]) dfW = as.numeric(anova[&quot;dfW&quot;]) q = cvSDDT(k=m, alpha=0.05, alternative=&quot;U&quot;, corr=0.5, df=dfW) # use the rest of group as experiment groups for (i in 2:m) { n.i = length(groups[[i]]); u.i = mean(groups[[i]]); s.i = var(groups[[i]]) g.i = factors[i] std.i = s.i / n.i d = abs( u.c - u.i ) s2p = ( std.c + std.i ) se = sqrt( (msw / 2) * (1/n.i + 1/n.c)) t = d / sqrt( s2p ) # welch&#39;s t p = ptukey( t * sqrt(2), nmeans = m, df = dfW, lower.tail=FALSE) dunn = q[i] * se lower = d - dunn upper = d + dunn signif = &#39;&#39; if (d &gt; dunn ) { signif = &#39;*&#39; } rname = c(rname, paste0(g.i, &quot;-&quot;, g.c,signif)) dunr = rbind(dunr, c(se, dfW, d, lower, upper, dunn, q[i], p )) } colnames(dunr) = c(&quot;se&quot;, &quot;df&quot;, &quot;diff&quot;, &quot;lower&quot;, &quot;upper&quot;, &quot;Qdunn&quot;, &quot;Qcrit&quot;, &quot;adj. pval&quot;) rownames(dunr) = rname dunr } dunnett.comparison(mtcars$mpg, mtcars$cyl) ## se df diff lower upper Qdunn Qcrit adj. pval ## 6-4* 1.1019 29 6.921 4.726 9.116 2.195 1.992 1.596e-04 ## 8-4* 0.9183 29 11.564 9.589 13.538 1.974 2.150 6.646e-08 The result shows that all the experiment groups differ significantly from the control group, as indicated by the asterisk(*). 6.5.7 Duncan’s Test Duncan’s Multiple Range Test is another Post-HOC test that compares differences in group means based on rank. It is a variant of SNK. To illustrate, let us perform Duncan’s Multiple Range Test. First, let us use the mtcars dataset as before with cyl as the factor variable of three groups: (groups = levels(as.factor(mtcars$cyl))) ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; Second, rank the means of groups in ascending order. rank &lt;-function(dependent, factor) { group_means = aggregate(x = dependent, by = list(factor), FUN=&quot;mean&quot;) means_ascend = sort(x = group_means$x, decreasing=FALSE, index.return = TRUE) r = seq(1, length(groups)) names(r) = groups[means_ascend$ix] r = rbind(r, means_ascend$x) rownames(r) = c(&quot;rank&quot;, &quot;mean&quot;) r } rank(mtcars$mpg, mtcars$cyl) ## 8 6 4 ## rank 1.0 2.00 3.00 ## mean 15.1 19.74 26.66 Third, run One-Way ANOVA for analysis. (anova_outcome = anova = one_way_anova(mtcars$mpg, mtcars$cyl)) ## SSB SSW dfB dfW MSB MSW F ## 824.78 301.26 2.00 29.00 412.39 10.39 39.70 Fourth, recall the standard error (SE) formula we used in Tukey’s Test (See Equation \\(\\ref{eqn:eqnnumber28}\\)). \\[\\begin{align} SE_{(within\\ comparison)} &amp;= \\sqrt{\\frac{MS_W}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)}\\ \\leftarrow \\text{if different group sizes}\\\\ &amp;= s_w \\sqrt{\\frac{1}{2}\\left(\\frac{1}{n_i}+\\frac{1}{n_j}\\right)} \\end{align}\\] Fifth, compute for Duncan’s statistic: \\[\\begin{align} Q_{dunc} = q_{\\alpha}{(r,df_W)} \\times SE \\end{align}\\] where: \\(\\mathbf{Q_{dunc}}\\) is Duncan’s Significance range value, the Duncan’s Criterion SE is the standard error q is Duncan’s range value \\(\\mathbf{\\alpha}\\) is the alpha representing type I error r is the rank distance \\(\\mathbf{df_W}\\) is degrees of freedom (residual or within-group df) Sixth, to determine significance, we use the following equation: \\[\\begin{align} (\\bar{x}_i - \\bar{x}_j)_n &gt; Q_{dunc} \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{there is a significant difference} \\end{align}\\] where: \\(\\bar{x}_{i}\\) is the mean of the first group in a pair \\(\\bar{x}_{j}\\) is the mean of the second group in a pair Here is a naive implementation of Duncan’s MRT in R code. Here, we use the Tukey’s studentized range table: duncan.comparison &lt;- function(dependent, factor) { factors = levels(as.factor(factor)) groups = split( dependent, as.factor(factor)) anova = one_way_anova(groups) rank = rank(dependent, factor) m = length(groups) # number of groups dunr = c(); rname = c() msw = as.numeric(anova[&quot;MSW&quot;]) dfW = as.numeric(anova[&quot;dfW&quot;]) for (i in 1:m) { idx = factors[i] group = groups[[idx]] n.i = length(group); u.i = mean(group); g.i = idx r.i = rank[,idx] for (j in i:m) { if (i != j) { idx = factors[j] group = groups[[idx]] n.j = length(group); u.j = mean(group); g.j = idx r.j = rank[,idx] r_dist = r.i[&quot;rank&quot;] - r.j[&quot;rank&quot;] + 1 r = rank[,r_dist] se = sqrt( (msw / 2) * (1/n.i + 1/n.j)) q = qtukey(p = 0.05, nmeans = rank[2,i], df = dfW, lower.tail=FALSE) p = qtukey(p = 0.05, nmeans = rank[2,i], df = dfW, lower.tail=FALSE) dunc = q * se d = abs( u.i - u.j ) lower = d - dunc upper = d + dunc signif = &#39;&#39; if (d &gt; dunc ) { signif = &#39;*&#39; } rname = c(rname, paste0(g.j, &quot;-&quot;, g.i,signif)) dunr = rbind(dunr, c(se, dfW, d, lower, upper, r_dist, dunc, q, p )) } } } colnames(dunr) = c(&quot;se&quot;, &quot;df&quot;, &quot;diff&quot;, &quot;lower&quot;, &quot;upper&quot;, &quot;dist.&quot;, &quot;Qdunc&quot;, &quot;Qcrit&quot;, &quot;adj.pval&quot;) rownames(dunr) = rname dunr } duncan.comparison(mtcars$mpg, mtcars$cyl) ## se df diff lower upper dist. Qdunc Qcrit adj.pval ## 6-4* 1.1019 29 6.921 1.155 12.69 2 5.766 5.232 5.232 ## 8-4* 0.9183 29 11.564 6.759 16.37 3 4.805 5.232 5.232 ## 8-6 1.0550 29 4.643 -1.138 10.42 2 5.781 5.479 5.479 The outcome shows the first and second mean differences to be significant. Let us validate. First, let us generate a summary of ANOVA using aov(): mtcars_$cyl_factor = as.factor(mtcars_$cyl) aov.model = aov(mpg ~ cyl_factor, data = mtcars_) # One-Way Anova summary(aov.model) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl_factor 2 825 412 39.7 5e-09 *** ## Residuals 29 301 10 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Second, use duncan.test() to perform the Duncan’s test. Here, we use a third-party library called agricolae which comes with a function called duncan.test(). library(agricolae) ## This version of &#39;bslib&#39; is designed to work with &#39;shiny&#39; &gt;= 1.6.0. ## Please upgrade via install.packages(&#39;shiny&#39;). (duncan.model = duncan.test(aov.model,&quot;cyl_factor&quot;)) ## $statistics ## MSerror Df Mean CV ## 10.39 29 20.09 16.04 ## ## $parameters ## test name.t ntr alpha ## Duncan cyl_factor 3 0.05 ## ## $duncan ## NULL ## ## $means ## mpg std r Min Max Q25 Q50 Q75 ## 4 26.66 4.510 11 21.4 33.9 22.80 26.0 30.40 ## 6 19.74 1.454 7 17.8 21.4 18.65 19.7 21.00 ## 8 15.10 2.560 14 10.4 19.2 14.40 15.2 16.25 ## ## $comparison ## NULL ## ## $groups ## mpg groups ## 4 26.66 a ## 6 19.74 b ## 8 15.10 c ## ## attr(,&quot;class&quot;) ## [1] &quot;group&quot; Third, to visualize, let us plot the variance: Figure 6.16: Groups and Interquantile range 6.5.8 Meta-Analysis Test A single study of observation may sometimes not suffice in analyzing data. There is a benefit when merging data from many different studies at times. This meta-analysis of results from different studies may render a different range of values of variables. Also, specific data are manipulated or manufactured rather than observed from real-world events - we call this data synthetic data. We compare the differences and similarities of data properties across studies in analyzing synthetic data. There are two qualities we are looking for in synthetic data: Heterogeneity of variances refers to the differences in quality or property of data (possibly synthesized and) merged from different studies. Homogeneity of variances refers to the similarities in quality or property of data (possibly synthesized and) merged from different studies. Here, we show a forest plot of synthetic data from different studies (See Figure 6.17) to check if data is either homogenous or heterogenous. library(forestplot) set.seed(142) beta0 = 0.5 beta1 = 1.4 X = matrix ( runif(45, min=-2, max=2), nrow=5, ncol=9 ) coefs = matrix(0, nrow=9, ncol=3 ) for (i in 1:9) { x = as.vector( X[,i] ) random_noise = rnorm(n=5, mean=0, sd=1) y = beta0 + beta1 * x + random_noise model = lm( y ~ x) coefs[i,] = c( Mean = coef(model)[2], Lower = confint(model)[2,1], Upper=confint(model)[2,2] ) } labels = matrix( c( &quot;Studies&quot;, paste0(&quot;Study &quot;, 1:9), &quot;Coefficients&quot;, round(coefs[,1],2)), nrow=10, ncol=2) coefs = rbind(NA, coefs) forestplot(labeltext = labels, mean=coefs[,1], lower= coefs[,2], upper= coefs[,3], title=&quot;Meta-Analysis Study&quot; , col=fpColors(box=&quot;black&quot;, line=&quot;red&quot;), grid = structure(c(1, 2, 3), gp = gpar(col = &quot;grey&quot;, lty=2)), new_page = TRUE, boxsize=0.25, xlab=&quot;Coef Index&quot;) Figure 6.17: Heterogeniety It shows in the forest plot above that studies 1 through 9 render results that seem reasonably within neighboring means and ranges. However, studies 3 and 9 may require some attention since they have confidence intervals whose ranges are further apart. Thus, that can be considered to have a higher heterogeneity than the other studies - meaning, studies 3 and 9 have more significant differences than the other studies (in the context of the confident interval). If we are more strict regarding the placement of the blue box (the coefficient mean), then study 9 has a negative value and is placed almost outside the grid between 0 and 3. So in this context, we can say that study 9 has a higher heterogeneity in our being strict about placement. 6.6 Statistical Modeling Domain (Subject) Knowledge is key to modeling. For example, suppose we try to tackle a known subject in climatology around climate change and global warming. Suppose we declare the following statement; albeit, admittedly, we are not being savvy around this domain: Global warming is due to one of the following reasons, namely deforestation, greenhouse gasses, or interaction between deforestation and greenhouse gasses. We translate the statement into the following linear equation: \\[\\begin{align} y = \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1 x_2 \\end{align}\\] where: y is a response variable corresponding to global warming x1 is a predictor variable corresponding to greenhouse gases x2 is a predictor variable corresponding to deforestation x1x2 is an interaction between deforestation and greenhouse gases Our goal is to be able to find a model that can describe the relationship of predictor variables with their additive effect on the response variable. 6.6.1 Model Specification The statement above about climate change is an example of describing a domain problem. Our goal is to translate the problem into a statistical model, particularly into a regression model. One aspect of regression modeling is the formulation of model specification. From a few dictionary sources (e.g., Merriam-webster and Wikipedia), the term specification refers to the process of describing and identifying the requirements of a given domain problem. Here, statistical model specification refers to just that. However, a true definition of specification comes from the word itself - being specific or being detailed. A model can either be correctly specified or incorrectly specified. A specification error happens when a model is incorrectly specified - which suffers one of two states: Underspecified or Overspecified. Such specification errors may affect bias and variance, which we expound further in Chapter 9 (Computation Learning I). In Model Specification, one of our goals is to make sure that our model is correctly specified, which is intrinsically achieved as a result of explicitly performing feature and model selection, parameter optimization and regularization through validationand evaluation and test through inference - among many other considerations. As a starting point, let us discuss preparatory modeling operations, starting with understanding variable interaction and then a discussion of regression analysis. In turn, we also cover the Significance of Regression, including evaluating models by inference. Note that we end this Chapter covering Statistical Computation with a discussion on Regression. We defer the topic of Classification until we reach Chapter 10 (Computation Learning II). 6.6.2 Statistical Interaction Most of our discussion in Linear regression includes simple additive linear combination, e.g.: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\ ...\\ + \\beta_n x_n + \\epsilon \\end{align}\\] We also discussed Polynomial regression, e.g. \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2^2 + \\ ...\\ + \\beta_n x_n^n + \\epsilon \\end{align}\\] In this section, we discuss interactions of predictor variables, e.g.: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 x_1x_2 + \\epsilon \\end{align}\\] Notice that the first three terms of the linear combination form a simple linear equation. The fourth term however comprises the independent variable \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\). We call this interaction. In determining significance of regression for interactions, our null hypothesis may be stated as such: \\[\\begin{align} H_0 {}&amp;: \\beta_3 = 0 \\ \\ \\ \\ \\text{there is no effect of the interaction of}\\ \\mathbf{x_1}\\ and\\ \\mathbf{x_2} \\\\ H_1 &amp;: \\beta_3 \\ne 0\\ \\ \\ \\ \\text{there is an effect of the interaction of}\\ \\mathbf{x_1}\\ and\\ \\mathbf{x_2} \\end{align}\\] To illustrate, let us generate three independent variables, { \\(\\mathbf{x1,\\ x2,\\ x3}\\) }: sample_size = 20 range = seq(0, 1, length.out=200) set.seed(1020) x1 = sample(range, size=sample_size, replace=TRUE) set.seed(2020) x2 = sample(range, size=sample_size, replace=TRUE) set.seed(3020) x3 = sample(range, size=sample_size, replace=TRUE) e = rnorm(sample_size, mean=0, sd = 1) # residual y = sort( rnorm(sample_size, mean=0, sd = 1) + e ) We then create four linear models with the following combination: lm1.model = lm( y ~ x1 + x2 + x3 + x1:x2 + x2:x3 + x1:x3 + x1:x2:x3 ) lm2.model = lm( y ~ x1 * x2 * x3) lm3.model = lm( y ~ x1 * I(x1^2) * I(x1^3)) The first model, lm1.model, and the second model, lm2.model, are the same but different formulae. The first model uses a formula with arbitrary interactions. The second formula uses a much simpler and more convenient format which expands to include every possible combination of additive and interaction terms. For example, an interaction term has the format as the following: x1:x2, x2:x3, x1:x3, x1:x2:x3. Notice that both models render the same summarized coefficients. summary(lm1.model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.945 2.119 1.390 0.1898 ## x1 -8.243 5.260 -1.567 0.1430 ## x2 -5.422 4.019 -1.349 0.2022 ## x3 -6.802 4.152 -1.638 0.1273 ## x1:x2 16.006 9.822 1.630 0.1291 ## x2:x3 14.235 8.781 1.621 0.1309 ## x1:x3 21.916 13.267 1.652 0.1245 ## x1:x2:x3 -37.256 24.178 -1.541 0.1493 summary(lm2.model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.945 2.119 1.390 0.1898 ## x1 -8.243 5.260 -1.567 0.1430 ## x2 -5.422 4.019 -1.349 0.2022 ## x3 -6.802 4.152 -1.638 0.1273 ## x1:x2 16.006 9.822 1.630 0.1291 ## x1:x3 21.916 13.267 1.652 0.1245 ## x2:x3 14.235 8.781 1.621 0.1309 ## x1:x2:x3 -37.256 24.178 -1.541 0.1493 Also, in terms of significance of effect, none of the coefficients show any significance given the data. The third model uses the notation I(.) to preserve the literal syntax and therefore does not perform a square or cube computation. summary(lm3.model)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -2.023 1.066 -1.898 0.08012 ## x1 69.667 41.699 1.671 0.11866 ## I(x1^2) -607.596 448.989 -1.353 0.19904 ## I(x1^3) 2373.538 1870.448 1.269 0.22670 ## x1:I(x1^3) -4444.136 3525.578 -1.261 0.22963 ## I(x1^2):I(x1^3) 3906.938 3062.678 1.276 0.22440 ## x1:I(x1^2):I(x1^3) -1295.589 1000.864 -1.294 0.21802 anova(lm3.model) ## Analysis of Variance Table ## ## Response: y ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## x1 1 3.52 3.52 2.17 0.164 ## I(x1^2) 1 2.11 2.11 1.31 0.274 ## I(x1^3) 1 5.33 5.33 3.29 0.093 . ## x1:I(x1^3) 1 0.91 0.91 0.56 0.467 ## I(x1^2):I(x1^3) 1 0.06 0.06 0.04 0.845 ## x1:I(x1^2):I(x1^3) 1 2.71 2.71 1.68 0.218 ## Residuals 13 21.03 1.62 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice that we can produce seven terms with only three independent variables (with seven coefficients including the intercept) based on the additional combination of their interactions. Therefore, if we have many more independent variables, the combination gets exponential. We may also have to determine if all independent variables are significant (meaningful), or we review each of them and perhaps exclude those that do not contribute to the effect. In Chapters 9 (Computational Learning I) and 10 (Computational Learning II), we discuss Regression and Classification techniques such as Regression Trees that allow us to handle a large number of independent variables that create complex interactions. 6.6.3 Dummy Variables To extend the concept of Interaction, we introduce the notion of Dummy variables, also called indicator variables. These variables act as switches, and they apply to Categorical variables. We also call them Binary variables if the possible number of values of the categorical variables is two. For example, for a categorical variable with two levels (e.g., female and male), we create one dummy variable: \\[ v1 = \\begin{cases} 1 &amp; \\text{female} \\\\ 0 &amp; \\text{male} \\end{cases} \\] For a categorical variable with three levels (e.g., weekly, monthly, yearly), we create three dummy variables: \\[ v1 = \\begin{cases} 1 &amp; \\text{weekly} \\\\ 0 &amp; \\text{non-weekly} \\end{cases}\\ \\ \\ \\ \\ v2 = \\begin{cases} 1 &amp; \\text{monthly} \\\\ 0 &amp; \\text{non-monthy} \\end{cases}\\ \\ \\ \\ \\ v3 = \\begin{cases} 1 &amp; \\text{yearly} \\\\ 0 &amp; \\text{non-yearly} \\end{cases} \\] We also introduce the concept of reference level in which our linear model uses only two dummy variables for a three-level categorical variable. Two dummy variables are chosen to represent two levels, and the other level becomes a reference level. First, let us create a dataset with three predictor variables, namely productivity, gender, and timeline (tl) and a response variable, namely label. The gender and timeline (tl) variables are factors. set.seed(2020) N = 30 gender = c(&quot;female&quot;, &quot;male&quot;) sched = c(&quot;weekly&quot;, &quot;monthly&quot;, &quot;yearly&quot;) sample.gender = sample(gender, size=N, replace=TRUE) sample.sched = sample(sched, size=N, replace=TRUE) prod = seq(1, N) + rnorm(n = N, mean=0, sd=1) label = seq(1, N) + rnorm(n = N, mean=2, sd=2) + as.numeric(as.factor(sample.gender)) * as.numeric(as.factor(sample.sched)) dataset = data.frame( productivity = prod, gender = sample.gender, tl = sample.sched, label = label) # head(dataset) ## productivity gender tl label ## 1 0.1875 male yearly 12.818 ## 2 1.2563 female weekly 6.502 ## 3 4.0953 male yearly 7.803 ## 4 6.4354 female yearly 15.403 ## 5 5.3881 female weekly 10.910 ## 6 6.2906 female monthly 9.737 Let us list the levels for both gender and timeline (tl) variables: levels(dataset$gender) ## [1] &quot;female&quot; &quot;male&quot; levels(dataset$tl) ## [1] &quot;monthly&quot; &quot;weekly&quot; &quot;yearly&quot; Using the linear model function lm(.), let us review the coefficients given only productivity and gender as predictors: (model = lm(label ~ productivity + gender, data = dataset)) ## ## Call: ## lm(formula = label ~ productivity + gender, data = dataset) ## ## Coefficients: ## (Intercept) productivity gendermale ## 5.52 0.95 1.97 We notice three coefficients: one for the intercept, one for the productivity predictor, and one for a newly created dummy variable labeled gendermale. This dummy variable belongs to the male gender level. The female gender is chosen as a reference level because it is the first level alphabetically. The general formula for our model is like so: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 v_1 + \\epsilon \\end{align}\\] where: \\(x_1\\) is the predictor variable for productivity, \\(v_1\\) is a dummy variable for male gender category or level, \\(\\beta0\\) is the coefficient for the intercept, \\(\\beta1\\) is the coefficient for \\(x_1\\), \\(\\beta2\\) is the coefficient for \\(v_1\\). Here, \\(v_1\\) is like a switch (or indicator) variable. If the observation shows that the gender is male, then \\(v_1\\) becomes one, and thus we end up with the following regression formula: \\[\\begin{align} y = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon \\end{align}\\] Otherwise, if \\(v_1\\) is zero, then we get the following regression formula: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\epsilon \\end{align}\\] Now, let review our linear model when we use both productivity and timeline as predictors: (model = lm(label ~ productivity + tl, data = dataset)) ## ## Call: ## lm(formula = label ~ productivity + tl, data = dataset) ## ## Coefficients: ## (Intercept) productivity tlweekly tlyearly ## 2.51 1.04 3.39 4.43 We notice four coefficients: one for the intercept, one for the productivity predictor, and two newly created dummy variables called timelineweekly and timelineyearly. These dummy variables belong to the weekly and yearly timeline levels. The monthly timeline is chosen to be a reference level because it is the first level alphabetically. The general formula for our model is like so: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 v_1 + \\beta_3 v_2 + \\epsilon \\end{align}\\] where: \\(x_1\\) is the predictor variable for productivity, \\(v_1\\) is a dummy variable for weekly timeline category or level, \\(v_2\\) is a dummy variable for yearly timeline category or level, \\(\\beta0\\) is the coefficient for the intercept, \\(\\beta1\\) is the coefficient for \\(x_1\\), \\(\\beta2\\) is the coefficient for \\(v_1\\), \\(\\beta3\\) is the coefficient for \\(v_2\\). We then have the following regression formulas: \\[ \\underbrace{y = (\\beta_0 + \\beta_2) + \\beta_1 x_1 + \\epsilon}_\\text{weekly observation}\\ \\ \\ \\ \\ \\ \\ \\underbrace{y = (\\beta_0 + \\beta_3) + \\beta_1 x_1 + \\epsilon}_\\text{yearly observation}\\ \\ \\ \\ \\ \\ \\ \\underbrace{y = (\\beta_0 + \\beta_2 + \\beta_3) + \\beta_1 x_1 + \\epsilon}_\\text{monthly observation} \\] Lastly, let review our linear model when we use all predictors: (model = lm(label ~ ., data = dataset)) ## ## Call: ## lm(formula = label ~ ., data = dataset) ## ## Coefficients: ## (Intercept) productivity gendermale tlweekly tlyearly ## 2.35 1.02 1.16 3.39 3.95 Here, both female gender and monthly timeline become the reference levels. The general formula with three dummy variables becomes: \\[ y = \\beta_0 + \\beta_1 x_1 + \\beta_2 v_1 + \\beta_3 v_2 + \\beta_4 v_3 + \\epsilon \\] where: \\(x_1\\) is the predictor variable for productivity, \\(v_1\\) is a dummy variable for male gender category or level, \\(v_2\\) is a dummy variable for weekly timeline category or level, \\(v_3\\) is a dummy variable for yearly timeline category or level, \\(\\beta0\\) is the coefficient for the intercept, , \\(\\beta1\\) is the coefficient for \\(x_1\\), \\(\\beta2\\) is the coefficient for \\(v_1\\), \\(\\beta3\\) is the coefficient for \\(v_2\\). \\(\\beta4\\) is the coefficient for \\(v_3\\). For an observation with female gender and weekly timeline we have the following regression formula: \\[ y = (\\beta_0 + \\beta_3) + \\beta_1 x_1 + \\epsilon \\] We leave readers to generate the other regression formulas. Regarding significance of regression, we can see below that the productivity predictor is significant with P-value less than the chosen alpha, which is \\(\\alpha = 0.001\\). It also shows that the dummy variables gendermale and timelineyearly are significant at their own chosen alpha (see significance code). (p = summary(model)) ## ## Call: ## lm(formula = label ~ ., data = dataset) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.838 -1.844 -0.297 1.551 5.372 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 2.3461 1.4350 1.63 0.1146 ## productivity 1.0211 0.0616 16.58 5.4e-15 *** ## gendermale 1.1617 1.0667 1.09 0.2865 ## tlweekly 3.3898 1.2541 2.70 0.0122 * ## tlyearly 3.9514 1.3003 3.04 0.0055 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.66 on 25 degrees of freedom ## Multiple R-squared: 0.924, Adjusted R-squared: 0.912 ## F-statistic: 76.1 on 4 and 25 DF, p-value: 1.25e-13 On the other hand, the weekly timeline level has 1.2% chance of not being meaningful. See Figure 6.18 for the plot of our dataset. In the illustration, we emphasize the timeline categories. model = lm(label ~ productivity + tl, data = dataset) model$coefficients ## (Intercept) productivity tlweekly tlyearly ## 2.508 1.037 3.386 4.431 Let us generate the intercepts for each timeline and the slope to plot the regression lines. slope= model$coefficients[2] # B0, if V1 and V2 are zero intrcpt.monthly = model$coefficients[1] # B0 + B2, if V1 = 1 intrcpt.weekly = intrcpt.monthly + model$coefficients[3] # B0 + B3, if V2 = 1 intrcpt.yearly = intrcpt.monthly + model$coefficients[4] col=c(&quot;red&quot;, &quot;black&quot;, &quot;brown&quot;) plot(label ~ productivity , data = dataset, col = col[as.numeric(tl)], pch = as.numeric(tl), xlab=&quot;predictors&quot;, ylab=&quot;response&quot;, main=&quot;Dummy Variable (Timeline)&quot; ) legend(&quot;bottomright&quot;, legend=c( &quot;weekly&quot;, &quot;monthly&quot;, &quot;yearly&quot;), col=c(&quot;black&quot;, &quot;red&quot;, &quot;brown&quot;), pch=c(2,1,3), cex=0.8) abline(intrcpt.weekly, slope, col=&quot;black&quot;, lty=2) abline(intrcpt.monthly, slope, col=&quot;red&quot;, lty=2) abline(intrcpt.yearly, slope, col=&quot;brown&quot;, lty=2) Figure 6.18: Dummy Variable (Timeline) We leave readers to experiment with the interaction of gender and timeline. 6.6.4 Model Selection Model selection, in the context of Linear Regression, is about determining relevant independent variables. There are complementary techniques such as regularization and primary component analysis (PCA) that allow the determination of relevant independent variables. Such techniques are discussed in Chapter 9 (Computational Learning I) for Feature Selection as part of Feature Engineering. Here, we discuss one technique not so much in evaluating independent variables but by modeling a combination of additive and interactive relationships of covariates. Then, we evaluate the resulting models and determine which model may be deemed the best (correctly specified) based on using AIC and BIC. Aikike Information Criterion (AIC) In F-Test with Two-Way Anova, we perform significance of difference analysis using dataset mtcars. Here, we use the same dataset and perform model selection using a step-wise greedy iterative leave-one-out cross-validation (LOOCV) approach with the following equation: \\[\\begin{align} LOOCV(formula_{\\{inititial\\}}) = \\underset{aic}{\\mathrm{argmin}}\\ AIC(formula_{\\{inititial\\}}). \\end{align}\\] AIC is computed based on the following equation: \\[\\begin{align} AIC = -2\\cdot \\log_e\\ \\mathcal{L} + p \\cdot 2 = n\\cdot \\log_e\\ \\left(\\frac{RSS}{n}\\right) + p\\cdot k \\ \\ \\ \\ \\ \\ where\\ k = 2 \\end{align}\\] and where: p is the number of \\(\\beta\\) coefficients \\(\\mathbf{\\log_e\\mathcal{L}}\\) is the log likelihood Note that Likehood is discussed in much depth in Chapter 7 (Bayesian Computation I) under Likelihood Subsection under Bayes Theorem Section. . Here is a sample implementation of AIC using a built-in function called step(). initial.model = lm(mpg ~ disp + hp + drat + wt + qsec , data = mtcars) selected.model = step(initial.model, direction = &quot;both&quot;, k=2, trace = TRUE) ## Start: AIC=65.47 ## mpg ~ disp + hp + drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - disp 1 4.0 174 64.2 ## &lt;none&gt; 170 65.5 ## - hp 1 11.9 182 65.6 ## - qsec 1 12.7 183 65.8 ## - drat 1 15.5 186 66.3 ## - wt 1 81.4 252 76.0 ## ## Step: AIC=64.21 ## mpg ~ hp + drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - hp 1 9.4 184 63.9 ## - qsec 1 9.6 184 63.9 ## &lt;none&gt; 174 64.2 ## - drat 1 12.0 186 64.3 ## + disp 1 4.0 170 65.5 ## - wt 1 113.9 288 78.3 ## ## Step: AIC=63.89 ## mpg ~ drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 184 63.9 ## - drat 1 11.9 195 63.9 ## + hp 1 9.4 174 64.2 ## + disp 1 1.5 182 65.6 ## - qsec 1 85.7 269 74.2 ## - wt 1 275.7 459 91.2 First of all, tracing is enabled so that we see each step that AIC takes. The idea is to minimize the CV value using the LOOCV approach. In step 1, we see an initial AIC score of 65.47 based on the following initial formula: mpg ~ disp + hp + drat + wt + qsec. From there, it tries to evaluate all possible combination based on the initial formula. Note that the entry with &lt;none&gt; is the entry with the current formula: mpg ~ disp + hp + drat + wt + qsec. It first tries to remove the variable disp given the notation (- disp) and then computes for RSS and AIC. It finds that if it drops the disp variable from the formula, ending up with only this: mpg ~ hp + drat + wt + qsec, then the AIC score is 64.2. This score is better (smaller) than the current score of 65.47. Then it continues to try the next combination by dropping the hp variable only. This generates the score of 65.6 with the following formula: mpg ~ disp + drat + wt + qsec. It finds that the score is higher than the current. AIC performs further validations and tries dropping other variables one at a time. After which, it determines that the best combination with the lowest score at 64.2 is one in which it has to drop the disp variable to get the following final formula for the next step: mpg ~ hp + drat + wt + qsec. Therefore, in step 2, we have a score of 64.2 with the formula: *mpg ~ hp + drat + wt + qsec**. Then, the process starts again with the evaluation until it finds a score lower than the current one in step 2. AIC makes three steps, and each one is scored with an AIC score. It determines that after step 3, there is no other AIC score lower than 63.89 with the final formula: mpg ~ drat + wt + qsec. It stops the iteration. We now display the best model selected by AIC step. The best model selected is based on the lowest AIC score. In this case, the lowest score is 63.89 in step 3. selected.model ## ## Call: ## lm(formula = mpg ~ drat + wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) drat wt qsec ## 11.394 1.656 -4.398 0.946 To illustrate how we computed for the lowest AIC score and ultimately arrived at the best model, let us recall the Least-Squares equation: \\[\\begin{align} RSS(\\beta_{least}) = |y - X\\beta|^2 = \\sum_{i=1}^n (y_{i} - x_{i}\\beta)^2, \\end{align}\\] Our objective function is expressed as such: \\[\\begin{align} \\hat{\\beta}_{least} = \\underset{\\beta}{\\mathrm{argmin}}\\ RSS(\\beta_{least}). \\end{align}\\] Let us show two ways to generate the RSS and, ultimately, the AIC score for step 3. First, we can use the built-in function lm(.) like so (note that we use the formula obtained for the best model for demonstration): y.model = lm(mpg ~ drat + wt + qsec, data = mtcars) coef(y.model) ## (Intercept) drat wt qsec ## 11.3945 1.6561 -4.3978 0.9462 RSS = sum( resid(y.model)^2 ) c(&quot;RSS (using manual)&quot;=RSS, &quot;RSS (using deviance function)&quot;=deviance(y.model)) ## RSS (using manual) RSS (using deviance function) ## 183.5 183.5 Second, let us do it the manual way using matrix equation (or we can also use the equation above for least square): A = with(mtcars, cbind(1 , drat , wt, qsec)) y = mtcars$mpg beta.hat = solve(t(A) %*% A) %*% t(A) %*% y # matrix equation colnames(beta.hat) = c(&quot;coefficients&quot;) rownames(beta.hat) = c(&quot;intercept&quot;, &quot;drat&quot; , &quot;wt&quot;, &quot;qsec&quot;) t(beta.hat) ## intercept drat wt qsec ## coefficients 11.39 1.656 -4.398 0.9462 y.hat = ( beta.hat[1,] + beta.hat[2,] * mtcars$drat + beta.hat[3,] * mtcars$wt + beta.hat[4,] * mtcars$qsec ) # or y.hat = A %*% beta.hat (RSS = sum( (y - y.hat)^2 )) ## [1] 183.5 Finally, let us compute for the AIC score at step 3. This uses natural log and therefore we use base exp(1) = 2.7183: n = nrow(mtcars) p = length(beta.hat) # no of. coefficients (including intercept) k = 2 # used by BIC base = exp(1) # natural log (AIC = n * log( RSS / n, base ) + p * k) ## [1] 63.89 Another way to obtain AIC score is to use the built-in R function called AIC(.): AIC(y.model) ## [1] 156.7 This call to AIC() undergoes comprehensive cross-validation, and so the AIC score is based on the lowest score from this exhaustive validation. We can also compute the AIC score by hand like so (note, we can use the built-in function called logLik() to compute for log-likelihood): n = nrow(mtcars) p = length(beta.hat) # coefficients df = n - p se = sqrt( RSS / df) # standard error sd.hat = se * sqrt((n-p)/n) mu.hat = fitted(y.model) logLikelihood = sum(dnorm(mtcars$mpg, mean = mu.hat, sd = sd.hat, log=TRUE)) # or logLikelihood = as.numeric(logLik(y.model)) (AIC = -2 * as.numeric( logLikelihood ) + k * (p + 1)) ## [1] 156.7 Bayesian Information Criterion (BIC) BIC is similar to AIC in that it is also a step-wise iterative method. The difference is with the use of k. \\[\\begin{align} BIC = -2\\cdot \\log_e\\ \\mathcal{L} + p \\cdot ln(n) = n\\cdot \\log_e\\left(\\frac{RSS}{n}\\right) + p\\cdot k \\end{align}\\] where: k is \\(ln(n)\\), p is the number of \\(\\beta\\) coefficients, and \\(\\mathbf{\\log_e\\mathcal{L}}\\) is the log likelihood. Note that we discuss likelihood in-depth in Chapter 7 (Bayesian Computation I) under the Likelihood Sub-section. Here is a sample implementation of AIC using a built-in function called step(.). n = nrow(mtcars) selected.model = step(initial.model, direction = &quot;both&quot;, k=log(n), trace = TRUE) ## Start: AIC=74.26 ## mpg ~ disp + hp + drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - disp 1 4.0 174 71.5 ## - hp 1 11.9 182 73.0 ## - qsec 1 12.7 183 73.1 ## - drat 1 15.5 186 73.6 ## &lt;none&gt; 170 74.3 ## - wt 1 81.4 252 83.3 ## ## Step: AIC=71.53 ## mpg ~ hp + drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - hp 1 9.4 184 69.8 ## - qsec 1 9.6 184 69.8 ## - drat 1 12.0 186 70.2 ## &lt;none&gt; 174 71.5 ## + disp 1 4.0 170 74.3 ## - wt 1 113.9 288 84.2 ## ## Step: AIC=69.75 ## mpg ~ drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - drat 1 11.9 195 68.3 ## &lt;none&gt; 184 69.8 ## + hp 1 9.4 174 71.5 ## + disp 1 1.5 182 73.0 ## - qsec 1 85.7 269 78.6 ## - wt 1 275.7 459 95.6 ## ## Step: AIC=68.31 ## mpg ~ wt + qsec ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 195 68.3 ## + drat 1 12 184 69.8 ## + hp 1 9 186 70.2 ## + disp 1 0 195 71.8 ## - qsec 1 83 278 76.1 ## - wt 1 733 929 114.7 Notice that BIC, in this particular case only, iterates one more step, the fourth step, which has an AIC score of 68.31. It is, however, higher than the score of 61.89 we obtained previously. We now display the best model selected by BIC step. The best model selected is based on the lowest BIC score. In this case, the lowest score is 68.31 in step 4. selected.model ## ## Call: ## lm(formula = mpg ~ wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) wt qsec ## 19.746 -5.048 0.929 Let us compute for RSS using the model’s selected formula: y.model = lm(mpg ~ wt + qsec, data = mtcars) coef(y.model) ## (Intercept) wt qsec ## 19.7462 -5.0480 0.9292 (RSS = sum( resid(y.model)^2 )) ## [1] 195.5 Then we compute for the AIC score in step 4. This uses natural log and therefore we use base exp(1) = 2.7183: n = nrow(mtcars) base = exp(1) # natural log p = length(coef(y.model)) # no. of coefficients (including intercept) k = log(n, base) # used by BIC (AIC = n * log( RSS / n, base ) + p * k) ## [1] 68.31 Similarly, we can obtain BIC score using the built-in R function called BIC(.): BIC(y.model) ## [1] 162.6 In terms of the step-wise iteration method itself, there are three ways to step through model selection: forward step, backward step, or both. We require an initial model as a starting point for the iteration. The initial model, in part, dictates how many steps to take - of course, if the best model is closer from the beginning, then a forward step is preferable and much faster. However, that is if we have some sense of the estimated step. Below is a forward step direction used starting from the intercept and forward up to the final formula provided: ~disp + drat + wt + qsec: initial.model = lm(mpg ~ 1 , data = mtcars) selected.model = step(initial.model, scope= ~ disp + drat + wt + qsec, direction = &quot;forward&quot;, k=2, trace = TRUE) ## Start: AIC=115.9 ## mpg ~ 1 ## ## Df Sum of Sq RSS AIC ## + wt 1 848 278 73.2 ## + disp 1 809 317 77.4 ## + drat 1 522 604 98.0 ## + qsec 1 197 929 111.8 ## &lt;none&gt; 1126 115.9 ## ## Step: AIC=73.22 ## mpg ~ wt ## ## Df Sum of Sq RSS AIC ## + qsec 1 82.9 196 63.9 ## + disp 1 31.6 247 71.4 ## &lt;none&gt; 278 73.2 ## + drat 1 9.1 269 74.2 ## ## Step: AIC=63.91 ## mpg ~ wt + qsec ## ## Df Sum of Sq RSS AIC ## + drat 1 11.9 184 63.9 ## &lt;none&gt; 196 63.9 ## + disp 1 0.0 196 65.9 ## ## Step: AIC=63.89 ## mpg ~ wt + qsec + drat ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 184 63.9 ## + disp 1 1.51 182 65.6 Below is a backward step direction used starting from the provided formula, ~disp + drat + wt + qsec, and then steps back down to a formula with only the intercept. Notice that it does not have to reach the intercept because, after two steps, it has found a model with the lowest AIC score. initial.model = lm(mpg ~ disp + drat + wt + qsec , data = mtcars) selected.model = step(initial.model, scope= ~ 1, direction = &quot;backward&quot;, k=2, trace = TRUE) ## Start: AIC=65.63 ## mpg ~ disp + drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## - disp 1 1.5 184 63.9 ## &lt;none&gt; 182 65.6 ## - drat 1 13.4 196 65.9 ## - qsec 1 61.7 244 73.0 ## - wt 1 109.3 291 78.7 ## ## Step: AIC=63.89 ## mpg ~ drat + wt + qsec ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 184 63.9 ## - drat 1 11.9 195 63.9 ## - qsec 1 85.7 269 74.2 ## - wt 1 275.7 459 91.2 In a case where we want to have a more comprehensive iteration for the best model, we can set the trace to false and get the final result like so: initial.model = lm(mpg ~ ., data = mtcars) ( selected.model = step(initial.model, scope= ~ 1, direction = &quot;backward&quot;, k=2, trace = FALSE) ) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = mtcars) ## ## Coefficients: ## (Intercept) wt qsec am ## 9.62 -3.92 1.23 2.94 anova(selected.model) ## Analysis of Variance Table ## ## Response: mpg ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## wt 1 848 848 140.21 2e-12 *** ## qsec 1 83 83 13.70 0.00093 *** ## am 1 26 26 4.33 0.04672 * ## Residuals 28 169 6 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We can use the following built-in function called extractAIC(.) to derive the lowest AIC score immediately for an exhaustive search. extractAIC(selected.model, k=log(n)) ## [1] 4.00 67.17 Additionally, we also can plot the search path taken by AIC. See Figure 6.19. n = nrow(mtcars) selected.model = step(initial.model, direction = &quot;both&quot;, k=log(n), trace = FALSE) plot(selected.model$anova$AIC, ylim=range(65,88), xlab=&quot;steps&quot;, ylab=&quot;AIC score&quot;, main=&quot;Model Selection (Using LOOCV-BIC)&quot;) lines(selected.model$anova$AIC, col=&quot;red&quot;, lty=2) text(selected.model$anova$AIC -2, labels=round(selected.model$anova$AIC,1)) grid() Figure 6.19: Model Selection (Using LOOCV-BIC) As we can see now, model selection is also about coefficient selection. The independent variables with coefficients that can contribute to a low AIC score get to be included in the model. In effect, this is also about variable selection. We continue this discussion in Chapter 9 (Computational Learning I) under Feature Selection Section. 6.7 Regression Analysis Apart from handling model selection, it helps to also analyze the quality of regression, especially around the selected model and its goodness of fit. This section discusses the distribution property of residuals and their relationship with the fitted outcome. 6.7.1 Assumptions In statistics, we need to be able to measure statistics and understand assumptions and the violations against these assumptions. We look at the following: Correlation Homoscedasticity vs. Heteroscedasticity Normality and Leverage Collinearity 6.7.2 Correlation Coefficients We introduce three common correlation coefficients in this section that deal with measuring the strength of the relationship or association between two quantities. Pearson’s Rank Correlation Coefficients measures the strength of the relationship between two continuous variables. The measure is expressed as such: \\[\\begin{align} r_{xy} = \\frac{S_{R_xR_y}}{\\sqrt{S_{R_x} S_{R_y}}} = \\frac{\\sum(R_x - \\bar{R}_x)(R_y-\\bar{R}_y)}{\\sqrt{\\sum(R_x-\\bar{R}_x)^2\\sum(R_y - \\bar{R}_y)^2}} \\end{align}\\] where: \\(\\mathbf{R_x}\\) is x’s rank \\(\\mathbf{R_y}\\) is y’s rank and where: \\[\\begin{align} S_{R_xR_y} {}&amp;= n \\sum_{i=1}^n R_{x_i} R_{y_i} - \\sum_{i=1}^n R_{x_i} \\sum_{i=1}^n R_{y_i} \\\\ S_{R_x} &amp;= n\\sum_{i=1}^n R_{x_i}^2 - (\\sum_{i=1}^n R_{x_i})^2 \\\\ S_{R_y} &amp;= n \\sum_{i=1}^n R_{y_i}^2 -(\\sum_{i=1}^n R_{y_i})^2 \\end{align}\\] Spearman’s Rank Correlation Coefficients measures the strength of the monotonic relationship between two continuous variables. A monotonic relationship means that as one variable increases continuously, the other variable also increases or decreases. The variable cannot increase along the way and then decreases. It also cannot decrease along the way then increases. \\[\\begin{align} r_s = 1 - \\frac{6\\left( \\sum_{i=1}^n D_{i=1}^2\\right)}{n(n^2 - 1)} \\end{align}\\] where: D is the difference in rank within pairs Kendall’s Rank Correlation measures the strength of the ordinal relationship between two measured variables. It is also called Kendall Tau (\\(\\tau\\)) Rank Correlation in which tau (\\(\\tau\\)) is the coefficient. \\[\\begin{align} \\tau = \\frac{n_c - n_d}{\\frac{1}{2}n(n - 1)} \\end{align}\\] where: \\(\\mathbf{n_c}\\) - number of concordant pairs \\(\\mathbf{n_d}\\) - number of discordant pairs Note that a concordant pair is a pair of observations in which a subject is higher on both pairs or lower on both pairs, e.g. ( S &gt; X and S &gt; Y ) or ( S &lt; X and S &lt; Y ). On the other hand, a discordant pair is a pair of observations in which a subject is higher on one observation of a pair and lower on the other, e.g. ( S &gt; X and S &lt; Y ) or ( S &lt; X and S &gt; Y ). We leave readers to investigate two other types of the tau (\\(\\tau\\)) coefficients: Tau-a and Tau-b. To illustrate, suppose we have the following dataset (See Table 6.14): Table 6.14: Correlation Coefficient Test Category Sample A Sample B Rank A Rank B Diff in Rank (R) Red 4 5 2 2 0 (G) Green 2 3 3 4 -1 (B) Blue 1 4 4 3 1 (O) Orange 7 6 1 1 0 The highest score gets the top rank as one. We can use a built-in function called cor(.) to compute for the Pearson, Spearman, and Kendall correlation coefficients: RankA = c(2, 3, 4, 1) RankB = c(2, 4, 3, 1) pearson = cor(RankA, RankB, method=&quot;pearson&quot;, use=&quot;all.obs&quot;) spearman = cor(RankA, RankB, method=&quot;spearman&quot;, use=&quot;all.obs&quot;) kendall = cor(RankA, RankB, method=&quot;kendall&quot;, use=&quot;all.obs&quot;) round( c(&quot;pearson&quot;=pearson, &quot;spearman&quot;=spearman, &quot;kendall&quot;=kendall), 2) ## pearson spearman kendall ## 0.80 0.80 0.67 We also can use another built-in function called cor.test(.): cor.test(RankA, RankB, method = &quot;pearson&quot;, alternative = &quot;greater&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: RankA and RankB ## t = 1.9, df = 2, p-value = 0.1 ## alternative hypothesis: true correlation is greater than 0 ## 95 percent confidence interval: ## -0.4977 1.0000 ## sample estimates: ## cor ## 0.8 cor.test(RankA, RankB, method = &quot;spearman&quot;, alternative = &quot;greater&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: RankA and RankB ## S = 2, p-value = 0.2 ## alternative hypothesis: true rho is greater than 0 ## sample estimates: ## rho ## 0.8 cor.test(RankA, RankB, method = &quot;kendall&quot;, alternative = &quot;greater&quot;) ## ## Kendall&#39;s rank correlation tau ## ## data: RankA and RankB ## T = 5, p-value = 0.2 ## alternative hypothesis: true tau is greater than 0 ## sample estimates: ## tau ## 0.6667 Note that a coefficient closer to 1 or -1 shows a very strong (positive or negative) correlation. A value of zero means no correlation at all. Let us perform manual computation: pairs = c(&quot;(R,G)&quot;, &quot;(R,B)&quot;, &quot;(R, O)&quot;, &quot;(G,B)&quot;, &quot;(G,O)&quot;, &quot;(B,O)&quot;) RankA = c(&quot;2&lt;3&quot;, &quot;2&lt;4&quot;, &quot;2&gt;1&quot;, &quot;3&lt;4&quot;, &quot;3&gt;1&quot;, &quot;4&gt;1&quot;) RankB = c(&quot;2&lt;4&quot;, &quot;2&lt;3&quot;, &quot;2&gt;1&quot;, &quot;4&gt;3&quot;, &quot;4&gt;1&quot;, &quot;3&gt;1&quot;) concord = c(&quot;T&quot;, &quot;T&quot;, &quot;T&quot;, &quot;&quot;, &quot;T&quot;, &quot;T&quot;) discord = c(&quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;T&quot;, &quot;&quot;, &quot;&quot;) X = rbind(pairs, RankA) X = rbind(X,RankB) X = rbind(X, concord) X = rbind(X, discord) X ## [,1] [,2] [,3] [,4] [,5] [,6] ## pairs &quot;(R,G)&quot; &quot;(R,B)&quot; &quot;(R, O)&quot; &quot;(G,B)&quot; &quot;(G,O)&quot; &quot;(B,O)&quot; ## RankA &quot;2&lt;3&quot; &quot;2&lt;4&quot; &quot;2&gt;1&quot; &quot;3&lt;4&quot; &quot;3&gt;1&quot; &quot;4&gt;1&quot; ## RankB &quot;2&lt;4&quot; &quot;2&lt;3&quot; &quot;2&gt;1&quot; &quot;4&gt;3&quot; &quot;4&gt;1&quot; &quot;3&gt;1&quot; ## concord &quot;T&quot; &quot;T&quot; &quot;T&quot; &quot;&quot; &quot;T&quot; &quot;T&quot; ## discord &quot;&quot; &quot;&quot; &quot;&quot; &quot;T&quot; &quot;&quot; &quot;&quot; We have four observations ( n = 4) Our Pearson rank coefficient is computed like so: \\[\\begin{align*} Sxy {}&amp;= 4(2\\times2 + 3\\times4 + 4\\times3 + 1\\times1) - (2+3+4+1)(2+4+3+1) \\\\ &amp;= 4\\times29 - 10\\times 10\\\\ &amp;= 16\\\\ \\\\ Sx &amp;= \\sqrt{4\\times (2^2+3^2+4^2+1^2) - (2+3+4+1)^2} = \\sqrt{20} \\\\ \\\\ Sy &amp;= \\sqrt{4\\times (2^2+4^2+3^2+1^2) - (2+4+3+1)^2} = \\sqrt{20} \\end{align*}\\] \\[\\begin{align} r_{xy} = \\frac{S_{xy}}{S_x S_y} = \\frac{16}{\\sqrt{20} \\times \\sqrt{20}} = 0.80 \\end{align}\\] Our Spearman rank coefficient is computed like so: \\[\\begin{align} r_s = 1 - \\frac{6\\left( \\sum_{i=1}^n D_{i=1}^2\\right)}{n(n^2 - 1)} = 1 - \\frac{6 (0^2 + (-1)^2+1^2+0^2)}{4(4^2-1)} = \\frac{6 \\times 2}{4\\times 15} = 1 - 0.20 = 0.80 \\end{align}\\] Our Kendall tau (\\(\\tau\\)) coefficient is computed like so: \\[\\begin{align} \\tau = \\frac{n_c - n_d}{\\frac{1}{2}n(n - 1)} = \\frac{5-1}{\\frac{1}{2} 4(4 - 1)} = \\frac{4}{6} = 0.67 \\end{align}\\] All three correlation coefficients obtained indicate a positive correlation. 6.7.3 Homoscedasticity and Heteroscedasticity Data comes in many shapes and forms. Ideally, the distribution of data follows constant variance and normal distribution. Here, there are two shapes that we need to consider; but it is essential to emphasize that we are analyzing the type of distribution of residuals (also known as errors, noise, or perturbation) in the data instead of the type of distribution of the data itself. In this section, we start with the concept of Scedasticity, which refers to the distribution of residuals. There are two types of residual distributions: Homoscedasticity refers to the uneven spread of residuals. It shows a constant distribution of residuals, following a normal distribution. We create a sample data set with random noise - the residual. beta0 = 0.5 beta1 = 1.2 x = runif(n=500, min=0, max=1) random_noise = rnorm( 500, mean=0, sd=1) # Simulating homoscedastic expected_y = beta0 + beta1 * x + random_noise * x Notice that the residual points follow a uniform-like or box-like shape. Let us plot a case where the residual follows a homoscedastic distribution. See Figure 6.20. y.model = lm(expected_y - I(random_noise * x) + random_noise ~ x) plot( fitted(y.model), resid(y.model), lwd=1, cex=1, ylim=range(-3,3), main=&quot;Homoscedasticity&quot;, col=&quot;black&quot;, ylab=&quot;Residual&quot;, xlab=&quot;Fitted&quot; ) abline(h=0, lwd=2, col=&quot;red&quot;) Figure 6.20: Constant Variance Heteroscedasticity refers to the uneven spread of residuals. It does not show a constant distribution of residuals and therefore does not follow a normal distribution. Let us plot a case where the residual follows a heteroscedastic distribution. See Figure 6.21. y.model = lm(expected_y ~ x) plot( fitted(y.model), resid(y.model), lwd=1, cex=1, ylim=range(-3,3), main=&quot;Heteroscedasticity&quot;, col=&quot;black&quot;, ylab=&quot;Residual&quot;, xlab=&quot;Fitted&quot; ) abline(h=0, lwd=2, col=&quot;red&quot;) Figure 6.21: Uneven Variance Notice that the residual points follow a funnel-like shape. Other shapes could show as a polynomial-like shape. If the shape does not follow a box-like shape or the red line curves around a non-normal distribution, the residual then follows a heteroscedastic distribution. Additionally, if a residual distribution is not centered along the y-axis, it may also be interpreted as being heteroscedastic. 6.7.4 Normality and Leverage Normality is a measure of how data follows a normal distribution. We discuss the Normal Q-Q plot in a later section under Diagnostic Plots to have a visual view of Normality. However, we can also use the Shapiro-Wilk test, which we introduced in Tukey’s test or the Kolmogorov-Smirnov test to measure Normality. Here is a sample of the Shapiro-Wilk test. aov.model = aov(mpg ~ disp + wt, data = mtcars) # One-Way Anova shapiro.test(residuals(aov.model)) ## ## Shapiro-Wilk normality test ## ## data: residuals(aov.model) ## W = 0.89, p-value = 0.004 A W value closer to 1 indicates that our data set adequately follows a normal distribution; otherwise, our data set departs from the distribution. To test how much data departs from the normal distribution, we can also use the Anderson-Darling test. library(nortest) x = rnorm(n=20, mean=0, sd=1) ad.test(x) ## ## Anderson-Darling normality test ## ## data: x ## A = 0.35, p-value = 0.4 An A value is the test statistic and measures how much our data departs from a normal distribution. Leverage measures the distance between each observation to the mean of all observations. This measure of the distance of each observation is also known as the hat-value of the observation and can be used to identify outliers. We discuss outliers further in terms of Cook’s distance. Leverage is computed using the following equation: \\[\\begin{align} h_i = \\frac{1}{n} + \\frac{(x_i - \\bar{x})^2}{SS_E}\\ \\ \\ \\ \\ where\\ \\ \\ \\ SS_E = \\sum_{j=1}^n(x_j - \\bar{x})^2 \\end{align}\\] To illustrate, let us prepare our simple dataset and generate a simple linear regression model like so: set.seed(2020) sample_size = 20 range1 = seq(0, 1, length.out=200) x = sample(range1, size=sample_size, replace=TRUE) e = rnorm(sample_size, mean=0, sd = 1) # residual y = sort( rnorm(sample_size, mean=0, sd = 1) + e ) model = lm( y ~ x) Now, let us compute for the leverage (or hatvalue). The following R implementation illustrates a calculation for simple linear regression only. my.hatvalues &lt;- function(x) { n = length(x) x.mean = mean(x) hat.values = c() sse = sum( (x - x.mean)^2 ) for (i in 1:n) { hat.values[i] = 1 / n + (x[i] - x.mean)^2 / sse } names(hat.values) = seq(1, n) hat.values } # let us display only the 1st five hatvalues head(my.hatvalues(x), n=5) ## 1 2 3 4 5 ## 0.07113 0.05592 0.06421 0.05001 0.13954 Let us validate by using a built-in R function called hatvalues() and influence(): head(hatvalues(model), 5) ## 1 2 3 4 5 ## 0.07113 0.05592 0.06421 0.05001 0.13954 head(influence(model)$hat, 5) ## 1 2 3 4 5 ## 0.07113 0.05592 0.06421 0.05001 0.13954 For Multi-linear Regression, we use the following Matrix equation from Chapter 3 (Numerical Linear Algebra II): \\[\\begin{align} Recall: \\hat{\\beta} = (A^T \\cdot A)^{-1} \\cdot A^T \\cdot y\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ y = A\\hat{\\beta} \\label{eqn:eqnnumber3a} \\end{align}\\] Therefore, we got: \\[\\begin{align} H = A \\cdot (A^T \\cdot A)^{-1} \\cdot A^T = A \\beta y^{-1} \\end{align}\\] We leave readers to investigate using Leverage in a multi-linear regression. 6.7.5 Collinearity Collinearity is a measure of how much independent variables are correlated. We discuss this topic in more detail in Chapter 9 (Computational Learning I). 6.7.6 Dispersion Dispersion describes the variation of data distribution. A Normal distribution tends to have an observed constant variance (\\(\\sigma^2\\)) all through the mean (\\(\\mu\\)) of linear regression. A Poisson distribution tends to have an observed variance equal to the mean. As the mean increases or decreases, so does the variance. On the other hand, a distribution with an observed variance that is not constant but is higher than the expected variance indicates signs of Overdispersion; otherwise, it shows signs of Underdispersion. Dispersion is measured as a ratio of deviance to the degrees of freedom (df), so that if the ratio is higher than 1 (e.g., ratio&gt;1:1), then it indicates some overdispersion. One common way to handle Overdispersion is by using Quasi-Distribution or Quasi-Likelihood. An example is using Quasi-Binomial distribution and Quasi-Poisson distribution in Generalized Linear Modeling (GLM). The intuition behind GLM is discussed in Chapter 10 (Computational Learning II) under Logistic Regression Subsection under Regression Section. A Quasi-Distribution or a Quasi-Likelihood has a revised variant of its PDF equation. For example, the PDF of a Binomial distribution has the following equation: \\[\\begin{align} P(X = x| n, \\rho) = \\binom{n}{x} \\rho^x(1 - \\rho)^{n - x} \\end{align}\\] A Quasi-Binomial has the following PDF equation: \\[\\begin{align} P(X = x| n, \\rho) = \\binom{n}{x} \\rho(\\rho + k\\psi)^{x-1}(1 - \\rho - x\\psi)^{n -x} \\end{align}\\] The added symbol psi (\\(\\psi\\)) represents a parameter used to describe and explain the additional variance observed in the data distribution. Dispersion is further discussed in Chapter 10 (Computational Learning II) under Poisson Regression Subsection under Regression Section. 6.7.7 Diagnostic Plots There are six diagnostic plots presented here to help us discuss linear regression analysis: Residuals vs. Fitted plot - Measure non-linearity and exposes outliers. Normal Q-Q plot - Measure data distribution against normal distribution. Scale-Location plot - Measure variance (homoscedasticity vs. heteroscedasticity). Cook’s Distance plot - Measure of influence based on Cook’s distance formula. Residual vs. Leverage plot - Measure influence based on Residual variance. Cook’s Distance vs. Leverage plot - Standardized Residuals vs. Leverage. Let us generate a regression model for diagnostics. Here, we induce two outliers to explain leverage and Cook’s distance. See Figure 6.22. beta.0 = 1.0 beta.1 = 1.3 sample_size = n = 50 e = rnorm(n, mean=0, sd = 1) * 3 # residual x.true.fit = seq(1, n) # x baseline y.true.fit = beta.0 + beta.1 * x.true.fit # y baseline x.observed = x.true.fit + e # add noise to gen. predictor y.observed = y.true.fit + e # add noise to gen. response reg.model = lm(y.observed ~ x.observed) y.expected = fitted(reg.model) # fitted expectation # create outliers outlier.idx = c(49,50) outlier.val = c(130, 150) y.with.outliers = y.observed y.with.outliers[outlier.idx] = outlier.val outlier.model = lm(y.with.outliers ~ x.observed) y.outlier.expected = fitted(outlier.model) # fitted expectation ymax = max(y.true.fit) + 10 xmax = max(x.observed) plot(NULL, xlim=range(0, 50), ylim = range(0,160), xlab = &quot;indepdendent&quot;, ylab=&quot;dependent&quot;, main=&quot;Linear Regression&quot;) grid() x.i = seq(1, n) points(x.i, y.observed, col=&quot;black&quot;) lines(x.i, y.true.fit, col=&quot;red&quot;) abline(reg.model, col=&quot;navyblue&quot;, lty=2) abline(outlier.model, col=&quot;brown&quot;, lty=2) points(outlier.idx, outlier.val, col=&quot;navyblue&quot;, pch=16) legend(1, 150, c( &quot;true fit&quot;, &quot;expected fit&quot;, &quot;fit with outliers&quot; ), col=c(&quot;red&quot;, &quot;navyblue&quot;, &quot;brown&quot;), horiz=FALSE, cex=0.8, lty=c(1,2,2)) Figure 6.22: Linear Regression (Outlier) Notice that with the existence of two outliers, the regression line is pulled slightly towards the outliers, indicating influence. Residuals vs Fitted plot For this diagnostic plot, see Figure 6.23. par(mfrow = c(1, 2)) plot(reg.model, which=1) plot(outlier.model, which=1) Figure 6.23: Linear Regression Analysis # we also can use the plot and abline to produce the same plots # plot( fitted(reg.model2), resid(reg.model2), ...) # abline(h = 0, lwd=2, col=&quot;red&quot;) There are two plots in the figure: one without outliers and one with outliers. As can be seen, the plot can give us the location of outliers. Because we only deal with a small number of residuals, linearity is not entirely visible. Linearity is more visible, however, for residuals with extreme outliers, as shown in the second plot. Normal Q-Q plot For this diagnostic plot, see Figure 6.24. par(mfrow = c(1, 2)) plot(reg.model, which=2) plot(outlier.model, which=2) Figure 6.24: Linear Regression Analysis # we also can use qqnorm and qqline to produce the same plots # qqnorm( resid(reg.model), main = &quot;Normal Q-Q&quot;) # data points # qqline( resid(outlier.model), lwd = 2, col = &quot;red&quot;) A Normality test is when residuals all fall along a straight line, representing Normality. A typical good fit is when the plot shows a normal distribution of residuals forming along a straight line in analyzing our linear regression. Residuals tend to crowd at the center and spread out. Extreme residuals dissipate as they spread out, but they still fall closer along the straight line while being extreme. Residuals that are outliers, however, tend to fall off the straight line, as seen in the second plot. Scale-Location plot For this diagnostic plot, see Figure 6.25. par(mfrow = c(1, 2)) plot(reg.model, which=3) plot(outlier.model, which=3) Figure 6.25: Linear Regression Analysis In the previous section, we discuss homoscedasticity - a residual that follows a constant normal distribution. While we have a smaller number of residuals, homoscedasticity may not be as pronounced as when we deal with a larger number of residuals; though, still, the first plot shows the red line passing through an even constant distribution of the residuals. However, the second plot is bent - but only because of the influence of the outliers. Still, the residuals (except for the outliers) are evenly dispersed across the red bent line. Cook’s Distance plot For this diagnostic plot, see Figure 6.26. par(mfrow = c(1, 2)) plot(reg.model, which=4) plot(outlier.model, which=4) Figure 6.26: Linear Regression Analysis In the second plot, we can see that the outliers are more visible than the other residuals. On the other hand, the other residuals become more refined because of the outliers. If we remove the outliers, we see a more coarse plot similar to the first plot. Let us discuss Cook’s distance to show how we evaluate and remove the outliers. The formula for Cook’s distance is as follows: \\[\\begin{align} D_i^{(cook&#39;s\\ distance)} = H_i^{(leverage)} \\times R_i^{2(studentized\\ residual)} \\times \\frac{1}{k} \\end{align}\\] where: k is the number of coefficients (including the interface) H is the leverage: \\[\\begin{align} H_i^{(leverage)} = \\left(\\frac{h_i}{ 1 - h_i}\\right) \\end{align}\\] R is the Studentized Residual expressed as: \\[\\begin{align} R_i = \\frac{ e_i }{ SE_i^{(standard\\ error)}} \\end{align}\\] SE is the estimated Standard deviation or Standard Error expressed as: \\[\\begin{align} SE_i^{(standard\\ error)} = \\sqrt{MSE \\times ( 1 - h_i)} \\approx \\sqrt{\\sigma^2 \\times ( 1 - h_i)} \\end{align}\\] and MSE is the mean squared error which we use in place of \\(\\sigma^2\\) which is unknown: \\[\\begin{align} MSE = \\frac{\\sum_{i=1}^n (y_i - \\hat{y}_i)^2}{(n - k)} \\end{align}\\] We can use a built-in function called cooks.distance() to compute for the Cook’s distance. We then apply a threshold for eliminating outliers. Here, we use the following cut-off: \\[\\begin{align} cutoff = \\frac{4}{n} \\end{align}\\] cutoff = 4 / length(y.with.outliers) cooks.dist = cooks.distance(reg.model) outlier.idx = which(ifelse( cooks.dist &lt; cutoff, FALSE, TRUE)) cooks = c(&quot;Cooks.Dist&quot; = cooks.dist[outlier.idx], &quot;Cut-Off&quot; = cutoff, &quot;Outlier Index&quot; = outlier.idx, &quot;Outlier&quot;= y.with.outliers[outlier.idx] ) round(cooks,2) ## Cooks.Dist.14 Cut-Off Outlier Index.14 Outlier ## 0.08 0.08 14.00 28.80 Here is a naive implementation of Cook’s Distance in R code: cooks &lt;- function(model ) { hi = hatvalues(model) e.resid = resid(model) n = length(length(e.resid)) # k = length( coef(model)) # no. coeffs including intercept e.resid.df = df.residual(model) # (n - k) leverage = hi / ( 1 - hi) e2 = (e.resid)^2 mse = sum((e.resid)^2) / e.resid.df # estimate standard error s.e = sqrt( mse * (1 - hi)) # studentized residual (standard residual) r.s = e.resid / s.e # also can use rstandard(model) cooks.dist = leverage * ( r.s^2) * (1/k) cooks.dist } cooks.dist = cooks(reg.model ) outlier.idx = which(ifelse( cooks.dist &lt; cutoff, FALSE, TRUE)) cooks = c(&quot;Cooks.Dist&quot; = cooks.dist[outlier.idx], &quot;Cut-Off&quot; = cutoff, &quot;Outlier Index&quot; = outlier.idx, &quot;Outlier&quot;= y.with.outliers[outlier.idx] ) round(cooks,2) ## Cooks.Dist.14 Cut-Off Outlier Index.14 Outlier ## 0.08 0.08 14.00 28.80 Residual vs Leverage plot For this diagnostic plot, see Figure 6.27. par(mfrow = c(1, 2)) plot(reg.model, which=5) plot(outlier.model, which=5) Figure 6.27: Linear Regression Analysis We measure leverage in terms of how far each data is from the mean. We know that the mean is where most typical data crowds, and therefore, any data farther away from the mean are candidate leverage points that may have influence. Most - if not all - outliers can influence our model and thus are candidates to be removed. However, there may also be data that are not outliers yet has some influence on the model. The second plot shows a few outliers. We evaluate the lower and upper right corners outside the dotted lines for this plot. For example, there is one outlier outside the upper right boundary with a threshold of 1. In addition, we see another outlier outside the 0.5 boundary. They indicate high leverage with the potential to influence. Cook’s Distance vs Leverage plot For this diagnostic plot, see Figure 6.28. par(mfrow = c(1, 2)) plot(reg.model, which=6) plot(outlier.model, which=6) Figure 6.28: Linear Regression Analysis The dotted lines are numbered from zero to a number (right to left) that could reach as high as 6 in the case of the second plot. These dotted lines are boundaries - the higher a data point reaches a boundary, the more highly influential it is. Any residual that gets promoted to the status of being a leverage point has the potential to influence. For example, the outlier at index 50 has a Cook’s distance value between 5 and 6. Ideally, even at 1, it should already be considered highly influential. As the last topic in this section, one useful function is influence.measures(). We leave readers to experiment with the use of the function. Running the function tends to output a long list of results for a large number of observations. influence.measures(outlier.model, infl = influence(outlier.model)) 6.8 The Significance of Regression Recall in previous sections that we deal with significance of difference of groups. In our examples, our null hypothesis claims that there is no difference between groups, while our alternative hypothesis supports the claim that there is a difference between groups: \\[\\begin{align} H_0 : \\mu = \\mu_0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_1 : \\mu \\ne \\mu_0 \\end{align}\\] In this section, we focus on significance of regression in which we want to study the relationship, association, or effect of explanatory variables on response variables. We do this by examining the coefficients that are associated with the explanatory variables and then determining the significance based on a test of effect. Here, we are also required to formulate our own null hypothesis and alternative hypothesis using a linear equation starting with the following simple linear equation: \\[\\begin{align} y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{align}\\] where: \\(y_i\\) is the response (dependent) variable \\(B_0\\) and \\(B_1\\) are coefficients \\(x_i\\) is the explanatory (predictor or independent) variable \\(e_i\\) is the unexplained residual Our null hypothesis claims that our response variable does not depend on the predictor variable. \\[\\begin{align} H_0 {}&amp;: \\beta_1 = 0,\\ \\ \\ \\ \\ y_i = \\beta_0 + \\epsilon_i\\\\ \\nonumber \\\\ H_1 &amp;: \\beta_1 \\ne 0,\\ \\ \\ \\ \\ y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i \\end{align}\\] While we can use ANOVA and Sum of Squares to measure Variance, in Significance of Regression, it also helps consider the effect of predictor variables on response variables. What we want to do is to settle over the least amount of error. Therefore, we measure error based on the distance between two data points deviating from one another. In the following sections, we introduce a few tests of null hypothesis for linear regression, starting with simple linear regression. 6.8.1 Simple Linear Regression In Simple Linear Regression, we deal with one continuous independent variable denoted as \\(\\mathbf{x_i}\\) and one continuous dependent variable denoted as \\(\\hat{y}\\). Being linear, we try to find the best fit of a line into the data and determine if we can even fit a line - is there a relationship between the dependent and independent variables? We need to determine if the \\(H_0\\) supports the claim that the predicted outcome does not regress - is not close - to the actual value by any random chance (from an independent random variable). To illustrate, let us continue to use the mtcars dataset. However, this time, we intend to know if there is an effect of the displacement, disp, to fuel consumption, mpg. We use a built-in R function called lm(.) to generate a linear model; (simple.model = lm(mpg ~ disp, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Coefficients: ## (Intercept) disp ## 29.5999 -0.0412 Notice in the model that we have two coefficients: \\(\\beta_0\\) = 29.6 and \\(\\beta_1\\) = -0.041. Let us run a summary against the linear model using summary(.): summary(simple.model) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.892 -2.202 -0.963 1.627 7.231 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.59985 1.22972 24.07 &lt; 2e-16 *** ## disp -0.04122 0.00471 -8.75 9.4e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.25 on 30 degrees of freedom ## Multiple R-squared: 0.718, Adjusted R-squared: 0.709 ## F-statistic: 76.5 on 1 and 30 DF, p-value: 9.38e-10 Based on the summary, we can say that the displacement variable significantly affects fuel consumption. That is asserted because of the triple asterisk (***) for the coefficient of the variable disp. The code indicates a significant effect at \\(\\alpha=0.001\\). To determine if such a predictor variable regresses to a response variable, we may plot our dataset and determine if we can model the regression based on measuring the closeness of distance of each observed data point denoted as \\(y_{i}\\) to a fitted line denoted as \\(f(x_{i})\\). In other words, we should be able to see the linear relationship pattern between disp and mpg: plot(mpg ~ disp, data = mtcars, col=&quot;black&quot;, pch=16, main=&quot;Effect of Displacement to Fuel Consumption&quot;) abline(simple.model, col=&quot;red&quot;) # fit a line, f(xi) grid(col=&quot;lightgrey&quot;) arrows(200,23, 250,28, code=1, length=0.1) text(350, 28, label=&quot;fitted line, f(x), the linear model&quot;) Figure 6.29: Smoothing In the plot, we fit a line to show the linear regression. The \\(B_1\\) is negative which indicates a decreasing effect to the fuel consumption; meaning, for every unit increase of \\(x\\) (disp), there is a 0.041 unit decrease of \\(y\\) (mpg). We will see how to apply the formulas when we get into multilinear regression. Meanwhile, Figure 6.30 shows the relationships of the formulas: Figure 6.30: Statistics Note, based on Figure 6.30, that RSS compares the regression line against the horizontal line - the null hypothesis (H0). Similarly, R squared is compared the same. RSS (Residual Sum of Squares) That is the sum of squares of the difference between \\(\\hat{y}\\) and \\(y\\). At times, in other literature, it may read as SSR (Sum of Squares Residual) or SSE (Sum of Squares Error). \\[\\begin{align} RSS = \\sum_{i=1}^{n}(y_{i} - \\hat{y}_i)^2 \\end{align}\\] ESS (Explained Sum of Squares) That is the sum of squares of the difference between \\(\\hat{y}\\) and \\(\\bar{y}\\). We may read this as RSS (Regression Sum of Squares) in other literature. Alternatively, it may read as SSR (Sum of Squares Regression) or SSE (Sum of Squares Explained). \\[\\begin{align} ESS = \\sum_{i=1}^{n}(\\hat{y}_i - \\bar{y})^2 \\end{align}\\] TSS (Total Sum of Squares) That is the sum of squares of the difference between \\(y\\) and \\(\\bar{y}\\). We may sometimes read this as SST (Sum of Squares Total). \\[\\begin{align} TSS = \\sum_{i=1}^{n}(y_{i} - \\bar{y})^2 \\end{align}\\] The total sum of squares combines RSS (residual sum of squares) and ESS (explained sum of squares). \\[\\begin{align} TSS = RSS + ESS \\end{align}\\] We have the following implementation of the measures for the regression, for which we can demonstrate the usage in the next section: reg.summary &lt;- function(A, coefficients, y) { n = nrow(A) m = length(coefficients) y_hat = A %*% coefficients df = n - m # degrees of freedom rss = sum((y - y_hat)^2) # residual sum squared ess = sum((y_hat - mean(y))^2) # explained sum squared tss = sum((y - mean(y))^2) # total sum squared se = sqrt( rss / df ) # standard error r2 = ess/ tss # multiple r-squared adj.r2 = 1 - ((1-r2) * (n-1) / (n - m)) out = c(n, m, df, rss, ess, tss, se, r2, adj.r2) names(out) = c(&quot;n&quot;, &quot;m&quot;, &quot;df&quot;, &quot;RSS&quot;, &quot;ESS&quot;, &quot;TSS&quot;, &quot;SE&quot;, &quot;R-squared&quot;, &quot;Adj.R2&quot;) out } 6.8.2 Multilinear Regression In Multilinear Regression, we deal with more than one continuous independent variable, denoted as X, and one continuous dependent variable, denoted as \\(\\hat{y}\\), using the following multilinear equation: \\[\\begin{align} \\hat{y} = \\beta^T X \\end{align}\\] which can be expanded like so: \\[\\begin{align} y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}\\ + ... +\\ \\beta_n x_{n,i} + \\epsilon_i \\end{align}\\] where: \\(y_i\\) is the response (dependent) variable \\(B_0\\) … \\(B_n\\) are coefficients \\(x_1\\) … \\(X_n\\) are the explanatory (predictor or independent) variables \\(e_i\\) is the unexplained residual Our null hypothesis claims that our response variable does not depend on any predictor variable. Otherwise, our alternative hypothesis claims that our response variable depends on at least one predictor variable. \\[\\begin{align} H_0 {}&amp;: \\beta_1 = \\beta_2 = \\beta_3 =\\ ...\\ =\\ \\beta_n = 0,\\ \\ \\ \\ \\ y_i = \\beta_0 + \\epsilon_i\\\\ \\nonumber \\\\ H_1 &amp;: at\\ least\\ one\\ \\ \\beta_j \\ne 0,\\ \\ \\ \\ \\ y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}\\ + ... +\\ \\beta_n x_{n,i} + \\epsilon_i \\end{align}\\] We need to determine if the \\(H_0\\) supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from multiple independent random variables). To illustrate, we now use more than one predictor variable from the mtcars dataset and model a multilinear regression. We choose four continuous predictor variables: displacement (disp), horsepower (hp), weight (wt), and 1/4 mile time (qsec). str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... We generate our multilinear model: (multi.model = lm(mpg ~ disp + hp + wt + qsec, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) disp hp wt qsec ## 27.32964 0.00267 -0.01867 -4.60912 0.54416 and derive the following coefficients: \\(\\beta_0\\) = 27.3296, \\(\\beta_1\\) = 0.0027, \\(\\beta_2\\) = -0.0187, \\(\\beta_3\\) = -4.6091, \\(\\beta_4\\) = 0.5442 We can also use a built-in function called coef(): coef(multi.model) ## (Intercept) disp hp wt qsec ## 27.329638 0.002666 -0.018666 -4.609123 0.544160 Recall Polynomial Regression in Chapter 3 (Numerical Linear Algebra II). We use the same matrix manipulation to get the list of coefficients, where A is the matrix of equations (See also Equation \\(\\ref{eqn:eqnnumber3a}\\)). \\[\\begin{align*} \\hat{\\beta} \\approx (A^T \\cdot A)^{-1} \\cdot A^T \\cdot y \\end{align*}\\] For example: A = with(mtcars, cbind(1, disp, hp, wt, qsec)) y = mtcars$mpg beta.hat = solve(t(A) %*% A) %*% t(A) %*% y colnames(beta.hat) = c(&quot;coefficients&quot;) rownames(beta.hat) = c(&quot;intercept&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;wt&quot;, &quot;qsec&quot;) t(beta.hat) ## intercept disp hp wt qsec ## coefficients 27.33 0.002666 -0.01867 -4.609 0.5442 Note that there are other improvements to matrix manipulation we can numerically use to compute the coefficients. Please refer to Chapter 3 (Numerical Linear Algebra II). To get the approximate response outcome, \\(\\hat{y}\\), we use the following equation: \\[\\begin{align} \\hat{y} = A\\times\\hat{\\beta} \\\\ \\nonumber \\\\ y_hat = A %*% beta.hat \\end{align}\\] And to get the Standard Residual Error and R-squared, we use the following equations: \\[\\begin{align} SE = \\frac{RSS}{df}\\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ df = n - m, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ R^2 = \\frac{ESS}{TSS} \\end{align}\\] where: RSS is the residual sum of squares n is size of the sample m is the number of coefficients (including intercept) df is the degrees of freedom reg.out = round( reg.summary(A, beta.hat, y), 4) reg.out[c(&quot;n&quot;, &quot;m&quot;, &quot;df&quot;, &quot;RSS&quot;, &quot;ESS&quot;, &quot;TSS&quot;, &quot;SE&quot;)] ## n m df RSS ESS TSS SE ## 32.000 5.000 27.000 185.635 940.412 1126.047 2.622 reg.out[c(&quot;R-squared&quot;, &quot;Adj.R2&quot;)] ## R-squared Adj.R2 ## 0.8351 0.8107 Here is the summary: (summary.out = summary(multi.model)) ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.866 -1.582 -0.379 1.171 5.647 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.32964 8.63903 3.16 0.0038 ** ## disp 0.00267 0.01074 0.25 0.8058 ## hp -0.01867 0.01561 -1.20 0.2423 ## wt -4.60912 1.26585 -3.64 0.0011 ** ## qsec 0.54416 0.46649 1.17 0.2536 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.62 on 27 degrees of freedom ## Multiple R-squared: 0.835, Adjusted R-squared: 0.811 ## F-statistic: 34.2 on 4 and 27 DF, p-value: 3.31e-10 As shown, the predictor variable wt has a significant effect to mpg at \\(\\alpha=0.01\\). Also, notice that our R-squared is at \\(\\sigma\\) = 0.8351. Any value closer to one indicates a better fit. However, while we consider this coefficient of determinant to measure the goodness of fit, it becomes less reliable as the number of predictors increases. Let us calculate for the adjusted R-squared using the equation below (with the intent to have an adjusted \\(R^2\\) event at a high number of predictors - although, here, we are not going to perform that): \\[\\begin{align} adj.R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-m } \\end{align}\\] r2 = as.numeric( reg.out[c(&quot;R-squared&quot;)] ) n = as.numeric(reg.out[c(&quot;n&quot;)]) m = as.numeric(reg.out[c(&quot;m&quot;)]) ( adj.r2 = 1 - (1 - r2)*(n - 1)/(n - m) ) ## [1] 0.8107 6.8.3 Logistic Regression We review Logistic Regression in the context of Generalized Linear Model (GLM), emphasizing the use of a link function. Note that GLM and link function are covered in Chapter 9 (Computational Learning I). In Logistic Regression, we deal with independent variables denoted by \\(X\\) and a logit-transformed dependent variable that follows a binomial probability denoted by \\(\\hat{p}\\). We express the distribution like so: \\[y \\sim Bern(\\hat{p})\\] The intuition behind Logistic Regression starts with two functions: Link Function (Logit Function): Note that a Logit Function is the inverse of a Sigmoid Function (also known as Logistic Function). \\[\\begin{align} \\text{logit}(\\hat{p}) = \\log_e\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right) = z \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where z in }[-\\infty,\\infty] \\end{align}\\] Inverse Link Function (Sigmoid Function): \\[\\begin{align} \\text{logit}^{-1}(z) = \\hat{p} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where }\\hat{p}\\text{ in }[0,1] \\end{align}\\] and where: \\(z = \\beta^TX\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\beta_0 + \\sum_{i=1}^n\\beta_i x_i\\). The Inverse Link Function, also called Inverse Logit Function, can be expanded and be shown to be inversely related to Logit function like so: \\[\\begin{align} \\hat{p} = \\frac{\\text{exp}(z)}{1 + \\text{exp}(z)} = \\frac{1}{1 + \\text{exp}(-z)} = \\frac{1}{1 + \\text{exp}(-\\text{logit}(z))} \\end{align}\\] where: \\(\\hat{p}\\) is interpreted as the probability of observing a successful event. \\[\\begin{align} \\underbrace{\\hat{p} = P(y = 1| x)}_{\\text{successful event}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\hat{q} = 1 - \\hat{p} =P(y = 0| x) }_{\\text{failed event}} \\end{align}\\] The Logit Function, on the other hand, can be interpreted as the logarithm of the odds ratio of observing a successful event over observing a failed event. This function allows us to model regression linearly. \\[\\begin{align} \\text{logit}(\\hat{p}) = \\log_e\\left(\\frac{\\hat{p}}{1 - \\hat{p}}\\right) = \\log_e\\left(\\frac{\\hat{p}}{\\hat{q}}\\right) = \\beta^TX \\end{align}\\] Both functions can be implemented such that we have inverse.logit(.) and logit(.). ln &lt;- function(x) { log(x, exp(1))} # exp(1) = 2.718282 inverse.logit &lt;- function(z) { 1 / (1 + exp(-z)) } logit &lt;- function(p) { ln(p / (1 - p)) } Here, the base of our log is e = 2.7183. Alternatively, R comes with two functions, namely plogis(.) and qlogis(.) respectively, e.g.: c(&quot;inverse.logit&quot; = inverse.logit(8), &quot;plogis&quot; = plogis(8)) ## inverse.logit plogis ## 0.9997 0.9997 c(&quot;logit&quot; = logit(0.90), &quot;qlogis&quot; = qlogis(0.90)) ## logit qlogis ## 2.197 2.197 To visualize the logistic distribution using the plogis function, we show Figure 6.31. plot(NULL, xlim=range(-5,5), ylim=range(0,1), xlab=&quot;Z (Log-Odds)&quot;, ylab=&quot;plogis(z)&quot;, main=&quot;Inverse Logit (Log-Odds as Input)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) # Using plogis outcome z = x = seq(-5, 5, length.out=50) # z is log-odds. curve(plogis(x), col=&quot;navyblue&quot;, lwd=2, add=TRUE) Figure 6.31: Logistic Regression (CDF) To visualize the logistic distribution using qlogis function, we show Figure 6.32. The range is [\\(-\\infty, \\infty\\)]. plot(NULL, xlim=range(0,1), ylim=range(-5,5), xlab=&quot;P (Probability)&quot;, ylab=&quot;qlogis(p)&quot;, main=&quot;Logit (Probabilities as Input)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) # Using qlogis outcome p = x = seq(0, 1, length.out=150) # p is sequence of probabilities curve(qlogis(x), col=&quot;navyblue&quot;, lwd=2, add=TRUE) Figure 6.32: Logit - Quantile To complement our understanding of Logistic Regression, let us review the concept of Proportionality and the Odds of observing a successful event. For example, suppose we flip a coin 200 times, then calculate the proportion of our experiment landing on heads: set.seed(2020) range = seq(0,1) overall.outcome = 200 tosses = sample(range, size=overall.outcome, replace=TRUE) heads = length( which( tosses == 1) ) # no of times coin landed on head tails = length( which( tosses == 0) ) # no of times coin landed on tail c(&quot;head&quot; = heads / overall.outcome, &quot;tail&quot; = tails / overall.outcome) ## head tail ## 0.53 0.47 Flipping a coin 200 times shows that the proportion of landing on heads overall outcome is around 0.53 and that the proportion of landing on tails overall outcome is around 0.47. So the total proportionally should sum to 1. The odds of heads over tails is around 1.1277, and the odds of tails over heads is around 0.8868. p = heads / overall.outcome c(&quot;odds of heads&quot; = p / (1 - p), &quot;odds of tails&quot; = (1-p) / p) ## odds of heads odds of tails ## 1.1277 0.8868 The result can be equivalently achieved using a very simple odds ratio formula: c( &quot;odds of heads&quot; = heads/tails, &quot;odds of tails&quot; = tails / heads) ## odds of heads odds of tails ## 1.1277 0.8868 To be able to transform the result of our odds to a symmetrical range, e.g. [\\(-\\infty,\\infty\\)], we use log-odds. c( &quot;log-odds of heads&quot; = ln(heads/tails), &quot;odds of tails&quot; = ln(tails / heads)) ## log-odds of heads odds of tails ## 0.1201 -0.1201 Now, in terms of Logistic Regression, we need to determine if the \\(H_0\\) supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from independent random variables). To illustrate, we choose a simple continuous independent variable, namely fuel consumption (mpg), from the mtcars dataset and model a logistic regression. Our dependent variable is dichotomous (binary), namely V/Straight Engine (vs). In determining whether an independent variable regresses to a dependent variable, we use the logit link function to obtain the estimated logit values using glm(.), generating our logistic model in the process. Note that we discuss Generalized Linear Models (GLM) in detail in Chapter 10 (Computational Learning II) under Regression Section. # binomial family with logit link logit.model = glm(vs ~ mpg, data=mtcars, family=binomial(link=logit)) logit.hat = logit.model$fitted.values The model produces fitted values called logit values for the log odds. In other words, the values are the logarithm of the odds ratio of Straight Engine to V Engine for the vs dependent variable. See Figure 6.33. sort.mpg = sort(mtcars$mpg, index.return = TRUE) x = sort.mpg$x y = logit.hat[sort.mpg$ix] plot(NULL, col=&quot;deepskyblue&quot;, pch=16, xlim = range(x), ylim = range(y), xlab = &quot;mpg (Predictor)&quot;, ylab = &quot;logit(p)&quot;, main=&quot;Logit&quot;) grid(col=&quot;lightgrey&quot;) points(x, y, pch=20, col=&quot;navyblue&quot;) lines(x, y, pch=20, col=&quot;navyblue&quot;, lwd=2, lty=1) points(mtcars$mpg, mtcars$vs, col= ifelse(mtcars$vs == 1, &quot;darkgreen&quot;, &quot;brown&quot;), pch=16) arrows(24.5, 0.81, 27.5,0.6, code=1, length=0.1) text(30, 0.55, label=expression(paste(&quot;logit(p)=&quot;,beta^T, &quot;X&quot;))) Figure 6.33: Logit (Estimated Values) Note that the logit model in the figure seems to show a sigmoid curve. However, that is not the case. The model produces an outcome that follows a linear logit. That becomes more apparent if we feed the model with missing values and predict the outcome. See Figure 6.34. x = mpg.predictor = seq(0, 40, length.out=100) predicted.logit = predict.glm(logit.model, data.frame(mpg = mpg.predictor)) y = predicted.logit plot(NULL, col=&quot;deepskyblue&quot;, pch=16, xlim = range(x), ylim = range(y), xlab = &quot;mpg (Predictor)&quot;, ylab = &quot;logit(p)&quot;, main=&quot;Logit (Linear)&quot;) grid(col=&quot;lightgrey&quot;) lines(x, y, pch=20, col=&quot;navyblue&quot;, lwd=2, lty=1) text(20, 3, label=expression(paste(&quot;logit(p)=&quot;, beta^T, &quot;X&quot;))) Figure 6.34: Logit (Linear) Next, we convert the log odds into the inverse form (which becomes the P-hat \\(\\mathbf{\\hat{p}}\\)). See Figure 6.35. The figure does show the expected sigmoid curve. p.hat = y = inverse.logit(predicted.logit) plot(NULL, col=&quot;deepskyblue&quot;, pch=16, xlim = range(x), ylim = range(y), xlab = &quot;mpg (Predictor)&quot;, ylab = &quot;inverse.logit(z) = p.hat&quot;, main=&quot;Inverse Logit (Sigmoid)&quot;) abline(simple.model, col=&quot;darksalmon&quot;) # grid(col=&quot;lightgrey&quot;) lines(x, y, pch=20, col=&quot;navyblue&quot;, lwd=2, lty=1) text(25, 0.6, label=&quot;Sigmoid&quot;) Figure 6.35: Inverse Logit (Sigmoid) The coefficients of the fitted values show as follows: beta_0 = as.numeric(logit.model$coefficients[1]) beta_1 = as.numeric(logit.model$coefficients[2]) \\(\\beta_0\\) = -8.8331 and \\(\\beta_1\\) = 0.4304. If we review Figure 6.34, \\(\\beta_0\\) is the intercept that touches the y-axis. We can re-formulate our log-odds model, recalculating \\(\\mathbf{z_i}\\). \\(z_i\\) = \\(\\beta_0\\) + \\(\\beta_1\\) \\(x_i\\) = -8.8331 + 0.4304 \\(x_i\\) x.observed = new.mpg = 30 (z = beta_0 + beta_1 * x.observed) ## [1] 4.079 (z = as.vector(predict.glm(logit.model, newdata = data.frame(mpg = x.observed)))) ## [1] 4.079 In terms of statistical significance, let us summarize the fitted model using glm(.). summary(logit.model) ## ## Call: ## glm(formula = vs ~ mpg, family = binomial(link = logit), data = mtcars) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.213 -0.512 -0.228 0.640 1.698 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -8.833 3.162 -2.79 0.0052 ** ## mpg 0.430 0.158 2.72 0.0066 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 43.860 on 31 degrees of freedom ## Residual deviance: 25.533 on 30 degrees of freedom ## AIC: 29.53 ## ## Number of Fisher Scoring iterations: 6 In the summary of the model, we see that the observed data (mpg) is also statistically significant at alpha \\(\\alpha = 0.01\\) (e.g., less than one in a hundred chance of being wrong). In other words, the fuel consumption has a significant association with the type of engine (vs) with 99% confidence. To illustrate further, let us this time consider a multiple logistic regression - that is, with multiple independent variables. Let us use a few new functions; for example, mvrnorm(.) from a library called MASS to generate a multivariate normal distribution, rbinom(.) to generate our binomial response variable, and finally glm(.) using a generalized linear model with binomial family. Here is what it looks like: set.seed(1) binary = seq(0,1) range = seq(-5,5, length.out=100) sample_size = 100 predictors = 3 # simulate an unexplained residual (Gaussian) e = rnorm(n=sample_size, 0, 1) / 20 mu = rep(0, predictors) sigma = diag(1, predictors, predictors) x_observed = x = MASS::mvrnorm(n = sample_size, mu=mu, Sigma=sigma) y_observed = inverse.logit(x_observed) + e # try some initial beta values and inject some bias b0 = 2.3; b1 = 3; b2 = 1; b3 = 1.5 z = b0 + b1 * x[,1] + b2 * x[,2] + b3 * x[,3] y_expected = rbinom(n = sample_size, size=1, prob=inverse.logit(z)) x_obs = x_observed; y_exp = y_expected (glm.model = glm(y_exp ~ x_obs, family=binomial(link=&quot;logit&quot;))) ## ## Call: glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;)) ## ## Coefficients: ## (Intercept) x_obs1 x_obs2 x_obs3 ## 2.26 4.04 0.99 1.95 ## ## Degrees of Freedom: 99 Total (i.e. Null); 96 Residual ## Null Deviance: 120 ## Residual Deviance: 49.9 AIC: 57.9 Here, we see four beta hat coefficients (including the intercept) which allows the logistic model to fit: \\(\\hat{\\beta}_0\\) = 2.2559 \\(\\hat{\\beta}_1\\) = 4.0386 \\(\\hat{\\beta}_2\\) = 0.9902 \\(\\hat{\\beta}_3\\) = 1.948. Let us compare the outcome of running summary() vs anova(): summary(glm.model) ## ## Call: ## glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.1530 -0.0481 0.0826 0.3347 2.4757 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.256 0.572 3.94 0.000081 *** ## x_obs1 4.039 0.998 4.05 0.000052 *** ## x_obs2 0.990 0.427 2.32 0.0205 * ## x_obs3 1.948 0.664 2.93 0.0033 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 120.43 on 99 degrees of freedom ## Residual deviance: 49.94 on 96 degrees of freedom ## AIC: 57.94 ## ## Number of Fisher Scoring iterations: 7 The outcome of our summary shows that the first coefficient \\(x\\_obs1\\) has three asterisks (***), indicating that this coefficient is statistically significant at \\(\\alpha=0.001\\). The second coefficient \\(x\\_obs1\\) has one asterisk (*) indicating that this coefficient is significant at \\(\\alpha=0.05\\). The third coefficient is significant at \\(\\alpha=0.10\\). In other words, the first predictor has a significant effect on the outcome with 99% confidence. The second predictor has a significant association (or effect) on the outcome with 95% confidence. Lastly, the third predictor has a significant association (or effect) on the outcome with 90% confidence. The result rejects the null hypothesis at their respective alphas. anova(glm.model, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model: binomial, link: logit ## ## Response: y_exp ## ## Terms added sequentially (first to last) ## ## ## Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi) ## NULL 99 120.4 ## x_obs 3 70.5 96 49.9 3.4e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Overall, an ANOVA using the Chi-square test shows that the observed predictors have a significant association or effect on the outcome with 99% confidence (at \\(\\alpha=0.001\\)). One that is almost identical to the logit link function is what we call the probit link function. In the inverse-logit (CDF) function, we generate an S-shaped non-linear curve and determine the dichotomous nature of the outcome in which we say that the upper half of the curve corresponds to the positive half (e.g., YES, TRUE, ONE, and so on) versus the lower half corresponds to the negative half (e.g., NO, FALSE, ZERO, and so on). On the other hand, a bell-shaped non-linear curve is generated by the inverse-probit (CDF) function, which is a Gaussian Cumulative Density function. When using glm(), we merely replace the link with probit like so: (probit.model = glm(dependent.y ~ independent.x, family=binomial(link=&quot;probit&quot;))) We continue to expand on the concept of Logistic Regression and Generalized Regression Models (GLM) using glm(.) in Chapter 10 (Computational Learning II) under Poisson Regression Subsection under Regression. 6.8.4 Poisson Regression Similar to Logistic Regression, we also review Poisson Regression in the context of Generalized Linear Model (GLM), emphasizing the use of a link function. Poisson Regression models a regression that describes the number of incidents occurring during a follow-up period (per hour, per day, per week). Poisson models are suitable for data in which, as examples, we count the number of tweets a person submits per day, the number of eye blinks per minute, or the number of cars paying in a toll plaza every hour. Here, we deal with Poisson distribution, which is expressed as such: \\[y \\sim Pois(\\hat{\\lambda}) \\equiv N(\\mu = \\hat{\\lambda}, \\epsilon)\\] The intuition behind Poisson Regression also starts with two functions similar to Logistic Regression. Link Function: A Link Function allows us to model regression in linear fashion. \\[\\begin{align} \\text{link}(\\hat{\\lambda}) = \\log_e\\left(y\\right) = z \\end{align}\\] Inverse Link Function: \\[\\begin{align} \\text{link}^{-1}(z) = \\hat{\\lambda} = \\text{exp}(z) \\end{align}\\] where: \\(z = \\beta^TX\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\beta_0 + \\sum_{i=1}^n\\beta_i x_i\\). The Lambda, denoted by \\(\\lambda\\), represents the mean of a Poisson distribution. The expectation is that lambda grows in an exponential non-linear fashion, as seen in the Inverse Link Function. However, our goal is to linearize the function; thus, we have the Link Function. A detailed discussion of Poisson Regression is covered in Chapter 10 (Computational Learning II) under Regression Section. 6.8.5 Cox Regression Cox Regression, also known as Cox proportional hazard regression, models a regression describing the length of time an event occurs, also called time-to-event (TTE) occurrence. That applies to cases such as survival rate and hazard rate of subjects (Cox D. R. 1972). Let us review Figure 6.36 to illustrate. Figure 6.36: Survival Probability Suppose from the figure that we split a group of subjects into two environments - one environment with treatment and another environment with no treatment. We then measure the probability of survival of the two groups. Below is a summary of the survival probability in the figure: ## T1 T2 T3 T4 T5 ## Total Subjects in Treatment 9.00 7.00 6.00 2.00 1 ## Total Subjects not in Treatment 11.00 9.00 9.00 6.00 4 ## Treated Subjects Failing to Survive 2.00 1.00 4.00 1.00 1 ## Untreated Subjects Failing to Survive 2.00 0.00 3.00 2.00 4 ## Survival Probab. of Treated Subjects 0.78 0.67 0.22 0.11 0 ## Survival Probab. of Untreated Subjects 0.82 0.82 0.55 0.36 0 Survival probability is the probability of surviving through time (t) and is expressed as: \\[\\begin{align} S(t) = \\prod_{i = 1}^t \\frac{n_i - d_i}{n_i} \\end{align}\\] For example, there are a total of 9 subjects in treatment at time one, T1 and a total of 11 subjects that are not in treatment. Now, at time three, T3, because some subjects failed to survive previous to T3, we are left with 6 subjects in treatment and 9 subjects not in treatment. At time one, T1, we have 2 subjects in treatment that failed to survive and 2 subjects not in treatment that failed to survive. At time three, T3, we have 4 subjects in treatment that failed to survive and 3 subjects not in treatment that failed to survive. The survival probability at time one, T1, of the group with treatment and the group with no treatment are 0.78 and 0.82, respectively. At time three, T3, the survival probabilities of the group with treatment and the group without treatment are 0.22 and 0.55, respectively. We calculate the survival probability at T1 like so: \\[\\begin{align} S_{(treated)}(t=1) = \\left(\\frac{9-4}{9}\\right) = 0.56 \\ \\ \\ \\ \\ \\ \\ \\ S_{(not treated)}(t=1) = \\left(\\frac{11 - 0}{11}\\right) = 1.00 \\end{align}\\] The survival probability at T3 is calculated like so: \\[\\begin{align*} S_{(treated)}(t=3) {}&amp;= \\left(\\frac{9-4}{9}\\right)_{T1} \\times \\left(\\frac{5-0}{5}\\right)_{T2} \\times \\left(\\frac{5-3}{5}\\right)_{T3} = 0.22\\\\ S_{(not\\ treated)}(t=3) &amp;= \\left(\\frac{11-0}{11}\\right)_{T1} \\times \\left(\\frac{11-1}{11}\\right)_{T2} \\times \\left(\\frac{10-4}{10}\\right)_{T3} = 0.55 \\end{align*}\\] Also, it helps to understand the Hazard Ratio (or risk ratio) which is expressed as: \\[\\begin{align} R(t) = \\frac{risk_{(treatment)}}{risk_{(control)}} \\end{align}\\] where: treatment refers to the exposure of a group to treatment (this is an experimental group). control refers to a baseline group in a sample not exposed to treatment (this is a control group). For example, suppose we have a sample of plants which we expose to a type of soil with special treatment. Our experiment is to determine if the plant survives under such conditions. set.seed(142) sample_size = 20 # T - with treatment, N - with no treatment treatment_range = c(&quot;T&quot;, &quot;N&quot;) population = sample( treatment_range, size = 300, replace=TRUE ) sample = sample( population , size =sample_size, replace=FALSE ) treated = sample[which(sample==&quot;T&quot;)] not_treated = sample[which(sample==&quot;N&quot;)] In terms of risk ratio, we have the following computation: (risk_ratio = length(treated) / length(not_treated)) ## [1] 1.222 A risk ratio gives the following interpretation: \\[\\begin{align*} R {}&amp;= 1\\ \\ \\ \\ \\text{exposure to treatment has no impact on risk of an incident}\\\\ R &amp;&gt; 1\\ \\ \\ \\ \\text{suggests an increased risk of an incident in the treatment group} \\\\ R &amp;&lt; 1\\ \\ \\ \\ \\ \\text{suggests a reduced risk of an incident in the treatment group} \\end{align*}\\] Because cox regression covers a wide range of topics, we only cover here significance of regression. The Cox regression model at time (t) is expressed as: \\[\\begin{align} ln \\frac{h(t|x_1, x_2,...,x_n)}{h_0(t)} = \\sum_{i=1}^n \\beta_i x_i \\end{align}\\] which is derived from the following equation: \\[\\begin{align} h(t) = h_0(t|x_1, x_2,...,x_n)\\times exp \\left(\\sum_{i=1}^n \\beta_i x_i \\right) \\end{align}\\] where: \\(h(\\mathbf{t|x_1, x_2,...,x_n)}\\) is the expected incidence (hazard) at time (t) given independent variables. \\(h_0(t)\\) is the baseline incidence (hazard) at time (t) when \\(x_1 = x_2 = ... = x_n = 0\\). \\(\\mathbf{\\beta_i}\\) are the coefficients, \\(\\mathbf{x_i}\\) are the predictive variables. To illustrate, we create a dataset with survival range, environment type, and status. The idea is to understand the survival pattern of a specie exposed to three different types of environments where status indicates the following (1=survived, 0=failed to survive): set.seed(2020) sample_size = 300 survival_range = seq(1, 50) status_range = seq(0,1) environment_range = seq(1,3) time = sample( survival_range, size =sample_size, replace=TRUE ) status = sample( status_range , size =sample_size, replace=TRUE ) environment = sample( environment_range , size =sample_size, replace=TRUE ) data = cbind(time, status) data = cbind(data, environment ) data = as.data.frame(data) data$environment = as.factor(data$environment) head(data) ## time status environment ## 1 33 0 3 ## 2 20 0 3 ## 3 31 0 2 ## 4 24 0 3 ## 5 7 0 3 ## 6 4 1 1 Let us use a built-in function called coxph(.) from survival library to model our Cox regression: library(survival) (cox.model = coxph( Surv(time, status) ~ environment, data = data )) ## Call: ## coxph(formula = Surv(time, status) ~ environment, data = data) ## ## coef exp(coef) se(coef) z p ## environment2 -0.1 0.9 0.2 -0.5 0.6 ## environment3 0.2 1.2 0.2 1.0 0.3 ## ## Likelihood ratio test=2 on 2 df, p=0.3 ## n= 300, number of events= 152 Now, we summarize the model for analysis: summary(cox.model) ## Call: ## coxph(formula = Surv(time, status) ~ environment, data = data) ## ## n= 300, number of events= 152 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## environment2 -0.104 0.901 0.194 -0.54 0.59 ## environment3 0.204 1.226 0.209 0.98 0.33 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## environment2 0.901 1.110 0.616 1.32 ## environment3 1.226 0.816 0.814 1.85 ## ## Concordance= 0.528 (se = 0.027 ) ## Likelihood ratio test= 2.31 on 2 df, p=0.3 ## Wald test = 2.36 on 2 df, p=0.3 ## Score (logrank) test = 2.38 on 2 df, p=0.3 The z is a wald statistics computed based on \\(z = \\frac{coef}{se(coef)}\\). In the example above, our p-value is greater than our typical alpha values, and so our null hypothesis holds in which our coefficient does not have any significant effect (or the different environment does not contribute to survivability). \\[\\begin{align} H_0 {}&amp;: \\beta_1 = 0 \\\\ H_1 &amp;: \\beta_1 \\ne 0 \\end{align}\\] However, if we are to consider inference for regression, which we discuss further in the next section, we may at least be able to predict the probability of survivability. We use a built-in function called survfit(.) to fit our cox model, then we capture a predicted probability at time (t) (where i=20): p = survfit(cox.model) i = 20 t = p$time[i]; r = p$n.risk[i]; e = p$n.event[i]; s = p$surv[i] l = p$lower[i]; u = p$upper[i]; c = p$conf.int t(as.matrix(round(c(&quot;survival prob.&quot;=round(s,2), &quot;time&quot;= t, &quot;lower&quot;=l, &quot;upper&quot;=u, &quot;conf&quot;=c), 2))) ## survival prob. time lower upper conf ## [1,] 0.81 20 0.76 0.86 0.95 Let us plot the survival probability (see Figure 6.37). Figure 6.37: Kaplan-Meier Estimate In the next few sections, let us cover Polynomial regressions as we continue to emphasize on significance of regression. 6.8.6 Polynomial Regression So far, we have discussed data points that follow normal distribution and use linear regression to fit a model to data. We also discussed data points that follow exponential distribution and use generalized linear regression to fit. Now in cases in which the distribution of data may not necessarily reflect a simple straight line or a simple curve line but rather curvier or deeply curvy, it helps to carefully obtain a model that is not too wiggly causing an overfit, or that is not approaching a straight line causing an underfit. Both situations offset the power of inference or prediction. Therefore, in this section, we discuss the importance of fitting a model - ensuring our model regresses to the actual data set. The more representative our model is, the more we can use it for prediction in general. However, an overfitted model may not be able to apply to a new data set; likewise, one that is underfitted may just as well not apply. This section talks about higher-degree polynomials as an extended discussion of simple linear regression - one-degree polynomial. A good understanding of this section requires a good review of B-spline regression in Chapter 3 (Numerical Linear Algebra II). It may help to re-iterate the four knobs that we can use to manipulate and influence the curvature of B-splines, especially when fitting a model: the coefficients the number of knots the placements of knots the basis function For illustration, let us use Chebyshev polynomial of the first kind as a template for our next discussion. Chebyshev polynomial is introduced in Chapter 3 (Numerical Linear Algebra II) under the Higher Degree Polynomials Section. Out of four polynomials, let us use the quintic polynomial and generate the response variables: one for observed, one for expected: quadratic &lt;- function(x) { 2*x^2 - 1 } cubic &lt;- function(x) { 4*x^3 - 3*x } quartic &lt;- function(x) { 8*x^4 - 8*x^2 + 1 } quintic &lt;- function(x) { 16*x^5 - 20*x^3 + 5*x } x = seq(-1,1, length.out=150) e = rnorm(length(x), mean=0, sd=1) / 5 # simulate Gaussian residual y_observed = quintic(x) + e y_expected = quintic(x) Here, to create a model that fits the quintic polynomial above for Polynomial regression, let us use a new function called bs(.) from the splines library. The bs stands for B-spline. Notice that we use five degrees of freedom. That is to attempt to render enough wiggles or oscillation to form a quintic polynomial curve, as we see after plotting. library(splines) poly.model = lm(y_expected ~ bs(x, df=5)) Now, let us plot (See Figure 6.38). Notice the polynomial generated by \\(bs(x,df=5)\\). It can generate a quintic polynomial that fits close to the actual. plot(NULL, xlim=range(-1,1), ylim=range(-1,1), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis (quintic)&quot;, main=&quot;Chebyshev polynomial of the first kind (Quintic poly)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, y_observed, col=&quot;black&quot;, lwd=1) lines(x, y_expected, col=&quot;navyblue&quot;, lwd=2, lty=1) lines(x, fitted(poly.model), col=&quot;darksalmon&quot;, lwd=2, lty=1) legend(0, -0.5, legend=c( &quot;expected fit&quot;, &quot;poly.model&quot;), col=c(&quot;navyblue&quot;, &quot;darksalmon&quot;), lty=1, cex=0.8) Figure 6.38: Chebyshev polynomial of the first kind (Quintic poly) Let us evaluate the significance of regression for our polynomial model: summary(poly.model) ## ## Call: ## lm(formula = y_expected ~ bs(x, df = 5)) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.377 -0.098 0.000 0.098 0.377 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.6227 0.0520 -12.0 &lt;2e-16 *** ## bs(x, df = 5)1 3.4213 0.0992 34.5 &lt;2e-16 *** ## bs(x, df = 5)2 -3.6255 0.0669 -54.2 &lt;2e-16 *** ## bs(x, df = 5)3 4.8710 0.0910 53.5 &lt;2e-16 *** ## bs(x, df = 5)4 -2.1758 0.0740 -29.4 &lt;2e-16 *** ## bs(x, df = 5)5 1.2455 0.0757 16.4 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.118 on 144 degrees of freedom ## Multiple R-squared: 0.973, Adjusted R-squared: 0.972 ## F-statistic: 1.04e+03 on 5 and 144 DF, p-value: &lt;2e-16 Notice that we base the number of coefficients (excluding intercept) on the number of degrees of freedom provided to our model. Looking at the geometrical fit of our model, it may seem that the fitted model using bs(x, df=5) may not be close enough to the actual polynomial curve - it does not regress enough. Nonetheless, our coefficients are significant at \\(\\alpha = 0.001\\). We may accept the model or continue to adjust and improve. In the later section, we show how to adjust the model by covering power of prediction under Inference for Regression. 6.8.7 B-Splines and Natural Splines We extend our discussion of B-Splines and Natural Splines from Chapter 3 (Numerical Linear Algebra II) under Polynomial Regression Section and Polynomial Interpolation Section. In the previous section, we show how to interpret the significance of regression. In this section, we continue B-spline regression from the perspective of prediction. We can attain Better prediction if only we can adjust a B-spline model to fit better. Here, we show two adjustments to fit a B-SPline and one adjustment to fit a Natural Spline. From there, we obtain three models which we can use to predict. Let us continue to use the same dataset (using one of the Chebyshev polynomials): quadratic &lt;- function(x) { 2*x^2 - 1 } cubic &lt;- function(x) { 4*x^3 - 3*x } quartic &lt;- function(x) { 8*x^4 - 8*x^2 + 1 } quintic &lt;- function(x) { 16*x^5 - 20*x^3 + 5*x } x = seq(-1,1, length.out=150) e = rnorm(length(x), mean=0, sd=1) / 5 # simulate Gaussian residual y_observed = quintic(x) + e y_expected = quintic(x) Let us now try to adjust: use df=3 (3 degrees of freedom) - for the sake of showing insufficient oscillation (underfit): adj1.model = lm(y_expected ~ bs(x, df=3)) still use df=5 (5 degrees of freedom) with the following number of knots and placements - to show that the wrong number of knots and placement could create greater displacement of the oscillation (with possible signs of rank deficiency - see next topic around knob adjustments): x.df5.knots4 = bs(x, df=5, knots=c(-0.6, -0.2, 5, 0.9)) adj2.model = lm(y_expected ~ x.df5.knots4) use another function called ns() - natural spline with the following parameters - to show that even a natural spline, we can also find ways to fit a model: adj3.model = lm(y_expected ~ ns(x, df=5)) Let us try to plot (this time coloring the actual fit with grey color to emphasize the other adjusted models). See Figure 6.39. plot(NULL, xlim=range(-1,1), ylim=range(-1,1), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis (quintic)&quot;, main=&quot;Fitting Adjusted B-Spline Models&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, y_observed, col=&quot;lightgrey&quot;, lwd=1) lines(x, y_expected, col=&quot;navyblue&quot;, lwd=2, lty=1) lines(x, fitted(adj1.model), col=&quot;brown&quot;, lwd=2, lty=2) lines(x, fitted(adj2.model), col=&quot;red&quot;, lwd=2, lty=2) lines(x, fitted(adj3.model), col=&quot;darksalmon&quot;, lwd=2, lty=2) legend(0.1, -0.5, legend=c( &quot;expected fit&quot;, &quot;adj1.model&quot;, &quot;adj2.model&quot;, &quot;adj3.model&quot;), col=c(&quot;navyblue&quot;, &quot;brown&quot;, &quot;red&quot;, &quot;darksalmon&quot;), lty=1, cex=0.8) Figure 6.39: Fitting Adjusted B-Spline Models As one can imagine, there are more ways than just one to generate a linear model with multiple knobs available to adjust, allowing us to try to tune a polynomial model so it can fit well to a given data. Now, using the adjusted models, we demonstrate three predictions: # predict via interpolation x.new = seq(-1, 1, length.out=150) y1.predict = stats::predict(adj1.model, newdata = data.frame(x = x.new)) y2.predict = stats::predict(adj2.model, newdata = data.frame(x = x.new)) ## Warning in predict.lm(adj2.model, newdata = data.frame(x = x.new)): prediction ## from a rank-deficient fit may be misleading y3.predict = stats::predict(adj3.model, newdata = data.frame(x = x.new)) We should also notice that the second prediction renders a warning about rank deficiency. If we are to run a summary of the model, it turns out that the estimated value of one of the coefficients is not available due to singularity, effectively rendering a linear combination with one being deficient in forming a full rank. summary(adj2.model) ## ## Call: ## lm(formula = y_expected ~ x.df5.knots4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.659 -0.271 -0.013 0.285 0.972 ## ## Coefficients: (1 not defined because of singularities) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -1.271 0.194 -6.54 1.0e-09 *** ## x.df5.knots41 4.207 0.349 12.05 &lt; 2e-16 *** ## x.df5.knots42 -0.342 0.218 -1.57 0.12 ## x.df5.knots43 1.607 0.308 5.22 6.2e-07 *** ## x.df5.knots44 2.650 0.271 9.80 &lt; 2e-16 *** ## x.df5.knots45 -0.261 0.250 -1.05 0.30 ## x.df5.knots46 2.929 0.348 8.43 3.5e-14 *** ## x.df5.knots47 NA NA NA NA ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.362 on 143 degrees of freedom ## Multiple R-squared: 0.75, Adjusted R-squared: 0.739 ## F-statistic: 71.3 on 6 and 143 DF, p-value: &lt;2e-16 Let us plot the predictions (see Figure 6.40). plot(NULL, xlim=range(-1,1), ylim=range(-1,1), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis (quintic)&quot;, main=&quot;Plotting Predicted B-Spline Lines&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, y_observed, col=&quot;lightgrey&quot;, lwd=1) lines(x, y_expected, col=&quot;navyblue&quot;, lwd=2, lty=1) lines(x.new, y1.predict, col=&quot;brown&quot;, lwd=2, lty=2) lines(x.new, y2.predict, col=&quot;red&quot;, lwd=2, lty=2) lines(x.new, y3.predict, col=&quot;darksalmon&quot;, lwd=2, lty=2) legend(0.1, -0.5, legend=c( &quot;expected fit&quot;, &quot;y1.predict&quot;, &quot;y2.predict&quot;, &quot;y3.predict&quot;), col=c(&quot;navyblue&quot;, &quot;brown&quot;, &quot;red&quot;, &quot;darksalmon&quot;), lty=1, cex=0.8) Figure 6.40: Plotting Predicted B-Spline Lines Notice that the curves in the plot correspond to those in Figure 6.40. That is only because the predicted curves are already the fitted curves. Any new data always predicts a value in the y-axis inside the fitted curve. 6.8.8 Spline Smoothing There are times when curves seem rough - rather than smooth. Hence, another form of adjustment to make is by smoothing splines. In terms of the formula, we reference Chapter 3 (Numerical Linear Algebra II) under B-Spline Regression Subsection under Approximating Polynomial Functions by Interpolation Section. Note here that if done optimally, smoothing a curve allows us to also fit a smooth spline. To illustrate, we intentionally reduce our dataset to a sample size of maybe only 10. That creates a crooked line instead of a smooth line. We use a function in R called approx(.) to simulate a crooked line. set.seed(2020) sample_size = 20 x = seq(1,sample_size) y = rnorm(n=sample_size, mean=0, sd=1) crooked.line = approx(x, y, method=&quot;linear&quot;, n=sample_size) We then apply a few smoothing adjustments to the crooked line by using two built-in R functions called spline() and smooth.spline() to demonstrate our case. set.seed(2020) spline = spline(x, y, n=100) spline1.model = smooth.spline(x, y, spar=0.30) spline2.model = smooth.spline(x, y, spar=1.00) spline3.model = smooth.spline(x, y, df = 9) We are now ready to predict. x.interpolate = seq(1, sample_size, length.out=100) smooth1.line = stats::predict(spline1.model, x.interpolate) smooth2.line = stats::predict(spline2.model, x.interpolate) smooth3.line = stats::predict(spline3.model, x.interpolate) Let us plot our prediction (see Figure 6.41). plot(NULL, xlim=range(1,sample_size), ylim=range(-3,3), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis (approx)&quot;, main=&quot;Fitting a Smooth Spline&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(crooked.line, col=&quot;navyblue&quot;, lwd=1) lines(spline, col=&quot;red&quot;, lwd=1, lty=2) lines(smooth1.line, col=&quot;brown&quot;, lwd=1, lty=2) lines(smooth2.line, col=&quot;darksalmon&quot;, lwd=1, lty=2) lines(smooth3.line, col=&quot;purple&quot;, lwd=1, lty=2) points(x, y, col=&quot;black&quot;, pch=16) legend(7, -0.5, legend=c( &quot;crooked.line&quot;, &quot;spline&quot;, &quot;smooth1.line&quot;, &quot;smooth2.line&quot;, &quot;smooth3.line&quot;), col=c(&quot;navyblue&quot;, &quot;red&quot;, &quot;brown&quot;, &quot;darksalmon&quot;, &quot;purple&quot;), lty=1, cex=0.8) Figure 6.41: Fitting a Smooth Spline The spline shows a smooth curve that travels almost through all the points touched by the crooked line. Therefore, we can say that spline is just a smoothing method for a crooked line. However, modeling a spline to fit our data gets overfitting this way. The spline1.model has a smooth parameter (spar) of 0.30. That is a way to force the spline to fit about 30% through the data. So it does not overfit much. On the other hand, spline2.model has a spar of 1.0, which simulates a straight linear regression fitting. As we can see, fitting this way could end up underfitting. Overfitting may risk including outliers while, at the same time, the model cannot be generalized in usage. On the other hand, underfitting may eliminate outliers but exclude significant influencers. Overall, spline1.model is a better polynomial regression model than the others but may still be improved. Other parameters can be tuned using the smooth.spline(.) function. To view a list of parameters, we can just run it like so (notice how it calculates the other parameters by default): # output of a spline model smooth.spline(x, y, spar=0.30) ## Call: ## smooth.spline(x = x, y = y, spar = 0.3) ## ## Smoothing Parameter spar= 0.3 lambda= 0.00001489 ## Equivalent Degrees of Freedom (Df): 12.87 ## Penalized Criterion (RSS): 5.869 ## GCV: 2.308 The output includes two performance metrics: RSS - calculates a Penalized criterion. Recall residual sum squares covered in least squares. GCV - generalized cross-validation. Recall bandwidth selection criterion covered in KDE. Similarly, we use GCV for parameter selection for optimal spline smoothing. There are three parameters shown: Spar, Lambda, and Degrees of Freedom. We can use degrees of freedom like so (notice how it calculates the other parameters by default). Here, fewer degrees of freedom generate a linear regression: smooth.spline(x, y, df=6) ## Call: ## smooth.spline(x = x, y = y, df = 6) ## ## Smoothing Parameter spar= 0.5182 lambda= 0.0005617 (12 iterations) ## Equivalent Degrees of Freedom (Df): 6.001 ## Penalized Criterion (RSS): 25.63 ## GCV: 2.616 Notice in Figure 6.41, the parameter spar=0.30 generates a polynomial curve that closes matches one with the parameter df=6.873 and vice-versa. A linear regression happens if spar=1.00 or df=2. Next, we also can use lambda like so: smooth.spline(x, y, lambda= 0.0001300587) ## Call: ## smooth.spline(x = x, y = y, lambda = 0.0001300587) ## ## Smoothing Parameter spar= NA lambda= 0.0001301 ## Equivalent Degrees of Freedom (Df): 8.163 ## Penalized Criterion (RSS): 18.58 ## GCV: 2.653 We can use a combination like so (spar and lambda are mutually exclusive): smooth.spline(x, y, df=6.78, lambda= 0.00013) ## Call: ## smooth.spline(x = x, y = y, df = 6.78, lambda = 0.00013) ## ## Smoothing Parameter spar= NA lambda= 0.00013 ## Equivalent Degrees of Freedom (Df): 8.164 ## Penalized Criterion (RSS): 18.58 ## GCV: 2.653 Note that any parameter adjustments affect the power of prediction. Our optimal choices should allow a more general model to fit a broader set of cases. 6.8.9 LOESS and LOWESS Two common smoothers discussed in other literature are LOESS and LOWESS. We discussed the theory and math in Chapter 3 (Numerical Linear Algebra II) under the Polynomial Smoothing Section. Here, we show a simple use of both smoothers in R code (this time, we once again use the built-in R function predict(.) for inference): set.seed(2020) # degree of polynomial is 2 loess.model = loess(y ~ x, span=0.70, degree=2) loess.line = stats::predict(loess.model) plot(NULL, xlim=range(1,sample_size), ylim=range(-3,3), xlab=&quot;x-axis&quot;, ylab=&quot;y-axis (approx)&quot;, main=&quot;Fitting using LOWESS and LOESS&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(lowess(x, y, f=0.20), col=&quot;navyblue&quot;, lwd=2) lines(lowess(x, y, f=1.70), col=&quot;red&quot;, lwd=2, lty=2) lines(smooth1.line, col=&quot;brown&quot;, lwd=2, lty=2) lines(loess.line , col=&quot;darksalmon&quot;, lwd=2, lty=2) points(x, y, col=&quot;black&quot;, pch=16) legend(7, -1, legend=c( &quot;lowess(f=0.20)&quot;, &quot;lowess(f=1.70)&quot;, &quot;smooth1.line&quot;, &quot;loess.line(span=0.70)&quot;), col=c(&quot;navyblue&quot;, &quot;red&quot;, &quot;brown&quot;, &quot;darksalmon&quot;), lty=1, cex=0.8) Figure 6.42: Fitting using LOWESS and LOESS We use the loess(.) function to model a fit for our dataset. We then use the generated model to predict a new set of data (equivalently performing interpolation) which we then use to plot a smooth curve. In the previous discussion, spline1.model is chosen to be the better smoothing model to fit our data. Now, in Figure 6.42, we compare spline1.model with loess.model to show how both are trying to fit data. Even though it may show that spline1.model is still a better model than loess.model; we can still tune loess.model to get a good fit. We leave readers to experiment and perhaps tune the smoothing span (span) parameter. On the other hand, a lowess model with f=0.70 renders a line and is slightly smooth; but still able to fit our data. We can increase the parameter, but while it may not regress ideally, it nevertheless tries to do so, risking underfitting. Also, the span parameter in loess(.) function settles into a linear regression similar to a higher f parameter in lowess(.) function. Otherwise, if tuned to ideal lower values, both parameters can fit a polynomial model. 6.9 Inference for Regression Prediction and Inference require a fitted model. Here, the better the fit, the better the Prediction or Inference. 6.9.1 Goodness of Fit (Linear Regression) The quality of a good fit depends on specific criteria based on the metrics used. The assumption is that our linear model fits data that follows a Normal distribution. Evaluating goodness of fit for linear regression is based on two common metrics (out of many others). Coefficient of Determinant \\(\\mathbf{(R^2)}\\): Coefficient of Determination measures the proportion of the expected outcome over the observed outcome and is expressed as: \\[\\begin{align} R^2 = \\frac{\\sum_{i=1}^{n}(\\hat{y}_{i} - \\bar{y})^2}{\\sum_{i=1}^{n}(y_{i} - \\bar{y})^2} = \\frac{|\\hat{y}_{i} - \\bar{y}|^2}{|y_{i} - \\bar{y}|^2} \\end{align}\\] Because this is a proportion of both outcomes, a value close to 1 means that the expected outcome is very close to the observed outcome. That means that the model has a higher power of Prediction closer to one and a lower power of Prediction closer to zero. Another variation of this equation is as follows: \\[\\begin{align} R^2 = \\frac{VAR_{(explained)}}{VAR_{(tot)}} = \\frac{\\text{Explained Sum of Squares}}{\\text{Total Sum of Squares}} = \\frac{ESS}{TSS} = 1 - \\frac{RSS}{TSS} \\end{align}\\] Root Mean Square Error (RMSE): RMSE measures the degree of error between the expected and observed outcomes. We calculate the degree of error as the standard deviation of the error (or residual). \\[\\begin{align} RMSE = \\sqrt{\\frac{1}{n}\\sum_{i=1}^n\\left(\\hat{y}_i - y_i\\right)} \\end{align}\\] Perhaps, we can think of this in analogy to standard normal distribution with spread (or variance) in which the variance is transformed into a square root, effectively becoming a measure of standard deviation. Similarly, RMSE transforms into a square root of MSE. In other words, RMSE is the square root of the variance of residuals. And it measures regression - or the closeness of distance from the data point to the fitted line. The equation is better understood if expressed this way: \\[\\begin{align} RMSE = \\sqrt{MSE} \\end{align}\\] where MSE represents the variance of the residuals and is expressed as: \\[\\begin{align} MSE = \\frac{1}{n}\\sum_{i=1}^n\\left(y_i - \\hat{y}_i\\right)^2 \\end{align}\\] From that line of thinking, we can say that a residual with one unit of standard deviation (closer to zero - or closer to the mean or the typical value) indicates higher power of prediction; otherwise, a higher standard deviation means that the model has lower power of prediction because the expected outcome is moving away from the mean (typical value). We revisit MSE in Chapter 9 (Computational Learning I) under Regularization Section as an L2-loss regularization technique. 6.9.2 Goodness of Fit (Non-Linear Regression) We start with the premise that the following equation is true: \\[\\begin{align} y = \\beta^TX\\ + \\epsilon\\ \\ \\ \\ \\ \\ \\ where\\ \\epsilon \\sim \\mathcal{N}(0, \\sigma^2I) \\end{align}\\] The noise denoted by the epsilon (\\(\\epsilon\\)) symbol describes either a normal distribution for linear regression or an exponential distribution for non-linear regression. The estimators for \\(\\hat{\\beta}\\) and \\(\\hat{\\sigma}^2\\) generated by optimization play a role in how our fit \\(\\hat{y}\\) regresses towards \\(y\\). For Non-Linear Regression such as Logistic and Poisson Regressions, we assume that the non-linear regression model fits data that follows an Exponential distribution such as Binomial or Poisson distribution. Evaluating Goodness of Fit for non-linear regression can be measured using three related tests (R. F Engle 1984) because of their association with the Chi-square distribution. See Figure 6.43. Figure 6.43: Goodness of Fit Figure 6.43 illustrates two models. The first model is characterized by its parameter, namely \\(\\hat{\\theta}\\). This model is also called the unrestrictive/unconstrained model or the full (\\(H_1\\)) model. The second model is characterized by its parameter, namely \\(\\theta_0\\). This model is also called the restrictive/constrained model or the null (\\(H_0\\)) model. We can obtain the \\(\\hat{\\theta}\\) model using MLE procedures discussed in Chapter 7 (Bayesian Computation I) under the Bayesian Inference Section so that the MLE for a binomial distribution as an example is obtained based on the derived equation for MLE: \\[\\begin{align} \\hat{\\theta}_{(MLE)} = \\frac{x}{n}\\ \\ \\ \\text{for point-estimate}. \\end{align}\\] As always, our hypothesis consists of the null hypothesis and its alternate hypothesis: \\[\\begin{align} H_0: \\theta = \\theta_0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_1: \\theta \\ne \\theta_0 \\end{align}\\] Here, we resort to a null distribution to represent \\(\\theta_0\\), which we then compare against our sample distribution denoted by \\(\\hat{\\theta}\\). If the null distribution is not consistent with - or if it is far from - our sample distribution, e.g., \\(\\theta_0 \\ne \\hat{\\theta}\\), then it is likely that we reject \\(H_0\\) based on a statistic, e.g., using the p-value. To measure how far \\(\\theta_0\\) is from \\(\\hat{\\theta}\\), we use any of the three tests below, starting with likelihood ratio. Likelihood Ratio Test: The LR test compares the fit of a full model (\\(\\hat{\\theta}\\)) to the fit of a null model (\\(\\theta_0\\)). The test is expressed as such: \\[\\begin{align} LR(x) = -2 \\log_e \\left(\\frac{\\mathcal{L}(\\theta_0)}{\\mathcal{L}(\\hat{\\theta})}\\right) = 2\\left[\\log_e \\mathcal{L}(\\hat{\\theta}) - \\log_e \\mathcal{L}(\\theta_{0})\\right]\\ \\sim\\ \\ \\mathcal{X}^2(\\nu) \\end{align}\\] The test result follows a Chi-square distribution constrained by \\(\\nu\\) degrees of freedom. We take the result and find the p-value from a p-value table and evaluate if the full model is statistically significant, meaning it fits significantly better a model than the null model, e.g. (p &lt; 0.01). If the full model moves away from the null model, we reject the null hypothesis. Wald Test: Wald Test performs an evaluation against the unrestrictive model, namely (\\(\\hat{\\theta}\\)). It is expressed as such: \\[\\begin{align} WT = \\frac{(\\hat{\\theta} - \\theta_{0})^2}{var(\\hat{\\theta})}\\ \\sim\\ \\ \\mathcal{X}^2(\\nu) \\end{align}\\] The denominator in the equation indicates variance of the full model which requires \\(\\sigma^2\\). Note that \\(\\sigma^2\\) is usually unknown in which case, we can estimate the statistic given the following: \\[\\begin{align} var(\\hat{\\theta}) = \\underbrace{I(\\hat{\\theta})^{-1}}_{\\text{Fisher Information}} = - \\mathbb{E}[\\mathcal{L}&#39;&#39;(\\hat{\\theta})]^{-1} \\ \\ \\ \\ \\ \\ where: \\underbrace{\\mathcal{L}&#39;&#39;(\\hat{\\theta}) = \\left(\\frac{\\partial^2\\mathcal{L}}{\\partial\\hat{\\theta}^2}\\right)}_{ \\text{Observed Fisher Information} } \\end{align}\\] Secondly, knowing that standard error (SE) is the square root of variance, then we have: \\[\\begin{align} \\sqrt{var(\\hat{\\theta})} = SE(\\hat{\\theta}) = \\sqrt{I(\\hat{\\theta})^{-1}} \\end{align}\\] Therefore, we get: \\[\\begin{align} WT = \\frac{(\\hat{\\theta} - \\theta_{0})^2}{I(\\hat{\\theta})^{-1}}\\ \\sim\\ \\ \\mathcal{X}^2(\\nu) \\end{align}\\] Similarly, the test result follows a Chi-square distribution constrained by \\(\\nu\\) degrees of freedom. We take the result and find the corresponding p-value for analysis. We leave readers to investigate Cramer Rao Lower Bound relevant to Fisher Information. Score Test (Lagrange Multiplier Test): Score Test evaluates the restrictive model, namely (\\(\\theta_0\\)). That is because the slope of the unrestrictive model tends toward zero - at the top of the curve. The test formula is expressed as such: \\[\\begin{align} LM = \\frac{S(\\theta_{0})^2}{I\\left(\\theta_{0}\\right)}\\ \\sim\\ \\ \\mathcal{X}^2(\\nu) \\end{align}\\] where S describes the score equation and I describes the Fisher Information: \\[\\begin{align} S(\\theta_0) = \\underbrace{\\mathcal{L}&#39;(\\theta_0) = \\frac{\\partial \\mathcal{L}}{\\partial \\theta_0}}_{\\text{score equation}} \\ \\ \\ \\ \\ \\ \\ \\ \\ I(\\theta_0) = var(\\theta_0) \\end{align}\\] Just as the same, the test result follows a Chi-square distribution constrained by \\(\\nu\\) degrees of freedom. We then analyze its corresponding p-value. The paper from Daniel F. Kohler (1982) explains the following simplified equation based on the assumptions of a linear model and certain constraints: \\[ LR = n \\log_e \\hat{\\theta}\\ \\ \\ \\ \\ \\ \\ \\ \\ WT = n ( \\hat{\\theta} - 1)\\ \\ \\ \\ \\ \\ \\ \\ \\ LM = n (1 - 1/\\hat{\\theta}) \\] From which, we get the known inequality relationship: \\[ LM \\le LR \\le WT \\] Other tests to consider are Deviance test, Pearson’s chi-square test, Hosmer-Lemeshow test. These tests are introduced further in Chapter 10 (Computational Learning II) under Logistic Regression Section. 6.9.3 Confidence interval In cases where we are not confident about our point estimates, it may help to lay out a range of values within which we may fairly be sure that the actual value lies within such range. Then, in predicting for the value, we can use a confidence level, e.g., 95%, so that our average fit falls within \\(\\pm\\) (plus-minus) of our confidence interval. simple.model = lm(mpg ~ disp, data = mtcars) new_x = data.frame(disp = c(301, 302)) predict.lm(object = simple.model, newdata = new_x, interval=c(&quot;confidence&quot;), level=0.95) ## fit lwr upr ## 1 17.19 15.84 18.55 ## 2 17.15 15.79 18.51 We can then readily provide our interpretation with 95% confidence that our findings fall within the average range between a lower and upper boundary. In predicting for the value, we also can use a prediction interval. However, the lower and upper bound applies only to the value of our observation and not the average (or expectation) of our observation. See below: predict.lm(object = simple.model, newdata = new_x, interval=c(&quot;prediction&quot;)) ## fit lwr upr ## 1 17.19 10.42 23.97 ## 2 17.15 10.37 23.93 6.10 Summary This chapter builds intuition around Statistical Analysis and the required math. However, we have yet to describe what a statistical modeling is all about. If we search for a meaningful yet straightforward description, we can go with the description from Stobierski T. (2019), which also follows a similar description from Wikipedia. We should note that a typical description of Statistical modeling is about “applying statistical analysis” (or applying the mathematical model) to observed data. There is the math, and there is the data. Below, we have the most simple regression model, and all we need to capture is the most fitting value as a model weight for our parameter (\\(\\theta\\)). \\[ \\hat{y} = \\theta x + \\epsilon \\] We then use the model for inference. "],["bayesian.html", "Chapter 7 Bayesian Computation I 7.1 Probability 7.2 Probability Rules 7.3 Bayes Theorem 7.4 Conjugacy 7.5 Information Theory 7.6 Bayesian Inference", " Chapter 7 Bayesian Computation I Naturally unpredictable events, such as the outcome of a presidential election or the rise and fall of the stock market, are the main subjects of Bayesian Analysis. Here, we try to work around the uncertainty of such events. In this chapter, we focus on Bayesian Analysis instead of Non-Bayesian Analysis. The use of probabilistic means and reliance on prior knowledge make Bayesian Analysis different from Non-Bayesian Analysis. Here, we recognize the randomness of events occurring, represented by random variables. Similar to Non-Bayesian Analysis, we sample data, but we consider each sample as the occurrence of some random event. Our objective in this respect is to predict or approximate the probability of an event occurring. Then we use credible intervals in the same way we use confidence intervals in Statistics to gauge the accuracy of our prediction. The most notable point here is to be aware that prior knowledge is not discounted even before we sample or see the data. In this chapter and the next, we continue to focus on Numerical Analysis in the context of Bayesian Analysis as we reference the great works of Vapnik V. (2000), Stewart W.J. (2009), Murphy K.P. (2012), Gelman A. et al. (2013), Kruschke J.K. (2015), and Lambert B. (2018), along with other additional references for consistency. 7.1 Probability We extend the concept of probability from Chapter 5 (Numerical Probability and Distribution). Here, we come from the idea of sampling (or making random observations) from a population. Such a list of random observations may follow a particular distribution. For example, one of the most common distributions is the Bell-shaped Gaussian distribution using a normal probability density function (PDF). Other types of distributions are discussed in detail in Chapter 5 (Numerical Probability and Distribution) under the Types of Distribution Section. Recall the symbol \\(\\rho(x)\\) as a notation for a probability density function. Here, we use the small-case \\(f(x)\\) for our purposes instead. \\[\\begin{align} \\rho(x;\\mu, \\sigma^2) = f(x; u, \\sigma2) = \\text{&lt;normal density function&gt;} \\end{align}\\] The density of a random variable x with parameters \\(\\mathbf{\\mu}\\) and \\(\\mathbf{\\sigma^2}\\). For example, to compute for the probability of a single event, x equal to 1, we express it as such (granting x follows a normal distribution): \\[\\begin{align} f(x;\\mu, \\sigma^2) = P(x = 1 | \\mu, \\sigma^2)\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ X ~ \\sim \\mathcal{N}(\\mu, \\sigma^2) \\end{align}\\] Hereafter, we may be using different notations for probabilities. For example, we can use any notations to mean that the probability of X (uppercase) is equal to a value x (lowercase). \\[\\begin{align} P(X = x) \\equiv P_X(X = x) \\equiv P_X(x) \\end{align}\\] 7.1.1 Marginal Probability Marginal probability is a probability in which an event does not depend on the outcome of another event. For example, suppose X is a random variable, and x is an observable event. In terms of density with the probability of one event occurring, we have the following expression (we use small-case notation f(x)): \\[\\begin{align} f_X(x) = P_X(X = x) \\end{align}\\] It reads as Probability of X. It can also be said that X is not constrained by any event. Therefore, Marginal Probability is also known as Unconditional Probability. In terms of cumulative density with a range of probabilities (of multiple observable events occurring), we have the following expression (we use upper-case notation F(x)): \\[\\begin{align} \\mathcal{F}_X(X = x) = \\begin{cases} \\sum_x P_X(x) &amp; \\text{if x is discrete}\\\\ \\\\ \\int P_X(x) dx &amp; \\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber36} \\end{align}\\] Note that for continuous cumulative probability, we can use the integrated probability to obtain the density probability by derivatives: \\[\\begin{align} f_X(x) = \\frac{d}{dx} \\mathcal{F}_X(X = x)\\ \\ \\ \\rightarrow\\ \\ \\ \\ \\mathcal{F}x(X = x) = \\int f_X(x) dx \\end{align}\\] 7.1.2 Joint Probability Joint probability is the probability of two or more events occurring together - it represents the intersection of two or more events. Suppose X and Y are two random variables and that x and y are the corresponding observable events. In terms of density with the probability of one event occurring per random variable, we have the following expression: \\[\\begin{align} f_{X,Y}(x,y) = P_{X,Y}(X = x, Y = y) \\end{align}\\] That is also denoted by any of the following notations: \\[\\begin{align} P(X \\perp\\!\\!\\perp Y) = P(X \\cap Y) = P(X\\ and\\ Y) = P(XY),\\ \\ \\ \\ \\ \\ \\ e.g. P(X = 5, Y = 5) \\end{align}\\] The probability of drawing a card from a deck and a heart from another is calculated as: \\[\\begin{align} P(X = 9\\ and\\ Y = heart) = 1/52 + 13/52 = 7/26 \\end{align}\\] In terms of cumulative density with a range of probabilities, we have the following expression: \\[\\begin{align} \\mathcal{F}_{X,Y}(x, y) = \\begin{cases} \\sum_x \\sum_y P_{X,Y}(x,y) &amp; \\text{if x,y are discrete}\\\\ \\\\ \\int \\int P_{X,Y}(x,y) dx dy &amp; \\text{if x,y are continuous} \\end{cases} \\label{eqn:eqnnumber37} \\end{align}\\] One rule worth mentioning is the Sum rule, which involves the use of marginal and joint probabilities and is expressed in general as: \\[\\begin{align} P(X) = \\begin{cases} \\sum_{Y} P_{X,Y}(x,y) &amp; \\text{if y is discrete}\\\\ \\\\ \\int_y P_{X,Y}(x,y) dy &amp; \\text{if y is continuous}\\\\ \\end{cases} \\label{eqn:eqnnumber38} \\end{align}\\] Similarly, because \\(P_{X,Y}(x,y) = P_{X,Y}(x|y)P_Y(y)\\), then we get: \\[\\begin{align} P(X) = \\begin{cases} \\sum_{Y} P_{X|Y}(x|y)P_Y(y) &amp; \\text{if y is discrete}\\\\ \\\\ \\int_y P_{X|Y}(x|y)P_Y(y) dy &amp; \\text{if y is continuous}\\\\ \\end{cases} \\label{eqn:eqnnumber39} \\end{align}\\] That states that the density \\(P(X)\\) can be expressed as the joint distribution between X and Y, namely \\(P(X, Y)\\), with the sum of all possible values of Y, granting that Y has a proper distribution (in which its cumulative probability sums up or integrates to 1). That is called marginalization. We marginalize the probability of X. For example: \\[\\begin{align} P(X) = P(X,Y = y_1) + P(X, Y = y_2) \\end{align}\\] where \\(P(Y = y_1)=0.50,\\ P(Y = y_2)=0.50\\). Another example: \\[\\begin{align} P(X) = P(X,Y = y_1) + P(X, Y = y_2) + P(X, Y = y_3) \\end{align}\\] where \\(P(Y = y_1)=0.25,\\ P(Y = y_2)=0.25,\\ P(Y = y_3)=0.50\\). If we sum (e.g., discrete) or integrate (e.g., continuous) all probabilities of Y, then we end up with only the probability of X: \\(P(X)\\). Sum rule is further discussed in the Law of total probability section. In terms of joint distribution, we can show the following: \\[\\begin{align} P(X) = P(X_1, X_2) \\sim \\mathcal{N}(\\mu, \\Sigma) \\ \\ \\ \\ where\\ \\mu = \\binom{\\mu_1}{\\mu_2}\\ and\\ \\Sigma = \\left[ \\begin{array}{cc} \\Sigma_{1,1} &amp; \\Sigma_{1,2}\\\\ \\Sigma_{2,1} &amp; \\Sigma_{2,2} \\end{array} \\right] \\label{eqn:eqnnumber40} \\end{align}\\] 7.1.3 Conditional Probability Conditional probability, on the other hand, is a probability in which an event is conditioned on (or is constrained by) another event. In terms of density with one probability, we have the following expression: \\[\\begin{align} f_{X|Y}(x | y) = P_{X|Y}(X = x | Y = y) \\end{align}\\] It reads: Probability of X given Y. For example, if we have two baskets of fruits. One basket contains three apples and two oranges. The other basket contains one apple and four oranges. Given a basket, the probability of choosing an apple is: \\[ P(\\text{an apple}|\\text{1st basket}) = 3/5\\ \\ \\ \\ \\ \\ P(\\text{an apple}|\\text{2nd basket}) = 1/5 \\] Note here that the problem statement does not say that we randomly choose a basket. If it were so, then the computation becomes different: \\[\\begin{align*} {}&amp;P(\\text{an apple}|\\text{1st basket})P(\\text{1st basket}) = 3/5 \\times 1/2 = 3/10 \\\\ &amp;P(\\text{an apple}|\\text{2nd basket})P(\\text{2nd basket}) = 1/5 \\times 1/2 = 1/10 \\end{align*}\\] In terms of cumulative density with a range of probabilities, we have the following expression: \\[\\begin{align} \\mathcal{F}_{X|Y}(x|y) = \\begin{cases} \\sum_x P_{X|Y}(x|y) &amp; \\text{if x is discrete}\\\\ \\\\ \\int P_{X|Y}(x|y) dx &amp; \\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber41} \\end{align}\\] One rule worth mentioning as it involves the use of joint and conditional probabilities is the Chain rule, which is expressed in general as: \\[\\begin{align} P(X_1,X_2,...Xn) = P(X_1|X_2,...,X_n)P(X_2|X_3,...,X_n)P(X_{n-1}|X_n)P(X_n) \\end{align}\\] For example, the joint probability of two events using chain rule: \\[\\begin{align} P(X=x, Y=y) = P(x,y) = P(x|y)P(y) \\end{align}\\] In a Bayesian setting, if two random events are independent, then the joint probability enforces a product rule: \\[\\begin{align} P(X=x, Y=y) = P(x,y) = P(x)P(y) \\end{align}\\] For another example, the joint probability of three events using chain rule: \\[\\begin{align} P(X=x, Y=y, Z=z) = P(x,y,z) = P(x|y,z)P(y,z) = P(x|y,z)P(y|z)P(z) \\end{align}\\] 7.1.4 Negation Probability Negation probability is the complement (reverse) of a probability. For example, any of the following notations denotes this: \\[\\begin{align} P(X&#39;) = 1 - P(X)\\ \\ \\ \\ \\ \\text{negation} \\end{align}\\] \\[\\begin{align} P(Y|X&#39;)P(X&#39;)\\ \\ \\rightarrow\\ \\ \\ \\ P(Y|X)P(X)\\ \\ \\ \\ \\ \\text{negation} \\end{align}\\] \\[\\begin{align} P(X&#39;|Y,X) = P(X&#39;|Y)\\ \\ \\ \\ \\ \\text{negation} \\end{align}\\] 7.1.5 Combination of Probabilities We can use the different types of probabilities above to perform mathematical manipulation such as below. The Marginal probability is also expressed as such: \\[\\begin{align} \\text{Marginal prob.} = \\frac{\\text{Joint prob.}} {\\text{Conditional prob.}} \\ \\ \\rightarrow\\ \\ \\ P(X) = \\frac{P(X,Y)}{P(X|Y)} \\end{align}\\] The Joint Probability has the following expression: \\[\\begin{align} \\text{Joint prob.} = (\\text{Marginal prob.}) (\\text{Conditional prob.}) \\ \\ \\rightarrow\\ \\ \\ P(X,Y) = P(Y)P(X|Y) \\end{align}\\] The Conditional Probability has the following expression: \\[\\begin{align} \\text{Conditional prob.} = \\frac{\\text{Joint prob.}}{\\text{Marginal prob.}} \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ P(X|Y) = \\frac{P(X,Y)}{P(Y)} \\end{align}\\] For a continuous random variable X and a discrete random variable Y, we can obtain marginal probabilities such as: \\[\\begin{align} P(X) = \\int P(y) P(x | y) dy\\\\ P(Y) = \\int P(z) P(y | z) dz\\\\ P(Z) = \\int P(x) P(z | x) dx \\end{align}\\] For Joint probability of three continuous random variables X, Y, and Z, we can obtain lower-order marginal probabilities and lower-order joint probabilities, respectively, this way: \\[\\begin{align} P(X) {}&amp;= \\int \\int P(x, y, z) dy dz\\ \\ \\ \\ \\ \\ \\ \\ P(X,Y) = \\int \\int P(x, y, z) dz\\\\ P(Y) &amp;= \\int \\int P(x, y, z) dx dz\\ \\ \\ \\ \\ \\ \\ \\ P(Y,Z) = \\int \\int P(x, y, z) dx\\\\ P(Z) &amp;= \\int \\int P(x, y, z) dx dy\\ \\ \\ \\ \\ \\ \\ \\ P(Z,X) = \\int \\int P(x, y, z) dy \\end{align}\\] For Joint probability of two continuous random variables X, Y given Z, we can obtain a conditional probability: \\[\\begin{align} P(X|Y) {}&amp; = \\int P(X,Z|Y) dz\\\\ P(Y|Z) {}&amp; = \\int P(Y,X|Z) dx\\\\ P(Z|X) {}&amp; = \\int P(Z,Y|X) dy \\end{align}\\] We can also obtain a marginalized probability with the other combinations: \\[\\begin{align} P(X) {}&amp;= \\int \\int P(z)P(y|z)P(x|y,z) dy dz = \\int \\int P(y)P(z|y)P(x|z,y) dz dy\\\\ P(Y) &amp;= \\int \\int P(x)P(z|x)P(y|z,x) dz dx = \\int \\int P(z)P(x|z)P(y|x,z) dx dz\\\\ P(Z) &amp;= \\int \\int P(y)P(x|y)P(z|x,y) dx dy = \\int \\int P(x)P(y|x)P(z|y,x) dy dx \\end{align}\\] 7.2 Probability Rules Let us review a few rules and theorems that govern Probabilities. 7.2.1 Law of Total Probability The axiom says that the total of all probabilities in a given sample space (or a population) equates to 1. In another way to say it, a sample space denoted as \\(\\mathbf{\\Omega}\\) refers to the set of all possible outcomes in which the sum of all probabilities equates to 1. A few of the notations below reflect the axiom above: \\[\\begin{align} P(X) + P(X&#39;) = 1 \\end{align}\\] It reads: the probability of X plus the probability of the rest (the complement of X) equals 1. \\[\\begin{align} P(X|Y) + P(X&#39;|Y) = 1 \\end{align}\\] It reads as the probability of X given Y plus the probability of the rest (the complement of X) given the same B equals 1. \\[\\begin{align} P(X,Y) = P(X|Y)P(Y) = P(Y,X) = P(Y|X)P(X) \\end{align}\\] \\[\\begin{align} P(X|Y&#39;) = \\frac{P(X,Y&#39;)}{P(Y)} = \\frac{P(X) - P(X,Y)}{1-P(Y)} \\end{align}\\] Let us use R code to simulate a sample that is distributed as a binary distribution: \\[\\begin{align} X \\sim Bin(n, \\rho) \\end{align}\\] set.seed(142) # Simulate a binary sample of size=1 and of 20 data points # with a success probability of 30%. (X = rbinom(n=20, size=1, prob=0.30)) ## [1] 1 0 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 0 0 # Proportion of the sample that has value = 1 mean(X==1) ## [1] 0.35 # Proportion of the sample that has value = 0 mean(X==0) ## [1] 0.65 Based on the law of total probability, we have the following: \\[P(X=1) = 0.35\\] \\[P(X=0) = 0.65\\] therefore: \\[P(X=1) + P(X=0) = 0.35 + 0.65 = 1\\] Note that this is also called the Sum Rule. 7.2.2 Law of Total Expectation Expectation refers to the expected value of a random variable. The average value (the mean) and expected value are often interchangeable. In that respect, Expectation can also be considered as one of the central tendencies in statistics and has the familiar notation as: \\[\\begin{align} \\mathbb{E}(X) = \\mu = \\frac{1}{n}\\sum_{i=1}^n X_i \\end{align}\\] However, there are cases in which the expected value is the weighted sum of the random variable, e.g.: \\[\\begin{align} \\mathbb{E}(X) = \\underbrace{\\sum_{i=1}^n X_i \\omega_i(X_i)}_{\\text{Arbitrary Weight}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbb{E}(X) = \\underbrace{\\sum_{i=1}^n X_i P_i(X_i)}_{ \\begin{array}{l} \\text{Weighted based} \\\\ \\text{on Probability} \\end{array} \\label{eqn:eqnnumber310} } \\end{align}\\] The Law of Total Expectation (or Total Mean) is also known as the Law of iterated expectation or conditional expectation and is expressed as such: \\[\\begin{align} \\mathbb{E}(Y) = \\mathbb{E}_X(\\mathbb{E}_Y(Y|X)\\ ) \\end{align}\\] The expression \\(\\mathbb{E}_Y(Y|X)\\) is interpreted as the expected value of Y given X. It has the following derived formula: \\[\\begin{align} \\mathbb{E}_Y(Y|X) = \\sum_{y \\in R(Y)} y \\times P(Y=y|X) \\end{align}\\] Note that \\(R(Y) = Range(Y)\\) contains all possible values of \\(Y\\) and does not have to be sequential or ordinal. The expression \\(\\mathbb{E}_X(\\mathbb{E}_Y(Y|X)\\ )\\) is interpreted as the conditional expectation (or conditional expected value) of Y given X. It can also be expressed in its general form, given a sample space \\(\\Omega = \\{ x_1, x_2, x_3,...,x_n\\}\\): \\[\\begin{align} \\mathbb{E}(Y) {}&amp;= \\sum_{x \\in \\Omega} \\mathbb{E}(Y|X = x)\\times P_X(x) \\\\ &amp;= \\sum_{x \\in \\Omega} \\sum_{y \\in R(Y)} y \\times P_{Y|X}(y|x)\\times P_X(x)\\\\ &amp;= \\sum_{y \\in R(Y)}\\sum_{x \\in \\Omega} y \\times P_{Y|X}(y|x)\\times P_X(x)\\\\ &amp;= \\sum_{y \\in R(Y)} y \\times P_{Y}(y) \\end{align}\\] where \\(P_X(x_{1}) + P_X(x_{2})\\ + ...\\ + P_XP(x_{n})\\) = 1. To illustrate, suppose we have three challenging problems to solve; each one bears distinct weight in terms of staffing requirements. All three problems require ten, twenty, and thirty staffing hours, respectively, to solve. All else being equal, the probability of solving the first, second, and third problems is 35%, 25%, and 40%, respectively. Estimate the expected number of staffing hours to solve the problem. Let: S be the solutions to the problem, and H be the staffing hours. \\[\\begin{align} \\mathbb{E}(H) {}&amp;= \\mathbb{E}(H|S_1)P(S_1) + \\mathbb{E}(H|S_2)P(S_2) + \\mathbb{E}(H|S_3)P(S_3)\\\\ &amp;= 10 \\times 0.35 + 20 \\times 0.25 + 30 \\times 0.40 \\nonumber \\\\ &amp;= 20.5\\ \\text{manpower hours} \\nonumber \\end{align}\\] 7.2.3 Law of Total Variance The Law of Total Variance is also known as Law of iterated variance or conditional variance and is expressed as: \\[\\begin{align} Var(Y) {}&amp;= Var(Y_{within}|X) + Var(Y_{between}|X) \\\\ &amp;= \\mathbb{E}(Var(Y|X)) + Var(\\mathbb{E}(Y|X)) \\end{align}\\] Here, the average variance of Y given X is expressed as: \\[\\begin{align} Var(Y_{within}|X) = \\mathbb{E}(Var(Y|X)) \\end{align}\\] However, that may not be enough to compute the total variance. That is because we computed the variance within each sample but not between samples. We need to compute the variance between the individual sample means - this is the variability of the mean of \\(Y\\) given \\(X\\). \\[\\begin{align} Var(Y_{between}|X) = Var(\\mathbb{E}(Y|X)) \\end{align}\\] To derive the within-sample variance from law of total expectation, we have the following: \\[\\begin{align} Var(Y_{within}|X) {}&amp;= \\mathbb{E}(Var(Y|X)) = \\mathbb{E}\\left[\\mathbb{E}(Y^2|X)\\right] - \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2 \\right]\\\\ &amp;=\\mathbb{E}(Y^2) - \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2\\right] \\end{align}\\] To derive the between-sample variance from the law of total expectation, we have the following: \\[\\begin{align} Var(Y_{between}|X) {}&amp;= Var(\\mathbb{E}(Y|X)) = \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2 \\right] - \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2 \\right]^2\\\\ &amp;= \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2\\right] - \\mathbb{E}(Y)^2 \\end{align}\\] Therefore, combining the two variances, we get: \\[\\begin{align} Var(Y) {}&amp;= \\left( \\mathbb{E}(Y^2) - \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2\\right] \\right) + \\left( \\mathbb{E}\\left[\\mathbb{E}(Y|X)^2\\right] - \\mathbb{E}(Y)^2 \\right)\\\\ &amp;= \\mathbb{E}(Y^2) - \\mathbb{E}(Y)^2 \\end{align}\\] 7.2.4 Law of Total Covariance Similarly, if we decompose the total covariance, we also get two terms: \\[\\begin{align} Cov(Y,X) = \\mathbb{E}(\\ Cov(Y,X|Z)\\ ) + Cov(\\ \\mathbb{E}(Y|Z),\\mathbb{E}(X|Z)\\ ) \\end{align}\\] The first term is the average covariance within samples, and the second is the covariance between samples. 7.2.5 Law of Large Numbers The theorem has it that if we keep repeating our trials up to a considerably large number of repetitions (perhaps closer to infinity), at some point, our test results will get closer to the actual value. At times, this can be regarded as the fate of the average if our test result is about getting the average mean. In effect, errors or biases decrease and cancel each other out. 7.2.6 Central Limit Theorem We start from the premise that any single sample of a population does not accurately reflect the actual value of the entire population. If we are to compute for the average (or mean) from one single sample, the result may not necessarily reflect the average (or mean) from another sample, and another, and another. Therefore, it is proper to aggregate the mean of many samples to get closer to the actual value of the mean - in fact, the larger the number of samples, the more accurate we may get to the true mean. Another way to say this is that as we process a more significant number of samples, the sample means start to follow a normal distribution. 7.2.7 Rule of Independence In a multivariate setting in which we deal with more than one random variable, e.g., X, Y, Z, it helps to know if they are independent of the others and, perhaps more so, still share the same distribution characteristic - following the same (identical) distribution. Such property of a random variable is called independent and identical distribution or commonly written as IID. It is easy to point out that while a random variable forms a distribution, it is probabilistically independent if it does not affect the outcome of other random variables. However, to say that a random variable is both independent and identical with respect to other random variables, such property has to prove, based on central limit theorem, that the average mean of a collection of random variables tends to form a normal distribution. Now, for two independent random variables, \\(X \\in U\\) and \\(Y \\in V\\), their joint probability can be expressed as: \\[\\begin{align} P(X,Y) = P(X)\\cdot P(Y) \\end{align}\\] For the same two independent random variables, X and Y, their conditional probability can be expressed as: \\[\\begin{align} P(X|Y) = P(X)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where P(X) is marginal probability for Y} \\end{align}\\] Given three independent random variables, X and Y, and Z, the conditional probability of X and Y, given Z, is expressed as: \\[\\begin{align} P(X,Y\\ |\\ Z) = P(X|Z)\\cdot P(Y|Z) \\end{align}\\] 7.2.8 Rule of Exchangeability When it comes to being exchangeable - or the exchangeability of random variables, any unforeseen data for the random variables will follow the same sequence or pattern as existing data regardless of order or arrangement. In any case, exchangeable random variables are conditionally independent. In a sampling distribution where \\(\\theta = \\{\\theta_{1},\\theta_{2}, \\theta_{3},...,\\theta_{n}\\}\\) and ordering of \\(\\theta_{i}\\) can be exhangeable if it does not affect the overall \\(\\theta\\) distribution. Here are a few other items to be aware of: IIDs are exchangeable. Exchangeable sequences are not necessarily IIDs However, every exchangeable sequence is an identical distribution but does not necessarily require independence. 7.2.9 Rule of Expectation and Variance The expected value of a marginal probability of one random variable is expressed as: \\[\\begin{align} \\mathbb{E}(X) = \\begin{cases} \\sum_{x} xf(x) &amp; \\text{if x is discrete}\\\\ \\\\ \\int_{-\\infty}^{\\infty} xf(x)dx &amp;\\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber42} \\end{align}\\] and if a random variable is represented by a function, g(X), then we have the following expression: \\[\\begin{align} \\mathbb{E}[g(X)] = \\begin{cases} \\sum_{x} g(x)f(x) &amp; \\text{if x is discrete}\\\\ \\\\ \\int_{-\\infty}^{\\infty} g(x)f(x)dx &amp; \\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber43} \\end{align}\\] The expected value of a joint probability of two random variables of which the variables are represented by a function, g(X,Y), is expressed as: \\[\\begin{align} \\mathbb{E}[g(X,Y)] = \\begin{cases} \\sum_{x} \\sum_{y} g(x,y)f(x,y) &amp; \\text{if x is discrete}\\\\ \\\\ \\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} g(x,y)f(x,y)dx dy &amp; \\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber44} \\end{align}\\] The expected value of a conditional probability of two random variables, X and Y, of which X is represented by a function, g(X), is expressed as: \\[\\begin{align} \\mathbb{E}[g(X) | Y] = \\begin{cases} \\sum_{x} g(x)f(x|y) &amp; \\text{if x is discrete}\\\\ \\\\ \\int_{-\\infty}^{\\infty} g(x)f(x|y)dx &amp; \\text{if x is continuous} \\end{cases} \\label{eqn:eqnnumber45} \\end{align}\\] The variance of a conditional probability is written as: \\[\\begin{align} Var(X|(f(x))) = \\mathbb{E}\\left[\\left(X - \\mathbb{E}(X|f(x))\\right)\\right]^2 \\end{align}\\] 7.3 Bayes Theorem The formula for Bayes Rule (or Bayes Theorem) is expressed this way (Bruyninckx H. 2002): \\[\\begin{align} \\text{Posterior} = \\frac{\\text{Likelihood}\\times \\text{Prior}}{\\text{Marginal Likelihood}}\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ P(X|Y) = \\frac{P(Y|X)\\times P(X)}{P(Y)} \\end{align}\\] where \\(P(Y|X)P(X)\\) is an expanded version of joint probability: \\[\\begin{align} P(X,Y) = P(Y|X)\\times P(X) \\end{align}\\] and where \\(P(Y)\\) is the observed probability (the evidence) and can be expanded as: \\[\\begin{align} P(Y) = P(Y|X)\\times P(X) + P(Y|X&#39;)\\times P(X&#39;) \\end{align}\\] Note that marginal likelihood is also called evidence. Therefore, we have: \\[\\begin{align} P(X|Y) = \\frac{P(X,Y)}{P(Y)} = \\frac{P(Y|X)\\times P(X)}{P(Y|X)\\times P(X) + P(Y|X&#39;)\\times P(X&#39;)} \\end{align}\\] To illustrate, let us use Figure 7.1. Note that, hereafter, we also use the figure to illustrate other concepts around estimation and inference in later chapters, such as the Variational Inference. Figure 7.1: Bi-variate Mixture Model Figure 7.1 shows diagram of a bivariate mixture model with three circular contours representing a cluster (or classification), namely \\(\\mathbf{c} = \\{ c_1, c_2, c_3\\}\\). There are two random variables, namely \\(X1\\) and \\(X2\\). Each random variable is characterized by a univariate gaussian mixture distribution - it is a mixture of possibly three individual gaussian distributions. We start with the prior of the Bayes Theorem, which is written, in this case, as the initial proportion of each classification like so: \\[\\begin{align} P(c_1) = \\pi_1 = \\frac{ c_1}{ \\sum_i^k c_i}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(c_2) = \\pi_2 = \\frac{ c_2}{ \\sum_i^k c_i}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(c_3) = \\pi_3 = \\frac{ c_3}{ \\sum_i^k c_i} \\end{align}\\] Then we follow that by computing the likelihood. For that, we first calculate the probability of a random event (a random variable) belonging to a cluster. \\[\\begin{align} \\begin{array}{ll} P(x^{(1)}|c_1) \\sim \\mathcal{N}\\left(\\mu^{(1)}_1, \\sigma^{2(1)}_1\\right) &amp; P(x^{(2)}|c_1) \\sim \\mathcal{N}\\left(\\mu^{(2)}_1, \\sigma^{2(2)}_1\\right)\\\\ P(x^{(1)}|c_2) \\sim \\mathcal{N}\\left(\\mu^{(1)}_2, \\sigma^{2(1)}_2\\right) &amp; P(x^{(2)}|c_2) \\sim \\mathcal{N}\\left(\\mu^{(2)}_2, \\sigma^{2(2)}_2\\right)\\\\ P(x^{(1)}|c_3) \\sim \\mathcal{N}\\left(\\mu^{(1)}_3, \\sigma^{2(1)}_3\\right) &amp; P(x^{(2)}|c_3) \\sim \\mathcal{N}\\left(\\mu^{(2)}_3, \\sigma^{2(2)}_3\\right)\\\\ \\end{array} \\label{eqn:eqnnumber47} \\end{align}\\] Then we construct the likelihood: \\[\\begin{align} \\begin{array}{l} P(X|c_1) = P(x^{(1)},x^{(2)}| c_1 ) = P(x^{(1)}| c_1 )P(x^{(2)}| c_1 )\\\\ P(X|c_2) = P(x^{(1)},x^{(2)}| c_2 ) = P(x^{(1)}| c_2 )P(x^{(2)}| c_2 )\\\\ P(X|c_3) = P(x^{(1)},x^{(2)}| c_3 ) = P(x^{(1)}| c_3 )P(x^{(2)}| c_3 ) \\end{array} \\label{eqn:eqnnumber48} \\end{align}\\] Finally, we construct the posterior: \\[\\begin{align} P(c_1|X) = \\frac{P(X|c_1)P(c_1)} {\\sum_i P(X|c_i)P(c_i)} \\ \\ \\ \\ \\ \\ \\ \\ P(c_2|X) = \\frac{P(X|c_2)P(c_2)} {\\sum_i P(X|c_i)P(c_i)} \\nonumber \\\\ P(c_3|X) = \\frac{P(X|c_3)P(c_3)} {\\sum_i P(X|c_i)P(c_i)} \\end{align}\\] To illustrate the Bayes Theorem, suppose we perform an analysis of strange flu-like symptoms and a particular pathogen currently floating around a small local community. Suppose, after reviewing historical records, that 25% of the locals reported similar flu-like symptoms in the past. However, in testing individuals, we find that 10% are positive for the pathogen. Additionally, 3% of the locals with flu-like symptoms also carry the pathogen. Let us determine the probability of locals being positive for flu-like symptoms. Let X be locals with flu-like symptoms and Y be locals tested positive for a particular pathogen. Therefore: \\[\\begin{align*} P(X) {}&amp;= 25\\% = 0.25\\\\ P(Y) &amp;= 10\\% = 0.10\\\\ P(Y|X) &amp;= 3\\% = 0.03 \\end{align*}\\] Using Bayes Theorem, we compute the following: \\[ P(X|Y) = \\frac{P(Y|X)\\times P(X)}{P(Y)} = \\frac{(0.03)\\times (0.25)}{(0.10)} = 0.075 = 7.5\\% \\] That means that the probability of developing flu-like symptoms for subjects found to have the pathogen is 7.5%. 7.3.1 Naïve Bayes Naïve Bayes is the normalized form of Bayes Theorem formula. Recall the following Bayes Formula: \\[\\begin{align} P(X|Y) = \\frac{P(X,Y)}{P(Y)} = \\frac{P(Y|X) \\times P(X)}{P(Y|X) \\times P(X) + P(Y|X&#39;)\\times P(X&#39;)} \\end{align}\\] With multivariate probabilities, the equation expands into the following: \\[\\begin{align} P(X_{1}, X_{2},X_{3},...|Y) = \\frac{P(X_{1},X_{2},X_{3},...,Y)}{P(Y)} \\end{align}\\] where \\(P(Y)\\) is a normalizing (scaling) factor. For discrete densities, the normalizing factor is summed up using the following expression: \\[\\begin{align} P(Y) = \\sum_{X_{i}=1}^{N_{1}}\\sum_{X_{2}=1}^{N_{2}} \\sum_{X_{3}=1}^{N_{3}} ... P(X_{1},X_{2},X_{3},...,Y) \\end{align}\\] For continuous densities, the normalizing factor integrates using the following expression: \\[\\begin{align} P(Y) = \\int_{-\\infty}^{\\infty}\\int_{-\\infty}^{\\infty} \\int_{-\\infty}^{\\infty} ... P(X_{1},X_{2},X_{3},...,Y)...dX_{3}dX_{2}dX_{1} \\end{align}\\] Note that every observed density added becomes a challenge to integrate. As a result, it can lead to being intractable. Fortunately, we use \\(P(Y)\\) for normalization, and so with or without it, the proportionality remains intact (which we can use for approximation). Because of that, we end up with the below Naïve formula: \\[\\begin{align} \\underbrace{P(X|Y)}_\\text{posterior}\\ \\propto\\ \\underbrace{P(Y|X)}_\\text{likelihood} \\times \\underbrace{P(X)}_\\text{prior} \\end{align}\\] It is also common to see the following equivalent notation to emphasize evidence versus hypothesis: \\[\\begin{align} P(H|E) \\propto P(E|H) \\times P(H) \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ P(H|E) = \\frac{ P(E|H) \\times P(H) }{ P(E)} \\end{align}\\] The notation starts with the premise that we need to evaluate the state (or probability) of a hypothesis given a set of observations - our evidence. This is our posterior probability denoted as \\(P(H|E)\\). Our posterior hypothesis remains to be weak until proven to hold. Here, weak may refer to being subjective (or even anecdotal) in describing (or proving) a piece of evidence. Our goal is apparently to collect more evidence (e.g., perform more clinical tests). If we can collect more (objective) evidence backed by our (subjective) prior hypothesis, we can give weight to the collected evidence, generating a newly updated posterior knowledge which becomes less weak and closer to describing a piece of objective evidence; nonetheless, it is a newly updated information. In a sense, we begin to see the scheme of things. A prior knowledge becomes a posterior knowledge for every new evidence, and a posterior knowledge becomes a prior knowledge for the next evidence. To illustrate, when testing positive for symptoms (or signs) of a particular disease (whether cancer or COVID-19 infection as an example), we find that complex solutions tend to root or reference back the below fundamental equation. Note that symptoms (or signs) of disease remains hypothetical until tested positive: \\[\\begin{align} P(\\text{symptom}|\\mathbf{+}) \\propto P(\\mathbf{+}|\\text{symptom}) \\times P(\\text{symptom}) \\end{align}\\] where normalizing factor \\(P(+)\\) is omitted: \\[\\begin{align} P(+) = P(\\mathbf{+}|\\text{symptom})\\times P(\\text{symptom}) + P(\\mathbf{+}|\\text{no symptom})\\times P(\\text{no symptom}) \\end{align}\\] Let us continue to further our discussion of hypothesis in the next section. Note that our description of hypothesis in the next section focuses on parameter estimation using the theta \\(\\theta\\) symbol. 7.3.2 Likelihood In this section, it is essential to distinguish two statements that can help us understand likelihood: The likelihood of observing data given a sampled distribution. The most likely distribution that produces observed data. Both statements focus around the parameter theta \\(\\theta\\) for a data distribution. The symbol theta \\(\\theta\\) represents a set (or vector) of parameter quantities, particularly the mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)) for a normal distribution. Note that other types of distributions have their own corresponding set of parameters for theta, \\(\\theta\\). Here, we use normal distribution to explain a case. We begin our discussion by mentioning - in the Bayesian sense - that there is uncertainty in the parameter theta \\(\\theta\\). The uncertainty comes about because of the idea, for example, that the average height (the mean height) of the world population is difficult to exact because it requires a global census which is impossible to achieve. For that reason, we settle on our observations - a sampled data limited in terms of the distribution it follows in comparison to the actual distribution of an entire population. We use Figure 7.2 as reference. Figure 7.2: Likelihood It is fair to assume that, given a distribution characterizes by \\(\\theta = (\\mu=2, \\sigma=1)\\), the likelihood of observing data (e.g. x=3) is 0.242. This is illustrated by the left-side diagram. dnorm(x=3, mean=2, sd= 1) ## [1] 0.242 It is also fair to assume that, given a list of distribution - for example, a distribution characterized by \\(\\theta = (\\mu=2, \\sigma=1)\\) and another distribution characterized by \\(\\theta = (\\mu=3, \\sigma=1)\\), the one distribution that most likely produces the data (e.g. x=3) has an assumed maximum likelihood estimate (MLE) that is equal to 0.3989. This is illustrated by the right-side diagram which references our second statement. dnorm(x=3, mean=3, sd= 1) ## [1] 0.3989 As we note, we assume an initial MLE in the diagram. We know that this is not necessarily accurate because of the insufficient number of sampling distributions. Regarding likelihood, we are most often interested in knowing which sample distribution closely relates to the actual distribution (such as one representing the population). We can achieve such a close estimate by way of MLE. We expand on the concept of MLE in the Bayesian Inference section. As we expound further, likelihood comes into the picture every time we have limited access to a complete set of data, albeit the parameter \\(\\theta\\) is known. To illustrate, assume by luck that we have a distribution that is as complete a set as we can gather. Consider this to be almost the actual distribution. Ignore that we are using sampling to generate our actual distribution. Let us take the mean and variance - our parameter model - like so: set.seed(2020) range = seq(1, 50) true.distribution = sample(range, size=1000, prob=NULL, replace=TRUE) ( theta = c(&quot;mean&quot; = mean(true.distribution), &quot;sd&quot; = sd(true.distribution))) ## mean sd ## 25.11 14.45 The characteristic of the true distribution is recorded as such: \\(\\theta\\) = (\\(\\mu\\) = 25.11, \\(\\sigma\\) = 14.45). Suppose now that we have limited access to the actual data in that we only have a significantly small portion of the data set. Let us start with three small samples. See below: set.seed(2020) # suppose each sample is IID (identical distribution) sample1 = sample(true.distribution, size=7, prob=NULL, replace=TRUE) sample2 = sample(true.distribution, size=4, prob=NULL, replace=TRUE) sample3 = sample(true.distribution, size=3, prob=NULL, replace=TRUE) samples = matrix( c( paste0(sample1, collapse=&quot;, &quot;), round(mean(sample1),3), round(sd(sample1),3), paste0(sample2, collapse=&quot;, &quot;), round(mean(sample2),3), round(sd(sample2),3), paste0(sample3, collapse=&quot;, &quot;), round(mean(sample3),3), round(sd(sample3),3) ) , nrow=3, ncol=3, byrow=TRUE) colnames(samples) = c(&quot;observations&quot;, &quot;sample mean&quot;, &quot;sample sd&quot;) rownames(samples) = c(&quot;sample.1&quot;, &quot;sample.2&quot;, &quot;sample.3&quot;) as.data.frame(samples) ## observations sample mean sample sd ## sample.1 40, 16, 22, 28, 27, 29, 31 27.571 7.458 ## sample.2 7, 31, 27, 14 19.75 11.177 ## sample.3 37, 32, 21 30 8.185 Notice how the mean and standard deviation for each sample are very disproportionate compared to the actual mean and standard deviation of the true data distribution. See Figure 7.3. Figure 7.3: Disproportioned parameters In Figure 7.3, we have one single observation, namely \\(x=14\\). Let us get the likelihood of the four distributions: ## sample mean sample sd likelihood ## population 25.11 14.446 0.020546 ## sample.1 27.57 7.458 0.010216 ## sample.2 19.75 11.177 0.031269 ## sample.3 30.00 8.185 0.007213 At first glance, the data, x=14, is likely to be observed from each of the three sample distributions; however, we see that sample.2 has the most likelihood of observing data (x=14) at 0.0072. Therefore, we can assume that sample.2 is the closest estimate (or can be a good representative) of the actual distribution. In other words, sample.2 has a better parameter model that can characterize (or can be proportional to) the actual distribution. However, we have to note that our assumption is based only on one observation (x=14) and on only three samplings. Our goal is to find a better parameter model proportional to that of the actual distribution, but we need sufficient observations and sufficient samplings. We need to maximize the likelihood that a set of observed data (not just one data point) belongs to a sample distribution (of a list of sample distributions) represented by theta \\(\\theta\\) that hopefully can characterize the actual distribution. As we pointed out, we can achieve this using MLE. Now, likelihood is expressed in the form of a PDF formula: \\[\\begin{align} Lik(\\theta|X) \\equiv P(X|\\theta)\\ \\ \\ \\leftarrow \\ \\ f(x; \\theta)\\ \\ or\\ \\ f(x; \\mu, \\sigma^2) \\end{align}\\] The \\(Lik(\\theta|X)\\) is the likelihood function for the probability of a random event to occur, given that we have observed such occurrence. Notice that the parameters in the likelihood notation get swapped (for semantics), and we use the equivalence (\\(\\equiv\\)) notation. While the meaning or interpretation is different, we compute the likelihood using the equation from Density function. However, to avoid confusing this equation as a Density function, we can call it a Likelihood function. Also, a Density function forms the curve (as we discussed in Chapter 5 (Numerical Probability and Distribution)), and the probability forms the area in the curve. In the case of Likelihood function, it is a function of theta \\(\\theta\\) conditioned on the observed data. We can perform point estimates for marginal sampling density or joint density likelihood like so: \\[\\begin{align} Lik(\\mu, \\sigma^2 | X = x_1) {}&amp;\\equiv P(X = x_1| \\mu, \\sigma^2 ) &amp; \\text{marginal-density likelihood} \\\\ \\nonumber \\\\ Lik(\\mu, \\sigma^2 | x_1,\\ ....,\\ x_n) &amp;\\equiv P(x_1,\\ ....,\\ x_n| \\mu, \\sigma^2 ) &amp; \\text{joint-density likelihood} \\end{align}\\] MLE works best with joint-density likelihood. The formula for both likelihoods for different types of densities is provided in later sections under Conjugacy and also in Table 7.2. The formula helps to compute the joint-density likelihood. Recall that the \\(\\mu\\) parameter controls the location of the bell-shape curve and that the \\(\\sigma\\) parameter controls the scale (or spread) of the curve. In terms of evaluating multiple sampling densities, it is easy to visualize the shape of each sample density curve based on the location (via the mean) and the spread (via the variance). To illustrate the likelihood of observing a single data point, namely \\(x = 71\\), given known constant standard deviation, namely \\(\\sigma = 2.5\\) and an unknown mean \\(\\mu\\), let us treat \\(\\mu\\) as a random variable and plug in a few random values for the mean like so: \\(\\mu = (67,\\ 68.5,\\ 70.5,\\ 72.5)\\). Then, we use Figure 7.4 to show four bell-shaped curves for each of the random \\(\\mu\\) values. Figure 7.4: Likelihood (constant variance but variant mean) The likelihood estimate of observing \\(x=71\\) for each of the means \\(\\mu\\) is as follows: \\(Lik\\mu_1\\) = 0.0444, \\(Lik\\mu_2\\) = 0.0968, \\(Lik\\mu_3\\) = 0.1564, \\(Lik\\mu_4\\) = 0.1333. We derive that from the following example implementation of likelihood using our likelihood function for normal distribution: normal.likelihood &lt;- function(x, mean, sd) { # for normal outcome variance = sd^2 (1 / (sqrt( 2 * pi * variance ))) * exp(-(x - mean)^2/(2 * variance)) } # log likelihood for joint density loglikelihood &lt;- function(density) { loglik = log(density, exp(1)) -sum(loglik) # for iid } For binomial likelihood, we have the following implementation: binomial.likelihood &lt;- function(x, rho, n) { # for binomial outcome rho^x * ( 1 - rho)^(n - x) # rho = probability of success } For poisson likelihood, see Chapter 10 (Computational Learning II) under Poisson Regression Subsection under Regression Section. Using the normal likehood, we get the following: (likelihood = normal.likelihood(x, mean = mean, sd = sigma)) ## [1] 0.04437 0.09679 0.15642 0.13329 From there, we take the maximum likelihood (assumed MLE) for the mean: (MLE = max(likelihood)) ## [1] 0.1564 Finally, we get the optimal paramater - the mean \\(\\hat{\\mu}\\) - based on the MLE: (mean.est = mean[ which.max(likelihood) ] ) # MLE ## [1] 70.5 Because we deal with a random variable for mean, we can also visualize its distribution. Note that this is mean distribution and not data distribution. We show the ML, which is located at the peak of the bell-shaped curve where the slope is zero. See Figure 7.5. Figure 7.5: Likelihood (parameter mean distribution) In other words, by maximizing the likelihood, we can get a parameter model, namely the mean \\(\\mu\\) = 70.5 and standard deviation \\(\\sigma\\) = 2.5, that describes the sample distribution as a possible representative of (and possibly proportional to) our actual distribution. We can also say that this distribution is the one that produced our observed data. Next, to illustrate the likelihood of observing a single data point, namely \\(x = 71\\), given known constant mean, namely \\(\\mu = 71\\) and an unknown variance \\(\\sigma^2\\), let us treat \\(\\sigma\\) as a random variable and plug-in a few random values like so: \\(\\sigma = (1.5,\\ 2.0,\\ 2.5,\\ 3.5)\\). See Figure 7.6 for each of the random \\(\\sigma\\) values. Figure 7.6: Likelihood (constant variance but variant mean) Similarly, the probability of observing \\(x=71\\) for each of the variance \\(\\sigma^2\\) is as follows: \\(Lik\\sigma_1\\) = 0.266, \\(Lik\\sigma_2\\) = 0.1995, \\(Lik\\sigma_3\\) = 0.1596, \\(Lik\\sigma_4\\) = 0.114. (likelihood = normal.likelihood(x, mean = mean, sd = sd)) ## [1] 0.2660 0.1995 0.1596 0.1140 From there, we take the maximum likelihood (alsu assumed MLE) for variance: (MLE = max(likelihood)) ## [1] 0.266 Finally, we get the optimal parameter - the variance \\(\\hat{\\sigma}^2\\) estimate (or sd \\(\\hat{\\sigma}\\) estimate for that matter) - based on our MLE: (sd.est = sd[ which.max(likelihood) ] ) #MLE ## [1] 1.5 Because we deal with a random variable for variance, we can also visualize its distribution and show the assumed MLE at the peak of the curve. See Figure 7.7. Figure 7.7: Likelihood (parameter variance distribution) For a joint-density likelihood, let us introduce negative log-likelihood (NLL) - a counterpart version of ML. We use NLL to avoid multiplication overflows (but more importantly, for our loss function - see Chapter 9 (Computational Learning I) under the General Modeling section. Also, for more discussion around maximum estimation and negative log-likelihood (NLL), see parameter estimation in a later section. To compute for joint-density likelihood, let us first suppose that the known parameters are derived from an actual distribution (these parameters become our reference for validation): c(&quot;known mean&quot;=true.mean, &quot;known sd&quot;= true.sigma) ## known mean known sd ## 50.222 2.534 Next, let us perform sampling. In the later section, we discuss a list of more advanced sampling techniques such as the Markov Chain Monte Carlo (MCMC) technique using JAGS. For now, to perform a simple sampling, let us use the rnorm() function: (sample = round( rnorm(n = 10, mean = true.mean, sd = true.sigma), 3)) ## [1] 48.06 52.53 53.25 49.28 49.91 54.78 54.54 42.52 44.42 50.37 The sample may provide the initial estimates of the parameters (they are quite close to the known parameters): c(&quot;initial mean&quot; = mean(sample), &quot;initial sd&quot;= sqrt( var(sample) ) ) ## initial mean initial sd ## 49.966 4.108 Suppose now that we only know the variance \\(\\sigma\\) = 2.5339. The mean \\(\\mu\\) is unknown. Let us compute for the mean estimate \\(\\hat{\\mu}\\) of the true mean \\(\\mu\\). loglike.mean = c() for (mu in mean) { lk.mean = loglikelihood( normal.likelihood(sample, mean=mu, sd = true.sigma) ) loglike.mean = cbind(loglike.mean, lk.mean ) } loglike.mean = round(loglike.mean, 3) The overall NLL for mean and its corresponding value \\(\\mu^*\\): mean.nll = round( min(loglike.mean), 3) mean.est = round( mean[ which.min(loglike.mean) ], 3) c(&quot;NLL&quot;= mean.nll, &quot;mean&quot; = mean.est) ## NLL mean ## 31.0 50.9 Next, suppose that we only know the mean \\(\\mu\\) = 50.2218. The variance \\(\\sigma\\) is unknown. Let us solve for the \\(\\sigma\\). loglike.sd = c() for (sd in sigma) { lk.sigma = loglikelihood( normal.likelihood(sample, mean= true.mean, sd = sd) ) loglike.sd = cbind(loglike.sd, min(lk.sigma) ) } loglike.sd = round(loglike.sd, 3) The overall NLL for sd and its corresponding value \\(\\sigma^*\\): sd.nll = round ( min(loglike.sd), 3) sd.est = round( sigma[ which.min(loglike.sd)], 3) c(&quot;NLL&quot;= sd.nll, &quot;sd&quot;= sd.est) ## NLL sd ## 28.014 3.412 Here is a table of the likelihood for mean and variance with corresponding overall NLL (see Table 7.1). Table 7.1: Parameter Estimation mean minimum NLL (\\(\\theta_{\\mu}^*\\)) sd minimum NLL (\\(\\theta_{\\sigma}^*\\)) 54.70 47.76 2.910 28.88 50.90 31.00 2.859 29.02 54.29 44.84 3.073 28.49 52.14 34.01 2.055 34.45 47.03 37.03 2.018 34.95 45.98 42.71 2.344 31.59 46.92 37.56 3.412 28.01 50.90 31.00 2.633 29.87 45.04 49.23 2.369 31.40 54.32 45.10 1.666 41.78 where: minimum NLL (\\(\\theta_{\\mu}^*\\)) \\(\\rightarrow\\) min (-\\(nl Lik (\\theta_{\\mu}^* = 50.902| X )) =30.998\\) and minimum NLL (\\(\\theta_{\\sigma}^*\\)) \\(\\rightarrow\\) min (-\\(nl Lik (\\theta_{\\sigma}^* = 3.412 | X )) =28.014\\). Let us introduce a few family of joint likelihood formulas (suppose data is IID): Table 7.2: Family of Likelihood Family — \\(Lik (\\theta\\)|\\(x_1,...,x_n\\)) \\(logLik (\\theta\\)|\\(x_1,...,x_n\\)) \\(\\theta\\) Uniform \\(\\prod_{i=1}^n\\frac{1}{\\theta}, 0 \\le x_i \\le \\theta\\) \\(-n \\cdot ln(\\theta)\\) \\(\\theta\\) \\(\\text{ }\\) Normal \\(\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\cdot exp\\left(-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right)\\) \\(\\frac{n}{2}ln(2\\pi\\sigma^2) - \\frac{1}{2\\sigma^2} \\sum_{i=1}^n(x_i - \\mu)^2\\) \\(\\mu,\\sigma^2\\) Binomial \\(\\prod_{i=1}^m \\binom{n}{x_i}\\cdot \\rho^{x_i}(1 - \\rho)^{n-x_i}\\) \\(ln \\binom{n}{x} + x ln(\\rho) + (n - x) ln (1 - \\rho)\\) \\(\\rho\\) \\(\\rho^X( 1 - \\rho)^{n-X}, X = \\sum_{i=1}^n x_i\\) Geometric \\(\\prod_{i=1}^n \\rho^n( 1 - \\rho)^{x_i - n}\\) \\(n\\cdot ln (\\rho) + \\left(\\sum_{i=1}^n x_i - n \\right) ln ( 1 - \\rho)\\) \\(\\rho\\) \\(\\rho^n ( 1 - \\rho)^{\\sum_{i=1} (x_i - n)}\\) Poisson \\(\\prod_{i=1}^n \\frac{\\lambda^{x_i}}{x_i!} \\cdot exp(-\\lambda)\\) \\(\\sum_{i=1}^n\\left( x_i ln(\\lambda) - ln(x_i!) - \\lambda \\right )\\) \\(\\lambda\\) \\(\\frac{\\lambda \\sum_{i=1}^n x_i}{\\prod_{i=1}^n x_i} \\cdot exp(-n \\lambda)\\) Exponential \\(\\prod_{i=1}^n \\lambda \\cdot exp(-\\lambda x), x\\ge 0\\) \\(-n ln(\\lambda) - \\frac{1}{\\lambda} \\sum_{i=1}^n x_i, 0 &lt; \\lambda &lt; \\infty\\) \\(\\lambda\\) \\(\\frac{1}{\\lambda^n} \\cdot exp\\left(\\frac{-\\sum_{i=1}^n x_i}{\\lambda}\\right)\\) Note that the likelihood for Normal Distribution can also be written as: \\[ Lik (\\theta|x_1,...,x_n) = \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n \\cdot exp \\left[\\frac{-1}{2\\sigma^2} \\sum_{i=1}^n (x_i - \\mu)^2\\right] \\] One other topic relevant to Likelihood is around base rate fallacy, which is often the cause of missing pertinent evidence (e.g., due to subjective (bias) decisions such as omitting relevant information). Base Rate Fallacy can also be called base rate neglect or base rate bias. It is Likelihood not measured based on frequency or commonality; rather based on the causal bias for which the Likelihood of an event to occur is based on information from other events based on premonition or irrational belief or feeling. We leave readers to investigate this topic further, including the Uncertainty Quantification, which discusses Aleatoric uncertainty (Statistical uncertainty) and Epistemic uncertainty (Systematic uncertainty) - balancing knowledge vs. practice. 7.3.3 Posterior Probability The term Posterior can be defined generally as that which comes after. In Bayesian statistics, a Posterior probability, also called a-posteriori probability, refers to the proportion of uncertainty of a random event after evidence is taken into consideration. Other terms are relevant to Posterior such as posterior odds, posterior uncertainty, posterior density, or posterior distribution. A posterior probability, \\(P(\\theta|X)\\), is obtained using the following formula: \\[\\begin{align} \\underbrace{P(\\theta|X)}_\\text{posterior}\\ \\propto\\ \\underbrace{Lik(\\theta|X)}_\\text{likelihood} \\times \\underbrace{P(\\theta)}_\\text{prior}\\ \\ \\ \\ \\ where\\ \\ \\ \\ Lik(\\theta|X) \\equiv P(X|\\theta) \\end{align}\\] Or for multiple observations, we have the following notation: \\[\\begin{align} P(\\theta|x_1,...x_n) \\propto Lik(\\theta|x_1,...x_n) \\times P(\\theta) \\end{align}\\] where: the sampling density (likelihood) represents the new evidence (new observation) the prior is a-priori probability that represents weight for the likelihood the posterior is a-posteriori probability that represents the new summary (new model) for theta (\\(\\theta\\)), e.g. the mean and variance that characterize an updated posterior normal distribution. Note that for IIDs, we can drop the normalizing factor: \\(P(x_1,...,x_n)\\). Now, recall the following equivalent equation again when testing positive for symptoms of a certain disease: \\[\\begin{align} P(\\text{symptom}|\\mathbf{+})_{(posterior)} \\propto P(\\mathbf{+}|\\text{symptom}) \\times P(\\text{symptom}) \\end{align}\\] In previous sections, we analyze flu-like symptoms in the presence of a particular pathogen in a small local community. We denote flu-like symptoms with theta (\\(\\theta\\)). We also denote the presence of the pathogen with the variable X. There is a previous recording of about 25% of the local community experiencing similar strange flu-like symptoms in that particular case. That is a prior knowledge. Based on our posterior probability computation in our previous case, we know that the resulting probability is 7.5%, \\(P(\\theta|X) = 7.5\\%\\). Therefore, we can make this our new prior knowledge to further our analysis. \\[\\begin{align} P(\\theta)\\ \\ \\leftarrow P(\\theta|X)\\ \\ \\ \\ \\ \\ \\ where\\ \\ new\\ P(\\theta) = 7.5\\% \\end{align}\\] If we are to perform another analysis today, granting today we find a piece of new evidence that 11% are found positive with the pathogen, and 4% of the locals with flu-like symptoms also carry the pathogen, then we have a new posterior probability using the new prior knowledge and new evidence: \\[ P(\\theta|X) = \\frac{Lik(\\theta|X)P(\\theta)}{P(X)} = \\frac{(0.04)(0.075)}{(0.11)} = 0.0273 = 2.73\\% \\] 7.3.4 Prior Probability The term Prior can be defined generally as that which comes before. In Bayesian statistics, a Prior probability, also called a-priori probability, refers to the proportion of uncertainty of a random event before evidence is taken into consideration. There are other terms relevant to prior such as prior odds, prior uncertainty, prior density, or prior distribution. A prior probability, \\(P(\\theta)\\), is formulated based on prior knowledge and is applied to a likelihood of observing data given theta (\\(\\theta\\)). \\[\\begin{align} P(\\theta|x_1,...x_n) \\propto Lik(\\theta|x_1,...x_n)\\times P(\\theta)\\ \\ \\leftarrow\\ \\ \\ \\ \\ \\text{posterior} \\propto \\text{likelihood} \\times \\text{prior} \\end{align}\\] For example, recall the following equivalent equation when testing positive (\\(\\mathbf{+}\\)) for symptoms of a particular disease: \\[\\begin{align} P(\\text{symptom}|\\mathbf{+}) \\propto P(\\mathbf{+}|\\text{symptom})\\times P(\\text{symptom})_{(prior)} \\end{align}\\] We may show cold-like or flu-like symptoms when contracting certain diseases, for an illustrative example. These symptoms feed into our initial hypothetical prognosis. This prognosis describes an initial temporary belief of the state or condition of our health until we visit our general physician for professional diagnosis to know the cause of our symptoms - actual evidence of what we contracted. Our prognosis is mere hypothetical and becomes an initial prior knowledge. However, we can be creative and start imagining all sorts of other hypothetical predispositions of our condition, but that, of course, does not help our physician to reach a better recommendation or conclusion. To help guide our physician, we have to provide our most accurate condition or risk a less accurate diagnosis. In other words, we start with the premise that our prior is merely an uninformed belief - our prognosis. It can be regarded as a weakly informed prior. Such weak belief requires a doctor’s diagnostics. Therefore, more diagnostic evidence allows us to offset our belief and thus creates a more evidence-based, diagnostic-based (or data-based) posterior result. Otherwise, partial analysis (likelihood) and prognosis (prior belief) may need to be combined for a proper posterior if the diagnostic analysis is insufficient. Also, mathematically, our prior distribution becomes improper if we end up with an infinite range of prognostic possibilities, meaning that our prior does not integrate to 1. An improper prior happens when corresponding prior probability does not integrate to a finite number: \\[\\begin{align} P(\\theta) = \\int f(\\theta) d\\theta = +\\infty \\end{align}\\] Nonetheless, as long as the posterior probability is proper - meaning that it integrates to 1 - then our prior probability does not have to be proper. We continue to extend our discussion on informative prior in the Hyperparameters section. 7.4 Conjugacy This section and the subsections listing conjugate priors include additional references from Daniel Fink (1997), Ben Lambart (2014), and Deetorah (2013) as the basis for a long list of Conjugacy Prior derivations. When dealing with bayesian probabilities, we may think of data distribution as normal distribution by default. So if we see the following Naïve Bayes formula: \\[\\begin{align} posterior \\propto likelihood \\times prior, \\end{align}\\] It may be appropriate if only we highlight the type of distribution used. So let us do that: \\[\\begin{align} \\underbrace{posterior}_\\text{normal} \\propto \\underbrace{likelihood}_\\text{normal} \\times \\underbrace{prior}_\\text{normal} \\end{align}\\] Both the posterior and prior components of a Bayesian formula are considered conjugate if they share the same family of probability distributions. It means that the parameters of both components follow the same family of distribution, e.g., a normal distribution. 7.4.1 Precision Before discussing a few conjugate combinations, let us introduce the concept of Precision. A normal distribution has two common parameters: mean and variance. However, there is one other parameter that we can use for normal distribution called precision (let us use the Lambda \\(\\Lambda\\) symbol for this) which is the inverse of variance, \\(\\Lambda = \\frac{1}{\\sigma^2} = (\\sigma^2)^{-1}\\). Precision has an advantage over variance in that it has an additive property, which is useful for multivariate normal distribution. For multivariate normal distribution, we have the following adjusted normal density function: \\[\\begin{align} f(x; \\theta) = \\frac{1}{\\sqrt{(2\\pi)^d \\Sigma}} exp\\left[-\\frac{1}{2}\\underbrace{(x - \\mu)^T{\\Sigma}^{-1}(x - \\mu)}_\\text{squared mahalanobis distance}\\right] \\end{align}\\] where: \\[\\begin{align} {\\Sigma}^{-1} = \\sum uu^T\\ \\ \\ \\ \\ \\ \\ \\text{(an inverse covariance positive definite matrix)} \\end{align}\\] Note that Squared Mahalanobis distance measures the distance between distributions (e.g. between x and \\(\\mu\\)). See a brief definition in Chapter 9 (Computational Learning I) under Distance metrics Section. 7.4.2 Conjugate Prior A way for a posterior to characterize a proper density is to have a prior density be applied to a sampling density (likelihood). Therefore, it is essential to be able to choose an appropriate prior density that leads to a proper posterior density. We need to choose a prior distribution that fits a good distribution for the posterior. We call this chosen prior distribution a conjugate prior. In machine learning, it is common to regard prior as a weighing factor denoted as \\(\\omega(.)\\) or \\(\\pi(.)\\), e.g.: \\[\\begin{align} P(\\mu|x) \\propto P(\\mu) \\times Lik(\\mu|x) \\rightarrow P(\\mu|x) \\propto \\mathcal{\\pi}(\\mu) \\times Lik(\\mu|x) \\end{align}\\] 7.4.3 Normal-Normal Conjugacy The idea is to be able to tailor a Normal density distribution for posterior given that a Normal density is conjugate prior for a Normal likelihood where the mean \\(\\mu\\) is unknown. For a notation, we use the following: \\[\\begin{align} P(\\mu|x) \\propto P(\\mu) \\times Lik(\\mu|x) \\ \\ \\ \\ \\ \\ \\text{for } -\\infty &lt; \\mu\\ &lt; \\infty \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)\\ \\ \\ \\ \\ \\text{where}\\ \\sigma^2\\ \\text{ is known, but}\\ \\mu\\ \\text{is unknown} \\end{align}\\] For marginal-density likelihood: \\[\\begin{align} Lik_X(\\mu, \\sigma^2|x) {}&amp;\\equiv P_X(X=x|\\mu, \\sigma^2)\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] For joint-density likelihood (where n is sample size): \\[\\begin{align} Lik_X(\\mu, \\sigma^2|x_1,...,x_n) &amp;\\equiv P_X(x_1,...,x_n|\\mu, \\sigma^2) \\\\ &amp;= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]\\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n exp\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] For prior, we choose a Normal distribution for the \\(\\mu\\) parameter: \\[\\begin{align} \\mu \\sim \\mathcal{N}(\\mu_0, \\sigma^2_0)\\ \\ \\ \\ \\ \\text{where}\\ \\mu_0\\ \\text{and}\\ \\sigma^2_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\mu) = P(\\mu; \\mu_0, \\sigma^2_0) = \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] \\end{align}\\] For posterior, we want to arrive at a Normal density given an observed data: \\[\\begin{align} \\mu, \\sigma^2|x \\sim \\mathcal{N}(\\mu_1, \\sigma^2_1) \\end{align}\\] However, first, let us derive the posterior density with respect to \\(\\mu\\) by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents). For a Normal posterior with marginal-density likelihood: \\[\\begin{align} P(\\mu,\\sigma^2|x) {}&amp;\\propto \\underbrace { \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right]}_\\text{normal prior} \\times \\underbrace { \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] }_\\text{normal likelihood}\\\\ P(\\mu,\\sigma^2|x) &amp;\\propto exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] \\times exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right]\\ \\ \\text{(drop constant)} \\\\ &amp;\\propto exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0} -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] \\\\ &amp; \\propto exp\\left[-\\left(\\frac{\\sigma^2}{\\sigma^2}\\right)\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0} -\\left(\\frac{\\sigma^2_0}{\\sigma^2_0}\\right)\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] \\\\ &amp;\\propto exp\\left[-\\frac{\\sigma^2(\\mu - \\mu_0)^2}{2\\sigma^2_0\\sigma^2} -\\frac{\\sigma^2_0(x - \\mu)^2}{2\\sigma^2\\sigma^2_0}\\right] \\\\ &amp;\\propto exp\\left[\\frac{ -\\sigma^2(\\mu^2 - 2\\mu\\mu_0 + \\mu^2_0) + -\\sigma^2_0(x^2 - 2x\\mu + \\mu^2)} {2\\sigma^2\\sigma^2_0}\\right] \\\\ &amp;\\propto exp\\left[\\frac{ (-\\mu^2\\sigma^2 + 2\\mu\\mu_0\\sigma^2 - \\mu^2_0\\sigma^2) + (-x^2\\sigma^2_0 + 2x\\mu\\sigma^2_0 - \\mu^2\\sigma^2_0)} {2\\sigma^2\\sigma^2_0}\\right] \\end{align}\\] Let us pull out contents of the exponent and drop the constants with respect to \\(\\mu\\), then simplify further: \\[\\begin{align} {}&amp;\\rightarrow \\frac{ (-\\mu^2\\sigma^2 + 2\\mu\\mu_0\\sigma^2) + (2x\\mu\\sigma^2_0 - \\mu^2\\sigma^2_0)} {2\\sigma^2\\sigma^2_0} \\\\ &amp;\\rightarrow \\frac{1}{2}\\frac{ -\\mu^2(\\sigma^2 + \\sigma^2_0) +2\\mu(\\mu_0\\sigma^2 + x\\sigma^2_0)} {\\sigma^2\\sigma^2_0} \\\\ &amp;\\rightarrow \\frac{1}{2}\\frac{ -\\mu^2(\\sigma^2 + \\sigma^2_0) +2\\mu(\\mu_0\\sigma^2 + x\\sigma^2_0)} {\\sigma^2\\sigma^2_0} \\left(\\frac{\\frac{1}{\\sigma^2 + \\sigma^2_0}}{\\frac{1}{\\sigma^2 + \\sigma^2_0}}\\right)\\\\ &amp;\\rightarrow -\\frac{1}{2}\\left(\\frac{ \\mu^2 - 2\\mu \\frac{ \\mu_0\\sigma^2 + x\\sigma^2_0 }{\\sigma^2 + \\sigma^2_0} } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + \\sigma^2_0} } \\right)\\ \\ \\ \\rightarrow -\\frac{1}{2}\\left(\\frac{ \\left( \\mu - \\frac{ \\mu_0\\sigma^2 + x\\sigma^2_0 }{\\sigma^2 + \\sigma^2_0}\\right)^2 } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + \\sigma^2_0} } \\right) \\end{align}\\] If we then put back the equation inside the exponent, we then have the following: \\[\\begin{align} P(\\mu,\\sigma^2|x) \\propto exp\\left[-\\frac{(\\mu - \\mu_1)^2}{2\\sigma^2_1}\\right] = exp \\left[ -\\frac{1}{2}\\left(\\frac{ \\left( \\mu - \\frac{ \\mu_0\\sigma^2 + x\\sigma^2_0}{\\sigma^2 + \\sigma^2_0} \\right)^2 } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} } \\right) \\right] \\end{align}\\] Notice that given a Normal posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\mu_1 = \\frac{ \\mu_0\\sigma^2 + x\\sigma^2_0}{\\sigma^2 + \\sigma^2_0} = \\frac{\\left(\\frac{\\mu_0}{\\sigma^2_0} + \\frac{x}{\\sigma^2}\\right)} {\\left(\\frac{1}{\\sigma^2_0} + \\frac{1}{\\sigma^2}\\right) } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma^2_1 = \\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + \\sigma^2_0} = \\left(\\frac{1}{\\sigma^2_0} + \\frac{1}{\\sigma^2}\\right)^{-1} \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\mu,\\sigma^2|x \\sim ~ N(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow N\\left[ \\sigma^2_1\\left(\\frac{\\mu_0}{\\sigma^2_0} + \\frac{x}{\\sigma^2}\\right), \\left(\\frac{1}{\\sigma^2_0} + \\frac{1}{\\sigma^2}\\right)^{-1} \\right] \\end{align}\\] For a Normal posterior with joint-density likelihood: \\[\\begin{align} P(\\mu,\\sigma^2|x_1,...,x_n) {}&amp;\\propto \\underbrace{ \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right]}_\\text{normal prior} \\times \\underbrace{ \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n exp\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right]}_\\text{normal likelihood}\\\\ P(\\mu,\\sigma^2|x_1,...,x_n) &amp;\\propto exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] \\times exp\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right]\\ \\\\ &amp;\\propto exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0} -\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\\\ &amp; \\propto exp\\left[-\\left(\\frac{\\sigma^2}{\\sigma^2}\\right)\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0} -\\left(\\frac{\\sigma^2_0}{\\sigma^2_0}\\right)\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\\\ &amp;\\propto exp\\left[-\\frac{\\sigma^2(\\mu - \\mu_0)^2}{2\\sigma^2_0\\sigma^2} -\\frac{\\sigma^2_0\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2\\sigma^2_0}\\right] \\\\ &amp;\\propto exp\\left[\\frac{ -\\sigma^2(\\mu^2 - 2\\mu\\mu_0 + \\mu^2_0) + -\\sigma^2_0(\\sum_{i=1}^nx_i^2 - 2n\\bar{x}\\mu + n\\mu^2)} {2\\sigma^2\\sigma^2_0}\\right] \\end{align}\\] \\[\\begin{align} &amp;\\propto exp\\left[\\frac{ (-\\mu^2\\sigma^2 + 2\\mu\\mu_0\\sigma^2 - \\mu^2_0\\sigma^2) + (-\\sigma^2_0 \\sum_{i=1}^nx_i^2 + 2n \\bar{x}\\mu\\sigma^2_0 - n\\mu^2\\sigma^2_0)} {2\\sigma^2\\sigma^2_0}\\right] \\end{align}\\] Let us pull out contents of the exponent and drop the constants with respect to \\(\\mu\\): \\[\\begin{align} {}&amp;\\rightarrow \\frac{ (-\\mu^2\\sigma^2 + 2\\mu\\mu_0\\sigma^2) + (2n\\bar{x}\\mu\\sigma^2_0 - n\\mu^2\\sigma^2_0)} {2\\sigma^2\\sigma^2_0} \\\\ &amp;\\rightarrow \\frac{ -\\mu^2(\\sigma^2 + n\\sigma^2_0) + 2\\mu(\\mu_0\\sigma^2 + \\bar{x}n\\sigma^2_0)} {2\\sigma^2\\sigma^2_0} \\\\ &amp;\\rightarrow \\frac{ -\\mu^2(\\sigma^2 + n\\sigma^2_0) + 2\\mu(\\mu_0\\sigma^2 + \\bar{x}n\\sigma^2_0)} {2\\sigma^2\\sigma^2_0} \\left(\\frac{\\frac{1}{\\sigma^2 + n\\sigma^2_0}}{\\frac{1}{\\sigma^2 + n\\sigma^2_0}}\\right)\\\\ &amp;\\rightarrow -\\frac{1}{2}\\left(\\frac{ \\mu^2 - 2\\mu \\frac{ \\mu_0\\sigma^2 + n\\bar{x}\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} } \\right)\\ \\ \\ \\rightarrow\\ \\ \\ -\\frac{1}{2}\\left(\\frac{ \\left( \\mu - \\frac{ \\mu_0\\sigma^2 + n\\bar{x}\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} \\right)^2 } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} } \\right) \\end{align}\\] If we then put back the equation inside the exponent, we then have the following: \\[\\begin{align} P(\\mu,\\sigma^2|x1,...,x_n) \\propto exp\\left[-\\frac{(\\mu - \\mu_1)^2}{2\\sigma^2_1}\\right] = exp \\left[ -\\frac{1}{2}\\left(\\frac{ \\left( \\mu - \\frac{ \\mu_0\\sigma^2 + n\\bar{x}\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} \\right)^2 } {\\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} } \\right) \\right] \\end{align}\\] Notice that given a Normal posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\mu_1 {}= \\frac{ \\mu_0\\sigma^2 + n\\bar{x}\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} = \\frac{\\left(\\frac{\\mu_0}{\\sigma^2_0} + \\frac{n\\bar{x}}{\\sigma^2}\\right)} {\\left(\\frac{1}{\\sigma^2_0} + \\frac{n}{\\sigma^2}\\right) } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma^2_1 = \\frac{\\sigma^2\\sigma^2_0}{\\sigma^2 + n\\sigma^2_0} = \\left(\\frac{1}{\\sigma^2_0} + \\frac{n}{\\sigma^2}\\right)^{-1} \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\mu,\\sigma^2|x_1,...,x_n \\sim ~ N(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow N\\left[ \\sigma^2_1\\left(\\frac{\\mu_0}{\\sigma^2_0} + \\frac{n\\bar{x}}{\\sigma^2}\\right), \\left(\\frac{1}{\\sigma^2_0} + \\frac{n}{\\sigma^2}\\right)^{-1} \\right] \\end{align}\\] 7.4.4 Normal-Inverse Gamma Conjugacy The idea is to be able to tailor an Inverse Gamma density distribution for posterior given that an Inverse Gamma density is conjugate prior for a Normal likelihood where the variance \\(\\sigma^2\\) is unknown. For a notation, we use the following: \\[\\begin{align} P(\\sigma^2|x) \\propto P(\\sigma^2) \\times Lik(\\sigma^2|x) \\ \\ \\ \\ \\ \\ \\text{for } -\\infty &lt; \\sigma^2\\ &lt; \\infty \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\mu,\\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)\\ \\ \\ \\ \\ \\text{where}\\ \\mu\\ \\text{ is known, but}\\ \\sigma^2\\ \\text{is unknown} \\end{align}\\] For marginal-density likelihood: \\[\\begin{align} Lik_X(\\mu,\\sigma^2|x) {}&amp;\\equiv P(X=x|\\mu,\\sigma^2)\\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] For joint-density likelihood: \\[\\begin{align} Lik_X(\\mu, \\sigma^2|x_1,...,x_n) &amp;\\equiv P_X(x_1,...,x_n|\\mu,\\sigma^2) \\\\ &amp;= \\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]\\\\ &amp;= \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n exp\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] For prior, we choose an Inverse Gamma distribution for the \\(\\sigma^2\\) parameter: \\[\\begin{align} \\sigma^2 \\sim Inv.\\ Gamma(\\alpha_0,\\beta_0)\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ \\text{and}\\ \\beta_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\sigma^2) = P(\\sigma^2; \\alpha_0, \\beta_0) = \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right)\\ \\ \\text{(inverse)} \\end{align}\\] For posterior, we want to arrive at an Inv. Gamma density given an observed data: \\[\\begin{align} \\sigma^2|x \\sim Inv.\\ Gamma(\\alpha_1,\\beta_1) \\end{align}\\] But first, let us derive the posterior density with respect to \\(\\sigma^2\\) by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents). For an Inv. Gamma posterior with marginal-density likelihood: \\[\\begin{align} P(\\mu,\\sigma^2|x) {}&amp;\\propto \\underbrace{ \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right) }_\\text{inverse-gamma prior} \\times \\underbrace{ \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right]}_\\text{normal likelihood} \\\\ &amp;\\propto\\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right) \\times (2\\pi)^{-\\frac{1}{2}} (\\sigma^2)^{-\\frac{1}{2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right]\\\\ P(\\mu,\\sigma^2|x) &amp;\\propto \\left[{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right) \\right] \\times (\\sigma^2)^{-\\frac{1}{2}} exp\\left[-\\frac{(x - \\mu)^2}{2\\sigma^2}\\right] \\ \\text{(drop const.)} \\\\ &amp;\\propto \\left[{(\\sigma^2)}^{-(\\alpha_0+\\frac{1}{2} + 1)} exp\\left( -\\frac{1}{\\sigma^2} \\left(\\beta_0 + \\frac{(x - \\mu)^2}{2} \\right) \\right) \\right] \\end{align}\\] Notice that given an Inv. Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = \\alpha_0 + \\frac{1}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = \\left(\\beta_0 + \\frac{(x - \\mu)^2}{2} \\right) \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x \\sim Inv. Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Inv. Gamma \\left[\\alpha_0 + \\frac{1}{2}, \\left(\\beta_0 + \\frac{(x - \\mu)^2}{2} \\right) \\right] \\end{align}\\] For an Inv. Gamma posterior with joint-density likelihood: \\[\\begin{align} P(\\mu&amp;,\\sigma^2|x_1,...,x_n) \\propto \\nonumber \\\\ &amp;\\underbrace{ \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right)}_\\text{inverse-gamma prior} \\times \\underbrace{ \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n exp\\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] }_\\text{normal likelihood} \\\\ &amp;\\propto \\frac{\\beta_0^{\\alpha_0}}{\\Gamma(\\alpha_0)}{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right) \\times (2\\pi)^{-\\frac{n}{2}} (\\sigma^2)^{-\\frac{n}{2}} exp\\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] \\[\\begin{align} P(\\mu,\\sigma^2|x_1,...,x_n) &amp;\\propto \\left[{(\\sigma^2)}^{-(\\alpha_0+1)}exp\\left({-\\frac{\\beta_0}{\\sigma^2}}\\right) \\right] \\times (\\sigma^2)^{-\\frac{n}{2}} exp\\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\\\ &amp;\\propto \\left[{(\\sigma^2)}^{-(\\alpha_0+\\frac{n}{2} + 1)} exp\\left( -\\frac{1}{\\sigma^2} \\left(\\beta_0 + \\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2} \\right) \\right) \\right] \\end{align}\\] Notice that given an Inv. Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = \\alpha_0 + \\frac{n}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = \\beta_0 + \\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2} \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x_1,...,x_n \\sim Inv. Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Inv. Gamma \\left[\\alpha_0 + \\frac{n}{2}, \\beta_0 + \\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2} \\right ] \\end{align}\\] 7.4.5 Multivariate Normal Conjugacy The idea is to be able to tailor a Multivariate Normal (MVN) distribution for posterior given that MVN density is conjugate prior for the mean parameter, \\(\\mu\\). For a notation, we use the following: \\[\\begin{align} P(\\mu|x) \\propto P(\\mu) \\times Lik(\\mu|x) \\end{align}\\] For likelihood, we have the following p-variate normal distribution: \\[\\begin{align} X_m|\\mu,\\Sigma \\sim \\mathcal{N}_p(\\mu, \\Sigma)\\ \\ \\ \\ \\ \\text{where}\\ \\mu\\ \\text{is unknown and }\\ \\Sigma \\text{ is known } \\end{align}\\] and where: \\[\\begin{align*} X = \\left[\\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]_\\text{(nxp)} \\ \\ \\ \\ \\mu = \\left[\\begin{array}{c}\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\vdots \\\\ \\bar{x}_p \\end{array}\\right] = \\left[\\begin{array}{c}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{array}\\right]_\\text{(1xp)} \\end{align*}\\] \\[\\begin{align*} \\Sigma = \\left[\\begin{array}{rrrr}\\sigma^2_{1} &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1p}\\\\ \\sigma_{21} &amp; \\sigma^2_{2} &amp; \\cdots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\cdots &amp; \\sigma^2_{p} \\end{array}\\right]_\\text{(pxp)} \\end{align*}\\] The notation \\(\\Sigma\\) above represents a p-variate positive-definite variance-covariance matrix. For joint-density likelihood: \\[\\begin{align} Lik_X(\\mu, {}&amp;\\Sigma|x_1,...,x_n) \\equiv P_X(x_1,...,x_n|\\mu,\\Sigma) \\\\ &amp;= \\prod_{i=1}^n\\frac{|\\Sigma|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(x_i - \\mu)^T\\Sigma^{-1}(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Sigma|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^T\\Sigma^{-1}(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Sigma|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{n}{2} (\\bar{x} - \\mu)^T\\Sigma^{-1}(\\bar{x} - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Sigma|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{n}{2} (\\bar{x}^T\\Sigma^{-1}\\bar{x} - \\bar{x}^T\\Sigma^{-1}\\mu - \\mu^T\\Sigma^{-1}\\bar{x} + \\mu^T\\Sigma^{-1}\\mu )\\right] \\end{align}\\] For prior, we choose a Multivariate normal distribution for the \\(\\mu\\) parameter: \\[\\begin{align} \\mu \\sim \\mathcal{N}_p(\\mu_0,\\Sigma_0)\\ \\ \\ \\ \\ \\text{where}\\ \\mu_0\\ \\text{and}\\ \\Sigma_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\mu) = P(\\mu; \\mu_0, \\Sigma_0) {}&amp;= \\frac{|\\Sigma_0|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(\\mu - \\mu_0)^T\\Sigma_0^{-1}(\\mu - \\mu_0)\\right]\\\\ &amp;= \\frac{|\\Sigma_0|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}\\left( \\mu^T \\Sigma_0^{-1} \\mu - \\mu^T\\Sigma_0^{-1} \\mu_0 - \\mu_0^T\\Sigma_0^{-1} \\mu + {\\mu_0}^T \\Sigma_0^{-1}\\mu_0 \\right)\\right] \\end{align}\\] For posterior, we want to arrive at a Multivariate normal density given some observed data: \\[\\begin{align} \\mu|X \\sim \\mathcal{N}_p(\\mu_1,\\Sigma_1) \\end{align}\\] But first, let us derive the posterior density with respect to \\(\\mu\\) by dropping the constants that do not affect the shape or proportionality of the distribution. For a multivariate normal posterior: \\[\\begin{align} P(\\mu|x) &amp;= \\underbrace{\\frac{|\\Sigma_0|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}\\left( \\mu^T \\Sigma_0^{-1} \\mu - \\mu^T\\Sigma_0^{-1} \\mu_0 - {\\mu_0}^T\\Sigma_0^{-1} \\mu + {\\mu_0}^T \\Sigma_0^{-1}\\mu_0 \\right)\\right]}_\\text{MVN prior} \\nonumber \\\\ &amp;\\times \\underbrace{\\left(\\frac{|\\Sigma|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{n}{2} (\\bar{x}^T\\Sigma^{-1}\\bar{x} - \\bar{x}^T\\Sigma^{-1}\\mu - \\mu^T\\Sigma^{-1}\\bar{x} + \\mu^T\\Sigma^{-1}\\mu ) \\right] }_\\text{MVN likelihood} \\end{align}\\] \\[\\begin{align} \\rightarrow &amp; \\text{(drop constants not relevant to } \\mu \\text{. Also, recall matrix transposition properties)} \\nonumber \\\\ &amp;\\propto exp\\left[-\\frac{1}{2}\\left( \\mu^T \\Sigma_0^{-1} \\mu - \\mu^T\\Sigma_0^{-1} \\mu_0 - {\\mu_0} ^T\\Sigma_0^{-1} \\mu \\right)\\right]\\\\ &amp;\\times exp\\left[-\\frac{n}{2} \\left( - \\bar{x} ^T\\Sigma^{-1}\\mu - \\mu^T\\Sigma^{-1}\\bar{x} + \\mu^T\\Sigma^{-1}\\mu\\right)\\right] \\\\ &amp;\\propto exp\\biggl[-\\frac{1}{2}\\biggl(\\left[ \\mu^T \\Sigma_0^{-1} \\mu - \\mu^T\\Sigma_0^{-1} \\mu_0 - {\\mu_0} ^T\\Sigma_0^{-1}\\mu \\right] + \\left[ - n\\bar{x} ^T\\Sigma^{-1}\\mu - n\\mu^T\\Sigma^{-1}\\bar{x} + n\\mu^T\\Sigma^{-1}\\mu\\right]\\biggr)\\biggr]\\\\ &amp;\\propto exp\\biggl[-\\frac{1}{2}\\biggl( \\mu^T \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right] \\mu - \\mu^T \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right] - \\left[\\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right]^T\\mu \\biggr)\\biggr]\\\\ &amp;\\propto exp\\biggl[-\\frac{1}{2}\\biggl( \\mu^T \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right] \\mu - 2\\mu^T \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right] \\biggr)\\biggr]\\\\ &amp;\\propto exp\\biggl[-\\frac{1}{2}\\biggl( \\left(\\mu - \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right]^{-1} \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right] \\right)^T \\left( \\Sigma_0^{-1} + n\\Sigma^{-1} \\right) \\nonumber \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left(\\mu - \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right]^{-1} \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right] \\right) \\biggr)\\biggr] \\end{align}\\] Notice that given an MVN posterior distribution, it becomes apparent that the parameters \\(\\mu_1\\) and \\(\\Sigma_1\\) correspond to the following: \\[\\begin{align} \\Sigma_1 = \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right]^{-1}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_1 = \\Sigma_1 \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right] \\end{align}\\] Therefore, we arrive at the following reparameterized p-variate posterior distribution: \\[\\begin{align} \\mu|x \\sim \\mathcal{N}_p(\\mu_1,\\Sigma_1) \\ \\ \\ \\rightarrow \\mathcal{N}_p \\left( \\Sigma_1 \\left[ \\Sigma_0^{-1}\\mu_0 + \\Sigma^{-1}n\\bar{x}\\right],\\ \\left[ \\Sigma_0^{-1} + n\\Sigma^{-1}\\right]^{-1} \\right) \\end{align}\\] 7.4.6 Normal Wishart Conjugacy The idea is to be able to tailor a Normal Wishart distribution for posterior given that Normal Wishart density is conjugate prior for mean, \\(\\mu\\), and positive-definite precision matrix parameter, \\(\\Lambda\\), of a Multivariate normal likelihood. For a notation, we use the following: \\[\\begin{align} P(\\mu, \\Lambda|x) \\propto P(\\mu,\\Lambda) \\times Lik(\\mu, \\Lambda|x) \\end{align}\\] For likelihood, we have the following p-variate normal distribution: \\[\\begin{align} X|\\mu,\\Lambda \\sim \\mathcal{N}_p(X; \\mu, \\Lambda) \\ \\ \\ \\ \\ \\text{where}\\ \\mu\\ and\\ \\Lambda\\ \\text{are unknown } \\end{align}\\] Recall the below structure. See Chapter 5 (Numerical Probability and Distribution) under Wishart Distribution section: \\[\\begin{align*} X = \\left[\\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]_\\text{(nxp)} \\ \\ \\ \\ \\mu = \\left[\\begin{array}{c}\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\vdots \\\\ \\bar{x}_p \\end{array}\\right] = \\left[\\begin{array}{c}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{array}\\right]_\\text{(1xp)} \\end{align*}\\] \\[\\begin{align*} \\Lambda = \\left[\\begin{array}{rrrr}\\sigma^2_{1} &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1p}\\\\ \\sigma_{21} &amp; \\sigma^2_{2} &amp; \\cdots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\cdots &amp; \\sigma^2_{p} \\end{array}\\right]_\\text{(pxp)}^{(-1)} \\end{align*}\\] For multivariate joint-density likelihood: \\[\\begin{align} Lik_X(\\mu, \\Lambda|x_1,...,x_n) &amp;\\equiv P_X(x_1,...,x_n|\\mu,\\Lambda) \\\\ &amp;= \\prod_{i=1}^n\\frac{|\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(x_i - \\mu)^T\\Lambda(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^T\\Lambda(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\sum_{i=1}^n \\Lambda(x_i - \\mu)(x_i - \\mu)^T\\right]\\\\ &amp;= \\left(\\frac{|\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\Lambda\\left(\\sum_{i=1}^n x_i x_i^T - \\mu (n\\bar{x})^T - (n\\bar{x})\\mu^T + n\\mu\\mu^T\\right)\\right] \\end{align}\\] It helps to be aware of the following mathematical manipulation (derivation not included): \\[\\begin{align} \\sum_{i=1}^n (x_i - \\mu)^T\\Lambda(x_i - \\mu) = tr( \\Sigma \\Lambda)\\ \\ \\ \\ \\ where\\ \\ \\ \\Sigma = \\sum_{i=1}^n (x_i - \\mu)(x_i - \\mu)^T \\end{align}\\] For prior, we choose a Normal-Wishart distribution for both \\(\\Lambda\\) and \\(\\mu\\) parameters. Recall description of Central Wishart notation under Wishart distribution in Chapter 5 (Numerical Probability and Distribution. \\[\\begin{align} \\Lambda {}&amp;\\sim \\mathcal{W}_p(v_0, \\Sigma_0)\\ \\ \\ \\ \\ \\text{where}\\ v_0\\ and\\ \\Sigma_0 \\text{ are known } \\mathbf{hyperparameters}\\\\ \\nonumber \\\\ \\mathcal{\\pi}(\\Lambda) &amp;= P(\\Lambda; \\nu_0, \\Sigma_0) = \\frac{|\\Lambda|^{\\frac{\\nu_0-p-1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda)\\right]} {2^{\\frac{\\nu_0 p}{2}}|\\Sigma_0|^{\\frac{\\nu_0}{2}}\\ \\Gamma_p\\left(\\frac{\\nu_0}{2}\\right)} \\end{align}\\] \\[\\begin{align} \\mu|\\Lambda {}&amp;\\sim \\mathcal{N}_p(\\alpha_0, (\\beta_0\\Lambda)^{-1})\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ and\\ \\beta_0\\Lambda \\text{ are known } \\mathbf{hyperparameters}\\\\ \\nonumber \\\\ \\mathcal{\\pi}(\\mu|\\Lambda) &amp;= P(\\mu; \\alpha_0, (\\beta_0\\Lambda)^{-1}) = \\frac{|\\beta_0\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(\\mu - \\alpha_0)^T\\beta_0\\Lambda(\\mu - \\alpha_0)\\right] \\end{align}\\] \\[\\begin{align} \\rightarrow &amp; \\text{(join distributions and drop constants)} \\nonumber \\\\ \\nonumber \\\\ P(\\mu, \\Lambda) &amp;\\propto \\underbrace{ |\\beta_0\\Lambda|^{\\frac{1}{2}} exp\\left[-\\frac{1}{2}(\\mu - \\alpha_0)^T\\beta_0\\Lambda(\\mu - \\alpha_0)\\right]}_\\text{gaussian prior} \\times \\underbrace{ |\\Lambda|^{\\frac{\\nu_0-p-1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda)\\right]}_\\text{wishart prior}\\\\ &amp;\\propto |\\beta_0|^\\frac{1}{2} |\\Lambda|^{\\frac{1}{2}} |\\Lambda|^{\\frac{\\nu_0-p-1}{2}} exp\\left[-\\frac{\\beta_0}{2}\\Lambda(\\mu - \\alpha_0)(\\mu - \\alpha_0)^T + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\right] \\\\ &amp;\\propto |\\beta_0|^\\frac{1}{2} |\\Lambda|^{\\frac{\\nu_0-p}{2}} exp\\left[-\\frac{\\beta_0}{2}\\Lambda(\\mu\\mu^T - \\mu{\\alpha_0}^T - {\\alpha_0}\\mu^T + \\alpha_0{\\alpha_0}^T) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\right] \\end{align}\\] where: \\(\\alpha_0\\) is hyper mean for \\(\\mu\\). \\(\\beta_0\\Lambda\\) is hyper precision for \\(\\mu\\). See Figure 7.19 as reference model. \\(\\Lambda\\) is positive-definite precision matrix that follows a wishart distribution. For posterior, we want to join the likelihood and the normal-wishart prior: \\[\\begin{align} \\mu, \\Lambda|x \\sim \\ \\mathcal{NW}_p(\\mu, \\Lambda; \\alpha_1, (\\beta_1\\Lambda), v_1, \\Sigma_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\mu\\) and \\(\\Lambda\\) by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents). For a Normal-Wishart posterior with joint-density likelihood: \\[\\begin{align} P{}&amp;(\\mu,\\Lambda|X) \\propto \\underbrace{ |\\beta_0|^\\frac{1}{2} |\\Lambda|^{\\frac{\\nu_0-p}{2}} exp\\left[-\\frac{\\beta_0}{2}\\Lambda(\\mu\\mu^T - \\mu{\\alpha_0}^T - {\\alpha_0}\\mu^T + \\alpha_0{\\alpha_0}^T) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\right] }_\\text{normal-wishart prior} \\nonumber \\\\ &amp;\\times \\underbrace{\\left(\\frac{|\\Lambda|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\Lambda\\left(\\sum_{i=1}^n x_i {x_i}^T - \\mu (n\\bar{x})^T - (n\\bar{x})\\mu^T + n\\mu\\mu^T\\right)\\right]}_\\text{normal likelihood} \\end{align}\\] \\[\\begin{align} \\rightarrow &amp; \\text{(drop constants and simplify. Recall matrix transposition properties)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0-p}{2}} |\\Lambda|^{\\frac{n}{2}} exp\\biggl[-\\frac{\\beta_0}{2}\\Lambda(\\mu\\mu^T - \\mu{\\alpha_0}^T - {\\alpha_0}\\mu^T + \\alpha_0{\\alpha_0}^T) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\nonumber \\\\ &amp;+ -\\frac{1}{2}\\Lambda\\biggl(\\sum_{i=1}^n x_i {x_i}^T - \\mu (n\\bar{x})^T - (n\\bar{x})\\mu^T + n\\mu\\mu^T\\biggr) \\biggr] \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((\\beta_0 + n) \\mu\\mu^T - (\\alpha_0\\beta_0 + n\\bar{x})\\mu^T - \\mu(\\alpha_0\\beta_0 + n\\bar{x})^T + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) + \\nonumber \\\\ &amp; -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] \\[\\begin{align} \\rightarrow &amp; \\text{(inject placeholders)} \\nonumber \\\\ \\rightarrow &amp;\\text{let}\\ a = (\\beta_0 + n)\\ and\\ b = (\\alpha_0\\beta_0 + n\\bar{x})\\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((a) \\mu\\mu^T - (b)\\mu^T - \\mu(b)^T + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] \\[\\begin{align} \\rightarrow &amp;\\text{(add terms so that}\\ bb^T - bb^T = 0\\ \\text{)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((a) \\mu\\mu^T-(b)\\mu^T - \\mu(b)^T \\mathbf{\\ + bb^T - bb^T} + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr]\\\\ \\rightarrow &amp; \\text{(add factor so that}\\ \\frac{a}{a} = 1\\ \\text{)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl(\\frac{a}{a}\\biggl[(a) \\mu\\mu^T - (b)\\mu^T - \\mu(b)^T bb^T-bb^T \\biggr] + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) \\nonumber \\\\ &amp;-\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] \\[\\begin{align} \\rightarrow &amp;\\text{(split exponentials and re-arrange terms)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((a) \\biggl[ \\mu\\mu^T - \\frac{1}{(a)}(b)\\mu^T - \\frac{1}{(a)}\\mu(b)^T + \\frac{1}{(a)}bb^T \\biggr] \\biggr) \\biggr] \\times \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl(- \\frac{1}{(a)}bb^T + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr]\\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((a) \\biggl( \\mu - \\frac{(b)}{(a)} \\biggr)\\biggl( \\mu - \\frac{(b)}{(a)} \\biggr)^T \\biggr) \\biggr] \\times \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl(- \\frac{1}{(a)}bb^T + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] \\[\\begin{align} \\rightarrow &amp; \\text{(substitute placeholder)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((\\beta_0 + n) \\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)\\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)^T \\biggr) \\biggr] \\times \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl(- \\frac{(\\alpha_0\\beta_0 + n\\bar{x})(\\alpha_0\\beta_0 + n\\bar{x})^T }{(\\beta_0 + n)} + \\beta_0\\alpha_0{\\alpha_0}^T + \\sum_{i=1}^n x_i {x_i}^T \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] Let us extract terms from the second exponential equation to simplify. First, add terms so that \\(- n\\bar{x}\\bar{x}^T + n\\bar{x}\\bar{x}^T = 0\\), and then simplify (a.l.a sum of squares). \\[\\begin{align} &amp;\\rightarrow \\sum_{i=1}^nx_i {x_i}^T \\mathbf{\\ - n\\bar{x}\\bar{x}^T + n\\bar{x}\\bar{x}^T} \\\\ &amp;\\rightarrow \\sum_{i=1}^n\\biggl(x_i {x_i}^T - \\bar{x}\\bar{x}^T\\biggr) + \\mathbf{n\\bar{x}\\bar{x}^T} \\\\ &amp;\\rightarrow \\sum_{i=1}^n\\biggl(x_i {x_i}^T - \\bar{x}{x_i}^T - x_i\\bar{x}^T + \\bar{x}\\bar{x}^T \\biggr) + \\mathbf{\\ n\\bar{x}\\bar{x}^T} \\\\ &amp;\\rightarrow \\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^T + n\\bar{x}\\bar{x}^T \\end{align}\\] Let S = \\(\\sum_{i=1}^n (x_i - \\bar{x})(x_i - \\bar{x})^T\\). Therefore, we get: \\[\\begin{align} P{}&amp;(\\mu,\\Lambda|X) \\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((\\beta_0 + n) \\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)\\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)^T \\biggr) \\biggr] \\times \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl(- \\frac{(\\alpha_0\\beta_0 + n\\bar{x})(\\alpha_0\\beta_0 + n\\bar{x})^T }{(\\beta_0 + n)} + \\beta_0\\alpha_0{\\alpha_0}^T + \\mathbf{n\\bar{x}\\bar{x}^T + S} \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr] \\end{align}\\] Second, add a factor so that \\(\\frac{(\\beta_0 + n)}{(\\beta_0 + n)} = 1\\), and then expand the first three terms of the second exponential equation. \\[\\begin{align} &amp;\\rightarrow \\biggl(- \\frac{(\\alpha_0\\beta_0 + n\\bar{x})(\\alpha_0\\beta_0 + n\\bar{x})^T }{(\\beta_0 + n)} + \\beta_0\\alpha_0{\\alpha_0}^T + n \\bar{x}\\bar{x}^T\\biggr) \\\\ &amp;\\rightarrow - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})(\\alpha_0\\beta_0 + n\\bar{x})^T }{(\\beta_0 + n)} + \\beta_0\\alpha_0{\\alpha_0}^T \\frac{(\\beta_0 + n)}{(\\beta_0 + n)} + n \\bar{x}\\bar{x}^T \\frac{(\\beta_0 + n)}{(\\beta_0 + n)} \\\\ &amp;\\rightarrow \\frac{-(\\alpha_0\\beta_0 + n\\bar{x})(\\alpha_0\\beta_0 + n\\bar{x})^T + \\beta_0^2\\alpha_0{\\alpha_0}^T + n \\beta_0\\alpha_0{\\alpha_0}^T + n \\beta_0 \\bar{x}\\bar{x}^T + n^2 \\bar{x}\\bar{x}^T }{(\\beta_0 + n)} \\\\ &amp;\\rightarrow \\frac{ - (\\beta_0^2\\alpha_0\\alpha_0^T + n\\beta_0\\alpha_0\\bar{x}T + n\\beta_0\\bar{x}\\alpha_0^T + n^2\\bar{x}\\bar{x}^T ) + \\beta_0^2\\alpha_0{\\alpha_0}^T + n \\beta_0\\alpha_0{\\alpha_0}^T +n \\beta_0 \\bar{x}\\bar{x}^T +n^2 \\bar{x}\\bar{x}^T }{(\\beta_0 + n)}\\\\ &amp;\\rightarrow \\frac{ - n\\beta_0\\alpha_0\\bar{x}^T - n\\beta_0\\bar{x}{\\alpha_0}^T + n \\beta_0\\alpha_0{\\alpha_0}^T + n \\beta_0 \\bar{x}\\bar{x}^T }{(\\beta_0 + n)}\\\\ &amp;\\rightarrow \\frac{ n\\beta_0(\\bar{x}\\bar{x}^T - \\alpha_0\\bar{x}^T - \\bar{x}{\\alpha_0}^T + \\alpha_0{\\alpha_0}^T ) }{(\\beta_0 + n)}\\\\ &amp;\\rightarrow \\frac{ n\\beta_0}{(\\beta_0 + n)}(\\bar{x} - \\alpha_0)(\\bar{x} - \\alpha_0 )^T \\end{align}\\] Finally, we get the following gaussian-wishart equation: \\[\\begin{align} P(\\mu,\\Lambda|X) {}&amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl((\\beta_0 + n) \\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)\\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)^T \\biggr) \\biggr] \\times \\nonumber \\\\ &amp;exp\\biggl[-\\frac{1}{2}\\Lambda\\biggl( \\frac{ n\\beta_0}{(\\beta_0 + n)}(\\bar{x} - \\alpha_0)( \\bar{x} - \\alpha_0 )^T + S \\biggr) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda) \\biggr]\\\\ \\nonumber \\\\ &amp;\\propto |\\Lambda|^{\\frac{\\nu_0+n-p}{2}} \\nonumber \\\\ &amp;\\underbrace{ exp\\biggl[-\\frac{(\\beta_0 + n) }{2}\\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr)^T\\Lambda\\biggl( \\mu - \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)} \\biggr) \\biggr]}_\\text{gaussian} \\times \\nonumber \\\\ &amp;\\underbrace{exp\\biggl[-\\frac{1}{2}tr\\biggl( \\frac{ n\\beta_0}{(\\beta_0 + n)}(\\bar{x} - \\alpha_0)( \\bar{x} - \\alpha_0 )^T + S + \\Sigma_0^{-1} \\biggr) \\Lambda \\biggr]}_\\text{wishart} \\end{align}\\] Notice that given a Normal Wishart posterior distribution, it becomes apparent that the parameters correspond to the following: \\[\\begin{align} \\alpha_1 &amp;= \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)}\\\\ \\beta_1 &amp;= \\beta_0 + n \\\\ \\nu_1 {}&amp;= \\nu_0 + n \\\\ \\Sigma_1 &amp;= \\frac{ n\\beta_0}{(\\beta_0 + n)}(\\bar{x} - \\alpha_0)( \\bar{x} - \\alpha_0 )^T + S + \\Sigma_0^{-1} \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution using a gaussian-wishart joint distribution: \\[\\begin{align} \\mu, \\Lambda|x\\sim \\ \\mathcal{NW}_p(\\alpha_1, \\beta_1, \\nu_1,\\Sigma_1) = \\mathcal{N}_p(\\alpha_1, \\beta_1)\\times\\mathcal{W}_p(\\nu_1, \\Sigma_1) \\end{align}\\] 7.4.7 Normal-Inverse Wishart Conjugacy The idea is to be able to tailor an Inverse Wishart distribution for posterior given that Inverse Wishart density is conjugate prior for a positive-definite covariance matrix parameter, \\(\\Lambda\\), of a Multivariate Normal Likelihood; the same as we use Inverse Gamma density as conjugate prior for the variance parameter, \\(\\sigma^2\\), of a Univariate Normal Likelihood. For a notation, we use the following: \\[\\begin{align} P(\\Lambda|x) \\propto P(\\Lambda) \\times Lik(\\Lambda|x) \\end{align}\\] For likelihood, we have the following p-variate normal distribution: \\[\\begin{align} X|\\mu,\\Lambda \\sim \\mathcal{N}_p(X; \\mu, \\Lambda) \\ \\ \\ \\ \\ \\text{where}\\ \\mu\\ \\text{is known and }\\ \\Lambda \\text{ is unknown } \\end{align}\\] Recall the below structure. See Chapter 5 (Numerical Probability and Distribution) under Wishart Distribution Section as reference: \\[\\begin{align} X = \\left[\\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1p}\\\\ x_{21} &amp; x_{22} &amp; \\cdots &amp; x_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{n1} &amp; x_{n2} &amp; \\cdots &amp; x_{np} \\end{array}\\right]_\\text{(nxp)} \\ \\ \\ \\ \\mu = \\left[\\begin{array}{c}\\bar{x}_1 \\\\ \\bar{x}_2 \\\\ \\vdots \\\\ \\bar{x}_p \\end{array}\\right] = \\left[\\begin{array}{c}\\mu_1 \\\\ \\mu_2 \\\\ \\vdots \\\\ \\mu_p \\end{array}\\right]_\\text{(1xp)} \\label{eqn:eqnnumber311} \\end{align}\\] \\[\\begin{align} \\Lambda = \\left[\\begin{array}{rrrr}\\sigma^2_{1} &amp; \\sigma_{12} &amp; \\cdots &amp; \\sigma_{1p}\\\\ \\sigma_{21} &amp; \\sigma^2_{2} &amp; \\cdots &amp; \\sigma_{2p} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{p1} &amp; \\sigma_{p2} &amp; \\cdots &amp; \\sigma^2_{p} \\end{array}\\right]_\\text{(pxp)} \\label{eqn:eqnnumber312} \\end{align}\\] For multivariate joint-density likelihood: \\[\\begin{align} Lik_X(\\mu, \\Lambda|x_1,...,x_n) &amp;\\equiv P_X(x_1,...,x_n|\\mu,\\Lambda) \\\\ &amp;= \\prod_{i=1}^n\\frac{|\\Lambda|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left[-\\frac{1}{2}(x_i - \\mu)^T\\Lambda^{-1}(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Lambda|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}\\sum_{i=1}^n (x_i - \\mu)^T\\Lambda^{-1}(x_i - \\mu)\\right]\\\\ &amp;= \\left(\\frac{|\\Lambda|^{-\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}}\\right)^n exp\\left[-\\frac{1}{2}tr(\\Lambda^{-1} \\Sigma)\\right] \\end{align}\\] It helps to be aware of the following mathematical manipulation (derivation not included): \\[\\begin{align} \\sum_{i=1}^n (x_i - \\mu)^T\\Lambda^{-1}(x_i - \\mu) = tr( \\Lambda^{-1} \\Sigma)\\ \\ \\ \\ \\ where\\ \\ \\ \\Sigma = \\sum_{i=1}^n (x_i - \\mu)(x_i - \\mu)^T \\end{align}\\] For prior, we choose an Inverse Wishart distribution for the \\(\\Lambda\\) parameter: \\[\\begin{align} \\Lambda \\sim IW_p(v_0, \\Sigma_0)\\ \\ \\ \\ \\ \\text{where}\\ v_0\\ and\\ \\Sigma_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\Lambda) = P(\\Lambda; \\nu_0, \\Sigma_0) = \\frac{ |\\Lambda|^{-\\frac{\\nu_0 +p+1}{2}} exp\\left[-\\frac{1}{2}tr(\\Lambda^{-1}\\Sigma_0)\\right]} {2^{\\frac{\\nu_0 p}{2}}|\\Sigma_0|^{-\\frac{\\nu_0}{2}}\\ \\Gamma_p\\left(\\frac{\\nu_0}{2}\\right)} \\end{align}\\] Recall description of Inverse Wishart notation in Chapter 5 (Numerical Probability and Distribution) under Wishart Distribution Section. For posterior, we want to arrive at an Inverse Wishart density given an observed data: \\[\\begin{align} \\Lambda|x \\sim \\ \\mathcal{W}^{-1}_p(v_1,\\Sigma_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\Lambda\\) by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents). For an Inverse Wishart posterior with joint-density likelihood: \\[\\begin{align} \\mathcal{W}(\\Lambda|x_{1:n}) {}&amp;\\propto \\underbrace{ \\frac{ |\\Lambda|^{-\\frac{\\nu_0 +p+1}{2}} exp\\left[-\\frac{1}{2}tr(\\Lambda^{-1}\\Sigma_0)\\right]} {2^{\\frac{\\nu_0 p}{2}}|\\Sigma_0|^{-\\frac{\\nu_0}{2}}\\ \\Gamma_p\\left(\\frac{\\nu_0}{2}\\right)} }_\\text{wishart prior} \\times \\underbrace{ \\left(\\frac{|\\Lambda|^{\\frac{1}{2}}}{\\sqrt{2\\pi}}\\right)^n exp\\left[\\Lambda^{-1}\\Sigma \\right]}_\\text{normal likelihood} \\\\ \\rightarrow &amp;\\text{(drop constants)} \\nonumber \\\\ &amp;\\propto |\\Lambda|^{-\\frac{(\\nu_0+n)+p+1}{2}} exp\\left[-\\frac{1}{2}tr(\\Lambda^{-1}\\Sigma_0)\\right] \\times exp\\left[\\Lambda^{-1} \\Sigma \\right] \\\\ &amp;\\propto |\\Lambda|^{-\\frac{(\\nu_0+n)+ p + 1}{2}} exp\\left[-\\frac{1}{2} tr\\left( \\Lambda^{-1}\\Sigma_0 + \\Sigma^{-1} \\Sigma \\right) \\right] \\\\ &amp;\\propto |\\Lambda|^{-\\frac{(\\nu_0+n) + p+1}{2}} exp\\left[-\\frac{1}{2} tr\\left( \\Lambda^{-1} ( \\Sigma_0 + \\Sigma)\\right) \\right] \\end{align}\\] Notice that given an Inverse Wishart posterior distribution, it becomes apparent that the parameters \\(v_1\\) and \\(\\Sigma_1\\) correspond to the following: \\[\\begin{align} \\Sigma_1 = (\\Sigma_0 + \\Sigma)^{-1}\\ \\ \\ \\ \\ \\ \\ \\ \\nu_1 = \\nu_0 + n \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\Lambda|x_1,...,x_n \\sim \\ \\mathcal{IW}_p(\\nu_1,\\Sigma_1)\\ \\ \\ \\rightarrow \\mathcal{W}^{-1}_p(\\nu_0 + n\\ ,\\ (\\Sigma_0 + \\Sigma)^{-1}) \\end{align}\\] For MAP we can use the following equation: \\[\\begin{align} \\Lambda_{(map)} = \\frac{\\Lambda_1}{\\nu_1 + p + 1} \\end{align}\\] 7.4.8 Normal-LKJ Conjugacy The idea is to be able to tailor an LKJ distribution for posterior. We leave readers to investigate this conjugacy as a modern alternative to the normal Wishart conjugacy. While normal Wishart conjugacy operates on the covariance of MVN, normal LKJ conjugacy operates on correlation. 7.4.9 Binomial-Beta Conjugacy The idea is to be able to tailor a Beta density distribution for posterior given that Beta density is conjugate prior for a Binomial likelihood. For a notation, let us use the following: \\[\\begin{align} P(\\rho|x) \\propto P(\\rho) \\times Lik(n,\\rho|x) \\ \\ \\ \\ \\ \\ \\text{for } 0 \\le \\rho\\ \\le 1 \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|n,\\rho \\sim Binom(n,\\rho) \\end{align}\\] For marginal (Bernoulli) likelihood: \\[\\begin{align} Lik_X(n,\\rho|x) \\equiv P(X=x|n,\\rho) = \\binom{n}{x}\\rho^x(1 - \\rho)^{n-x} \\end{align}\\] For joint (Binomial) likelihood: \\[\\begin{align} Lik_X(n_1,...,n_m,\\rho|x_1,...,x_m) \\equiv P_X(x_1,...,x_m|n_1,...,n_m,\\rho) = \\prod_{i=1}^m \\binom{n_i}{x_i}\\rho^{x_i}(1 - \\rho)^{n_i - x_i} \\end{align}\\] For prior, we choose a Beta distribution for the \\(\\rho\\) parameter: \\[\\begin{align} \\rho \\sim Beta(\\alpha_0,\\beta_0)\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ \\text{and}\\ \\beta_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\rho) = P(\\rho; \\alpha_0, \\beta_0) = \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\end{align}\\] For posterior, we want to arrive at a Beta density given an observed data: \\[\\begin{align} \\rho|x \\sim Beta(\\alpha_1,\\beta_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\rho\\) by dropping the constants that do not affect the shape or proportionality of the distribution. For a Beta posterior with marginal (Bernoulli) likelihood: \\[\\begin{align} P(\\rho|x) {}&amp;\\propto \\underbrace{ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1}}_\\text{beta prior} \\times \\underbrace{ \\binom{n}{x}\\rho^x(1 - \\rho)^{n-x} }_\\text{binomial likelihood} \\\\ \\rightarrow &amp;\\text{(drop constants)} \\nonumber \\\\ P(\\rho|x) &amp;\\propto \\left(\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\right) \\times \\left( \\rho^x(1 - \\rho)^{n-x} \\right) \\\\ &amp;\\propto \\left(\\rho^{(\\alpha_0 + x) - 1}(1 - \\rho)^{(\\beta_0 + n - x) - 1} \\right) \\end{align}\\] Notice that given a Beta posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = \\alpha_0 + x \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = \\beta_0 + (n - x) \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x \\sim Beta(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Beta(\\alpha_0 + x,\\ \\beta_0 + (n - x)) \\end{align}\\] For a Beta posterior with joint (Binomial) likelihood: \\[\\begin{align} P(n_{1:m},\\rho|x_1,...,x_m) {}&amp;\\propto \\underbrace{ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} }_\\text{beta prior} \\times \\underbrace{ \\prod_{i=1}^m \\binom{n_i}{x_i}\\rho^{x_i}(1 - \\rho)^{n-x_i} }_\\text{binomial likelihood} \\\\ \\rightarrow &amp; \\text{(drop constants)} \\nonumber \\\\ P(n_{1:m},\\rho|x_1,...,x_m) &amp;\\propto \\left(\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\right) \\times \\left( \\rho^{\\sum_{i=1}^m x_i}(1 - \\rho)^{\\sum_{i=1}^m n_i-\\sum_{i=1}^m x_i} \\right) \\\\ &amp;\\propto \\left(\\rho^{(\\sum_{i=1}^m x_i + \\alpha_0) - 1}(1 - \\rho)^{(\\sum_{i=1}^m n_i + \\beta_0 - \\sum_{i=1}^m x_i) - 1} \\right) \\\\ &amp;\\propto \\left(\\rho^{(\\alpha_0 + m \\bar{x} ) - 1}(1 - \\rho)^{( \\beta_0 + ( m \\bar{n} - m \\bar{x})) - 1} \\right) \\end{align}\\] Notice that given a Beta posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = \\alpha_0 + m \\bar{x} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = \\beta_0 + ( m \\bar{n} - m \\bar{x}) \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x_1,...,x_n \\sim Beta(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Beta(\\alpha_0 + m \\bar{x},\\ \\beta_0 + ( m \\bar{n} - m \\bar{x})) \\end{align}\\] 7.4.10 Geometric-Beta Conjugacy The idea is to be able to tailor a Beta density distribution for posterior given that Beta density is conjugate prior for a Geometric likelihood. For a notation, let us use the following: \\[\\begin{align} P(\\rho|x) \\propto P(\\rho) \\times Lik(\\rho|x) \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\rho \\sim Geo(\\rho) \\end{align}\\] For marginal-density likelihood: \\[\\begin{align} Lik_X(\\rho|x) \\equiv P(X=x|\\rho) = \\rho(1 - \\rho)^{x - 1} \\end{align}\\] For joint-density likelihood: \\[\\begin{align} Lik_X(\\rho|x_1,...,x_n) \\equiv P_X(x_1,...,x_n|\\rho) = \\prod_{i=1}^n \\rho(1 - \\rho)^{x - 1} \\end{align}\\] For prior, we choose a Beta distribution for the \\(\\rho\\) parameter: \\[\\begin{align} \\rho \\sim Beta(\\alpha_0,\\beta_0)\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ \\text{and}\\ \\beta_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\rho) = P(\\rho; \\alpha_0, \\beta_0) = \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\end{align}\\] For posterior, we want to arrive at a Beta density given an observed data: \\[\\begin{align} \\rho|x \\sim Beta(\\alpha_1,\\beta_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\rho\\) by dropping the constants that do not affect the shape or proportionality of the distribution. For a Beta posterior with marginal-density likelihood: \\[\\begin{align} P(\\rho|x) {}&amp;\\propto \\underbrace{ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1}}_\\text{beta prior} \\times \\underbrace{ \\rho(1 - \\rho)^{x - 1} }_\\text{geometric likelihood} \\\\ \\rightarrow &amp; \\text{(drop constants)} \\nonumber \\\\ P(\\rho|x) &amp;\\propto \\left(\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\right) \\times \\left( \\rho (1 - \\rho)^{x - 1} \\right) \\\\ &amp;\\propto \\left(\\rho^{(1 + \\alpha_0) - 1}(1 - \\rho)^{(x + \\beta_0 - 1) - 1} \\right) \\end{align}\\] Notice that given a Beta posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = 1 + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = x + \\beta_0 - 1 \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x \\sim Beta(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Beta(1 + \\alpha_0,\\ x + \\beta_0 - 1 ) \\end{align}\\] For a Beta posterior with joint-density likelihood: \\[\\begin{align} P(\\rho|x_1,...,x_n) {}&amp;\\propto \\underbrace{ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1}}_\\text{beta prior} \\times \\underbrace{ \\prod_{i=1}^n \\rho(1 - \\rho)^{x_i - 1}}_\\text{geometric likelihood} \\\\ \\rightarrow &amp; \\text{(drop constants)} \\nonumber \\\\ P(\\rho|x_1,...,x_n) &amp;\\propto \\left(\\rho^{\\alpha_0-1}(1 - \\rho)^{\\beta_0 - 1} \\right) \\times \\left( \\rho^n(1 - \\rho)^{\\sum_{i=1}^n x_i - n} \\right) \\\\ &amp;\\propto \\left(\\rho^{(n + \\alpha_0) - 1}(1 - \\rho)^{(\\sum_{i=1}^n x_i + \\beta_0 - n ) - 1} \\right) \\\\ &amp;\\propto \\left(\\rho^{(n + \\alpha_0) - 1}(1 - \\rho)^{(n \\bar{x} + \\beta_0 - n) - 1} \\right) \\end{align}\\] Notice that given a Beta posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = n + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = n \\bar{x} + \\beta_0 - n \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x_1,...,x_n \\sim Beta(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Beta(n + \\alpha_0,\\ n \\bar{x} + \\beta_0 - n) \\end{align}\\] 7.4.11 Poisson-Gamma Conjugacy The idea is to be able to tailor a Gamma density distribution for posterior given that Gamma density is conjugate prior for a Poisson likelihood. For a notation, let us use the following: \\[\\begin{align} P(\\lambda|x) \\propto P(\\lambda) \\times Lik(\\lambda|x) \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\lambda \\sim Pois(\\lambda) \\end{align}\\] For marginal-density likelihood: \\[\\begin{align} Lik_X(\\lambda|x) \\equiv P(X=x|\\lambda) = \\frac{1}{x!} \\lambda^x e^{-\\lambda} \\end{align}\\] For joint-density likelihood: \\[\\begin{align} Lik_X(\\lambda|x_1,...,x_n) \\equiv P_X(x_1,...,x_n|\\lambda) = \\prod_{i=1}^n \\frac{1}{x!} \\lambda^x e^{-\\lambda} \\end{align}\\] For prior, we choose a Gamma distribution for the \\(\\lambda\\) parameter: \\[\\begin{align} \\lambda \\sim Gamma(\\alpha_0,\\beta_0)\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ \\text{and}\\ \\beta_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\lambda) = P(\\lambda; \\alpha_0, \\beta_0) = \\frac{1}{\\beta_0^{\\alpha_0} \\Gamma(\\alpha_0)} \\lambda^{\\alpha_0-1} e^ {-\\frac{\\lambda}{\\beta_0}} = \\frac{\\beta^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\lambda^{\\alpha_0-1}e^{-\\beta_0 \\lambda} \\end{align}\\] For a posterior, we want to arrive at a Gamma density given an observed data: \\[\\begin{align} \\lambda|x \\sim Gamma(\\alpha_1, \\beta_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\lambda\\) by dropping the constants that do not affect the shape or proportionality of the distribution. For a Gamma posterior with marginal (Bernoulli) likelihood: \\[\\begin{align} P(\\lambda|x) {}&amp;\\propto \\underbrace{ \\left(\\frac{\\beta_0^{\\alpha_0}}{ \\Gamma(\\alpha_0)}\\right) \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda} }_\\text{gamma prior} \\times \\underbrace{ \\left(\\frac{1}{x!}\\right) \\lambda^x e^{-\\lambda} }_\\text{poisson likelihood} \\\\ \\rightarrow &amp;\\text{(drop constants)} \\nonumber \\\\ P(\\lambda|x) &amp;\\propto \\left( \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda}\\right) \\times \\left( \\lambda^x e^{-\\lambda} \\right) \\\\ &amp;\\propto \\left(\\lambda^{x + \\alpha_0-1} e^ {-\\lambda - \\beta_0 \\lambda} \\right) \\\\ &amp;\\propto \\left(\\lambda^{(x + \\alpha_0)-1} e^ {-(1 + \\beta_0) \\lambda} \\right) \\end{align}\\] Notice that given a Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = x + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = 1 + \\beta_0 \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\lambda|x \\sim Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Gamma(x + \\alpha_0, 1 + \\beta_0) \\end{align}\\] For a Gamma posterior with joint (Binomial) likelihood: \\[\\begin{align} P(\\lambda|x) {}&amp;\\propto \\underbrace{ \\left(\\frac{\\beta_0^{\\alpha_0}}{ \\Gamma(\\alpha_0)}\\right) \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda}}_\\text{gamma prior} \\times \\underbrace{ \\prod_{i=1}^n \\left(\\frac{1}{x_i!}\\right) \\lambda^{x_i} e^{-\\lambda} }_\\text{poisson likelihood}\\\\ \\rightarrow &amp; \\text{(drop constants)} \\nonumber \\\\ P(\\lambda|x) &amp;\\propto \\left( \\lambda^{\\alpha_0-1} e^ {- \\beta_0 \\lambda}\\right) \\times \\left( \\lambda^{n \\bar{x}} e^{-n \\lambda} \\right),\\ \\ \\ \\ \\ n \\bar{x} = \\sum_{i=1}^n x_i \\\\ &amp;\\propto \\left( \\lambda^{(n \\bar{x} +\\alpha_0)-1} e^ {-(n + \\beta_0) \\lambda}\\right) \\end{align}\\] Notice that given a Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = n \\bar{x} + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = n + \\beta_0 \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\lambda|x_1,...,x_n \\sim Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Gamma(n \\bar{x} + \\alpha_0, n + \\beta_0) \\end{align}\\] 7.4.12 Exponential-Gamma Conjugacy The idea is to be able to tailor a Gamma density distribution for posterior given that Gamma density is conjugate prior for an Exponential likelihood. For a notation, let us use the following: \\[\\begin{align} P(\\lambda|x) \\propto P(\\lambda) \\times Lik(\\lambda|x) \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\lambda \\sim Expo(\\lambda) \\end{align}\\] For marginal-density likelihood: \\[\\begin{align} Lik_X(\\lambda|x) \\equiv P(X=x|\\lambda) = \\lambda e^{-\\lambda x} \\end{align}\\] For joint-density likelihood: \\[\\begin{align} Lik_X(\\lambda|x_1,...,x_n) \\equiv P_X(x_1,...,x_n|\\lambda) = \\prod_{i=1}^n \\lambda e^{-\\lambda x} \\end{align}\\] For prior, we choose a Gamma distribution for the \\(\\lambda\\) parameter: \\[\\begin{align} \\lambda \\sim Gamma(\\alpha_0,\\beta_0)\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_0\\ \\text{and}\\ \\beta_0 \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\lambda) = P(\\lambda; \\alpha_0, \\beta_0) = \\frac{1}{\\beta_0^{\\alpha_0} \\Gamma(\\alpha_0)} \\lambda^{\\alpha_0-1} e^ {-\\frac{\\lambda}{\\beta_0}} = \\frac{\\beta^{\\alpha_0}}{\\Gamma(\\alpha_0)}\\lambda^{\\alpha_0-1}e^{-\\beta_0 \\lambda} \\end{align}\\] For a posterior, we want to arrive at a Gamma density given an observed data: \\[\\begin{align} \\lambda|x \\sim Gamma(\\alpha_1, \\beta_1) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\lambda\\) by dropping the constants that do not affect the shape or proportionality of the distribution. For a Gamma posterior with marginal-density likelihood: \\[\\begin{align} P(\\lambda|x) {}&amp;\\propto \\underbrace{ \\left(\\frac{\\beta_0^{\\alpha_0}}{ \\Gamma(\\alpha_0)}\\right) \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda}}_\\text{gamma prior} \\times \\underbrace{ \\lambda e^{-\\lambda x} }_\\text{exponential likelihood}\\\\ \\rightarrow &amp; \\text{(drop constants)} \\nonumber \\\\ P(\\lambda|x) &amp;\\propto \\left( \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda}\\right) \\times \\left( \\lambda e^{-\\lambda x} \\right) \\\\ &amp;\\propto \\left(\\lambda^{(1 + \\alpha_0) - 1} e^ {-(1 + \\beta_0) \\lambda} \\right) \\end{align}\\] Notice that given a Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = 1 + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = 1 + \\beta_0 \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\lambda|x \\sim Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Gamma(1 + \\alpha_0, 1 + \\beta_0) \\end{align}\\] For a Gamma posterior with joint-density likelihood: \\[\\begin{align} P(\\lambda|x) {}&amp;\\propto \\underbrace{ \\left(\\frac{\\beta_0^{\\alpha_0}}{ \\Gamma(\\alpha_0)}\\right) \\lambda^{\\alpha_0-1} e^ {-\\beta_0 \\lambda} }_\\text{gamma prior} \\times \\underbrace{ \\prod_{i=1}^n \\lambda e^{-\\lambda x_i}}_\\text{exponential likelihood} \\\\ \\rightarrow &amp;\\text{(drop constants)} \\nonumber \\\\ P(\\lambda|x) &amp;\\propto \\left( \\lambda^{\\alpha_0-1} e^ {- \\beta_0 \\lambda}\\right) \\times \\left( \\lambda^{n} e^{-\\lambda n \\bar{x}} \\right),\\ \\ \\ \\ \\ n \\bar{x} = \\sum_{i=1}^n x_i \\\\ &amp;\\propto \\left( \\lambda^{(n +\\alpha_0)-1} e^ {-(n \\bar{x} + \\beta_0) \\lambda}\\right) \\end{align}\\] Notice that given a Gamma posterior distribution, it becomes apparent that the parameters \\(\\alpha_1\\) and \\(\\beta_1\\) correspond to the following: \\[\\begin{align} \\alpha_1 = n + \\alpha_0 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta_1 = n \\bar{x} + \\beta_0 \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\lambda|x_1,...,x_n \\sim Gamma(\\alpha_1, \\beta_1)\\ \\ \\ \\rightarrow Gamma(n + \\alpha_0, n \\bar{x}+ \\beta_0) \\end{align}\\] 7.4.13 Multinomial-Dirichlet Conjugacy The idea is to be able to tailor a Dirichlet density distribution for posterior given that Dirichlet density is conjugate prior for a Multinomial (or Categorical) likelihood. For a notation, let us use the following: \\[\\begin{align} P(\\rho|x) \\propto P(\\rho) \\times Lik(n, \\rho|x) \\end{align}\\] For likelihood, we have the following distribution: \\[\\begin{align} x|\\rho\\sim Multi(n,\\rho) \\end{align}\\] \\[\\begin{align} Lik_{X_{1:k}}(n, \\rho|x_1,...,x_k) \\equiv P_{X_{1:k}}(x_1,...,x_k|n, \\rho) {}&amp;= \\frac{n!}{x_1! \\times ... \\times x_k!} \\rho_1^{x_1} \\times ... \\times \\rho_k^{x_k}\\\\ &amp;= \\frac{n!}{\\prod_{i=1}^k x_i!} \\prod_{i=1}^k \\rho_i^{x_i} \\end{align}\\] For prior, we choose a Dirichlet distribution for the \\(\\lambda\\) parameter: \\[\\begin{align} \\rho \\sim Dir(\\alpha_{0_{1:k}})\\ \\ \\ \\ \\ \\text{where}\\ \\alpha_{0_{1:k}}\\ \\text{ are known } \\mathbf{hyperparameters} \\end{align}\\] \\[\\begin{align} \\mathcal{\\pi}(\\rho) = P(\\rho; \\alpha_{0_{1:k}}) = \\frac{1}{\\mathcal{B}(\\alpha_{0_{1:k}})} \\prod_{i=1}^k \\rho_i^{\\alpha_{0_i}-1},\\ \\ \\ \\ \\ where\\ \\alpha_{0_{1:k}} = (\\alpha_{0_1},...,\\alpha_{0_k}) \\end{align}\\] and where: \\[\\begin{align} \\mathcal{B}(\\alpha_{0_{1:k}}) = \\frac{\\Gamma(\\alpha_{0_1})\\times ...\\times \\Gamma(\\alpha_{0_k})}{\\Gamma(\\alpha_{0_1} + ... + \\alpha_{0_k})} \\end{align}\\] For a posterior, we want to arrive at a Dirichlet density given an observed data: \\[\\begin{align} \\rho|x_1,...,x_k \\sim Dir(\\alpha_{1_{1:k}}),\\ \\ \\ \\ \\ where\\ \\alpha_{1_{1:k}} = (\\alpha_{1_1},...,\\alpha_{1_k}) \\end{align}\\] However, let us first derive the posterior density with respect to \\(\\rho\\) by dropping the constants that do not affect the shape or proportionality of the distribution. \\[\\begin{align} P(\\rho|x_1,...,x_k) {}&amp;\\propto \\underbrace { \\frac{1}{\\mathcal{B}(\\alpha_0)} \\prod_{i=1}^k \\rho_i^{\\alpha_{0_i}-1} }_\\text{dirichlet prior} \\times \\underbrace{ \\frac{n!}{\\prod_{i=1}^k x_i!} \\prod_{i=1}^k \\rho_i^{x_i} }_\\text{multinomial likelihood}\\\\ \\rightarrow &amp;\\text{(drop constants)} \\nonumber \\\\ P(\\rho|x_1,...,x_k) &amp;\\propto \\left( \\prod_{i=1}^k \\rho_i^{\\alpha_{0_i}-1} \\right) \\times \\left( \\prod_{i=1}^k \\rho_i^{x_i} \\right) \\\\ &amp;\\propto \\left( \\prod_{i=1}^k \\rho_i^{(x_i + \\alpha_{0_i}) - 1}\\right) \\end{align}\\] Notice that given a Dirichlet posterior distribution, it becomes apparent that the parameters \\(\\rho_1\\) corresponds to the following: \\[\\begin{align} \\alpha_{1_{1:k}} = \\sum_{i=1}^k (x_i + \\alpha_{0_i}) \\end{align}\\] Therefore, we arrive at the following reparameterized posterior distribution: \\[\\begin{align} \\rho|x_1,...,x_k \\sim Dir(\\alpha_{1_{1:k}})\\ \\ \\ \\rightarrow Dir(\\sum_{i=1}^k (x_i + \\alpha_{0_i})) \\end{align}\\] Application of this conjugacy becomes apparent in Variational Bayes section. 7.4.14 Hyperparameters A proper prior is as essentially fitting as the chosen quantities of its hyperparameters. Depending on its hyperparameters, a prior may stretch within the spectrum of being weakly informed and well informed. The idea that a prior does not have information may be contested because there is always truly information about a prior. Therefore, we can say that non-informative prior is a misnomer. From that perspective, we also use weakly informative prior. Morever, we can use other terms such as vague prior, objective prior, imprecise prior, and insufficient prior. One way to complement and at the same time improve our weakly informative prior knowledge is to keep accumulating evidence and to keep seeking prior knowledge from domain experts. Consequently, our goal is to achieve a well-behaved proper posterior. However, with only an initial piece of evidence to use on hand and minimal expert knowledge, we can use a uniform or flat prior instead. A uniform prior is improper in that it integrates infinitely; however, it leads to a proper posterior when combined with likelihood. An example set of hyperparameter quantities used for Normal flat prior is: \\[\\begin{align} \\mu \\sim U(a = 0, b = 1) = Beta(\\alpha_0 = 1, \\beta_0 = 1)\\ \\ \\text{(weakly-informed prior)} \\end{align}\\] An example set of hyperparameter quantities used for Beta flat prior is: \\[\\begin{align} \\rho \\sim Beta\\left(\\alpha_0 = \\frac{1}{2}, \\beta_0 = \\frac{1}{2}\\right)\\ \\ \\text{(weakly-informed prior)}, \\ \\ \\ \\ \\alpha_0 &gt; 0,\\ \\beta_0 &gt; 0 \\end{align}\\] An example set of hyperparameter quantities used for Inverse Gamma flat prior is: \\[\\begin{align} \\sigma^2 \\sim Inv. Gamma \\left(\\alpha_0 = 1, \\beta_0 = 1 \\right) \\ \\ \\text{(weakly-informed prior)} \\end{align}\\] An example set of hyperparameter quantities used for Gamma flat prior is: \\[\\begin{align} \\lambda \\sim Gamma\\left(\\alpha_0 = \\frac{1}{2}, \\beta_0 = \\frac{1}{2}\\right)\\ \\ \\text{(weakly-informed prior)} \\end{align}\\] Figure 7.8 shows graph of the prior distribution with corresponding hyperparameter. Figure 7.8: Flat Prior (Hyperparameter) We leave readers to investigate the topic around Jeffrey’s prior for the chosen hyperparameters above. Also, we illustrate the use of flat prior and hyperparameter in the Bayesian modeling section. In summary, Table 7.3 lists conjugate priors for a few distribution families corresponding to their likelihood distributions. That includes the corresponding hyperparameters. Table 7.3: Conjugate Prior-Posterior Conjugacy Family General Notation Prior HyperParameter Likelihood Normal \\(\\mu\\mid\\sigma^2 \\sim N(\\mu,\\sigma^2)\\) \\(\\mu_0, \\sigma^2_0\\) Normal (unknown \\(\\mu\\)) Inverse Gamma \\(\\sigma^2\\mid\\mu \\sim Inv.\\Gamma(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Normal (unknown \\(\\sigma^2\\)) Normal \\(\\mu,\\sigma^2 \\sim N(\\mu,\\sigma^2)\\) \\(\\mu_0, \\sigma^2_0\\) Normal (unknown \\(\\mu\\),\\(\\sigma^2\\)) Gamma \\(\\mu,\\sigma^2 \\sim \\Gamma(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Normal (unknown \\(\\mu\\),\\(\\sigma^2\\)) Beta \\(\\rho \\sim Beta(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Binomial Beta \\(\\rho \\sim Beta(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Geometric Gamma \\(\\lambda \\sim \\Gamma(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Poisson Gamma \\(\\lambda \\sim \\Gamma(\\alpha,\\beta)\\) \\(\\alpha_0, \\beta_0\\) Exponential Dirichlet \\(\\rho \\sim Dir(\\alpha)\\) \\(\\alpha_0\\) Multinomial 7.5 Information Theory In this section, we introduce Information Theory. Some concepts are helpful when we cover Variational Bayes, especially around quantifying information. There are cases in which we need to transform our data set, and it is natural for us to compare the original (actual) data set and the transformed data set. To do that, we use measures such as Entropy, Information gain, Mutual Information, Gini Index, Kullback-Leibler divergence, and many others, which are metrics used to quantify information, mainly optimized by a cost function commonly denoted as \\(\\mathcal{J}(\\theta)\\) in machine learning. Here, we reference the great works of Cover T.M. (2006) and Ebrahimi N. et al. (2010). For most of the discussions in this section, we settle only on Gaussian and Binomial processes; albeit, we do not restrict ourselves to only those distributions. Other familiar distributions should apply. Equivalently, continuous random variables do apply as well. 7.5.1 Information In this section, information is based on quantifying the uncertainty of random events. Rare events tend to be more uncertain, and when they happen, they become more surprising (more impure). Such rare events require additional information. The amount of information to measure is called Shannon information and can be expressed as such: \\[\\begin{align} \\mathcal{I}(X) = - \\log_e P(X) \\end{align}\\] 7.5.2 Entropy We describe Entropy, also called Shannon Entropy, in terms of the degree of randomness, uncertainty, impurity, or disorder based on the amount of information. Entropy is also discussed in Physics and Thermodynamics. It measures the amount of information required to eliminate the degree of randomness or uncertainty (see Figure 7.9). Figure 7.9: Entropy Entropy is denoted by the symbol \\(\\mathcal{H}(X)\\) and is expressed as: \\[\\begin{align} \\mathcal{H}(X) = \\begin{cases} - \\sum_x P_X(x)\\ \\log_e P_X(x) &amp; \\text{(discrete entropy)}\\\\ \\\\ - \\int_x P_X(x)\\ \\log_e P_X(x) dx &amp; \\text{(continuous differential entropy)}\\\\ \\end{cases} \\label{eqn:eqnnumber313} \\end{align}\\] Entropy (\\(\\mathcal{H}\\)) measures the level of impurity (or surprise) of the probability of an outcome. If the expected information of a random event is always 100% to the point of perfection (or purity), then it becomes unsurprising. The entropy is therefore zero. For example, if we have a coin with a head on both sides, no matter how many times we flip the coin, there is always a 100% probability that it lands on a head. Thus, there are zero surprises right there. \\[\\begin{align} \\mathcal{H}(X) = -P(x) log P(x) = -1 \\times \\log_e (1) = 0,\\ \\ \\ \\ \\ \\ where\\ P(x) = 100\\% \\end{align}\\] Now, let us assume that we have four coins. As strange as it may sound, let us suppose three coins have heads on both sides, and the last coin has tails on both sides. Let us then compute the entropy of the set. \\[\\begin{align} \\mathcal{H}(X) &amp;= - P(x_1) \\log_e P(x_1) - P(x_2) \\log_e P(x_2) \\\\ &amp;= - \\frac{3}{4} \\log_e \\left(\\frac{3}{4} \\right) - \\frac{1}{4} \\log_e \\left( \\frac{1}{4} \\right) \\nonumber \\\\ &amp;= - (-0.2157616) - (-0.3465736 ) \\nonumber \\\\ &amp;= 0.5623352 \\nonumber \\end{align}\\] Notice that the higher the entropy, the more we see some information content. This measurement is proper when we split a dataset into corresponding features, similar to the techniques in decision trees in machine learning. Another contending measurement is the Gini Index which we discuss in the following subsection. Now, let us consider Joint Entropy, which has the following equation: \\[\\begin{align} \\mathcal{H}(X, Y) = - \\sum_{x \\in X} \\sum_{y \\in Y} P(x,y) \\log_e P(x,y) \\end{align}\\] Joint Entropy computes the entropy of all possible pairs of two events. For example, if \\(X \\in \\{A,B\\}\\) and \\(Y \\in \\{C,D\\}\\), and we have the following probabilities of each pair of combination: \\[\\begin{align*} P(X=A,Y=C) {}&amp;= 0.30\\ \\ \\ \\ \\ \\ \\ P(X=A,Y=D) = 0.30\\ \\ \\ \\ \\ \\ \\\\ P(X=B,Y=C) &amp;= 0.20\\ \\ \\ \\ \\ \\ \\ P(X=B,Y=D) = 0.20 \\end{align*}\\] then we can compute for the Joint Entropy: \\[\\begin{align*} \\mathcal{H}(X, Y) {}&amp;= - \\left[\\ 0.30 \\log_e (0.30) + 0.30 \\log_e (0.30) + 0.20 \\log_e (0.20) + 0.20 \\log_e (0.20)\\ \\right]\\\\ &amp;= 1.366159 \\end{align*}\\] Conditional Entropy computes the entropy of one possible event given another event which has the following equation: \\[\\begin{align} \\mathcal{H}(X|Y) = - \\sum_{x \\in X} P(x) \\sum_{y \\in Y} P(y|x) \\log_e P(y|x) \\end{align}\\] Cross-Entropy is another concept for measuring the difference between two distributions: one being the actual distribution, namely \\(P(x)\\), and the other being a training (approximating) distribution, namely \\(\\mathcal{Q}(x)\\). It has the following equation: \\[\\begin{align} \\mathcal{H}_Q(P) \\equiv \\mathcal{H}(P,Q) = \\begin{cases} - \\sum_x P_X(x)\\ \\log_e \\mathcal{Q}_X(x) &amp; \\text{(discrete)}\\\\ \\\\ - \\int_x P_X(x)\\ \\log_e \\mathcal{Q}_X(x) dx &amp; \\text{(continuous)}\\\\ \\end{cases} \\label{eqn:eqnnumber314} \\end{align}\\] A cross-entropy value of zero means that the two distributions are almost identical. The higher the value, the farther away the two distributions are alike. In a later section, we discuss KL divergence, which is another measurement of closeness between two distributions such that we can express the divergence this way (given an actual distribution (P) and an estimated distribution (Q)): \\[\\begin{align} \\text{KL Divergence} = \\underbrace{H(P,Q)}_{\\text{cross entropy}} - \\underbrace{H(P)}_{\\text{entropy}} \\end{align}\\] Note that cross-entropy is used in machine learning as a loss function, generally replacing the common notation \\(\\mathcal{J}(\\theta)\\) with \\(\\mathcal{H}_Q(P)\\). Note that the notation can be confused with a joint entropy notation. In our case, we use \\(\\mathcal{H}_Q(P)\\) to refer to cross-entropy. 7.5.3 Gini Index Gini Index, also called Gini Impurity, is denoted by the symbol \\(\\mathcal{G}(X)\\) and is expressed as: \\[\\begin{align} \\mathcal{G}(X) = \\underbrace{1 - \\underbrace{\\sum_x P_X(x)^2}_{\\text{Gini}}}_{\\text{Gini Impurity}} \\end{align}\\] Gini Index (\\(\\mathcal{G}\\)) can be used to measure the impurity of information similar to Entropy. To illustrate, we use the same example we used for Entropy. Suppose we have four coins. The three coins have heads on both sides, and the last coin has tails on both sides. Compute for the Gini index of the set. \\[\\begin{align} \\mathcal{G} {}&amp;= 1 - ( P(x_1)^2 + P(x_2)^2 \\\\ &amp;= 1 - \\left[ \\left(\\frac{3}{4}\\right)^2 + \\left(\\frac{1}{4}\\right)^2 \\right] \\nonumber \\\\ &amp;= 1 - (0.5625 + 0.0625 ) = 1 - 0.625 \\nonumber \\\\ &amp;= 0.375 \\nonumber \\end{align}\\] Similarly, if we have a coin with one side head and another side also head, no matter how many times we flip the coin, there is always a 100% probability that it lands on the head. Thus, there are zero surprises right there. \\[\\begin{align} \\mathcal{G} = 1- P(x)^2 = -1 \\times (1)^2 = 0,\\ \\ \\ \\ \\ \\ where\\ P(x) = 100\\% \\end{align}\\] Therefore, similar to Entropy, the higher the Gini index, the more we see some information content. 7.5.4 Information Gain Information Gain is denoted by the symbol \\(\\mathcal{I}(T,X)\\) and is expressed like so: for Entropy: \\[\\begin{align} \\mathcal{I}_{entropy}(T, X) {}&amp;= \\mathcal{H}(parent) - \\binom{weighted}{average} \\mathcal{H}(children) \\\\ &amp;= \\mathcal{H}(T) - \\mathcal{H}(T,X)\\ \\ \\ \\ \\ \\ where\\ \\sum_x \\mathcal{H}(x) \\end{align}\\] for Gini: \\[\\begin{align} \\mathcal{I}_{gini}(T, X) {}&amp;= \\mathcal{G}(parent) - \\binom{weighted}{average} \\mathcal{G}(children) \\\\ &amp;= \\mathcal{G}(T) - \\mathcal{G}(T,X)\\ \\ \\ \\ \\ \\ where\\ \\sum_x \\mathcal{G}(x) \\end{align}\\] Information Gain (\\(\\mathcal{I}\\)) measures the quality of split. So that if there are 15 red balls and five green balls in an urn and we split those 20 balls into two groups such that the 1st group has eight red balls and three green balls and the 2nd group has seven red balls and two green balls, then the information gain of the split is computed as such: \\[\\begin{align*} \\mathcal{H}(T) {}&amp;= -\\frac{15}{20}\\ \\log_e \\left( \\frac{15}{20} \\right) - \\frac{5}{20}\\ \\log_e \\left( \\frac{5}{20} \\right) = 0.5623352 \\\\ \\mathcal{H}(x_1) &amp;= -\\frac{8}{11}\\ \\log_e \\left( \\frac{8}{11} \\right) - \\frac{3}{11}\\ \\log_e \\left( \\frac{3}{11} \\right)= 0.5859526 \\\\ \\mathcal{H}(x_2) &amp;= -\\frac{7}{9}\\ \\log_e \\left( \\frac{7}{9} \\right) - \\frac{2}{9}\\ \\log_e \\left( \\frac{2}{9} \\right) = 0.5297062 \\\\ \\mathcal{H}(T, X) &amp;= \\frac{11}{20}\\ \\times 0.5859526 + \\frac{9}{20}\\ 0.5297062 = 0.5606417\\\\ \\\\ \\mathcal{I} &amp;= 0.5623352 - 0.5606417 = 0.0016935 \\end{align*}\\] Note that the higher the Information Gain, the better the split. An Information Gain of zero means that the split is worst. For Information Gain using Gini, see Multi-Classification section in Chapter 10 (Computation Learning II). 7.5.5 Mutual Information Mutual Information is denoted by the symbol \\(\\mathcal{I}(X; Y)\\) and is expressed as: \\[\\begin{align} \\mathcal{I}(X; Y) = \\sum_{x \\in X} \\sum_{y \\in Y} P(x,y) \\log_e \\frac{P(x,y)}{P(x)P(y)} \\end{align}\\] To illustrate, suppose we have two coins (X and Y), and we toss them five times, choosing any of the two coins randomly so that we end up with the following data: X = (H, H, T), Y = ( T, H ). We then compute for the probabilities: \\[\\begin{align} \\begin{array}{rrrr} P(X) = \\frac{3}{5} &amp; P(Y) = \\frac{2}{5} &amp; P(H) = \\frac{3}{5} &amp; P(T) = \\frac{2}{5}\\\\ P(X,H) = \\frac{2}{5} &amp; P(X,T) = \\frac{1}{5} &amp; P(Y,H) = \\frac{1}{5} &amp; P(Y,T) = \\frac{1}{5}\\\\ \\end{array} \\label{eqn:eqnnumber315} \\end{align}\\] Therefore: \\[\\begin{align} \\mathcal{I}(X; Y) {}&amp;= P(X, H) \\log_e \\frac{P(X, H)}{P(X)P(H)} + P(X, T) \\log_e \\frac{P(X, T)}{P(X)P(T)} \\nonumber \\\\ &amp;+P(Y, H) \\log_e \\frac{P(Y, H)}{P(Y)P(H)} + P(Y, T) \\log_e \\frac{P(Y, T)}{P(Y)P(T)} \\\\ &amp;= 0.40\\ \\log_e \\frac{0.40}{0.60 \\times 0.60} + 0.20\\ \\log_e \\frac{0.20}{0.60 \\times 0.40 } \\nonumber \\\\ &amp;+ 0.20\\ \\log_e \\frac{0.20}{0.40 \\times 0.60} + 0.20\\ \\log_e \\frac{0.20}{0.40 \\times 0.40}\\nonumber \\\\ &amp;= 0.01384429 \\nonumber \\end{align}\\] Below is a list of a few important properties of Mutual Information in relation to entropy. \\[\\begin{align} \\mathcal{I}(X; Y) {}&amp;= \\mathcal{H}(X) - \\mathcal{H}(X|Y)\\\\ \\mathcal{I}(X; Y) &amp;= \\mathcal{H}(Y) - \\mathcal{H}(Y|X)\\\\ \\mathcal{I}(X; Y) &amp;= \\mathcal{H}(X) + \\mathcal{H}(Y) - \\mathcal{H}(X,Y) \\end{align}\\] 7.5.6 Kullback-Leibler Divergence Kullback-Leibler (KL) Divergence, also called Relative Entropy, measures the closeness of an approximating distribution \\(\\mathcal{Q}(X)\\) to a true distribution \\(P(X)\\). KL divergence is intimately related to Cross-Entropy as we continue to deal with a true distribution \\(P(X)\\) along with a new approximating distribution denoted as \\(\\mathcal{Q}(X)\\). Note that the distribution Q(X) is also called reference distribution, approximate distribution, training distribution, etc. The KL divergence equation comes either in the form of Forward KL divergence, also called mean-seeking, zero-avoiding method; or in the form of Reverse KL divergence, also called mode-seeking, zero-forcing method. See Figure 7.10. Figure 7.10: KL Divergence There are two components (factors) of the KL divergence equation: the weighting function, namely \\(w(x)\\), and the penalty function, namely \\(g(x)\\). The penalty function is interpreted as a log-likelihood ratio, which measures the ratio of how likely an approximating distribution \\(Q(X)\\) describes a true distribution \\(P(X)\\) and is expressed as: \\[\\begin{align} \\log_e \\mathcal{LR} \\approx \\int_x \\log_e \\left(\\frac{P(X)}{\\mathcal{Q}(X)}\\right) \\end{align}\\] The Forward KL divergence has the following formula in which the penalty function measures the likelihood ratio of the actual distribution \\(P(X)\\) over \\(\\mathcal{Q}(X)\\). It is written as: \\[\\begin{align} \\mathcal{D}_{KL}(P || Q) \\equiv \\mathcal{KL}(\\ P\\ ||\\ Q\\ ) = \\sum_{x} \\underbrace{P(x)}_\\text{weight}\\ \\underbrace{ \\log_e \\left[ \\frac{P(x)}{\\mathcal{Q}(x)}\\right] }_\\text{penalty} = - \\sum_{x} \\underbrace{P(x)}_\\text{weight}\\ \\underbrace{ \\log_e \\left[ \\frac{\\mathcal{Q}(x)}{P(x)}\\right] }_\\text{penalty} \\end{align}\\] For unimodal, the Forward KL divergence moves the Q in the direction towards P for measuring the closeness of the approximating distribution to the actual distribution. For multimodal, the divergence seeks to settle on the average (the mean) of an actual multimodal distribution. The entropy term allows the approximating distribution to control the spread, encouraging it to have broader coverage across the actual multimodal distribution; hence, this divergence is inclusive. Below is the continuous version of the forward KL divergence split into two terms: the relative entropy and the cross-entropy. \\[\\begin{align} \\mathcal{KL}(\\ P\\ ||\\ Q\\ ) {}&amp;= \\int_x P(x)\\left(\\log_e P(x) - \\log_e \\mathcal{Q}(x)\\right) dx\\\\ &amp;=\\underbrace{\\int_x P(x) \\log_e P(x) dx}_\\text{relative entropy} \\underbrace{ - \\int_x P(x) \\log_e \\mathcal{Q}(x) dx }_\\text{cross-entropy} \\end{align}\\] On the other hand, the Reverse KL divergence has the following formula in which the penalty function measures the likelihood ratio of the approximating distribution \\(\\mathcal{Q}(X)\\) over \\(P(X)\\). \\[\\begin{align} \\mathcal{D}_{KL}(Q || P) \\equiv \\mathcal{KL}(\\ Q\\ ||\\ P\\ ) = \\sum_{x} \\underbrace{\\mathcal{Q}(x)}_\\text{weight}\\ \\underbrace{ \\log_e \\left[ \\frac{\\mathcal{Q}(x)}{P(x)}\\right] }_\\text{penalty} \\end{align}\\] Here, the penalty term has higher influence to the divergence if P(x) &gt; 0. The divergence seeks to settle on the mode (the most common value) of an actual multimodal distribution. Therefore, in a mixture distribution, it may prefer one with the higher probability - and a mode that has a higher scale (e.g., variance) gets to be one in which the approximating distribution may tend to follow; hence, the divergence is exclusive. Also, note that KL divergence is non-symmetric; meaning, that \\(\\mathcal{KL}(\\ P\\ ||\\ Q\\ ) \\ne \\mathcal{KL}(\\ Q\\ ||\\ P\\ )\\). Additionally, it has the following properties called the Gibb’s Inequality: \\[\\begin{align} \\mathcal{KL}(P||Q) \\ge 0\\ \\ and\\ \\ \\mathcal{KL}(P||P) = 0 \\end{align}\\] Below is an example implementation of Forward KL-divergence in R code: KL.divergence &lt;- function(X, mu1, sd1, mu2, sd2) { ln &lt;- function(n) { log(n, exp(1)) } # exp(1) = 2.718282 P = dnorm(x = X, mean=mu1, sd=sd1) Q = dnorm(x = X, mean=mu2, sd=sd2) sum( P * ln( P / Q) ) } To illustrate, let us solve for the KL divergence by generating two distributions (P and R) whose means \\(\\mu\\) are ten apart. Note that we are not explaining mean-seeking and mode-seeking in this illustration; instead, how an approximating distribution estimates the mean parameter of an actual distribution. \\[\\begin{align} P|\\mu,\\sigma^2 \\sim N(0, 1)\\ \\ \\ \\ \\ \\ \\ \\ \\ R|\\mu,\\sigma^2 \\sim N(10, 1.2) \\end{align}\\] We simulate the sampling distributions like so: set.seed(2020) P = rnorm(n=10, mean=0, sd = 1) R = rnorm(n=10, mean=10, sd = 1.2) Let us show the two true distributions (P, R) in a graph - note that both distributions are independent and are not components of a bimodal mixture distribution. See Figure 7.11. Figure 7.11: Model Distribution (P and R) The goal is to approximate a distribution given a fixed variance by estimating the mean. In other words, we have an unknown mean, and we need to find the proper values of the corresponding parameters (\\(\\mu\\) and \\(\\sigma^2\\)) that best characterize one of the distributions in the figure (P or R). Let us call this approximating (moving) distribution as Q. Let us plot the KL divergence between an approximating Q and a true P and between the same Q and another true R. Figure 7.12: Model Distribution (P and R) In Figure 7.12, as the approximating distribution Q moves in the direction towards P (\\(\\mu\\) = 0, \\(\\sigma=1\\)), the KL divergence becomes zero as it gets a \\(\\mu=0\\) which completely matches the P distribution. P.mu = 0; P.sd = 1; Q.mu = 1; Q.sd = 1 Q.sample = sample(range(-10,10), size=20, replace=TRUE) round( KL.divergence(Q.sample, mu1=P.mu, sd1=P.sd, mu2=Q.mu, sd2=Q.sd), 1e-10) ## [1] 0 Similarly, as Q moves towards R with (\\(\\mu\\) = 10, \\(\\sigma=1.2\\)), Q settles on \\(\\mu\\) with KL divergence equating to zero divergence. R.mu = 10; R.sd = 1.2; Q.mu = 10; Q.sd = 1.2 Q.sample = sample(range(-10,10), size=20, replace=TRUE) round( KL.divergence(Q.sample, mu1=R.mu, sd1=R.sd, mu2=Q.mu, sd2=Q.sd), 1e-10) ## [1] 0 Assume that Q.mu = 5, then we get a high divergence for Q moving away from R: R.mu = 10; R.sd = 1.2; Q.mu = 5; Q.sd = 1.2 Q.sample = sample(range(-10,10), size=20, replace=TRUE) round( KL.divergence(Q.sample, mu1=R.mu, sd1=R.sd, mu2=Q.mu, sd2=Q.sd), 1e-10) ## [1] 20 For comparison, we leave readers to investigate Wasserstein distance as an alternative measurement to KL divergence. Now that we have seen the capability of KL divergence, we show how KL divergence is minimized for Variational inference in a later discussion as a way to solve the computational challenge inherent in Markov Chain Monte Carlo. We leave readers also to investigate Bregman Divergence, Jensen-Bregman Divergence, and Jensen-Shannon Divergence. 7.5.7 Jensen’s Inequality Jensen’s Inequality states that the secant line drawn between any two points on a convex curve is always above the convex curve. Similarly, a secant line drawn between two points on a concave curve is always below the concave curve. See Figure 7.13. Figure 7.13: Jensen’s Inequality Mathematically, the average point in a secant line is always greater than a convex function; but lesser than a concave function. \\[\\begin{align} \\underbrace{\\overbrace{\\mathbb{E}(f(x))}^\\text{secant line} \\ge \\overbrace{ f(\\mathbb{E}(x))}^\\text{curve function} }_\\text{convex} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{ \\overbrace{\\mathbb{E}(f(x))}^\\text{secant line} \\le \\overbrace{f(\\mathbb{E}(x))}^\\text{curve function} }_\\text{concave} \\end{align}\\] In variational inference, we use Jensen’s inequality as a trick to be able to derive the upper/lower bound: \\[\\begin{align} \\underbrace{\\overbrace{\\mathbb{E}(f(x))}^\\text{secant line} = \\overbrace{ f(\\mathbb{E}(x))}^\\text{curve function} }_\\text{convex (upper bound)} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{ \\overbrace{\\mathbb{E}(f(x))}^\\text{secant line} = \\overbrace{f(\\mathbb{E}(x))}^\\text{curve function} }_\\text{concave (lower bound)} \\end{align}\\] 7.6 Bayesian Inference Bayesian Inference mostly, if not all cases, operates in the context of optimization problems where the object of interest is around the posterior. We begin this section by recalling the discussion around Linear Regression in Chapter 3 (Numerical Linear Algebra II). See Linear Regression Figure under Approximating Polynomial Functions by Regression Section in Chapter 3. A review of Linear Regression shows a deterministic linear model expressed as such: \\[\\begin{align} \\hat{y}_i = \\beta_0 + \\beta_1 x_i \\end{align}\\] However, this model is assumed to be a perfect model that is not mostly and not practically applicable in real-world situations. Most of our observed data are mixed with random noise (\\(\\mathbf{\\epsilon_i}\\)), and thus, we model in a stochastic manner by adding noise into the equation to form a stochastic linear model; thus, we have the following: \\[\\begin{align} \\hat{y}_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i\\ \\ \\ \\ \\ \\ \\ \\ \\epsilon_i \\sim \\mathcal{N}(\\mu, \\sigma^2),\\ \\ \\ \\ \\ \\ \\ i = 1,...,n \\end{align}\\] There are two notes to mention here. First, our response variable - being \\(\\mathbf{\\hat{y}_i}\\), given value \\(\\mathbf{x_i}\\) - is a point-estimate; meaning, our estimate renders one single value. Second, linear regression is accomplished by optimizing the \\(\\beta\\) parameters, namely \\(\\beta_0\\) and \\(\\beta_1\\), by minimizing the error (or loss) function, e.g. least square based on the residual (\\(\\epsilon_i\\)). In this section, we also deal with two notes corresponding to the response variable and parameter estimation. First, instead of dealing with point-estimates, we deal with stochastic estimates in which our response variable - being \\(\\mathbf{\\hat{y}_i}\\) - assumes a random variable; meaning, our estimate does not render a single value, but rather a random sampling that follows a normal posterior distribution and therefore it is expressed as such: \\[\\begin{align} \\hat{y}_i|x_i \\sim \\mathcal{N}(\\mu, \\sigma^2) \\end{align}\\] For every \\(\\mathbf{x_i}\\) in the X space, there is a corresponding \\(\\mathbf{\\hat{y}_i}\\) normal posterior distribution. Another way to get the intuition is to use Figure 7.14. Figure 7.14: Point-Estimate vs Stochastic Estimate Second, recall \\(\\theta\\) parameter in the Likelihood section. In this case, the theta \\(\\theta\\) parameter is a vector that contains \\(\\beta_0\\) and \\(\\beta_1\\). For clear notation, let us use \\(\\alpha\\) for \\(\\beta_1\\) and use \\(\\beta\\) for \\(\\beta_1\\). To optimize theta \\(\\theta\\) parameter, we focus on the likelihood term in the normalized Bayes Theorem and perform maximum likelihood estimation (MLE): \\[\\begin{align} \\underbrace{P(\\theta|X)}_\\text{posterior}\\ \\propto\\ \\underbrace{Lik(\\theta|X)}_\\text{likelihood} \\times \\underbrace{P(\\theta)}_\\text{prior} \\end{align}\\] Taking Figure 7.14 as an example in point, there are four sampling densities in the Y posterior space: \\(Y = (\\hat{y}_1, \\hat{y}_2, \\hat{y}_3, \\hat{y}_4 )\\). Such list of sampling densities forms the following notation: \\[\\begin{align} \\forall y: \\hat{y}_i|x_i;\\alpha,\\beta,\\sigma \\sim \\overbrace{ \\underbrace{ \\mathcal{N}(\\alpha_i + \\beta_i x_i\\ , \\sigma^2) }_\\text{likelihood}}^{sampling\\ density} \\end{align}\\] Note that each of the sampling densities is independent and thus we can form a joint distribution like so: \\[\\begin{align} Lik(X; \\alpha, \\beta, \\sigma^2|Y) = Lik(x_1,...,x_n; \\alpha, \\beta, \\sigma^2|y_1,...,y_n) = \\prod_{i=1}^n P(y_i|x_i; \\alpha_i, \\beta_i, \\sigma^2) \\end{align}\\] To avoid underflows and overflows, we use log-likelihood: \\[\\begin{align} -\\log_e Lik(X; \\alpha, \\beta, \\sigma^2|Y) = - \\sum_{i=1}^n ln\\ P(y_i|x_i; \\alpha_i, \\beta_i, \\sigma^2) \\end{align}\\] Note that it is common to minimize a loss or cost function. We minimize the log-likelihood by negating it to conform to the same practice. Therefore, to maximize the likelihood estimate (MLE) is also to minimize the negative log-likelihood (NLL). Here is the minimization equation: \\[\\begin{align} \\hat{y} = \\underset{\\alpha,\\beta,\\sigma^2}{argmin}\\ -\\log_e Lik(X; \\alpha, \\beta, \\sigma^2|Y) = \\underset{\\alpha,\\beta,\\sigma^2}{argmin}\\ -\\sum_{i=1}^n ln\\ P(y_i|x_i; \\alpha_i, \\beta_i, \\sigma^2) \\end{align}\\] As for the variance \\(\\sigma^2\\) parameter, if we lack assumptions, we can use uniform distribution: \\[\\begin{align} \\sigma^2 \\sim \\mathcal{U}(1,1) \\end{align}\\] 7.6.1 Maximum Likelihood (MLE) In Linear regression, we look for a model that fits our data. The goodness of fit is determined using RMSE or \\(\\mathbf{R^2}\\). The model is expressed as a line function (or a curve function for non-linear regression) described by the \\(\\beta\\) parameters. Fitting a model depends upon optimizing the \\(\\beta\\) parameters. The same concept applies to Stochastic regression in which we also look for a model that fits our data. And the goodness of fit is determined using MLE or NLL. The model is expressed as a likelihood function and is described by the \\(\\theta\\) parameters. Fitting a model depends upon optimizing the \\(\\theta\\) parameters. See Figure 7.15 for reference. Figure 7.15: Modeling by Parameter Estimation In this section, the idea here is to optimize the \\(\\theta\\) parameters to maximize the likelihood of observing data. That is called maximum likelihood estimation (MLE). An MLE takes the following general form: \\[\\begin{align} \\hat{\\theta} = \\underset{\\theta}{argmax}\\ P(X|\\theta) = \\underset{\\theta}{argmax} \\prod_{i=1}^n f( x_i ; \\theta) \\end{align}\\] MLE for Normal Distribution Let us derive the equation to find the optimal mean \\(\\mu^*\\), given a known \\(\\sigma^{2*}\\). Here, the likelihood function takes multivariates as input and performs a multiplication of the normal PDF for each variate. \\[\\begin{align} {}&amp;Lik(\\mu, \\sigma^2 | x_1,...,x_n) {} \\equiv P(x_1,...,x_n\\ |\\ \\mu, \\sigma^2)\\\\ &amp;= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right]\\\\ &amp;= \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right)^n exp\\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] Use log-likelihood and simplify: \\[\\begin{align} {}&amp; \\log_e Lik(\\mu, \\sigma^2 | x_1,...,x_n) \\\\ &amp;= ln \\left( \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right)^n + ln\\ exp\\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right]\\\\ &amp;= \\sum_{i=1}^n ln\\ \\left[\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\right] + \\left[-\\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{2\\sigma^2}\\right] \\end{align}\\] Now, take the partial derivative with respect to \\(\\mu\\): \\[\\begin{align} {}&amp; \\frac{ \\partial\\ \\log_e Lik(\\mu, \\sigma^2 | x_1,...,x_n) }{\\partial \\mu}\\\\ &amp;= -\\frac{1}{2\\sigma^2} \\frac{\\partial}{\\partial\\mu} \\left[\\sum_{i=1}^n(x_i - \\mu)^2\\right] &amp; \\text{(drop 1st term constant)}\\\\ &amp;= -\\frac{2}{2\\sigma^2} \\left[\\sum_{i=1}^n x_i - n\\mu \\right] \\end{align}\\] Set left-side of equation to zero and solve for \\(\\mu_{(MLE)}\\): \\[\\begin{align} {}&amp;\\rightarrow -\\frac{1}{\\sigma^2} \\left[\\sum_{i=1}^n x_i - n\\mu \\right] = 0\\\\ &amp;\\rightarrow \\left[\\sum_{i=1}^n x_i - n\\mu \\right] = 0\\\\ \\nonumber \\\\ &amp;\\mu_{(MLE)} = \\frac{1}{n}\\sum_{i=1}^n x_i \\end{align}\\] Notice that MLE for the mean for a normal distribution gets simplified to a simple average computation. Using the same log-likelihood, let us take the partial derivative with respect to \\(\\sigma^2\\), given \\(\\mu\\). In what follows, we temporarily use placeholder variables: \\[\\begin{align} v = \\sigma^2\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ X_u = \\sum_{i=1}^n(x_i - \\mu)^2 \\end{align}\\] Therefore, we get: \\[\\begin{align} \\frac{ \\partial\\ ln\\ Lik(\\mu, v | x_1,...,x_n) }{\\partial v} {}&amp;= \\frac{\\partial}{\\partial v}\\sum_{i=1}^n ln\\ \\left[\\frac{1}{\\sqrt{2\\pi v}} \\right] + \\frac{\\partial}{\\partial v}\\left[-\\frac{X_u}{2 v}\\right]\\\\ &amp;= -\\frac{n}{2v} + \\frac{X_u}{2v^2} \\end{align}\\] Set left-side of equation to zero and solve for \\(\\sigma^2_{(MLE)}\\): \\[\\begin{align} {}&amp;\\rightarrow -\\frac{1}{2}\\left[\\frac{n}{v} - \\frac{X_u}{v^2} \\right]= 0\\\\ {}&amp;\\rightarrow v^* = \\frac{X_u}{n},\\ \\ \\ \\ \\ \\ \\ \\ n \\ne 0\\\\ \\nonumber \\\\ \\sigma^2_{(MLE)} &amp;= \\frac{\\sum_{i=1}^n(x_i - \\mu)^2}{n} &amp; \\text{(substitute placeholders)} \\end{align}\\] MLE for Binomial Distribution Let us use binomial distribution to illustrate a case. Here, we perform the following: \\[\\begin{align} Lik(n,\\rho|x_1,...,x_m) = \\prod_{i=1}^m \\binom{n}{x_i} \\rho^{x_i}(1-\\rho)^{n-x_i} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where:\\ \\binom{n}{x_i}\\ \\text{is a constant} \\end{align}\\] Note that the constant \\(\\binom{n}{x_i}\\) gets dropped in the partial derivatives eventually; therefore, we can drop the constant up-front given data. We then calculate the log-likelihood: \\[\\begin{align} \\log_e Lik(n,\\rho|x_1,...,x_m) {}&amp;= ln\\left( \\prod_{i=1}^m \\binom{n}{x_i} \\rho^{x_i}(1-\\rho)^{n-x_i} \\right)\\\\ &amp;= \\sum_{i=1}^m \\left( ln\\binom{n}{x_i} + ln(\\rho)^{x_i} + ln(1-\\rho)^{n-x_i} \\right)\\\\ &amp;= \\sum_{i=1}^m \\left( ln \\binom{n}{x_i} + x_i \\cdot ln(\\rho) + (n-x_i)\\cdot ln(1-\\rho) \\right) \\end{align}\\] The key to optimization in this respect is based on taking the partial derivatives with respect to the \\(\\rho\\) parameter for the log-likelihood: \\[\\begin{align} \\frac{\\partial}{\\partial_\\rho} \\log_e Lik(n,\\rho|x_1,...,x_m) {}&amp;= \\sum_{i=1}^m \\left( ln \\binom{n}{x_i} + x_i \\cdot \\frac{\\partial}{\\partial_\\rho}ln(\\rho) + (n-x_i)\\cdot \\frac{\\partial}{\\partial_\\rho}ln(1-\\rho) \\right) \\\\ &amp;= \\sum_{i=1}^m \\left( 0 + x_i \\cdot \\frac{\\partial}{\\partial_\\rho}ln(\\rho) + (n-x_i)\\cdot \\frac{\\partial}{\\partial_\\rho}ln(1-\\rho) \\right) \\\\ &amp;= \\sum_{i=1}^m \\left( \\frac{1}{\\rho} x_i - \\frac{1}{1 - \\rho}(n - x_i) \\right) . \\end{align}\\] Then we simplify: \\[\\begin{align} \\sum_{i=1}^m \\left[ \\frac{1}{\\rho} x_i - \\frac{1}{1 - \\rho}(n - x_i) \\right] = 0 \\end{align}\\] With a few algebraic calculations, we obtain the following optimized parameter (MLE): For marginal-density (univariate) likelihood: \\[\\begin{align} \\hat{\\rho}_{(MLE)} = \\underset{\\rho}{argmax}\\ \\log_e Lik(n,\\rho|X = x) = \\frac{x}{n} \\end{align}\\] For joint-density (multivariate) likelihood: \\[\\begin{align} \\hat{\\rho}_{(MLE)} = \\underset{\\rho}{argmax}\\ \\log_e Lik(n,\\rho|x_1,...,x_m) = \\frac{\\sum^m x}{mn} \\end{align}\\] Below is a family of distributions with their corresponding maximum likelihood estimates. See Table 7.4. Table 7.4: Maximum Likelihood Estimate Family Parameters (\\(\\hat{\\theta}\\)) Derived Function Uniform \\(\\hat{\\theta}\\) \\(X_n\\) Normal \\(\\hat{\\theta_1} = \\mu\\) \\(\\mu\\) = \\(\\frac{1}{n} \\sum_{i=1}^n x_i\\) \\(\\hat{\\theta_2} = \\sigma^2\\) \\(\\frac{1}{n} \\sum_{i=1}^n(x_i - \\hat{\\mu})^2\\) Binomial \\(\\hat{\\rho}\\) \\(\\frac{\\sum^m x}{mn}\\) Geometric \\(\\hat{\\rho}\\) \\(\\frac{1}{X}\\) = \\(\\frac{n}{\\sum_{i=1}^n x_i}\\) Poisson \\(\\hat{\\lambda}\\) \\(\\frac{1}{n} \\sum_{i=1}^n x_i\\) Exponential \\(\\hat{\\lambda}\\) \\(\\bar{X}\\) = \\(\\frac{1}{n} \\sum_{i=1}^n x_i\\) In the following two sections, we discuss the Expectation-Maximization technique in dealing with multiple latent distributions instead of just one, which we can solve with MLE. Also, in Chapter 9 (Computational Learning I), we expand further on the concept of MLE (NLL) in the context of Generalized Linear Model (GLM) and Logistic Regression. For further reading, we encourage readers to investigate MLE in the context of misspecified models (White H. 1982). 7.6.2 Maximum A-posteriori (MAP) A Gaussian distribution can be characterized by its mean (the average) and variance (the spread). In some cases, we may also want to characterize (or estimate) the distribution in terms of mode. For example, in a unimodal Gaussian distribution, we may want to estimate the most common value or the highest peak in the distribution - this is the mode, denoted as \\(\\mu_{(MAP)}\\), with the most number of expected values. Additionally, we also want to identify the variance \\(\\sigma^2_{(MAP)}\\) between the mode \\(\\mu_{(MAP)}\\) and certain error levels, \\(\\epsilon_0\\), for our confidence level. In the previous section, we try to optimize \\(\\theta\\) parameters for likelihood. This section aims to optimize the \\(\\theta\\) parameters to maximize the posterior. That is called maximum a-posteriori estimation (MAP). The same concept applies in which our goal is to find the closest approximating posterior distribution proportional to the actual posterior distribution. MAP takes the following general form: \\[\\begin{align} \\theta^* = \\underset{\\theta}{argmax}\\ P(\\theta | X) \\end{align}\\] Here, we require a proper posterior distribution. Based on the derived closed-form conjugate posterior from the Conjugacy section, let us evaluate the posterior from the Normal-Normal conjugacy and Binomial-Beta conjugacy and try to maximize the posterior. MAP for Normal Posterior To illustrate, let us use the Normal family of distribution to compute for MAP. However, it helps to review the Normal-Normal conjugacy for the posterior equation derived in the Conjugacy section. \\[\\begin{align} P(\\mu,\\sigma^2|x1,...,x_n) {}&amp;\\propto Lik_X(\\mu, \\sigma^2|x_1,...,x_n) \\times P(\\mu_0,\\sigma^2_0)\\\\ &amp;\\propto \\underbrace{\\prod_{i=1}^n\\frac{1}{\\sqrt{2\\pi\\sigma^2}} exp\\left[-\\frac{(x_i - \\mu)^2}{2\\sigma^2}\\right] }_\\text{normal likelihood} \\times \\underbrace{ \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right]}_\\text{normal prior} \\\\ &amp;\\propto \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right)^n exp\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right] \\times \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} exp\\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] \\end{align}\\] We then calculate for the log a-posteriori (dropping constants up-front): \\[\\begin{align} {}&amp;\\log_e P(\\mu,\\sigma^2|x1,...,x_n) \\nonumber \\\\ &amp;\\propto \\sum_{i=1}^nln\\ \\left(\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\right) + \\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right] + ln\\ \\frac{1}{\\sqrt{2\\pi\\sigma^2_0}} + \\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] \\end{align}\\] Next, we take the partial derivative with respect to \\(\\mu\\) and simplify: \\[\\begin{align} {}&amp;\\frac{\\partial\\ \\log_e P(\\mu,\\sigma^2|x1,...,x_n)}{\\partial \\mu} \\nonumber \\\\ &amp;\\propto \\frac{\\partial}{\\partial \\mu}\\left[-\\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{2\\sigma^2}\\right] + \\frac{\\partial}{\\partial \\mu} \\left[-\\frac{(\\mu - \\mu_0)^2}{2\\sigma^2_0}\\right] &amp; \\text{(drop constants)}\\\\ &amp;\\propto -\\frac{2}{2} \\left[\\frac{n\\mu - \\sum_{i=1}^n x_i}{\\sigma^2}\\right] -\\frac{2}{2} \\left[\\frac{(\\mu - \\mu_0)}{\\sigma^2_0}\\right]\\\\ &amp;\\propto - \\left[\\frac{ \\sigma^2_0 ( n\\mu - \\sum_{i=1}^n x_i ) + \\sigma^2(\\mu - \\mu_0)}{\\sigma^2 \\sigma^2_0}\\right] \\end{align}\\] Then we set the left-side of equation to zero to solve for \\(\\mu_{(MAP)}\\) : \\[\\begin{align} {}&amp;\\rightarrow - \\left[\\frac{ \\sigma^2_0 ( n\\mu - \\sum_{i=1}^n x_i ) + \\sigma^2(\\mu - \\mu_0)}{\\sigma^2 \\sigma^2_0}\\right] = 0\\\\ &amp;\\rightarrow \\sigma^2_0 \\left( n\\mu - \\sum_{i=1}^n x_i \\right) + \\sigma^2(\\mu - \\mu_0) = 0 \\\\ \\nonumber \\\\ \\mu_{(MAP)} &amp;= \\frac{ \\sigma^2_0 \\sum_{i=1}^n x_i + \\sigma^2 \\mu_0 }{n\\sigma^2_0 + \\sigma^2} \\end{align}\\] Let us now take the partial derivative with respect to \\(\\sigma^2\\) and simplify. In what follows, we temporarily use placeholder variables: \\[\\begin{align} v = \\sigma^2\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Q_x = \\sum_{i=1}^n (x_i - \\mu)^2 \\end{align}\\] \\[\\begin{align} {}&amp;\\frac{\\partial\\ \\log_e P(\\mu,v|x1,...,x_n)}{\\partial v} \\nonumber \\\\ &amp;\\propto \\frac{\\partial}{\\partial v} \\sum_{i=1}^nln\\ \\left(\\frac{1}{\\sqrt{2\\pi v}}\\right) + \\frac{\\partial}{\\partial v}\\left[-\\frac{Q_x}{2v}\\right] &amp; \\text{(drop constants)}\\\\ &amp;\\propto -\\frac{n}{2v} + \\frac{Q_x}{2v^2} \\end{align}\\] Then we set the left-side of equation to zero to solve for \\(\\sigma^2_{(MAP)}\\) : \\[\\begin{align} {}&amp;\\rightarrow -\\frac{n}{2v} + \\frac{Q_x}{2v^2} = 0\\\\ &amp;\\rightarrow v = \\frac{Q_x}{n}\\\\ \\nonumber \\\\ \\sigma^2_{(MAP)} &amp;= \\frac{\\sum_{i=1}^n (x_i - \\mu)^2}{n} &amp; \\text{(substitute placeholders)} \\end{align}\\] It is worth mentioning that in a case where we do not have a piece of prior knowledge, we then can use a uniform prior, e.g., \\(P(\\theta) \\propto 1\\) where \\(\\theta \\sim U(1,1)\\). That renders the prior with lesser or no influence on the posterior, so then the maximum posterior (MAP) becomes proportional to maximum likelihood (MLE). In that regard, we can say that MAP is a regularization of ML because of the influence of the prior. MAP for Beta Posterior To illustrate, let us use the Binomial family of distribution to compute for MAP. For this, it helps to review the Binomial-Beta conjugacy for the posterior equation as derived in the Conjugacy section. Here, we calculate using the following equation (where the choice of our prior is a beta prior - we discuss more of prior probability in subsequent sections): \\[\\begin{align} P(n,\\rho|x_1,...,x_m) {}&amp;\\propto Lik(n,\\rho|x_1,...,x_m) \\times P(n,\\rho) \\\\ &amp;\\propto \\underbrace{ \\prod_{i=1}^m \\binom{n}{x_i} \\rho^{x_i}(1-\\rho)^{n-x_i}}_\\text{binomial likelihood} \\times \\underbrace{ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0 - 1} (1 - \\rho)^{\\beta_0 - 1} }_\\text{beta prior} \\end{align}\\] We then calculate for the log a-posteriori (dropping constants up-front): \\[\\begin{align} \\log_e P(n,\\rho|x_1,...,x_m) {}&amp;\\propto ln\\left[ \\prod_{i=1}^m \\binom{n}{x_i} \\rho^{x_i}(1-\\rho)^{n-x_i} \\times \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0 - 1} (1 - \\rho)^{\\beta_0 - 1} \\right]\\\\ &amp;\\propto ln\\left[ \\prod_{i=1}^m \\binom{n}{x_i} \\rho^{x_i}(1-\\rho)^{n-x_i} \\right] + ln \\left[ \\frac{1}{\\mathcal{B}(\\alpha_0,\\beta_0)}\\rho^{\\alpha_0 - 1} (1 - \\rho)^{\\beta_0 - 1} \\right]\\\\ &amp;\\propto \\sum_{i=1}^m ln(\\rho)^{x_i} + \\sum_{i=1}^mln(1-\\rho)^{n-x_i} + ln(\\rho)^{\\alpha_0 - 1}+ ln (1 - \\rho)^{\\beta_0 - 1} \\end{align}\\] Note that we zero-out the constant up-front. We then take the partial derivative with respect to \\(\\rho\\) and simplify: \\[\\begin{align} {}&amp;\\frac{\\partial}{\\partial_\\rho}\\log_e P(n,\\rho|X = x) \\nonumber \\\\ &amp;\\propto \\sum_{i=1}^m \\frac{\\partial}{\\partial_\\rho}ln(\\rho)^{x_i} + \\sum_{i=1}^m \\frac{\\partial}{\\partial_\\rho}ln(1-\\rho)^{n-x_i} + \\frac{\\partial}{\\partial_\\rho} ln(\\rho)^{\\alpha_0 - 1} + \\frac{\\partial}{\\partial_\\rho} ln (1 - \\rho)^{\\beta_0 - 1}\\\\ &amp;\\propto \\sum_{i=1}^m \\frac{x_i}{\\rho} - \\sum_{i=1}^m \\frac{n - x_i}{1 - \\rho} + \\frac{\\alpha_0-1}{\\rho} - \\frac{\\beta_0 - 1}{1 - \\rho}\\\\ &amp;\\propto \\frac{(1-\\rho)(\\sum_{i-1}^m x_i + \\alpha_0 - 1) - \\rho(\\sum_{i=1}^m (n-x_i) + \\beta_0 - 1)}{\\rho(1 - \\rho)} \\end{align}\\] Then we set the left-side of equation to zero to solve for \\(\\rho_{(MAP)}\\): \\[\\begin{align} {}&amp;\\rightarrow (1-\\rho)(\\sum_{i-1}^m x_i + \\alpha_0 - 1) - \\rho(\\sum_{i=1}^m (n-x_i) + \\beta_0 - 1) = 0\\\\ {}&amp;\\rightarrow (1-\\rho)(m\\bar{x} + \\alpha_0 - 1) - \\rho( (mn - m\\bar{x}) + \\beta_0 - 1) = 0 \\end{align}\\] Therefore, for joint posterior: \\[\\begin{align} \\rho_{(MAP)} = \\frac{m \\bar{x} + \\alpha - 1}{\\alpha + \\beta + m \\bar{x} + (mn - m\\bar{x}) - 2} \\end{align}\\] For marginal posterior: \\[\\begin{align} \\rho_{(MAP)} = \\frac{x + \\alpha - 1}{\\alpha + \\beta + x + (n- x) - 2} \\end{align}\\] We leave readers to investigate the maximum a-posteriori for other a-posteriori distributions. 7.6.3 Laplace Approximation Similar to Maximum a-posteriori, the Laplace Approximation method is another technique used to approximate an actual posterior distribution. As always, we start with the Bayes theorem: \\[\\begin{align} P(\\theta|X) = \\frac{P(X|\\theta)P(\\theta)} {P(X)},\\ \\ \\ \\ where\\ P(X)= \\int P(X|\\theta)P(\\theta) d\\theta \\end{align}\\] The object of interest is still the true posterior, which is rendered with no closed-form solution in cases, for example, where its marginal likelihood, namely \\(P(X)\\), tends to be intractable. Laplace approximation uses an approximating distribution denoted as \\(\\mathcal{Q}(X)\\) for the true posterior. To illustrate, let us consider the following steps (Murphy K. section 8.4.1 2012; Bishop C.M., section 4.4 2006): First, to approximate the posterior distribution, we replace the equation with the following approximating distribution: \\[\\begin{align} \\mathcal{Q}(\\mu) = \\frac{q(u)}{\\int q(u)du}\\ \\ \\ \\ \\ \\ \\text{let Z = } \\int q(u)du \\end{align}\\] Here, Z takes the role of a normalizing constant for \\(P(X)\\) - the denominator. And \\(q(u)\\) assumes the relation between the likelihood and the prior, namely \\(P(X|\\theta)P(\\theta)\\), - the numerator. Second, we assume that \\(\\mathcal{Q}(\\mu)\\) follows a unimodal Gaussian distribution but with a shape based on unknown mean and unknown variance such that \\(\\mathcal{Q} \\sim \\mathcal{N}(X; \\mu, \\sigma^2)\\). Naturally, to approximate a Gaussian distribution for \\(\\mathcal{Q}(\\mu)\\), we need to find the peak instead (or the mode), which we denote as \\(\\mu_0\\) for which the slope (first derivative) is zero, e.g., \\(q&#39;(\\mu_0) = 0\\). \\[\\begin{align} \\left.\\frac{d}{d\\mu}q(\\mu)\\right|_{\\mu={\\mu_0}} \\end{align}\\] Third, let us derive a gaussian-like equation for \\(\\mathcal{Q}(\\mu)\\) by using second-order Taylor series expansion for the log of \\(q(u)\\), centered at the mode (\\(\\mu_0\\)): \\[\\begin{align} \\log_e q(u) {}&amp; = \\log_e \\left[P(X|\\theta = \\mu) P(\\theta =\\mu)\\right] \\\\ \\nonumber \\\\ {}&amp;\\approx \\sum_{n=0}^{N=2} \\left(\\frac{q^{(n)}(\\mu_0)(\\mu - \\mu_0)^{(n)}}{n!}\\right) &amp; \\text{(2nd-order Taylor series)}\\\\ &amp;= q(\\mu_0) + q&#39;(\\mu_0)(\\mu - \\mu_0) + \\frac{1}{2}q&#39;&#39;(\\mu_0)(\\mu - \\mu_0)^2 \\\\ &amp;= q(\\mu_0) + \\frac{1}{2}q&#39;&#39;(\\mu_0)(\\mu - \\mu_0)^2 &amp; \\text{(1st-order vanishes at slope=0)}\\\\ &amp;= q(\\mu_0) + \\left(-\\frac{1}{2}\\right)(-q&#39;&#39;(\\mu_0))(\\mu - \\mu_0)^2 &amp; \\text{(negate for concave quadratic)}\\\\ &amp;= q(\\mu_0) + \\left(-\\frac{1}{2}\\right)\\frac{(\\mu - \\mu_0)^2}{\\Sigma} &amp; \\Sigma=-q&#39;&#39;(\\mu_0)^{-1} \\leftarrow\\ \\text{(precision)} \\\\ q(\\mu) &amp;= exp\\left[q(\\mu_0) + \\left(-\\frac{1}{2}\\right)\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right] &amp; \\text{(exp-log)} \\\\ &amp;= exp(q(\\mu_0)) \\times exp \\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right)\\right] \\end{align}\\] Fourth, let us also solve for the normalizing constant, namely Z = \\(\\int q(\\mu)d\\mu\\). \\[\\begin{align} Z {}&amp;= \\int exp(q(\\mu_0)) \\times exp \\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right)\\right] d\\mu &amp; \\text{(normalizing constant)} \\\\ &amp;= exp(q(\\mu_0)) \\int exp\\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right) \\right] d\\mu \\end{align}\\] Now, recall the equation for gaussian integral with polar coordinates used to derive the following sample equation (See Chapter 5 (Numerical Probability and Distribution) under Normal Distribution Subsection). Similarly, with some integration, we arrive at the following: \\[\\begin{align} \\int exp\\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right) \\right] d\\mu = \\sqrt{2\\pi\\Sigma}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\int_{-\\infty}^{\\infty} e^{-x^2} dx = \\sqrt{\\pi} \\end{align}\\] Therefore: \\[\\begin{align} Z = exp(q(\\mu_0)) \\times \\sqrt{2\\pi\\Sigma}\\ \\ \\ \\ \\text{(normalizing constant)} \\end{align}\\] That simplifies our approximating distribution for posterior, namely \\(\\mathcal{Q}(\\mu)\\), by avoiding the use of integration. Therefore, we end up with the following: \\[\\begin{align} \\mathcal{Q}(\\mu) {}&amp;= \\frac{1}{\\mathbf{Z}} \\times q(u)\\ = \\frac{1}{exp(q(\\mu_0)) \\times \\sqrt{2\\pi\\Sigma}}\\times exp(q(\\mu_0)) \\times exp\\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right) \\right]\\\\ \\nonumber \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi\\Sigma}}exp\\left[-\\frac{1}{2}\\left(\\frac{(\\mu - \\mu_0)^2}{\\Sigma}\\right) \\right] \\end{align}\\] where: \\[\\begin{align} \\mu_0 = \\underbrace{q&#39;(\\mu_0) = \\left.\\frac{d}{d\\mu}q(\\mu)\\right|_{\\mu={\\mu_0}}}_\\text{the mode}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\Sigma^{-1} = \\underbrace{-q&#39;&#39;(\\mu_0) = -\\left.\\frac{d^2}{d\\mu^2}q(\\mu)\\right|_{\\mu={\\mu_0}}}_\\text{the precision} \\end{align}\\] In terms of M-dimensional multivariate gaussian distribution, we leave the readers to derive the following equation: \\[\\begin{align} \\mathcal{Q}_X(\\mu)= \\frac{1}{\\sqrt{(2\\pi)^M |H|^{-1}}} exp\\left[-\\frac{1}{2} (\\mu - \\mu_0)^T H (\\mu - \\mu_0) \\right]\\ \\ \\ \\leftarrow \\text{(H is Hessian Matrix)} \\end{align}\\] where: \\[ \\mu = \\left[\\begin{array}{r}\\mu_0 \\\\ \\mu_1 \\\\ \\vdots \\\\ \\mu_m\\end{array}\\right],\\ \\ \\ \\ \\ H^{-1} = \\left[\\begin{array}{rrrr} \\sigma_{1,1} &amp; \\sigma_{1,2} &amp; ... &amp; \\sigma_{1,m}\\\\ \\sigma_{2,1} &amp; \\sigma_{2,2} &amp;... &amp; \\sigma_{2,m}\\\\ \\vdots &amp; \\ldots &amp; \\ddots &amp; \\vdots \\\\ \\sigma_{m,1} &amp; \\sigma_{m,2} &amp; \\ldots &amp; \\sigma_{m,m} \\\\ \\end{array}\\right] \\] Depending on the sample size, Laplace approximation is adjustable to the n-order of a Taylor series expansion necessary to improve the estimation. That is called Bayesian Information Criterion (BIC). Laplace approximation is an old technique demonstrating approximation of simple unimodal posterior distribution, and it performs well if the mean and mode are not far apart. However, for complex models, it may help look at other techniques such as Expectation-Maximization and Variational Bayes. 7.6.4 Expectation-Maximization (EM) There are real-world applications in object recognition such as facial and speech recognition, behavior and gesture recognition, and hand-writing recognition that deal with latent or hidden data. These applications provide solutions for identifying, describing, or recognizing latent data, such as recognizing speech by evaluating speech patterns. There are powerful techniques used, in whole or in part, to solve such problems, and one of them uses the EM algorithm. Expectation-Maximization (EM) (Dempster, Laird, Rubin 1977) is an iterative and recursive technique that extends MLE and MAP, optimizing parameter models. We begin the discussion of the EM algorithm by enumerating a few items (Note here that our discussion is driven by the use of the Gaussian mixture model (GMM)): A cluster of unknown distributions (or components) that can be classified, e.g. \\(Y \\in \\{\\ y_1, y_2,...,\\ y_k\\ \\}\\). A sampling of data from an unknown distribution, e.g. \\(X \\in \\{\\ x_1,\\ x_2,\\ ...,\\ x_n\\ \\}\\) A set of classification to label cluster components, e.g., \\(c_j \\in \\{\\ A, B, C\\ \\}\\) A set of proportionality (to serve as weight), e.g. \\(\\omega \\in \\{\\ \\omega_1,\\ \\omega_2,\\ ...,\\ \\omega_k\\ \\}\\). An unknown parameter model denoted as theta \\(\\theta\\) that models each unknown distribution. Note that in mixture models, we also call the unobserved distributions as latent components (or latent samplings). We may use the first component referring to the first unobserved distribution from time to time. Now EM starts with an initialization step and then iterates between the expectation and maximization steps. Model initialization (Initialization step) Unlike our discussion around MLE in characterizing a single latent distribution, we illustrate how to characterize latent mixture distributions. For illustration, assume we have three latent distributions, \\(k = 3\\), with corresponding parameter models \\(\\theta = \\{ \\theta_1, \\theta_2, \\theta_3 \\}\\) where: \\[ \\theta_j = \\begin{cases} (\\mu, \\sigma^2) &amp; \\text{(gaussian distribution)}\\\\ (n, \\rho) &amp; \\text{(binomial distribution)}\\\\ ... &amp; \\text{(other types)} \\end{cases}\\ \\ \\ \\ \\ \\ \\ \\ \\forall j : 1,...,k \\] Suppose that each latent distribution, namely \\(y_j\\), can be classified accordingly: \\(c_j \\in \\{\\ A,\\ B,\\ C\\ \\}\\). Here, we characterize each of our k models with gaussian distribution with corresponding gaussian parameters, namely \\(\\theta_j = (\\mu_j, \\sigma^2_j)\\). We start EM by initializing our model parameters for each unobserved distribution at time \\(t=0\\). \\[ \\theta_1^0 = (25, 2)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta_2^0 = (30, 2)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\theta_3^0 = (35, 2) \\] As for our observed data, suppose we have \\(X = \\{\\ 27,\\ 23,\\ 18,\\ ...,\\ x_n\\ \\}\\) where \\(n = sample\\ size\\). We assume the following proportionality (or prior probability) of three samplings from the mixture distribution. Here, we arbitrarily assign the proportions like so: \\[ P(\\theta_1) = \\omega_1 = 33\\%\\ \\ \\ \\ \\ \\ \\ \\ \\ P(\\theta_2) = \\omega_2 = 33\\%\\ \\ \\ \\ \\ \\ \\ \\ \\ P(\\theta_3) = \\omega_3 = 34\\% \\] The proportionality, also called weight, sums up to 1, for example, \\(\\sum_{j=1}^k \\omega_j\\) = 1. We can use uniform distribution equally spread across all prior probabilities if we do not make any assumptions using the following proportionality formula: \\[ w_j = \\frac{n_j}{n}\\ \\ \\ \\ \\ \\ \\text{where}\\ n_j \\text{= size of jth component and n = total size of X} \\] Expectation Step (E-STEP) In this step, we use the initialized (and eventually the optimized) model to calculate the posterior probability using the Bayes Theorem equation. \\[\\begin{align} P(x_i \\in y_j|x_i) = \\frac{P(x_i | x_i \\in y_j) P(y_j)}{P(x_i) } \\end{align}\\] The idea is to evaluate every observation, namely (\\(x_i\\)), and estimate the probability that each observation in the sample belongs to or exists (\\(\\in\\)) in a particular sampling component, namely (\\(y_i\\)). However, it may help show how the model theta \\(\\theta\\) contributes to the equations. Therefore, we can express the posterior term like so: \\[\\begin{align} P(x_i \\in y_j|x_i) \\equiv \\underbrace{ P\\left(Y = y_j|X = x_i,\\ \\theta_j^{(t)}\\right) }_\\text{posterior} \\end{align}\\] where \\(\\theta_j^t\\) models (or characterizes) \\(\\mathbf{y_j}\\). The likelihood term can be defined as: \\[\\begin{align} P(x_i | x_i \\in y_j) \\equiv \\underbrace{P\\left(X=x_i|\\theta_j^{(t)}\\right)}_\\text{likelihood} = \\frac{1}{\\sqrt{2\\pi\\sigma_j^{2{(t)}}}} exp\\left[-\\frac{\\left(x_i - \\mu_j^{(t)}\\right)^2}{2\\sigma_j^{2{(t)}}}\\right] \\end{align}\\] Here, we use gaussian pdf and is implemented like so: norm.prob &lt;- function(x, mean, sd) { dnorm(x, mean= mean, sd = sd) } Finally, our normalizer - the marginal likelihood - can be written as: \\[\\begin{align} P(x_i) \\equiv \\sum_{l=1}^k P\\left(X=x_i|\\theta_l^{(t)}\\right)\\times\\omega_l \\end{align}\\] We use the log marginal likelihood, namely \\(\\log_eP(x_i)\\), to evaluate convergence during iteration. Because of the intractable nature of the normalizer for more complex models, we discuss a way to solve the intractability problem in the next section under variational EM. For now, here is the equivalent equation for our vanilla EM. \\[\\begin{align} P(Y = y_j|X = x_i,\\ \\theta_j^{(t)}) = \\frac{P\\left(X=x_i|\\theta_j^{(t)}\\right)\\times\\omega_j}{\\sum_{l=1}^k P\\left(X=x_i|\\theta_l^{(t)}\\right)\\times\\omega_l} ,\\ \\ \\ \\ \\ \\ \\forall j : 1,...,k \\end{align}\\] Now, let us use a placeholder variable for the outcome of the calculation in the above equation which we use in the maximization step: \\[\\begin{align} \\Upsilon_{j,x_i}^{(t)}\\ \\ \\leftarrow P\\left(Y = y_j|X = x_i,\\ \\theta_j^{(t)}\\right) \\end{align}\\] Note that this variable represents a posterior proportion of the jth component. Also, it is essential to note further that the proportionality of all components must sum up to one. In the context of ML classification, calculating the proportion of every value over the sum of all values in a vector is called Softmax. A Softmax function performs proportion evaluation (especially in neural networks) to decide the path with the highest proportion. See our simple classification implementation later in this section. Maximization Step (M-STEP) In this step, we maximize the parameter model. In the case of gaussian mixture, we maximize \\(\\mu\\) and \\(\\sigma^2\\): \\[\\begin{align} \\mu_j^{(t+1)} {}&amp;= \\frac{\\sum_{i=1}^n x_i \\Upsilon_{j,x_i}^{(t)}}{\\sum_{i=1}^n \\Upsilon_{j,x_i}^{(t)}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma_j^{t+1} = \\frac{\\sum_{i=1}^n \\left(x_i - \\mu_j^{(t)}\\right)^2 \\Upsilon_{j,x_i}^{(t)}}{\\sum_{i=1}^n \\Upsilon_{j,x_i}^{(t)}}\\\\ \\theta_j^{(t+1)} &amp;= (\\mu_j^{(t+1)} , \\sigma_j^{t+1}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega_j^{t+1} = \\frac{ \\sum_{i=1}^n \\Upsilon_{j,x_i}^{(t)}}{n} \\end{align}\\] We then iterate between E-step and M-step until convergence. Note that we use the log of the normalizer being computed in E-STEP to evaluate convergence, for example: \\[\\begin{align} | \\underbrace{\\log_e P(x_i)^{(t)}}_\\text{new loglik} - \\underbrace{\\log_e P(x_i)^{(t-1)}}_\\text{old loglik} | &lt; tolerance \\end{align}\\] To illustrate, let us perform the initialization step: set.seed(2020) k = 3 sample_size = 5 n = k * sample_size theta = matrix( c( c(25, 2), c(30, 1.5), c(35, 1.5) ), nrow=k, byrow=TRUE) x1 = sample1 = rnorm(n = sample_size, mean = theta[1,1], sd = theta[1,2]) x2 = sample2 = rnorm(n = sample_size, mean = theta[2,1], sd = theta[2,2]) x3 = sample3 = rnorm(n = sample_size, mean = theta[3,1], sd = theta[3,2]) w1 = length(sample1) / length(X) w2 = length(sample2) / length(X) w3 = length(sample3) / length(X) W = c(w1, w2, w3) # prior probabilities X = c(x1, x2, x3) # mixture distribution Y = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # components Albeit we have generated three samples, they are merely used to simulate a gaussian mixture for our observed data, namely X. For demonstration, the three samples (or three components) become latent but labeled as A, B, and C accordingly. The parameters for each component, namely theta \\(\\theta_j\\) and prior weight \\(\\omega_j\\), are kept unknown but arbitrarily initialized. See Figure 7.16. Figure 7.16: Gaussian Mixture Model - 3 hidden components Before we show the implementation of each step, let us first define the natural log function. ln &lt;- function(n) { log(n, exp(1))} # exp(1) = 2.718282 For the Expectation Step, we calculate the weighted likelihood of each observation belonging to a component. Notice that each weighted likelihood gets a higher weighting score the more relevant the corresponding data points become to their component. weight.likelihd1 = norm.prob(X, mean=theta[1,1], sd=theta[1,2]) * W[1] weight.likelihd2 = norm.prob(X, mean=theta[2,1], sd=theta[2,2]) * W[2] weight.likelihd3 = norm.prob(X, mean=theta[3,1], sd=theta[3,2]) * W[3] round(matrix( c(weight.likelihd1, weight.likelihd2, weight.likelihd3), nrow=3, byrow=TRUE),4)[,1:8] # display 1st 8 columns ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] ## [1,] 0.0464 0.0477 0.0273 0.0263 0.001 0.0005 0.0003 0.0033 ## [2,] 0.0012 0.0009 0.0000 0.0000 0.000 0.0513 0.0428 0.0648 ## [3,] 0.0000 0.0000 0.0000 0.0000 0.000 0.0022 0.0038 0.0001 Then we calculate for the normalizer - the joint probability of the Gaussian mixture. normalizer = weight.likelihd1 + weight.likelihd2 + weight.likelihd3 round(normalizer,3)[1:10] # display 1st 10 columns ## [1] 0.048 0.049 0.027 0.026 0.001 0.054 0.047 0.068 0.033 0.068 Finally, we calculate the normalized posterior of each component. Let us show the posterior result of the 1st component. posterior1 = weight.likelihd1 / normalizer posterior2 = weight.likelihd2 / normalizer posterior3 = weight.likelihd3 / normalizer round(posterior1,2)[1:10] # display 1st 10 columns ## [1] 0.97 0.98 1.00 1.00 1.00 0.01 0.01 0.05 0.00 0.03 For Maximization step: We first calculate the normalizer for mean, standard deviation, and prior weight. cnm1 = comp.normalizer1 = sum( posterior1 ) cnm2 = comp.normalizer2 = sum( posterior2 ) cnm3 = comp.normalizer3 = sum( posterior3 ) c(&quot;comp.norm1&quot; = cnm1, &quot;comp.norm2&quot; = cnm2, &quot;comp.norm3&quot; = cnm3) ## comp.norm1 comp.norm2 comp.norm3 ## 5.047 4.330 5.623 Then we calculate the parameter mean \\(\\mu_j^{(t=1)}\\) itself for each component at time=1 (or 1st iteration). mu1 = sum( posterior1 * X ) / comp.normalizer1 mu2 = sum( posterior2 * X ) / comp.normalizer2 mu3 = sum( posterior3 * X ) / comp.normalizer3 c(&quot;mean1&quot; = mu1, &quot;mean2&quot; = mu2, &quot;mean3&quot; = mu3) ## mean1 mean2 mean3 ## 23.36 30.79 34.89 We do the same for standard deviation by calculating for sd \\(\\sigma_j^{(t=1)}\\). sd1 = sum( posterior1 * (X - mu1)^2 ) / comp.normalizer1 sd2 = sum( posterior2 * (X - mu2)^2 ) / comp.normalizer2 sd3 = sum( posterior3 * (X - mu3)^2 ) / comp.normalizer3 c(&quot;sd1&quot; = sd1, &quot;sd2&quot; = sd2, &quot;mean3&quot; = sd3) ## sd1 sd2 mean3 ## 6.145 1.252 2.124 We also re-calculate for the next value of our prior weights \\(\\omega_j^{(t=1)}\\): w1 = comp.normalizer1 / length(X) w2 = comp.normalizer2 / length(X) w3 = comp.normalizer3 / length(X) c(&quot;prior1&quot; = w1, &quot;prior2&quot; = w2, &quot;prior3&quot; = w3) ## prior1 prior2 prior3 ## 0.3364 0.2887 0.3749 After computing for mean, sd, and prior weight we continue to iterate until convergence. Below is an example of EM in R code (Here, we change the sample size to 500). Note that the implementation and samples are based on gaussian distribution. expectation &lt;- function(X, theta, W) { posterior = matrix(0, nrow=k, ncol=n, byrow=TRUE) normalizer = 0 for (j in 1:k) { posterior[j,] = norm.prob(X, mean= theta[j,1], sd = theta[j,2]) * W[j] normalizer = normalizer + posterior[j,] } for (j in 1:k) { posterior[j,] = posterior[j,] / normalizer } list(&quot;posterior&quot; = posterior, &quot;loglik&quot; =sum(log(normalizer, exp(1)))) } maximization &lt;- function(X, posterior ) { comp.normalizer = rep(0, k) mu = rep(0, k); var = rep(0, k); W = rep(0, k) for (j in 1:k) { comp.normalizer[j] = sum( posterior[j,] ) mu[j] = sum( posterior[j,] * X ) / comp.normalizer[j] var[j] = sum( posterior[j,] * (X - mu[j])^2 ) / comp.normalizer[j] W[j] = comp.normalizer[j] / length(X) } theta = matrix( c(mu, sqrt(var)), nrow=k, byrow=FALSE) list(&quot;theta&quot;=theta, &quot;W&quot;= W) } iterate &lt;- function(x, theta, limit=1000) { tol = 1e-10; err = loglik.old = 0 sequence = matrix(0, 0, 2*k + 2) for (t in 1:limit) { component &lt;- expectation(x, theta, W) # new posterior model = maximization(X, component$posterior) # new model parameters loglik = component$loglik err = abs ( loglik - loglik.old ) sequence = rbind(sequence, c(t, round(theta[,1],2), round(theta[,2],2), err )) if ( err &lt; tol ) { break } theta = model$theta # old model parameter update W = model$W # old prior update loglik.old = loglik # old likelihood update } colnames(sequence) = c(&quot;Iteration&quot;, paste0(&quot;mu&quot;, seq(1,k)), paste0(&quot;sd&quot;, seq(1,k)), &quot;error&quot; ) iteration = as.data.frame( tail( sequence)) list(&quot;theta&quot;=theta, &quot;iteration&quot;=iteration, &quot;posterior&quot; = component$posterior) } By executing the EM steps, we can iterate and converge with an optimized value for mean and standard deviation parameters. Note that parameters are more optimized with more observations. em = iterate(X, theta, limit=1000) em$iteration ## Iteration mu1 mu2 mu3 sd1 sd2 sd3 error ## [180,] 180 24.96 29.92 34.95 2.14 1.32 1.57 1.601e-10 ## [181,] 181 24.96 29.92 34.95 2.14 1.32 1.57 1.446e-10 ## [182,] 182 24.96 29.92 34.95 2.14 1.32 1.57 1.291e-10 ## [183,] 183 24.96 29.92 34.95 2.14 1.32 1.57 1.155e-10 ## [184,] 184 24.96 29.92 34.95 2.14 1.32 1.57 1.037e-10 ## [185,] 185 24.96 29.92 34.95 2.14 1.32 1.57 9.277e-11 Also, we can now perform stratification by classifying each observation according to the component it belongs. The table below shows the number of observations and proportions belonging to each component after the EM iteration. classify &lt;- function(theta, posterior) { component = apply(posterior, 2, which.max) strata = table (component) # groupings or classes model = cbind(theta, strata, strata / n) colnames(model) = c(&quot;mean&quot;, &quot;sd&quot;, &quot;count&quot;, &quot;proportion&quot;) rownames(model) = Y # components round( model, 2) list(&quot;model&quot;=model, &quot;component&quot;=component) } classified = classify(em$theta, em$posterior) classified$model ## mean sd count proportion ## A 24.96 2.142 492 0.3280 ## B 29.92 1.320 488 0.3253 ## C 34.95 1.570 520 0.3467 Finally, we also can show how the proportions are broken down into their Gaussian components. See Figure 7.17. Figure 7.17: Four mixture densities We leave readers to experiment on EM by using other parameter models of other familiar distributions. 7.6.5 Variational Inference In a previous section, we used Laplace approximation to estimate our posterior with ease. That is possible only because the problem statement in our case involves a unimodal Gaussian distribution that may be easy to compute. Also, in the last section, we use Expectation-Maximization to solve for a more complex distribution; and that is possible only because the problem statement allows for a tractable posterior. However, if our Gaussian mixture distribution consists of a larger number of components with a larger number of observations than as seen in Figure 7.17, then the marginal likelihood - the denominator in the Bayes theorem - becomes intractable. The equation below demonstrates the intractability of the marginal likelihood (Blei D et al., n.d.): \\[\\begin{align} P(\\mu_{1:k}, y_{1:n}|x_{1:n}) = \\frac{\\prod_{j=1}^kP(\\mu_j) \\prod_{i=1}^n P(y_i)P(x_i|y_i,\\mu_{1:k})} {\\int_{\\mu_{1:k}}\\sum_{y_{1:n}} \\prod_{j=1}^k P(\\mu_j)\\prod_{i=1}^nP(y_i)P(x_i|y_i,\\mu_{1:k})} \\end{align}\\] Consequently, our posterior distribution is rendered intractable. That is also true if none of the standard Conjugacy methods apply, otherwise allowing us to compute for the posterior distribution exactly (in closed-form). Additionally, the inference techniques discussed in previous sections, such as MLE and MAP, fail to operate on complex models. Therefore, it behooves us to find other techniques in mitigating the challenge. Here, we introduce Variational Bayes. Note that our discussion references some of the works of Bishop C.M (2006), Blei D. et al. (2017), and C.W., Roberts S.J. (2012). The motivation in this section is to find an approximating distribution denoted as \\(\\mathcal{Q}(y)\\) to eventually take the place of our intractable posterior distribution denoted as \\(P(y|x)\\). In the context of Bayesian inference, also referenced as variational inference (VI), this approximating distribution is also called variational distribution with parameters optimized based on an approach different from one offered by Laplace approximation for reasons that become apparent later. One essential concept in this section is the Bayesian Network, which depicts the dependency between variables. For example, Figure 7.18 demonstrates a joint probability that follows the product rule (Nguyen L. 2013; Horny M. 2014). Figure 7.18: Bayesian Network Let us now discuss how variational Bayes works. First, our goal is essentially to develop a model for our inference. This model represents a family of distributions. We define this family of distributions depending on the problem statement. In our case, let us scheme to use a model that involves a multivariate Gaussian mixture distribution: \\[\\begin{align*} \\underbrace{x_i|y_i \\sim \\mathcal{N}_p(\\mu_k, \\Lambda_k)}_\\text{observed variable} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{y_i \\sim Multi(n,\\pi_k)}_\\text{latent variable} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\pi_k \\sim Dirichlet(\\omega_0)}_\\text{mixing weight/coefficient}\\\\ \\end{align*}\\] \\[\\begin{align*} \\underbrace{\\mu_k\\ \\sim \\mathcal{N}_p(\\alpha_0, \\beta_0) }_\\text{1st latent conjugate prior} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\Lambda_k\\ \\sim Wishart(\\nu_0, \\Sigma_0) }_\\text{2nd latent conjugate prior} \\end{align*}\\] where \\(\\beta_0\\) is scaling factor, \\(\\pi_k\\) is a mixture coefficient, and \\(\\Lambda_k\\) is a positive-definite precision matrix. The observed variable, namely x, has a corresponding latent (classifying) variable, namely y, which contains a hot-encoding of a binary vector of K-size, namely \\(\\mathbf{\\tau_k}\\) (to be further explained), describing which cluster each corresponding observation belongs. We also can revisit Figure 7.1 discussed in Bayes Theorem section as reference along with Figure 7.19 for two models. Albeit here, we operate on p-variate gaussian mixture models. Figure 7.19: Multivariate Gaussian Mixture Model From Figure 7.19, the Bayesian model captures five hyperparameters, namely \\(\\{\\ \\omega_0, \\alpha_0, (\\beta_0\\Lambda_k), \\nu_0, \\Sigma_0 \\}\\), for which we calculate the corresponding conjugacy. Note here that we assume a conjugacy relationship between parameters, hinting at a closed-form solution for our model, which is useful. In Bayesian Network notation, the dependency in the Bayesian model (excluding hyperparameters) is written as: \\[\\begin{align} P(x_i, y_i, \\pi_k, \\mu_k, \\Lambda_k) {}&amp;= P(x_i | y_i, \\mu_k, \\Lambda_k) &amp; x_i \\text{ depends on } \\{y_i, \\mu_k, \\Lambda_k \\} \\nonumber \\\\ &amp;\\times P(y_i|\\pi_k) &amp; y_i \\text{ depends on } \\{\\pi_k \\} \\nonumber \\\\ &amp;\\times P(\\pi_k) \\nonumber \\\\ &amp;\\times P(\\mu_k|\\Lambda_k) &amp; \\mu_k \\text{ depends on } \\{\\Lambda_k \\} \\nonumber \\\\ &amp;\\times P(\\Lambda_k) \\end{align}\\] For posterior distribution with \\(\\mu_k\\) and \\(\\Lambda_k\\) being both unknown, recall the derivation for a normal-Wishart conjugacy we made in the Conjugacy section: \\[\\begin{align} \\mu_k, \\Lambda_k|x\\sim \\ \\mathcal{NW}_p(\\alpha_1, \\beta_1, \\nu_1,\\Sigma_1) = \\mathcal{N}_p(\\alpha_1, \\beta_1)\\times\\mathcal{W}_p(\\nu_1, \\Sigma_1) \\end{align}\\] obtaining the following parameters: \\[\\begin{align} \\alpha_1 &amp;= \\frac{(\\alpha_0\\beta_0 + n\\bar{x})}{(\\beta_0 + n)}\\\\ \\beta_1 &amp;= \\beta_0 + n \\\\ \\nu_1 {}&amp;= \\nu_0 + n \\\\ \\Sigma_1 &amp;= \\frac{ n\\beta_0}{(\\beta_0 + n)}(\\bar{x} - \\alpha_0)( \\bar{x} - \\alpha_0 )^T + S + \\Sigma_0^{-1} \\end{align}\\] In the case in which \\(\\mu_k\\) is known and \\(\\Lambda_k\\) is unknown, we can use the normal inverse Wishart conjugacy instead: \\[\\begin{align} \\mu_k|x\\sim \\ \\mathcal{N}_p(\\alpha_1, \\beta_1)\\ \\ \\ \\ \\ \\ \\Lambda_k|x\\sim \\ \\mathcal{IW}_p(\\nu_1, \\Sigma_1) \\end{align}\\] obtaining the following parameters: \\[\\begin{align} \\Sigma_1 = (\\Sigma_0 + \\Sigma)^{-1}\\ \\ \\ \\ \\ \\ \\ \\ \\nu_1 = \\nu_0 + n \\end{align}\\] For the posterior distribution of our category, \\(\\pi_k\\), let us recall the derivation for a multinomial-Dirichlet conjugacy: \\[\\begin{align} \\pi_k \\sim \\mathcal{Dir}\\left(\\omega_1\\right) = \\frac{1}{\\mathcal{B}\\left(\\omega_1\\right)}\\prod_{k=1}^K x_i^{\\omega_1-1} \\end{align}\\] obtaining the following: \\[\\begin{align} \\omega_1 = \\sum_{i=1}^k\\left(x_i + \\omega_0\\right) \\end{align}\\] A natural step to take here is to initialize the hyperparameters and iteratively optimize the model parameters \\(\\{\\ \\pi_k, \\mu_k, \\Lambda_k\\ \\}\\), including the latent variable y. We have shown this in the Expectation-Maximization section. However, to illustrate variational inference, let us skip the analytical operations and the dependency to conjugate priors. Instead, we move on to the Mean-Field modeling for approximation. For the Mean-Field model, the family of distributions above forms our variational distribution that approximates our true posterior such that \\(P(y,\\pi, \\mu, \\Lambda |x) \\approx \\mathcal{Q}(y,\\pi, \\mu, \\Lambda)\\). For mathematical convenience, let us temporarily use z as placeholder for \\(\\{\\ y, \\pi, \\mu, \\Lambda\\ \\}\\) so that we have the following: \\[\\begin{align} P(z|x) \\approx \\mathcal{Q}(z)\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ P(y,\\pi, \\mu, \\Lambda |x) \\approx \\mathcal{Q}(y,\\pi, \\mu, \\Lambda). \\end{align}\\] Then, to generalize, we factorize into a simpler tractable granular composition (a factor) such that we have the following: \\[\\begin{align} \\mathcal{Q}(z) = \\prod_{j=1} \\mathcal{Q}_i(z_i) \\end{align}\\] That is called mean-field variational family (Blei D. et al. 2017). We explain the reason for factorization in a few steps. It is important to assume that the latent variable and its parameters are independent (by virtue of mean-field variational property), and so we can model a joint distribution - our variational distribution - like so: \\[\\begin{align} \\mathcal{Q}(z) = \\mathcal{Q}(y,\\pi, \\mu, \\Lambda ) = \\prod_{i=1}^N \\mathcal{Q}_y(y_i) \\times \\prod_{k=1}^K\\left[\\mathcal{Q}_\\pi(\\pi_k) \\times \\mathcal{Q}_\\mu(\\mu_k)\\times\\mathcal{Q}_{\\Lambda}\\left(\\Lambda_k\\right)\\right] \\end{align}\\] Second, because our objective is to optimize our variational distribution, we need to develop an objective function that we can maximize. Let us show a couple of derivations that we need later for our inference. We start by deriving a variational lower bound equation, also called evidence lower bound (ELBO), and a KL divergence equation (Yang X. 2017). Both are derived from the marginal likelihood factor in the Bayes Theorem - the evidence. \\[\\begin{align} P(x) {}&amp;= \\int_z P(x, z)\\ dz &amp; \\text{(marginal/sum rule)}\\\\ \\log_e P(x) &amp;= \\log_e \\int_z P(x, z)\\ dz &amp; \\text{(logarithm)}\\\\ &amp;= \\log_e \\int_z \\frac{\\mathcal{Q}(z)}{\\mathcal{Q}(z)} P(x,z)\\ dz &amp; \\text{(variational distribution)}\\\\ &amp;\\equiv \\log_e\\left(\\mathbb{E}_{\\mathcal{Q}(z)} \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right) &amp; \\text{(expectation)}\\\\ &amp;\\ge \\mathbb{E}_{\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right) &amp; \\text{(jensen&#39;s inequality)} \\end{align}\\] \\[\\begin{align} &amp; \\text{(focus on the equal sign of the inequality,} &amp; \\nonumber \\\\ &amp; \\text{extract the lower bound)} &amp; \\nonumber \\\\ \\nonumber \\\\ \\mathcal{LB}[Q(z)] &amp;= \\mathbb{E}_{\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right)\\ \\ \\ \\ (\\text{lower bound - } \\mathbf{\\text{ELBO}})\\\\ &amp;\\equiv \\int_z {\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right) dz &amp; \\text{(equivalent)}\\\\ &amp;= \\int_z {\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(z|x)P(x)}{\\mathcal{Q}(z)}\\right) dz &amp; \\text{(chain rule)}\\\\ &amp;= \\int_z {\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(z|x)}{\\mathcal{Q}(z)} + \\log_e P(x)\\right) dz &amp; \\text{(simplify)}\\\\ &amp;= \\underbrace{\\int_z \\mathcal{Q}(z) \\log_e \\frac{P(z|x)}{\\mathcal{Q}(z)}\\ dz }_\\text{-KL divergence} + \\underbrace{\\int_z \\mathcal{Q}(z) \\log_e P(x)\\ dz }_\\text{marginal log likelihood} &amp; \\text{(logarithm)}\\\\ \\nonumber \\\\ \\mathcal{LB}[Q(z)] &amp;= -KL(\\mathcal{Q}(z) || P(z|x)) + \\log_e P(x) &amp; \\text{(ELBO eq. 1)}\\\\ \\nonumber \\\\ \\mathcal{KL}(\\mathcal{Q}(z) || P(z|x)) &amp;= -\\mathcal{LB}[Q(z)] + \\log_e P(x) &amp; \\text{(KL divergence)} \\end{align}\\] The second term in the ELBO equation can be treated as a constant, and because it is independent of the approximating distribution, it can therefore be ignored. Note that the lower bound (ELBO) is a functional expression as it accepts a probability function, namely \\(Q(z)\\). We now have an objective function we can use in the form of the ELBO. Both ELBO and KL divergence can be used as an objective function. See below. \\[\\begin{align} \\mathcal{LB}(\\mathcal{Q}(z)) = \\int_z {\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right) dz \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{KL}(\\mathcal{Q}(z)||\\mathcal{Q}(z|x)) = - \\int_z \\mathcal{Q}(z) \\log_e \\frac{P(z|x)}{\\mathcal{Q}(z)}\\ dz \\end{align}\\] By maximizing ELBO, we are effectively minimizing KL divergence which means that our variational distribution is closer to our posterior, e.g. \\(\\mathcal{Q}(z) \\approx P(z|x)\\). See Figure 7.20 for the balance: Figure 7.20: KL Divergence vs Lower Bound However, notice that the KL divergence has a conditional distribution that can potentially be intractable. That also effectively renders ELBO intractable. Let us show an alternative ELBO measurement. We use Entropy over KL divergence, which we can also derive from the lower bound. We avoid the use of the chain rule in what follows. \\[\\begin{align} \\mathcal{LB}[Q(z)] {}&amp;= \\int_z {\\mathcal{Q}(z)} \\left( \\log_e \\frac{P(x,z)}{\\mathcal{Q}(z)}\\right) dz &amp; \\text{(lower bound)}\\\\ &amp;= \\int_z \\mathcal{Q}(z) \\left( \\log_e P(x,z) - \\log_e \\mathcal{Q}(z) \\right) \\ dz &amp; \\text{(logarithm rule)}\\\\ &amp;= \\underbrace{\\int_z \\mathcal{Q}(z)\\log_e P(x,z)\\ dz}_\\text{expected energy}\\ \\underbrace{-\\int_z \\mathcal{Q}(z)\\log_e \\mathcal{Q}(z) \\ dz}_\\text{Entropy} &amp; \\text{(simplify)}\\\\ \\nonumber \\\\ \\mathcal{LB}[Q(z)] &amp;= \\mathbb{E}_{Q(z)}\\left[\\log_e P(x,z)\\right] + \\mathcal{H}(z) &amp; \\text{(ELBO eq. 2)} \\end{align}\\] Third, now that we have an objective function in the form of ELBO eq. 2, we also need to generate an update function to optimize our model parameters. That is where we need to perform more derivation. So let us expand ELBO eq. 2 (Yuling Yao et al 2018; Keng B. 2018). \\[\\begin{align} {}&amp;\\mathcal{LB}[Q(z)] = \\mathbb{E}_{Q(z)}(P(x,z)) + \\mathcal{H}(z) \\\\ &amp;= \\int_z \\mathcal{Q}(z)\\log_e P(x,z)\\ dz -\\int_z \\mathcal{Q}(z)\\log_e \\mathcal{Q}(z) \\ dz\\\\ &amp;= \\int_z \\prod_{i=1} \\mathcal{Q_i}(z_i)\\log_e P(x,z)\\ dz -\\int_z \\prod_{i=1} \\mathcal{Q_i}(z_i)\\log_e \\prod_{i=1} \\mathcal{Q_i}(z_i) \\ dz &amp; \\text{(mean-field assumption)}\\\\ &amp;\\rightarrow \\prod_{i=1} \\mathcal{Q}_i(z_i) = \\mathcal{Q}_j(z_j) \\times \\prod_{i \\ne j} \\mathcal{Q}_i(z_i) &amp; \\text{(extract jth component)} \\end{align}\\] \\[\\begin{align} &amp;= \\int_{z_j} \\mathcal{Q}_j(z_j) \\int_{z_{-j}} \\left[ \\prod_{i \\ne j} \\mathcal{Q}_i(z_i) \\log_e P(x,z) \\right] d_{z_{-j}} d_{z_j} &amp; \\text{(notation:} -j \\equiv i\\ne j ) \\nonumber \\\\ &amp;- \\int_{z_j} \\mathcal{Q}_j(z_j) \\int_{z_{-j}} \\prod_{i \\ne j} \\mathcal{Q}_i(z_i) \\left( \\log_e \\mathcal{Q}_j(z_j) + \\log_e \\prod_{i \\ne j} \\mathcal{Q}_i(z_i)\\right) d_{z_{-j}}d_{z_j}\\\\ &amp;\\rightarrow \\int_{z_{-j}} \\ \\prod_{i \\ne j} \\mathcal{Q}_i(z_i) d_{z_{-j}} = 1 &amp; \\text{(simplify)}\\\\ &amp;\\rightarrow \\int_{z_{-j}} \\ \\prod_{i \\ne j} \\mathcal{Q}_i(z_i) \\log_e \\prod_{-j} \\mathcal{Q}_i(z_i) d_{z_{i\\ne j}} = const &amp; \\text{(simplify)}\\\\ &amp;= \\int_{z_j} \\mathcal{Q}_j(z_j) \\mathbb{E}_{Q_{z_{i\\ne j}}} \\left( \\log_e P(x,z) \\right) d_{z_j} &amp; \\text{(expectation)} \\nonumber \\\\ &amp;- \\int_{z_j} \\mathcal{Q}_j(z_j) \\left( \\log_e \\mathcal{Q}_j(z_j) + const \\right)d_{z_j}\\\\ \\nonumber \\\\ &amp;= \\int_{z_j} \\mathcal{Q}_j(z_j) \\left(\\mathbb{E}_{Q_{z_{i\\ne j}}}(\\log_e P(x,z)) - \\log_e \\mathcal{Q}_j(z_j)\\right) dz_j &amp; \\text{(factorized ELBO)} \\end{align}\\] Notice that we have factorized the lower bound. We also can generate the factorized version of the KL divergence (which we may not be using in our discussion): \\[\\begin{align} \\mathcal{LB}[\\mathcal{Q}(z)] &amp;= \\int_{z_j} \\mathcal{Q}_j(z_j) \\log_e \\frac{\\mathbb{E}_{Q_{z_{i\\ne j}}}(\\log_e P(x,z)) } {\\mathcal{Q}_j(z_j)} dz_j &amp; \\text{(-KL divergence)}\\\\ &amp;= -KL(\\mathcal{Q}_j(z_j)||\\mathbb{E}_{Q_{z_{i\\ne j}}}(\\log_e P( x,z)) ) \\end{align}\\] To get the update function, we maximize the factors. We use Lagrange multiplier and partial derivative with respect to the function \\(\\mathcal{Q}_j\\) to extract (formulate) the equation for any arbitrary jth component - this uses functional differential: \\[\\begin{align} \\frac{ \\partial\\ \\mathcal{LB}[\\mathcal{Q_j}(z_j)]}{\\partial\\ \\mathcal{Q}_j(z_j)} {}&amp;\\equiv \\frac{ \\partial}{\\partial\\ \\mathcal{Q}_j(z_j)} \\left[ \\int_{z_j} \\mathcal{Q}_j(z_j) \\left(\\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z)) - \\log_e \\mathcal{Q}_j(z_j)\\right) dz_j \\right] = 0\\\\ &amp;\\equiv \\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z)) - \\log_e \\mathcal{Q}_j(z_j) - 1 = 0 \\nonumber\\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(entropy functional derivative)}\\\\ &amp;\\equiv exp\\left[\\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z)) - \\log_e \\mathcal{Q}_j(z_j) \\right] = const \\ \\ \\ \\text{(exp-log)}\\\\ &amp;\\equiv exp\\ \\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z)) - \\mathcal{Q}_j(z_j) ) = const\\ \\ \\ \\text{(exp-log)}\\\\ \\nonumber \\\\ \\mathcal{Q}_j(z_j) &amp;\\propto exp\\left[\\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z))\\right] + const\\\\ &amp;\\propto exp\\left[\\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z))\\right] + const\\ \\ \\ \\ \\text{(factorized; mean-field property)} \\end{align}\\] Herein lies a template for our update function that we can use to update individual factors. \\[\\begin{align} \\mathcal{Q}_j(z_j) \\propto exp\\left[\\mathbb{E}_{Q_{z_{i \\ne j}}}(\\log_e P(x,z))\\right] + const \\end{align}\\] Equivalently, we have the following expectation equation for the log factor: \\[\\begin{align} \\log_e \\mathcal{Q}_j(z_j) \\propto \\mathbb{E}_{Q_{z_{i \\ne j}}}\\left[\\log_e P(x,z)\\right] + const \\end{align}\\] That naturally completes the picture in which we have a factor-level update function and an ELBO eq 2 to use for convergence. Fourth, at this point, we still do not have a shape for each factor of our variational distribution, namely \\(\\mathcal{Q}(z) = \\mathcal{Q}(y, \\pi, \\mu, \\Lambda)\\). Our next goal is to pick or make assumptions on the closest type of distribution to use for each factor. For that, we use Figure 7.19 around the conditional dependency on the Bayesian model. Notice that both our ELBO eq. 2 and update function rely on \\(\\log_e P(x, z)\\). Let us expand the log probability function: \\[\\begin{align} \\log_e P(x, z) {}&amp;= \\log_e P(x, y, \\pi, \\mu, \\Lambda)\\\\ &amp;= \\log_e \\left[ P(x| y, \\mu, \\Lambda) P(y|\\pi)P(\\pi)P(\\mu|\\Lambda)P(\\Lambda)\\right] \\\\ &amp;= \\sum_{k=1}^K \\biggl[ \\log_e \\prod_{i=1}^N P(x_i| y_{ik}, \\mu_k, \\Lambda_k) + \\log_e \\prod_{i=1}^N P(y_{ik}|\\pi_k) + \\log_e P(\\pi_k) +\\log_e P(\\mu_k,\\Lambda_k) \\biggr] \\end{align}\\] Equivalently, the approximate joint distribution is shown below. \\[\\begin{align} P(x, y, \\pi, \\mu, \\Lambda) \\approx \\mathcal{Q}(y, \\pi, \\mu, \\Lambda) = \\mathcal{Q}_y(y)\\mathcal{Q}_\\pi(\\pi) \\prod_{k=1}^K\\mathcal{Q}(\\mu_k, \\Lambda_k) \\end{align}\\] Both \\(\\mu_k\\) and \\(\\Lambda_k\\) are not split apart into their conditional form, e.g.: \\[\\begin{align} \\log_e P(\\mu_k,\\Lambda_k) = \\log_e P(\\mu_k|\\Lambda_k) + \\log_e P(\\Lambda_k). \\end{align}\\] Note that the shape of the joint distribution is not the one of interest, but we use the factors above to approximate the shape of the closest distribution we pick. For \\(\\mathbf{\\mathcal{Q}_y(y)}\\), we absorb terms with respect to y: \\[\\begin{align} \\log_e \\mathcal{Q}^*_y(y) {}&amp;\\propto \\mathbb{E}_{Q_{-y}}\\left[\\log_e P(x,z)\\right] + const\\\\ &amp;\\propto \\mathbb{E}_{Q_{\\mu,\\Lambda}}\\left[ \\log_e P(x| y, \\mu, \\Lambda)\\right] + \\mathbb{E}_{Q_{\\pi}}\\left[\\log_e P(y|\\pi)\\right] + const\\\\ &amp;\\propto \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik}) \\left[ \\log_e \\mathcal{N}_p(\\ x_i\\ ;\\ \\mu_k, {\\Lambda_k}^{-1}\\ ) + \\log_e {\\pi_k} \\right] + const \\end{align}\\] Operate on the first term, recalling that \\(\\Lambda_k\\) is a positive-definite precision matrix: \\[\\begin{align} \\log_e \\mathcal{N}_p(\\ x_i\\ ;\\ \\mu_k, {\\Lambda_k}^{-1}\\ ) &amp;= \\log_e \\left[\\frac{|\\Lambda_k|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left( -\\frac{1}{2}(x_i - \\mu_k)^T\\Lambda_k(x_i - \\mu_k)\\right)\\right] \\end{align}\\] Note that \\((\\log_e \\pi_k)\\) is a categorical weight or mixture coefficient and thus should not to be confused with the gaussian normalizing constant, namely \\((2\\pi)^{\\frac{p}{2}}\\). Combine the two terms and use a placeholder, namely \\(\\log_e\\ \\rho_{ik}\\). \\[\\begin{align} \\log_e\\ \\rho_{ik} {}&amp;= \\log_e \\left[\\frac{|\\Lambda_k|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left( -\\frac{1}{2}(x_i - \\mu_k)^T\\Lambda_k(x_i - \\mu_k)\\right)\\right] + \\log_e \\pi_k\\\\ &amp;=\\frac{1}{2}\\log_e |\\Lambda_k| - \\frac{p}{2}\\log_e (2\\pi) - \\frac{1}{2}(x_i - \\mu_k)^T\\Lambda_k(x_i - \\mu_k) + \\log_e \\pi_k \\end{align}\\] Also, take a note of the following expectation and its vector of cluster sizes, namely \\(\\tau_{k}\\), used by other variational factors: \\[\\begin{align} \\mathbb{E}_{Q_{y}}\\left[y_{ik} \\right] = \\tau_{ik} = \\frac{\\rho_{ik}}{\\sum_{j=1}^K \\rho_{ij}} \\ \\ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\ \\ \\ \\ \\tau_k = \\sum_{i=1}^N \\tau_{ik} \\end{align}\\] where: \\[\\begin{align} \\rho_{ik} = \\frac{\\pi_k|\\Lambda_k|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left(-\\frac{1}{2}\\biggl[ (x_i - \\mu_k)^T\\Lambda_k(x_i - \\mu_k)\\biggr]\\right) \\end{align}\\] Now, combine the two terms and solve for \\(\\mathcal{Q}^*_y(y)\\): \\[\\begin{align} \\log_e \\mathcal{Q}^*_y(y) {}&amp;= \\sum_{k=1}^K \\sum_{n=1}^N (y_{ik}) \\log_e \\rho_{ik} = \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik}) \\log_e \\tau_{ik} \\label{eqn:eqnnumber316} \\\\ \\mathcal{Q}^*_y({{y_{ik}}; \\tau_{ik}}) &amp;= Multi( {{y_{ik}}; \\tau_{ik}}) = \\prod_{i=1}^N \\prod_{k=1}^K {\\tau_{ik}}^{y_{ik}} &amp; \\begin{array}{rr} \\text{(exponentiate)}\\\\ \\text{(Multinomial PDF)} \\end{array} \\label{eqn:eqnnumber317} \\end{align}\\] For \\(\\mathbf{\\mathcal{Q}_\\pi(\\pi)}\\), we absorb terms with respect to \\(\\pi\\): \\[\\begin{align} \\log_e \\mathcal{Q}_\\pi(\\pi) {}&amp;\\propto \\mathbb{E}_{Q_{-\\pi}}\\left[\\log_e P(x,z)\\right] + const\\\\ &amp;\\propto \\mathbb{E}_{Q_{y}}\\left[\\log_e P(z|\\pi)\\right] + \\log_e P(\\pi) + const\\\\ &amp;\\propto \\sum_{k=1}^K \\left[\\sum_{i=1}^N (y_{ik}) \\log_e {\\pi_k} + \\log_e \\mathcal{Dir}(\\pi_{k}; \\omega_0) \\right] + const\\\\ &amp;\\propto \\left[\\sum_{k=1}^K \\sum_{i=1}^N (y_{ik}) \\log_e {\\pi_k} + \\log_e \\mathcal{Dir}(\\pi_{1:k}; \\omega_0) \\right] + const \\end{align}\\] Operate on the first term: \\[\\begin{align} \\left[\\sum_{k=1}^K \\sum_{i=1}^N (y_{ik}) \\log_e {\\pi_k} \\right] = \\sum_{k=1}^K \\sum_{i=1}^N (\\tau_{ik}) \\log_e {\\pi_k} \\ \\leftarrow \\ {\\tau}_{ik} &amp; \\text{ (normalized)} \\end{align}\\] Operate on the second term (See Dirichlet distribution in Chapter 5 (Numerical Probability and Distribution) for \\(\\mathcal{B}(w_0)\\)): \\[\\begin{align} \\log_e \\mathcal{Dir}(\\pi_{1:k}; \\omega_0) {}&amp;\\rightarrow \\log_e \\left[\\frac{1}{\\mathcal{B}(\\omega_0)}\\prod_{k=1}^K {\\pi_k}^{\\omega_0 - 1}\\right]\\\\ &amp;= \\log_e \\frac{1}{\\mathcal{B}(\\omega_0)} + \\log_e \\prod_{k=1}^K {\\pi_k}^{\\omega_0 - 1} &amp; \\text{(logarithm)}\\\\ &amp;= \\log_e \\prod_{k=1}^K {\\pi_k}^{\\omega_0 - 1} + const &amp; \\text{(constant)}\\\\ &amp;= (\\omega_0 - 1)\\sum_{k=1}^K \\log_e {\\pi_k} + const \\end{align}\\] Now, combine the two terms and solve for \\(\\mathcal{Q}^*_\\pi(\\pi)\\): \\[\\begin{align} \\log_e \\mathcal{Q}_\\pi(\\pi) {}&amp;= \\sum_{k=1}^K \\sum_{i=1}^N (\\tau_{ik}) \\log_e {\\pi_k} + (\\omega_0 - 1)\\sum_{k=1}^K \\log_e {\\pi_k} + const \\label{eqn:eqnnumber318}\\\\ &amp;= \\sum_{k=1}^K \\left[\\sum_{i=1}^N (\\tau_{ik}) + (\\omega_0 - 1) \\right] \\log_e {\\pi_k} + const \\label{eqn:eqnnumber319}\\\\ \\mathcal{Q}_\\pi^*(\\pi) &amp;= \\sum_{k=1}^K {\\pi_k} \\times exp \\left[\\left( \\tau_{k} + \\omega_0 \\right) - 1 \\right] + const &amp; \\begin{array}{rr} \\text{(exponentiate)} \\\\ \\text{(Dirichlet PDF)} \\\\ \\end{array} \\label{eqn:eqnnumber320} \\\\ \\nonumber \\\\ \\mathcal{Q}_\\pi^*(\\pi_{1:k}; \\omega_1) &amp;= \\text{Dir}\\left(\\pi_{1:k}| \\omega_1 \\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\omega_1 = \\tau_{k} + \\omega_0 \\label{eqn:eqnnumber321} \\end{align}\\] For \\(\\mathbf{\\mathcal{Q}(\\mu, \\Lambda) }\\), we absorb terms with respect to \\(\\mu\\) and \\(\\Lambda\\). Here, we choose to compute both parameters via normal-Wishart distribution; albeit, one can choose to calculate the two parameters independently. \\[\\begin{align} \\log_e {}&amp;\\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) \\propto \\mathbb{E}_{Q_{-\\mu,\\Lambda}}\\left[\\log_e P(x,z)\\right] + const\\\\ &amp;\\propto \\mathbb{E}_{Q_{y}}\\left[ \\log_e P(x| y, \\mu, \\Lambda) + \\log_e P(\\mu,\\Lambda) \\right] + const\\\\ &amp;\\propto \\left[ \\sum_{k=1}^K \\sum_{i=1}^N (y_{ik})\\log_e \\mathcal{N}_p(x_i; \\mu_k, {\\Lambda_k}^{-1}) \\right] + \\nonumber \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left[\\sum_{k=1}^K \\log_e\\ \\mathcal{NW}_p(\\mu_k, \\Lambda_k; \\alpha_0, (\\beta_0\\Lambda_k)^{-1}, \\nu_0, \\Sigma_0) \\right] + c\\\\ \\nonumber \\\\ \\rightarrow&amp; \\text{Recalling that } \\mathbb{E}_{Q_{y}}\\left[y_{ik} \\right] = \\tau_{ik}. \\nonumber \\\\ &amp;\\propto \\left[ \\sum_{k=1}^K \\sum_{i=1}^N (\\tau_{ik})\\log_e \\mathcal{N}_p(x_i; \\mu_k, {\\Lambda_k}^{-1}) \\right] + \\nonumber \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left[\\sum_{k=1}^K \\log_e\\ \\mathcal{NW}_p(\\mu_k, \\Lambda_k; \\alpha_0, (\\beta_0\\Lambda_k)^{-1}, \\nu_0, \\Sigma_0) \\right] + c\\\\ \\nonumber \\\\ \\rightarrow &amp;\\text{Let } \\tau_k = \\sum_{i=1}^N (\\tau_{ik}) \\text{ and } \\bar{x}_k = \\frac{1}{\\tau_k} \\sum_{i=1}^N \\left[ (\\tau_{ik}) x_i\\right]. \\\\ &amp;\\propto \\left[ \\sum_{k=1}^K (\\tau_{k}) \\log_e \\mathcal{N}_p(\\bar{x}_k; \\mu_k, {\\Lambda_k}^{-1}) \\right] + \\nonumber \\\\ &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\left[\\sum_{k=1}^K \\log_e\\ \\mathcal{NW}_p(\\mu_k, \\Lambda_k; \\alpha_0, (\\beta_0\\Lambda_k)^{-1}, \\nu_0, \\Sigma_0) \\right] + c \\end{align}\\] Operate on the first term (dropping constants): \\[\\begin{align} \\log_e \\mathcal{N}&amp;(\\ \\bar{x}_k \\ ;\\ \\mu_k, {\\Lambda_k}^{-1}\\ ) \\nonumber \\\\ &amp;= \\log_e \\left[\\frac{|\\Lambda_k|^{\\frac{1}{2}}}{(2\\pi)^{\\frac{p}{2}}} exp\\left( -\\frac{1}{2}(\\bar{x}_k - \\mu_k)^T\\Lambda_k(\\bar{x}_k - \\mu_k)\\right)\\right] \\\\ &amp;= (\\log_e |\\Lambda_k|^{ \\frac{1}{2}} ) - \\frac{p}{2}(\\log_e 2\\pi ) -\\frac{1}{2}(\\bar{x} - \\mu_k)^T\\Lambda_k(\\bar{x} - \\mu_k) &amp; \\text{(logarithm)}\\\\ &amp;\\propto (\\log_e |\\Lambda_k|^{ \\frac{1}{2}} ) -\\frac{1}{2}\\Lambda_k(\\bar{x}_k \\bar{x}_k^T - \\bar{x}_k {\\mu_k}^T - \\mu_k \\bar{x}_k^T + \\mu_k {\\mu_k}^T) &amp; \\text{(logarithm)} \\end{align}\\] Operate on the second term (dropping constants). \\[\\begin{align} \\log_e &amp;\\mathcal{NW}_p{}(\\mu_k, \\Lambda_k ;\\ \\alpha_0, (\\beta_0 \\Lambda_k)^{-1}, \\nu_0, \\Sigma_0\\ ) \\nonumber \\\\ &amp;\\propto log_e \\left( \\ \\underbrace{ |\\beta_0\\Lambda_k|^{\\frac{1}{2}} exp\\left[-\\frac{1}{2}(\\mu_k - \\alpha_0)^T\\beta_0\\Lambda_k(\\mu_k - \\alpha_0)\\right]}_\\text{gaussian} \\times \\underbrace{ |\\Lambda_k|^{\\frac{\\nu_0-p-1}{2}} exp\\left[-\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda_k)\\right] }_\\text{wishart } \\right) \\\\ &amp;\\propto log_e \\left( |\\beta_0|^\\frac{1}{2} |\\Lambda_k|^{\\frac{1}{2}} |\\Lambda_k|^{\\frac{\\nu_0-p-1}{2}} exp\\left[-\\frac{\\beta_0}{2}\\Lambda_k(\\mu_k - \\alpha_0)(\\mu_k - \\alpha_0)^T + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda_k) \\right] \\right) \\\\ &amp;\\propto log_e \\left[ |\\Lambda_k|^{\\frac{\\nu_0-p}{2}} exp\\left[-\\frac{\\beta_0}{2}\\Lambda_k(\\mu_k\\mu_k^T - \\mu{\\alpha_0}^T - {\\alpha_0}\\mu^T + \\alpha_0{\\alpha_0}^T) + -\\frac{1}{2}tr(\\Sigma_0^{-1}\\Lambda_k) \\right] \\right] \\end{align}\\] Combine the two terms: \\[\\begin{align} \\log_e \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) {}&amp;\\propto \\left[ \\sum_{k=1}^K \\left( \\tau_{k}\\log_e |\\Lambda_k|^{ \\frac{1}{2}} -\\frac{\\tau_k}{2}\\Lambda_k(\\bar{x}_k \\bar{x}_k^T - \\bar{x}_k {\\mu_k}^T - \\mu_k \\bar{x}_k^T + \\mu_k {\\mu_k}^T) \\right)\\right] \\nonumber \\\\ &amp;+ \\sum_{k=1}^K \\left[log_e |\\Lambda_k|^{\\frac{\\nu_0 - p}{2}} -\\frac{1}{2}\\left(\\beta_0 \\Lambda_k(\\mu_k\\mu_k^T - \\mu_k{\\alpha_0}^T - {\\alpha_0}\\mu_k^T + \\alpha_0{\\alpha_0}^T) -tr(\\Sigma_0^{-1}\\Lambda_k) \\right) \\right] \\end{align}\\] Exponentiate: \\[\\begin{align} \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) {}&amp;\\propto \\left[ \\prod_{k=1}^K \\left(|\\Lambda_k|^{ (\\tau_{k})/2} exp \\left[ -\\frac{\\tau_k}{2}\\Lambda_k\\left(\\bar{x}_k \\bar{x}_k^T - \\bar{x}_k {\\mu_k}^T - \\mu_k \\bar{x}_k^T + \\mu_k {\\mu_k}^T\\right) \\right]\\right)\\right] \\nonumber \\\\ &amp;\\times \\prod_{k=1}^K \\left[ |\\Lambda_k|^{\\frac{\\nu_0 - p}{2}} exp \\left( -\\frac{1}{2}\\biggl[\\beta_0 \\Lambda_k(\\mu_k\\mu_k^T - \\mu_k{\\alpha_0}^T - {\\alpha_0}\\mu_k^T + \\alpha_0{\\alpha_0}^T) -tr(\\Sigma_0^{-1}\\Lambda_k) \\biggr] \\right)\\right]\\\\ \\nonumber \\\\ &amp;\\propto \\prod_{k=1}^K |\\Lambda_k|^{(\\nu_0 + \\tau_{k} - p)/2} exp \\biggl[ -\\frac{1}{2}\\biggl(\\Lambda_k \\biggl(\\Upsilon \\biggr) - tr\\biggl(\\Sigma_0^{-1}\\Lambda_k\\biggr) \\biggr) \\biggr] \\end{align}\\] \\[ \\text{let }\\Upsilon = \\tau_k(\\bar{x}_k \\bar{x}_k^T - \\bar{x}_k {\\mu_k}^T - \\mu_k \\bar{x}_k^T + \\mu_k {\\mu_k}^T) + \\beta_0(\\mu_k\\mu_k^T - \\mu_k{\\alpha_0}^T - {\\alpha_0}\\mu_k^T + \\alpha_0{\\alpha_0}^T) \\] Now, solving for \\(\\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda)\\) from here, recall the simplification of the exponent under the Normal Wishart Conjugacy Section. That leads to the following: \\[\\begin{align} \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda\\ ) {}&amp;\\propto \\prod_{k=1}^K |\\Lambda_k|^{(\\nu_0+\\tau_{k}-p)/2} \\nonumber \\\\ &amp;\\underbrace{ exp\\biggl[-\\frac{(\\beta_0 + \\tau_{k}) }{2}\\biggl( \\mu_k - \\frac{(\\alpha_0\\beta_0 + \\tau_{k}\\bar{x}_k)}{(\\beta_0 + \\tau_{k})} \\biggr)^T\\Lambda_k\\biggl( \\mu_k - \\frac{(\\alpha_0\\beta_0 + \\tau_{k}\\bar{x}_k)}{(\\beta_0 + \\tau_{k})} \\biggr) \\biggr]}_\\text{gaussian} \\times \\nonumber \\\\ &amp;\\underbrace{exp\\biggl[-\\frac{1}{2}tr\\biggl( \\frac{ \\tau_k\\beta_0}{(\\beta_0 + \\tau_{k})}(\\tau_k\\bar{x}_k - \\alpha_0)( \\tau_k\\bar{x}_k - \\alpha_0 )^T + (\\tau_{k}) S_k + \\Sigma_0^{-1} \\biggr) \\Lambda_k \\biggr]}_\\text{wishart} \\end{align}\\] Reparameterize the prior hyperparameters: \\[\\begin{align} \\alpha_1 {}&amp;= \\frac{(\\alpha_0\\beta_0 + \\tau_{k}\\bar{x}_k)}{(\\beta_0 + \\tau_{k})}\\\\ \\beta_1 &amp;= \\beta_0 + \\tau_{k} \\\\ \\nu_1 {}&amp;= \\nu_0 + \\tau_{k} \\\\ \\Sigma_1 &amp;= \\frac{ \\tau_k \\beta_0}{(\\beta_0 + \\tau_{k})}(\\bar{x}_k - \\alpha_0)( \\bar{x}_k - \\alpha_0 )^T + \\tau_{k} S_k + \\Sigma_0^{-1} \\\\ \\nonumber \\\\ &amp;where: S_k = \\frac{1}{\\tau_{k}} \\sum_{i=1}^n (\\tau_{ik}) (x_i - \\bar{x}_k)(x_i - \\bar{x}_k)^T. \\end{align}\\] We then get the following equation: \\[\\begin{align} \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) &amp;= \\prod_{k=1}^K \\mathcal{N}_p(\\mu_k; \\alpha_1, (\\beta_1\\Lambda_k)^{-1})\\times \\mathcal{W}_p(\\Lambda_k; \\nu_1, \\Sigma_1) &amp; \\text{(Normal-Wishart PDF)} \\end{align}\\] Finally, our variational distribution has the following equation: \\[\\begin{align} \\mathcal{Q}(z) {}&amp;= \\mathcal{Q}^*_y(y) \\times \\mathcal{Q}_\\pi^*(\\pi) \\times \\prod_{k=1}^K \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) \\\\ &amp;= \\text{Multi}( {{y_{ik}}; \\tau_{ik}}) \\times \\text{Dir}\\left(\\pi_{1:k}| \\omega_1 \\right) \\times \\prod_{k=1}^K \\mathcal{N}_p(\\mu_k; \\alpha_1, (\\beta_1\\Lambda_k)^{-1})\\times \\mathcal{W}_p(\\Lambda_k; \\nu_1, \\Sigma_1) \\end{align}\\] Fifth, we need to use an iterative algorithm to optimize our model. We choose to use the Variational EM algorithm or a straight-forward Coordinate Ascent Variational Inference (CAVI). As for CAVI, we have the below algorithm (Bishop C.M 2006; Blei D. et al 2017): \\[\\begin{align*} \\begin{array}{ll} \\mathcal{LB}[Q(z)] = \\mathbb{E}_{Q(z)}\\left[\\log_e P(x,z)\\right] + \\mathcal{H}(z) &amp; \\text{(ELBO eq. 2)}\\\\ \\text{while}\\ \\mathbf{ELBO}\\ \\text{has not converged}\\\\ \\ \\ \\ \\ \\text{for j in 1,...,m}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{set } \\log_e \\mathcal{Q}_j(z_j) \\propto \\mathbb{E}_{Q_{z_{i \\ne j}}}\\left[\\log_e P(x,z)\\right] \\\\ \\ \\ \\ \\ \\text{end}\\\\ \\ \\ \\ \\ \\text{compute}\\ \\mathbf{ELBO}\\\\ \\text{end} \\end{array} \\end{align*}\\] On the other hand, to illustrate the use of variational EM (which has a different arrangement), we first initialize the parameters. Initialization: We start VEM by initializing the parameters for each variational factor, given the following assumptions: p is the number of random variables (p-dimensions). n is the number of observations for each random variable. k is the number of classes or clusters (e.g., tri-modal mixture). For model parameters: \\[ \\mathbf{x} = \\left[ \\begin{array}{c} x_{1n} \\\\ x_{2n} \\\\ \\end{array} \\right]_{p=2}\\ \\ \\ \\ \\mathbf{\\mu} = \\left[ \\begin{array}{c} \\mu_{1} = \\bar{x}_1 \\\\ \\mu_{2} = \\bar{x}_2\\\\ \\end{array} \\right]_{p=2} \\] \\[ \\Lambda = \\left(\\left[ \\begin{array}{cc} \\sigma_{11} &amp; \\sigma_{1p}\\\\ \\sigma_{p1} &amp; \\sigma_{pp} \\end{array} \\right]_{pxp}^{-1} \\left[ \\begin{array}{cc} \\sigma_{11} &amp; \\sigma_{1p}\\\\ \\sigma_{p1} &amp; \\sigma_{pp} \\end{array} \\right]_{pxp}^{-1} \\left[ \\begin{array}{cc} \\sigma_{11} &amp; \\sigma_{1p}\\\\ \\sigma_{p1} &amp; \\sigma_{pp} \\end{array} \\right]_{pxp}^{-1} \\right)_{k=3}^T \\] Note that \\(\\mu_k\\) and \\(\\Lambda_k\\) are unknown. For hyperparameters: \\[\\begin{align*} \\omega_0 &amp;= \\left[\\begin{array}{lll}1/K &amp; 1/K &amp; 1/K \\end{array}\\right] &amp; \\text{(hyper-proportionality of } \\pi_k \\text{)}\\\\ \\alpha_0 &amp;= \\left[\\begin{array}{ll}0 &amp; 0 \\end{array}\\right] &amp; \\text{(hyper-mean of } \\mu_k \\text{)} \\\\ \\beta_0 &amp;= 1 &amp; \\text{(hyper-variance of } \\mu_k \\text{)} \\\\ \\nu_0 &amp;= p -1 &amp; \\text{(degrees of freedom of } \\Lambda_k \\text{)} \\\\ \\Sigma_0 &amp;= \\left[\\begin{array}{cc} 1 &amp; 0\\\\ 0 &amp; 1\\\\ \\end{array}\\right]_{pxp} &amp; \\text{(hyper-covariance of } \\Lambda_k \\text{)}\\\\ \\end{align*}\\] Variational Estimation Step (VE-Step): In this step, we use the initialized (and eventually the optimized) parameters to calculate expectations and \\(\\mathcal{Q}^*(y)\\). For expectation of log-determinant of Wishart covariance and log of the mixture coefficient, we can reference the following use of digamma function for an estimation. Also, let us use an asterisk to denote estimation for the following parameters, namely \\(\\Lambda_k^*\\) and \\(\\pi_k^*\\). We reference Bishop C.M., pp.475-479 (2006) for the expectation formulas. The hyperparameters, namely \\(\\omega_1, \\alpha_1, \\beta_1, \\nu_1, \\Sigma_1\\), are available after calculating the other factors. \\[\\begin{align} \\log_e\\ \\Lambda_k^* \\equiv \\mathbb{E}_{\\Lambda}[\\log_e\\ |\\Lambda_k|] {}&amp;= p\\log_e2 + \\log_e|\\Sigma_1| + \\sum_{i=1}^p\\Psi\\left(\\frac{\\nu_1 - i + 1}{2}\\right)\\\\ \\log_e\\ \\pi_k^* \\equiv \\mathbb{E}_{\\pi}[\\log_e\\ \\pi_k] &amp;= \\Psi(\\omega_1) - \\Psi\\left( \\sum_{k} \\omega_1 \\right)\\\\ \\mathbb{E}_{\\mu,\\Lambda}\\left[(x_i - \\mu_k)^T\\Lambda_k(x_i - \\mu_k)\\right] &amp;= p\\beta_1^{-1} + \\nu_1(x_i - \\alpha_1)^T\\Sigma_1(x_i - \\alpha_1)\\\\ \\nonumber \\\\ \\text{where } \\Psi(.)\\text{ is the } &amp;\\text{digamma function.} \\nonumber \\end{align}\\] Therefore, given the expectations, we formulate the equation below for the responsibilities: \\[\\begin{align} \\log_e\\ \\rho_{ik} =\\frac{1}{2}\\log_e \\Lambda_k^* - \\frac{p}{2}\\log_e (2\\pi) - \\frac{1}{2}\\left[p\\beta_1^{-1} + \\nu_1(x_i - \\alpha_1)^T\\Sigma_1(x_i - \\alpha_1)\\right] + \\log_e \\pi_k^* \\end{align}\\] \\[\\begin{align} \\rho_{ik} \\propto \\pi_k^*|\\Lambda_k^*|^{\\frac{1}{2}} exp\\left(-\\frac{1}{2}\\biggl[ p\\beta_1^{-1} + \\nu_1(x_i - \\alpha_1)^T\\Sigma_1(x_i - \\alpha_1)\\biggr]\\right) \\end{align}\\] \\[\\begin{align} \\mathbb{E}_{Q_{y}}\\left[y_{ik} \\right] = \\tau_{ik} = \\frac{\\rho_{ik}}{\\sum_{j=1}^K \\rho_{ij}} \\ \\ \\ \\ \\ \\ \\ and \\ \\ \\ \\ \\ \\ \\ \\ \\tau_k = \\sum_{i=1}^N \\tau_{ik} \\end{align}\\] \\[\\begin{align} \\mathcal{Q}^*_y(y) &amp;= \\text{Multi}( {{y_{ik}}; \\tau_{ik}}) = \\prod_{i=1}^N \\prod_{k=1}^K {\\tau_{ik}}^{y_{ik}} \\end{align}\\] Update Cluster Statistics: The following statistics depend on the \\(\\tau_{ik}\\) as one-hot encoding for classification (clustering) and are structured as so: \\[ \\tau_{ik} = \\left[ \\begin{array}{cccccccc} 1 &amp; 1 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; ... &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; ... &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 1 &amp; ... &amp; 0 \\\\ \\end{array} \\right]_{kxn}^T\\ \\ \\ \\ \\ \\ \\ \\ where\\ k = 3 \\] \\[\\begin{align} \\tau_k &amp;= \\sum_{i=1}^n \\tau_{ik} = \\left[\\begin{array}{c} n_1 \\\\ n_2 \\\\ n_3 \\end{array}\\right]_k^T \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{l} \\text{ where } n_i \\text{ is number of observations per cluster - }\\\\ \\text{ the responsibilities}\\\\ \\end{array} \\label{eqn:eqnnumber322} \\\\ \\bar{x}_k &amp;= \\frac{1}{\\tau_k} \\sum_{i=1}^N \\left[ (\\tau_{ik}) x_i\\right] \\label{eqn:eqnnumber323}\\\\ S_k &amp;= \\frac{1}{\\tau_{k}} \\sum_{i=1}^n (\\tau_{ik}) (x_i - \\bar{x}_k)(x_i - \\bar{x}_k)^T. \\label{eqn:eqnnumber324} \\end{align}\\] Variational Maximization Step (VM-Step): In this step, we optimize the parameters and calculate \\(\\mathcal{Q}^*(\\pi)\\) and \\(\\mathcal{Q}^*(\\mu,\\Lambda)\\). \\[\\begin{align} \\mathcal{Q}_\\pi^*(\\pi_{1:k}; \\omega_1) {}&amp;= \\text{Dir}\\left(\\pi_{1:k}| \\omega_1 \\right)\\\\ \\nonumber \\\\ \\omega_1 &amp;= \\tau_{k} + \\omega_0\\\\ \\nonumber \\\\ \\mathcal{Q}^*_{\\mu,\\Lambda}(\\mu, \\Lambda) &amp;= \\prod_{k=1}^K \\mathcal{N}_p(\\mu_k; \\alpha_1, (\\beta_1\\Lambda_k)^{-1})\\times \\mathcal{W}_p(\\Lambda_k; \\nu_1, \\Sigma_1) \\\\ \\nonumber \\\\ \\alpha_1 &amp;= \\frac{(\\alpha_0\\beta_0 + \\tau_{k}\\bar{x}_k)}{(\\beta_0 + \\tau_{k})}\\\\ \\beta_1 &amp;= \\beta_0 + \\tau_{k} \\\\ \\nu_1 {}&amp;= \\nu_0 + \\tau_{k} \\\\ \\Sigma_1 &amp;= \\frac{ \\tau_k \\beta_0}{(\\beta_0 + \\tau_{k})}(\\bar{x}_k - \\alpha_0)( \\bar{x}_k - \\alpha_0 )^T + \\tau_{k} S_k + \\Sigma_0^{-1} \\end{align}\\] From here, we perform iteration until ELBO converges. Sixth, we need to calculate ELBO eq. 2 for convergence. Recall log marginal likelihood, namely \\(\\log_e P(X)\\), which we use for convergence as illustrated in the previous section for EM. Here, we use the derived ELBO eq. 2 to evaluate convergence. \\[\\begin{align} \\mathcal{LB}[Q(z)] {}&amp;= \\int_z \\mathcal{Q}(z)\\log_e P(x,z)\\ dz\\ -\\int_z \\mathcal{Q}(z)\\log_e \\mathcal{Q}(z) \\ dz\\\\ &amp;= \\mathbb{E}_{Q(z)}\\left[\\log_e P(x,z)\\right] + \\mathcal{H}(z) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(ELBO eq. 2)}\\\\ &amp;= \\mathbb{E}_{Q(z)}\\left[\\log_e P(x,z)\\right] - \\mathbb{E}_{Q(z)}\\left[\\log_e \\mathcal{Q}(z)\\right]\\\\ &amp;= \\mathbb{E}_{Q(z)}\\left[\\log_e P(x,y,\\pi,\\mu,\\Lambda)\\right] - \\mathbb{E}_{Q(z)}\\left[\\log_e \\mathcal{Q}(y,\\pi,\\mu,\\Lambda)\\right]\\\\ &amp;= \\mathbb{E}\\left[\\log_e P(x|y,\\mu,\\Lambda)\\right] + \\mathbb{E}\\left[\\log_e P(y,\\pi)\\right] + \\mathbb{E}\\left[\\log_e P(\\pi)\\right] + \\mathbb{E}\\left[\\log_e P(\\mu,\\Lambda)\\right] \\nonumber \\\\ &amp;\\ \\ \\ \\ \\ - \\mathbb{E}\\left[\\log_e \\mathcal{Q}(y)\\right] - \\mathbb{E}\\left[\\log_e \\mathcal{Q}(\\pi)\\right] - \\mathbb{E}\\left[\\log_e \\mathcal{Q}(\\mu,\\Lambda)\\right] \\end{align}\\] We then have to calculate the individual expectations. We leave readers to derive the individual terms of the lower bound as exercise. For reference, see Bishop C.M., section 10.2.2 (2006). Let us go through the process with an example implementation of Variational Bayes in R code using the Variational EM algorithm. First, let us generate our dataset (bivariate trimodal) like so: K = 3 # number of clusters (tri-modal) P = 2 # number of random variables (p-variate) N = 60 # number of observations per random variable ksample &lt;- function(m, mu, sd, seed) { set.seed(seed); rnorm(n=m, mean=mu, sd=sd) } dataset &lt;- function() { # simulate bivariate tri-modal mixture model (cluster: A, B, C) #set.seed(2020) mu = c(10, 20, 30) # assume true mean of all three clusters sd = c(2.0, 1.5, 1.5) # assume true std dev of all three clusters m = 20 A.x1 = ksample(m, mu[1], sd[1], 150 ) A.x2 = ksample(m, mu[1], sd[1], 180 ) B.x1 = ksample(m, mu[2], sd[2], 160 ) B.x2 = ksample(m, mu[2], sd[2], 190 ) C.x1 = ksample(m, mu[3], sd[3], 170 ) C.x2 = ksample(m, mu[3], sd[3], 200 ) x1 = c(A.x1 , B.x1 , C.x1) x2 = c(A.x2 , B.x2 , C.x2) x = cbind(x1, x2) list(&quot;m&quot; = m, &quot;x&quot; = x ) } data = dataset() plot(NULL, xlim=range(0,40), ylim=range(0,40), xlab=&quot;x1&quot;, ylab=&quot;x2&quot;, main=&quot;Bivariate tri-modal mixture model&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(data$x[,1], data$x[,2], col=c(&quot;black&quot;), pch=16) Figure 7.21: Bivariate tri-modal mixture model Figure 7.21 shows the data points that are intentionally colored with black and distributed in three clusters with centroid points (10,10), (20,20), and (30,30). The goal is to see if we can correctly identify the cluster that each data point belongs to by assigning colors to each. Second, let us perform initialization. initialize &lt;- function(data) { omega = rep(1/K, K) # dirichlet hyper-parameter alpha = kmeans(data$x, K)$centers # mean approximation using k-means beta = rep(1, K) # variance hyper-parameter v = rep(P,K) # degrees of freedom hyperparameter sigma = array(0, c(P,P,K)) sigma[,,] = rep(diag(P),K) # covariance hyperparameter loge_pi = rep(0, K) loge_lambda = rep(0, K) E_mulambda = matrix(0, N, K) list( &quot;omega&quot; = omega, &quot;alpha&quot;= alpha, &quot;beta&quot; = beta, &quot;v&quot; = v, &quot;sigma&quot; = sigma, &quot;loge_pi&quot;=loge_pi, &quot;loge_lambda&quot; = loge_lambda, &quot;E_mulambda&quot; = E_mulambda) } params = initialize(data) Third, let us implement variational EM. Note that the VEM implementation references an R code published in public by Jean Arreola (2018). We made a few re-arrangement and slight modifications to reflect corresponding notations in the discussion above. Additionally, our implementation is motivated by an R code from Fabian Dablander (2018)). As always, our implementation and the referenced implementations are examples only that should be used only to supplement our understanding, and thus they may not be production-proof): ln &lt;- function(x) { log(x, exp(1))} # exp(1) = 2.718282 one_hot_encoding &lt;- function(n) { # using log-sum-exp t( apply(n, 1, function(x) { offset = max(x); y = x - offset return ( exp(y)/ sum(exp(y) )) }) ) } update_xmean &lt;- function(x, rik, rk) { xm = matrix(0, K, P) for (k in 1:K) { xm[k,] = colSums(rik[,k] * x / rk[k]) } xm } update_S &lt;- function(x, rik, rk, xm) { # Update covariance S = array(0, c(P,P,K)) for (k in 1:K) { sum_sk = 0 for (i in 1:N) { sum_sk = sum_sk + rik[i,k] * (x[i,] - xm[k,]) %*% t(x[i,] - xm[k,]) } S[,,k] = sum_sk / rk[k] } S } VEM &lt;- function(x, params) { # consider k = 3 ( tri-modal ) omega.0 = 1/K # mixing weight hyperparameter alpha.0 = rep(0, P) # mean hyperparameter beta.0 = 1 # variance hyperparameter v.0 = P # degrees of freedom hyperparameter sigma.0 = diag(P) # lambda - covariance hyperparameter # hyperparameters omega = params$omega alpha = params$alpha beta = params$beta v = params$v sigma = params$sigma # expectation estimations loge_pi = params$loge_pi loge_lambda = params$loge_lambda E_mulambda = params$E_mulambda pik = matrix(0, N, K) ############### Variational E-Step ########################### for (k in 1:K) { loge_lambda[k] = 0 for (i in 1:P) { loge_lambda[k] = loge_lambda[k] + digamma( (v[k] - i + 1) / 2) } loge_lambda[k] = P * ln(2) + ln(det(sigma[,,k])) + loge_lambda[k] loge_pi[k] = digamma(omega[k]) - digamma(sum(omega)) for (i in 1:N) { E_mulambda[i,k] = (P / beta[k]) + v[k] * t(x[i,] - alpha[k,]) %*% sigma[,,k] %*% (x[i,] - alpha[k,]) pik[i,k] = loge_pi[k] + 0.5 * loge_lambda[k] - 0.5 * E_mulambda[i,k] - (P/2) * ln(2*pi) } } ############### Update Cluster Statistics ##################### rik = one_hot_encoding(pik) rk = apply(rik, 2, sum) # capture no of obsv per cluster xm = update_xmean(x, rik, rk) S = update_S(x, rik, rk, xm) ############### Variational M-Step ########################### # Update hyperparameters for (k in 1:K) { # beta1 beta[k] = beta.0 + rk[k] # alpha1 alpha[k,] = (alpha.0 * beta.0 + rk[k] * xm[k,]) / beta[k] # v1 v[k] = v.0 + rk[k] # sigma1 sigma[,,k] = ((beta.0 * rk[k]) / beta[k ]) * (xm[k,] - alpha.0) %*% t(xm[k,] - alpha.0) + + rk[k] * S[,,k] + solve(sigma.0) } # hyperparameters params$omega = omega params$alpha = alpha params$beta = beta params$v = v params$sigma = sigma # expectation estimations params$loge_pi = loge_pi params$loge_lambda = loge_lambda params$E_mulambda = E_mulambda list( &quot;params&quot; = params, &quot;rik&quot;=rik, &quot;rk&quot;=rk, &quot;S&quot; = S, &quot;xm&quot; = xm) } Finally, we step through the iteration until ELBO converges. tol = 1e-5 limit = 100 old_elbo = 0 params = initialize(data) for (iterate in 1:limit) { vem = VEM(data$x, params) elbo = with(vem, ELBO(params, rik, rk, S, xm)) if (!is.nan(elbo)) { err = abs(elbo - old_elbo) if (err &lt; tol) break } old_elbo = elbo params = vem$params } print(paste0(&quot;Number of Iterations : &quot;, iterate)) ## [1] &quot;Number of Iterations : 3&quot; And now we plot the data points with the assigned colors (see Figure 7.22). color = apply(vem$rik, 1, which.max) + 1 plot(NULL, xlim=range(0,40), ylim=range(0,40), xlab=&quot;x1&quot;, ylab=&quot;x2&quot;, main=&quot;Bivariate tri-modal mixture model&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(data$x[,1], data$x[,2], col=color, pch=16) Figure 7.22: Approximate Bivariate tri-modal mixture model "],["bayesian2.html", "Chapter 8 Bayesian Computation II 8.1 Bayesian Models 8.2 Simulation and Sampling 8.3 Bayesian Analysis 8.4 Summary", " Chapter 8 Bayesian Computation II In this chapter, we not only try to approximate the function that can closely represent the actual distribution of data, but we also try to simulate the sampling of data that can represent the actual distribution. Thus, we narrow it down to a sampling that can ultimately yield an approximate function for the simulated distribution. That is where we emphasize Bayesian Modeling. Some concepts in this chapter, for example, Factor Graphs and Kalman Filters, are helpful in Robotics and Signal Processing (Dellaert F. and Kaess M. 2017; Dellaert F. 2021; Marco Cox M. 2018). It is therefore essential to get some fundamental intuition around such concepts. 8.1 Bayesian Models We begin this chapter by introducing Bayesian Models and some concepts around Dynamic Systems, knowing that such systems tend to hold properties of uncertainty. Here, we briefly introduce three models borrowed from Graph Theory that can represent Bayesian models in three ways: Factor Graph, Undirected Graph, and Directed Graph (Pernkopf F. et al. 2014). Factor Graph and Bipartite Graph: Figure 8.1 illustrates two Bipartite graphs (or a bigraph). The left-side graph has the more common form of a Factor graph. The right-side graph has the more common form of a Bipartite graph. Figure 8.1: Factor Graph and Bipartite Graph A Factor graph is a probabilistic graphical representation of a joint distribution involving random variables and non-negative functions called Factors. A Factor graph consists of nodes and edges. There are two types of nodes: a variable node and a factor node. In Figure 8.1, we have fourteen nodes. There are seven variable nodes, namely {A,B,C,D,E,F,G,H}, and seven factor nodes, namely {\\(f_a,f_b,f_c,f_d,f_e,f_f,f_g,f_h\\)}. The Factor graph in the figure is expressed as so: \\[ g(a,b,c,d,e,f,g,h) = f_a(a,e)f_b(b,e)f_c(c,f)f_d(d,f)f_e(e,f)f_f(e,g)f_g(f,g)f_h(g,h) \\] A Bipartite graph is a graph whose vertices can be divided into two independent (disjoint) sets, U and V, such that each edge connects one U vertex and one V vertex (Dey A., n.d.). The joint probability is factorized into a product of seven factors. In the Belief Propagation section, we discuss marginalization which uses Factor graph as a graphical model. Undirected Graph: Figure 8.2 shows an undirected graph and directed graph. Figure 8.2: Undirected and Directed Graph Markov Network depicts a structured Undirected Graph similar to Factor Graph in that it also describes joint distribution but focused more on cliques than on factors. A clique describes a fully connected sub-graph. Markov Random Field (MRF) is one type of a Markov Network. Notice also in the figure that a so-called Markov Blanket covers colored nodes. Given a root - in our case, we use node E - the Markov Blanket of node E for Undirected Graph includes the neighboring nodes of the root. As for the Directed Graph, the Markov Blanket includes the direct parents, direct children, and direct children’s parents. Directed Graph: Bayesian Network depicts a structured Directed Acyclic Graph (DAG) similar to Figure 7.18 that offers a Probabilistic Graphical modeling (PGM) to guide in forming dependencies, eliminating extraneous conditional probabilities. On the other hand, Markov Chain depicts a structured Directed Graph used to visually represent more complex structural relationships amongst variables. We discuss Markov Chain in detail two subsections ahead. One of the popular cross-platform packages to graphically render any graph is called TikZ, which interprets Tex language. Figure 8.3 is an example of a Directed graph rendered using TikZ. Figure 8.3: Directed Graph Here is an example of Tex commands used to generate the three-state graph as shown in Figure 8.3: \\begin{tikzpicture} \\begin{scope}[every node/.style={circle,thick}] \\node[draw] (A) at (4,6) {A}; \\node[draw] (B) at (2,2) {B}; \\node[draw] (C) at (6,2) {C}; \\node[rectangle] (R) at (4,0) {Three-State Graph}; \\end{scope} \\begin{scope}[every node/.style={fill=white,circle}, every edge/.style={draw=red,very thick,bend right=20}] \\path [-&gt;] (A) edge[very thick] node {0.10} (B); \\path [-&gt;] (A) edge[very thick] node {0.90} (C); \\path [-&gt;] (B) edge[very thick] node {0.60} (A); \\path [-&gt;] (B) edge[very thick] node {0.40} (C); \\path [-&gt;] (C) edge[very thick] node {0.23} (A); \\path [-&gt;] (C) edge node {0.77} (B); \\end{scope} ... And here is an example of Tex commands used to generate the 3x3 matrix or table: ... \\begin{scope}[ cell/.style={rectangle,draw=black}, space/.style={ matrix of nodes, minimum height = 1.5em, minimum width = 2em, row sep = -\\pgflinewidth, column sep = -\\pgflinewidth, column/.style = {font=\\ttfamily}, text width = 2em, align=left, align=right }, text depth = 0.5ex, text height = 2ex, nodes in empty cells] \\node (B) [right=of R, text width=4cm] at (6.5,0) {Bayesian Matrix}; \\matrix (second) [above=1cm of B, space, row 1/.style={ nodes={draw=gray, fill=gray!50!white,text centered}}, column 1/.style={ nodes={draw=gray, fill=gray!50!white,text centered}}, column 2/.style={nodes={draw=black}}, column 3/.style={nodes={draw=black}}, column 4/.style={nodes={draw=black}}, column 5/.style={nodes={align=right}} ] { &amp; A &amp; B &amp; C &amp; $\\sum$\\\\ A &amp; &amp; 0.10 &amp; 0.90 &amp; 1\\\\ B &amp; 0.60 &amp; &amp; 0.40 &amp; 1\\\\ C &amp; 0.23 &amp; 0.77 &amp; &amp; 1\\\\ }; \\end{scope} \\end{tikzpicture} In a few sections ahead, we show a few examples of Directed Graphs generated by TikZ to demonstrate Markov Chain graphs. See Figure 8.9. However, it pays to review a few preliminaries first while introducing Belief Propagation and Expectation Propagation. 8.1.1 Belief Propagation Belief Propagation is a message passing sum-product algorithm for marginalization, also called variable elimination. To illustrate Belief Propagation, let us review a few preliminaries, starting with Marginalization. This section and the next section reference the works of Thomas P. Minka (2001b, 2001a). Marginalization We begin the discussion of Belief Propagation with joint probability distribution like so: \\[ P(x_1, x_2, x_3,...,x_n) \\] Marginalization sums all random variables in a joint probability distribution except the one to be marginalized, so that if we are to marginalize \\(x_1\\), given n variables, then we have the following: \\[\\begin{align} P(x_1) = P(x_1)\\left[ \\sum_{x_2}\\sum_{x_3} ... \\sum_{x_n} P_{x_2,...,x_n}(x_2, x_3,...,x_n) \\right] \\end{align}\\] \\[ where\\ \\ \\ \\ \\sum_{x_2}\\sum_{x_3} ... \\sum_{x_n} P_{x_2,...,x_n}(x_2, x_3,...,x_n) = 1 \\] In general, we can write the equation like so: \\[\\begin{align} P(x_k) = \\sum_{-x_k}\\mathcal{Q}_{-x_k}(x_1,...,x_n) \\end{align}\\] This naive way of marginalization can get computationally costly, especially with a larger set of variables. We discuss two algorithms to solve this in the sections ahead. Message Passing Message passing is an algorithm that allows messages to pass from one type of node to a different type of node, e.g., passing a message from a variable node to a factor node or passing a message from a factor node to a variable node. Let us use Figure 8.4 to discuss message passing. Figure 8.4: Message Passing The factor graph in Figure 8.4 represents a joint probability distribution with factors. \\[ P(A,B,C,D,E,F,G) = f_1(A,B,C) f_2(D,E,F) f_3(C,G) f_4(F,G) f_5(G) \\] If we treat the edges as channels for which messages pass through, then for marginalization, one can arbitrarily choose a variable node as root to marginalize and let factor nodes allow the flow of messages toward the root. In our case, we choose G as our root; therefore, messages flow starting from the leaf nodes, namely variable leaf nodes {A, B, D, E} and factor leaf node {\\(f_5\\)}. Notice the existence of factor \\(f_a\\), which we ignore for now (greyed out in the figure) as this creates a loop. That is also called loopy belief propagation which tends to prevent convergence. We introduce Junction tree afterward as a possible solution for loopy graphs. In terms of message passing, let us use the following generic notation: \\[ m_i = \\mu_{{&lt;source\\ node&gt;}\\rightarrow{&lt;dest\\ node&gt;}}(&lt;referenced\\ node&gt;) \\] Here are a few examples based on the figure: \\[\\begin{align*} m_1 {}&amp;= \\mu_{A \\rightarrow f_1}(A)\\ \\ \\ \\text{(message from variable node A to factor node }f_1 \\text{)}\\\\ m_3 &amp;= \\mu_{f_1 \\rightarrow C}(C)\\ \\ \\ \\text{(message from factor node }f_1 \\text{ to variable node }C \\text{)}\\\\ \\end{align*}\\] A message from a leaf node produces the following: \\[\\begin{align*} {}&amp;m_1 = \\mu_{A \\rightarrow f_1}(A) = 1 &amp; \\text{(variable leaf node)}\\\\ &amp;m_2 = \\mu_{B \\rightarrow f_1}(B) = 1 &amp; \\text{(variable leaf node)}\\\\ &amp;m_4 = \\mu_{D \\rightarrow f_2}(D) = 1 &amp; \\text{(variable leaf node)}\\\\ &amp;m_5 = \\mu_{E \\rightarrow f_2}(E) = 1 &amp; \\text{(variable leaf node)}\\\\ &amp;m_{11} = \\mu_{f_5 \\rightarrow G}(G) = f_5(G) &amp; \\text{(factor leaf node)}\\\\ \\end{align*}\\] where we assume uniform priors that produce an initial value of 1 for the above variable leaf nodes. Here, let us discuss two message passing algorithms: Sum-Product algorithm An example of this algorithm is Belief Propagation. Viterbi in HMM section follows a similar algorithm. We calculate a random marginal probability, e.g., \\(P(x_i)\\), by message passing starting from the leaf nodes. In other words, we marginalize out other variables except for variable \\(\\mathbf{x_i}\\) - so-called variable elimination. In general, a message from factor node to variable node is expressed as: \\[\\begin{align} \\mu_{f_j \\rightarrow x_i}(x_i) = \\sum_{x_n \\in Ne(f_j)\\backslash x_i}^N f_j(x_n) \\prod_{x_m \\in Ne(f_j)\\backslash x_i}^N \\mu_{x_m \\rightarrow f_j}(X_m) \\end{align}\\] where (\\(x_n \\in Ne(f_j)\\)\\ \\(x_i\\)) denotes a set of neighboring (Ne) variable nodes connecting to factor nodes \\(f_j\\) excluding \\(x_i\\). And a message from variable node to factor node is expressed as: \\[\\begin{align} \\mu_{x_i \\rightarrow f_j}(x_i) = \\prod_{f_n \\in Ne(x_i)\\backslash f_j}^N \\mu_{f_n \\rightarrow x_i}(x_i) \\end{align}\\] where (\\(f \\in Ne(x_i)\\)\\ \\(f_j\\)) denotes a set of neighboring (Ne) factor nodes connecting to variable nodes \\(x_i\\) excluding \\(f_j\\). To illustrate, we can get the marginal \\(P(G)\\) this way in \\(\\mathcal{O}(N^7)\\) complexity: \\[\\begin{align} P(G) = \\sum_{F}\\sum_{C}\\sum_{E}\\sum_{D} \\sum_{B} \\sum_{A} f_5(G) f_4(F,G)f_3(C,G) f_2(D,E,F) f_1(A,B,C) \\end{align}\\] But with message passing sum-product (belief propagation) algorithm, we can reduce the complexity even down to \\(\\mathcal{O}(N^2)\\) by pushing in the summations. \\[\\begin{align} m_1 &amp;= \\mu_{A \\rightarrow f_1}(A) = 1\\\\ m_2 &amp;= \\mu_{B \\rightarrow f_1}(B) = 1\\\\ m_4 &amp;= \\mu_{D \\rightarrow f_2}(D) = 1\\\\ m_5 &amp;= \\mu_{E \\rightarrow f_2}(E) = 1\\\\ m_3 &amp;= \\mu_{f_1 \\rightarrow C}(C) = \\sum_A \\sum_B f_1(A,B,C) \\\\ m_6 &amp;= \\mu_{f_2 \\rightarrow F}(F) = \\sum_D \\sum_E f_2(D,E,F) \\\\ m_7 &amp;= \\mu_{C \\rightarrow f_3}(C) = \\sum_C f_3(C,G)\\\\ m_8 &amp;= \\mu_{F \\rightarrow f_4}(F) = \\sum_F f_4(F,G) \\\\ m_9 &amp;= \\mu_{f_3 \\rightarrow G}(G) = \\sum_C f_3(C,G) \\sum_A \\sum_B f_1(A,B,C)\\\\ m_{10} &amp;= \\mu_{f_4 \\rightarrow G}(G) = \\sum_F f_4(F,G) \\sum_D \\sum_E f_2(D,E,F)\\\\ m_{11} &amp;= \\mu_{f_5 \\rightarrow G}(G) = f_5(G)\\\\ \\nonumber \\\\ P(G) &amp;= \\underbrace{f_5(G)}_{\\mu_{f_5 \\rightarrow G}(G)} \\cdot \\underbrace{ \\underbrace{ \\sum_F f_4(F,G) }_{m_8 = \\mu_{F \\rightarrow f_4}(F)}\\cdot \\underbrace{ \\sum_D \\sum_E f_2(D,E,F) }_{m_6 = \\mu_{f_2 \\rightarrow F}(F)}\\cdot }_{m_{10} = \\mu_{f_4 \\rightarrow G}(G)} \\underbrace{ \\underbrace{ \\sum_C f_3(C,G) }_{m_7 = \\mu_{C \\rightarrow f_3}(C)}\\cdot \\underbrace{ \\sum_A \\sum_B f_1(A,B,C) }_{m_3 = \\mu_{f_1 \\rightarrow C}(C)} }_{m_9 = \\mu_{f_3 \\rightarrow G}(G)} \\end{align}\\] Max-Product algorithm An example of a Max-Product algorithm is the Baum-Welch algorithm in HMM section. Similarly, the summation is replaced with maximization. \\[\\begin{align} P(G&amp;) = \\nonumber\\\\ &amp; \\underbrace{f_5(G)}_{\\mu_{f_5 \\rightarrow G}(G)} \\cdot \\underbrace{ \\underbrace{ \\max_F f_4(F,G) }_{m_8 = \\mu_{F \\rightarrow f_4}(F)}\\cdot \\underbrace{ \\max_D \\max_E f_2(D,E,F) }_{m_6 = \\mu_{f_2 \\rightarrow F}(F)}\\cdot }_{m_{10} = \\mu_{f_4 \\rightarrow G}(G)} \\underbrace{ \\underbrace{ \\max_C f_3(C,G) }_{m_7 = \\mu_{C \\rightarrow f_3}(C)}\\cdot \\underbrace{ \\max_A \\max_B f_1(A,B,C) }_{m_3 = \\mu_{f_1 \\rightarrow C}(C)} }_{m_9 = \\mu_{f_3 \\rightarrow G}(G)} \\end{align}\\] In cases where we observe loops in our graph, we can still use the belief propagation algorithm; however, convergence is not guaranteed. Therefore, let us see how the Junction Tree algorithm may help in place of loopy belief propagation. Junction Tree Algorithm Junction Tree, also called Clique Tree, algorithm decomposes a directed graph into maximal subgraphs that are consequently referenced to form a tree of cliques, localizing computation within tree nodes. The algorithm composes of the following significant steps: Moralization adds extra edges that connect parents sharing a child node while at the same time transforming a directed graph into an undirected graph. Triangulation is the process of adding Chords, effectively forming subgraphs called cliques. Chords are edges that connect non-adjacent nodes. Here, it helps to be aware of the Tarjan elimination algorithm. Clique Factorization involves identifying Cliques. A Clique is a subgraph in which every two distinct nodes in the set are adjacent (or connected by a distinct edge). Tree Construction - here, we leave readers to investigate the Kruskal or Prims algorithm used to construct junction trees, taking into account junction tree properties such as running intersections, preservation of local and global consistency such that neighboring cliques share a common marginal when other variables are summed out, and so on. See Figure 8.5 for the transformation of a directed graph into a clique tree. Node h has two parents, namely nodes g and e. So we connect the two parents by moralization. Afterwhich, we can triangulate the graph and arrive at six cliques, namely {ADB, CBE, DBE, EGD, FGD, EGH} with five separators {DB, BE, ED, GD, EG}. Figure 8.5: Moralized and Triangulated Graph Even with the triangulated graph, we can use, for example, subgraph &lt;b,d,e,g&gt; to formulate an equation for its joint probability, in which the intersection \\(\\omega(e,d)\\) is a normalizer so that: \\[\\begin{align} P(b,d,e,g) = \\frac{\\gamma(d,b,e)\\gamma(e,g,d)}{Z} = \\frac{\\gamma(d,b,e)\\gamma(e,g,d)}{\\omega(e,d)} \\end{align}\\] But for all purposes, our goal is to marginalize by the sum-product algorithm using the constructed Junction Tree. We perform variable elimination by starting from the tree leaves and using message passing to arrive at the root. For message passing in the Junction tree form, below is a marginal sub-computation for a particular clique, namely \\(\\gamma(e,g,h)\\), granting messages pass through \\(\\omega(e,g)\\) towards the clique, assuming the following: \\[\\begin{align} c1 = \\gamma(e,g,d)\\ \\ \\ \\ \\ \\ \\ c2 = \\gamma(e,g,h)\\ \\ \\ \\ \\ \\ \\mu_{c_1 \\rightarrow c_2}(e,g) \\end{align}\\] then we get: \\[\\begin{align} P(e,g,h) {}&amp;\\propto \\gamma(e,g,h)\\times \\mu_{c_1 \\rightarrow c_2}(e,g)\\\\ &amp;\\propto \\gamma(e,g,h)\\times \\omega(e,g) \\end{align}\\] If messages flow to \\(c1\\) from \\(c2\\) and \\(c3\\) and exit towards \\(\\gamma(d,b,e)\\): \\[\\begin{align} c1 = \\gamma(e,g,d)\\ \\ \\ \\ \\ \\ c2 = \\gamma(e,g,h)\\ \\ \\ \\ \\ c3 = \\gamma(f,g,d)\\ \\ \\ \\ \\ \\mu_{c_2 \\rightarrow c_1}(e,g)\\ \\ \\ \\ \\ \\mu_{c_3 \\rightarrow c_1}(g,d) \\end{align}\\] then for clique c1 (focusing exclusively on the listed sub-graphs for illustration), we get: \\[\\begin{align} P(e,g,h) {}&amp;\\propto \\gamma(e,g,h)\\times\\mu_{c_2 \\rightarrow c1}(e,g)\\times\\mu_{c_3 \\rightarrow c1}(g,d)\\\\ &amp;\\propto \\gamma(e,g,h)\\times\\omega(e,g)\\times\\omega(g,d) \\end{align}\\] For other message-passing algorithms and approximations, we leave readers to investigate the following: Shafer Shenoy algorithm Hugin algorithm Bethe approximation Kikuchi approximation. 8.1.2 Expectation Propagation In this section, we start the discussion of Expectation Propagation (EP) to solve a clutter problem in which we deal with a p-dimensional Gaussian distribution with noise and unknown clutter. Figure 8.6: Factor Graph For A Clutter Problem The factor graph in Figure 8.6 illustrates a joint distribution that translates into the following: \\[\\begin{align} \\underbrace{P(\\theta, x)}_\\text{joint}\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ \\underbrace{P(\\theta|x_1,...,x_n)}_\\text{posterior} \\propto \\underbrace{P(\\theta)}_\\text{prior} \\cdot \\underbrace{\\prod_{i=1}^n P(x_i|\\theta)}_\\text{likelihood} \\end{align}\\] where gaussian expression for prior corresponds to: \\[\\begin{align} P(\\theta) \\rightarrow \\ \\ \\ \\theta \\sim \\mathcal{N}( 0, \\nu_1\\mathbf{I_p}) \\end{align}\\] and gaussian expression for likelihood corresponding to: \\[\\begin{align} P(x|\\theta)\\ \\rightarrow\\ \\ \\ \\ \\ x_i|\\theta \\sim (1 - \\omega_i)\\mathcal{N}(\\theta, \\mathbf{I}) + (\\omega_i)\\mathcal{N}(0, \\nu_2\\mathbf{I}) \\end{align}\\] The likelihood equation suggests a Gaussian distribution of two mixture components. The first being an observation following a normal distribution and the second component being the clutter. Here, theta \\(\\theta\\) acts as placeholder for latent variable with parameters (e.g. unkown mean and variance ) and X denotes observation. The notation \\(\\omega\\) represents proportionality of the clutter (a.l.a mixture coefficients in Gaussian mixture model) for which we can use Bernoulli distribution, e.g. \\(\\omega_i \\sim \\mathbf{B}ern(\\pi)\\). We initialize the following parameters, e.g. \\(\\nu_1 = 100\\), \\(\\nu_2 = 10\\). For inference, the use of Belief Propagation is discouraged due to the number of Gaussians involved in the computation, namely \\(2^n\\); rather, we can use the EP algorithm (Minka T. 2001b). In general, with the Clutter problem in mind, Expectation Propagation has the following algorithm. First, as often our case, the parameters of interest is based on the posterior distribution, namely \\(P(\\theta|x)\\). The posterior is in the form of a product of factors like so: \\[\\begin{align} P(x,\\theta) \\propto \\prod_{i=0}^N f_i(\\theta),\\ \\ \\ \\ \\ where: f_0(\\theta) = P(\\theta), \\ \\ \\ f_i(\\theta) = P(x_i|\\theta). \\end{align}\\] Second, then similar to VI, we introduce a corresponding approximating distribution for the posterior that, in the case of ADF, is also decomposed into a product of factors, each factor corresponding to our choice of a Gaussian exponential family, e.g., Spherical Gaussian is a convenient choice: \\[\\begin{align} \\mathcal{Q}(\\theta) = \\prod_{i=0}^N \\hat{f}_i(\\theta) = \\prod_{i=0}^N \\left[\\mathcal{N}(x_i; m_i, v_i\\mathbf{I_p}) \\cdot s_i \\right] \\end{align}\\] where each factor, namely \\(\\hat{f}_i\\), is approximated using the following example implementation of the Gaussian function in R code: N.Gauss &lt;- function(x, m, v) { exp(- 1/(2*v) * ( t(x - m) %*% v^(-1) %*% (x -m ) ) ) } and where parameters for the prior, \\(\\hat{f}_0\\), are initialized to the following: \\[ m_0 = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nu_0 = 100\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ s_0 = (2\\pi v_i)^{(-p/2)} \\] and parameters for the rest of the factors correspond to the following initialization: \\[ m_i = 0\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ v_i\\ \\rightarrow \\infty\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ s_i = 1\\ \\ \\ \\ \\ \\ \\text{(initialized to unity)} \\] such that the approximation distribution, namely \\(\\mathcal{Q}(\\theta)\\), therefore corresponds to the initialized prior. initialize &lt;- function(N, P = 1) { m = rep(0, N); v = rep(Inf, N); s = rep(1, N) v[1] = 100; s[1] = (2 * pi * v[1])^(-P/2) m.0 = m[1]; v.0 = s[1] list(&quot;m&quot; = m, &quot;v&quot; = v, &quot;s&quot; = s, &quot;m.0&quot; = m.0, &quot;v.0&quot; = v.0) } where \\(\\theta = (m_\\theta, v_\\theta)\\) so that initially, we have \\(m_\\theta = m_0\\) and \\(\\nu_\\theta = \\nu_0.\\) Third, we create a cavity distribution by arbitrarily excluding the kth likelihood approximation from \\(\\mathcal{Q}(\\theta)\\). We can show this using any of two variants: \\[\\begin{align} \\mathcal{Q}^{\\backslash k}(\\theta) {}&amp;\\propto \\frac{\\mathcal{Q}(\\theta)}{\\hat{f}_i(\\theta)} = \\frac{\\mathcal{N}(\\theta; m, \\nu \\mathbf{I})}{s_i\\mathcal{N}(\\theta; m_i, \\nu_i\\mathbf{I})} &amp; \\text{(ref M. Korvas 2013)}\\\\ \\mathcal{Q}^{\\backslash k}(\\theta) &amp;\\propto \\prod_{i\\ne k}^N \\hat{f}_i(\\theta) = \\prod_{i\\ne k}^N \\mathcal{N}(\\theta; m_i, \\nu_i) &amp; \\text{(ref O. Du}\\check{s}\\text{ek 2013)} \\end{align}\\] The approach is to calculate the moments like so: \\[\\begin{align} \\nu_{\\theta}^{\\backslash k} = (\\nu_{\\theta}^{\\backslash k*})^{-1} - \\nu_{i}^{-1})^{-1} \\ \\ \\ \\ \\ \\ \\ \\ \\ m_{\\theta}^{\\backslash k} = m_{\\theta}^{\\backslash k*} + \\nu_{\\theta}^{\\backslash k}\\nu_i^{-1}( m_{\\theta}^{\\backslash k*} - m_i ) \\end{align}\\] where \\(\\hat{f}_i(\\theta)\\) is represented by the following sufficient statistics, namely \\(\\{m_i, \\nu_i, s_i\\}\\). Fourth, we derive a refined distribution called tilted distribution, also called hybrid distribution, by taking the product of the actual likelihood and the cavity distribution - in our case, this is the product of Gaussian distributions. \\[\\begin{align} \\underbrace{ \\mathcal{Q}^*(\\theta) }_{ \\begin{array}{c}tilted\\\\distribution \\end{array} } = \\underbrace{\\mathcal{Q}^{\\backslash k}(\\theta) }_{ \\begin{array}{c}cavity\\\\distribution \\end{array} } \\cdot \\underbrace{f_i(\\theta)}_{ \\begin{array}{c}true\\\\likelihood \\end{array} } \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ Z_i = \\int_{i\\ne k} \\mathcal{Q}^*(\\theta) d \\theta \\label{eqn:eqnnumber325} \\end{align}\\] Equivalently, we calculate the projection (a new approximated distribution) by minimizing the KL divergence. \\[\\begin{align} \\mathcal{Q}^*(\\theta) = \\underset{q}{argmin}\\ \\mathcal{KL}( \\mathcal{Q}^{\\backslash k}(\\theta) \\cdot f_k(\\theta) ||\\mathcal{Q}^{\\backslash k}(\\theta) \\cdot \\hat{f}_k(\\theta) ) \\end{align}\\] Here, the approach is to calculate the sufficient statistics of the resulting distributions instead so that if the moments between \\(\\mathcal{Q}^*(\\theta)\\) and \\(\\mathcal{Q}(\\theta)\\) are the same, then minimizing KL divergence is equivalently achieved. This is also called moment matching. We calculate parameters \\((\\nu_{\\theta}^{\\backslash k*}, m_{\\theta}^{\\backslash k*}, Z_i )\\) from \\((m_{\\theta}^{\\backslash k}, \\nu_{\\theta}^{\\backslash k} )\\) as in ADF: \\[\\begin{align} Z_i {}&amp;= (1 - \\omega)\\mathcal{N}(y_i; m_{\\theta}^{\\backslash k}, (v_{\\theta}^{\\backslash k} + 1)\\mathbf{I}) + \\omega\\mathcal{N}(y_i; 0, 10\\mathbf{I})\\\\ \\tau_i &amp;= \\frac{1}{Z_i} (1 - \\omega)\\mathcal{N}(y_i; m_{\\theta}^{\\backslash k}, (v_{\\theta}^{\\backslash k} + 1)\\mathbf{I})\\\\ m_{\\theta}^{\\backslash k*} &amp;= m_{\\theta}^{\\backslash k} + v_{\\theta}^{\\backslash k} \\tau_i \\left( \\frac{ y_i - m_{\\theta}^{\\backslash k}}{v_{\\theta}^{\\backslash k} + 1} \\right) \\\\ \\nu_{\\theta}^{\\backslash k*} &amp;= \\nu_{\\theta}^{\\backslash k} - \\tau_i \\left( \\frac{(\\nu_{\\theta}^{\\backslash k})^2}{v_{\\theta}^{\\backslash k} + 1} \\right) + \\tau_i ( 1 - \\tau_i) \\left( \\frac{(v_{\\theta}^{\\backslash k})^2\\|y_i - m_{\\theta}^{\\backslash k} \\|^2}{p(v_{\\theta}^{\\backslash k}+ 1)^2} \\right) \\end{align}\\] Fifth, we update (refine) the chosen approximate factor, namely \\(\\hat{f}_i\\). \\[\\begin{align} \\hat{f}_{i}^*(\\theta) \\propto \\frac{ \\mathcal{Q}^*(\\theta) }{ \\mathcal{Q}^{\\backslash k}(\\theta)} \\end{align}\\] The approach is to update parameters of the chosen factor: \\[\\begin{align} v_i {}&amp;= \\left((v_{\\theta}^{\\backslash k*})^{-1} - (v_{\\theta}^{\\backslash k})^{-1} \\right)^{-1}\\\\ m_i &amp;= m_\\theta^{\\backslash k} + (v_i + v_{\\theta}^{\\backslash k})(v_{\\theta}^{\\backslash k})^{-1}(m_\\theta^{\\backslash k*} - m_\\theta^{\\backslash k})\\\\ s_i &amp;= \\frac{Z_i}{(2\\pi v_i)^{(p/2)} \\mathcal{N}(m_i; m_\\theta^{\\backslash k}, (v_i + v_\\theta^{\\backslash k})I)} \\end{align}\\] Sixth, we then move on and choose the next factor and repeat the steps. Note that if all factors, e.g., their corresponding moments {\\(m_i, v_i, s_i\\)}, are processed with no convergence in sight, we iterate again, starting with the first chosen factor. Clutter Problem In summary, the idea is to refine an approximating distribution, \\(\\mathcal{Q}(\\theta)\\), by arbitrarily choosing and replacing one of its factors, \\(\\hat{f}_i(\\theta)\\), with its corresponding true factor, \\(f_i(\\theta)\\), from the true posterior. We then create (so-called projecting) a new tilted distribution, namely \\(\\mathcal{Q}^*(\\theta)\\). We compare the distribution with the original approximating distribution using KL divergence; however, we use moment matching to minimize KL divergence. Subsequentially, we continue to perform the replacement for all factors in the distribution. This one pass is reflected in the ADF algorithm and tends to fall short of accuracy. We solve this in EP by performing multiple passes until convergence. Figure 8.7 illustrates the difference between a partial Variational Inference result and a partial Expectation Propagation result in terms of coverage. For example, VI covers a specific modal (e.g., as shown in the figure, it matches the first mode), whereas EP tends to be global. Note that the second mode in the figure is clutter. Figure 8.7: Bimodal Mixture model With the parameter calculations presented above, we reference the EP algorithm from T. Minka’s thesis (p. 21, MIT 2001) for a direct (crude) naive implementation of the clutter problem in R below. Fundamentally, we show the EP as a multi-pass algorithm: y = pd$y N = length(y) Q = initialize(N, 1) P = 1 w = 0.5 limit = 100 for (iter in 1:limit) { # limiting iteration for illustration # the actual alternative is to compare moments for convergence # (e.g Q$m[i] == Q.old$m[i], Q$v[i] == Q.old$v[i],... ) for (i in 2:N) { # create cavity distribution (represented by moments) v.0 = ( Q$v.0^(-1) - Q$v[i]^(-1))^(-1) m.0 = Q$m.0 + v.0 * Q$v[i]^(-1) * ( Q$m.0 - Q$m[i]) # create tilted distribution # recompute (Q$m.0 , Q$v.0, Zi) from (m.0, v.0) - ADF ri = (1 - w) * N.Gauss(y[i], m.0, (v.0 + 1)) / ( (1 - w) * N.Gauss(y[i], m.0, (v.0 + 1)) + w * N.Gauss(y[i], 0, 10) ) Q$m.0 = m.0 + v.0 * ri * (y[i] - m.0) / (v.0 + 1) Q$v.0 = v.0 - ri * v.0^2 / (v.0 + 1) + ri * ( 1 - ri) * v.0^2 * (t (y[i] - m.0) %*% (y[i] - m.0) ) / P * (v.0 + 1)^2 Zi = (1 - w) * N.Gauss(y[i], m.0, (v.0 + 1)) + w * N.Gauss(y[i], 0, 10) # Update chosen factor Q$v[i] = ( Q$v.0^(-1) - v.0^(-1) )^(-1) Q$m[i] = m.0 + (Q$v[i] + v.0) * v.0^(-1) * ( Q$m.0 - m.0) Q$s[i] = Zi / ( 2* pi * Q$v[i]^(P/2) * N.Gauss(Q$m[i], m.0, (Q$v[i] + v.0))) } Q.old = Q } Now, as we can imagine, dealing with a more significant number of factors can invite scalability concerns. The goal is to factorize the approximating distribution so that each factor maps to a corresponding site for parallel computation. See Figure 8.8 derived from the idea of using a posterior server with multiple site workers (L. Hasenclever et al, 2017). Figure 8.8: Expectation Propagation In the paper by L. Hasenclever et al., a posterior server is utilized to aggregate individual tilted distributions into a global posterior approximation and, in turn, delivers back individual cavity moments to the corresponding sites for each iteration until convergence. A few methods are suggested (Gelman A. et al. 2017) to approximate the moments, such as Laplace Approximation, which we cover in a previous section, and Markov Chain Monte Carlo (MCMC), which we discuss under Simulation and Sampling section. In the next section, we discuss Markov Chain. Here, we reference an introductory material from Anders Tolver (2016). 8.1.3 Markov Chain Markov Chain, named after Andrey Markov (a Russian Mathematician), models a state sequence of random events that happen over time. The sequence is modeled based on a structured graph of all distinct countable states of a random event and their corresponding transition probabilities. Figure 8.9 shows six state diagrams, also called transition diagram, for a Markov Chain; the first model being the simplest. Each model is a system of nodes and edges in which a node represents a state and an edge represents a transition probability. For example, to interpret the fifth graph (six-state graph), the transition probability of jumping from state E to state C is 25%. Figure 8.9: Markov Chain Models Equivalently, we can also illustrate the models through a transition matrix, also called Markov matrix. The columns describe the next transition states. The rows describe a list of all distinct states (each row computes a total probability of one). Here, the values of each cell are called the transition probabilities. Figure 8.10: Markov Chain (Transition Matrix) A transition probability in a transition matrix is denoted as: \\[\\begin{align} (P^n)_{i,j} \\equiv P(X_{n} = S_j|X_{n-1} = S_i) \\end{align}\\] The notation reads as the probability of transitioning from i state to j state in the nth step. A simple example of using the Markov Chain is the famous Random Walk problem. Let us use the fifth model above in which the state-space contains all distinct countable states that a random event \\(X_t\\) can transition at any given time (t): \\[ S \\in \\{ A,B,C,D,E,F \\} \\] For a random walk (random event), \\(X\\), we start walking at time zero (t = 0). Such an event at time zero is denoted by \\(X_t = X_0\\). Suppose the initial state at time zero is \\(X_0 = A\\). Therefore, we can express the marginal probability of such a random event at time zero with an initial state of A like so: \\[ P(X_0 = A) = 1 \\] Now, if we start from time zero in state A and decide to walk to state D for some random choice, we then have the following expression: \\[ P(X_1 = D | X_0 = A) = 0.85 \\] That is because the probability of walking from state A to D is 85%. From state D, if we further randomly walk to state B, we then end up with the following probability: \\[ P(X_2 = B | X_1 = D, X_0 = A) = 0.72 \\] And if we continue to walk further from states B to C to F and back to D, we then end up with the following probability: \\[ P(X_5 = D | X_4 = F, X_3 = C, X_2 = B, X_1 = D, X_0 = A) = 0.52 \\] Notice that no matter how random and how many times we walk, we end up inheriting the probability of walking to the final next state, given only the last previous state: \\[\\begin{align} P(X_{n} {}&amp;= S_{k_{n}} | X_{n-1} = S_{j_{n-1}}\\ ,...,\\ X_0 = S_{i_0}) \\\\ &amp;= P(X_{n}|X_{n-1}) \\times P(X_{n-1}|X_{n-2})\\times ... \\times P(X_1|X_0)\\times P(X_0) \\\\ &amp;=P(X_{n} = S_{k_{n}} | X_{n-1} = S_{j_{n-1}})\\\\ &amp;=(P^{n})_{j_{n-1}, k_{n}} \\end{align}\\] where: \\((P^{n})\\) is the transition probability in nth (temporal) steps \\(S_{j_{n-1}}\\) is the jth state in (n-1)th (temporal) steps \\(S_{k_{n}}\\) is the kth state in nth (temporal) steps Historical events are ignored, and only the last previous state is relevant. We call this memoryless property the Markov property. This characteristic is also called state independence. There are five other essential properties of the Markov chain. Irreducibility Irreducibility states that every state node in the system (in the chain) communicates with every other state node. Note that communication happens if one state node can access the other node and vice versa. For example, in the second graph above (two-state graph) of Figure 8.9, state nodes A and B can access each other and thus can communicate with each other. However, in the fourth graph (three-state graph), state node A can access state node B, but state node B cannot access state node A. Therefore, A and B cannot communicate with each other. Every node in graphs 1, 2, and 3 can communicate with every other node. Therefore each chain is an Irreducible Markov Chain. On the other hand, each chain in graphs 4, 5, and 6 is a Reducible Markov Chain because no other state node can communicate back with state node A. Aperiodicity Aperiodicity states that a Markov Chain is aperiodic if all its state nodes are aperiodic. Here, periodicity generally refers to the repeating (regular) occurrence of an event (or a state) at fix interval. In Markov Chain, one full cycle happens if we start walking from the state node (\\(S_i\\)) to at least another non-repeating state node and back to the starting state node. This full cycle is called a period (d) and is expressed as: \\[\\begin{align} d(S_i) = gcd\\{\\ n \\ge 1: (P^{n})_{i,i} &gt; 0\\ \\} = \\begin{cases} =1 &amp; \\text{(aperiodic)}\\\\ &gt;1 &amp; \\text{(periodic)} \\end{cases} \\label{eqn:eqnnumber326} \\end{align}\\] such that: \\[\\begin{align} (P^{n})_{i,i} = P(X_n = S_i| X_0 = S_i) \\end{align}\\] where: n is the number of steps taken. d is the period function taking in the gcd (greatest common denominator) outcome. P is the transition probability in a transition matrix. The notation, \\((P^{n})_{i,i}\\), describes the transition probability from \\(S_i\\) to \\(S_i\\) in nth steps. In Figure 8.9, graphs 1,2,3 are periodic. The first graph is a 2-periodic Markov Chain because any node can cycle back to itself in 2 steps (d = 2). The second graph has nodes that can cycle back to themselves in 3 steps (d = 3). And the third graph has nodes that can cycle back to themselves in 4 steps ( d = 4). Graphs 4,5,6 are not periodic because they are not irreducible in the first place. That is because other state nodes cannot communicate with state node A - there is no path to use to communicate back. But granting there is a path from state node C to state node A in graph 4 to make the chain reducible (see Figure 8.11). That still makes graph 4 aperiodic. Figure 8.11: Aperiodic Markov Chain If we are to capture all possible trajectories, we end up with the following: \\[\\begin{align*} A {}&amp;\\rightarrow B \\rightarrow C \\rightarrow A \\\\ B &amp;\\rightarrow C \\rightarrow B\\\\ C &amp;\\rightarrow B \\rightarrow C\\\\ \\end{align*}\\] We have two periods: a 3-step period for the first trajectory and a 2-step period for the second and third trajectories. So if we get the GCD, it ends up being 1. \\[ gcd\\{2, 3\\} = 1 \\leftarrow \\text{(aperiodic)} \\] Ergodicity A Markov Chain that is both aperiodic and irreducible is called an Ergodic Markov Chain. Transcience Transcience cites that a state node is transient if there is no other path to cycle back to itself. For example, in Figure 8.9, graphs 4, 5, and 6 show that state node A is a transient node because as soon as we leave state node A, there is no probability or no path that we can use to travel back to itself. Recurrence Recurrence cites that a state node is recurrent if there is at least one path to cycle back to itself. For example, in Figure 8.9, graphs 1, 2, and 3 show that all state nodes are recurrent because we can leave any state node and have a probability that we can find ourselves back to the same state node in some future number of steps. 8.1.4 Hidden Markov Model Hidden Markov model deals with a pair of sequences: a hidden state sequence and an emitted observation sequence. Figure 8.12 shows a diagram of an HMM along with its corresponding trellis diagram. Figure 8.12: Hidden Markov Chain Note that the trellis diagram is based on the state diagram. The sequence of hidden states is as follows: \\[ \\text{Hidden State Sequence } \\mathbf{(X^{seq})} = ( X_1,\\ X_2,\\ X_3,\\ X_4 ) = ( A,\\ C,\\ B,\\ A ) \\] The sequence of emitted observations is as follows: \\[ \\text{Emitted Observation Sequence} \\mathbf{(Y^{seq})} = ( Y_1 ,\\ Y_2 ,\\ Y_3 ,\\ Y_4 ) = (\\ R,P,Q,P ) \\] Though we receive a sequence of observed data, e.g., { R, P, Q, P }, we do not know the sort of system dynamics or sequence of random events responsible for emitting the outcome. For all we know, the unknown sequence of states could be any of the following sequences: \\[ ( A,\\ C,\\ B,\\ B)\\ \\ \\ \\ \\ \\ \\ ( B,\\ A,\\ C,\\ B)\\ \\ \\ \\ \\ \\ \\ ( C,\\ B,\\ A,\\ C)\\ \\ \\ \\ \\ \\ \\ ( A,\\ B,\\ C,\\ A)\\ \\ \\ \\ \\ \\ \\ \\] There are many examples of systems or sequences of random events in which we obtain a sample of observations \\(\\mathbf{Y^{seq}}\\), but the state or condition of \\(\\mathbf{X^{seq}}\\) under which each of these observations is exposed is unknown to us. A typical toy example of such a sequence of random events is around activity patterns depending on weather patterns. For example, we estimate the probability of bringing an umbrella to the park for a walk based on observed weather patterns. Another case is in studying the cause of death in multiple regions over time due to the spread of a particular pathogen (e.g., COVID-19) from region to region. Such a case does not necessarily demonstrate a one-to-one map between the hidden Markov Chain and the observable data; thus, we may call for the use of interpolation to fill the gap. There are three common goals we need to accomplish when dealing with HMM. To help accomplish the goals, we have a few items (or guides) to be aware of: First, we require three matrix-based parameters to form our hidden markov model (HMM), namely \\(\\mathbb{M}\\): the (\\(\\mathbb{T}\\)) transition probability matrix , the (\\(\\mathbb{E}\\)) emission probability matrix, and the (\\(\\mathbf{\\pi}\\)) initial probability matrix. See Figure 8.13. Figure 8.13: Transition and Emission Matrix where: n is the number of all possible distinct countable states, namely \\(S \\in \\{\\ A,\\ B,\\ C\\ \\}\\). m is the number of all possible distinct countable outcomes, namely \\(O \\in \\{\\ P,\\ Q,\\ R\\ \\}\\). Note that both transition and emission probability matrices contain rows of stochastic probabilities summing to 1. The initial probability matrix also sums to 1. Second, we also require the use of the HMM diagram in Figure 8.14. Just for illustration, let us treat each state node - in particular \\(X_t\\) - as a “blocking” state node that cuts off dependencies. Figure 8.14: Blocked State Third, to show what a blocking state node is all about, let us first simplify the joint probability equation below by using the Chain rule and memoryless property of the Markov Chain. In Figure 8.14, the probability of HMM system in \\(X_4\\) is expressed as such using the Chain rule: \\[\\begin{align} P(X_{1:4}) {}&amp;= P(X_1,...,X_4) \\\\ &amp;= P(X_4, X_3, X_2 , X_1)\\\\ &amp;= P(X_4| X_3, X_2 , X_1) P(X_3 , X_2, X_1) \\\\ &amp;= P(X_4| X_3, X_2 , X_1) P(X_3 | X_2, X_1) P(X_2, X_1)\\\\ &amp;= P(X_4| X_4, X_2 , X_1) P(X_3 | X_2, X_1) P(X_2|X_1) P(X_1) \\end{align}\\] However, with the memoryless property, the dependency of \\(X_4\\) to other previous states is “blocked” by \\(X_3\\); the states \\(X_{1:2}\\), therefore, become irrelevant and can be dropped. \\(X_4\\) depends on \\(X_3\\) and so \\(X_3\\) stays. Here is what we have: \\[\\begin{align} \\begin{array}{lll} P(X_1,...,X_4) &amp;= P(X_4 , X_3 , X_2, X_1) &amp; = P(X_4 , X_3)\\\\ &amp;= P(X_4|X_3 , X_2 , X_1) P(X_3, X_1 , X_1) &amp; = P(X_4|X_3 ) P(X_3 )\\\\ \\end{array} \\label{eqn:eqnnumber327} \\end{align}\\] In general: \\[\\begin{align} P(X_{1:t}) = P(X_t| X_{t-1})P(X_{t-1}). \\end{align}\\] Fourth, the same manner applies for the below equation in which the next state, \\(X_t\\), depends on the previous state, \\(X_{t-1}\\). The \\(X_4\\) state blocks \\(X_5\\) from \\(Y_{1:4}\\) and \\(X_{1:3}\\). Therefore, we have: \\[\\begin{align} \\begin{array}{lll} P(X_4| Y_1,...,Y_3, X_1,...,X_3) &amp;= P(X_4 | Y_3, Y_2 , Y_1, X_3, X_2 , X_1) &amp; = P(X_4| X_3 )\\\\ \\end{array} \\label{eqn:eqnnumber328} \\end{align}\\] In general: \\[\\begin{align} P(X_t | Y_{1:{t-1}}, X_{1:{t-1}}) = P(X_t|X_{t-1})\\ \\ \\ \\ \\leftarrow \\mathbf{\\text{(state transition probability)}}. \\end{align}\\] Fifth, the same manner applies with the below equation in which the observed data, \\(Y_3\\), depends on the sequence of states. We drop \\(X_{1:2}\\) and \\(Y_{1:2}\\) as they are blocked by state \\(X_3\\) from \\(Y_3\\). Here is what we have: \\[\\begin{align} \\begin{array}{lll} P(Y_3| Y_1,...,Y_2, X_1,...,X_3) &amp;= P(Y_3 | Y_2 , Y_1, X_3, X_2 , X_1) &amp; = P(Y_3| X_3 ). \\end{array} \\label{eqn:eqnnumber329} \\end{align}\\] In general: \\[\\begin{align} P(Y_t | Y_{1:{t-1}}, X_{1:t}) = P(Y_t|X_t)\\ \\ \\ \\ \\leftarrow \\mathbf{\\text{(emission probability)}}. \\end{align}\\] Lastly, we, therefore, have five HMM items to use in accomplishing our three goals, broken down into a model (\\(\\mathbb{M}\\)) and a pair of sequences (\\(\\mathbb{S}\\)) \\[\\begin{align} \\mathbb{M} = ( \\mathbb{T}^{\\ transition}, \\mathbb{E}^{\\ emission},\\pi^{\\ initial} ) = \\{ \\mathbb{T}, \\mathbb{E}, \\pi \\},\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbb{Seq} = \\{\\ X^{seq}, Y^{seq}\\ \\} \\end{align}\\] Here, we use the following notations for convenience: \\[\\begin{align*} \\mathbb{T}_{X_1}(X_2) &amp;\\leftarrow \\text{ the transition probability from current hidden state } X_1\\ \\text{to state }X_2\\\\ \\mathbb{E}_{X_1}(Y_1) &amp;\\leftarrow \\text{ the emission probability of observing $Y_1$ given current hidden state $X_1$}. \\end{align*}\\] With all that, we now discuss the three goals (Rabiner and Juang 1986). Other literature may refer to these three goals in terms of Evaluation, Decoding, and Learning tasks in solving HMM problems. First Goal (Evaluation) In an HMM system, determine the probability of observing \\(Y^{seq}\\), given that the temporal series of states, namely \\(X^{seq}\\), is unknown. Here, the model \\(\\mathbb{M}\\) is known. This is expressed as: \\[\\begin{align} P_{\\mathbb{M}}(Y^{seq}) {}&amp;= \\sum P_{\\mathbb{M}}(Y^{seq}, X^{seq}) &amp; \\text{(sum rule)}\\\\ &amp;= \\sum P_{\\mathbb{M}}(Y^{seq}| X^{seq})P_{\\mathbb{M}}(X^{seq}) &amp; \\text{(chain rule)} \\label{eqn:eqnnumber35} \\end{align}\\] Note that we are using the notation \\(P_{\\mathbb{M}}(Y^{seq})\\) instead of \\(P(Y^{seq}|\\mathbb{M})\\) to show that \\(\\mathbb{M}\\) is used as a known and already given (model) parameter. Also, it may be a bit cleaner that way. We break down the problem by computing the probability of seeing \\(Y^{seq}\\) and the probability of having \\(X^{seq}\\). \\[\\begin{align} P_{\\mathbb{M}}( X^{seq}) {}&amp;= \\sum \\pi (X_1) P_{\\mathbb{M}}(X_2| X_1) P_{\\mathbb{M}}(X_3| X_2) ... P_{\\mathbb{M}}(X_T| X_{T-1})\\\\ &amp;= \\sum \\pi(X_1)\\mathbb{T}_{X_1}(X_2)\\mathbb{T}_{X_2}(X_3) ... \\mathbb{T}_{X_{T-1}}(X_T) \\\\ &amp;= \\sum \\pi(X_1)\\prod_{i=1}^{T-1} \\mathbb{T}_{X_i}(X_{i+1})\\ \\ \\ \\ \\text{(all possible X sequences)} \\end{align}\\] \\[\\begin{align} P_{\\mathbb{M}}( Y^{seq}| X^{seq}) &amp;= \\sum P_{\\mathbb{M}}(Y_1| X_1) P_{\\mathbb{M}}(Y_2| X_2) P_{\\mathbb{M}}(Y_3 | X_3 ... P_{\\mathbb{M}}(Y_T| X_T) \\\\ &amp;= \\sum \\mathbb{E}_{X_1}(Y_1)\\mathbb{E}_{X_2}(Y_2)\\mathbb{E}_{X_3}(Y_3) ... \\mathbb{E}_{X_T}(Y_T) \\\\ &amp;= \\sum \\prod_{j=1}^{T} \\mathbb{E}_{X_j}(Y_j)\\ \\ \\ \\ \\text{(all possible Y sequences)} \\end{align}\\] \\[\\begin{align} \\therefore P_{\\mathbb{M}}(Y^{seq}) &amp;= \\sum P_{\\mathbb{M}}(Y^{seq}| X^{seq})P_{\\mathbb{M}}(X^{seq}) \\\\ &amp;=\\sum \\pi(X_1)\\prod_{i=1}^{T-1} P_{\\mathbb{M}}(X_{i+1}| X_{i})\\prod_{i=1}^{T} P_{\\mathbb{M}}(Y_{i}| X_{i})\\\\ &amp;= \\sum \\pi(X_1)\\prod_{i=1}^{T-1} \\mathbb{T}_{X_i}(X_{i+1})\\prod_{j=1}^{T} \\mathbb{E}_{X_j}(Y_j)\\ \\ \\ \\ \\text{(all possible XY sequences)} \\end{align}\\] where \\(\\pi(X_1)\\) is the probability of starting in state \\(X_1 \\in \\{A,B,C\\}\\) at step 1. The computational requirement for the equation is \\(O(T\\cdot N^T)\\), which is expensive, and it can get impractical to use if our model deals with larger countable states and longer series. An alternative solution is to optimize the computation and bring it to \\(O(T\\cdot N^2)\\). We call this Forward Algorithm. The alternative solution determines the most probable chance of being in a hidden state \\(X_t\\) at step t for a partial series of observations up to step t, namely \\(Y_{1:t}\\). Mathematically, we can derive an equation for the Forward algorithm and write the problem statement as a recursive function: \\[\\begin{align} \\alpha_{S_j}(t) {}&amp;= \\sum P_\\mathbb{M}(X_t,Y_{1:t})\\\\ &amp;= \\sum P_\\mathbb{M}(X_t, X_{t-1}, Y_t, Y_{1:t-1}) \\ \\text{(expand)} \\\\ &amp;= \\sum P_\\mathbb{M}(Y_t| X_t, X_{t-1}, Y_{1:t-1}) P_\\mathbb{M}( X_t, X_{t-1},Y_{1:t-1}) \\ \\text{(chain rule)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_t| X_t) P_\\mathbb{M}( X_t, X_{t-1}, Y_{1:t-1}) \\ \\text{( markov property)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_t| X_t) P_\\mathbb{M}( X_t | X_{t-1}, Y_{1:t-1}) P_\\mathbb{M}( X_{t-1}, Y_{1:t-1}) \\ \\text{(chain rule)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_t| X_t) P_\\mathbb{M}( X_t | X_{t-1}) P_\\mathbb{M}( X_{t-1},Y_{1:t-1}) \\ \\text{(markov property)}\\\\ &amp;= \\sum_{i=1:n} \\mathbb{E}_{S_j}(Y_t)\\mathbb{T}_{S_i}(S_j)\\alpha_{S_i}(t-1) \\ \\text{(matrix lookup)} \\end{align}\\] where: \\[ \\alpha_{S_i}(t-1) = P_\\mathbb{M}( X_{t-1},Y_{1:t-1})\\ \\ \\ \\ \\ \\ \\text{(probability of all previous observations, given state }S_i ) \\] The algorithm has three steps: Initialize \\(\\alpha_{S_i}(1)\\), where \\(1 \\le i \\le n\\) and \\(S \\in \\{\\ A,B,C\\ \\}\\). For \\(\\mathbf{i \\leftarrow i+1}\\) until \\(\\mathbf{i=n}\\): \\[\\begin{align} \\alpha_{S_i}(1) = \\pi(S_i) \\cdot \\mathbb{E}_{S_i}(Y_1) \\end{align}\\] Perform recursive computation using the below equation. For step \\(\\mathbf{t} \\leftarrow \\mathbf{2}\\) until \\(\\mathbf{T}\\): \\[\\begin{align} \\alpha_{S_j}(t) = \\left[\\sum_{i=1}^n \\alpha_{S_i}(t-1) \\mathbb{T}_{S_i}(S_j)\\right] \\mathbb{E}_{S_j}(Y_t) \\end{align}\\] Finally, at the end of the recursion, we perform the following computation for \\(P_{\\mathbb{M}}(Y^{seq})\\). \\[\\begin{align} P_{\\mathbb{M}}(Y^{seq}) \\leftarrow P(Y^{T}|\\mathbb{M}) = \\sum_{i=1}^{n=3} \\alpha_{S_i}(T) \\end{align}\\] Let us review Figure 8.15. The model \\(\\mathbb{M}\\) is based on Figure 8.13. Figure 8.15: Forward Algorithm Diagram To illustrate, let us build the model \\(\\mathbb{M}\\) and the pair of sequences \\(\\mathbb{S}\\) in R code: S = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # list of states O = c(&quot;P&quot;, &quot;Q&quot;, &quot;R&quot;) # list of observation symbols X.seq = c(&quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;) # hidden states Y.seq = c(&quot;R&quot;, &quot;P&quot;, &quot;Q&quot;, &quot;P&quot;) # observations T = matrix( # transition probabilities matrix c(0.00, 0.10, 0.90, 0.60, 0.00, 0.40, 0.23, 0.77, 0.00), nrow=3, ncol=3, byrow=TRUE ) colnames(T) = S rownames(T) = S E = matrix( # emission probabilities matrix c(0.43, 0.37, 0.20, 0.24, 0.76, 0.00, 0.52, 0.48, 0.00), nrow=3, ncol=3, byrow=TRUE ) colnames(E) = O rownames(E) = S # Initial probabilities matrix I = c(0.28, 0.12, 0.60); names(I) = S # the hidden markov model (M = list(&quot;T&quot; = T, &quot;E&quot; = E, &quot;I&quot; = I, &quot;S&quot;=S, &quot;O&quot;=O)) ## $T ## A B C ## A 0.00 0.10 0.9 ## B 0.60 0.00 0.4 ## C 0.23 0.77 0.0 ## ## $E ## P Q R ## A 0.43 0.37 0.2 ## B 0.24 0.76 0.0 ## C 0.52 0.48 0.0 ## ## $I ## A B C ## 0.28 0.12 0.60 ## ## $S ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## ## $O ## [1] &quot;P&quot; &quot;Q&quot; &quot;R&quot; Below is an example implementation of the Forward algorithm in R code: hmm.forward &lt;- function(Y, M) { T = length(Y) a = M$T; b = M$E; r = M$I; n = nrow(a) alpha = matrix(0, nrow = n, ncol = T) alpha[,1] = r * b[, Y[1]] for (t in 2:T) { # use log-sum-exp to avoid underflow for large multiplications for (j in 1:n) { alpha[j,t] = sum( alpha[,t-1] * a[,j] ) * b[j,Y[t]] } } list(&quot;alpha.matrix&quot;=alpha, &quot;Probability&quot;=sum(alpha[,T])) } hmm.forward(Y.seq, M) ## $alpha.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.056 0.000000 0.002529 0.0039824 ## [2,] 0.000 0.001344 0.015337 0.0001084 ## [3,] 0.000 0.026208 0.000258 0.0043735 ## ## $Probability ## [1] 0.008464 Let us validate by using backward() function from a 3rd-party library called HMM: library(HMM) hmm = initHMM(S, O, startProbs = M$I, transProbs = M$T, emissionProbs = M$E) exp(forward(hmm, Y.seq)) # library uses log ## index ## states 1 2 3 4 ## A 0.056 0.000000 0.002529 0.0039824 ## B 0.000 0.001344 0.015337 0.0001084 ## C 0.000 0.026208 0.000258 0.0043735 We can also validate the result by using Backward algorithm, which is the reverse of Forward algorithm. The derivation of the Backward equation is as such: \\[\\begin{align} \\beta_{Si}(t) {}&amp;= \\sum P_\\mathbb{M}(Y_{t+1:T}|X_t)\\\\ &amp;= \\sum P_\\mathbb{M}(Y_{t+1},Y_{t+2:Y_T},X_{t+1}|X_t) \\ \\text{(expand)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_{t+2:Y_T}|Y_{t+1},X_{t+1},X_t) P_\\mathbb{M}(Y_{t+1},X_{t+1},X_t) \\ \\text{(chain rule)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_{t+2:Y_T}|Y_{t+1},X_{t+1},X_t) P_\\mathbb{M}(Y_{t+1}|X_{t+1},X_t)P_\\mathbb{M}(X_{t+1},X_t) \\ \\text{(chain rule)}\\\\ &amp;= \\sum P_\\mathbb{M}(Y_{t+2:Y_T}|X_{t+1}) P_\\mathbb{M}(Y_{t+1}|X_{t+1})P_\\mathbb{M}(X_{t+1}|X_t) \\ \\text{(markov property)}\\\\ &amp;= \\sum_{j=1:n} \\beta_{Sj}(t+1) \\mathbb{E}_{S_j}(Y_{t+1})\\mathbb{T}_{S_i}(X_{S_j}) \\end{align}\\] where: \\[ \\beta_{S_j}(t+1) = P_\\mathbb{M}(Y_{t+2:Y_T}|X_{t+1})\\ \\ \\ \\ \\ \\ \\text{(probability of all future observations, given state }S_j ) \\] The algorithm has three steps: Initialize \\(\\beta_{Si}(T)\\). For \\(\\mathbf{i \\leftarrow i+1}\\) until \\(\\mathbf{i=n}\\): \\[\\begin{align} \\beta_{Si}(T) = 1 \\end{align}\\] Perform recursive computation using the below equation. For step \\(\\mathbf{t} \\leftarrow \\mathbf{2}\\) until \\(\\mathbf{T}\\): \\[\\begin{align} \\beta_{S_i}(t) = \\sum_{j=1}^n \\mathbb{T}_{S_i}(S_j) \\mathbb{E}_{S_j}(Y_{t+1}) \\beta_{S_j}(t+1) \\end{align}\\] Finally, at the end of the recursion, we perform the following computation for \\(P_{\\mathbb{M}}(Y^{seq})\\). \\[\\begin{align} P_{\\mathbb{M}}(Y^{seq}) \\leftarrow P(Y^{T}|\\mathbb{M}) = \\sum_{i=1}^{n=3} \\pi(S_i) \\mathbb{E}_{S_i}(Y_1)\\beta_{S_i}(1) \\end{align}\\] Let us review Figure 8.16. The model \\(\\mathbb{M}\\) is based on Figure 8.13. Figure 8.16: Backward Algorithm Diagram Below is an example implementation of the Backward algorithm in R code: hmm.backward &lt;- function(Y, M) { T = length(Y) a = M$T; b = M$E; r = M$I; n = nrow(a) beta = matrix(1, nrow = n, ncol = T) beta[,T] = 1 for (t in (T-1):1) { # use log-sum-exp to avoid underflow for large multiplications for (i in 1:n) { beta[i,t] = sum( a[i,] * b[, Y[t+1]] * beta[,t+1]) } } list( &quot;beta.matrix&quot;=beta, &quot;Probability&quot;=sum(r * b[, Y[1]] * beta[,1] )) } hmm.backward(Y.seq, M) ## $beta.matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.15115 0.1580 0.4920 1 ## [2,] 0.10619 0.1637 0.4660 1 ## [3,] 0.04587 0.3146 0.2837 1 ## ## $Probability ## [1] 0.008464 Let us validate by using backward() function from a 3rd-party library called HMM: library(HMM) hmm = initHMM(S, O, startProbs = M$I, transProbs = M$T, emissionProbs = M$E) exp(backward(hmm, Y.seq)) # library uses log ## index ## states 1 2 3 4 ## A 0.15115 0.1580 0.4920 1 ## B 0.10619 0.1637 0.4660 1 ## C 0.04587 0.3146 0.2837 1 Second Goal (Decoding) A second goal when dealing with HMM is to estimate the most probable (or likely) trajectory (or path) of the state sequence (\\(\\mathbf{X^{seq}}\\)) after seeing the observation sequence, namely (\\(\\mathbf{Y^{seq}}\\)). We use the famous Viterbi (Decoding) Algorithm to do that. Recall the following equation from the first goal (See Equation \\(\\ref{eqn:eqnnumber35}\\)): \\[\\begin{align} P_{\\mathbb{M}}(Y^{seq}, X^{seq}) {}&amp;= \\sum P_{\\mathbb{M}}(Y^{seq}| X^{seq})P_{\\mathbb{M}}(X^{seq}) &amp; \\text{(likelihood)} \\end{align}\\] Here, instead of using the summation notation, we use the maximization notation: \\[\\begin{align} P_{\\mathbb{M}}(Y^{seq}, X^{seq}) {}&amp;= max\\ P_{\\mathbb{M}}(Y^{seq}| X^{seq})P_{\\mathbb{M}}(X^{seq}) &amp; \\text{(decode)} \\end{align}\\] For the trajectory of probable state sequence, we have the following: \\[\\begin{align} X^* = \\underset{1 \\le i \\le n}{argmax}\\ P_{\\mathbb{M}}(Y^{seq}| X^{seq})P_{\\mathbb{M}}(X^{seq}) \\end{align}\\] We can break down the problem by computing the probability of seeing \\(Y^{seq}\\) and the probability of having \\(X^{seq}\\). \\[\\begin{align} P_{\\mathbb{M}}( X^{seq}) {}&amp; = max\\ \\pi(X_1)\\prod_{i=1}^{T-1} \\mathbb{T}_{X_i}(X_{i+1})\\ \\ \\ \\ \\text{(all possible X sequences)} \\\\ \\nonumber \\\\ P_{\\mathbb{M}}( Y^{seq}| X^{seq}) &amp;= max\\ \\prod_{j=1}^{T} \\mathbb{E}_{X_j}(Y_j)\\ \\ \\ \\ \\text{(all possible Y sequences)} \\\\ \\nonumber \\\\ \\therefore P_{\\mathbb{M}}(Y^{seq}) &amp;= max\\ \\pi(X_1)\\prod_{i=1}^{T-1} \\mathbb{T}_{X_i}(X_{i+1})\\prod_{j=1}^{T} \\mathbb{E}_{X_j}(Y_j)\\ \\ \\ \\ \\text{(all possible XY sequences)} \\end{align}\\] Similarly, to optimize computation, we use the following alternative equation: \\[\\begin{align} \\alpha_{S_j}(t) {}&amp;= max\\ P_\\mathbb{M}(X_t,Y_{1:t})\\\\ &amp;= \\underset{i=1:n}{max}\\ \\mathbb{T}_{S_i}(S_j)\\mathbb{E}_{S_j}(Y_t)\\alpha_{S_i}(t-1) &amp; \\text{(decode)} \\end{align}\\] Therefore, the Viterbi algorithm follows the Forward algorithm, except it does not sum probabilities; instead, it finds the maximum probabilities. Below is an example implementation of the Viterbi algorithm in R code: hmm.viterbi &lt;- function(Y, M) { T = length(Y) a = M$T; b = M$E; S = M$S; r = M$I; n = nrow(a) alpha = matrix(0, nrow = n, ncol = T) alpha[,1] = r * b[, Y[1]] for (t in 2:T) { # use log-sum-exp to avoid underflow for large multiplications for (j in 1:n) { alpha[j,t] = max( alpha[,t-1] * a[,j] * b[j,Y[t]] ) } } # Let&#39;s do the decoding m = apply(alpha, 2, which.max) # argmax code = S[m] rownames(alpha) = S list(&quot;alpha.matrix&quot;=alpha, &quot;Probability&quot;=max(alpha[,T]), &quot;Decoded State Sequence&quot;=code) } hmm.viterbi(Y.seq, M) ## $alpha.matrix ## [,1] [,2] [,3] [,4] ## A 0.056 0.000000 0.002230 0.00395693 ## B 0.000 0.001344 0.015337 0.00005353 ## C 0.000 0.026208 0.000258 0.00319008 ## ## $Probability ## [1] 0.003957 ## ## $`Decoded State Sequence` ## [1] &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; Let us validate by using viterbi(.) function from a 3rd-party library called HMM: library(HMM) hmm = initHMM(S, O, startProbs = M$I, transProbs = M$T, emissionProbs = M$E) viterbi(hmm, Y.seq) ## [1] &quot;A&quot; &quot;C&quot; &quot;B&quot; &quot;A&quot; Note that other efforts are taken to improve the Viterbi algorithm. For that, we leave readers to investigate other variations. Third Goal (Learning) A third goal when dealing with HMM is to estimate the probability of (or “train”) a new model \\(\\mathbb{M}^* = \\{ \\mathbb{T}^*, \\mathbb{E}^*, \\pi^*\\}\\) after seeing the observation sequence, namely \\(Y^{seq}\\). Here, an initial model, namely \\(\\mathbb{M}^0\\), is randomly provided. To do that, we use the Baum-Welch Algorithm, also called Forward-Backward Algorithm. The Baum-Welch Algorithm is an Expectation-Maximization (EM) algorithm. For the Estimation (E) Step, we follow the following steps: First, we randomly initialize the elements of each parameter in the model \\(\\mathbb{M}^0\\). Here, the size of each of the parameters is fixed. \\[\\begin{align} \\mathbb{M}^0 = \\{ \\mathbb{T}^0, \\mathbb{E}^0, \\pi^0\\} \\end{align}\\] Second, we compute the probability of all observations. That is the start of the iteration. Recall the Forward algorithm. We compute for \\(\\alpha_{S_j}(t)\\): \\[\\begin{align} \\alpha_{S_i}(1) {}&amp;= \\pi(S_i)^0 \\cdot \\mathbb{E}_{S_i}^0(Y_1),\\ \\ \\ \\ \\ \\ \\ i = 1\\ ...\\ n \\\\ \\alpha_{S_j}(t) {}&amp;= \\left[\\sum_{i=1}^n \\alpha_{S_i}(t-1) \\mathbb{T}_{S_i}^0(S_j)\\right] \\mathbb{E}_{S_j}^0(Y_t) \\end{align}\\] Recall the Backward algorithm. We compute for \\(\\beta_{S_i}(t)\\): \\[\\begin{align} \\beta_{Si}(T) {}&amp;= 1,\\ \\ \\ \\ \\ \\ \\ i = 1\\ ...\\ n \\\\ \\beta_{S_i}(t) &amp;= \\sum_{j=1}^n \\mathbb{T}_{S_i}^0(S_j) \\mathbb{E}_{S_j}^0(Y_{t+1}) \\beta_{S_j}(t+1) \\end{align}\\] Third, using Bayes Theorem, we combine both Forward and Backward equations to generate the two-state probabilities below: \\[\\begin{align} \\gamma_{S_i}(t) {}&amp;= P_\\mathbb{M^0}(X_t = S_i | Y^{seq}) = \\frac{P_\\mathbb{M^0}(X_t = S_i , Y^{seq})}{P_\\mathbb{M^0}(Y^{seq})}\\ \\ \\ \\ \\ \\ \\leftarrow\\ \\text{posterior} = \\frac{\\text{belief}}{\\text{marginal}}\\\\ &amp;= \\frac{ \\alpha_{S_i}(t)\\beta_{S_i}(t) } {\\sum_{j=1}^n \\alpha_{S_j}(t)\\beta_{S_j}(t)}\\ \\ \\ \\ \\text{(normalized)}\\\\ \\nonumber \\\\ \\xi_{S_i,S_j}(t) &amp;= P_\\mathbb{M^0}(X_t = S_i, X_{t+1} = S_j | Y^{seq}) = \\frac{P_\\mathbb{M^0}(X_t = S_i , X_{t+1} = S_j, Y^{seq})}{P_\\mathbb{M^0}(Y^{seq})}\\\\ &amp;= \\frac{ \\alpha_{S_i}(t)\\mathbb{T}_{S_i}^0(S_j)\\beta_{S_j}(t+1) \\mathbb{E}_{S_j}^0(Y_{t+1}) } {\\sum_{k=1}^n \\sum_{l=1}^n \\alpha_{S_k}(t)\\mathbb{T}_{S_k}^0(S_l)\\beta_{S_l}(t+1) \\mathbb{E}_{S_l}^0(Y_{t+1}) }\\ \\ \\ \\ \\text{(normalized)} \\end{align}\\] where, given \\(Y^{seq}\\): \\(\\gamma_{Si}(t)\\) is the probability of being in state \\(S_i\\) at time t. \\(\\xi_{S_i,S_j}(t)\\) is the expected transition from state \\(S_i\\) to state \\(S_j\\) at time t+1. Fourth, for the Maximization (M) step, we then train the model \\(\\mathbb{M^1}\\). We compute for the expected number of transitions from \\(S_i\\) and from \\(S_i\\) to \\(S_j\\): \\[\\begin{align} \\sum_{t=1}^{T} \\gamma_{Si}(t)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sum_{t=1}^{T} \\xi_{S_i,S_j}(t) \\end{align}\\] With that, we use the following model parameters: \\[\\begin{align} \\mathbb{T}^1_{S_i}(S_j) = \\frac{\\sum_{t=1}^{T-1} \\xi_{S_i,S_j}(t) }{\\sum_{t=1}^{T-1} \\gamma_{Si}(t)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbb{E}^1_{S_i}(O_k) = \\frac{\\sum_{t=1}^{T} \\left[I(Y_t = O_k) \\times \\gamma_{S_i}(t) \\right]}{\\sum_{t=1}^{T} \\gamma_{S_i}(t)} \\end{align}\\] \\[\\begin{align} \\pi^1(S_i) = \\gamma_{S_i}(1) \\end{align}\\] where: \\[\\begin{align} I(Y_t = O_k) = 1_{Y_t = O_k} = \\begin{cases} 1 &amp; Y_t = O_k\\\\ 0 &amp; otherwise \\end{cases}\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\text{(indicator function)} \\label{eqn:eqnnumber330} \\end{align}\\] Finally, from there, we iterate the EM steps until we get a final model (by convergence): \\[\\begin{align} \\mathbb{M^0}\\ \\ \\rightarrow\\ \\ \\mathbb{M^1}\\ \\ \\rightarrow\\ \\ \\mathbb{M^2}\\ \\ \\rightarrow \\ \\ ...\\ \\ \\rightarrow\\ \\ \\mathbb{M^*} \\end{align}\\] To illustrate, let us first randomly initialize a hidden markov model, namely \\(\\mathbb{M}^0\\): S = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;) # list of states O = c(&quot;P&quot;, &quot;Q&quot;, &quot;R&quot;) # list of observation symbols X.seq = c(&quot;A&quot;, &quot;C&quot;, &quot;B&quot;, &quot;A&quot;) # hidden states Y.seq = c(&quot;R&quot;, &quot;P&quot;, &quot;Q&quot;, &quot;P&quot;) # observations random.range &lt;- function(ncols, seed, skip=0) { set.seed(seed) x = runif(ncols, min=0, max=1) x = round( x / sum(x), 2) x[ncols] = 1 - sum(x[1:(ncols-1)]) if (skip != 0) { # mimic our HMM structure if (skip == 1) { x[ncols] = x[ncols] + x[skip] } else { x[skip-1] = x[skip-1] + x[skip] } x[skip] = 0 } x } T0 = matrix( # transition probabilities matrix c(random.range(3, seed=12, skip=1), random.range(3, seed=85, skip=2), random.range(3, seed=75, skip=3)), nrow=3, ncol=3, byrow=TRUE ) colnames(T0) = S rownames(T0) = S E0 = matrix( # emission probabilities matrix c(random.range(3, seed=45,skip=0), random.range(3, seed=67,skip=3), random.range(3, seed=41,skip=3)), nrow=3, ncol=3, byrow=TRUE ) colnames(E0) = O rownames(E0) = S # Initial probabilities matrix I0 = random.range(3, seed=382); names(I0) = S # the hidden markov model (M0 = list(&quot;T&quot; = T0, &quot;E&quot; = E0, &quot;I&quot; = I0, &quot;S&quot; = S, &quot;O&quot; = O)) ## $T ## A B C ## A 0.00 0.45 0.55 ## B 0.68 0.00 0.32 ## C 0.23 0.77 0.00 ## ## $E ## P Q R ## A 0.53 0.27 0.2 ## B 0.59 0.41 0.0 ## C 0.12 0.88 0.0 ## ## $I ## A B C ## 0.08 0.54 0.38 ## ## $S ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## ## $O ## [1] &quot;P&quot; &quot;Q&quot; &quot;R&quot; Below is an example implementation of the Baum-Welch algorithm in R code: hmm.estimate_gamma &lt;- function(alpha, beta) { ( alpha * beta ) / colSums(alpha * beta ) } hmm.estimate_xi &lt;- function(Y, alpha, beta, a, b) { T = length(Y); n = nrow(a) xi = array(0, dim = c(n, n, T)) # 3-dimensional for (t in 1:(T-1)) { sum.xi = 0 for (k in 1:n) { for (l in 1:n) { sum.xi = sum.xi + alpha[k,t] * a[k,l] * beta[l,t+1] * b[l, Y[t+1]] } } for (i in 1:n) { for (j in 1:n) { xi [i, j, t] = alpha[i,t] * a[i,j] * beta[j,t+1] * b[j, Y[t+1]] xi [i, j, t] = xi [i, j, t] / sum.xi } } } xi } hmm.estimate_T &lt;- function(Y, S, gamma, xi) { T = length(Y); n = nrow(gamma) T1 = matrix(0, nrow=n, ncol=n) colnames(T1) = S; rownames(T1) = S for (i in 1:n) { sum.gamma = 0 for (t in 1:(T-1)) { sum.gamma = sum.gamma + gamma[i,t] } for (j in 1:n) { sum.xi = 0; for (t in 1:(T-1)) { sum.xi = sum.xi + xi[i,j,t] } T1[i,j] = sum.xi / sum.gamma } } T1 } hmm.estimate_E &lt;- function(Y, S, O, gamma) { T = length(Y); n = nrow(gamma); m = length(O) E1 = matrix(0, nrow=n, ncol=m) colnames(E1) = O; rownames(E1) = S; for (i in 1:n) { sum.gamma.denom = 0 for (t in 1:T) { sum.gamma.denom = sum.gamma.denom + gamma[i,t] } for (k in 1:m) { sum.gamma.num = 0; for (t in 1:T) { I = (Y[t] == O[k]) + 0 sum.gamma.num = sum.gamma.num + I * gamma[i,t] } E1[i,k] = sum.gamma.num / sum.gamma.denom } } E1 } hmm.baum_welch &lt;- function(Y, M, n.iter = 2, tol = 1e-5) { # set variables T = length(Y); n = nrow(M$T) S = M$S; O = M$O; limit = n.iter; T.norm = E.norm = err = 0 sequence = matrix(0, 0, 4) for (h in 1:limit) { a = M$T; b = M$E # Elimination Step: generate transition probabilities alpha = hmm.forward(Y,M)$alpha.matrix beta = hmm.backward(Y,M)$beta.matrix gamma = hmm.estimate_gamma(alpha, beta) xi = hmm.estimate_xi(Y, alpha, beta, a, b) # Maximization Step: estimate model T1 = hmm.estimate_T(Y, S, gamma, xi) E1 = hmm.estimate_E(Y, S, O, gamma) I1 = gamma[,1] # this seems fix for baumWelch() # Update Model M$T = T1; M$E = E1; M$I = I1 # Evaluate convergence T.norm = norm(T1); E.norm = norm(E1) err = abs( norm(T1) - norm(a)) sequence = rbind(sequence, c(h, T.norm, E.norm, err)) if ( err &lt; tol ) break } colnames(sequence) = c(&quot;Iteration&quot;, &quot;T.norm&quot;, &quot;E.norm&quot;, &quot;error&quot;) list(&quot;Iteration&quot;= sequence, M) } hmm.baum_welch(Y.seq, M0, n.iter=5, tol=1e-5) ## $Iteration ## Iteration T.norm E.norm error ## [1,] 1 1.649 1.334 4.292e-01 ## [2,] 2 1.947 1.090 2.976e-01 ## [3,] 3 1.999 1.013 5.171e-02 ## [4,] 4 2.000 1.000 1.484e-03 ## [5,] 5 2.000 1.000 2.143e-06 ## ## [[2]] ## [[2]]$T ## A B C ## A 0.000e+00 1 2.124e-38 ## B 3.351e-08 0 1.000e+00 ## C 4.592e-12 1 0.000e+00 ## ## [[2]]$E ## P Q R ## A 4.592e-12 3.351e-08 1 ## B 1.000e+00 3.065e-66 0 ## C 2.124e-38 1.000e+00 0 ## ## [[2]]$I ## [1] 1 0 0 ## ## [[2]]$S ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## ## [[2]]$O ## [1] &quot;P&quot; &quot;Q&quot; &quot;R&quot; Let us validate by using baumWelch(.) function from a 3rd-party library called HMM: library(HMM) hmm = initHMM(S, O, startProbs = M0$I, transProbs = M0$T, emissionProbs = M0$E) baumWelch(hmm, Y.seq, maxIterations=5, delta=1e-5) ## $hmm ## $hmm$States ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; ## ## $hmm$Symbols ## [1] &quot;P&quot; &quot;Q&quot; &quot;R&quot; ## ## $hmm$startProbs ## A B C ## 0.08 0.54 0.38 ## ## $hmm$transProbs ## to ## from A B C ## A 0.000e+00 1 2.124e-38 ## B 3.351e-08 0 1.000e+00 ## C 4.592e-12 1 0.000e+00 ## ## $hmm$emissionProbs ## symbols ## states P Q R ## A 4.592e-12 3.351e-08 1 ## B 1.000e+00 3.065e-66 0 ## C 2.124e-38 1.000e+00 0 ## ## ## $difference ## [1] 1.5226767 0.8705965 0.3070583 0.0386797 0.0005191 To be more effective, Baum-Welch requires more observed data (\\(Y^{seq}\\)) to be able to learn and train a better model. Also, the algorithm is not necessarily aware of the structure of the hidden Markov model (let alone the graphical representation); nonetheless, it uses MLE for estimation. That said, it helps to estimate closer to the actual model for an expected convergence. There are other efforts made to improve upon the Baum-Welch algorithm. We leave readers to investigate other variations, including the application of HMM in speech recognition, gesture recognition, natural language processing (e.g., pos-tagging), genome/DNA sequencing, time-series forecasting, etc. For further reading, we encourage readers to investigate the application of HMM in Speech Recognition (Rabiner L. R. 1988). 8.1.5 Dynamic System Model In HMM section, recall how we use a model \\(\\mathbb{M}\\) consisting of a set of matrix-based parameters, namely \\(\\{\\mathbb{T}, \\mathbb{E}, \\pi\\}\\). The transition probability matrix, namely \\(\\mathbb{T}\\), contains an arbitrary fix (constant) probability of transitioning from one discrete state to another. The emission probability matrix, namely \\(\\mathbb{E}\\), contains an arbitrary fix (constant) probability of emitting an observation (or measurement) state. Here, we have the following list of discrete states and discrete observation symbols. \\[ S \\in \\{\\ A,\\ B,\\ C\\ \\}\\ \\ \\ \\ \\ \\ \\ \\ \\ O \\in \\{\\ P,\\ Q,\\ R\\ \\} \\] In this section, we extend the concept of HMM by introducing Dynamic Systems, which may deal with moving targets, and therefore the states and measurements (or observations) are not arbitrary (not fixed) and are thus continuous. And so, being continuous, we deal with distributions - particularly Gaussian distributions. Here, we have the following continuous states. \\[ S\\ \\in\\ \\mathbb{R}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ O\\ \\in\\ \\mathbb{R} \\] It is essential to switch context from a set of constant categorical states to a set of dynamic continuous states that follow Gaussian distribution. Also, observed data is not only constrained within a set of discrete categorical states but also within possibly continuous (moving) linear states. we use two models: process model is also called the action model or motion model. measurement model is also called the sensor model. In the context of Dynamic Systems, let us use a simple Feedback loop diagram (see Figure 8.17). Figure 8.17: Recursive Dynamic System Diagram In the diagram, we have a dynamic system that accepts an input and produces an output; but also, based on certain conditions, it can recursively loop back to update the system for better output. Our focus on dynamic systems includes three tasks: Filtering, Smoothing, and Prediction. Especially for Filtering, we cover four techniques: Bayes Filter (Stochastic Filter) Kalman Filter (Linear Filter) Extended Kalman Filter (Non-Linear Filter by Local Linearization) Unscented Kalman Filter (Non-Linear, non-Jacobian Filter) Particle Filter (Non-parametric Filter, sampling required) 8.1.6 Bayes Filter We start the discussion of Bayes Filtering (Ho and Lee), also called Recursive Bayesian Estimation and Bayesian Learning (Lin and Yau), using Bayesian Network with the addition of a control. In this section and the following sections, we reference Wan E.A. et al. (2000), Fox D. et al. (2003), Urrea C. and Agramonte R. (2021), Let us use Figure 8.18 to explain Bayes Filter (borrowed from sample HMM graph and extended). Figure 8.18: Dynamic System Diagram The figure shows a simple general model which depicts a single state, namely X, of an object transitioning from time t-1 to t and from time t to time t+1. It explains a rather fundamental part of a dynamic system. A good example is an autonomous robot that navigates a landscape guided by some control in the form of action inputs (Thrun, Burgard, Fox, Abbeel). In the figure, notice the introduction of \\(\\mathbf{U}\\) variable, which represents an input. Localization (mapping) and navigation are common problems that require location estimation, presented by X - the state - guided by some input U. In other words, we use the general notation below to approximate the probability of the state of a system denoted by X, conditioned on both an observation, namely Y, and possibly control, namely U: \\[ P(X_{0:T},M| Z_{1:T}, U_{1:T}) \\] where: Y is sensor measurement (the observation) U is action (the control - user input to control robot) X is state (the state of object) M is map (or landmarks or environment constraints) T is time In other complex cases, we may require to form a joint posterior estimate with M, which represents landmarks surrounding an object depending on the problem statement. \\[\\begin{align} P(X_t,M| Z_{1:t}, U_{1:t}) = \\int_{x_0} ... \\int_{x_{t-1}} p(x_{0:t},m| z_{1:t}, u_{1,t}) dx_{t-1} ... d_{x_0} \\end{align}\\] Observations, namely Y, are generated by sensors in the form of measurements. We use such measurements to estimate the posterior density of the state of the object. Our goal is to estimate whether an object is locally positioned at an exact place. Otherwise, we estimate if the object is positioned somewhere in (or around) the place (which calls for uncertainty - probabilistic) by estimating measurements (and, in addition, by predicting the next state). If uncertainty is extremely large (e.g., over 99.99999%), then we can ignore uncertainty. In our simple toy robot example, however, the world or environment around a robot is dynamic and is therefore full of uncertainty. With no assumptions about the surrounding environment, or object itself, we can start with a uniform distribution (e.g., we do not know X - the state is hidden). Let us exclude the map of the environment for a moment: \\[ P(X_{0:T}| Z_{1:T}, U_{1:T}) \\] Here, we explore the idea that the location of an object is conditioned on a single sensor. In reality, a self-driving car requires multiple sensors to feed the dynamic system to generate a better state of the car from time t to time t+1. Our goal is to predict a posterior distribution, which we term as Belief, denoted by Bel(.). The following general equation drives our Belief for X at time t: \\[\\begin{align} {}&amp;\\underbrace{Bel(X_t)}_\\text{belief} = \\underbrace{P(X_t|Y_{1:t}, U_{1:t})}_\\text{posterior} \\nonumber \\\\ &amp;= \\eta P(y_t|x_t, y_{1:t-1}, u_{1:t})P(x_t|y_{1:t-1}, u_{1:t-1}, u_t)\\\\ &amp;= \\eta P(y_t|x_t)P(x_t|y_{1:t-1},u_{1:t-1}, u_t) \\ \\text{(markov property)}\\\\ &amp;= \\eta P(y_t|x_t) \\int_{x_{t-1}} P(x_t|x_{t-1}, y_{1:t-1}, u_{1:t}) P(x_{t-1}|y_{1:t-1}, u_{1:t-1}, u_t) dx_{t-1} \\ \\text{(sum rule)}\\\\ &amp;= \\eta P(y_t|x_t) \\int_{x_{t-1}} P(x_t|x_{t-1}, u_{1:t}) P(x_{t-1}|y_{1:t-1},u_{1:t-1}) dx_{t-1} \\ \\text{(markov property}\\\\ &amp;= \\eta P(y_t|x_t) \\int_{x_{t-1}} P(x_t|x_{t-1}, u_{1:t}) Bel(X_{t-1}) dx_{t-1} \\ \\text{(recursive belief)} \\end{align}\\] where: \\(\\eta\\) is a normalizing state. Note that all possible probabilities of state X sum up to 1. We use the recursive bayesian equation above to illustrate the algorithm. First, we start the algorithm by constructing two models - recall transition probabilities model and emission probabilities model in HMM). Here, we use process model and measurement model. See Figure 8.19. Figure 8.19: Bayes Filter Model Process (Transition) model - this model deals with state changes. For example, Figure 8.19 shows a three-state process matrix that defines the transition probability of one state to another state. Measurement (Emission) model - this model, also called Sensor model, deals with measurements (observed data). Second, assume our problem statement includes the following sequence of binary inputs and observed measurements - the sequence of states is hidden: \\[ U = \\{0_{t=1}, 1_{t=2},1_{t=3}... \\}\\ \\ \\ \\ \\ \\ Y = \\{Q_{t=1},Q_{t=2},R_{t=3},...\\} \\] Assume that the probabilities in the process and measurement models correspond to uncertain results (with noise already inclusive). Third, we perform a recursive iteration by alternately executing a prediction step, which calculates a prior and a correction step, which calculates a posterior in turn. The iteration becomes recursive from t to t+1. Here, the prior in the prediction step is needed to calculate the posterior in the correction step. Likewise, the posterior in the correction step is needed to calculate the prior in the prediction step. The general algorithm for Discrete Bayes Filter is as follows: For all possible states, namely \\(\\mathbf{X_t}\\), then do the following: Prediction Step (e.g. Action/Movement): \\[\\begin{align} \\underbrace{\\overline{Bel}(X_t)}_{\\begin{array}{c}new\\\\prior\\end{array}} = \\sum_{x_{t-1}} \\underbrace{ P (X_t|X_{t-1}, U_{t})}_{\\begin{array}{c}motion\\\\model\\end{array}} \\underbrace{ Bel(X_{t-1})}_{\\begin{array}{c}previous\\\\posterior\\end{array}} \\label{eqn:eqnnumber331} \\end{align}\\] where: \\[\\begin{align} Bel(X_{t-1}) = P(X_{t-1}|Y_{1:t-1}) \\end{align}\\] Correction (Update) Step (e.g. Sensor): \\[\\begin{align} \\underbrace{Bel(X_t)}_{\\begin{array}{c}new\\\\posterior\\end{array}} = \\eta \\underbrace{\\overbrace{ P(y_t|x_t)}^{\\begin{array}{c}measurement\\\\model \\end{array}}}_{likelihood} \\underbrace{\\overline{Bel}(X_{t})}_{\\begin{array}{c}current\\\\prior\\end{array}} \\label{eqn:eqnnumber332} \\end{align}\\] where: \\[\\begin{align} \\frac{1}{\\eta} = P(Y_t|Y_{1:t-1}) = \\int P(Y_t|X_t)\\mathcal(X_t|Y_{1:t-1}) dx_t \\end{align}\\] We then obtain the final \\(Bel(X_t)\\) upon reaching the tolerance level, ending the recursive iteration. To illustrate, given the matrices in Figure 8.19, let us perform some recursive iteration: Starting at t=1, let us solve for a new prior in the prediction step. Then, using the new prior as the current prior, we solve for the posterior in the correction step: Prediction step: \\[\\begin{align} \\overline{Bel}(x_1) = \\sum P(x_1|x_0, u_1) Bel(x_0) \\end{align}\\] Based on the process model, \\(P(x_1|x_0, u_1 = 0)\\) is read as the transition probability from \\(x_0\\) to \\(x_1\\), given an input \\(u_1 = 0\\). For example: \\[\\begin{align*} P(x_1 = A|x_0 = A, u_1 = 0) = 1.00\\\\ P(x_1 = B|x_0 = A, u_1 = 0) = 0.00\\\\ P(x_1 = C|x_0 = A, u_1 = 0) = 0.00 \\end{align*}\\] We derive the same for the other initial transitions. The initial priors are: \\[ Bel(x_0 = A) = 0.28\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_0 = B) = 0.12\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_0 = C) = 0.60 \\] Therefore: \\[\\begin{align} \\overline{Bel}(x_1 = A) {}&amp;= P(x_1 = A|x_0 = A, u_1 = 0) Bel(x_0 = A) \\nonumber \\\\ &amp;+ P(x_1 = A|x_0 = B, u_1 = 0) Bel(x_0 = B) \\nonumber \\\\ &amp;+ P(x_1 = A|x_0 = C, u_1 = 0) Bel(x_0 = C) \\\\ &amp;= (1.00)(0.28) + (0.00)(0.12) + (0.00)(0.60) = 0.28 \\nonumber \\\\ \\overline{Bel}(x_1 = B) &amp;= (0.00)(0.28) + (1.00)(0.12) + (0.00)(0.60) = 0.12 \\nonumber \\\\ \\overline{Bel}(x_1 = C) &amp;= (0.00)(0.28) + (0.00)(0.12) + (1.00)(0.60) = 0.60 \\nonumber \\end{align}\\] Correction (Update) step: \\[\\begin{align} Bel(x_1 ) = \\eta P(y_1|x_1) \\overline{Bel}(x_1 ) \\end{align}\\] Based on the measurement model, \\(P(y_1|x_1)\\) is read as likelihood of \\(y_1\\) given \\(x_1\\). \\[\\begin{align*} P(y_1 = P|x_1 = A) {}&amp;= 0.90\\ \\ \\ \\ \\ \\ P(y_1 = Q|x_1 = A) = 0.07\\ \\ \\ \\ \\ \\ P(y_1 = R|x_1 = A) = 0.03\\\\ P(y_1 = P|x_1 = B) &amp;= 0.09\\ \\ \\ \\ \\ \\ P(y_1 = Q|x_1 = B) = 0.80\\ \\ \\ \\ \\ \\ P(y_1 = R|x_1 = B) = 0.11\\\\ P(y_1 = P|x_1 = C) &amp;= 0.07\\ \\ \\ \\ \\ \\ P(y_1 = Q|x_1 = C) = 0.08\\ \\ \\ \\ \\ \\ P(y_1 = R|x_1 = C) = 0.85\\\\ \\end{align*}\\] Here, we solve for \\(\\eta\\) first (using observation at t=1, e.g. \\(y_1\\) = Q). \\[\\begin{align} Bel(x_1 = A) {}&amp;= \\eta P(y_1 = Q|x_1 = A) \\overline{Bel}(x_1 = A) \\\\ &amp;= \\eta (0.07)(0.28) = \\eta (0.0196) \\nonumber \\\\ Bel(x_1 = B) &amp;= \\eta P(y_1 = Q|x_1 = B) \\overline{Bel}(x_1 = B) \\\\ &amp;= \\eta (0.80)(0.12) = \\eta (0.096) \\nonumber \\\\ Bel(x_1 = C) &amp;= \\eta P(y_1 = Q|x_1 = C) \\overline{Bel}(x_1 = C) \\\\ &amp;= \\eta (0.08)(0.60) = \\eta (0.048) \\nonumber \\end{align}\\] \\[\\begin{align} Bel(x_1 = A) + Bel(x_1 = B) + Bel(x_1 = C) {}&amp;= 1 \\\\ \\eta (0.0196) + \\eta (0.096) + \\eta (0.048) &amp;= 1 \\nonumber \\\\ \\eta &amp;= 6.112469 \\nonumber \\end{align}\\] Therefore, our new set of posteriors at t=1 is: \\[ Bel(x_1 = A) = 0.1198044 \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_1 = B) = 0.586797 \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_1 = C) = 0.2933985 \\] Here, it suggests that our hidden state is closer to B. At t=2, we use the above beliefs as the posterior for the next iteration. This time, our input is \\(\\omega_2\\); meaning, we are now going to use process model 2 so that we have the following: Prediction step Using process model 2, we have the following examples for \\(P(x2|x_1 = A, u_2 = 1)\\): \\[\\begin{align*} P(x_2 = A|x_1 = A, u_2 = 1) = 0.20\\\\ P(x_2 = B|x_1 = A, u_2 = 1) = 0.10\\\\ P(x_2 = C|x_1 = A, u_2 = 1) = 0.70 \\end{align*}\\] We derive the same for the other initial transitions. Our new prior becomes then: \\[\\begin{align*} \\overline{Bel}(x_2 = A) {}&amp;= (0.20)(0.1198044) + (0.50)(0.586797) + (0.25)(0.2933985) = 0.390709\\\\ \\overline{Bel}(x_2 = B) &amp;= (0.10)(0.1198044) + (0.10)(0.586797) + (0.65)(0.2933985) = 0.2613692\\\\ \\overline{Bel}(x_2 = C) &amp;= (0.70)(0.1198044) + (0.40)(0.586797) + (0.10)(0.2933985) = 0.3479217\\\\ \\end{align*}\\] Correction (Update) step: Here, we solve for \\(\\eta\\) using observations at t=2, e.g. \\(y_2 = Q\\). \\[\\begin{align*} Bel(x_2 = A) {}&amp;= \\eta(0.07)(0.390709) = \\eta (0.02734963)\\\\ Bel(x_2 = B) &amp;= \\eta(0.80)(0.2613692) = \\eta (0.2090954)\\\\ Bel(x_2 = C) &amp;= \\eta(0.08)(0.3479217) = \\eta (0.02783374) \\end{align*}\\] Our new \\(\\eta\\) is then: \\[ \\eta = 3.783883 \\] Therefore, our new set of posteriors is now: \\[ Bel(x_2 = A) = 0.1034878 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_2 = B) = 0.7911925 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Bel(x_2 = C) = 0.1053196 \\] Here, our hidden state X looks more like B. We can continue to iterate further to see if X gets 100% closer to being B. We leave readers to review some applications of Bayes Filter in surveillance, mine exploration, terrain mapping, and reef monitoring. A similar concept of localization applies to consumer wares such as vacuum cleaners, drones, and self-driving cars. Additionally, we leave readers to investigate sensor fusion, sensor aliasing, SLAM (simultaneous localization and mapping), and DATMO (detection and tracking of moving objects). 8.1.7 Kalman Filter Kalman Filter, also called Linear Quadratic Estimator (LQE), extends upon HMM by dealing with continuous states. It complements the concept of Bayes Filter. The basic idea of Kalman Filter is to estimate a true measurement. Here, we deal with an Estimated measurement denoted as X and a True measurement denoted as Y. It may help to assume that estimated measurements come with errors and thus may best be denoted as \\(\\mathbf{X^e}\\) for emphasis. Similarly, we often deal with observed measurements which also come with errors instead of the true measurements and thus it may also help to denote that as \\(\\mathbf{Y^e}\\). \\[ \\begin{array}{llllll} X&amp;\\leftarrow&amp;\\text{Estimation} &amp; Y&amp;\\leftarrow&amp;\\text{True Measurement}\\\\ X^e&amp;\\leftarrow&amp;\\text{Estimation with Error} &amp; Y^e&amp;\\leftarrow&amp;\\text{(Observed) Measurement with Error}\\\\ E^x&amp;\\leftarrow&amp;\\text{Error in Estimation} &amp; E^y&amp;\\leftarrow&amp;\\text{Error in Observed Measurement} \\end{array} \\] The Kalman Filter algorithm starts with an initial estimate of the true measurement. Then, we perform iteration such that the estimate gets closer to the true measurement. Because we may not necessarily know the true measurement, we rely on the available observed measurement. The iteration goes through three calculations. \\[\\begin{align} K = \\frac{E^x}{E^x + E^y} \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ \\ X^e_t = X^e_{t-1} + K(Y^e_t + X^e_{t-1}) \\\\ \\ \\ \\ \\ &amp;\\rightarrow \\ \\ \\ \\ \\ E^x_t = \\frac{(E^y)(E^x_{t-1})}{(E^y)(E^x_{t-1})} = (1 - K)(E^x_{t-1}) \\end{align}\\] where K is the Kalman Gain. To illustrate a simple example, suppose we are estimating the weight of an object using a weighing scale. The true weight is 21.40 kilograms (assume this is not known - but we use it as our mean and add error/noise, e.g., 0.5 variance, to simulate an observed measurement ). What is known to us, however, are the following (in kilograms): \\[ \\begin{array}{ll} X^e = 22.5 &amp; Y^e \\sim N(21.40, 0.5) \\\\ E^x = 2 &amp; E^y = 2.5 \\end{array} \\] Let us implement the equations into R code: set.seed(2000) kalman_gain &lt;- function(E.x, E.y) { E.x / ( E.x + E.y) } estimate &lt;- function(X.e, Y.e, K) { X.e + K * (Y.e - X.e) } error &lt;- function(E.x, K) { (1 - K ) * E.x } n = 50 x = seq(1, 10, length.out=n) y = 21.40 X.e = 100.5; E.x = 90; E.y = 2.5 Y.e = rnorm(n, y, 0.5) outcome = matrix(0, n, 4) for (t in 1:n) { K = kalman_gain(E.x, E.y) X.e = estimate(X.e, Y.e[t], K) E.x = error(E.x, K) outcome[t,] = round(c(Y.e[t], X.e, E.x, K ),2) } colnames(outcome) = c(&quot;Measurement&quot;, &quot;Estimate&quot;, &quot;Error&quot;, &quot;Kalman Gain&quot;) tail(outcome) ## Measurement Estimate Error Kalman Gain ## [45,] 22.17 21.46 0.06 0.02 ## [46,] 21.83 21.46 0.05 0.02 ## [47,] 21.88 21.47 0.05 0.02 ## [48,] 21.14 21.47 0.05 0.02 ## [49,] 21.71 21.47 0.05 0.02 ## [50,] 21.68 21.48 0.05 0.02 Our final estimate is at 21.48 kilograms at 10 iterations. See Figure 8.20. plot(NULL, ylim=range(20, 23), xlim=range(1,10), xlab=&quot;T&quot;, ylab=&quot;Weight (Lbs)&quot;, main=&quot;Kalman Filter model&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(x, Y.e, col=c(&quot;black&quot;), pch=16, cex=0.80) abline(h=y, col=c(&quot;darksalmon&quot;), lwd=3) lines(x, outcome[,2], col=c(&quot;navyblue&quot;), lwd=3) legend(&quot;bottomright&quot;, inset=.02, c(&quot;True Measurement&quot;,&quot;Estimate (KF)&quot;, &quot;Observation&quot;), col=c(&quot;darksalmon&quot;, &quot;navyblue&quot;, &quot;black&quot;), horiz=FALSE, cex=0.8, lty=c(1,1, 0), lwd=c(2,2), pch=c(-1,-1, 16)) Figure 8.20: Kalman Filter For a multi-dimensional case, let us reference Figure 8.21 (a modified version of the diagram from Farhad Merchant et al. (2018), Ghosh A (2019), and shown in Michel van Biezen (2015). The notation used in the discussion references M. van Biezen (2015). Figure 8.21: Kalman Filter Similar to Bayesian Filter, the KF algorithm uses a prediction step and an update step with the corresponding equations as shown in Figure 8.21. For a dynamic system, we can relate the calculation of estimated prediction of measurement, namely \\(X_t\\), with the popular kinematics equation, e.g., newton’s law of motion, as demonstrated by M. van Biezen (2015) for a falling and moving object as an example. The newton equation is as follows: \\[\\begin{align} d = vt + a\\frac{1}{2}t^2 \\end{align}\\] where: v = velocity of the moving object a = accelaration of the moving object d = displacement (new location) t = time In general, Kalman filter uses the following dynamic system of equations. For State Prediction, we have the following general linear system of equations (each term maps to the corresponding term in the kinematics equation): \\[\\begin{align} \\underbrace{X_t}_\\text{d} = \\underbrace{AX_{t-1}}_\\text{vt} + \\underbrace{B_t U_t}_{a\\frac{1}{2}t^2} + w_{t},\\ \\ \\ \\ \\ \\ \\ w_{t} \\sim \\mathcal{N}(0, W_{t}) \\end{align}\\] where: \\(\\mathbf{X_t}\\) is prediction state, \\(\\mathbf{X_{t-1}}\\) is prior state, \\(\\mathbf{U_{t}}\\) is control state, \\(\\mathbf{w_{t}}\\) is system perturbation (gaussian noise), \\(\\mathbf{W_{t}}\\) is stochastic diffusion (covariance), \\(\\mathbf{A}\\) and \\(\\mathbf{B_t}\\) depend upon the application or system used (e.g., gravity for falling object and acceleration for moving object). A is a state-transition matrix and B is a control-transition matrix similar to the Bayes Filter process models previously discussed. The state X can represent a vector that includes position (p), velocity (v), acceleration (a): \\[ X_t = \\left[ \\begin{array}{c} x_1^{(p)} \\\\ x_1^{(v)} \\\\ x_1^{(a)} \\end{array} \\right] \\] For cases in which \\(\\mathbf{x_1}\\) and \\(\\mathbf{x_2}\\) are vectors that include only position (p) and velocity (v), we show the following: \\[ X_t = \\left[ \\begin{array}{c} x_1^{(p)} \\\\ x_1^{(v)}\\end{array} \\right] \\ \\ \\ \\ \\ A_{2x2} = \\left[ \\begin{array}{cc} 1 &amp; \\Delta t\\\\ 0 &amp; 1 \\end{array} \\right]_{2x2} \\ \\ \\ \\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ X_t = \\left[ \\begin{array}{c} x_1^{(p)} \\\\ x_2^{(p)} \\\\ x_1^{(v)} \\\\ x_2^{(v)} \\end{array} \\right] \\] \\[ A_{4x4} = \\left[ \\begin{array}{cccc} 1 &amp; 0 &amp; \\Delta t &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; \\Delta t \\\\ 0 &amp; 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{array} \\right]_{4x4} \\] Let us consider the 1D case like so: \\[ X_t = \\underbrace{\\left[\\begin{array}{cc}1 &amp; \\Delta t\\\\ 0 &amp; 1 \\end{array}\\right]}_\\text{A} X_{t-1} + \\underbrace{\\left[\\begin{array}{c } \\frac{1}{2}\\Delta t^2\\\\ \\Delta t \\end{array}\\right]}_\\text{B} U_{t} + w_t \\ \\ \\ \\ \\ where:\\ U_t = \\begin{cases}g = 9.8 \\frac{m}{s^2} \\\\ a = accel.\\end{cases} \\] For Measurement (Sensor) Observation, we have the following general linear system of equations: \\[\\begin{align} Y_t = CX_t + v_t,\\ \\ \\ \\ \\ \\ \\ v_t \\sim \\mathcal{N}(0, V_t) \\end{align}\\] where: \\(\\mathbf{Y_t}\\) is sensor (measurement) state, \\(\\mathbf{X_t}\\) is prediction state, \\(\\mathbf{v_t}\\) is measurement perturbation (gaussian noise), \\(\\mathbf{V_t}\\) is stochastic diffusion (covariance), \\(\\mathbf{C}\\) is a measurement transformation matrix that maps the state \\(X_t\\) to a corresponding observed measurement \\(Y_t\\). So then, we have the following: \\[ Y_t = \\underbrace{\\left[\\begin{array}{cc}1 &amp; 0 \\end{array}\\right]}_\\text{C} X_{t} + v_t \\] For Error in Estimate, we have: \\[\\begin{align} P_t = AP_{t-1}A^T + Q_t \\end{align}\\] where: A = Predicted State Transformation Matrix \\(P_t\\) = Process Covariance Matrix (Error in Predicted State Estimate) Q = Process Noise Covariance Matrix Assume we have obtained the following initial errors: \\[ \\underbrace{E^{P} = 16\\ m\\ \\ \\ \\ \\ \\ \\ E^{U} = 2.5\\ m/s}_\\text{Error in Estimate} \\] \\[where: \\ 16 \\times 16 = 256,\\ \\ \\ \\ 16 \\times 2.5 = 40,\\ \\ \\ \\ 2.5 \\times 2.5 = 6.25\\] \\[ \\underbrace{E^{X} = 15\\ m\\ \\ \\ \\ \\ \\ \\ E^{V}= 3.0\\ m/s}_\\text{Error in Measurement} \\] \\[where: \\ 15 \\times 15 = 225,\\ \\ \\ \\ 15 \\times 3.0 = 45,\\ \\ \\ \\ 3.0 \\times 3.0 = 9.00\\] So then, we have the following: \\[ P_t = \\left[\\begin{array}{rr}1 &amp; \\Delta t\\\\ 0 &amp; 1 \\end{array}\\right] \\left[\\begin{array}{rr}256 &amp; 0\\\\ 0 &amp; 6.25 \\end{array}\\right] \\left[\\begin{array}{rr}1 &amp; 0 \\\\ \\Delta t &amp; 1 \\end{array}\\right] + 0 = \\left[\\begin{array}{rr}262.25 &amp; 0\\\\ 0 &amp; 6.25 \\end{array}\\right] \\] \\[ where: \\Delta t = 1\\ and\\ Q_t = 0 \\] For Kalman Gain, we have: \\[\\begin{align} K = \\frac{P_t H}{HP_t H^T + R} \\end{align}\\] where: H = Identity matrix P = Process Covariance Matrix R = Sensor Noise Covariance Matrix So then, we have the following: \\[ K = \\frac{ \\left[\\begin{array}{rr}262.25 &amp; 0\\\\ 0 &amp; 6.25 \\end{array}\\right] \\left[\\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] }{ \\left[\\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] \\left[\\begin{array}{rr}262.25 &amp; 0\\\\ 0 &amp; 6.25 \\end{array}\\right] \\left[\\begin{array}{rr} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] + \\left[\\begin{array}{rr} 225 &amp; 0 \\\\ 0 &amp; 9 \\end{array} \\right] } = \\left[\\begin{array}{rr} 487.25 &amp; 0 \\\\ 0 &amp; 15.25 \\end{array} \\right] \\] For State Error Update, we have: \\[\\begin{align} P_t^* = (I - KH)P_t \\end{align}\\] where: \\(P^*_t\\) = Process Covariance Matrix (Update in Error in Estimate) I = Identity matrix K = Kalman Gain H = Identity matrix So then, we have the following: \\[ P_t^* = \\left( \\left[\\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] - \\left[\\begin{array}{rr} 487.25 &amp; 0 \\\\ 0 &amp; 15.25 \\end{array} \\right] \\left[\\begin{array}{cc} 1 &amp; 0 \\\\ 0 &amp; 1 \\end{array} \\right] \\right) \\left[\\begin{array}{rr}262.25 &amp; 0\\\\ 0 &amp; 6.25 \\end{array}\\right] \\] For State Estimate Update, we have: \\[\\begin{align} X_t^* = X_t + K(Y_t - HX_t) \\end{align}\\] where: \\(\\mathbf{X_t^*}\\) is Update in State estimate, \\(\\mathbf{Y_t}\\) is observed measurement, \\(\\mathbf{K}\\) is Kalman Gain, \\(\\mathbf{H}\\) is Identity Matrix (transformation). The Kalman Gain is a weight parameter to adjust the measurement error, namely \\((Y_t - HX_t)\\), thereby getting a state estimate closer to the actual state. The equation in Kalman Gain uses an H matrix which can be treated as an identity matrix. We preserve the updates for the estimate, including error measurements, for each iteration until all observations are processed. To illustrate, let us generate our models (movement and measurement models) using random walk. Here, we do not assume any control input; thus, the use of the B and U variables are ignored. n = 200 seed = 2020; set.seed(seed) x &lt;- seq(0, n, length.out = n) y &lt;- seq(2, 6, length.out = n) Y.e &lt;- seq(0, n, length.out = n) V = rnorm(n, 0, 0.50) # Simple 2 direction Random Walk for (i in 2:n) { set.seed(seed + i) y[i] = y[i-1] + runif(1, -0.5, 0.5) # true measurement Y.e[i] = y[i] + V[i] # observed measurement with noise } Below is the implementation of the functions in R code corresponding to the steps in Figure 8.21 above: predict.kalman &lt;- function(A,X,P,Q) { X = A * X P = A * P * t(A) + Q list(&quot;X&quot; = X, &quot;P&quot; = P) } update.kalman &lt;- function(Y, X, P, H, R) { I = 1 K = P * H / ( H * P * t(H) + R) X = X + K*(Y - H * X) P = (I - K*H) * P list(&quot;K&quot; = K, &quot;X&quot; = X, &quot;P&quot; = P) } Here is an example implementation of Kalman Filter in R code. Note that we reduce the variables to scalar instead of the matrices discussed above for the sake of intuition. Therefore, the identity and transformation matrices, namely H, R, Q, and A, are set to 1, including P (assuming constant control with stationary covariances): X.e = c() Q = 1; R = 1; A = 1; H = 1; P = 1 X = Y.e[1]; X.e = c(X) for (i in 2:n) { new.state = predict.kalman(A,X,P,Q) X = new.state$X; P = new.state$P current.state = update.kalman(Y.e[i], X, P, H, R) X = current.state$X P = current.state$P K = current.state$K X.e = cbind(X.e, X) } Try to play around with the R scalar value and observe the effect of the Kalman Gain. See Figure 8.22 for a plot of the random walk and the estimate. plot(NULL, xlim=range(0, n), ylim=range(0,10), xlab=&quot;T&quot;, ylab=&quot;Location&quot;, main=&quot;Kalman Filter&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(x, Y.e, col=c(&quot;grey&quot;), pch=16, cex=0.80) lines(x, y, col=c(&quot;darksalmon&quot;), lwd=3) lines(x, X.e, col=c(&quot;navyblue&quot;), lwd=3) legend(&quot;topleft&quot;, inset=.02, c(&quot;True Measurement&quot;,&quot;Estimate (KF)&quot;, &quot;Observation&quot;), col=c(&quot;darksalmon&quot;, &quot;navyblue&quot;, &quot;grey&quot;), horiz=FALSE, cex=0.8, lty=c(1,1, 0), lwd=c(2,2), pch=c(-1,-1, 16)) Figure 8.22: Kalman Filter We leave readers to investigate other variations or improvements of Kalman filter equations and algorithms. Apart from the same range of applications of Bayes Filter, other applications of Kalman filter are found in navigation tracking (e.g., GPS - global positioning system, IMU - inertial measurement unit). That includes object tracking (e.g., using radar to track moving targets), pod/drone landing, robotics, and econometrics with time-series and forecasting requirements. 8.1.8 Extended Kalman Filter Extended Kalman Filter (EKF) extends the concept of Kalman Filter by dealing with non-linear cases (e.g., non-gaussian distributions). In such cases, EFK transforms non-linearity into a locally linear model using Taylor series expansion around the mean up to a given order, e.g., first-order Taylor series. Let us use Figure 8.23. Figure 8.23: Extended Kalman Filter Our case starts with the assumption that the state X and measurement Y are non-linear; thus, we deal with non-linear functions. \\[\\begin{align} X_t = \\overbrace{A X_{t-1} + B U_t + W_t}^\\text{linear model}\\ \\ \\ {}&amp;\\rightarrow X_t = \\overbrace{f(X_{t-1}, U_t, W_t)}^\\text{non-linear model} \\\\ Y_t = C X_t + V_t \\ \\ \\ \\ &amp;\\rightarrow Y_t = g(X_t, V_t) \\end{align}\\] Both functions represent a first-order Taylor series expansion split into vectored functions of length m. To optimize the function, we perform partial derivatives and derive a jacobian matrix, effectively linearizing \\(X_t\\) and \\(Y_t\\) like so: \\[ F = \\left.\\frac{\\partial f}{\\partial x}\\right|_{X_{t-1}, U_t} = \\left[ \\begin{array}{cccc} \\frac{\\partial f_1}{\\partial x_1} &amp; \\frac{\\partial f_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_1}{\\partial x_n} \\\\ \\frac{\\partial f_2}{\\partial x_1} &amp; \\frac{\\partial f_2}{\\partial x_2} &amp; \\cdots &amp;\\frac{\\partial f_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial f_m}{\\partial x_1} &amp; \\frac{\\partial f_m}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial f_m}{\\partial x_n} \\\\ \\end{array} \\right]_{mxn} \\] \\[ G = \\left.\\frac{\\partial g}{\\partial x}\\right|_{X_t} = \\left[ \\begin{array}{cccc} \\frac{\\partial g_1}{\\partial x_1} &amp; \\frac{\\partial g_1}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial g_1}{\\partial x_n} \\\\ \\frac{\\partial g_2}{\\partial x_1} &amp; \\frac{\\partial g_2}{\\partial x_2} &amp; \\cdots &amp;\\frac{\\partial g_2}{\\partial x_n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\frac{\\partial g_m}{\\partial x_1} &amp; \\frac{\\partial g_m}{\\partial x_2} &amp; \\cdots &amp; \\frac{\\partial g_m}{\\partial x_n} \\\\ \\end{array} \\right]_{mxn} \\] After computing for the F and G jacobian matrices, we can now extend Kalman Filter by reformulating the steps: \\[ \\begin{array}{ll} \\mathbf{\\text{Kalman Filter}} &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{Extended Kalman Filter}}\\\\ \\text{==============} &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{=================}\\\\ \\begin{array}{ll} X_t &amp;= A X_{t-1} + B U_t + W_t \\\\ P_t &amp;= AP_{t-1}A^T + Q_t \\\\ K &amp;= \\frac{P_t H}{HP_t H^T + R} \\\\ P_t^* &amp;= (I - KH)P_t \\\\ X_t^* &amp;= X_t + K(Y_t - HX_t) \\\\ \\end{array} &amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{ll} X_t &amp;= f(X_{t-1}, U_t, W_t) \\\\ P_t &amp;= FP_{t-1}F^T + Q_t \\\\ K &amp;= \\frac{P_t G}{GP_t G^T + R} \\\\ P_t^* &amp;= (I - KG)P_t \\\\ X_t^* &amp;= X_t + K(Y_t - g(X_t, V_t)) \\\\ \\end{array} \\end{array} \\] The algorithm is the same as the original Kalman Filter, following the same prediction step and correction step as before. 8.1.9 Unscented Kalman Filter Unscented Kalman Filter (UKF) avoids linearization in the non-linear transformation; instead, it transforms a distribution into an approximate distribution by transforming a select set of data points into a set of transformed and weighted sigma points. For discussion, we follow van der Merwe R. et al (2013) and Stachniss C. (2013)]. See Figure 8.24. Figure 8.24: Unscented Kalman Filter Sigma Points Properties \\[\\begin{align} \\sum_i^n w_i = 1\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu = \\sum_i^n w_i x_i\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\Sigma = \\sum_i^n w_i (x_i - \\mu)(x_i - \\mu)^T \\end{align}\\] With the corresponding mean \\(\\mu\\) and covariance \\(\\Sigma\\), we should be able to recover the original gaussian distribution using the following formula: \\[\\begin{align} \\mu_0 = \\sum_{i=0}^{2p} w_i f(x_i)\\ \\ \\ \\ \\ \\Sigma_0 = \\sum_{i=0}^{2p} w_i ( f(x_i) - \\mu)( f(x_i) - \\mu)^T \\end{align}\\] Note that the function \\(f(x)\\) is an arbitrary non-linear function depending on the case; thus omits the required Taylor series expansion for EKF. In terms of selecting the candidate sigma points, we use the following Sigma points and Sigma weights: \\[\\begin{align} \\begin{array}{llllc} x_0 = \\mu &amp; &amp;\\ \\ \\ \\ &amp;\\omega_0^{(m)} = \\frac{\\lambda}{p + \\lambda} &amp; \\\\ x_i = \\mu + \\sqrt{(p + \\lambda)\\Sigma_x}_i &amp; i = 1,...,p &amp;\\ \\ \\ \\ &amp;\\omega_0^{(c)} = \\omega_0^{(m)} + (1 - \\alpha^2 + \\beta) &amp; \\\\ x_i = \\mu - \\sqrt{(p + \\lambda)\\Sigma_x}_{i-p} &amp; i = p+1,...,2p &amp;\\ \\ \\ \\ &amp;\\omega_i^{m} = \\omega_i^{(c)} = \\frac{1}{2(p + \\lambda)},\\ i = 1,...,2p \\end{array} \\label{eqn:eqnnumber333} \\end{align}\\] where: \\(\\Sigma_x\\) is the process-covariance matrix (see Cholesky Factorization for numerical computation), p is the p-dimensions, \\(\\alpha \\in (0,1)\\) (denoting spread of sigma points around the mean), \\(\\lambda = \\alpha^2(p + \\lambda) - p\\) (denoting degrees of freedom), \\(\\beta = 2\\) (2 being optimal). The UKF follows the same steps as KF and EKF like so: Prediction Step: \\[\\begin{align} X_{t-1} {}&amp;= (\\mu_{t-1}\\ \\ \\ \\ \\mu_{t-1} + \\lambda\\sqrt{\\Sigma_{t-1}}\\ \\ \\ \\ \\ \\mu_{t-1} - \\lambda\\sqrt{\\Sigma_{t-1}})\\\\ \\hat{X}_t &amp;= f(X_t, U_t, W_t)\\\\ \\bar{\\mu}_t &amp;= \\sum_{i=0}^{2p} w_i^{(m)} \\hat{X}_{t_i} \\\\ \\bar{\\Sigma}_t &amp;= \\sum_{i=1}^{2p} w_i^{(c)} ( \\hat{X}_{t_i} - \\bar{\\mu}_t) ( \\hat{X}_{t_i} - \\bar{\\mu}_t)^T + Q_t \\end{align}\\] Correction (Update) Step: \\[\\begin{align} \\bar{X}_t {}&amp;= (\\bar{\\mu}_t\\ \\ \\ \\ \\bar{\\mu}_t + \\lambda\\sqrt{\\bar{\\Sigma}_t}\\ \\ \\ \\ \\ \\bar{\\mu}_t - \\lambda\\sqrt{\\bar{\\Sigma}_t})\\\\ Y_t &amp;= g(\\bar{X}_t, V_t)\\\\ \\bar{y}_t &amp;= \\sum_{i=0}^{2p} w_i^{(m)} Y_{t_{i}}\\\\ S_t^{y,y} &amp;= \\sum_{i=0}^{2p} w_i^{(c)} ( Y_{t_{i}} - \\bar{y}_t) ( Y_{t_{i}} - \\bar{y}_t)^T + R_t\\\\ S_t^{x,y} &amp;= \\sum_{i=0}^{2p} w_i^{(c)} ( \\bar{X}_{t_{i}} - \\bar{\\mu}_t ) ( Y_{t_{i}} - \\bar{y}_t)^T \\\\ K_t &amp;= S_t^{x,y} (S_t^{y,y})^{-1}\\\\ \\mu_t &amp;= \\bar{\\mu}_t + K_t(y_t - \\bar{y}_t)\\\\ \\Sigma_t &amp;= \\bar{\\Sigma}_t - K_tS_t^{y,y} K_t^T \\end{align}\\] Similarly, we iterate over the two steps until all observations are accounted for. We then obtain the final result, namely \\(\\mu_t\\) and \\(\\Sigma_t\\), which we can use to generate the Gaussian distribution; in this case, it is an approximate distribution. 8.1.10 Particle Filter Particle Filter (PF) follows the same concept as UKF in that Sigma points are derived. However, the Sigma points are derived from an arbitrary sample in PF; rather than following the equations presented in the previous UKF discussion to select a candidate set of data points. Note that PF becomes more computationally expensive with larger samples though more flexible than the three Kalman filters discussed previously. In later sections, we discuss simulation and sampling techniques such as Markov Chain Monte Carlo (MCMC) and Metropolis-Hastings MC algorithms that can be used to support the random draws of the Sigma points. We leave readers to investigate Particle Filtersfurther. 8.1.11 Ensemble Kalman Filter Ensemble Kalman Filter (EnKF)uses Monte Carlo sampling to filter an ensemble of samples from a distribution. It is suitable for datasets with high dimensionality (high features). Similar to Particle Filter, EnKF uses multiple samples generated using Monte Carlo. However, the same steps apply as before with modifications in the equations to accommodate the datasets. We leave readers to investigate Ensemble Kalman Filterfurther. In addition, it helps to be aware of topics around Data Assimilation and Sensor Fusion and the use of multiple sensors such as Lidar, Infrared, and Radar for autonomous objects. 8.2 Simulation and Sampling This section discusses a few simulation and sampling techniques. Such need for simulation and sampling becomes apparent in situations where there are no practical means to obtain samples of an observation. Especially in models with very high dimensionality, such as lattice models in which we deal with thousands or even millions of variables (or features in machine learning), numerical computation may not be possible. Sampling data points from large datasets are ideal in this case. 8.2.1 Monte Carlo Estimation A concept behind Monte Carlo estimation is the Law of the Unconscious Statistician (LOTUS), which states that the expected value of a function can be calculated without depending on its distribution. That can be written as such: \\[\\begin{align} \\mathbb{E}\\left[f(X)\\right] = \\underbrace{\\int f(X) P_{(pdf)}(X) dX}_\\text{continuous} \\approx \\underbrace{\\sum_{i=1}^N f(X_i) P_{(pmf)}(X)}_\\text{discrete} \\end{align}\\] Here, we define the expected value as the sum of the product of the function (the transformed value) and the probability distribution (of the original value). The idea is to use this calculation to estimate the complex integration of a function; thus, this is also called Monte Carlo integration. \\[\\begin{align} \\mathcal{I}(f(x)) = \\mathbb{E}\\left[f(X)\\right] \\approx \\frac{1}{N} \\sum^N_{i=1} f(x_i) \\end{align}\\] To illustrate, suppose we have the following simple function: \\(f(X) = 3X\\). We then generate ten random outcomes between 1 and 5 for X that give us the calculated (transformed) values using \\(f(X)\\): set.seed(2020) f &lt;- function(X) { 3*X } N = 1000 possible.outcome = seq(1, 5) prob = c(0.10, 0.20, 0.30, 0.05, 0.35) X = sample(x = possible.outcome, size = N, replace=TRUE, prob=prob) Y = f(X) D = rbind(X[1:10], Y[1:10]) # show only the 10 values Df = data.frame(D) rownames(Df) = c(&quot;original (x)&quot;, &quot;transformed f(x)&quot;) Df ## X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 ## original (x) 3 3 3 3 5 5 5 3 5 3 ## transformed f(x) 9 9 9 9 15 15 15 9 15 9 hist(Y, breaks = 5, las=1, xlab=&quot;f(X)&quot;, cex.main = 1.55, cex.lab = 1, freq = TRUE, main=&quot;Monte Carlo Estimation&quot;, border=&quot;black&quot;, col=&quot;lightgrey&quot;) axis(1, seq(1,15,1) ) Figure 8.25: Monte Carlo Estimation Note that \\(f(X)\\) generates a new distribution different from the distribution of X. Now, suppose we know the probability distribution of our function: \\[ f(x) = (3,6,9,12,15)\\ \\ maps\\ to\\ \\ \\ \\ P\\left[f(x)\\right] = \\left( 0.10, 0.20, 0.30, 0.05, 0.35\\right). \\] In that case, we can calculate the expected value based on the known probabilities of all possible outcomes (of the function): \\[\\begin{align} I_q {}&amp;= f(x = 1)P\\left[f(x=1)\\right] + f(x = 2)P\\left[f(x=2)\\right] + f(x = 3)P\\left[f(x=3)\\right] + \\nonumber \\\\ &amp;f(x = 4)P\\left[f(x=4)\\right] + f(x = 5)P\\left[f(x=5)\\right]\\\\ &amp;= 3 \\times 0.10 + 6 \\times 0.20 + 9 \\times 0.30 + 12 \\times 0.05 + 15 \\times 0.35 \\nonumber \\\\ &amp;= 10.05 \\nonumber \\end{align}\\] Otherwise, if the probability distribution of the function is not known, then we use the probability distribution of the original values like so (here, we use uniform probabilities): \\[ X = (1,2,3,4,5)\\ \\ maps\\ to\\ \\ \\ \\ P\\left(X\\right) = \\left( \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5}, \\frac{1}{5} \\right). \\] so that, we then get the following expected value: \\[\\begin{align} I_q {}&amp;= f(x = 1)P\\left(x = 1\\right) + f(x = 2)P\\left(x = 2\\right) + f(x = 3)P\\left(x = 3\\right) + \\nonumber \\\\ &amp;f(x = 4)P\\left(x = 4\\right) + f(x = 5)P\\left(x = 5\\right)\\\\ &amp;= 3 \\times 0.20 + 6 \\times 0.20 + 9 \\times 0.20 + 12 \\times 0.20 + 15 \\times 0.20 \\nonumber \\\\ &amp;= 9.0 \\nonumber \\end{align}\\] Below is a simple implementation of Monte Carlo estimation in R code: Pf = prob # known probabilities of function E = 0 n = length(possible.outcome) for (i in 1:n) { E = E + f(possible.outcome[i]) * Pf[possible.outcome[i]] } print(paste0(&quot;Expected Value = &quot;, round(E,5))) ## [1] &quot;Expected Value = 10.05&quot; We further discuss using Monte Carlo estimation in the Importance sampling section. 8.2.2 Monte Carlo Simulation Monte Carlo simulation, also called Monte Carlo method and Monte Carlo sampling, offers the ability to draw random samples from a posterior distribution by simulating probabilities. The objective is to measure risk and uncertainty. To demonstrate Monte Carlo, we can use an R function called sample(.) to simulate multiple random tosses of a coin: data = c(&quot;H&quot;, &quot;T&quot;) sample(data, size=10, replace=TRUE) ## [1] &quot;T&quot; &quot;H&quot; &quot;H&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; &quot;T&quot; &quot;H&quot; Another popular function used in R to implement MC is the rnorm(.) which simulates generating random numbers, given a mean \\(\\mu = 0\\) and variance \\(\\sigma^2=1\\), like so: mu = 0; sigma = sqrt(1) rnorm(n=5, mean=mu, sd=sigma) ## [1] -0.8781 -0.4207 -2.6625 -1.6323 1.0070 A simple example of MC is a simulated random walk using the function rbinom(.). See Figure 8.26): random.walk &lt;- function(no_of_walks = 5) { set.seed(2020 ) direction = c(&quot;north&quot;, &quot;south&quot;, &quot;east&quot;, &quot;west&quot;) walk = rep(0, no_of_walks) rnd = rbinom(n = no_of_walks, size = 3, prob=0.50) + 1 for (i in 1:no_of_walks) { walk[i] = direction[rnd[i]] } walk } walk = random.walk(100) Figure 8.26: Random Walk (Monte Carlo) A more complex example of using Monte Carlo is studying autonomous vehicles, e.g., self-driving cars, especially around tracking multiple targets (Kim, D., Hong, S. 2013). One property of an autonomous moving vehicle is to detect surrounding objects and thus be able to avoid a collision ultimately. Therefore, the vehicle needs to monitor and measure the distance and velocity between the moving vehicle and multiple targets. The problem statement is to distinguish and map detected measurements with corresponding targets. There are six independent measurements generated from six sensors attached to an autonomous vehicle, namely Infrared Sensor, Ultrasonic Sensor, VideoCam, GPS, Lidar Sensor using light waves, and Radar Sensor using radio waves. Such sensor measurements are fed into the central computer for analysis. In particular, let us focus on linear Doppler shifts of light waves detected from an FMCW Lidar sensor. See Figure 8.27 showing two frequencies, namely a transmitted frequency from a moving vehicle and a reflected frequency from a stationary object straight up ahead. Figure 8.27: Triangular Chirp Modulation for an FMCW system The diagram demonstrates a triangular chirp (or beat) modulation for the two frequencies allowing a calculation of the distance and velocity of a moving source to a stationary target. To a moving object (being the source) and a stationary observer (being the target), classic radar sensors detect changes in wave frequency called Doppler shifts (denoted \\(\\mathbf{f_D}\\)). As for light-based sensors, Doppler frequency has the following equation: \\[\\begin{align} f_D = \\frac{2V_r f_o}{C} \\end{align}\\] where: \\(\\mathbf{V_r}\\) is relative Velocity, \\(\\mathbf{f_o}\\) is operating frequency of transmitted signal, C is a constant for speed of light, e.g. \\(3\\times 10^8\\) m/s. For example, suppose a moving vehicle has a speed of 30 m/s with a lidar signal transmitted at a frequency of 77 GHz. Therefore, the Doppler frequency is calculated to be: \\[ f_D = \\frac{2 (30) (77\\times 10^9)} {(3\\times 10^8)} = 15.4\\ \\text{kHz} \\] Now, suppose our goal is to prevent a collision by allowing a moving vehicle to autonomously slow down to a stop at a safe stopping distance from multiple stationary targets detected straight ahead. For that, we need to measure the distance and velocity of the moving object using the following series of equations and their derivations based on Figure 8.27: \\[\\begin{align} \\begin{array}{lllll} \\tau &amp;= \\frac{2R}{C}&amp;\\ \\ \\ \\ &amp;f_b &amp;= (\\tau)\\frac{B}{T_s} (slope = \\frac{rise}{run})\\\\ f_{bu} &amp;= f_b - f_D&amp;\\ \\ \\ \\ &amp;f_{bd} &amp;= f_b + f_D\\\\ R &amp;= \\frac{CT_s}{2B} \\times\\frac{(f_{bd} + f_{bu})}{2}&amp;\\ \\ \\ \\ &amp;V_r &amp;= \\frac{C}{2 f_o}\\times\\frac{(f_{bd} - f_{bu})}{2} \\end{array} \\label{eqn:eqnnumber334} \\end{align}\\] where: C is a constant for speed of light, e.g. \\(3\\times 10^8\\) m/s, B is chirp (triangular modulation) bandwidth, \\(\\mathbf{T_s}\\) is one chirp sweep period (wave period), \\(\\tau\\) is time delay, \\(\\mathbf{f_o}\\) is operating frequency of transmitted signal, \\(\\mathbf{f_b}\\) is frequency of beat (chirp rate), \\(\\mathbf{f_{bu}}\\) is frequency of beat (up=ramp rate of chirp), \\(\\mathbf{f_{bd}}\\) is frequency of beat (down-ramp rate of chirp), \\(\\mathbf{f_D}\\) is Doppler frequency shift, R is range (distance) from source to target, \\(\\mathbf{V_r}\\) is velocity of moving source. Note that measurements of Doppler shifts may come with distortion, leakage, biases, and ambiguity. Thus, with such uncertainty, our goal is to improve the accuracy of the sensors by calibration. One approach is to simulate sampling of multiple measurements across a wide range of targets positioned in different locations and distances and possibly inducing a wide range of distortions, leakage, biases, and ambiguity. That is where Monte Carlo comes into play. While we may not illustrate the case in this book, we leave readers to investigate this simulation. Notice that our discussion of Monte Carlo illustrates random sampling; however, in bayesian sense, we draw random samples from a posterior distribution. In the case of a Normal distribution, we use its sufficient statistics, namely \\(\\mu\\) and \\(\\sigma^2\\), to help generate the random sample. 8.2.3 Markov Chain Monte Carlo Markov Chain Monte Carlo (MCMC) is a Monte Carlo technique that draws random sampling in which each drawn sample is conditionally dependent upon the recent previous drawn sample. This dependency property is one of Markov Chain properties (suppl refs: Robert C., George Casella G. 2008; Speagle J. S. 2020). Unlike the previous random walk example, let us modify our random walk to illustrate MCMC. Here, let us introduce a table of transition probabilities so that if our next random step makes a transition to a greater probability than that of our recent previous step, then we take that step; otherwise, we use the direction of the previous step for our next step. Here, we only use three movements (F = walk forward, L = walk left, R = walk right). See Figure 8.28. Figure 8.28: MCMC Model Here is a sample R code for the MCMC random walk: random.walk &lt;- function(no_of_walks = 5) { set.seed(123) transition = matrix(c(0.10, 0.45, 0.45, 0.45, 0.10, 0.45, 0.45, 0.45, 0.10), nrow=3, byrow=TRUE) direction = c(&quot;F&quot;, &quot;L&quot;, &quot;R&quot;) walk = rep(0, no_of_walks) seq.of.steps = rbinom(n = no_of_walks, size = 2, prob=0.50) + 1 prev.prob = 0 for (i in 1:no_of_walks) { step = direction[seq.of.steps [i]] if (i &gt; 1) { previous.step = walk[i-1] previous.idx = which(direction==previous.step) probability = transition[previous.idx, seq.of.steps[i]] if (previous.step != step) { if (prev.prob &lt;= probability) { prev.prob = probability } else { step = previous.step } } } walk[i] = step } walk } walk = random.walk(100) head(walk) ## [1] &quot;L&quot; &quot;R&quot; &quot;L&quot; &quot;R&quot; &quot;R&quot; &quot;F&quot; Figure 8.29 shows the revised random walk. Figure 8.29: Random Walk (MCMC) 8.2.4 Metropolis-Hastings Monte Carlo Metropolis-Hastings Monte Carlo (MHMC) is an MCMC technique so that given an approximation (proposal) distribution, namely \\(\\mathcal{Q}(x^*|x)\\), the goal is to refine the distribution until such that it settles into a target (equilibrium) distribution, namely \\(\\pi(x)\\). While MHMC may best suit distributions that do not rather fall under the common list of distributions, let us use Gaussian distribution as a case in our discussion for simplicity and illustration, but only up to the constant (meaning, we ignore the normalizer). This section includes supplemental references: (C.P. Robert 2016; Burke N. 2018). \\[\\begin{align} \\mathcal{Q}(x^*|x) \\propto \\left\\{ \\pi(x; \\mu, \\sigma^2)= exp \\left(-\\frac{(x - \\mu)^2}{2 \\sigma^2}\\right) \\right\\} \\end{align}\\] where \\(x^*\\) is a proposed candidate data point. MHMC has the following algorithm: \\[ \\begin{array}{l} \\text{Initialize the first data point, e.g. }x_0 = \\text{&lt;an arbitrary value&gt;}\\\\ \\text{For i in 1,...,n, repeat the following:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Propose a candidate by drawing a sample from }\\mathcal{Q}(x^*|x)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x^* \\sim \\mathcal{Q}(x^*|x_{i-1})\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Calculate the Acceptance Probability (A)}:\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{A = min}\\left(\\frac{\\mathcal{Q}(x_{i-1}|x^*)\\pi(x^*)}{\\mathcal{Q}(x^*|x_{i-1})\\pi(x_{i-1})}, 1\\right)}_\\text{asymmetric} \\ \\ \\ \\ \\ \\ or \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{A = min}\\left(\\frac{\\pi(x^*)}{\\pi(x_{i-1})}, 1\\right)}_\\text{symmetric}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Draw a random number u and campare with A}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ u \\sim \\text{Uniform}(0,1)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{If u &lt; A, we accept the proposal}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x_i = x^*\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Otherwise, we reject:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x_i = x_{i-1}\\\\ \\text{return x where x = }(x_1, x_2,...) \\end{array} \\] Note that \\(\\frac{\\mathcal{Q}(x_{i-1}|x^*)}{\\mathcal{Q}(x^*|x_{i-1})}\\) is called the Hasting ratio. If the ratio is 1, it means that the distribution is symmetric; thus, we can use the symmetric version of the acceptance probability equation. The proposed candidate uses the previous sample value with added noise (perturbation) so that, in our case, we have the following: \\[\\begin{align} x^* = x_{i-1} + \\mathcal{N}(0, 1) \\end{align}\\] Or in the case of simulating a random walk for MHMC, we also can use: \\[\\begin{align} x^* = x_{i-1} + \\mathcal{U}(x_{i-1} - 1, x_{i-1} + 1) \\end{align}\\] Below is a simple example of MHMC implementation in R code (see Figure 8.30). set.seed(2020) kernel &lt;- function(x) { # Gaussian Kernel mu = 0; sd = 1 exp(-(x - mu)^2/ (2 * sd)) } pi.func &lt;- function(x) { kernel(x) } metropolis.hasting &lt;- function(n) { # symmetric x = rep(0,n) for (i in 2:n) { previous.x = x[i-1] proposed.x = previous.x + rnorm(n=1, mean=0, sd=1) A = min( pi.func(proposed.x)/pi.func(previous.x), 1) u = runif(n = 1, min=0, max=1) if ( u &gt; A) { proposed.x = previous.x } x[i] = proposed.x } x } plot.metropolis &lt;- function(n) { plot(NULL, xlim=range(-4,4), ylim=range(0,0.8), xlab=&quot;x&quot;, ylab=&quot;density&quot;, main=paste0(&quot;MHCM (n=&quot;, n, &quot;)&quot;), frame=TRUE) grid() x = seq(-5, 5, length.out=100) curve(dnorm(x, 0, 1), col=&quot;darksalmon&quot;, lwd=1, add=TRUE) pt = density(metropolis.hasting(n)) lines(pt$x, pt$y, col=&quot;navyblue&quot;, lwd=2, lty=2) } par(mfrow=c(2,2)) plot.metropolis(n=10) plot.metropolis(n=50) plot.metropolis(n=500) plot.metropolis(n=10000) Figure 8.30: Metropolis-Hastings MC (MHMC) 8.2.5 Hamiltonian Monte Carlo Hamiltonian Monte Carlo (HMC) algorithm, also called Hybrid Monte Carlo, extends the concept of the Metropolis-Hastings algorithm (Tianqi Chen et al. 2014). We start with the simple Monte Carlo concept of operating on a posterior distribution, given by the notation \\(\\mathcal{Q}(q) \\propto P(q|D)\\) where q is a latent variable for which we simulate drawing samples from D observations. We then introduce an auxiliary variable, namely p, so that we form a joint distribution like so: \\[\\begin{align} \\mathcal{Q}(q, p) = \\mathcal{Q}(q|p)\\mathcal{Q}(p)\\ \\ \\ \\ \\ \\leftarrow \\text{(chain rule)} \\end{align}\\] Here, HMC uses the Hamiltonian dynamics in physics to describe the conservation of energy so that our joint distribution adapts to the Hamiltonian equation below: \\[\\begin{align} \\mathcal{Q}(q, p) \\propto exp^{-H(q,p)} \\end{align}\\] The idea is to decompose the equation above into kinetic (kinematic) energy and potential energy functions so that we then have the following: \\[\\begin{align} \\mathcal{H}(q, p) {}&amp;= - \\log_e \\mathcal{Q}(q,p) \\\\ &amp;= \\underbrace{ \\underbrace{-\\log_e \\mathcal{Q}(p|q)}_\\text{Kinetic Energy}}_\\text{K(p)} - \\underbrace{ \\underbrace{\\log_e \\mathcal{Q}(q)}_\\text{Potential Energy}}_\\text{U(q)} \\end{align}\\] where: \\(\\mathbf{H(q,p)}\\) is the total energy of the system, \\(\\mathbf{U(q)}\\) is the potential energy (PE) function, \\(\\mathbf{K(p)}\\) is the kinetic energy (KE) function, q is the position state, described in terms of d-dimensional vector, p is the momentum state, described in terms of d-dimensional vector. The dynamics of particles in motion are described by a Hamiltonian system in which particles flow through a landscape of high-dimensionality, governed by forces of kinetic and potential energies. Such particles are sampled and approximated in terms of their position and momentum, for which the potential energy depends only on position (q) and the kinetic energy depends only on the momentum (p), expressed mathematically as so: \\[\\begin{align} \\underbrace{ U(q) = \\frac{q^2}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ K(p) = \\frac{p^2}{2}}_\\text{univariate} \\ \\ \\ or \\ \\ \\ \\ \\ \\underbrace{ U(q) = \\frac{q\\Sigma^{-1}q}{2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ K(p) = \\frac{p \\cdot p}{2}}_\\text{multivariate} \\end{align}\\] where \\(\\Sigma^{-1}\\) is a covariance matrix for a gaussian distribution - assuming some rough perturbation. The Hamiltonian functions are implemented as such: U &lt;- function(q) { d = 2 # d-dimension Sigma = matrix(c(1, 0.8, 0.8, 1), nrow=d, byrow=TRUE) sum( (q %*% Sigma^-1) * q ) / 2 } K &lt;- function(p) { sum( p * p) / 2 } H &lt;- function(q,p) { U(q) + K(p) } MH.correction &lt;- function(q, p, q.new, p.new) { exp(H(q, p) - H(q.new, p.new)) } Changes in position and momentum that occur over time are described in terms of the following Hamiltonian equations: \\[\\begin{align} \\frac{dq}{dt} = +\\frac{\\partial H}{\\partial p} = +\\frac{\\partial K}{\\partial p} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{dp}{dt} = - \\frac{\\partial H}{\\partial q} = -\\frac{\\partial K}{\\partial q} - \\frac{\\partial U}{\\partial q} \\end{align}\\] The idea is to calculate the gradient of the Hamiltonian function - the direction towards the next state (next position and momentum): \\[\\begin{align} (q_{t+1}, p_{t+1}) = (q_t, p_t) + \\epsilon \\times \\nabla \\mathcal{H}(q_t, p_t) \\end{align}\\] Alternatively in HMC, we use the LeapFrog algorithm as outlined below (Wei-Lun Chao 2015; Mohammed Alfaki M. 2008): \\[ \\begin{array}{l} p = p - \\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q)\\\\ \\text{for i in 1,...,L},\\ \\ \\ \\text{repeat the following}:\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{l} q = q + \\epsilon M^{-1} p\\\\ if\\ (i\\ne L)\\ p = p- \\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q)\\\\ \\end{array}\\\\ p = p - \\frac{\\epsilon}{2}\\frac{\\partial U}{\\partial q}(q)\\\\ (q^*, p^*) = (q^{(L)}, p^{(L)}) \\end{array} \\] where \\(M^{-1}\\) serves as a mass matrix (usually a covariance/identity matrix) and epsilon \\(\\epsilon\\) is a time interval (stepsize). Because time is continuous, we have to discretize time to simulate Hamiltonian dynamics using a time interval value as stepsize denoted by \\(\\epsilon\\). The LeapFrog algorithm is implemented in R code like so: library(pracma) U.gradient &lt;- jacobian leap.frog &lt;- function(q, p, e = 0.10, L=10) { p = p - e/2 * U.gradient(U,q) for (i in 1:L) { q = q + e * p if (i != L) { p = p - e/2 * U.gradient(U,q) } } p = p - e/2 * U.gradient(U,q) list(&quot;q&quot; = q, &quot;p&quot; = p) } With all that, HMC has the following algorithm: \\[ \\begin{array}{l} \\text{Initialize the position state } q_0 \\text{ drawn from } q \\sim \\mathcal{N}(0, \\Sigma^{-1})\\\\ \\text{For t in 1,2,... repeat the following:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Propose a candidate momentum by drawing a sample from }\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{Q}(p_{t-1}) \\text{ - the canonical distribution}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ p^* \\sim \\mathcal{N}(0, M) \\leftarrow \\mathcal{Q}(p_{t-1}) \\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Perform LeapFrog Algorithm}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (q^*, p^*) = LeapFrog(q_{t-1}, p_{t-1}, \\epsilon, L=5)\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Calculate the Acceptance Probability (A) - (}\\mathbf{\\text{MH}} \\text{ correction):}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{A = min}\\left(exp\\left[H(q_{t-1},p_{t-1}) - H(q^*, p^*)\\right], 1\\right) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ u \\sim \\text{Uniform}(0,1)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{If u &lt; A, we accept the proposal}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ q_t = q^*\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Otherwise, we reject:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ q_t = q_{t-1}\\\\ \\text{return q where q = }(q_1, q_2,...) \\end{array} \\] Here is an example implementation of HMC in R code for a bivariate normal case (motivated by a python implementation by Gergely-Flamich): library(MASS) set.seed(10) HMC &lt;- function(N = 300, L = 10, epsilon = 0.10) { d = 2 # Dimension current.q = rep(0, d) q = matrix(0, nrow = N, ncol=d, byrow=TRUE) for (t in 2:N) { current.p = rnorm(n = d, mean = 0, sd = 1) state = leap.frog(current.q, current.p , e = epsilon, L = L) proposed.q = state$q proposed.p = state$p A = min( MH.correction(current.q, current.p, proposed.q, proposed.p), 1) u = runif(n = 1, min=0, max=1) if ( u &gt; A) { proposed.q = current.q } q[t,] = proposed.q } q } N = 300; L = 10; epsilon = 0.10 q = HMC(N, L, epsilon) Figure 8.31 shows the left-side plot of only 20 data points using HMM random walk. The right-side plot shows the complete sampling from the HMM random walk. library(KernSmooth) data = cbind(q[,1], q[,2]) kde.grid = bkde2D(x = data, bandwidth=3) par(mfrow=c(1,2)) contour(kde.grid$x1, kde.grid$x2, kde.grid$fhat, xlim=range(-2.5, 2.5), ylim=range(-3.5, 3.5), xlab=&quot;q&quot;, ylab=&quot;p&quot;, col = &quot;darksalmon&quot;, main=paste0(&quot;HMM (Random Walk)&quot;) ) x1 = 0; x2 = 0 points(x1, x2, pch=20, col=&quot;red&quot;) for (i in 1:20) { x = q[i,1] - x1; y = q[i,2] - x2 segments(x1, x2, x1 + x, x2, col=&quot;deepskyblue&quot;) segments(x1 + x, x2, x1 + x, x2 + y, col=&quot;deepskyblue&quot;) x1 = q[i,1]; x2 = q[i,2] points(x1, x2, pch=20) } contour(kde.grid$x1, kde.grid$x2, kde.grid$fhat, xlim=range(-2.5, 2.5), ylim=range(-3.5, 3.5), xlab=&quot;q&quot;, ylab=&quot;p&quot;, col = &quot;darksalmon&quot;, main=paste0(&quot;HMM Sampling (Bivariate)&quot;) ) points(q[,1], q[,2], pch=4) Figure 8.31: Hamiltonian Monte Carlo (HMC) We leave readers to investigate other enhancements to the HMC algorithm, for example, tuning the stepsize \\(\\epsilon\\). Also, investigate MCMC for Lagrangian Dynamics. 8.2.6 Gibbs Sampling Gibbs Sampler algorithm is a variant of the Metropolis-Hastings MCMC algorithm in that instead of performing an acceptance-rejection evaluation, proposed probabilities are always accepted (Burke N. 2018). The algorithm operates on conditional probabilities such that we estimate the distribution of one random variable conditioned on all other random variables so that given the following (for a case with p dimensions), \\[\\begin{align} X = (X_1, X_2,...,X_p) \\end{align}\\] the goal is to generate a random sample from the following joint distribution: \\[\\begin{align} P(X_i|X_1,X_2,...,X_p) = \\frac{P(X_1,X_2,...,X_p)}{\\int P(X_1,X_2,...,X_p) d_{X_i}} \\end{align}\\] Here, Gibbs sampler simplifies the calculation by handling individual random variables like so: \\[\\begin{align} \\begin{array}{lll} X_1^{(k+1)} &amp;\\sim P(X_1| X_{\\backslash 1 }^{(k)} ) &amp;\\equiv P(X_1|X_2^{(k)}, X_3^{(k)},...,X_p^{(k)})\\\\ X_2^{(k+1)} &amp;\\sim P(X_2| X_1^{(k+1)} , X_{\\backslash 2 }^{(k)} ) &amp;\\equiv P(X_2|X_1^{(k+1)}, X_3^{(k)},...,X_p^{(k)})\\\\ X_3^{(k+1)} &amp;\\sim P(X_3| X_1^{(k+1)}, X_2^{(k+1)}, X_{\\backslash 3 }^{(k)} ) &amp;\\equiv P(X_3|X_1^{(k+1)}, X_2^{(k+1)},X_4,...,X_p^{(k)})\\\\ \\vdots&amp;\\vdots&amp;\\vdots \\nonumber \\\\ X_p^{(k+1)} &amp;\\sim P(X_p| X_{\\backslash p }^{(k+1)} ) &amp;\\equiv P(X_p|X_1^{(k+1)}, X_2^{(k+1)},...,X_{p-1}^{(k+1)})\\\\ \\end{array} \\end{align}\\] In general, Gibbs Sampling has the following algorithm: \\[ \\begin{array}{l} \\text{Initialize the variables } X\\ where\\ X = (X_1, X_2, ..., X_p)\\\\ \\text{For k in 1,2,... repeat the following:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{For i in 1,...,p repeat the following:}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Sample }\\ X_i\\text{, given all variables except the ith X variable}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ X_i^{k+1} = P\\left(X_i|X_{\\backslash i}^{(k) },X_{\\backslash i}^{(k+1)}\\right)\\\\ \\text{return X where X = }(X_1, X_2,..., X_p) \\end{array} \\] Below is a naive implementation of Gibbs Sampler in R code using only two random variables. Note that Gibbs sampling for posterior distribution is discussed in JAGS modeling in a section ahead: set.seed(2020) gibbs.sampler &lt;- function(N, mu0 = 1, mu1 = 1, scale = 0.8) { x1 = rep(0, N); x2 = rep(0, N) x1[1] = mu0; x2[1] = mu1 for (k in 1:N) { x1[k] = rnorm(n=1, mean = x2[k], sd = sqrt(1 - scale^2)) x2[k] = rnorm(n=1, mean = x1[k], sd = sqrt(1 - scale^2)) } list(&quot;x1&quot; = x1, &quot;x2&quot; = x2) } N = 200 X = gibbs.sampler(N) par(mfrow=c(1,2)) plot(NULL, xlim=range(-2,2), ylim=range(-2,2), xlab=&quot;x1&quot;, ylab=&quot;x2&quot;, main=paste0(&quot;Gibbs Sampler (Random Walk)&quot;), frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) x1 = 0; x2 = 0 points(x1, x2, pch=20, col=&quot;red&quot;) for (i in 1:20) { x = X$x1[i] - x1; y = X$x2[i] - x2 segments(x1, x2, x1 + x, x2, col=&quot;deepskyblue&quot;) segments(x1 + x, x2, x1 + x, x2 + y, col=&quot;deepskyblue&quot;) x1 = X$x1[i]; x2 = X$x2[i] points(x1, x2, pch=20) } plot(NULL, xlim=range(-2,2), ylim=range(-2,2), xlab=&quot;x1&quot;, ylab=&quot;x2&quot;, main=paste0(&quot;Gibbs Sampler (Bivariate)&quot;), frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(X$x1, X$x2, pch=20) Figure 8.32: Gibbs Sampler We leave readers to investigate blocked Gibbs sampling. The idea is to operate on groups (called blocks) of random variables instead of individual variables. Also, other important control properties for Gibbs Sampling are Burn-in and Thinning. The Burn-in or Warm-up* is discussed under JAGS modeling section further ahead. Thinning - which skips samples - is discussed under Autocorrelation section. 8.2.7 Importance Sampling Our discussion on Importance sampling focuses on the expectation of an arbitrary function, namely \\(f(x)\\), as a point of comparison between calculating the simple average of its outcome with respect to a given target density, namely \\(P(x)\\), and the weighted average of its outcome with respect to a chosen approximating density, namely \\(\\mathcal{Q}(x)\\). Here, the premise is to find an approximating distribution that we can use to sample when the target distribution is difficult to sample easily. For example, sometimes, the tail of a Normal distribution matters. There is a possibility that that important region may not have enough data points to sample; thus, it may be necessary to propose a distribution representing that critical region of the distribution. From there, we simulate sampling from the newly proposed distribution - this is called importance sampling (Bugallo M.F. et al. 2017; Elvira V. and Martino L. 2021). To illustrate Importance sampling, recall the Law of Unconscious Statistician in the Monte Carlo estimation section. We start by using the following integral of the distribution in question: \\[\\begin{align} \\begin{array}{ll} \\mathcal{I}_p = \\int f(x) P(x) dx &amp;\\ \\ \\ \\ \\ \\ \\ \\ where\\ \\mathcal{I}_p = \\mathbb{E}_p\\left[f(x)\\right]\\\\ \\end{array} \\label{eqn:eqnnumber335} \\end{align}\\] Here, we prescribe what we call an importance weight or a likelihood ratio by introducing an approximating distribution like so: \\[\\begin{align} \\begin{array}{ll} \\mathcal{I}_q = \\int f(x) P(x) \\frac{\\mathbf{Q}(x)}{\\mathbf{Q}(x)} dx &amp;\\ \\ \\ \\ \\ \\ \\ \\ where\\ \\mathcal{I}_q = \\mathbb{E}_q\\left[\\frac{f(x)P(x)}{\\mathcal{Q}(x)}\\right] \\end{array} \\label{eqn:eqnnumber336} \\end{align}\\] The prescribed importance weight or likelihood ratio is written as: \\[\\begin{align} \\mathcal{I}_q = \\int f(x)\\mathcal{Q}(x) \\omega(x) dx\\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\omega(x) = \\frac{P(x)}{\\mathcal{Q}(x)}\\ \\ \\text{(importance weight)} \\end{align}\\] This ratio bears weight to the region of interest. Note that choosing a distribution for \\(\\mathcal{Q}(x)\\) is arbitrary with no hard rules. However, ideally, it may be best to choose one that closely matches \\(P(x)\\) or \\(| f(x) | \\mathcal{\\omega}(x)\\). To illustrate, let us define and implement the three functions, namely f(x), \\(\\mathbf{P}(x)\\), and \\(\\mathbf{\\mathcal{Q}}(x)\\): mu1 = 4; mu2 =6 f &lt;- function(X, rate = 0.4) { # f(x) dexp(X, rate = rate) # arbitrary function } P.normal.pdf &lt;- function(X, mu = mu1, sd = 1, log=FALSE) { # P(x) dnorm(X, mean = mu, sd=sd, log = log) } Q.normal.pdf &lt;- function(X, mu = mu2, sd = 1, log=FALSE) { # Q(x) dnorm(X, mean = mu, sd=sd, log = log) } N = 1000 X = seq(0, 10, length.out = N) P = P.normal.pdf(X, mu = mu1, sd = 1) Q = Q.normal.pdf(X, mu = mu2, sd = 1) G = f(X, rate = 0.4) See Figure 8.33 for the plot of \\(f(x) \\sim Expo(x)\\), \\(\\mathbf{P}(x) \\sim \\mathcal{N}(4, 1)\\), and \\(\\mathbf{\\mathcal{Q}}(x) \\sim \\mathcal{N}(6, 1)\\). Figure 8.33: Importance Sampling Now, let us evaluate two calculations (the simple average and weighted average): set.seed(2020) simple.avarage &lt;- function(N) { E = rep(0, N) for (i in 1:N) { X = rnorm(n = 1, mean = mu1, sd = 1) E[i] = f(X, rate=0.4) } sum(E) / N } importance.sampling &lt;- function(N) { E = rep(0, N) for (i in 1:N) { X = rnorm(n = 1, mean = mu2, sd = 1) W = exp(P.normal.pdf(X, mu=mu1, log=TRUE) - Q.normal.pdf(X, mu=mu2, log=TRUE)) E[i] = f(X, rate=0.4) * W } sum(E) / N } c(&quot;Simple Average&quot; = simple.avarage(N), &quot;Weighted Average&quot; = importance.sampling (N) ) ## Simple Average Weighted Average ## 0.08889 0.09341 We leave readers to investigate Simulated Annealing and Annealed Importance Sampling (AIS). 8.2.8 Rejection Sampling Rejection Sampling, also called Acceptance-Rejection Sampling, introduces the concept of an envelope distribution (our proposal distribution) (Erraqabi A. et al. 2016; Gilks, W. R., &amp; Wild, P. 1992). The idea is to scale (or stretch) an approximating distribution, namely \\(\\mathcal{Q}(x)\\), such that it is tall and wide enough to envelope or cover the entire target distribution, \\(P(x)\\). It is natural and perhaps intuitive to use uniform distribution for our envelope distribution that is scaled to cover the target distribution; however, we use a normal distributionin our example. See Figure 8.34 for the plot of the Rejection Sampling. To illustrate, let us implement a univariate bimodal mixture distribution for our target distribution using a function called dnorml.mixt(.) from a third-party library called ks. Our bimodal mixture has an equal proportion. \\[\\begin{align} P(x) \\sim \\mathcal{N}(\\mu=25, \\sigma = 2 ) + \\mathcal{N}(\\mu=35, \\sigma = 4 ) \\end{align}\\] We then can choose any approximating distribution that closely match our target distribution. Here, we choose Gaussian distribution: \\[\\begin{align} \\mathcal{Q}(x) \\sim \\mathcal{N}(\\mu = 30, \\sigma = 6) \\end{align}\\] Next, we compute for the scale factor denoted as k: \\[\\begin{align} k = max\\left(\\frac{P(x)}{\\mathcal{Q}(x)}\\right) \\end{align}\\] Therefore, our envelope distribution is generated using the following equation - represented by an envelope function, \\(Q^*(x)\\): \\[\\begin{align} Q^*(x) = k \\times \\mathcal{Q}(x) \\end{align}\\] We then generate sufficient samples from a uniform distribution with a height equal to our envelope distribution. \\[\\begin{align} U \\sim \\mathcal{U}(N=1000, 0, max(Q^*{(x)})) \\end{align}\\] We then use the uniformly generated sample to accept data points as our primary sample with a height below our target distribution. \\[ if\\ U \\le P(x)\\ \\ \\ \\text{accept region below or along target density} \\] Additionally, we also can sample the region below the envelope density and above the target density for our plotting purpose: \\[ if\\ U &lt; \\mathcal{Q}^*(x)\\ and\\ U &gt; P(x)\\ \\ \\ \\begin{cases} \\begin{array}{l}\\text{accept region below envelope density}\\\\ \\text{and above the target density} \\end{array} \\end{cases} \\] Below is a simple implementation of Rejection sampling. library(ks) ## ## Attaching package: &#39;ks&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## kde set.seed(2020) P.normal.pdf &lt;- function(x) { # P(x) y = dnorm.mixt(x, mus=c(25,35), sigmas=c(2, 4), props=c(0.5, 0.5)) list(&quot;y&quot; = y) } Q.normal.pdf &lt;- function(x) { # Q(x) y = dnorm.mixt(x, mus=c(30), sigmas=c(6), props=c(1)) list(&quot;y&quot; = y) } rejection.sampling &lt;- function(X, N = 500) { target = P.normal.pdf(X) approximate = Q.normal.pdf(X) # compute for the scale factor k = max ( exp( log( target$y, 2) - log( approximate$y, 2)) ) envelop = approximate$y * k # generate sufficient samples xu1 = runif(N, min=15, max=45) yu1 = runif(N, min=0, max= max (envelop)) # density from target distribution p.density = P.normal.pdf(xu1)$y # density from envelope distribution q.density = Q.normal.pdf(xu1)$y * k # accept region below target density accept1 = which(yu1 &lt;= p.density) # accept region using envelope density accept2 = which(yu1 &lt; q.density &amp; yu1 &gt; p.density) list(&quot;P.x&quot; = xu1[accept1], &quot;P.y&quot; = yu1[accept1], &quot;Q.x&quot; = xu1[accept2], &quot;Q.y&quot; = yu1[accept2], &quot;envelop&quot; = envelop, &quot;approximate&quot; = approximate$y, &quot;k&quot; = k, &quot;target&quot; = target$y) } N = 500 X =seq(0, 45, length.out = N) RS = rejection.sampling (X, N ) Below is the plot of our final sampling (data points denoted by P.x and P.y): plot(NULL, xlim=range(15, 45), ylim=range(-0.01, 0.25), xlab=&quot;X&quot;, ylab=&quot;Y&quot;, lwd=2, col = &quot;red&quot;, main=paste0(&quot;Rejection Sampling&quot;), frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) points(RS$Q.x, RS$Q.y, pch=4, col=&quot;grey&quot;) points(RS$P.x, RS$P.y, pch=20, col=&quot;seagreen&quot;) lines(X, RS$target, col=&quot;seagreen&quot;, lwd=2) lines(X, RS$approximate, col=&quot;darksalmon&quot;, lwd=2, lty=2) lines(X, RS$envelop, col=&quot;navyblue&quot;, lwd=2 ) max.y1 = max(RS$approximate) max.y2 = max(RS$envelop) arrows(30, max.y1, 30, max.y2, col=&quot;blue&quot;) text(31.2, 0.15, label=&quot;scale&quot;) text(31.4, 0.13, label=paste0(&quot;by &quot;, round(RS$k,1))) text(30, -0.01, label=&quot;accept region&quot;, col=&quot;seagreen&quot;) text(30, 0.10, label=&quot;reject region&quot;, col=&quot;navyblue&quot;) legend(34, 0.25, legend=c( &quot;P(x) - Target Dist&quot;, &quot;Q(x) - Approx Dist&quot;, &quot;Envelope Dist&quot;), col=c(&quot;seagreen&quot;, &quot;darksalmon&quot;, &quot;navyblue&quot;), lty=c(1,3,1), lwd=2, cex=0.8) Figure 8.34: Rejection Sampling Below are other methods of sampling that are worth investigating: Grid Sampling Bridge Sampling Nested Sampling Stepping Stone Sampling Path Sampling Dependent Sampling Independent Sampling Slice Sampling We leave readers to investigate the Adaptive Rejection Sampling (ARS), which uses both envelope function and squeezing function to constraint the approximating function to upper and lower bounds (Gilks, W. R., &amp; Wild, P. 1992). 8.2.9 JAGS Modeling We extend the discussion around flat prior and hyperparameter by introducing a modeling technique called JAGS modeling relevant to our choice of prior and hyperparameters (Plummer M. 2003; Kruschke J.K. 2015). We introduce an R package called rjags in this section. JAGS stands for Just Another GIBBS Sampler. There are three overlapping concepts when dealing with JAGS: GIBBS sampling, Markov Chain Monte Carlo (MCMC) simulation, and JAGS modeling. This section focuses on JAGS modeling using JAGS using the other two concepts to demonstrate a case in modeling and simulating sampling. To illustrate, let us discuss how to model a few bayesian cases relevant to a family of conjugacy which is discussed in previous sections. Normal-Normal Model Let us recall normal-normal conjugacy with unknown mean, \\(\\mu\\), and known variance, \\(\\sigma^2\\). For this family of conjugacy, we have the following description of prior distribution and sampling density (likelihood): \\[\\begin{align} P(\\mu) {}&amp;\\rightarrow\\ \\ \\ \\ \\ \\mu \\sim \\mathcal{N}(\\mu_0, \\sigma^2_0)\\\\ \\nonumber \\\\ Lik(x_1,...,x_n|\\mu) &amp;\\rightarrow\\ \\ \\ \\ \\ x_1,...,x_n|\\mu, \\sigma^2 \\sim \\mathcal{N}(\\mu, \\sigma^2)\\ \\ \\ \\text{(sampling density)} \\end{align}\\] We use BUGS language to model in Jags. Below is an example of implementing Jags model for the prior and sampling density in R based on the two distributions above (note in JAGS that we use precision which is the inverse of the variance): model { for (i in 1:N) { # N = sampling size x[i] ~ dnorm(mu[i], 1/sigma[i]) # sampling density (likelihood) mu[i] ~ dnorm(mu0, 1/sigma0) # prior (mean) } mu0 ~ dunif(-1000, 1000) # hyperparameter (mean) sigma0 ~ dunif(0, 1000) # hyperparameter (sd for variance) } In the model, we use the following hyperparameters for a flat hyperprior: \\[\\begin{align*} \\mu_0 {}&amp;\\sim U(-1000,1000) &amp; -\\infty &lt; \\mu_0 &lt; \\infty\\\\ \\sigma^2_0 &amp;\\sim U(0,1000) &amp; \\sigma^2_0 &lt; \\infty \\end{align*}\\] For normal-normal conjugacy with known mean \\(\\mu\\) and unknown variance \\(\\sigma^2\\), we have the following description of prior distribution and sampling density (likelihood): \\[\\begin{align*} P(\\sigma^2) {}&amp;\\rightarrow\\ \\ \\ \\ \\ \\sigma^2 \\sim Inv.\\ Gamma(\\alpha_0,\\beta_0)\\\\ \\\\ Lik(x_1,...,x_n|\\sigma^2) &amp;\\rightarrow\\ \\ \\ \\ \\ x|\\sigma \\sim \\mathcal{N}(\\mu, \\sigma^2)\\ \\ \\ \\text{(sampling density)} \\end{align*}\\] We use BUGS language to model in Jags. Below is an example of implementing the Jags model for the prior and sampling density in R code based on the two distributions above (note in JAGS that we use precision which is the inverse of the variance): model { for (i in 1:N) { # N = sampling size x[i] ~ dnorm(mu[i], 1/sigma[i]) # sampling density (likelihood) sigma[i] ~ dgamma(alpha0, beta0) # prior (sd) } alpha0 ~ dunif(0, 1000) # hyperparameter (alpha) beta0 ~ dunif(0, 1000) # hyperparameter (beta) } In the model, we use the following hyperparameters for a flat hyperprior: \\[\\begin{align*} \\alpha_0 {}&amp;\\sim U(0,1000) &amp; \\alpha_0 &lt; \\infty\\\\ \\beta_0 &amp;\\sim U(0,1000) &amp; \\beta_0 &lt; \\infty \\end{align*}\\] Binomial-Beta Model For Binomial-Beta conjugacy, we have the following description of prior distribution and sampling density (likelihood): \\[\\begin{align} P(\\rho) {}&amp;\\rightarrow\\ \\ \\ \\ \\ \\rho \\sim Beta(\\alpha_0, \\beta_0)\\\\ \\nonumber \\\\ Lik(x_1,...,x_n|\\rho) &amp;\\rightarrow\\ \\ \\ \\ \\ x_1,...,x_n|\\rho \\sim Bin(n, \\rho)\\ \\ \\ \\text{(sampling density)} \\end{align}\\] Below is an example of implementing the Jags model for the prior and sampling density in R code based on the two distributions above: model { for (i in 1:M) { # M = sampling size x[i] ~ dbin(n, rho[i]) # sampling density (likelihood) rho[i] ~ dbeta(alpha0, beta0) # prior } alpha0 ~ dunif(0, 1) # hyperparameter (alpha) beta0 ~ dunif(0, 1) # hyperparameter (beta) } In the model, we use the following hyperparameters for a flat hyperprior: \\[\\begin{align*} \\alpha_0 {}&amp;\\sim U(0,1) &amp; \\alpha_0 &lt; \\infty\\\\ \\beta_0 &amp;\\sim U(0,1) &amp; \\beta_0 &lt; \\infty\\\\ \\end{align*}\\] Figure 8.35 shows a visual representation of the JAGS model. In the figure, the circle nodes represent random variables. There are five variables in each model. The shaded circle node labeled as \\(\\mathbf{x}\\) represents the observed data. The circles inside the rectangular plate are vector variables with a sampling size of \\(N\\ or\\ M\\). The model forms a hierarchical child-parent relationship and, in our case, follows a DAG model (directed acyclic graph). Figure 8.35: JAGS Model To illustrate, let us implement the first model using R code. Here, we only focus on the likelihood and prior for the normal-normal model with unknown \\(\\mu\\) and known \\(\\sigma^2\\). We simulate by sampling ten separate groups, each sample with about fifty observations. We then calculate the variance within each sample. For demonstration, we leave the mean unknown for each sample. Our goal is to parameterize the unknown \\(\\mu\\). # load coda and rjags library first library(coda); library(rjags) set.seed(2020) sample_size = 50 # observations sampling_size = 4 # group size range = seq(11, 20, length.out=10) sampling = replicate(n=sampling_size, sample(range, size=sample_size, replace=TRUE)) mean.with.noise &lt;- function(x) { noise = rnorm(n = 1, mean = 0, sd = 1.2) # introduce error/noise mean(x) + noise } data = as.data.frame( matrix( c( seq(1, sampling_size), sqrt(apply(sampling, 2, var)), apply(sampling, 2, mean.with.noise )), nrow=sampling_size, ncol=3, byrow=FALSE) ) colnames(data) = c(&quot;index&quot;, &quot;sd&quot;, &quot;mean&quot;) data ## index sd mean ## 1 1 3.051 13.65 ## 2 2 2.507 13.95 ## 3 3 2.862 15.12 ## 4 4 3.044 15.86 First, we construct a list of parameters to pass to the model: model.data &lt;- list(&quot;N&quot; = nrow(data), &quot;X&quot; = data$mean, &quot;sigma&quot; = data$sd) Second, we then generate and initialize the model: modelstring = &quot; model { for (i in 1:N) { # N = sampling size X[i] ~ dnorm(mu[i], 1/sigma[i]) # sampling density (likelihood) mu[i] ~ dnorm(mu0, 1/sigma0) # prior (mean) } mu0 ~ dunif(-1000, 1000) # hyperparameter (mean) sigma0 ~ dunif(0, 1000) # hyperparameter (sd for variance) } &quot; model = jags.model(textConnection(modelstring), data=model.data, n.chains=2) ## Compiling model graph ## Resolving undeclared variables ## Allocating nodes ## Graph information: ## Observed stochastic nodes: 4 ## Unobserved stochastic nodes: 6 ## Total graph size: 24 ## ## Initializing model Third, with the generated model, let us go through a burn-in stage. This stage is a preliminary warm-up stage, only hoping that our Markov Chain gets closer to a higher probability. # Burn-In update(model, 2000) Fourth, start the sampling process and monitor the prior hyperparameters: \\(\\mu_0\\) and \\(\\frac{1}{\\sigma^2_0}\\). # Iteration 5000 times for the mu, mu0 and sigma0 samples. cs &lt;- coda.samples(model, c( &quot;mu&quot;, &quot;mu0&quot;, &quot;sigma0&quot; ), n.iter=5000) Fifth, let us review a summary of the parameters. Here, we obtain statistics of the parameter estimates. It allows us to review the mean, standard deviation, standard error of (\\(\\mu_0\\) and \\(\\sigma^2_0\\)) hyperparameters and (\\(\\mu\\)) parameter for our prior. Notice that the unknown mean across the four samplings has values between 13 and 15 with a standard error of 0.019. # Summary for the mu, mu0, sigma0 samples summary(cs) ## ## Iterations = 3001:8000 ## Thinning interval = 1 ## Number of chains = 2 ## Sample size per chain = 5000 ## ## 1. Empirical mean and standard deviation for each variable, ## plus standard error of the mean: ## ## Mean SD Naive SE Time-series SE ## mu[1] 14.0 1.55 0.0155 0.0227 ## mu[2] 14.2 1.40 0.0140 0.0191 ## mu[3] 14.9 1.50 0.0150 0.0206 ## mu[4] 15.4 1.55 0.0155 0.0250 ## mu0 14.6 3.31 0.0331 0.1268 ## sigma0 40.5 86.11 0.8611 9.3145 ## ## 2. Quantiles for each variable: ## ## 2.5% 25% 50% 75% 97.5% ## mu[1] 10.846 12.95 14.00 15.0 16.9 ## mu[2] 11.317 13.26 14.20 15.1 16.8 ## mu[3] 12.051 13.96 14.89 15.9 18.0 ## mu[4] 12.456 14.36 15.35 16.4 18.5 ## mu0 7.633 13.42 14.65 15.8 21.2 ## sigma0 0.146 2.52 9.06 35.0 310.9 Sixth, we use Gelman-Rubin diagnostic tool to analyze convergence. The first diagram plots the iterations taken by JAGS for each of the sampled \\(\\mu\\). Notice that each posterior sampling of \\(\\mu\\) consistently stays somewhere around 15 through the iterations from 2000 to 5000, accompanied by the corresponding density. Figure 8.36: Gelman-Rubin (Trace and Density) Figure 8.37: Gelman-Rubin (Trace and Density) Reviewing the Potential scare reduction factors, the parameters \\(\\mu\\)s have a point estimate equal to or close to 1 and a confidence interval equal to or close to 1, indicating that convergence is sufficiently met. gelman.diag(cs, autoburnin=FALSE) ## Potential scale reduction factors: ## ## Point est. Upper C.I. ## mu[1] 1.00 1.00 ## mu[2] 1.00 1.00 ## mu[3] 1.00 1.00 ## mu[4] 1.00 1.00 ## mu0 1.03 1.03 ## sigma0 1.12 1.21 ## ## Multivariate psrf ## ## 1.01 To complement that, Figure 8.38 illustrates convergence to 1 of two chains for each of the parameters \\(\\mu\\)s. Figure 8.38: Gelman-Rubin (Convergence) Alternatively, we can manually plot the parameter estimates and the hyperparameter estimates for the prior: # Showing Densities result = as.matrix(cs) par(mfrow=c(2,2)) for (i in 1:sampling_size) { plot(density(result[, c(paste0(&quot;mu[&quot;,i,&quot;]&quot;))]), main=paste0(&quot;Density of mu[&quot;,i,&quot;]&quot;)) } Figure 8.39: Jags Result par(mfrow=c(1,2)) plot(density(result[, c(&quot;mu0&quot;)]), main=&quot;Density of mu0&quot;) plot(density(result[, c(&quot;sigma0&quot;)]), main=&quot;Density of sigma0&quot;) Figure 8.40: Jags Result We can now reparameterize our prior mean and generate our posterior by obtaining the ten means for the corresponding ten samples. If we are to monitor \\(X\\), we also should be able to capture the sampling densities: # Iteration 5000 times for the X samples. cs &lt;- coda.samples(model, c( &quot;X&quot; ), n.iter=5000) result = as.matrix(cs) samplings = c(&quot;X1&quot; = mean( result[, c(&quot;X[1]&quot;)]), &quot;X2&quot; = mean( result[, c(&quot;X[2]&quot;)]), &quot;X3&quot; = mean( result[, c(&quot;X[3]&quot;)]), &quot;X4&quot; = mean(result[, c(&quot;X[4]&quot;)])) Therefore, our estimated likelihood for the first sampling is: samplings ## X1 X2 X3 X4 ## 13.65 13.95 15.12 15.86 8.3 Bayesian Analysis In a Bayesian context, we use prior to represent our belief which comes with uncertainty. In a Non-Bayesian context, uncertainty happens due to residuals. Residual may be interpreted differently: perturbation, distortion, bias, and error. As such, we perform Residual Analysis. We start with some observation (or evidence) - backed by some initial prior belief - and perform the estimation. Estimation in most - if not all - cases has the property of being inaccurate (whether this affects our notion of uncertainty or not). Our goal is to reduce the inaccuracy which is analyzing such residuals. There are many approaches to doing that - in general, our Bayesian approach is to collect more evidence to update our prior beliefs. After all, in Chapter 9 (Computational Learning I), we follow the same strategy: we train and keep learning. Also, to complement Bayesian Analysis in line with quantifying uncertainty, let us recall in the previous Chapter that we encourage readers to investigate the topic around Uncertainty Quantification. Let us end this Chapter by enumerating a few other topics around Bayesian analysis starting with autocorrelation. 8.3.1 Autocorrelation When operating MCMC to generate multiple samples during sampling simulation, there are situations when the proposed samples begin to form a strong correlation. However, such autocorrelation of samples may not provide further improvements (in terms of convergence) even with a large number of iterations. Thus, we apply thinning to thin the sample by reducing the number of iterations. Here, for example, we can skip every kth iteration. As for mixing, a multimodal distribution is a good use case in which if our simulation is more biased on a particular peak, then we consider this as poor mixing; otherwise, our samples are well-mixed. Note that high correlation or poor mixing may slow down or even prevent convergence. Thus, one possible solution is to choose initialization values that are more closely centered on the target distribution when modeling the proposal distribution. 8.3.2 Predictive Probability Predictive distribution is a distribution formed by future observations. For example, if our observation is associated with posterior parameters, we expect a posterior predictive distribution; if our observation is associated with prior parameters, we expect a prior predictive distribution. The posterior predictive distribution of future observations, namely \\(y&#39;\\), can be expressed as so: \\[\\begin{align} P(y&#39;|y) = \\int P(y&#39;|\\theta)P(\\theta|y) d\\theta \\end{align}\\] For example, suppose we have a set of observations that follow a normal distribution characterized by posterior parameter \\(\\theta\\) such that \\(\\theta \\sim \\mathcal{N}(\\mu = 10, \\sigma = 1)\\); then we can draw our future observation \\(y&#39;\\). Here, we assume a posterior distribution with \\(\\mu = 10\\) and \\(\\sigma = 1\\). sample_size = 200 future_sample_size = 50 theta = rnorm(n = sample_size, mean = 10, sd = 1) y.pred = rnorm(n = future_sample_size, mean = theta, sd = 1) Our posterior predictive distribution has the following mean: mean(y.pred) ## [1] 9.868 with the following quantile: quantile(y.pred, c(0.025, 0.975) ) ## 2.5% 97.5% ## 7.027 12.407 8.3.3 Posterior Interval Posterior Interval, also called Credible Interval, is similar in concept as Prediction Interval. Credible Interval is enforced based on a given \\(\\alpha\\), e.g. \\(\\alpha = 0.05\\), and is denoted as: \\[ \\mathcal{C}(X) = 1 - \\alpha, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ e.g., \\mathcal{C}(X) = 0.95\\ \\ \\ where: \\alpha = 0.05 \\] If there is such a boundary, namely a and b, to limit our parameter space (X being our random variable parameter), then a credible interval of 95% limits X such that: \\[ P(a \\le X \\le b |\\theta) = 1 - \\alpha\\ \\ \\ \\ where\\ \\alpha = 0.05 \\] Interval Censorship is the term used to limit our parameter space. However, there are other ways to express a limit to our parameter space: \\[ P(X &lt; a) = \\alpha \\ \\ \\ \\ \\ \\ \\ \\ \\ P(X &gt; b) = \\alpha \\] 8.3.4 Bayes Factor Recall Significance of Difference and Regression in Chapter 6 (Statistical Computation), which discusses Statistical Significancetests. Similarly, in Bayesian Statistics, we perform hypothesis test using Bayes Factor which evaluates two competing hypotheses, namely H1 and H0. Bayes Factor is expressed as a ratio of two posterior probabilities: \\[\\begin{align} K = \\frac{P(\\theta_1|X_1)}{P(\\theta_2|X_2)} \\end{align}\\] It follows guidelines measured in terms of evidence levels based on the below Jeffrey’s scale table: Table 8.1: Bayes Factor K Intepretation for H1 K Interpretation for H0 &gt;100 Extreme evidence for H1 &lt; 1/100 Extreme evidence for H0 30 - 100 Very Strong evidence for H1 1/100 - 1/30 Very Strong evidence for H0 10 - 30 Strong evidence for H1 1/30 - 1/10 Strong evidence for H0 3 - 10 Substantial evidence for H1 1/10 - 1/3 Substantial evidence for H0 1 - 3 Anecdotal evidence for H1 1/3 - 1 Anecdotal evidence for H0 1 No evidence for H1 For example, if our Bayes Factor happens to be K=0.20, then there is a piece of very strong evidence that our Null Hypothesis (H0) holds. 8.4 Summary In Bayesian statistics, events are treated with uncertainty in that observations are random and thus follow a particular type of distribution. We introduce Bayes Theorem which is central to the Bayesian computation of uncertainty. Bayesian analysis sits on the premise of prior knowledge and likelihood, which are used to estimate a parameter model in the form of a posterior distribution. However, posterior distribution may not always be tractable (not closed-form). For this reason, we introduce the concept of Conjugacy by injecting conjugate priors into the equation. For intractable posterior distribution with no conjugate prior, we resort to the Variational Bayes method, simulation, and sampling techniques such as Markov Chain Monte Carlo. In dealing with point estimates, we introduce maximum likelihood (MLE) and maximum posterior (MAP) estimates to seek an optimal parameter model. Also, we introduce the expectation-maximization (EM) technique used in cases where we have missing data, and our model has not fully regressed. We leave readers to investigate Stochastic Tunneling and Local Relaxation around optimization ideas for further reading. In volume III of this book, we begin to discuss approximations in the context of Computational Learning and Computational Deep Learning, covering major topics around supervised and non-supervised learning. We cover Regressions, Classifications such as SVM, and Neural Networks such as Perceptrons, CNN, RNN, and Transformers. We end the book with Model Management discussions. "],["machinelearning1.html", "Chapter 9 Computational Learning I 9.1 Observation and Measurement 9.2 Input Data 9.3 Primitive Methods 9.4 Distance Metrics 9.5 Exploratory Data Analysis 9.6 Feature Engineering 9.7 General Modeling 9.8 Supervised vs. Unsupervised Learning 9.9 Summary", " Chapter 9 Computational Learning I Volumes I and II of this book uncover many tricks and techniques to approximate everyday phenomena. In Volume III, we introduce how machines learn to approximate by training. We call this Machine Learning (ML). Our focus in this chapter limits only to some of the more pervasive and ubiquitous ML techniques that are readily introduced in other literature. Furthermore, unlike the previous chapters, starting with this chapter, we emphasize the mechanics - the operation and process used. We try to accompany the algorithm with code using the R language. However, it is important to point out that the collection of code in this chapter (and in this entire book) may contain only vanilla implementations of the corresponding mathematics and algorithms presented. It should be noted, therefore, that when we describe the implementation as example, it is to be viewed and taken that the implementations are naively created and thus are not meant for production use as they are not at par with a good production code, favoring software engineering approaches over coding the intuition behind the algorithms. A quality production code is better structured and equipped with advanced optimization features with better dynamic programming. Before we cover machine learning concepts, we begin the chapter by outlining a few precursory concepts around Input Data, Metrics, and Exploratory Data Analysis (EDA) followed by Feature Engineering 9.1 Observation and Measurement Our five senses serve as biological measuring tools, giving us the ability to perceive the world; hence, the ability to capture and record measurements. However, our human senses can only estimate more frequently than pinpoint measurements accurately. So if we line up a team of professional basketball players and try to get the height of each one of them just by visual senses, note it down on a piece of paper, and calculate the average height, we may be able to guess that the average height is probably around 6 feet 5 inches. As a recap, we end up with the following steps: sample a team of basketball players visually look at their height estimate their height predict the average height That is perhaps the most rudimentary level of practicing data science. On the other hand, a scientist relies on measuring devices to capture measurements and record them for analysis. Following an experiment, the scientist repeats the test and further cross-references results with data points from experiments of other colleagues. When it comes to population data or sampled data, we deal primarily with a collection of observed measurements. We may treat this collection as our data set. Note that we may interchangeably use the term observations with collection. Now, it is crucial to recognize that our observed data becomes the only knowledge we have of the whole entity or phenomenon we are observing when it comes to observations. To our best measurement, we gauge things by how much we know - based on how much data we sample. Therefore, it is essential to know the level of measurements we use, the different measurements available, the methods and algorithms exposed to us, and most importantly, the interpretation of the results. In truth, there is no right or wrong about how we play our observations and measurements. For example, if we get an accuracy of 20%, settling on this initial result may be unwise. Instead, it is a common approach to admit that the result is only the beginning. That means that we have to proceed further with our experiment by re-adjusting, re-formulating, and re-introducing new ways, methods, and parameters to improve our experiment. If there is one leading objective, it is about reaching the actual value based on improved (or adjusted) iterations of our experiment, whether the actual value is known or not. Moreover, it is about sharing with others, hoping that we have a greater chance of hitting the target by collaborating with others. The idea is to find common grounds - to find consistency, stability, and accuracy in our results. 9.1.1 Levels of Measurements Let us start with three kinds of variables that may seem common in everyday datasets: Categorical, Continuous, and Discrete variables. Categorical variable - is a variable that holds any finite number of categories or groups. Continuous variable - is a variable that holds any infinite numerical value. Discrete variable - is a variable that can be categorical or continuous. If a discrete variable holds integer numbers (meaning, not a fraction), we consider this as a continuous variable. e.g. -1, 2, 8, 20, 40 If a discrete variable holds integer numbers but is constrained within a finite number of integer values, we consider this as a categorical variable. e.g. three colors with their corresponding assigned numbers: 1 = red, 2 = green, 3 = blue 9.1.2 Levels of Categorical measurements We can classify categorical variables into three kinds: Nominal, Ordinal, and Dichotomous. Ordinal variable - is a variable that holds categorical values that follow an order. Nominal variable - is a variable that holds categorical values that do not follow an order. Dichotomous (Binary) variable - is a variable that holds categorical values that have opposites. If a categorical variable follows an order, it is an ordinal variable. A value is either followed by another value, preceded by another value, or both. Table 9.1: Ordinal Variable Ordinal Size Expectations First X-Small Low expectation Second Small Just right Third Medium High Expectation Large X-Large If a categorical variable does not follow an order, it is a nominal variable. A value cannot be preceded or followed by another value. Table 9.2: Nominal Variable Mood Color Means of Transportation Branch of Government Angry Red Bus Judiciary Happy Blue Train Legislative Sad Green Plane Executive Excited Orange If a categorical variable can hold one of any binary values, it is a dichotomous (binary) variable. Table 9.3: Binary Variable Answer Facts Coin Transmission Tide Binary Yes True Head Automatic High 1 No False Tail Manual Low 0 9.1.3 Levels of Continuous measurements Continuous variables can be classified further into two: Interval and Ratio. Interval variable - is a variable whose numerical value is measured based on an interval scale. Ratio variable - is a variable whose numerical value is measured based on an interval scale with a pre-determined zero value. If the context is about height, then a 6 ft height is an example of an observed value of a ratio variable. If the context is about ranking, e.g., ranking between 1 and 10, then 10 is an example of an observed value of an interval variable. 9.1.4 Discrete vs Continuous measurements Here, we cover the difference between discrete and continuous numerical variables. Consider Figure 9.1 illustrating both discrete and continuous variables. Figure 9.1: Discrete and Continuous A continuous variable can hold any numerical values between 3 and 4. Divide the space between 3 and 4 and arrive at 3.5. Divide between 3.5 and 4 and get a value of 3.75. We can continue dividing, always arriving at a fraction between the two numbers. We can keep dividing infinitely. Discrete variables hold numerical values such as integers that we cannot break into smaller values. It can hold any whole number or counting number such as 1, 2, 3, 4, and on. We know there is nothing in between each of the counting numbers. On the contrary, it can also hold fraction numbers as long as the numbers hold specific values and the set of numbers is finite. 9.2 Input Data Data is defined as any observable and measurable entity. The term entity is important. Most of the dictionary definitions from the net carry attributes that describe an entity (Cambridge, collins, oxford, business, Merriam-Webster et al. - 2020). As examples: separate and independent distinct and identifiable self-contained real being or existence Data is everything that is observable, measurable, and quantifiable. Input is data that is fed into a computer. 9.2.1 Structured Data In the early days of computers, the proliferation of database systems has contributed to the idea of normalizing data. At that time, systems designers (SDs) were brought in to help in computerizing or modernizing a company’s manual systems. One of the many things that SDs do is collect data by surveying and interviewing companies looking to modernize their processes. The survey starts by asking simple questions about: Company profile (name, functions, processes, policies, responsibilities) Product profile (stock-keeping units) Customer profile (market-focused) In principle and practice, SDs use a universal modeling language (UML) to model and design systems. All the data being collected are mapped and translated into diagrams and tables. That makes data much more structured in a sense. For the sake of explanation, and to make it simpler, suppose we have a simple “buy and sell” system: merchant sells a product customer buys a product Already, there are 5 things (or items) to know about this system: Merchant Customer Product Sell Buy One can classify the five items into three categories: The actors, the actions, and the objects acted upon by the actions. Table 9.4: Actors and Objects Actors Actions Objects Merchant Sell Product Customer Buy In this simple system design, we perform some normalization using two simple and generic categories: Entities and Actions Table 9.5: Categories Entities Actions Merchant Sell Customer Buy Product We then form relationships amongst the categories. See Figure 9.2. Figure 9.2: Buy and Sell What do we do if a merchant sells a product to multiple customers? How do we distinguish or identify one customer from another? We can answer that by first identifying customers by their name, contact information, and certain customer types. Entity: Customer Properties: Name Contact Type Next, if the merchant has many products to sell, how do we distinguish or identify one product from another? We can identify products by their product name, product type, and product price. Entity: Product Properties: Name Type Price Inventory Similarly, we can identify a merchant by the following properties: Entity: Merchant Properties: Name Location Owner It helps to understand that when we talk about individual / unique customers such as John Doe or Jane Smith, we refer to instances of a customer entity. In other words, entities contain instances ( some literature may call them records, others call them tuples ). Table 9.6: Customers Name Contact Type John Doe 1.650.nnn.nnn1 Regular Customer Jane Smith 1.510.nnn.nnn2 Walk-in Customer Two things can be said about the detail above. John Doe and Jane Smith are instances of the Customer entity. Table 9.7: Products Name Type Price Inventory Table Furniture $100.00 200 Chair Furniture $50.00 100 Table and Chair are instances of the Product entity. Table 9.8: Merchants Name Location Owner ACME California Rich Joe ACME is an instance of a merchant entity Now, what characterizes an instance? In the case of a customer entity, a customer instance is characterized based on customer name, contact, and customer type. Characterizing an instance gives a unique personality to an instance, e.g., John Doe is a separate individual from Jane Smith, which means that he has all the characteristics of being a John Doe. Characters here are called attributes, which means that instances are described based on attributes. In subsequent sections, we will be delving more into other translations of attributes such as signals, features, predictor variables, and independent variables. How do we represent the action? Action can mean many things. For example, it can mean an event or an activity. In this specific case, we have transactions as an event. Here is a sample of a transaction: Transaction: customer buys a product Given a transaction in which John Doe bought a single furniture table at $100.00 on January 1, 2018, how do we want the transaction instance to look like: The transaction record may look like this: Table 9.9: Transactions Customer Action Product Amount Quantity Date John Doe Buy Table $100.00 1 Jan 01, 2018 Storage of information Given a complete picture of entities and transactions for our business and their related properties, we now have a model that we can translate into a schema that is nothing more than just a representation of the model we constructed. This schema will contain tables (representing the entities and transactions) and table columns (representing the entity properties and transaction properties). This schema will reside in a database system. Creating tables Given a structure for our entities, e.g., Customers, Products, Merchants, and Transactions, we now should be able to capture instances of each entity into its corresponding database table. We first have to create tables into a SQL-based database system to do that. Below is a simple database command that creates the tables. Create Customer table: create table customers( name text, contact text, type text ) Create Product table: create table products( name text, type text, price number, inventory number ) Create Merchant table: create table merchants( name text, location text, owner number ) Create Transaction table: create table transactions( customer text, action text, product text, amount number, quantity number, date timestamp ) Storing a record To store a transaction record into the TRANSACTIONS table. Record Transaction: insert into transactions(&quot;John Doe&quot;, &quot;Buy&quot;, &quot;Table&quot;, 100, 1, &quot;Jan 01, 2018&quot;) The table has a transaction record of John Doe, who bought a furniture table for $100.00. If John Doe buys multiple tables, the transaction table will have many more transaction records. Table 9.10: Transactions Customer Action Product Amount Quantity Date John Doe Buy Table $100.00 1 Jan 01, 2018 John Doe Buy Table $100.00 3 Jan 02, 2018 Jane Smith Buy Chair $50.00 3 Jan 03, 2018 Here, John Doe bought four furniture tables within two consecutive dates. Jane Smith bought three furniture chairs on Jan 3, 2018. Structured Query Language (SQL): SQL is a language used to interact with a database system. The customer, product, merchant, and transaction tables reside in a database system. Each table serves as a repository of records. For example, the customer record of John Doe is recorded in the customer table in the database system. To request the database system for the customer record of John Doe, we have to issue an SQL statement. SQL statement: select * from customers where name = ‘John Doe.’ The database system will output the following record based on the SQL statement above. Notice the asterisk ’*’, which asks the database system to output all the properties or attributes of the customer record. Table 9.11: John Doe Record Name Contact Type John Doe 1.650.nnn.nnn1 Regular Customer The following SQL statement selects only the name of a customer named ‘John Doe’: SQL statement: select name from customers where name = ‘John Doe’ The database system will output only the name of the John Doe record based on the SQL statement above. Table 9.12: John Doe Record Name John Doe On Data Integrity: Database systems have to follow the ACID rule. Atomicity - all relevant changes should be committed, or not at all. Consistency - the state of the resulting data should remain in full consistently and not partial. Isolation - competing transactions should process data in isolation. Durability - the correct state of data should be preserved even after failures. One example of dealing with data integrity is when a customer, e.g., John Doe, bought one furniture table on Jan 01, 2018. As soon as the transaction was recorded into the transaction table, the question would be, did we update the product table to reflect the transaction, meaning the inventory for the table should then be 199 units instead of 200 units? Table 9.13: Products Name Type Price Inventory Table Furniture $100.00 199 Chair Furniture $50.00 100 On Referential Integrity and Relationships: The relational nature of database systems is based on referential integrity. It means that the relationship between two entities needs to be protected. Therefore, constraints have to be put in place to protect the relationship. As an example of a constraint, a transaction record should not exist if there is no product to sell in the first place. In other words, it is not possible to contain transaction records whose product attributes belong to products not recorded in the product table. That is because the transaction record references a non-existing product. That breaks the referential integrity. To keep referential integrity intact, we need a way for both transaction records and product records to reference each other. In this case, we use a reference id. Table 9.14: Transactions Customer Action Product Id Amount Quantity Date John Doe Buy T12 $100.00 1 Jan 01, 2018 John Doe Buy T12 $100.00 3 Jan 02, 2018 Jane Smith Buy C09 $50.00 3 Jan 03, 2018 A new “product id” attribute is used as a reference id to refer to a specific product, replacing the “product” attribute. Table 9.15: Products Product Id Name Type Price Inventory T12 Table Furniture $100.00 199 C09 Chair Furniture $50.00 100 A new “product id” attribute is added as a reference id to reference the product. With regards to referential integrity, certain relationships need to be considered when designing a system: Table 9.16: Referential Integrity Relationship Description Parent-child A child entity references a parent entity One-to-One An instance of one entity references an instance of another One-to-Many An instance of one entity references many instances of another Many-to-One Many instances of one entity references an instance of another Many-to-Many Many instances of one entity references many instances of another Figure 9.3: Relationship The moment we begin to design relationships of individual entities, we are now modeling a schema. A schema is a model - a structured representation of entities and actions and how each is inter-connected via references. The inter-connected relationship can become complex if our schema consists of multiple entities and action tables. Figure 9.4: Schema with complex relationships Therefore, a designer of a system should be meticulous and aware of many things. A bad design could get costly, especially when the project is at the implementation stage. Any missed detail in the design may require redoing the entire implementation to fit the missing requirement or relationship between entities. We have seen how data can be categorized into entities and attributes when it comes to data. We also have shown how we have created a table for Merchant data and a separate table for Customer data. Imagine if we do not do that, it may turn out to be this: Table 9.17: Un-normalized Table Customer Contact CustType Prod Type Price Stock John Doe 1.650.nnn.nnn1 Regular Table Furniture $100.00 200 That table is what we call denormalized form. In contrast, the Merchant table and Product table are the normalized forms. When one suggests normalizing a schema, we ensure relevant attributes of one entity are grouped in a table so that instances of such entities get recorded into the table along with the corresponding attributes. A denormalized table has unrelated attributes all mixed into one table. The table becomes a flat table, therefore. Not to say that it is a bad design, but it may not necessarily fit a relational model - one where we have the data integrity and referential integrity constraints in place as possibly the minimum requirement. This book will not cover all other advanced topics in system or schema design because there is a whole book that covers database or system design. Topics such as normalization and indexing, database optimization and advanced SQL statements, and many other exciting topics have their rightful place in another more extensive discussion. While data science requires deep comfortable knowledge of database systems, data science does not stop there. We will visit this requirement in a later chapter of the book. 9.2.2 Non-Structured Data We have just introduced a way of organizing data into a structure. This time, let us now talk about unstructured data. The following items below are examples of unstructured data: news articles blogs comments or feedbacks customer reviews customer complaints product descriptions event descriptions alert notifications diagnostic logs system logs audit logs There is one common property shared by the data above. They are free text-based format. As such, they are unstructured. Mining information from unstructured data can be challenging. In most cases, processing unstructured data requires parsing and pattern matching. The crude way to do this is to split a text string into individual words and start analyzing each word. So here is an example - every time a user visits a website, the site logs the visit in this format. [January 01, 2018] index.html 500 The log text can be parsed and can be split into the following representations: Date URL Status Code Of course, we can convert the parsed data into a structured format and save it into a relational database system table: Table 9.18: Website Log Date URL Status Code January 01, 2018 index.html 500 Otherwise, we can handle the unstructured data in a non-SQL structured format. For example, we can convert the parsed data into XML format or JSON format: XML format: January 01, 2018 index.html 500 JSON format: { “weblog” : { “date” : “January 01, 2018”, “url” : “index.html”, “statuscode” : 500 } } We can then store the XML or JSON version of the data into a non-SQL database system as a document. If we prefer to query the name of the customer using XML, we can use the following SQL statement: SQL statement: select XMLELEMENT(“Customer”, XMLELEMENT(“Name”, name)) from customers where name = ‘John Doe’ XML Result: John Doe If we prefer to query the name of the customer using JSON, we can use the following SQL statement: SQL statement: select JSON_QUERY(name, “$.Customer.Name”) from customers where name = ‘John Doe’ JSON Result: { “Customer” : { “Name” : “John Doe” } } Structured Query Language (SQL) follows ISO SQL standards. ISO SQL standards are continuously and actively extended to support other features. For example, ISO SQL:2003/2006 supports XML documents, and after that, ISO SQL:2016 now also supports JSON documents. Many relational database systems follow SQL ISO standards. Ideally, we want to choose one that supports ISO SQL:2016. A quick summary: Structured data allows us to use SQL statements to retrieve records kept in normalized tables stored in relational database systems. On the other hand, unstructured data allows us to store data as documents instead of tables in “non-relational or no-SQL” based database systems. Data is documented in extensible markup language (XML) format or javascript object notation (JSON) format. We treat data as a document having no relationship with other documents. One important concept to emphasize is the key-value pair for lookup useful for unstructured data. Most in-memory systems utilize some memory cache technology for lookups. We leave readers to investigate memory cache. 9.2.3 Statistical Data We have been covering ‘entities’ and ‘attributes’ to describe data in the context of systems design. Here, we extend a few more terms from what we already have in Chapter 6 (Statistical Computation). We introduce population, samples, observations, and variables. A population represents the entirety or totality of entities of interest, e.g., the world’s human population. A sample represents a selected portion of a population; Hence, sampling a population means collecting a subset of the population for statistical analysis. An observation is the act of recording and measuring facts. Here, by just omitting the act of observing, we can refer to observed facts as observations. A variable represents the ingredients or conditions capable of change. Other terms such as covariates or variates are related but do not necessarily equate to the term variable - to be explained more in the topic of random variables. To hear someone say that there are other angles to consider, other factors to consider, or other variables to consider only means we have to consider more variables than what is presented on hand. To consider all variables for a given sample means to account for all factors of a given sample. Therefore, we need to consider variables (or factors) such as humidity, temperature, and wind velocity to study climate change. All three variables may contribute to climate change. However, there may be other variables to evaluate as well. Does human progress contribute to climate change? How do we measure human progress? Here, we need to consider all measurable observations of human progress. Table 9.19: Climate Change Date Humidity (%) Temperature (F) Wind Velocity (mph) January 01, 2018 20 70 30 January 02, 2018 13 100 10 In Table 9.19, there are two observations of a given sample of climate change data. The sample bears three variables: Humidity measured in percentage (%), Temperature measured in Fahrenheit (F), and Wind Velocity measured in miles per hour (mph). Notice the use of the term contribute when evaluating statistical data. We say that there are three variables (Humidity, Temperature, Wind Velocity) that contribute to climate change. The change in climate depends on observations of the three variables. In statistics, we do refer to climate change as a dependent variable and the other three variables as independent variables. We also refer to climate change as a response variable and the other three variables as predictor variables. And also, we refer to climate change as explained variable and the other three variables as explanatory variables. Table 9.20: Variables Answer Question Dependent variable Independent variables Response (Outcome) variable Predictor variables Explained variable Explanatory variables Response variables, Dependent variables, and Explained variables can be interchangeable. Other authors may not be strict in the subtle differences. For example, we can say that one variable affects another variable. In other cases, we say that one variable explains another variable. In some instances, we say that one variable predicts the outcome of another variable. The use of such terms may depend on the context of Statistics, Mathematics, or Machine Learning. It may be fair to say that whichever variable is the center of focus or the main study, then that is one of the three: dependent, explained, or response variable. On the other hand, whichever variable effects, contributes to, or explains the study’s outcome is one of the three: independent, explanatory, or predictor variable(s). We focus on measurable observations of a sample or population in statistical data. However, this does not mean that other data types are not qualified for statistical analysis. There are ways to translate or convert multimedia data into measurable data - the translation and conversion of data are part of pre-processing data, which we will cover at some point in the book. 9.2.4 Real-Time and Near Real-Time Data Real-Time data is one with the property of being immediately available and often associated with being streamed with no delay. Data is stored in memory and streamed directly to a system for real-time processing to achieve close to zero latency. Near Real-Time data is one with the property of not being immediately available but with a short delay. Such data are stored physically on a disk prior to being delivered. The latency added is measured against the time it takes to store and retrieve the data. 9.2.5 OLTP and Datawarehouse OLTP data is online transaction data that requires processing. Response for an online request is expected to have no delay (from a user perspective), e.g., depositing money to a bank represents an online transaction and cannot have a long delay for response to reach a customer as soon as the deposit is committed. Data warehouse data is processed offline in batches; it often requires jobs scheduled during off-business hours, e.g., processing payroll for a big organization every month. The chances are that data warehouse (DW) engineers undergo three standard ETL methods to process DW data: extract, transform, and load. They mean literally just that. Extract raw data. Transform the extracted raw data into a more summarized form, then load it into a DW database for reporting and analysis. 9.2.6 Data lake Data lake is a terminology often associated with Big Data. The premise behind the name explains how the volume of data is growing extremely large and complex in that it requires new methods to process such data. That is where map-reduce procedures come to play. That is also where distributed systems enter the picture. Moreover, that is where online streaming technologies also get more benefits. 9.2.7 Natural Language (NL) With natural language, data is in the form of text which requires parsing, structuring, and analyzing to gain context and some meaning. For example, taken from the NIPS dataset (as of 2018), we form a cloud of words (rendered using wordcloud R package) from a string set of text and identify the most common topic. See Figure 9.5 as an illustration. As it turns out, neural network seems to be one of the common topics in the word cloud. Figure 9.5: Wordcloud 9.2.8 Multimedia (MD) Here, we narrow the scope of Multimedia around photo images, sound, music, and video. Taken from the MNIST dataset (Y Lecun et al.), a handwritten digit is fed into a machine learning algorithm to predict and classify the digit. See Figure 9.6. Figure 9.6: MNIST dataset 9.3 Primitive Methods In this section, as part of manipulating or translating data, we introduce (what we call for the lack of a better word) the primitive methods. Because we find these elementary and primitive methods scattered across a spectrum of topics or areas, usually only hinted at or abstracted, it may be essential to gather and discuss them in this book and expose them from abstraction, favoring applied intuition more than the mathematical intuition of it. In manipulating or translating data (whether directly or indirectly), there are about a number of primitive methods to cover, namely weighting, binning, discretizing, stratifying, smoothing, normalizing, standardizing, centering, scaling, transforming, and regularizing. These are perhaps the most common and useful elementary and primitive methods of processing data. Gradually, we develop them into discussions that explore the more involved methods in later chapters. 9.3.1 Weighting We can explain the concept of weighting using a balance scale. Suppose we load each side of a scale with a stone and find that the scale does not balance. To balance the scale, we add small pebbles on each side. These added pebbles are what we call weights. We add extra weights to meet some level of balance. Similarly, weights (or coefficients) are needed to put additional value to data. In ML, we parameterize our model to fit our data by adjusting the parameters (weights). In Chapter 3 (Numerical Linear Algebra II), the idea of beta (\\(\\beta\\)) coefficients is introduced under Least Squares using the formula: \\[\\begin{align} \\hat{y} = \\beta_0 + \\beta_i x_i \\label{eqn:eqnnumber150} \\end{align}\\] The two coefficients, \\(\\beta_0\\), and \\(\\beta_1\\), can be regarded as weights. We shall see that when it comes to Machine Learning and Deep Neural Networks, all our discussions throughout the rest of the book are based upon optimizing the weights or parameters via some iteration in conjunction with a learning rate hyperparameter. The idea is that we can use the adjusted weights as models for inference. Usually, we pair such parameters with corresponding predictor variables (in statistics) or extracted features (in machine learning). Such variables can either be discrete, continuous, or categorical. Now, there are cases when we need to pre-process data before weighing. Unlike weights, here we deal with the data itself and not the coefficients. Data is denoted by \\(x_i\\) in Equation \\(\\ref{eqn:eqnnumber150}\\). All the primitive methods listed above except weighing and regularization can be used to adjust (or manipulate) data, e.g., \\(x_i\\). Both weighing and regularization, on the other hand, are used to adjust the coefficients. Specifically, regularization indirectly controls (punishes or rewards) the coefficients via an added regularizer in the Loss function (we discuss this in a later section). 9.3.2 Smoothing Smoothing is a primitive method in data science that deals with noise and fluctuations. In dealing with data, we may perceive some data to be outliers. They have random variations outside the norm. One way to deal with noise is just manually removing the noise or outlier. Figure 9.7 illustrates a smoothed data. Figure 9.7: Smoothing For a simple demonstration, our simple crude way of smoothing data (which may not be the best way) is to scale down the data to subdue the effect of outliers. Here, we perform a cube root. See Table 9.21. Table 9.21: Smoothing X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X (noisy) 27 27 8 -1 0 8 1 0 -8 0 X (smoothed) 3 3 2 -1 0 2 1 0 -2 0 We can eliminate noise via smoothing. On the other hand, we show later how we can scale back data to expose real noise (highest peak), which could surface when scaled and become candidates for analysis. That is another way to expose outliers and make them more visible by scaling up (e.g., raise to the power of 2). 9.3.3 Normalizing Normalizing is mapping values to the range between 0 and 1. See Figure 9.9. To normalize, we can use the below formula: \\[\\begin{align} normalized\\ x = \\dfrac{x-min(x)}{max(x)-min(x)} \\end{align}\\] As an example, see Table 9.22. In a more complex dataset, normalizing a dataset to a range between 0 to 1 is, in essence, virtually stripping off the units. So, for example, by measuring two variables (weight and height) where the weight comes in lbs (pounds) and height comes in ft (feet), we can normalize the values of both variables so that they show the same normalized range of values between 0 and 1 - and thus ignoring the units (lbs and ft) completely. Table 9.22: Normalization X1 X2 X3 X4 X5 X 50 100.0 150.0 250.0 300 Normalized 0 0.2 0.4 0.8 1 x1 = sort( runif(20, min=-50, max=150) ) x2 = 500 # simulate outlier x3 = sort( runif(20, min=151, max=250) ) x = c(x1, x2, x3) normalized_x = (x - min(x)) / ( max(x) - min(x)) Figure 9.8: Normalization In Figure 9.8 as can be seen, the scale of the original data ranges between 0-500. With normalization, the scale is now between 0 and 1. That also shows that outliers may still be visible even in their normalized version. Additionally, there is also what we call mean normalization, which has the following equation: \\[\\begin{align} mean.norm.x = \\dfrac{x-mean(x)}{max(x)-min(x)} \\end{align}\\] However, we can also treat this normalization as a type of standardization. 9.3.4 Standardizing Standardizing is the process of mapping values to the range between -N and N, e.g., between -1 and 1, where zero is always the mean (zero-mean). See Figure 9.9. Figure 9.9: Normalize vs Sandardize The formula to standardize (e.g. z-score normalization): \\[\\begin{align} standardized\\ x = \\dfrac{x-mean(x)}{stddev(x)} \\end{align}\\] As an example, see Table 9.23. Table 9.23: Standardization X1 X2 X3 X4 X5 X 50.00 100.00 150.00 250.00 300.00 Standardized -1.16 -0.68 -0.19 0.77 1.25 Note that our values are now in units of one standard deviation. Also, note that we also can use the function scale(.) to achieve the same standardization by scaling (including centering): t(scale(x = x, scale=TRUE, center=TRUE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] -1.157 -0.6751 -0.1929 0.7716 1.254 ## attr(,&quot;scaled:center&quot;) ## [1] 170 ## attr(,&quot;scaled:scale&quot;) ## [1] 103.7 If we do not center, then we have the following outcome: t(scale(x = x, scale=TRUE, center=FALSE)) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0.2309 0.4619 0.6928 1.155 1.386 ## attr(,&quot;scaled:scale&quot;) ## [1] 216.5 In a more complex dataset, similar to normalization, trying to standardize a dataset to a range between -N and N is, in essence, also virtually stripping off the units. See Figure 9.11. x1 = sort( runif(20, min=-50, max=150) ) x2 = 500 # simulate outlier x3 = sort( runif(20, min=151, max=250) ) x = c(x1, x2, x3) standardized_x = (x - mean(x)) / sd(x) Figure 9.10: Standardization To re-emphasize, in the normalization method, the normalized values are within the range of 0 and 1. However, the values revolve around 0 in the standardization method because of the mean and standard deviation. So, for example, we see values between -1 and 1 or between -2 and 2. In other words, the range for standardized values is between -N and N. Also, notice that the outlier is still visible while the data has been standardized. 9.3.5 Centering Centering is a simple primitive that transforms values to get closer to the zero-mean where the average value is centered - this is called centering on the mean. Here is the formula: \\[centered\\ x = x - mean(x)\\] Between centering and standardizing, centering does not require the transformed value to be divided by standard deviation. However, standardizing can also be considered another form of centering. The only difference is that centering is not scaled. Let us now compare centering and standardizing in a table. Table 9.24: Standardization vs Centering X1 X2 X3 X4 X5 X 50.00 100.00 150.00 250.00 300.00 Standardized -1.16 -0.68 -0.19 0.77 1.25 Centered -120.00 -70.00 -20.00 80.00 130.00 Since centering does not scale, it sometimes makes it a challenge to compare plots between centering and standardization since the gap between their values is significant, as seen in Figure 9.11. Figure 9.11: Centering 9.3.6 Scaling Smoothing, normalizing, standardizing are all methods of scaling. Other methods of scaling are logarithmic and exponential scaling. The formula to scale using log(X): \\[scaled\\ x = \\log({x})\\] In some cases, our dataset reflects some exponentially increasing behavior. In this case, we use the logarithm to scale large numbers within manageable ranges. For example, notice the range drops from 0-8000 to 0-9. Table 9.25: Scaling X1 X2 X3 X4 X5 X6 X7 X8 X9 X10 X 12.0 12.0 12.0 13.0 13.0 13.0 13.0 13.0 13.0 14.0 log(X) 2.5 2.5 2.5 2.6 2.6 2.6 2.6 2.6 2.6 2.6 Figure 9.12: Scaling Also, sometimes, it helps to know which log scaling to use. Table 9.26: Logarithms X \\(log(X)\\) \\(log2(X)\\) \\(log3(X)\\) \\(log4(X)\\) \\(log5(X)\\) \\(log10(X)\\) 1e+00 0.00 0.00 0.00 0.00 0.00 0 1e+01 2.30 3.32 2.10 1.66 1.43 1 1e+02 4.61 6.64 4.19 3.32 2.86 2 1e+03 6.91 9.97 6.29 4.98 4.29 3 1e+04 9.21 13.29 8.38 6.64 5.72 4 1e+05 11.51 16.61 10.48 8.30 7.15 5 1e+06 13.82 19.93 12.58 9.97 8.58 6 1e+07 16.12 23.25 14.67 11.63 10.01 7 1e+08 18.42 26.58 16.77 13.29 11.45 8 1e+09 20.72 29.90 18.86 14.95 12.88 9 Plotting different logarithms, let us see the result of each case. Figure 9.13: Scaling by Logarithms The range of the different logarithms falls within the range 0 and 20 for exponential values from 0 through 1e+9. So, if the ranges do not have significant differences (even within a range), using one of those logarithmic methods for scaling may not be a concern. 9.3.7 Transforming Transformation is a generalized method that extends scaling. Such examples of transformation are log transformation, Box-Cox transformation, and clipping. Similar to scaling, an example of transformation is when we raise the value of data to a power of 2 or 3. \\[ transformed \\ x = x^2\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ transformed \\ x = x^3 \\] Other power transformation is shown in Table 9.27. Table 9.27: Power of Exponents X \\(X^{1/4}\\) \\(X^{1/3}\\) \\(X^{1/2}\\) \\(X^0\\) \\(X^2\\) \\(X^3\\) \\(X^4\\) 1.0 0.25 0.33 0.5 1 1.00 1.00 1.00 1.2 0.30 0.40 0.6 1 1.44 1.73 2.07 1.4 0.35 0.47 0.7 1 1.96 2.74 3.84 1.6 0.40 0.53 0.8 1 2.56 4.10 6.55 1.8 0.45 0.60 0.9 1 3.24 5.83 10.50 2.0 0.50 0.67 1.0 1 4.00 8.00 16.00 2.2 0.55 0.73 1.1 1 4.84 10.65 23.43 2.4 0.60 0.80 1.2 1 5.76 13.82 33.18 2.6 0.65 0.87 1.3 1 6.76 17.58 45.70 2.8 0.70 0.93 1.4 1 7.84 21.95 61.47 Depending on the purpose, it may seem that using power (or order) of 4 can expose outliers. In statistics and machine learning, one powerful transformation method is used: Box-Cox transformation. Box-cox uses the following well-known formula: \\[ transformed \\ x =\\begin{cases} \\dfrac{x^\\lambda - 1}{\\lambda}\\ &amp; \\text{if } \\lambda \\neq 0 \\\\ \\\\ \\log ({x})\\ &amp; \\text{if } \\lambda = 0 \\end{cases} \\] Table 9.28: Box Cox Transformation X-lambda 0.3 0.2 0.1 0 1 2 3 1.0 0.00 0.00 0.00 0.00 0.0 0.00 0.00 1.2 0.19 0.19 0.18 0.18 0.2 0.22 0.24 1.4 0.35 0.35 0.34 0.34 0.4 0.48 0.58 1.6 0.50 0.49 0.48 0.47 0.6 0.78 1.03 1.8 0.64 0.62 0.61 0.59 0.8 1.12 1.61 2.0 0.77 0.74 0.72 0.69 1.0 1.50 2.33 2.2 0.89 0.85 0.82 0.79 1.2 1.92 3.22 2.4 1.00 0.96 0.91 0.88 1.4 2.38 4.27 2.6 1.11 1.05 1.00 0.96 1.6 2.88 5.53 2.8 1.21 1.14 1.08 1.03 1.8 3.42 6.98 Plotting Box-Cox Transformation. See Figure 9.14. Figure 9.14: Box-Cox Transformation Compared to the “Power of Exponents” transformation, the Box-Cox transformation has a much more gradual ascend/descend than the “Power of Exponents” transformation, avoiding an aggressive exponential growth or increase. 9.3.8 Clipping Clipping is another form of transformation. Clipping is a method of correcting the value of data based on constraints. Below is an example of how we clip the value. If the value is less than zero, then we assign the value of zero; otherwise, if the value is greater than 100, we assign the value of 100. \\[ clipped\\ x = \\begin{cases} 0 &amp; if\\ x &lt; 0 \\\\ 100 &amp; if\\ x &gt; 100 \\end{cases} \\] In a way, this can help to remove outliers. We make sure values do not fall outside a given boundary range. Here, we use minimum and maximum values to set the boundaries. In the example above, we decided that our minimum value is zero, and our maximum value is 100. 9.3.9 Regularizing Regularization is about reward and penalty - it is about rewarding the right and penalizing the wrong. It is about tipping the scales to favor the right side (See Figure ). However, when it comes to ML, it can be used to demote the value of coefficients, as we shall see later. Figure 9.15: Regularizing Here is one very rudimentary but elegant formula: \\[\\beta_0 * \\lambda + \\beta_1 * (1 - \\lambda ) = 1\\] Or \\[ \\beta_0 * \\frac{\\lambda - 1}{\\lambda} + \\beta_1 * \\frac{1}{\\lambda},\\ \\ \\ where\\ 0 \\le \\lambda \\le 1,\\ \\ \\beta_0 = 1,\\ and \\ \\beta_1 = 1 \\] We have two terms in the equation. Let us call them term A and term B. A = \\(\\beta_0 * \\lambda\\) B = \\(\\beta_1 * (1 - \\lambda)\\) So then we have: \\[A + B = 1\\] For the simplest demonstration, we intentionally set the two beta coefficients (\\(\\beta_0\\) and \\(\\beta_1\\)) to 1. So how does it work? It works by sneaking in a parameter, the lambda (\\(\\lambda\\)), whose responsibility is to offset the balance of power. The lambda parameter attaches itself to the coefficients via multiplication, and thus it either promotes or maximizes; otherwise, it demotes or minimizes the influence of coefficients; in the process, it is an effective way of regulating the relevance of features - or in another way to see it, it is an effective way to reduce the complexity of our model. We shall cover that in a later section when we get to discuss Lasso and Ridge regularization. We use the lambda parameter to introduce appropriate values to induce an effect, whether as a reward for or as a penalty against values or computations that may seem extreme or undesirable. Therefore, this parameter can also be considered a tunable parameter used to tune expressions in the direction we deem fit. Let us test some values for the lambda in the range between 0 and 1: Table 9.29: Regularizing Lambda 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 A 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 B 1 0.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0 Notice that if we assign values less than 0.5 to lambda, then the \\(\\beta_0\\) coefficient is penalized with small values, yielding smaller values for A. If we assign values greater than 0.5 to lambda, then the \\(\\beta_0\\) coefficient is rewarded with large values. The same happens with the \\(\\beta_1\\) coefficient in reverse. However, what does regularization signify? Below is a plot using transformation by the power of three against the lambda (the regularizer) \\[x_1 * \\lambda^3 + x_2 * (1 - \\lambda )^3 = 1\\] where \\[x_1 = 1\\ and\\ x_2 = 3\\] Figure 9.16: Regularizing We should see the power of transformation and regularization working in tandem to influence data. That is where the value of regularization comes into play. For example, our expected values for A and B are between 0 and 1, yet there is a big gap between A and B. Using lambda to regularize, we now know that a lambda less than 0.3 does not have a significant impact on both terms A and B. However, once we can limit lambda between 0.3 through 1, we have a good chance that terms A and B can play along in the range 0 and 1. What we have described is just a simple explanation of regularization. There are effective regularization solutions that are in use today. While they have contributed much, this is not to say that we should be limited to those solutions alone. If there is any way we can introduce a tunable parameter to regulate our model without overfitting or underfitting and yet be able to approach a good fit for our model or other relative purposes, it does deserve a good cheer, perhaps a lot more than that. In linear regression, we will encounter the following generalized regularizations: Lasso (L1) Ridge (L2) Elastic Net Below is a list of other regularization solutions. Laplace regularization (L1) Gauss regularization (L2) Tikhonov regularization If and when we see any of these regularizations, it does mean one thing. They are there to influence the outcome by regulating through reward or penalty. 9.4 Distance Metrics In this section, we cover classic distance measurements. The most fundamental measurement has to do with distance. We measure the distance between data points and interpret the result depending on the context. The result of our distance computation may show closeness in proximity; otherwise may not. Interpretation of a close distance may come in relevance, significance, correlation, and others; otherwise, data points that are farther away may connote irrelevance, no significance, no correlation, or simply no relationship whatsoever. Let us enumerate a few typical measurements. 9.4.1 Cosine Similarity Suppose we have generated some normalized random numbers and scaled them to 10. Let us have 6 of those random numbers for measurement. See Figure 9.17. Figure 9.17: Cosine Similarity There are many ways to compare the distance between those numbers. One such measurement is cosine similarity. Cosine similarity is mainly used to get the cosine distance between two vectors. We will see this more in use when we get to natural language processing (NLP). The following formula applies to cosine similarity: \\[\\begin{align} Cosine\\ Similarity = cos(\\theta) = \\frac{D_{1} \\cdotp D_{2}}{||D_{1}||\\ ||D_{2}||} \\end{align}\\] where \\[\\begin{align} dot\\ product = D_{1} \\cdotp D_{2} = \\sum_{i=1}^{n} D_{1i} D_{2i} \\end{align}\\] \\[\\begin{align} magnitude = ||D_{1}|| = \\sqrt[2]{d_{1}^2 + ...+ d_{n}^2} \\end{align}\\] \\[\\begin{align} magnitude = ||D_{2}|| = \\sqrt[2]{d_{1}^2 + ...+ d_{n}^2} \\end{align}\\] Also, here is another equivalent formula: \\[\\begin{align} Cosine = cos(\\theta) = \\frac{adjacent}{hypothenus}\\ ;\\ \\ \\therefore \\theta = cos^{-1}\\frac{a}{h} = acos(cos(\\theta)) \\end{align}\\] We can plot an example. See Figure 9.18. Figure 9.18: Cosine Similarity Let us put the cosine similarity into action (see Table 9.30): set.seed(142) v1 = round( runif(n=10, min=1, max=10) ) v2 = round( runif(n=10, min=1, max=10) ) dot_product = v1 %*% v2 v1_magnitude = sqrt( sum( v1 ^ 2) ) v2_magnitude = sqrt( sum( v2 ^ 2) ) cosine_similarity = dot_product / ( v1_magnitude * v2_magnitude ) ds = t( data.frame(v1,v2)) colnames(ds) = c(seq(1,10,1)) rownames(ds) =c (&quot;**vector P**&quot;, &quot;**vector Q**&quot;) knitr::kable( head(ds, 20), caption = &quot;Jaccard Similarity&quot;, booktabs = TRUE) Table 9.30: Jaccard Similarity 1 2 3 4 5 6 7 8 9 10 vector P 9 7 10 6 8 9 4 7 5 5 vector Q 9 9 8 6 6 6 2 6 6 3 Compute for Cosine Similarity: \\[\\begin{align*} P \\cdotp Q = (9, 7, 10, 6, 8, 9, 4, 7, 5, 5)\\ \\cdotp (9, 9, 8, 6, 6, 6, 2, 6, 6, 3) = 457 \\end{align*}\\] \\[\\begin{align} ||P|| = \\sqrt[2]{|p_{1}^2 + ...+ p_{n}^2} = 22.9347 \\end{align}\\] \\[\\begin{align} ||Q|| = \\sqrt[2]{|q_{1}^2 + ...+ q_{n}^2} = 20.4695 \\end{align}\\] \\[\\begin{align} cos(\\theta) = \\frac{P \\cdotp Q}{||P||\\ ||Q||} = \\frac{457}{22.9347 * 20.4695} = 0.9735 \\end{align}\\] 9.4.2 Manhattan and Euclidean Distance We have listed the following common distance measurements: \\[\\begin{align} (L1\\ norm) = D_{manhattan}(P,Q)=\\sqrt[1]{|p_{1} - q_{1}|^1 + ...+ |p_{j}-q_{j}|^1} = \\begin{pmatrix}\\sum_{i=1}^{j} |p_{i} - q_{i}|\\end{pmatrix}^{1/1}\\\\ (L2\\ norm) = D_{euclidean}(P,Q)=\\sqrt[2]{|p_{1} - q_{1}|^2 + ...+ |p_{j}-q_{j}|^2} = \\begin{pmatrix}\\sum_{i=1}^{j} |p_{i} - q_{i}|\\end{pmatrix}^{1/2} \\end{align}\\] Notice that the only difference between the two measurements is the radicals (see Figure 9.19). Figure 9.19: Euclidean and Manhattan Distances 9.4.3 Minkowski and Chebyshev (Supremum) Distance Minkowski and Supremum distance measurements also follow the same identical formula as Manhattan and Euclidean distance measurements. The only difference is also with the radicals. \\[\\begin{align} D_{minkowski}(P,Q)=\\sqrt[n]{|p_{1} - q_{1}|^n + ...+ |p_{j}-q_{j}|^n} = \\begin{pmatrix}\\sum_{i=1}^{j} |p_{i} - q_{i}|\\end{pmatrix}^{1/n} \\\\ \\lim_{n \\to \\infty}D_{chebyshev}(P,Q)=\\sqrt[n]{|p_{1} - q_{1}|^n + ...+ |p_{j}-q_{j}|^n} = \\begin{pmatrix}\\sum_{i=1}^{j} |p_{i} - q_{i}|\\end{pmatrix}^{1/n} \\end{align}\\] Let us compare the three popular distance measurements: manhattan, euclidean, minkowski (see Table 9.31). set.seed(142) v1 = round( runif(n=10, min=1, max=10) ) v2 = round( runif(n=10, min=1, max=10) ) n = 3 Vmanhattan = abs( v1 - v2 ) ^ 1 Veuclidean = abs( v1 - v2 ) ^ 2 Vminkowski = abs( v1 - v2 ) ^ n Sman = sum( Vmanhattan ) Seuc = sum( Veuclidean ) Smin = sum( Vminkowski ) Dman = Sman^(1/1); Deuc = Seuc^(1/2); Dmin = Smin^(1/3) ds = t( data.frame(v1,v2, abs( v1 - v2 ) ^ 1, abs( v1 - v2 ) ^ 2, abs( v1 - v2 ) ^ n )) colnames(ds) = c(seq(1,10,1)) rownames(ds) =c (&quot;**vector P**&quot;, &quot;**vector Q**&quot;, &quot;**manhattan $(Pi-Qi)^1$**&quot;, &quot;**euclidean $(Pi-Qi)^2$**&quot;, &quot;**minkowski $(Pi-Qi)^3$**&quot;) knitr::kable( head(ds, 20), caption = &quot;Distance Measurements&quot;, booktabs = TRUE) Table 9.31: Distance Measurements 1 2 3 4 5 6 7 8 9 10 vector P 9 7 10 6 8 9 4 7 5 5 vector Q 9 9 8 6 6 6 2 6 6 3 manhattan \\((Pi-Qi)^1\\) 0 2 2 0 2 3 2 1 1 2 euclidean \\((Pi-Qi)^2\\) 0 4 4 0 4 9 4 1 1 4 minkowski \\((Pi-Qi)^3\\) 0 8 8 0 8 27 8 1 1 8 Compute for Manhattan Distance: \\[\\begin{align} D_{manhattan}(P, Q) = \\sqrt[1]{sum(0, 2, 2, 0, 2, 3, 2, 1, 1, 2)} = \\sqrt[1]{15} = 15 \\end{align}\\] Compute for Euclidean Distance: \\[\\begin{align} D_{euclidean}(P, Q) = \\sqrt[2]{sum(0, 4, 4, 0, 4, 9, 4, 1, 1, 4)} = \\sqrt[2]{31} = 5.5678 \\end{align}\\] Compute for Minkowski Distance: \\[\\begin{align} D_{minkowski}(P, Q) = \\sqrt[3]{sum(0, 8, 8, 0, 8, 27, 8, 1, 1, 8)} = \\sqrt[3]{69} = 4.1016 \\end{align}\\] 9.4.4 Jaccard (Similarity and Distance) Given two vectors and comparing, one of the many ways to handle distance similarity is using Jaccard similarity. Jaccard similarity formula: \\[\\begin{align} Jaccard\\ Similarity = J(P,Q) = \\frac{\\sum_{i=1}^{j} min(p_{i},q_{i})}{\\sum_{i=1}^{j} max(p_{i},q_{i})} \\end{align}\\] Jaccard distance formula: \\[\\begin{align} Jaccard\\ Distance = D_{j}(P,Q) = 1 - J(P,Q) \\end{align}\\] Using the two formulas, we should be able to construct a simple example: set.seed(142) v1 = round( runif(n=10, min=1, max=10) ) v2 = round( runif(n=10, min=1, max=10) ) m = matrix( c(v1, v2), nrow=2, ncol=length(v1), byrow=TRUE) Jmin=apply(m, 2, min) Jmax=apply(m, 2, max) Jsimilarity = sum(Jmin)/sum(Jmax) Jdistance= 1 - Jsimilarity ds = t( data.frame(v1,v2, Jmin, Jmax)) colnames(ds) = c(seq(1,10,1)) rownames(ds) =c (&quot;**vector P**&quot;, &quot;**vector Q**&quot;, &quot;**min**&quot;, &quot;**max**&quot;) knitr::kable( head(ds, 20), caption = &#39;Jaccard Similarity&#39;, booktabs = TRUE ) Table 9.32: Jaccard Similarity 1 2 3 4 5 6 7 8 9 10 vector P 9 7 10 6 8 9 4 7 5 5 vector Q 9 9 8 6 6 6 2 6 6 3 min 9 7 8 6 6 6 2 6 5 3 max 9 9 10 6 8 9 4 7 6 5 Compute for Jaccard Similarity: \\[\\begin{align} J(P, Q) = \\frac{sum(9, 7, 8, 6, 6, 6, 2, 6, 5, 3)}{sum(9, 9, 10, 6, 8, 9, 4, 7, 6, 5)} = 0.7945 \\end{align}\\] Compute for Jaccard Distance: \\[\\begin{align} D_{j} = 1 - 0.7945 = 0.2055 \\end{align}\\] 9.4.5 Hamming Distance Hamming distance measures the distance between texts. In terms of calculation, it computes the number of iterations to replace letters between texts. The shortest iteration is the Hamming distance. For two-bit texts, it calculates the number of positions in the two texts that differ. The Hamming distance is three in the example below because positions 2, 6, and 7 are different between the two-bit strings. \\[ \\begin{array}{l} bit.string.1 = 0 1 1 0 0 1 1 0\\\\ bit.string.2 = 1 0 1 0 0 0 0 0 \\end{array} \\] 9.4.6 Mahalanobis Distance Mahalanobis distance is a distance measure between a point ( or distribution) to another distribution. It reduces the scale (variance) calculation into Euclidean distance. 9.4.7 Precision and Accuracy Often, we are asked to make crucial decisions. Decisions are made based on facts. If there is lacking facts, it definitely can post a challenge. Sometimes, facts do not come in full and may come only partially. However, then sometimes, some factual data are contrary to the truth. Therefore, we need to separate true positives from false positives and true negatives from false negatives. Given the responsibility to decide, we often want to be able to decide as accurately and precisely as possible. So, what can we do now? To answer that question, let us have a close look at the Figure 9.20: Figure 9.20: Actual vs Predict Table Now, there are a few metrics that will help in our decisions, all derived from Table . Precision is a measure of the ratio (or proportion) of subjects (samples) that are correctly identified (or labeled) as positive out of all the subjects (samples) that are predicted as positive. \\[\\begin{align} Precision = \\frac{True\\ Positive}{(True\\ Positive) + (False\\ Positive)} = \\frac{TP}{TP + FP} \\end{align}\\] Accuracy is a measure of the ratio (or proportion) of subjects (samples) that are correctly identified (or labeled) as positive or negative out of all the subjects (samples) predicted. \\[\\begin{align} Accuracy = \\frac{TP + TN}{TP + FP + FN + TN} \\end{align}\\] Sensitivity (Recall), also called probability of prediction (or detection), is a measure of the ratio (or proportion) of subjects (samples) that are correctly identified (or predicted or labeled) as positive, out of all subjects that are labeled as actually relevant. \\[\\begin{align} \\text{Sensitivity} = \\frac{True\\ Positive}{(True\\ Positive) + (False\\ Negative)} = \\frac{TP}{TP + FN} \\end{align}\\] Specificity is a measure of the ratio (or proportion) of subjects (samples) that are correctly identified (or predicted or labeled) as negative out of all subjects that are labeled as not actually relevant. \\[\\begin{align} \\text{Specificity} = \\frac{True\\ Negative}{(True\\ Negative) + (False\\ Positive)} = \\frac{TN}{TN + FP} \\end{align}\\] F1 Score as a measure that weighs Recall and Precision when there is an imbalance of class distribution. \\[\\begin{align} \\text{F1 Score} = \\frac{2*(Recall * Precision)}{(Recall + Precision)} = \\frac{2RP}{R+P} \\end{align}\\] Error Rate is a measure of inaccuracy. \\[\\begin{align} \\text{Error Rate} = 1 - \\text{Accuracy} \\end{align}\\] Jaccard Index(JI), also called Jaccard Similarity Coefficient, measures the interaction of two sets over their union. \\[\\begin{align} \\text{Jaccard Index} = \\frac{|A \\cap B|}{ |A\\cup B|} \\end{align}\\] On the other hand, JI is also the ratio (or proportion) of subjects (samples) that are correctly identified (or predicted or labeled) as positive out of all subjects predicted, excluding those labeled as true negative. \\[\\begin{align} \\text{Jaccard Index} = \\frac{TP}{TP + FP + FN} \\end{align}\\] 9.4.8 AUC on ROC We use receiver operating characteristic (ROC) to measure the performance of prediction or classification by evaluating the area under the curve (AUC) on ROC. ROC is generated using the True Positive Rate (TPR) and False Positive Rate (FPR) measures: \\[\\begin{align} \\underbrace{\\text{TPR} = \\frac{TP}{TP + FN}}_\\text{sensitivity} \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{FPR} = \\frac{FP}{FP + TN}}_\\text{1 - specificity} \\end{align}\\] To illustrate, let us simulate a binomial process (e.g., tossing a coin 400 times). Let us try to achieve 50% of success - that the coin lands on heads with a 50% probability of success. Hopefully, we can get 200 for heads and 200 for tails using a random seed of 2020 - though that may not always be the case. So let us see if we are close enough. set.seed(2020) N=400 actual = rbinom(n=N, size=1, prob=0.50) heads = length(which(actual == 1)) tails = length(which(actual == 0)) c(&quot;Heads&quot; = heads, &quot;Tails&quot; = tails) ## Heads Tails ## 206 194 We get an outcome with 206 heads and 194 tails - let us take note of this as our ground truth. Now, let us simulate a perfect prediction (just merely cloning the outcome of 400 tosses). Our confusion table looks like so: predicted = actual table(as.factor(predicted), as.factor(actual), dnn = c(&quot;Predicted&quot;, &quot;Actual&quot;)) ## Actual ## Predicted 0 1 ## 0 194 0 ## 1 0 206 Alternatively, we can use confusionMatrix(.) from the caret library to help us get some of the metrics namely, accuracy, sensitivity, specificity, etc. library(caret) confusionMatrix(as.factor(predicted), as.factor(actual)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 194 0 ## 1 0 206 ## ## Accuracy : 1 ## 95% CI : (0.991, 1) ## No Information Rate : 0.515 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 1 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.000 ## Specificity : 1.000 ## Pos Pred Value : 1.000 ## Neg Pred Value : 1.000 ## Prevalence : 0.485 ## Detection Rate : 0.485 ## Detection Prevalence : 0.485 ## Balanced Accuracy : 1.000 ## ## &#39;Positive&#39; Class : 0 ## Notice that most metrics such as accuracy, sensitivity, and specificity all show a value of 1. That indicates we have a perfect prediction. However, suppose our outcome turns out to be far from being perfect. Let us simulate three separate exercises and show the confusion table. set.seed(142) pred1 = exercise1.prediction = rbinom(n=N, size=1, prob=0.90) pred2 = exercise2.prediction = rbinom(n=N, size=1, prob=0.70) pred3 = exercise3.prediction = rbinom(n=N, size=1, prob=0.20) The first set of predictions follows a sample binomial distribution with a 90% probability of success that the coin lands on heads. The second set shows a 70% probability, and the third set shows a 20% probability. Let us show the confusion table for the first exercise using its prediction and the ground truth: (conf.tab = table(as.factor(pred1), as.factor(actual), dnn = c(&quot;Predicted&quot;, &quot;Actual&quot;))) ## Actual ## Predicted 0 1 ## 0 19 24 ## 1 175 182 The table shows that our prediction has 182 true positives (TP), 175 false positives (FP), 19 true negatives (TN), and 24 false negatives (FN). In terms of metrics, the specificity is 0.098 based on 19 / (19 + 175). The sensitivity is 0.883 based on 182 / (182 + 24). The accuracy is 0.502 based on (182 + 19) / (19 + 175 + 24 + 182). The true positive rate (TPR) is 0.883 based on 182 / (182 + 24). This is also equivalent to sensitivity. The false positive rate (TPR) is 0.902 based on 175 / (175 + 19). This is also equivalent to (1-specificity). Let us show the confusion table for the second exercise using its prediction and the ground truth: (conf.tab = table(as.factor(pred2), as.factor(actual), dnn = c(&quot;Predicted&quot;, &quot;Actual&quot;))) ## Actual ## Predicted 0 1 ## 0 52 63 ## 1 142 143 The table shows that our prediction has 143 true positives (TP), 142 false positives (FP), 52 true negatives (TN), and 63 false negatives (FN). In terms of metrics, the specificity is 0.268 based on 52 / (52 + 142). The sensitivity is 0.694 based on 143 / (143 + 63). The accuracy is 0.488 based on (143 + 52) / (52 + 142 + 63 + 143). The true positive rate (TPR) is 0.694 based on 143 / (143 + 63). This is also equivalent to sensitivity. The false positive rate (TPR) is 0.732 based on 142 / (142 + 52). This is also equivalent to (1-specificity). Finally, let us show the confusion table for the third exercise using its prediction and the ground truth: (conf.tab = table(as.factor(pred3), as.factor(actual), dnn = c(&quot;Predicted&quot;, &quot;Actual&quot;))) ## Actual ## Predicted 0 1 ## 0 151 158 ## 1 43 48 The table shows that our prediction has 48 true positives (TP), 43 false positives (FP), 151 true negatives (TN), and 158 false negatives (FN). In terms of metrics, the specificity is 0.778 based on 151 / (151 + 43). The sensitivity is 0.233 based on 48 / (48 + 158). The accuracy is 0.498 based on (48 + 151) / (151 + 43 + 158 + 48). The true positive rate (TPR) is 0.233 based on 48 / (48 + 158). This is also equivalent to sensitivity. The false positive rate (TPR) is 0.222 based on 43 / (43 + 151). This is also equivalent to (1-specificity). We showed three different sets of predictions and how they are interpreted in the table.Let us get a more complete and detailed interpretation using confusionMatrix(.) function. Here, we use the predictions from the third exercise and compare the metrics below. library(caret) confusionMatrix(as.factor(pred3), as.factor(actual)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 151 158 ## 1 43 48 ## ## Accuracy : 0.498 ## 95% CI : (0.447, 0.548) ## No Information Rate : 0.515 ## P-Value [Acc &gt; NIR] : 0.774 ## ## Kappa : 0.011 ## ## Mcnemar&#39;s Test P-Value : 8.92e-16 ## ## Sensitivity : 0.778 ## Specificity : 0.233 ## Pos Pred Value : 0.489 ## Neg Pred Value : 0.527 ## Prevalence : 0.485 ## Detection Rate : 0.378 ## Detection Prevalence : 0.772 ## Balanced Accuracy : 0.506 ## ## &#39;Positive&#39; Class : 0 ## For Multi-Classification, let us defer the subject in Chapter 10 (Computational Learning II) under AdaBoost Section in which we cover confusion matrix for multi-classes. Now, in terms of AUC on ROC, to get an intuition around the concept, it helps to tailor our dataset such that we construct four different predictions coming from four separate processes. N = 1000 pos = rep(1, N); neg = rep(0, N) pred = c(pos, neg) norm1 = list(&quot;mean&quot; = 2.0, &quot;sd&quot; = 0.2) norm2 = list(&quot;mean&quot; = 1.9, &quot;sd&quot; = 0.2) norm3 = list(&quot;mean&quot; = 1.7, &quot;sd&quot; = 0.2) norm4 = list(&quot;mean&quot; = 1.5, &quot;sd&quot; = 0.2) norm5 = list(&quot;mean&quot; = 1.3, &quot;sd&quot; = 0.2) pos.prob = rnorm(n=N, mean=norm1$mean, sd=norm1$sd) neg.prob = list() neg.prob[[1]] = rnorm(n=N, mean=norm2$mean, sd=norm2$sd) neg.prob[[2]] = rnorm(n=N, mean=norm3$mean, sd=norm3$sd) neg.prob[[3]] = rnorm(n=N, mean=norm4$mean, sd=norm4$sd) neg.prob[[4]] = rnorm(n=N, mean=norm5$mean, sd=norm5$sd) Let us review the four plots in Figure 9.21. Figure 9.21: Observation vs Expectation To understand the four plots, we have two distributions. The left-side blue distribution indicates all positives, and the right-side red distribution indicates all negatives. We put in place two thresholds. The left-side blue threshold marks the cut-off for the positive distribution, and the right-side red threshold marks the cut-off for the negative distribution. As the positive distribution moves towards the left side, most of its region gets into the false-positive territory. As the negative distribution moves towards the right side, most of its region gets into the false-negative. For simplicity, we made the positive distribution constant (not moving) and moved the negative distribution further away from its cut-off, meaning, as it moves farther away, it gets lesser to the false-negative territory. That also means improvement of AUC. Another interpretation without using the cut-off is that the more overlap between both distributions, the lesser the AUC becomes - prediction power lessens. Let us use roc(.) function in a 3rd-party library called pROC to illustrate. See Figure 9.22 library(pROC) color = c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot; ,&quot;brown&quot;) plot(NULL, xlim=range(0,1) , ylim=range(0,1), ylab=&quot;True Positive Rate (Sensitivity)&quot;, xlab=&quot;False Positive Rate (1 - Specificity)&quot;, main=&quot;AUC on ROC&quot;) auc = rep(0, 4) for (i in 1:4) { prob = c(pos.prob, neg.prob[[i]]) roc = roc(pred ~ prob, quiet=TRUE) TPR = roc$sensitivities FPR = 1 - roc$specificities lines(FPR, TPR, col=color[i]) auc[i] = round(roc$auc,3) } abline( a=0, b=1, lty=2 ) legend(0.4, 0.3, legend=c( paste0(&quot;predict.1 (auc=&quot;,auc[1],&quot;)&quot;), paste0(&quot;predict.2 (auc=&quot;,auc[2],&quot;)&quot;), paste0(&quot;predict.3 (auc=&quot;,auc[3],&quot;)&quot;), paste0(&quot;predict.4 (auc=&quot;,auc[4],&quot;)&quot;)), col=color, pch=20, cex=0.8) legend(0.60, 0.67, legend=c( &quot;0.90 - 1.00 = excellent&quot;, &quot;0.80 - 0.90 = good&quot;, &quot;0.70 - 0.80 = fair&quot;, &quot;0.60 - 0.70 = poor&quot;, &quot;0.50 - 0.60 = fail&quot; ), col=c(&quot;black&quot;), pch=c(16,16,16), cex=0.8) Figure 9.22: AUC on ROC To put a better interpretation of the AUC, we map the score like so: score &lt;- function(auc) { if(auc &lt;= 0.60) return(&quot;fail&quot;); if(auc &gt; 0.60 &amp;&amp; auc &lt;= 0.70) return(&quot;poor&quot;) if(auc &gt; 0.70 &amp;&amp; auc &lt;= 0.80) return(&quot;fair&quot;) if(auc &gt; 0.80 &amp;&amp; auc &lt;= 0.90) return(&quot;good&quot;) if(auc &gt; 0.90 &amp;&amp; auc &lt;= 1.00) return(&quot;excellent&quot;) } Based on the AUC result, prediction 1 is scored as poor. It means that the process producing the predictions does not provide enough prediction power to produce outcomes close to the actual values. For the other predictions, we have the following AUC. c(&quot;prediction 1&quot; = auc[1], &quot;prediction 2&quot;=auc[2], &quot;prediction 3&quot; = auc[3], &quot;prediction 4&quot;=auc[4] ) ## prediction 1 prediction 2 prediction 3 prediction 4 ## 0.644 0.866 0.963 0.995 We cover Feature Selection later in one of the sections ahead using another two 3rd-party packages called ROCR and randomForest. 9.5 Exploratory Data Analysis Exploratory Data Analysis (EDA) aims to study the underlying structure of data and to discover patterns and associations, including effects and influences. We have covered the nature of data distribution in previous chapters. We also covered statistical and bayesian analysis around data. This section introduces a few concepts in Data Mining to supplement our understanding of Exploratory Analysis. Data Mining is a superset course that includes Exploratory Data Analysis (EDA). Though there are many more topics in Data Mining that are not possible to cover in this book, let us frame the idea of Data Mining in the context of only the following topics: Data Cleaning Association Pattern Discovery Null Invariance Correlation and Collinearity Covariance Missingness and Imputation Others (e.g., Outliers, Leverage, Influence, Data Leakage, and Confounding Variables) 9.5.1 Data Cleaning (Wrangling) Data Cleaning is essential before handling Exploratory Data Analysis methods. Specialists in Data Analysis offer best practices to address this area (e.g., Osborne, J. W. 2013). Notwithstanding a thorough coverage, let us instead briefly list eight dimensions of Data Quality that can be used as a guide to know if our Data is Clean. Other literature may list only six dimensions of different combinations out of the eight dimensions. We reference articles that cover them (Elgabry O. 2019; Gupta A. 2021): Accuracy - Data is as close to its true value. Completeness - Data is as comprehensive. Consistency - Values of the same Data are the same everywhere. Validity - Data is within the prescribed constraints. Timeliness - Data is available as required. Uniformity - Data is uniform everywhere in terms of presentation. Uniqueness - Data is not duplicated anywhere. Integrity - Data references are intact (See Referential Integrity in Structured Data) The quality of data can also be improved during the gathering, filtering, and transforming (ETL) of data. 9.5.2 Association To explain the intuition behind Association, suppose we shop online and make a few orders (corresponding to a few transactions recorded into our database). Each order has a list of ordered items. Our goal is to find the most frequent set of ordered items called itemset across a set of transactions. We can use any of three algorithms starting with Apriori Algorithm (note here that we refer to a set of transactions as transactional database). Apriori Algorithm To explain Apriori Algorithm, let us use Figure 9.23. Figure 9.23: Apriori Algorithm Figure 9.23 illustrates the classic technique called Apriori algorithm used to find the most frequent itemsets (Srikant, R., &amp; Agrawal, R. 1996). In the figure, we notice multiple scans. Each scan results in a table of itemsets with corresponding frequency. For example, in table C1, we see an itemset (group) that contains only item A. It shows that this item in the first itemset counts 3. As we go to the next scan, we only consider itemsets with a frequency equal to or greater than the minimum support. We then perform the same count, but this time, notice that table C2 has itemsets (groups) that contain two items. The Apriori Algorithm is described below: \\[ \\begin{array}{l} L_1 = \\text{\\{frequent 1-itemset\\}}\\\\ \\text{For k in 2,... repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ C_k\\ \\text{= candidate k-itemsets from }L_{k-1}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{For t in 1,... repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{tx = transaction[t]}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{For each k-itemset in tx, repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{count frequency}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ L_k\\ \\text{= All candidate k-itemsets from } C_k \\text{ with minimum support/frequency}\\\\ \\end{array} \\] Grouping items into itemsets based on frequency is one type of association, and it follows specific association rules as a measure for the effectiveness of the association (note that we cover more measures in the Null-Invariant section). Association rules Support measures the frequency or number of occurrences of an itemset. An absolute support measures the number of occurrences. A relative support measures the fraction of the number of transactions in which the itemset occurs out of all transactions. For example, item A in Figure 9.23 is a 1-itemset with absolute Support of 3 and relative Support of 60% because it shows occurrences in three transactions out of five transactions. \\[ Relative\\ Support = P(A) = S(A) = \\frac{\\text{transaction in which A occurs}}{\\text{total transactions}}= \\frac{3}{5} = 0.60 \\] Confidence measures the probability that an itemset, say a 2-itemset containing A and B, will occur conditioned on the occurrence of an itemset, say A. It measures the fraction of the number of transactions in which A and B occur out of all transactions in which A occurs. For example, in Figure 9.23, there are three transactions in which B occurs. There is one transaction in which both A and B occur. Therefore, calculating the confidence of A and B to occur together in the same transaction, conditioned on B, we have: \\[\\begin{align*} Confidence &amp;= \\frac{P(A,B)}{P(A)} = C(A \\rightarrow B) = \\frac{\\text{transactions in which A and B occur}}{\\text{transactions in which A occurs}} \\\\ &amp;= \\frac{1}{3} = 0.33 \\end{align*}\\] Lift measures the ratio of confidence over expected confidence. For example, if A and B will occur conditioned on the occurrence of an item, say A; then the expected confidence is the relative support of B. \\[ Lift = L(A,B) = \\frac{P(A,B)}{P(A)P(B)} = \\frac{C(A \\rightarrow B)}{S(B)} = \\frac{ \\frac{\\text{tx in which A and B occur}}{\\text{tx in which B occurs}} }{ \\frac{\\text{tx in which B occurs}}{\\text{total transactions}} } = \\frac{ \\frac{1}{3}}{\\frac{3}{5}} = 0.56 \\] The candidate itemsets are selected based on the rules above. For example, in Figure 9.23, we have chosen a minimum support of 2 to limit the itemsets based on that frequency limit. The end result shows that our 3-itemset with the most number of occurrences is \\(\\{A, C, D\\}\\). Note that Apriori algorithm requires a repeated scan of the transactional database to generate candidate itemsets. In the next portion of this section, we cover two other alternatives. One of the two algorithms eliminates the need to scan the database repeatedly. FP-growth Algorithm The Frequent Pattern (FP) growth algorithm, also called FP-Growth algorithm, is proposed by Han J. et al. (2000) in that it avoids candidate generation found in Apriori algorithm when generating frequent itemsets. It scans transactions in a database to arrange the candidate itemsets in descending order and then makes one final scan to construct a compact FP-tree structure based on the ordered itemsets. See Figure 9.24. Figure 9.24: FP-Growth Data The algorithm proceeds as follows (granting our minimum support equals 2): First, generate a frequent 1-itemset by scanning the transaction database. \\[ \\text{\\{O:5\\},\\{A:4\\},\\{C:4\\},\\{R:4\\},\\{D:2\\},\\{E:2\\},\\{P:2\\},\\{B:2\\},\\{F:2\\}} \\] Second, generate an ordered list of k-itemsets by re-scanning the transaction database for each transaction based on the 1-itemset generated. See the first table in Figure 9.24 for the (Ordered) itemsets. Third with the ordered k-itemsets, construct the FP-tree. For example, starting with the first transaction TID=101, we create a tree with the Null {} node being the root. \\[ \\text{\\{\\}}\\rightarrow \\text{\\{O:1\\}}\\rightarrow \\text{\\{A:1\\}} \\rightarrow \\text{\\{C:1\\}}\\rightarrow \\text{\\{R:1\\}}\\rightarrow \\text{\\{B:1\\}} \\] Then, we read the second transaction TID=102 and continue to build the tree. \\[ \\text{\\{\\}}\\rightarrow \\text{\\{O:2\\}}\\rightarrow \\text{\\{A:2\\}} \\underset{\\text{\\{D:1\\}}\\rightarrow \\text{\\{E:1\\}}\\rightarrow \\text{\\{P:1\\}}\\rightarrow \\text{\\{B:1\\}}}{ \\underset{\\downarrow}{ \\rightarrow \\text{\\{C:2\\}}\\rightarrow \\text{\\{R:2\\}}\\rightarrow \\text{\\{B:1\\}}}} \\] Notice that while we traverse the tree, we increment the node corresponding to each occurring item in the current transaction. We also branch out to a new node if an occurring item reaches a node with no other neighboring node corresponding to the item. In our case, for the second transaction, {R:2} is the last matching node. So we branch out by creating a new node, namely {D:1}, and proceed with building the branch. Then, we read the third transaction TID=103 and continue to build the tree: \\[ \\underset{\\{D:1\\}\\rightarrow \\{E:1\\} \\rightarrow \\{P:1\\} }{ \\underset{\\downarrow}{ \\text{\\{\\}} \\rightarrow \\text{\\{O:3\\}}\\rightarrow }} \\text{\\{A:2\\}} \\underset{\\text{\\{D:1\\}}\\rightarrow \\text{\\{E:1\\}}\\rightarrow \\text{\\{P:1\\}}\\rightarrow \\text{\\{B:1\\}}}{ \\underset{\\downarrow}{ \\rightarrow \\text{\\{C:2\\}}\\rightarrow \\text{\\{R:2\\}}\\rightarrow \\text{\\{B:1\\}}}} \\] Here, we create a new branch from {O:3}. We continue processing the next transactions. The FP-tree is shown in Figure 9.24. Fourth, we create a conditional pattern base table column. We use the FP-tree to populate the conditional pattern base. For each of the 1-itemset, we traverse the tree to the root and build the k-itemset based on the traversed path. For example, the 1-itemset {F} forms a path to the root, namely {O, A, C, R:2}. That is a new 4-itemset. The number 2 in the new 4-itemset corresponds to the number in the node of the 1-itemset {F}, namely {F:2}. In another example, there are two paths for 1-itemset {E}. The first one forms a path to the root, namely {O, A, C, R, D:1}, and the second one forms another path to the root, namely {O, D:1}. Finally, we use the conditional pattern base column to generate our frequent patterns by intersecting the ordered itemsets and its 1-itemset for each unique item: \\[\\begin{align*} \\text{A-conditional = }&amp;\\text{\\{A,O:4\\}} \\\\ \\text{C-conditional = }&amp;\\text{\\{O,C:4\\},\\{A,C:4\\},\\{O,A,C:4\\}}\\\\ \\text{R-conditional = }&amp;\\text{\\{O,R:4\\},\\{A,R:4\\},\\{C,R:4\\},\\{O,A,R:4\\},\\{O,C,R:4\\},\\{O,A,C,R:4\\}}\\\\ \\text{D-conditional = }&amp;\\text{\\{O,D:2\\}}\\\\ \\text{E-conditional = }&amp;\\text{\\{O,E:2\\},\\{D,E:2\\}, \\{O,D,E:2\\}}\\\\ \\text{P-conditional = }&amp;\\text{\\{O,P:2\\},\\{D,P:2\\},\\{E,P:2\\},\\{O,D,P:2\\},\\{O,E,P:2\\},,\\{O,D,E,P:2\\}}\\\\ \\text{B-conditional = }&amp;\\text{\\{O,B:2\\},\\{A,B:2\\},\\{C,B:2\\},\\{R,B:2\\},\\{O,A,B:2\\},\\{O,C,B:2\\},\\{O,R,B:2\\}}\\\\ &amp; \\text{\\{A,R,B:2\\},\\{O,A,C,B:2\\},\\{O,A,R,B:2\\},\\{A,C,R,B:2\\},\\{O,C,R,B:2\\}}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ &amp;\\text{\\{O,A,C,R,B:2\\}}\\\\ \\text{F-conditional = }&amp;\\text{\\{O,F:2\\},\\{A,F:2\\},\\{C,F:2\\},\\{R,F:2\\},\\{O,A,F:2\\},\\{O,C,F:2\\},\\{O,R,F:2\\}}\\\\ &amp;\\text{\\{A,R,F:2\\},\\{O,A,C,F:2\\},\\{O,A,R,F:2\\},\\{A,C,R,F:2\\},\\{O,C,R,F:2\\}}\\\\ &amp;\\text{\\{O,A,C,R,F:2\\}} \\end{align*}\\] For D-conditional, E-conditional, and P-conditional, we discard any pattern having any of the following items &lt;A, B, C&gt; because the patterns will generate a frequency below the minimum. Similarly, for B-conditional, we discard any pattern having &lt;D, E, P&gt;. Eclat Algorithm The Equivalence Class Transformation (Eclat) algorithm is a Depth First Search algorithm that uses a vertically transposed data format. The algorithm proceeds as follows: First, get the transaction list for each item. For example, for item A, we have the following list of transactions: 101, 103, 104. See Figure 9.25. Figure 9.25: Eclat Data Second, list the frequently occurring patterns by intersecting the transaction list of each item with the transaction list of the other items. For example, suppose our minimum support equals 2. We then have the following intersection: \\[ \\begin{array}{llllll} A \\cap B &amp;= \\text{\\{104\\}} &amp; A \\cap C \\cap D &amp;= \\text{\\{104,103\\}} &amp; A \\cap C \\cap D \\cap E &amp;= \\text{\\{103\\}}\\\\ A \\cap C &amp;= \\text{\\{101,103\\}} &amp; A \\cap C \\cap E &amp;= \\text{\\{103\\}} \\\\ A \\cap D &amp;= \\text{\\{101,103\\}} &amp; A \\cap D \\cap E &amp;= \\text{\\{103\\}} \\\\ A \\cap E &amp;= \\text{\\{103\\}} \\end{array} \\] Third, with minimum support of 2, we reduce the list to the following: \\[ \\begin{array}{llll} A \\cap C &amp;= \\text{\\{101,103\\}} &amp; A \\cap C \\cap D &amp;= \\text{\\{104,103\\}}\\\\ A \\cap D &amp;= \\text{\\{101,103\\}} \\end{array} \\] Therefore the below itemsets are more likely to occur than other associations: \\[ \\text{\\{A,C\\}, \\{A,D\\}, \\{A,C,D\\}} \\] We leave readers to investigate variants and enhancements made to the algorithms mentioned above, particularly in computational costs such as space savings by compression. 9.5.3 Pattern Discovery In this section, we introduce algorithms that deal with items not only for their association but also to discover patterns that have sequential characteristics. Unlike a transaction database in which we have a list of transactions identified by transaction IDs, each containing an unordered list of items, we operate on a database with sequences identified by sequence IDs, each containing a sequentially-ordered list of items. We refer to a set of sequences as sequential database). See Figure 9.26. Figure 9.26: Sequential Data For example, the sequence with ID equal to 102 has 7 elements. \\[ \\text{&lt;(ABC)DEF(GH)(IJ)K&gt;}\\ \\ \\rightarrow \\] \\[ \\underbrace{ \\underbrace{\\text{&lt;(ABC)}}_{\\begin{array}{c}\\text{1st}\\\\ \\text{element}\\\\ \\text{(unordered)}\\end{array}} \\underbrace{ \\underbrace{\\text{D}}_{\\begin{array}{c}\\text{2nd}\\\\ \\text{element}\\end{array}} \\underbrace{\\text{E}}_{\\begin{array}{c}\\text{3rd}\\\\ \\text{element}\\end{array}} \\underbrace{\\text{F}}_{\\begin{array}{c}\\text{4th}\\\\ \\text{element}\\end{array}} }_\\text{(ordered)} \\underbrace{\\text{(GH)}}_{\\begin{array}{c}\\text{5th}\\\\ \\text{element}\\\\ \\text{(unordered)}\\end{array}} \\underbrace{\\text{(IJ)}}_{\\begin{array}{c}\\text{6th}\\\\ \\text{element}\\\\ \\text{(unordered)}\\end{array}} \\underbrace{\\text{K&gt;}}_{\\begin{array}{c}\\text{7th}\\\\ \\text{element}\\\\ \\text{(ordered)}\\end{array}} }_\\text{(ordered)} \\] Note the below alternative longer notation: \\[ \\text{&lt;(ABC)DEF(GH)(IJ)K&gt;} \\equiv \\text{&lt;(ABC)}\\rightarrow \\text{D}\\rightarrow\\text{E}\\rightarrow\\text{F}\\rightarrow\\text{(GH)}\\rightarrow\\text{(IJ)}\\rightarrow\\text{K&gt;} \\] Also, note that an unordered element produces a list of possible unordered combinations (vs ordered permutation). For example, any of the below combinations can represent the unordered element (ABC). \\[ (ABC) = \\text{ (A), (B), (C), (AB), (AC), (CB), ()} \\] We also can interpret the element (ABC) to mean that a customer ordered items A, B, and C together. On the other hand, a second customer may order a subset of the pattern, e.g., items A and B together, items B and C together, or items A and C together. Then a third customer may order only item A or nothing at all. So that if the element is part of a sequence like so, then the element can have any of the following sub-sequences: \\[ \\underbrace{\\text{&lt;D(A)E&gt;}}_\\text{sub-sequence}, \\underbrace{\\text{&lt;D(AB)E&gt;}}_\\text{sub-sequence}, \\underbrace{\\text{&lt;D(AC)E&gt;}}_\\text{sub-sequence}, \\underbrace{\\text{&lt;D(CB)E&gt;}}_\\text{sub-sequence}, \\underbrace{\\text{&lt;DE&gt;}}_\\text{sub-sequence},... \\ \\ \\ \\ \\ \\in \\underbrace{\\text{&lt;D(ABC)E&gt;}}_\\text{sequence} \\] The sub-sequence &lt; D(CB)E &gt; means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with items C and B together, then followed by a third-order with item E. The sub-sequence &lt; DE &gt; means that we see a pattern in which a customer made a sequence of orders in which the first order has item D followed by a second order with item E. there are no other orders made with any combination containing items A, B, and C together. Let us now introduce a few algorithms that allow us to discover (or mine) sequential patterns: Generalized Sequential Patterns (GSP) GSP uses Apriori algorithm to discover sequential patterns (Srikant, R., &amp; Agrawal, R. 1996). The algorithm proceeds as such (assume the maximum length of a sequence of interest is N): \\[ \\begin{array}{l} F_1 = \\text{\\{frequent 1-sequence\\}}\\\\ \\text{For k in 2,...,N repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ C_k\\ \\text{= candidate k-sequence from }F_{k-1}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{For i in 1,... repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{sq = sequence[i]}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{For each k-sequence in sq, repeat the following}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{count frequency}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ F_k\\ \\text{= All candidate k-sequence from } C_k \\text{ with minimum support/frequency}\\\\ \\end{array} \\] First, using Figure 9.26, let us generate the following length-1 candidate subsequences: \\[\\begin{align*} \\text{&lt;A&gt;:5},\\ \\text{&lt;B&gt;:5},\\ \\text{&lt;C&gt;:4},\\ \\text{&lt;D&gt;:3},\\ \\text{&lt;E&gt;:3},\\ \\text{&lt;F&gt;:3},\\\\ \\text{&lt;G&gt;:1},\\ \\text{&lt;H&gt;:1},\\ \\text{&lt;I&gt;:1},\\ \\text{&lt;J&gt;:1},\\ \\text{&lt;K&gt;:1} \\end{align*}\\] From there, we keep subsequences that meet the minimum support (e.g. minsup = 2). \\[ \\text{&lt;A&gt;:5},\\ \\ \\text{&lt;B&gt;:5},\\ \\ \\text{&lt;C&gt;:4},\\ \\ \\text{&lt;D&gt;:3},\\ \\ \\text{&lt;E&gt;:3},\\ \\ \\text{&lt;F&gt;:3} \\] Second, let us mine for length-2 sequential pattern using the list of items with minsup=2. Here, we construct a frequent item matrix. See Figure 9.27. Figure 9.27: GSP (51 2-length sequence) Based on the matrix, we see 17 length-2 sequential patterns that meet the minsup = 2. Third, we proceed mining for length-3 sequential patterns. Figure 9.28: GSP (6 3-length sequence) That gives us the following length-3 sequential patterns that meet the minsup = 2. A few of the patterns are listed below. \\[ \\text{&lt;ABF&gt;,&lt;ACF&gt;,&lt;ADE&gt;,&lt;BCF&gt;,&lt;CDE&gt;},\\ \\ \\cdots \\] We labeled the third column as 3-item sequence, which is different from the length-3 sequential pattern. For example, (AB)C is a 3-item sequence but falls under length-2 sequential patterns because it has only two elements, namely (AB) being an unordered element and C. Fourth, we continue to form all other length-k sequences until we reach a point where we do not have any other sequence that meets our minimum support of 2. Sequential Pattern Discovery using Equivalence classes (SPADE) SPADE is an alternative algorithm that uses ECLAT algorithm for its association (Zaki M.J. 2001). The data is vertically-formatted for Depth First Search pattern discovery. Figure 9.29: SPADE To illustrate the algorithm, we use Figure 9.29. First, we scan the sequential database for all sequences (see table T1) to construct an element table (see table T2). For example, the first element of the first sequence corresponds to the following: \\[ \\text{SID}^\\text{(sequence id)} = 101,\\ \\ \\ \\ \\ \\ \\ \\ \\text{EID}^\\text{(element id)} = 1,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Item} = \\text{A} \\] The second element of the first sequence corresponds to the following: \\[ \\text{SID}^\\text{(sequence id)} = 101,\\ \\ \\ \\ \\ \\ \\ \\ \\text{EID}^\\text{(element id)} = 2,\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Item} = \\text{AB} \\] Second, we use table T2 to construct our vertically-formatted length-2 sequence table (see table T3). For example, item A is a length-1 sequential pattern that has the following sequence ids and element ids: \\[ \\text{length-1 sequence for A = }\\left[ \\begin{array}{ll} 101 &amp; 1\\\\101 &amp; 2\\\\102 &amp; 1\\\\103 &amp; 1\\\\ \\vdots &amp; \\vdots \\end{array} \\right] \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{length-1 sequence for B = }\\left[ \\begin{array}{ll} 101 &amp; 2\\\\102 &amp; 1\\\\103 &amp; 2\\\\104 &amp; 1\\\\ \\vdots &amp; \\vdots \\end{array} \\right] \\] Third, we continue to use table T2 to construct our next vertically-formatted length-2 sequence table (see table T4). For example, item AB is a length-2 sequential pattern. The first entry in the table corresponds to the first sequence with SID=101 containing item A being the first element (EID=1) in the sequence and item B being one of the combinations of the second element (AB) with EID=2 in the sequence. Note that table T5 is just an extension of table T4 in which item BA is also a length-2 sequential pattern. The entry in the table corresponds to the fifth sequential pattern with SID=105 containing item B being one of the combinations of the second element (AB) in the sequence and item A being one of the combinations of the fourth element (FA) in the sequence. Fourth, we construct our next vertically-formatted length-3 sequence table using T2. The pattern corresponds to the following entry: \\[ \\text{ABC = } \\left[ \\begin{array}{llll} 101 &amp; 1 &amp; 2 &amp; 3\\\\ 105 &amp; 1 &amp; 2 &amp; 3 \\end{array} \\right] \\ \\ \\ \\ \\ \\ \\ \\ \\text{CDE = } \\left[ \\begin{array}{llll} 102 &amp; 1 &amp; 2 &amp; 3\\\\ 104 &amp; 1 &amp; 2 &amp; 3 \\end{array} \\right] \\ \\ \\ \\ \\ \\ \\ \\ \\cdots \\] To interpret the length-3 sequential pattern, we see a pattern in which a customer made a sequence of orders in which the first order has item A followed by a second order with item B and another order with item C. We see a frequency of two (one from SID=101 and the other from SID=105). Fifth, we continue to form all other length-k sequences until we reach a point where we do not have any other sequences that meet our minimum support of 2. Frequent Pattern-Projected Sequential Pattern (FreeSpan) FreeSpan uses a divide-and-conquer approach operating on projected subsequences based on length-k sequential patterns. These subsequences are the frequent pattern length-k projections (Han J. et al. 2000). To illustrate the FreeSpan algorithm, we proceed as follows: First, using the sequence table in Figure 9.26, we generate a frequent item list (or f_list) of frequent length-1 sequential patterns that meet a minimum support of 2 (sorted in descending order): \\[ \\text{&lt;A&gt;:5},\\ \\ \\text{&lt;B&gt;:5},\\ \\ \\text{&lt;C&gt;:4},\\ \\ \\text{&lt;D&gt;:3},\\ \\ \\text{&lt;E&gt;:3},\\ \\ \\text{&lt;F&gt;:3} \\] Second, based on the generated length-1 sequential pattern above, we construct our frequent item matrix (Han J. et al. 2000). The rows and columns are ordered in descending order of support. See Figure 9.30. Figure 9.30: FreeSpan (1-projection) To explain, each cell in the matrix is represented by the intersection of two items and has the format (X, Y, Z) so that if the two items are A and B, then X is the number of occurrences for the sequential pattern \\(\\mathbf{\\text{&lt;AB&gt;}}\\), Y is the number of occurrences for the sequential pattern \\(\\mathbf{\\text{&lt;BA&gt;}}\\), and Z is the number of occurrence for the sequential pattern \\(\\mathbf{\\text{&lt;(AB)&gt;}}\\). For example, in Figure 9.26, for items A and B, the intersecting cell has (3,1,5) which comes from the following: For X corresponding to \\(\\mathbf{\\text{&lt;AB&gt;}}\\): \\[ X:3\\ \\ \\ \\rightarrow 101:&lt;\\underline{A}(A\\underline{B})CF&gt;, 103:&lt;\\underline{A}(A\\underline{B})(ED)&gt;, 105:&lt;\\underline{A}(A\\underline{B})C(AF)&gt; \\] For Y corresponding to \\(\\mathbf{\\text{&lt;BA&gt;}}\\): \\[ Y:1\\ \\ \\ \\rightarrow 105:&lt;A(A\\underline{B})C(\\underline{A}F)&gt; \\] For Z corresponding to \\(\\mathbf{\\text{&lt;(AB)&gt;}}\\): \\[\\begin{align*} Z:5\\ \\ \\ {}&amp;\\rightarrow 101:&lt;A(\\underline{A}\\underline{B})CF&gt;, 102:&lt;(\\underline{A}\\underline{B}C)DEF(GH)(IJ)K&gt;, \\\\ \\ \\ \\ &amp;\\rightarrow 103:&lt;A(\\underline{A}\\underline{B})(ED)&gt;, 104:&lt;(\\underline{A}\\underline{B}C)(CD)E&gt;, 105:&lt;A(\\underline{A}\\underline{B})C(AF)&gt; \\end{align*}\\] Finally, for the cell intersecting A (column-wise) and A (row-wise), we see the following combination: \\[ &lt;AA&gt;:3\\ \\ \\ \\rightarrow 101:&lt;\\underline{A}(\\underline{A}B)CF&gt;, 103:&lt;\\underline{A}(\\underline{A}B)(ED)&gt;, 105:&lt;\\underline{A}(\\underline{A}B)C(AF)&gt; \\] Third, let us find length-2 sequential patterns based on the frequent item matrix (given a minsup = 2). \\[\\begin{align*} A{}&amp;:\\ \\ \\ \\ \\ \\text{&lt;AA&gt;:3}\\\\ B&amp;:\\ \\ \\ \\ \\ \\text{&lt;AB&gt;:3},\\text{&lt;(AB)&gt;:5}\\\\ C&amp;:\\ \\ \\ \\ \\ \\text{&lt;AC&gt;:3},\\text{&lt;(AC)&gt;:2},\\text{&lt;BC&gt;:3},\\text{&lt;(BC)&gt;:2}\\\\ D&amp;:\\ \\ \\ \\ \\ \\text{&lt;AD&gt;:3},\\text{&lt;BD&gt;:3},\\text{&lt;CD&gt;:2}\\\\ E&amp;:\\ \\ \\ \\ \\ \\text{&lt;AE&gt;:3},\\text{&lt;BE&gt;:3},\\text{&lt;CE&gt;:2}\\\\ F&amp;:\\ \\ \\ \\ \\ \\text{&lt;AF&gt;:3},\\text{&lt;BF&gt;:3},\\text{&lt;CF&gt;:3}\\\\ \\end{align*}\\] Fourth, we can continue with this approach for every smaller subsequences or projection with length-k sequential patterns. For example, for a length-3 sequential patterns, we have: \\[ \\text{&lt;ABC&gt;:2}\\ \\ \\ \\ \\ \\ \\ \\text{&lt;CDE&gt;:2}\\ \\ \\ \\ \\ \\cdots \\] Prefix-projected Sequential Pattern (PrefixSpan) PrefixSpan extends and enhances FreeSpan by reducing projections (Pei J. et al. 2004). Similar to Freespan, we use the sequence table in Figure 9.26 to generate a list of length-1 sequential patterns that meet minimum support of 2: \\[ \\text{&lt;A&gt;:5},\\ \\ \\text{&lt;B&gt;:5},\\ \\ \\text{&lt;C&gt;:4},\\ \\ \\text{&lt;D&gt;:3},\\ \\ \\text{&lt;E&gt;:3},\\ \\ \\text{&lt;F&gt;:3} \\] From here, we construct our projection databases based on the length-1 sequential patterns above. Then, we work our way from items that meet minimum support using a divide-and-conquer approach (by partitioning). First, we start with the initial length-1 sequential patterns. For example, let us show the length-1 prefix projections. We exclude G, H, I, J, and K as they do not meet the minimum support. Figure 9.31: PrefixSpan (1-sequence projection) Note that sequences get pruned starting from the position of the first occurrence of the prefix to the beginning of the sequence. If the prefix is in an unordered element, the prefixed is replaced with an underscore &quot;_&quot; as a placeholder. For example, suppose our prefix is A, then we have the following three examples of an A-projection (A-suffix). \\[ \\underbrace{\\text{&lt;B(AC)D&gt;}}_\\text{sequence} \\rightarrow \\underbrace{\\text{&lt;(}\\underline{}\\text{C)D&gt;}}_\\text{A-projection} \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{&lt;B(CA)DF&gt;}}_\\text{sequence} \\rightarrow \\underbrace{\\text{&lt;DF&gt;}}_\\text{A-projection} \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{&lt;A(BC)EF&gt;}}_\\text{sequence} \\rightarrow \\underbrace{\\text{&lt;(BC)EF&gt;}}_\\text{A-projection} \\] Also, note that we highlighted the “C-projection” column in Figure 9.31 for our next operation. Second, we now operate on 2-sequential patterns. For example, based on the result above for the length-1 sequence prefix patterns, let us use “C-prefix column” to generate a list of length-1 sequential patterns that meet the minimum support of 2: \\[ \\text{&lt;A&gt;:1},\\ \\ \\text{&lt;B&gt;:0},\\ \\ \\text{&lt;C&gt;:1},\\ \\ \\text{&lt;D&gt;:2},\\ \\ \\text{&lt;E&gt;:2},\\ \\ \\text{&lt;F&gt;:3} \\] Therefore, we get the following minimum length-1 sequential pattern from the C-projected column: \\[ \\text{&lt;D&gt;:2},\\ \\ \\text{&lt;E&gt;:2},\\ \\ \\text{&lt;F&gt;:3} \\] That corresponds to the following length-2 sequential patterns: \\[ \\text{&lt;CD&gt;:2}\\ \\ \\ \\ \\ \\ \\text{&lt;CE&gt;:2}\\ \\ \\ \\ \\ \\ \\text{&lt;CF&gt;:3}\\ \\ \\ \\ \\ \\ \\] We can do the same for the other projections, e.g., A-projection and B-projection, to get the rest o the length-2 sequential patterns. To generate a length-2 sequence projection from the C-projected column using the length-1 sequential patterns, we get the following: Figure 9.32: PrefixSpan (2-sequence projection) Third, for a length-3 sequential pattern, we can see in Figure 9.32 that given the CD-Projection, we have a sequential pattern that meets the minimum support because they both occur in the second and fourth sequences: \\[ \\text{&lt;CDE&gt;:2} \\] We can do the same for the other projections, e.g., AB-projection and AC-projection, to get the rest of the length-3 sequential patterns. For example: \\[ \\text{&lt;ABC&gt;:2} \\] Lastly, we can continue with the other length-k sequential patterns to get the complete list of sequential patterns. Let us now show an implementation of Association and Pattern Discovery using three libraries. The first library is called arules and it provides functions to work on association and association rules. The second library is called arulesSequences, which allows operating on sequences. To demonstrate the association and apriori algorithm, let us define our dataset and convert it into a transaction format. First, let us form a matrix with rows corresponding to a list of transaction orders and columns corresponding to a list of items. library(&quot;arules&quot;) set.seed(2020) TX = 100 trans = seq(1, TX) alpha = toupper(letters) orders = matrix(0, nrow=TX, ncol=length(alpha), byrow=TRUE) for (i in 1:TX) { orders[i,] = rbinom(n=26, size=1, prob=0.15) } colnames(orders) = alpha orders_trans = as(orders, &quot;transactions&quot;) We use the inspect(.) function to inspect the content of our transaction orders. inspect(head(orders_trans)) ## items ## [1] {Q,X,Y} ## [2] {E,G,H,L,O,Q,R} ## [3] {A,M,O,T} ## [4] {Z} ## [5] {J,Q,V,W} ## [6] {M,N,X} We use the summary(.) function for additional information about our transactional database. The function provides the most frequent 1-itemset, including quartile statistics. summary(orders_trans) ## transactions as itemMatrix in sparse format with ## 100 rows (elements/itemsets/transactions) and ## 26 columns (items) and a density of 0.1438 ## ## most frequent items: ## S Q R U M (Other) ## 21 19 19 19 18 278 ## ## element (itemset/transaction) length distribution: ## sizes ## 1 2 3 4 5 6 7 8 ## 8 18 21 23 11 14 4 1 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.00 2.00 4.00 3.74 5.00 8.00 ## ## includes extended item information - examples: ## labels ## 1 A ## 2 B ## 3 C Finally, if we need to view our data structure, we can use the str(.) function. str(orders_trans, strict.width=&quot;wrap&quot;) ## Formal class &#39;transactions&#39; [package &quot;arules&quot;] with 3 slots ## ..@ data :Formal class &#39;ngCMatrix&#39; [package &quot;Matrix&quot;] with 5 slots ## .. .. ..@ i : int [1:374] 16 23 24 4 6 7 11 14 16 17 ... ## .. .. ..@ p : int [1:101] 0 3 10 14 15 19 22 29 31 36 ... ## .. .. ..@ Dim : int [1:2] 26 100 ## .. .. ..@ Dimnames:List of 2 ## .. .. .. ..$ : NULL ## .. .. .. ..$ : NULL ## .. .. ..@ factors : list() ## ..@ itemInfo :&#39;data.frame&#39;: 26 obs. of 1 variable: ## .. ..$ labels: chr [1:26] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; ... ## ..@ itemsetInfo:&#39;data.frame&#39;: 0 obs. of 0 variables To implement the Apriori algorithm and limit patterns, we use the apriori(.) function. options(width=56) apriori.output &lt;- apriori(orders_trans, parameter = list(supp=0.001, conf=0.8,maxlen=10)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.001 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 0 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s]. ## sorting and recoding items ... [26 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 5 6 7 8 done [0.00s]. ## writing ... [3442 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. We use the inspect(.) function one more time to inspect the result of apriori, displaying four measurements (e.g., support, confidence, coverage, and lift). inspect(head(apriori.output)) ## lhs rhs support confidence coverage lift ## [1] {C,G} =&gt; {J} 0.01 1 0.01 7.692 ## [2] {G,J} =&gt; {C} 0.01 1 0.01 12.500 ## [3] {C,D} =&gt; {N} 0.01 1 0.01 6.667 ## [4] {C,D} =&gt; {S} 0.01 1 0.01 4.762 ## [5] {C,I} =&gt; {X} 0.01 1 0.01 7.692 ## [6] {I,X} =&gt; {C} 0.01 1 0.01 12.500 ## count ## [1] 1 ## [2] 1 ## [3] 1 ## [4] 1 ## [5] 1 ## [6] 1 If we want to limit the result only to a few items, we can use the following: apriori.output &lt;- apriori(orders_trans, parameter = list(supp=0.001, conf=0.8,maxlen=10), appearance=list(lhs=c(&quot;A&quot;,&quot;B&quot;,&quot;C&quot;))) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport ## 0.8 0.1 1 none FALSE TRUE ## maxtime support minlen maxlen target ext ## 5 0.001 1 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 0 ## ## set item appearances ...[3 item(s)] done [0.00s]. ## set transactions ...[26 item(s), 100 transaction(s)] done [0.00s]. ## sorting and recoding items ... [26 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 done [0.00s]. ## writing ... [3 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. inspect(head(apriori.output)) ## lhs rhs support confidence coverage lift ## [1] {B,C} =&gt; {X} 0.01 1 0.01 7.692 ## [2] {B,C} =&gt; {M} 0.01 1 0.01 5.556 ## [3] {A,B} =&gt; {P} 0.02 1 0.02 7.143 ## count ## [1] 1 ## [2] 1 ## [3] 2 For sequences, let us demonstrate the use of the SPADE algorithm. First, let us construct our sequence database using Figure 9.26. library(arulesSequences) orders = list(c(&quot;A&quot;),c(&quot;A&quot;,&quot;B&quot;), c(&quot;C&quot;), c(&quot;F&quot;), c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(&quot;D&quot;), c(&quot;E&quot;), c(&quot;F&quot;), c(&quot;G&quot;,&quot;H&quot;), c(&quot;I&quot;, &quot;J&quot;), c(&quot;K&quot;), c(&quot;A&quot;), c(&quot;A&quot;, &quot;B&quot;), c(&quot;D&quot;, &quot;E&quot;), c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c(&quot;C&quot;, &quot;D&quot;), c(&quot;E&quot;), c(&quot;A&quot;), c(&quot;A&quot;, &quot;B&quot;), c(&quot;C&quot;), c(&quot;A&quot;, &quot;F&quot;)) names(orders) = paste0(&quot;T&quot;, 1:21, sep=&quot;&quot;) trans.seq = as(orders, &quot;transactions&quot;) transactionInfo(trans.seq)$sequenceID = c(1,1,1,1,2,2,2,2,2,2,2, 3,3,3,4,4,4,5,5,5,5) transactionInfo(trans.seq)$eventID = c(1,2,3,4,1,2,3,4,5,6,7,1, 2,3,1,2,3,1,2,3,4) trans.seq ## transactions in sparse format with ## 21 transactions (rows) and ## 11 items (columns) Let us then inspect our sequential data. This should match table T2 in Figure 9.29. inspect(trans.seq) ## items transactionID sequenceID eventID ## [1] {A} T1 1 1 ## [2] {A,B} T2 1 2 ## [3] {C} T3 1 3 ## [4] {F} T4 1 4 ## [5] {A,B,C} T5 2 1 ## [6] {D} T6 2 2 ## [7] {E} T7 2 3 ## [8] {F} T8 2 4 ## [9] {G,H} T9 2 5 ## [10] {I,J} T10 2 6 ## [11] {K} T11 2 7 ## [12] {A} T12 3 1 ## [13] {A,B} T13 3 2 ## [14] {D,E} T14 3 3 ## [15] {A,B,C} T15 4 1 ## [16] {C,D} T16 4 2 ## [17] {E} T17 4 3 ## [18] {A} T18 5 1 ## [19] {A,B} T19 5 2 ## [20] {C} T20 5 3 ## [21] {A,F} T21 5 4 Finally, we use cspade(.) function to demonstrate SPADE algorithm. We mine sequential patterns that meet our minimum support (in this case, we are using the relative support of 0.40). For a length-2 sequential pattern, we obtain the following: spade.output = cspade(trans.seq,parameter = list(support = 0.40), control = list(verbose = FALSE)) Let us review how many sequential patterns have the support of 1,2, and 3: supports = size(spade.output) c(&quot;length-1&quot; = sum(supports == 1), &quot;length-2&quot; = sum(supports == 2), &quot;length-3&quot; = sum(supports == 3)) ## length-1 length-2 length-3 ## 10 25 16 For a length-1 sequential pattern, we can obtain by running the following: spade.output = cspade(trans.seq,parameter = list(support = 0.40, maxsize=1, maxlen=1), control = list(verbose = FALSE)) as(spade.output, &quot;data.frame&quot;) ## sequence support ## 1 &lt;{A}&gt; 1.0 ## 2 &lt;{B}&gt; 1.0 ## 3 &lt;{C}&gt; 0.8 ## 4 &lt;{D}&gt; 0.6 ## 5 &lt;{E}&gt; 0.6 ## 6 &lt;{F}&gt; 0.6 For a length-2 sequential pattern, we obtain the following list using the support: spade.output = cspade(trans.seq,parameter = list(support = 0.40), control = list(verbose = FALSE)) sub.spade = subset(spade.output, supports == 2) head(as(sub.spade , &quot;data.frame&quot;), n = 15) # display only the first 15 ## sequence support ## 7 &lt;{A},{F}&gt; 0.6 ## 8 &lt;{B},{F}&gt; 0.6 ## 9 &lt;{C},{F}&gt; 0.6 ## 16 &lt;{A,B},{F}&gt; 0.6 ## 20 &lt;{A},{E}&gt; 0.6 ## 21 &lt;{B},{E}&gt; 0.6 ## 22 &lt;{C},{E}&gt; 0.4 ## 23 &lt;{D},{E}&gt; 0.4 ## 31 &lt;{B,C},{E}&gt; 0.4 ## 32 &lt;{A,C},{E}&gt; 0.4 ## 33 &lt;{A,B,C},{E}&gt; 0.4 ## 34 &lt;{A,B},{E}&gt; 0.6 ## 35 &lt;{A},{D}&gt; 0.6 ## 36 &lt;{B},{D}&gt; 0.6 ## 37 &lt;{C},{D}&gt; 0.4 For a length-3 sequential pattern, we obtain the following: spade.output = cspade(trans.seq,parameter = list(support = 0.40, maxsize = 3, maxlen = 3), control = list(verbose = FALSE)) sub.spade = subset(spade.output, size(spade.output) == 3) as(sub.spade , &quot;data.frame&quot;) ## sequence support ## 10 &lt;{B},{C},{F}&gt; 0.4 ## 11 &lt;{A},{C},{F}&gt; 0.4 ## 12 &lt;{A,B},{C},{F}&gt; 0.4 ## 14 &lt;{A},{B},{F}&gt; 0.4 ## 15 &lt;{A},{A,B},{F}&gt; 0.4 ## 16 &lt;{A},{A},{F}&gt; 0.4 ## 21 &lt;{C},{D},{E}&gt; 0.4 ## 22 &lt;{B},{D},{E}&gt; 0.4 ## 23 &lt;{A},{D},{E}&gt; 0.4 ## 24 &lt;{B,C},{D},{E}&gt; 0.4 ## 25 &lt;{A,C},{D},{E}&gt; 0.4 ## 26 &lt;{A,B,C},{D},{E}&gt; 0.4 ## 27 &lt;{A,B},{D},{E}&gt; 0.4 ## 44 &lt;{A},{B},{C}&gt; 0.4 ## 45 &lt;{A},{A,B},{C}&gt; 0.4 ## 46 &lt;{A},{A},{C}&gt; 0.4 Apart from sequential patterns, part of mining data is to mine graph patterns. Here, we leave readers to investigate Graph pattern mining. 9.5.4 Null Invariance In this section, let us extend the concept of Association rules as discussed in a previous section and introduce Interestingness measures, which provide the strength of association of items. Suppose we sell items A, B, and C. While we know that customers may order any combination of the three items, we are interested in the association only between item A and item B. For example, it may help to know the possibility that when customers order item A, they also order item B. Alternatively, perhaps, customers are interested only in item C but not both items A and B - these orders without items A and B are called the null transactions with respect to A and B. To illustrate further, let us use Figure 9.33. Figure 9.33: Datasets with Null Transactions Analyzing the orders in table T1, we see that there are orders containing item A and item B, making 850 orders. There are orders containing only item A but not B making 965 orders. Finally, there are orders containing only item B but not A, making 750. Moreover, some orders do not contain both items A and B - the null transactions - making a total of 80. Let us compute for the Support and Confidence using T1. \\[ \\begin{array}{lll} S(B) = \\frac{1600}{2645} = 0.6049 &amp; S(A \\rightarrow B) = \\frac{850}{2645} = 0.3214 &amp; C(A \\rightarrow B) = \\frac{850}{1815} = 0.4683 \\\\ &amp; S(\\overline{A} \\rightarrow B) = \\frac{750}{2645} = 0.2836 &amp; C(\\overline{A} \\rightarrow B) = \\frac{750}{830} = 0.9036 \\end{array} \\] The above measures show that there is a confidence level of 46.83% for item B being ordered along with item A, with the association having the support of 32.14%. However, we see a higher confidence level of 90.36% for item B being on its own without item A. The strength of association can also be measured using the Lift measure in which a value less than one reflects a negative correlation; otherwise, we see a positive correlation. For example, below, we see a Lift value less than one; thus, we see a negative correlation. \\[ L(A,B) = \\frac{C(A \\rightarrow B)}{S(B)} = \\frac{850/1815}{1600/2645} = 0.7742 \\] Another measure of association is the Chi-squared \\((X^2)\\). Given the corresponding expected values, a Chi-squared value equal to zero means no correlations between items; otherwise, there is a positive or negative correlation depending on the sign of the value. Using the expected values: \\[\\begin{align*} \\mathbb{E}(A,B) {}&amp;= \\frac{A \\times B}{\\text{total tx}} = \\frac{1815 \\times 1600}{2645} = 1097.921\\\\ \\mathbb{E}(\\overline{A},B) &amp;= \\frac{ \\overline{A} \\times B}{\\text{total tx}} = \\frac{830 \\times 1600}{2645} = 502.0794\\\\ \\mathbb{E}(A,\\overline{B}) &amp;= \\frac{ A \\times \\overline{B}}{\\text{total tx}} = \\frac{1815 \\times 1045}{2645} = 717.0794\\\\ \\mathbb{E}(\\overline{A},\\overline{B}) &amp;= \\frac{\\overline{A} \\times \\overline{B}}{\\text{total tx}} = \\frac{830 \\times 1045}{2645} = 327.9206\\\\ \\end{align*}\\] we compute for the \\(X^2\\) like so: \\[\\begin{align*} X^2 {}&amp;= \\sum_{i,j} \\frac{(O_{i,j} - E_{i,j})^2}{E_{i,j}}\\\\ &amp;= \\frac{(850 - 1097.921)^2}{1097.921} + \\frac{(750 - 502.0794)^2}{502.0794} + \\frac{(965 - 717.0794)^2}{717.0794} + \\frac{(80 - 327.9206)^2}{327.9206}\\\\ &amp;= 451.5558 \\end{align*}\\] In terms of measuring similarity and distance, Interestingness of association can be measured using Cosine Similarity, Jaccard Coefficient, Jaccard Distance, Gini Index, and many others. For example, Figure 9.34 shows a partial list of Interestingness measures citing Tan P. et al. (2002) and Han J. et al. (2002). Figure 9.34: Interestingness Measures Notice that there are 10000 null transactions in table T2 compared to T1 which has only 80 null transactions; yet, the computed values do not change (hence invariant) for Confidence, Kulczynski, Jaccard Coefficient/Distance, Cosine Similarity, and Imbalance Ratio measures. Null Invariance (NI) is a binary property of a given Interestingness measure that describes the effect of a change in the number of null transactions. For example, it helps to know if the value of our measure changes as the number of customer orders not containing items A and B increases. Here, we introduce the term Interestingness measure, a ranking criterion to determine the strength of association of items. Many Interestingness measures are used to rank the strength of association while also classified based on being Null-Invariant or not. Three of which are discussed in the previous section, namely Support, Confidence, and Lift. Additionally, the table in Figure 9.34 lists a few additional measures (J. Han et al. 2004; Pang-Ning Tan et al. 2002). We only show three cases (tables T1, T2, T3); however, there are situations in which the number of transactions with respect to the combinations \\((A,B), (\\overline{A},B), (A,\\overline{B}),(\\overline{A},\\overline{B})\\) vary across many cases. For that reason, we see disagreements in measures. One measure that may help is the Imbalance Ratio (IR). Along with Kulczynski, the Imbalance ratio (IR) determines both neutrality and imbalance of certain different datasets (J. Han 2004). An IR value closer to one indicates a strong imbalance. Moreover, a Kulczynski value of 0.5 consistent across multiple datasets indicates neutrality. Comparing tables T1, T2, T3, we see that they are all neutral based on the Kulczynski measure; however, only table T3 is balanced based on the IR measure. For illustration, let us show a crude implementation of the measures listed in Figure 9.34. For reference, we provide the following probabilities for table T1: \\[ \\begin{array}{l} P(A,B) = \\frac{850}{2645} = 0.32136 \\\\ P(\\overline{A},B) = \\frac{750}{2645} = 0.28355 \\\\ P(A,\\overline{B}) = \\frac{965}{2645} = 0.36484 \\\\ P(\\overline{A},\\overline{B}) = \\frac{80}{2645} = 0.03025 \\end{array} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{l} P(A) = \\frac{1815}{2645} = 0.68620 \\\\ P(\\overline{A}) = \\frac{830}{2645} = 0.31380 \\\\ P(B) = \\frac{1600}{2645}= 0.60491 \\\\ P(\\overline{B}) = \\frac{1045}{2645} = 0.39509 \\end{array} \\] Let us also compute for conditional probabilities (based on chain rule): \\[ \\begin{array}{l} P(A|B) = \\frac{P(A,B)} {P(B)} = \\frac{0.32136}{0.60491} = 0.53125\\\\ P(\\overline{A}|B) = \\frac{P(\\overline{A},B)} {P(B)} = \\frac{0.28355}{0.60491} = 0.46875\\\\ P(A|\\overline{B}) = \\frac{P(A,\\overline{B})} {P(\\overline{B})} = \\frac{0.36484}{0.39509} = 0.92344\\\\ P(\\overline{A}|\\overline{B}) = \\frac{P(\\overline{A},\\overline{B})} {P(\\overline{B})} = \\frac{0.03025}{0.39509} = 0.07656\\\\ \\end{array} \\ \\ \\ \\ \\ \\ \\ \\begin{array}{l} P(B|A) = \\frac{P(B,A)} {P(A)} = \\frac{0.32136}{0.68620} = 0.46832\\\\ P(\\overline{B}|A) = \\frac{P(\\overline{B},A)} {P(A)} = \\frac{0.36484}{0.68620} = 0.53168\\\\ P(B|\\overline{A}) = \\frac{P(B,\\overline{A})} {P(\\overline{A})} = \\frac{0.28355}{0.31380} = 0.90360\\\\ P(\\overline{B}|\\overline{A}) = \\frac{P(\\overline{B},\\overline{A})} {P(\\overline{A})} = \\frac{0.03025}{0.31380} = 0.09640\\\\ \\end{array} \\] Here, the sum of the following probabilities should equal one: \\[\\begin{align*} 1 {}&amp;= P(A,B) + P(\\overline{A}, B) + P(A, \\overline{B}) + P(\\overline{A},\\overline{B})\\\\ 1 &amp;= P(A) + P(\\overline{A})\\\\ 1 &amp;= P(B) + P(\\overline{B})\\\\ 1 &amp;= P(A|B) + P(\\overline{A}|B)\\\\ 1 &amp;= P(A|\\overline{B}) + P(\\overline{A}|\\overline{B})\\\\ 1 &amp;= P(B|A) + P(\\overline{B}|A)\\\\ 1 &amp;= P(B|\\overline{A}) + P(\\overline{B}|\\overline{A})\\\\ \\end{align*}\\] First, we construct the three matrices and define a few helper functions: T1 = matrix(c(850, 750, 965, 80), nrow=2, ncol=2, byrow=TRUE) T2 = matrix(c(850, 750, 965, 10000), nrow=2, ncol=2, byrow=TRUE) T3 = matrix(c(850, 850, 850, 10000), nrow=2, ncol=2, byrow=TRUE) A.B &lt;- function(D) { D[1,1] } # (A,B) A_.B &lt;- function(D) { D[1,2] } # (not A, B) A.B_ &lt;- function(D) { D[2,1] } # (A, not B) A_.B_ &lt;- function(D) { D[2,2] } # (not A, not B) A &lt;- function(D) { A.B(D) + A.B_(D) } # (A) B &lt;- function(D) { A_.B(D) + A.B(D) } # (B) A_ &lt;- function(D) { A_.B(D) + A_.B_(D) } # (not A) B_ &lt;- function(D) { A.B_(D) + A_.B_(D) } # (not B) E &lt;- function(D, X, Y) { X(D) * Y(D) / Tot.Tx(D) } # expectation P &lt;- function(D, X) { X(D) / Tot.Tx(D) } # probability # conditional probability P.cond &lt;- function(D, X, Y) { P(D, X) / P(D, Y) } Tot.Tx &lt;- function(D) { A.B(D) + A_.B(D) + A.B_(D) + A_.B_(D) } Second, we define functions for the Interestingness measures: support &lt;- function(D, X) { X(D) / Tot.Tx(D) } confidence &lt;- function(D, X, Y) { X(D) / Y(D) } lift &lt;- function(D, X, Y) { X / Y } cosine.similarity &lt;- function(D) { P(D, A.B) / sqrt( P(D, A) * P(D, B)) } jaccard.coefficient &lt;- function(D) { P(D, A.B) / (P(D,A) + P(D,B) - P(D, A.B)) } jaccard.distance &lt;- function(D) { 1 - jaccard.coefficient(D) } leverage &lt;- function(D) { P(D,A.B) - P(D,A) * P(D,B) } conviction &lt;- function(D) { max( (P(D,A) * P(D,B_)) / P(D,A.B_), (P(D,B) * P(D,A_)) / P(D,A_.B) ) } chi.squared &lt;- function(D) { ( A.B(D) - E(D,A,B) )^2 / E(D,A,B) + ( A_.B(D) - E(D,A_,B) )^2 / E(D,A_,B) + ( A.B_(D) - E(D,A,B_) )^2 / E(D,A,B_) + ( A_.B_(D) - E(D,A_,B_) )^2 / E(D,A_,B_) } kappa &lt;- function(D) { ( P(D, A.B) + P(D, A_.B_) - P(D,A) * P(D,B) - P(D,A_) * P(D, B_) ) / ( 1- P(D,A) * P(D,B) - P(D,A_) * P(D, B_)) } gini.index &lt;- function(D) { P(D, A) * ( P.cond(D, A.B, A)^2 + P.cond(D, A.B_,A)^2) + P(D, A_) * (P.cond(D, A_.B, A_)^2 + P.cond(D, A_.B_, A_)^2) - P(D, B)^2 - P(D, B_)^2 } yule.Q &lt;- function(D) { ( P(D,A.B) * P(D,A_.B_) - P(D,A_.B) * P(D,A.B_) ) / ( P(D,A.B) * P(D,A_.B_) + P(D,A_.B) * P(D,A.B_) ) } yule.Y &lt;- function(D) { ( sqrt(P(D,A.B) * P(D,A_.B_)) - sqrt(P(D,A_.B) * P(D,A.B_)) ) / ( sqrt(P(D,A.B) * P(D,A_.B_)) + sqrt(P(D,A_.B) * P(D,A.B_)) ) } kulczynski &lt;- function(D) { 0.5 * (P.cond(D,A.B,B) + P.cond(D,A.B, A)) } odds.ratio &lt;- function(D) { (P(D,A.B) * P(D,A_.B_)) / (P(D,A.B) * P(D,A_.B)) } j.measure &lt;- function(D) { max( P(D,A.B) * log(P.cond(D,A.B,A) / P(D,B), 2) + P(D,A.B_) * log(P.cond(D,A.B_,A) / P(D,B_), 2), P(D,A.B) * log(P.cond(D,A.B,B) / P(D,B), 2) + P(D,A.B_) * log(P.cond(D,A_.B,B) / P(D,A_), 2)) } imbalance.ratio &lt;- function(D) { abs(support(D,A) - support(D,B)) / ( support (D,A) + support(D,B) - support(D,A.B)) } Third, we then compute for the measures of the three matrices: m = matrix(0, nrow=17, ncol=3, byrow=TRUE) for (j in 1:3) { if (j == 1) { D = T1 } else if (j == 2) { D = T2 } else if (j == 3) { D = T3 } m[, j] = c( support(D, A.B), confidence(D, A.B, A), lift(D, confidence(D, A.B, A), support(D,B) ), chi.squared(D), kulczynski(D), leverage(D), jaccard.coefficient(D), jaccard.distance(D), cosine.similarity(D), conviction(D), kappa(D), gini.index(D), yule.Q(D), yule.Y(D), odds.ratio(D), j.measure(D), imbalance.ratio(D)) } colnames(m) = c(&quot;T1&quot;, &quot;T2&quot;, &quot;T3&quot;) rownames(m) = c(&quot;Support&quot;, &quot;Confidence&quot;, &quot;Lift&quot;, &quot;Chi-Squared&quot;, &quot;Kulczynski&quot;, &quot;Leverage (Piatetsky-Shapiro)&quot;, &quot;Jaccard Coefficient&quot;, &quot;Jaccard Distance&quot;, &quot;Cosine Similarity&quot;, &quot;Conviction&quot;, &quot;Kappa&quot;, &quot;Gini Index&quot;, &quot;Yule Q&quot;, &quot;Yule Y&quot;, &quot;Odds Ratio&quot;, &quot;J-Measure&quot;, &quot;Imbalance Ratio&quot;) round(m, 3) ## T1 T2 T3 ## Support 0.321 0.068 0.068 ## Confidence 0.468 0.468 0.500 ## Lift 0.774 3.678 3.691 ## Chi-Squared 451.556 2219.674 2231.344 ## Kulczynski 0.500 0.500 0.500 ## Leverage (Piatetsky-Shapiro) -0.094 0.049 0.049 ## Jaccard Coefficient 0.331 0.331 0.333 ## Jaccard Distance 0.669 0.669 0.667 ## Cosine Similarity 0.499 0.499 0.500 ## Conviction 0.743 1.825 1.729 ## Kappa -0.407 0.419 0.422 ## Gini Index 0.082 0.039 0.042 ## Yule Q -0.828 0.843 0.843 ## Yule Y -0.531 0.548 0.549 ## Odds Ratio 0.107 13.333 11.765 ## J-Measure 0.151 0.073 0.074 ## Imbalance Ratio 0.084 0.084 0.000 9.5.5 Correlation and Collinearity Correlation measures the strength of relation amongst variables characterized by the linear movement in a positive (ascending) or negative (descending) direction. Thus, Correlation is also termed collinearity in its linear Correlation. Multicollinearity is a more common term for Correlation amongst multiple variables. In this section, we cover Correlation (or collinearity) between random variables - starting with two variables - by showing the plots that they form. It helps to visualize the Correlation between distributions as this becomes our baseline and foundation in comparing with other unusual, skewed, or out-of-the-ordinary distributions. Here, we introduce Correlation Coefficient to measure the correlation strength between two variables. A measure of zero means there is no correlation between the variables. A measure of one means a solidly positive correlation; otherwise, it is a solidly negative correlation. Below are the two Pearson r correlation formulae that render the same result. \\[\\begin{align} Cor = \\frac{n\\sum(AB) - \\sum{A} \\sum{B}} {\\sqrt{ \\left(n\\sum A^2 - (\\sum A)^2\\right) \\left(n\\sum B^2 - (\\sum B)^2\\right) }}\\ \\ \\ \\ \\ \\ \\ \\ \\text{range}: [-1,1] \\end{align}\\] \\[\\begin{align} Cor = \\frac{\\sum (A - \\overline{A})(B - \\overline{B})} { \\sqrt{\\sum (A - \\overline{A})^2} \\sqrt{\\sum(B - \\overline{B})^2}}\\ \\ \\ \\ \\ \\ \\ \\ \\text{range}: [-1,1] \\end{align}\\] A simple implementation of the Correlation Coefficient is provided below: correlation1 &lt;- function(A,B) { n = length(A) ( n * sum( A * B) - sum(A) * sum(B) ) / sqrt( ( n * sum(A^2) - sum(A)^2) * ( n * sum(B^2) - sum(B)^2) ) } correlation2 &lt;- function(A,B) { sum ( (A - mean(A)) * (B - mean(B)) ) / ( sqrt(sum((A - mean(A))^2)) * sqrt(sum((B - mean(B))^2)) ) } correlation = correlation1 Using Figure 9.35, we show a correlation chart between two random variables. It illustrates a solid increasing diagonal line. The correlation is 100% - given that we are using two duplicated random variables to simulate correlation. The two variables are dependent. As one increases, so does the other. The correlation coefficient is 1, suggesting a solidly positive correlation. The correlation coefficient for the descending line is -1, suggesting a solidly negative correlation. This case of having two variables with a correlation coefficient of -1 or 1 also means that one of the two variables can be representative of the other - and as each one duplicates the other, the chances are that we choose one of them for our purpose, whatever it may be. Figure 9.35: Very Strong Correlation (Uniform vs Uniform) In reality, most datasets with some degree of correlation come with noise (perturbation or error). In a later section, we discuss data leakage, confounding variables, outliers, and missing values. As for the plot in Figure 9.36, we try to determine any linear correlation. The correlation coefficient is 0.9244, suggesting a strong positive correlation (due to the added noise). Figure 9.36: Very Strong Correlation (Uniform vs Uniform) Using Figure 9.37, we show a correlation between a uniform distribution and a normal distribution. The plot follows a vertical convergence of data to the center for the normal distribution and an even (or uniform) horizontal distribution of data. That illustrates very low - to no - correlation, given that we are using two different random variables of two different types of distributions. Any pair of random variables will have a low correlation if a low percentage of the data closely matches. The correlation coefficient is 0.0252, suggesting a very weak correlation. This dataset type represents a constant timeline in which every moment captures some random event with uncertainty. Our goal is to estimate the point of convergence (the center or mean) of such a random event in a time series. Figure 9.37: Very Weak Correlation (Uniform vs Normal) Using Figure 9.38, we show a correlation chart between two random variables of uniform distribution. That illustrates no correlation, given that we are using two independent random variables. That also indicates that the correlation between two different ‘uniform’ distributions shows data points covering a rectangular region. The correlation coefficient is 0.0054. Figure 9.38: No Correlation (Uniform vs Uniform) In the next figure, let us use RMSE to measure the relationship and compare it with correlation coefficient. A measure of zero indicates no perturbation. \\[\\begin{align} RMSE = \\sqrt{\\frac{1}{n}\\sum(A-B)^2}\\ \\ \\ \\ \\ \\ \\ \\ \\text{range}: [0,\\infty] \\end{align}\\] A simple implementation of RMSE is provided below: rmse &lt;- function(A,B) { n = length(A) sqrt(mean((A - B)^2)) } Using Figure 9.35, we show a correlation chart between two random variables of normal distribution. It illustrates a more dispersed behavior (variance) converging to the center (mean). The correlation coefficient is 0.01 which suggests non-linear correlation. However, the RMSE is 1.2868. A zero RMSE indicates that the two variables have zero perturbation. An RMSE of around 1.2 indicates a standard deviation close to 1. In other words, RMSE increases as variance (error/perturbation) increases. Let us cover covariance later in the next section. Figure 9.39: Weak Correlation (Normal vs Normal) Categorically, Figure 9.40 shows a correlation chart between a uniform distribution and a categorical distribution stratified from the same uniform distribution. Because we are using the same uniform distribution to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numeric categories (1,2,3). The distribution of categories is not spread evenly - because of the uniform distribution. Category 1 is spread across data points from -1.5 to 0.5. Category 2 is spread across data points from -0.5 to 0.5. Category 3 is spread across data points from 0.5 to 1.5. set.seed(142) n=100 x1 = runif(n, min=-1.5, max=1.5) x2 = stratify(x1, c(1,2,3), n) plot(x2, x1, main=&quot;Strong Category (Uniform vs Stratified Uniform)&quot;, ylab=&quot;X1 (Uniform)&quot;, xlab=&quot;X2 (Stratified Uniform)&quot;, pch=20) Figure 9.40: Strong Category (Uniform vs Stratified Uniform) Categorically, Figure 9.41 shows a correlation chart between a uniform distribution and a categorical distribution stratified from another uniform distribution. Because we are using different uniform distributions to simulate categorical distribution, notice a plot of 3 vertical lines based on the three numerical categories (1,2,3). The distribution of the categories is spread evenly across data points -1.5 to 1.5. set.seed(142) n=100 x1 = runif(n, min=-1.5, max=1.5) x2 = stratify(runif(n, min=-1.5, max=1.5), c(1,2,3), n) plot(x2, x1, main=&quot;Uniform vs Stratified Uniform Non-Collinearity&quot;, ylab=&quot;Uniform&quot;, xlab=&quot;Different Stratified Uniform&quot;, pch=20) Figure 9.41: Uniform vs Stratified Uniform Non-Collinearity With all that said, let us use a function called pairs(.) to generate a single plot for all pair-wise combinations of multiple variables. We use the mtcars dataset with a select number of variables (mpg, cyl, disp, hp, drat, carb). See Figure 9.42. Notice that the mpg-disp pair seems to show a negative correlation. The mpg-drat pair seems to show a positive correlation. The cyl-disp pair seems to have a categorical correlation, however. pairs(mtcars[,c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;carb&quot;)], pch=20) Figure 9.42: Correlation pairs When our dataset comes with multiple variables, we then use multicollinearity to measure correlation across multiple variables. To illustrate, let us introduce a third-party package in R called corrplot. We use corrplot(.) function to analyze collinearity in one plot similar to the pairs(.) function. See Figure 9.43. The color spectrum maps to the range [-1,1]. Colors closer to 1 indicate positive collinearity. Colors closer to -1 indicate negative collinearity. Otherwise, there is no collinearity. Here, we use the mtcars for our dataset, which comes with 11 variables. library(corrplot) mt &lt;- cor(mtcars) corrplot(mt, method = &quot;circle&quot;) Figure 9.43: Collinearity Matrix Below, we select a few variables to compare collinearity. mt = mtcars[,c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;)] cor(mt) ## mpg cyl disp hp drat ## mpg 1.0000 -0.8522 -0.8476 -0.7762 0.6812 ## cyl -0.8522 1.0000 0.9020 0.8324 -0.6999 ## disp -0.8476 0.9020 1.0000 0.7909 -0.7102 ## hp -0.7762 0.8324 0.7909 1.0000 -0.4488 ## drat 0.6812 -0.6999 -0.7102 -0.4488 1.0000 Notice that the cyl-hp pair has a collinearity coefficient of 0.8324475. cyl = mtcars[,c(&quot;cyl&quot;)] hp = mtcars[, c(&quot;hp&quot;)] correlation(cyl,hp) ## [1] 0.8324 Recall in Chapter 2 (Numerical Linear Algebra I) about Dot Product and Determinant. To measure Collinearity, we can also use Dot Product and Determinant. If two vectors have a zero determinant, then the two vectors are not only Collinear, they are 100% collinear, and thus one is enough to represent the other. If the Dot Product, however, is zero, then the two vectors are orthogonal, and therefore there is no Collinearity; otherwise, there is some degree of Collinearity. 9.5.6 Covariance Datasets with gaussian distribution may be measured using covariance instead of collinearity. Below is our covariance formula: \\[\\begin{align} \\underbrace{Cov = \\frac{\\sum (A - \\overline{A})(B - \\overline{B})}{N}}_{\\text{population covariance}} \\ \\ \\ \\ \\ \\ \\ \\ \\text{range}: [0,\\infty] \\end{align}\\] \\[\\begin{align} \\underbrace{Cov = \\frac{\\sum (A - \\overline{A})(B - \\overline{B})}{(N-1)}}_{\\text{sample covariance}} \\ \\ \\ \\ \\ \\ \\ \\ \\text{range}: [0,\\infty] \\end{align}\\] A simple implementation of the sample Covariance is provided below: covariance &lt;- function(A,B) { N = length(A) sum ( (A - mean(A)) * (B - mean(B)) ) / (N - 1) } Let us use mtcars dataset and compare mpg and drat variables. We use three measures: Correlation, RMSE, and Covariance. Similar to RMSE, a high covariance indicates a high degree of deviation of the variables from the mean. A zero covariance means that all data points converge to the center. mpg = mtcars[,c(&quot;mpg&quot;)] drat = mtcars[,c(&quot;drat&quot;)] c(&quot;Correlation&quot; = correlation(mpg, drat), &quot;RMSE&quot; = rmse(mpg, drat), &quot;Covariance&quot; = covariance(mpg,drat)) ## Correlation RMSE Covariance ## 0.6812 17.4146 2.1951 9.5.7 Outliers, Leverage, Influence In Chapter 6 (Statistical Computation), we covered the topic of Outliers, Leverage, and Influence. Outliers are data points with y values (response) that are outside the usual trend of the data. Leverage are data points with x values (predictors) outside the normal range of the data. Influence measures the effect (or Influence) of data points on the rest of the data. set.seed(142) beta0 = 0.5 beta1 = 1.2 x = rnorm(n=30, mean=0, sd=1) random_noise = rnorm(n=30, mean=0, sd=1) expected_y = beta0 + beta1 * x + random_noise model = lm(expected_y ~ x) plot(expected_y ~ x, lwd=2, pch=16, cex=1, main=&quot;Goodness of Fit&quot;, col=&quot;grey&quot;, ylab=&quot;Response&quot;, xlab=&quot;Predictor&quot;, xlim=range(-3,6), ylim=range(-3,6)) abline(model, lwd=2, col=&quot;darksalmon&quot;) new_x = c(1,4, 4.5) new_y = stats::predict(model, newdata = data.frame(x = new_x)) + c(4, 0, -.5) points(new_x, new_y, lwd=2, pch=16, cex=1, col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;)) legend(&quot;topleft&quot;, inset=.02, pch=16, cex=0.8, legend=c(&quot;Outlier (Influential)&quot;, &quot;Leverage&quot;, &quot;Leverage (Influential)&quot;), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;) ) influence_x = c(4) influence_y = stats::predict(model, newdata = data.frame(x = influence_x)) + c(4) x1 = c(x, influence_x) y1 = c(expected_y, influence_y) influenced_model = lm ( y1 ~ x1 ) abline(influenced_model, lwd=1, col=&quot;red&quot;, lty=2) influence_x = c(4.5) influence_y = stats::predict(model, newdata = data.frame(x = influence_x)) + c(-.5) x1 = c(x, influence_x) y1 = c(expected_y, influence_y) influenced_model = lm ( y1 ~ x1 ) abline(influenced_model, lwd=1, col=&quot;blue&quot;, lty=2) Figure 9.44: Outliers, Leverages, Influentials For example, Figure 9.44 illustrates three data points (red dot, black dot, blue dot). The red dot is an outlier with a significant influence as its slope is steeper, pulling the fitted line further away from the usual trend - the red dotted line. The data point with blue dot color is not an outlier as its y value sticks to the trend; nonetheless, it is considered leverage since its x value is extremely away from the normal range of the other data points. Notice also that it slightly affects the slope, as can be seen by the blue dotted line. On the other hand, the data point with the black dot color does not influence since it can keep the slope unchanged - the orange line - even though it is considered leverage as its x value is extremely away from the normal range of the other data. 9.5.8 Dominating Factors There may be situations in which we see categorical variables whose values are extremely uneven in that given two levels (categories), one level extremely dominates the other levels. See Figure 9.45. Figure 9.45: Extreme Values Out of 1000 observations, 990 observations register a dominating value of 3.141593, whereas ten observations register a value of 2.718282. It would be good to trim (or remove) such variables if such extreme levels may not contribute. 9.5.9 Missingness and Imputation In this section, we introduce the concept of missing data. One negative effect of missing data is that it reduces the statistical power and analyzability of observations. It also reduces the inference power of ML algorithms. For that reason, we need to be able to manage missing data. There are three mechanisms of missingness that we need to be familiar with to manage missing data. MCAR (Missing completely at random) refers to missing data for no reason other than it being completely random. There is no dependency on the response or any observed or unobserved data. Therefore, this eliminates systematic bias. For example, given a list of observations, we simulate missing data by flipping a coin and removing values of observations if the coin lands on tails. In a real-world scenario, hardware storage may fail mechanically due to wear and tear, ,causing data loss. Without proper backup, some records may become corrupted and unrecoverable due to media failures or errors. MAR (Missing at random) refers to missing data for no reason other than it can be explained by other information (other factors). That is dependent on the observed data. For example, male participants may skip answering some questions in a survey. If we analyze the entire survey, we may find that 90% of females answered some specific questions that about 50% of male participants failed to answer. The missing data is not relevant to or dependent on the question itself; rather, it is dependent on the fact that the participant’s gender is male. Therefore, the missing data is dependent on other information in the observed data - in this case, gender is part of the survey (one of the questions in the survey). MNAR (Missing not at random) refers to missing data due to dependence on unobserved data or missing predictors. In the survey example above, the value of the missing data influences the probability of participants from willing to provide the value of the missing data due to some other reasons (that are not part of the survey - the unobserved data or missing predictor). One reason is if our survey includes personal or private information. Some questions may be skipped with no provided information. In such cases, sampled data may not become a true representative of the entire population because of bias. To manage missing data, we introduce Imputation, a method to replace the missing data with estimates. Indeed, the most simple solution for missing data is to ignore the records or perform row deletion. However, the consequence of deleting records is that it may create imbalances. For example, in the survey for MAR, 50% of male participants skipped answering some questions. Therefore, removing 50% of the male records creates BIAS towards female participants. One form of imputation is to compute the average (the mean) across the observations. However, we need to be wary of the size of the missing data. To replace a large number of missing data using the average over a lesser number of available data creates invariability. We reduce variance because then data becomes closely similar to each other. To solve this invariance problem, we can use the Hot Deck Methodmethod. That is achieved by averaging a group of relevant predictors together and replacing the missing data only for those observations belonging to the respective group. A better method but computationally costly is the Multiple Imputation method. That is achieved by performing regression (e.g., least-square) and prediction and then performing bayesian analysis. The idea is to generate M sub-samples from the overall observations - M being the number of imputations. For each sub-sample, we perform regression - fitting the data based on Least-Squares (as an example). We then use the regression model to predict all our missing data within the sub-sample. Next, we calculate the average or mean of the predicted values within each sub-sample. We repeat the process for all sub-samples. Afterwhich, we calculate the overall mean of all the sub-sample means. This overall mean becomes our replacement value. We also calculate the variance within each sub-sample to complement our value for discrepancies. To illustrate, let us use a third-party R package called mice and use a dataset called airquality. Unfortunately, our dataset has missing data for the Ozone and Solar.R predictors: # display only the first 10 observations. head(airquality, n= 10) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 ## 7 23 299 8.6 65 5 7 ## 8 19 99 13.8 59 5 8 ## 9 8 19 20.1 61 5 9 ## 10 NA 194 8.6 69 5 10 In the list, we see our Ozone predictor having two missing data (from the 1st displayed records) and two missing data for Solar.R. Assuming the missingness is MCAR. Using mice(.), let us review some missing data patterns. Here, we exclude Month and Day columns. See Figure 9.46. Figure 9.46: Imputation ## Wind Temp Solar.R Ozone ## 111 1 1 1 1 0 ## 35 1 1 1 0 1 ## 5 1 1 0 1 1 ## 2 1 1 0 0 2 ## 0 0 7 37 44 There are 153 observations in which the Zone predictor has 37 missing values and the Solar.R predictor has seven missing values. Let us perform imputation using mice(.). The function will run through 5 iterations for 2 imputations (m=2 sub-samples). We use Predictive mean matching (PMM) as our imputation method (Morris T.P. et al. 2014). imputed.datasets = mice(missing.data, m=2, maxit=5, meth=&#39;pmm&#39;, seed=2020) ## ## iter imp variable ## 1 1 Ozone Solar.R ## 1 2 Ozone Solar.R ## 2 1 Ozone Solar.R ## 2 2 Ozone Solar.R ## 3 1 Ozone Solar.R ## 3 2 Ozone Solar.R ## 4 1 Ozone Solar.R ## 4 2 Ozone Solar.R ## 5 1 Ozone Solar.R ## 5 2 Ozone Solar.R summary(imputed.datasets) ## Class: mids ## Number of multiple imputations: 2 ## Imputation methods: ## Ozone Solar.R Wind Temp ## &quot;pmm&quot; &quot;pmm&quot; &quot;&quot; &quot;&quot; ## PredictorMatrix: ## Ozone Solar.R Wind Temp ## Ozone 0 1 1 1 ## Solar.R 1 0 1 1 ## Wind 1 1 0 1 ## Temp 1 1 1 0 To determine the imputed values calculated by mice(.), we use the following format below. We use two imputations (m=2 sub-samples) and five iterations with seed=2020. The result below shows two dataset options from which to choose an imputed dataset. For example, the fifth observation with missing Solar.R value has an imputed value of 322 for the first dataset and an imputed value of 260 for the second dataset: # display only the 1st 10 records head( imputed.datasets$imp$Solar.R, n = 6) ## 1 2 ## 5 44 220 ## 6 322 260 ## 11 8 7 ## 27 8 14 ## 96 187 139 ## 97 36 193 Let us now complete the imputation by using complete(.) function. Below is the original dataset with missing data. head(airquality, n=6) ## Ozone Solar.R Wind Temp Month Day ## 1 41 190 7.4 67 5 1 ## 2 36 118 8.0 72 5 2 ## 3 12 149 12.6 74 5 3 ## 4 18 313 11.5 62 5 4 ## 5 NA NA 14.3 56 5 5 ## 6 28 NA 14.9 66 5 6 Let us choose the first dataset option for completing the imputation. imputed.airquality = complete(imputed.datasets,1) head(imputed.airquality, n=6) ## Ozone Solar.R Wind Temp ## 1 41 190 7.4 67 ## 2 36 118 8.0 72 ## 3 12 149 12.6 74 ## 4 18 313 11.5 62 ## 5 14 44 14.3 56 ## 6 28 322 14.9 66 Alternatively, we can choose the second dataset option, which produces different values for the missing data. imputed.airquality = complete(imputed.datasets,2) head(imputed.airquality, n=6) ## Ozone Solar.R Wind Temp ## 1 41 190 7.4 67 ## 2 36 118 8.0 72 ## 3 12 149 12.6 74 ## 4 18 313 11.5 62 ## 5 14 220 14.3 56 ## 6 28 260 14.9 66 We leave readers to also investigate Local Residual Draws (LRD). 9.5.10 Confounding Variable To understand the concept of Confounding Variable, let us first review the linear formula below: \\[\\begin{align} y = \\beta_0 + \\beta_1 \\cdot X + err \\end{align}\\] The \\(\\beta_0\\) represents an intercept, and \\(\\beta_1\\) is a coefficient (or a weight). The third term in the equation can be interpreted as noise (or maybe error) contributed by some unknown variable that is not included in the model. Because this variable is unknown, we treat it as noise. It has many interpretations, namely error, residual, perturbation, variation, or effect. This effect or residual is yet to be known. In such cases, this unknown variable (or a set of unknown variables) may be called a confounded variable if it contributes to noise (creating a non-zero covariance). In exploring and analyzing our data, there are cases in which our data does not just come with one predictor variable but with multiple predictor variables. Take the Table 9.33 listing contributing factors to climate change. There are three predictor variables: Humidity, Temperature, and Wind Velocity. Table 9.33: Climate Change Date Humidity (%) Temperature (F) Wind Velocity (mph) January 01, 2018 20 70 30 January 02, 2018 13 100 10 We use the three predictor variables to predict climate change (response variable). However, we know that the three predictors do not just influence climate change. Any or all of the following variables may contribute to climate change - deforestation, coal mining, greenhouse gases, and smog emissions. Notice that we just listed four more variables in the mix. Our task now is to study the effect of those additional predictor variables on climate change. That is where confounding variables may come into play. Mixing the right predictor variables is a challenge. However, if we mix unwanted input variables that distort our analysis, it is not in our desired position. A confounding variable tends to be described in terms of its bad significance. It means that its use in the model may cause chaos or distortion to the outcome. If so, in essence, a confounding variable is a bias variable - or an error variable, to put it simply. We commit an error in adding or excluding the variable if the variable distorts the outcome. 9.5.11 Data Leakage Data Leakage has a similar concept as Confounding Variable in terms of the effect of distorting outcomes. However, in this section, we deal with observations instead of predictors. Data leakage is a condition that happens when unseen data (e.g., from our testing set) gets mixed into our training set (and vice versa). Contaminating our training set distorts our outcome. Moreover, if we use the outcome as proof in announcing bold claims, it certainly creates an unwanted position. One way to determine data leakage is to analyze the correlation between our training and test set. Moreover, one way to avoid data leakage is to have a separate validation set to evaluate our final model. We also cover data leakage under General Modeling section. 9.5.12 One Hot Encoding One Hot Encoding is a method used in predictions when the dataset contains both categorical and numerical data. To fit our data well into our prediction model, we transform our categorical data into one with a vertical format. See Figure 9.47. The weather feature is translated into a set of bit vectors. Each bit vector represents a unique category in the weather feature. For example, we create a new Sunny feature, representing a bit vector with 1 for all weather patterns that fall under sunny weather; otherwise, the bit is set to zero. Similarly, we create a new feature called Rainy. Figure 9.47: One Hot Encoding To illustrate in R code, let us first create a simple dataset. We include a categorical feature called Weather like so: dataset = data.frame( Weather = c(&quot;Sunny&quot;, &quot;Rainy&quot;, &quot;Rainy&quot;, &quot;Sunny&quot;, &quot;Sunny&quot;), Items = c(&quot;A,C,D&quot;, &quot;B,E&quot;, &quot;A,C,D,E&quot;, &quot;A,B&quot;, &quot;B,F&quot;), Score = c(6.5, 9.8, 3.2, 4.4, 7.8) ) dataset ## Weather Items Score ## 1 Sunny A,C,D 6.5 ## 2 Rainy B,E 9.8 ## 3 Rainy A,C,D,E 3.2 ## 4 Sunny A,B 4.4 ## 5 Sunny B,F 7.8 We then perform One Hot encoding on the weather feature using model.matrix(.) function. That creates a dataset with a list of bit vectors. onehot = model.matrix( ~ 0 + Weather , data = dataset ) colnames(onehot) = c(&quot;Rainy&quot;, &quot;Sunny&quot;) onehot[,1:2] ## Rainy Sunny ## 1 0 1 ## 2 1 0 ## 3 1 0 ## 4 0 1 ## 5 0 1 Our next step is to incorporate the bit vectors into the dataset using mutate(.) function from dplyr package: library(dplyr) mutated.dataset = mutate(dataset, Weather=NULL, Rainy = onehot[,1], Sunny = onehot[,2]) mutated.dataset [, c(&quot;Sunny&quot;, &quot;Rainy&quot;, &quot;Items&quot;, &quot;Score&quot;)] ## Sunny Rainy Items Score ## 1 1 0 A,C,D 6.5 ## 2 0 1 B,E 9.8 ## 3 0 1 A,C,D,E 3.2 ## 4 1 0 A,B 4.4 ## 5 1 0 B,F 7.8 9.5.13 Winsorization and Trimming The concept of Winsorization and Trimming is best explained with a dataset that follows a Gaussian Normal distribution. We see the right-most tail and the left-most tail representing extreme outliers in a Normal distribution. Instead of stretching our dataset to cover both extreme outliers, it may be necessary at times to trim off those outliers - hence, the term trimming. For example, any data point below the 5% quartile or above the 95% quartile is excluded from the dataset. Our dataset covers only the range between the 5% quartile and 95% quartile. Another way to handle outliers is by Winsorization. The idea is to take the values right at the exact 5% quartile and 95% quartile. We then use the two values to replace any outlier below the 5% or above the 95% quartile. This concept is almost similar to Missingness and Imputation, in which we replace missing values with the average values of our observation (granting we choose the simplest method). In the case of Winsorization, we replace extreme values with a cut-off value taken at specified boundaries. To illustrate, let us apply winsorization in R code using a third-party package called DescTools along with its function Winsorize(.). To achieve Winsorization, let us simulate a dataset evenly spread between -4 and 4. We then winsorize at -3 and 3. options(width=56) library(DescTools) set.seed(2020) dataset = seq(-4, 4, length.out=10) dataset ## [1] -4.0000 -3.1111 -2.2222 -1.3333 -0.4444 0.4444 ## [7] 1.3333 2.2222 3.1111 4.0000 Winsorize(dataset, minval=-3.0, maxval = 3.0 ) ## [1] -3.0000 -3.0000 -2.2222 -1.3333 -0.4444 0.4444 ## [7] 1.3333 2.2222 3.0000 3.0000 9.5.14 Discretization Discretization, also called Binning, is the method of replacing a continuous number with its discretized counterpart. We divide the continuous space into discrete intervals called bins and then assign continuous numbers to its corresponding bin. To illustrate, let us use discretize(.) function found in arules package. library(arules) data = round( seq(0, 1, length.out=12), 2) discrete = discretize(data, method=&quot;frequency&quot;, breaks=4, labels=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;)) transformed.data = data.frame( continuous.form = data, discrete.form = c(discrete), label = factor(discrete) ) transformed.data ## continuous.form discrete.form label ## 1 0.00 1 A ## 2 0.09 1 A ## 3 0.18 1 A ## 4 0.27 2 B ## 5 0.36 2 B ## 6 0.45 2 B ## 7 0.55 3 C ## 8 0.64 3 C ## 9 0.73 3 C ## 10 0.82 4 D ## 11 0.91 4 D ## 12 1.00 4 D In our application of discretization, we use four intervals corresponding to four bins which are given labels (A, B, C, D). Another term for bin is category or level. Below, we use the function levels(.) to show the category used: levels(discrete) ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; If we omit the label, then we see the intervals used for the discrete counterparts: discrete = discretize(data, method=&quot;frequency&quot;, breaks=4, labels = NULL) levels(discrete) ## [1] &quot;[0,0.247)&quot; &quot;[0.247,0.5)&quot; &quot;[0.5,0.752)&quot; ## [4] &quot;[0.752,1]&quot; 9.5.15 Stratification Stratification, also called Segmenting or Stratifying, is a method to sort observations into groups (strata). An individual group is called the stratum. To illustrate, we use a 3rd-party package called splitstackshape. A function to use for stratification is stratified(.). Now, suppose we have the following dataset. library(splitstackshape) set.seed(2020) N=10 dataset = data.frame( Weather = sample(c(&quot;Sunny&quot;, &quot;Rainy&quot;), size=N, replace=TRUE), Participation = rbinom(n = N, size=100, prob=0.50) ) dataset ## Weather Participation ## 1 Rainy 57 ## 2 Sunny 53 ## 3 Rainy 60 ## 4 Sunny 46 ## 5 Sunny 53 ## 6 Sunny 50 ## 7 Sunny 54 ## 8 Sunny 59 ## 9 Sunny 47 ## 10 Rainy 42 Let us stratify the dataset using stratified(.). Our goal is to sort the activities into sample groups of rainy and sunny weather. After grouping the observations, we then take a sample per group, given a specific sample size - it can be a fraction of the total size of the sample (here, we use 50%). (strata = stratified(indt=dataset, group=c(&quot;Weather&quot;), size=0.50)) ## Weather Participation ## 1: Rainy 60 ## 2: Rainy 57 ## 3: Sunny 47 ## 4: Sunny 46 ## 5: Sunny 54 ## 6: Sunny 50 To validate, let us startify the dataset by using a combination of functions from dplyr package, namely group_by(.) and sample_frac(.). strata = dataset %&gt;% group_by(Weather) %&gt;% sample_frac(0.50) as.data.frame(strata) ## Weather Participation ## 1 Rainy 60 ## 2 Rainy 42 ## 3 Sunny 50 ## 4 Sunny 54 ## 5 Sunny 47 ## 6 Sunny 53 Our dataset is grouped by Weather. We can summarize each group using two functions. using aggregate: aggregate(dataset$Participation, by=list(Weather=dataset$Weather), FUN=length) ## Weather x ## 1 Rainy 3 ## 2 Sunny 7 using tapply: (group.size = tapply(dataset$Participation, dataset$Weather, FUN=length)) ## Rainy Sunny ## 3 7 The Rainy group has a size of 3 and the Sunny group has a size 7. If we apply a sample fraction of 50%, then we have the following sample size per group: Sample Size for Rainy group: 3 \\(\\times\\) 0.50 = 2. Sample Size for Sunny group: 7 \\(\\times\\) 0.50 = 4. 9.5.16 Fine and Coarse Classing Dataset may come with continuous independent variables. Therefore, there may be cases when we need to transform continuous independent variables into a discrete form before classifying them. One method we can use for the transformation is Binning, which we covered in Computational Learning I under the EDA section. Here, we call such Binning either Fine Classing or Coarse Classing, which groups the independent variable into K groups. Fine Classing is binning features with manageable cardinality, e.g., 20 and less. On the other hand, Coarse Classing tries to combine smaller Fine Classes into larger classes. A common method is called Weight of Evidence (WoE) and Information Value (IV) for interaction which flattens a distribution. For example, we can use log-odds to transform the dataset into a dichotomous dataset given a continuous distribution. Such methods try to address concerns about losing information during the transformation. \\[\\begin{align} \\text{WoE}_j = \\ \\log_e(\\text{Odds)} = \\ \\log_e \\left[\\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\\right] \\end{align}\\] \\[ \\text{where Odds} = \\frac{\\% \\text{ Positive}}{\\% \\text{ Negative}} \\] Such formula is related to Naive Bayes in which we have the following: \\[\\begin{align} \\underbrace{\\log_e \\left[\\frac{P(Y=1|X = x_j)}{P(Y = 0|X = x_j)}\\right]}_{\\text{posterior}} = \\underbrace{\\log_e \\left[\\frac{P(Y=1)}{P(Y = 0)}\\right]}_{\\text{prior}} + \\underbrace{\\log_e \\left[\\frac{P(X = x_j|Y=1)}{P(X = x_j|Y = 0)}\\right]}_{\\text{likelihood}} \\end{align}\\] where \\(x_j \\in B_i\\) such that B is a set of bins. Because prior excludes the X feature, we can ignore the Prior and thus end with the WeO formula. To then measure the predictive power of a feature, we can use IV: \\[\\begin{align} \\begin{array}{lll} \\text{IV}_j &amp;= \\text{WoE}_j \\times (\\% \\text{Positive } - \\% \\text{Negative}) \\\\ &amp;= \\text{WoE}_j \\times \\left(\\sum_i^k \\left[P(x_j \\in B_i|Y=1) - P(x_j \\in B_i|Y=0)\\right] \\right) \\end{array} \\label{eqn:eqnnumber400} \\end{align}\\] Based on IV, we can then interpret the predictive power like so (as an example): \\[ \\begin{array}{ll} 0\\% \\le \\text{IV} &lt; 20\\% &amp; \\text{Not Predictive (Exclude)} \\\\ 20\\% \\le \\text{IV} &lt; 40\\% &amp; \\text{Weak} \\\\ 40\\% \\le \\text{IV} &lt; 70\\% &amp; \\text{Medium} \\\\ 70\\% \\le \\text{IV} \\le 100\\% &amp; \\text{Highly Predictive (Strong)} \\\\ \\end{array} \\] We leave readers to investigate further the use of WoE and IV in terms of feature transformation and selection, excluding the need to handle missing values, dummy variables, and outliers. 9.5.17 Embedding Other types of data that we also have to explore deal with pictures (images), videos, music, and natural languages (words and sentences). To process these types of data computationally, somehow, we need to transform or map them into their numerical representation, structured in the form of matrices or vectors). Such an interpretable representation of a particular data type is called Embedding. Embedding has many definitions on the net (a few of which are): a mapping from high dimensional space to lower-dimensional space. a transformation of discrete (categorical) data into its continuous numerical representation. A few examples of embeddings are: images are mapped to a 3-D matrix (e.g., convnet). words are mapped into a vector (e.g., word2vec). medical codes are mapped into multi-layered representation (e.g., med2vec). In this section, let us use a simple embedding from a bag of words to a vector. First, let us define a few terms: Corpus - a collection of documents or texts (written or spoken). Documents - a collection of thoughts (that are informational). Dictionary - a collection of unique words (with lexical and semantic meaning). Term - is another synonym for word. Second, let us collect a corpus of documents like so: Document 1 - Tomorrow will be another day, but today is a great day. Document 2 - I have a great day today. Document 3 - It is a wonderful day. Third, let us construct our dictionary of words (terms) and count the occurrence per word. See Figure 9.48. Figure 9.48: Embedding (Vector Space Model) We introduce stop words (table T1) in the example to note that certain words are common enough to be filtered out as they may not add value (instead of add distortion or skewness) to our analysis. Depending on necessity, our list of stop words may vary. Table T2 represents our dictionary - a list of vocabulary. Fourth, let us map the three documents based on our generated vocabulary. Three documents are mapped to a term-document matrix (see table T3) - this simple embedding is called vector space model (VSM). Finally, the use of embedding becomes apparent when we search for a document given a query. For example, suppose we are searching for a document, and we have the following search query: \\[ \\text{Query = &quot;great day today&quot;}. \\] Let us discuss two simple solutions to help us rank our documents and retrieve the document with the highest rank. The first solution is called simple VSM, which counts for the existence of unique terms in each document. So, for example, there are three unique terms in our query that exist in D1 and D2. Moreover, only one unique term in our query exists in D3. \\[ \\text{search(Query, D1) = 3}\\ \\ \\ \\ \\ \\ \\ \\ \\text{search(Query, D2) = 3}\\ \\ \\ \\ \\ \\ \\ \\ \\text{search(Query, D3) = 1} \\] Therefore, the documents we need to retrieve are D1 and D2. A better approach is called Term-frequency (TF) weighing, which accounts for duplicate terms. For example: \\[ \\text{search(Query, D1)} = \\underbrace{1}_{\\text{great}} + \\underbrace{2}_{\\text{day}} + \\underbrace{1}_{\\text{today}} = 4 \\] \\[ \\text{search(Query, D2)} = \\underbrace{1}_{\\text{great}} + \\underbrace{1}_{\\text{day}} + \\underbrace{1}_{\\text{today}} = 3 \\] Therefore, the document we need to retrieve is D1. In Chapter 11 (Computational Learning III), we introduce TF-IDF and Cosine Similarity, which are covered in Text Mining, complementing and enhancing document ranking. We leave readers to investigate word2vec, med2vec, and convnet for some well-known embeddings. 9.6 Feature Engineering The essential primary ingredient in computational learning - or machine learning (ML) - is the input. We define input as any observable and measurable property of an entity or phenomenon (Wikipedia). Our goal is to study and analyze the input - a raw material (a raw metric) - and determine its importance and relevance. Any input extracted and selected based on domain knowledge is called a candidate feature. That is where Feature Engineering comes into play. 9.6.1 Machine Learning Features In Machine Learning, we introduce terms such as Feature(s) and Label(s), Input(s) and output(s). See Table 9.34 for equivalent terms. Table 9.34: Variables Answer Question Dependent variable Independent variables Response variable (Variate) Predictor variables (Regressor / Covariate) Explained variable Explanatory variables Label Features Output Inputs Barring statistics for a moment, we can treat features as traits, characteristics, or properties of entities. They describe the profile or personality of entities. For example, the weather has properties such as humidity, temperature, and wind velocity. Another example is a person with unique traits and characteristics that embody the personality of the individual. With machine learning data, we also need to give a name for the person. In other words, we need to label the personality. In essence, a label represents an object’s attributes. While we explain this concept using objects, machine learning data is not limited to just characterizing an object. We can treat features also in terms of inputs that result in an output (the label). As in statistics, any input may have the quality of being observable and measurable and thus can result in a definitive output that can be labeled. For example, humidity, temperature, and wind velocity are inputs (features) in our study of the effect of climate change. The effect of climate change represents the focus of our study and our labeling efforts. Recall that while we are interested in data that is only observable and measurable entities, we prefer to narrow down data into features that contribute significance and relevance for analysis in data science. Here, it is important to note that not all such attributes or features can contribute or influence; therefore, we have algorithms that reduce the number of Features to those more relevant. There seems to be a consensus about the lack of a unified formal definition of Feature Engineering. If so, perhaps we can prescribe our overarching definition with no intention to solicit merit. Let us first describe the basic premise of Engineering, which is a systematic and creative application of scientific, mathematical, theoretical, and empirical knowledge (sourced from britannica.com, linkengineering.org, bccampus.ca) to produce consumables (usable products) from raw materials. With that said, it seems sensible to regard Feature Engineering as both methodological (bearing the application of math and science) and ideological (bearing the demand for artful creativity) in the context of creating, selecting or extracting, and transforming features, bearing in mind the domain for which we apply such processes. One has to develop a system of methodologies describing the creation and reduction of features from a set of raw inputs unique to a domain. The goal is to eliminate redundant and irrelevant information. Furthermore, the intent is to enhance the predictive power of models, increase the accuracy of output, and enhance the performance of algorithms, among many other benefits. In this section, our view of Feature Engineering covers a system of three overlapping methodologies: Construction, Selection, and Transformation of Features. In some cases, these methodologies are more ideological, and thus other literature follows specific descriptions, categories, or order. However, ultimately, we follow the GIGO philosophy - garbage in, garbage out. In other words, we are more unified to believe that a solid, relevant feature provides more valuable and informative data that guides us toward better decision choices. 9.6.2 Dimensionality Reduction Dimension in ML refers to the number of features in a dataset (whether they are relevant, irrelevant, or redundant). A highly dimensional dataset means that the dataset contains a high number of features. Therefore, it is sensical to reduce the number of dimensions in our dataset to a minimum - this is called dimensionality reduction. The following sections cover strategies that allow us to perform such dimensionality reduction, starting with PCA. 9.6.3 Principal Component Analysis Principal Component Analysis (PCA) is widely discussed and explained on the net as one of the many dimensionality reduction methods. Here, we may have to recall concepts such as Eigenvectors, EigenValues, and Singular Valued Decomposition (SVD) discussed in Chapter 2 (Numerical Linear Algebra I). In addition, the concept of Covariance is also needed. It is briefly covered under the EDA section in this Chapter and Chapter 2 (Numerical Linear Algebra I) under the Types of Matrices Section. To get the intuition behind PCA, it helps to compare it with Ordinary Least Square (OLS), which we covered in Chapter 3 (Numerical Linear Algebra II) (see Figure 9.49). In OLS, we try to find the best fit through the data points by computing the minimum sum square of all vertical distances between the data points and the line. In other words, we compute for the least residual. The line becomes the model upon which we predict values of Y based on a given X. In PCA, it is essential to note in the figure that we are using X1 and X2 axes to indicate that we are instead of comparing two independent variables. Conceptually, in PCA, we also try to find the best fit of the line through the data points; however, the fitted line is a projection that represents the spread (or variance) of the data points being projected. The goal is to find the largest variance by computing the maximum sum square of all distances between the projected points and the point of origin, with the corresponding Principal axis. Equivalently, we can also compute the least square distance of the orthogonal projection to the corresponding Principal axis. Figure 9.49: OLS vs PCA To illustrate, let us first generate our own dataset: set.seed(2019) N = 25 range = seq(1,100) dataset = matrix(sample(range, size=N*4, replace=TRUE), nrow=N, ncol=4) colnames(dataset) = c(&quot;feature1&quot;, &quot;feature2&quot;, &quot;feature3&quot;, &quot;feature4&quot;) head(dataset) ## feature1 feature2 feature3 feature4 ## [1,] 77 2 31 100 ## [2,] 72 60 9 13 ## [3,] 31 95 4 52 ## [4,] 62 63 35 34 ## [5,] 6 46 52 77 ## [6,] 5 65 38 5 Second, we perform centering and scaling for our dataset by standardization. Note that our scaling and centering are column-wise. See standardization in Primitive Analytical Methods section. Hereafter, we only use centering. We defer scaling for now and illustrate the need to manually scale to unit eigenvector in further discussion later. head((std.dataset = scale(dataset, center=TRUE, scale=FALSE))[,]) ## feature1 feature2 feature3 feature4 ## [1,] 32.24 -46.24 -10.52 43.16 ## [2,] 27.24 11.76 -32.52 -43.84 ## [3,] -13.76 46.76 -37.52 -4.84 ## [4,] 17.24 14.76 -6.52 -22.84 ## [5,] -38.76 -2.24 10.48 20.16 ## [6,] -39.76 16.76 -3.52 -51.84 Third, for simple explanation of PCA, we use feature1 and feature2. X = std.sample = std.dataset[,c(1:2)] head(X) ## feature1 feature2 ## [1,] 32.24 -46.24 ## [2,] 27.24 11.76 ## [3,] -13.76 46.76 ## [4,] 17.24 14.76 ## [5,] -38.76 -2.24 ## [6,] -39.76 16.76 Fourth, we compute for the Principal components using the svd(.) function. Now, recall that we covered Single Valued Decomposition (SVD) under Matrix Factorization in Chapter 2 (Numerical Linear Algebra I). Here, we use the following SVD equation: \\[\\begin{align} X = UDV^T \\end{align}\\] where: X is the matrix (e.g., our dataset - centered and scaled). U holds the left eigenvectors, also called the loading matrix. D is the standard deviation (diagonal) for rotation. \\(V\\) holds the right eigenvectors, also called the scores matrix. svd.model = svd(std.sample) (D = svd.model$d) ## [1] 159.9 140.2 (U = svd.model$u)[1:5,] # show only first 5 rows ## [,1] [,2] ## [1,] 0.20133 0.32999 ## [2,] -0.12804 0.15314 ## [3,] -0.24445 -0.20766 ## [4,] -0.12396 0.07883 ## [5,] 0.09718 -0.25373 (V = svd.model$v) ## [,1] [,2] ## [1,] -0.3467 0.9380 ## [2,] -0.9380 -0.3467 We then compute the PCs using the following formula: \\[\\begin{align} PC = X \\cdot V \\end{align}\\] (PC = X %*% V)[1:5,] # show only first 5 rows ## [,1] [,2] ## [1,] 32.19 46.27 ## [2,] -20.47 21.47 ## [3,] -39.09 -29.12 ## [4,] -19.82 11.05 ## [5,] 15.54 -35.58 The second way is to compute for the covariance of our dataset and then use the eigen(.) function to compute for the EigenVectors and EigenValues using the covariance matrix. The covariance between the two features is computed using the following formula (for a large number of features, it may help to decompose the matrix using Cholesky Factorization): \\[\\begin{align} Cov(X) = \\frac{(X - \\bar{X})^T(X - \\bar{X})}{N - 1} \\end{align}\\] X.mean = apply(X, 2, mean) (cov.features = ( t(X - X.mean) %*% (X - X.mean) ) / (N - 1)) ## feature1 feature2 ## feature1 848.86 80.06 ## feature2 80.06 1035.86 The same covariance can be computed using the cov(.) function. (cov.features = cov(std.sample)) ## feature1 feature2 ## feature1 848.86 80.06 ## feature2 80.06 1035.86 We first compute for the EigenValues and EigenVectors. p.eigen = eigen(cov.features) (eigenVal = p.eigen$values) # EigenValues ## [1] 1065.4 819.3 (eigenVec = p.eigen$vectors) # EigenVectors ## [,1] [,2] ## [1,] 0.3467 -0.9380 ## [2,] 0.9380 0.3467 Then we compute for the PCs: (PC = X %*% eigenVec)[1:5,] # show only first 5 rows ## [,1] [,2] ## [1,] -32.19 -46.27 ## [2,] 20.47 -21.47 ## [3,] 39.09 29.12 ## [4,] 19.82 -11.05 ## [5,] -15.54 35.58 Note that the expanded version of the PCA formula is as such: \\[\\begin{align} PC = X \\cdot V = \\sum_i^n X_i \\times V_i \\end{align}\\] In this case, it is easy to just perform the computation with two features: V = loadings = eigenVec PC1 = X[,1] * V[1,1] + X[,2] * V[2,1] PC2 = X[,1] * V[1,2] + X[,2] * V[2,2] (PC = cbind(PC1, PC2))[1:5,] ## PC1 PC2 ## [1,] -32.19 -46.27 ## [2,] 20.47 -21.47 ## [3,] 39.09 29.12 ## [4,] 19.82 -11.05 ## [5,] -15.54 35.58 The third way is using the prcomp(.) function which readily computes for the PCs. (pca.model = prcomp(X)) ## Standard deviations (1, .., p=2): ## [1] 32.64 28.62 ## ## Rotation (n x k) = (2 x 2): ## PC1 PC2 ## feature1 -0.3467 0.9380 ## feature2 -0.9380 -0.3467 We can get the PCs via the PCA model produced: (scores = pca.model$x)[1:5,] # show only first 5 rows ## PC1 PC2 ## [1,] 32.19 46.27 ## [2,] -20.47 21.47 ## [3,] -39.09 -29.12 ## [4,] -19.82 11.05 ## [5,] 15.54 -35.58 The same PCA model has the Eigenvectors which we can use to also compute for PCA: V = loadings = pca.model$rotation # eigenvectors PC1 = X[,1] * V[1,1] + X[,2] * V[2,1] PC2 = X[,1] * V[1,2] + X[,2] * V[2,2] (PC = cbind(PC1, PC2))[1:5,] ## PC1 PC2 ## [1,] 32.19 46.27 ## [2,] -20.47 21.47 ## [3,] -39.09 -29.12 ## [4,] -19.82 11.05 ## [5,] 15.54 -35.58 We can now see that a Principal Component makes up of a linear combination - a dot product of the data points and corresponding eigenvectors. We will expound on this idea later. Fifth, let us plot the model (see Figure 9.50). The idea is to find the maximum deviation (or variance) of the two features. We maximize the distance between the projected points to the point of origin. par(pty=&quot;s&quot;) par(mfrow=c(1,2)) ### Plotting the first Principal Component (PC1) plot(NULL, xlim=range(-80,80), ylim=range(-100,80), xlab=&quot;feature1 (X1)&quot;, ylab=&quot;feature2 (X2)&quot;, main=&quot;Maximum Variance (PC1)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) ### Intercept and Slope of PC1 pc1.intrcpt = 0 pc1.slope = eigenVec[2,1]/eigenVec[1,1] lines(X[,1], pc1.slope * X[,1], col=&quot;red&quot;, lwd=1) ### Projection to PC1 slope.orthog = - (1 / pc1.slope) intrcpt.orthog = X[,2] - slope.orthog * X[,1] x.proj = pc1.x.proj = (pc1.intrcpt - intrcpt.orthog) / (slope.orthog - pc1.slope) y.proj = pc1.y.proj = pc1.slope * x.proj + pc1.intrcpt segments(X[,1], X[,2], x.proj, y.proj, col=&quot;red&quot;, lty=3, lwd=1) ### Plot the Points points(X, col=&quot;grey&quot;, pch=16, cex=0.8) points(x.proj, y.proj, col=&quot;red&quot;, pch=16, cex=0.8) points(0, 0, col=&quot;navyblue&quot;, pch=16) ### Legend and PC1 label legend(&quot;bottomright&quot;, legend=c(&quot;Projected Points&quot;, &quot;Point of Origin&quot;), col=c(&quot;red&quot;, &quot;navyblue&quot;), pch=c(16, 20, 20), cex=0.8) text(-55,60, label=&quot;PC1&quot;) ### Plotting the second Principal Component (PC2) plot(NULL, xlim=range(-80,80), ylim=range(-100,80), xlab=&quot;feature1 (X1)&quot;, ylab=&quot;feature2 (X2)&quot;, main=&quot;Maximum Variance (PC2)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) ### Intercept and Slope of PC2 pc2.intrcpt = 0 pc2.slope = eigenVec[2,2]/eigenVec[1,2] lines(X[,2], pc2.slope * X[,2], col=&quot;blue&quot;, lwd=1) ### Projection to PC2 slope.orthog = - (1 / pc2.slope) intrcpt.orthog = X[,2] - slope.orthog * X[,1] x.proj = pc2.x.proj = (pc2.intrcpt - intrcpt.orthog) / (slope.orthog - pc2.slope) y.proj = pc2.y.proj = pc2.slope * x.proj + pc2.intrcpt segments(X[,1], X[,2], x.proj, y.proj, col=&quot;blue&quot;, lty=3, lwd=1) ### Plot the Points points(X, col=&quot;grey&quot;, pch=16, cex=0.8) points(x.proj, y.proj, col=&quot;navyblue&quot;, pch=16, cex=0.8) points(0, 0, col=&quot;red&quot;, pch=16) ### Legend and PC2 label legend(&quot;bottomright&quot;, legend=c( &quot;Projected Points&quot;, &quot;Point of Origin&quot;), col=c(&quot;navyblue&quot;, &quot;red&quot;), pch=c(16, 20, 20), cex=0.8) text(-50,-40, label=&quot;PC2&quot;) Figure 9.50: Principal Component Analysis (PCA) Here, it helps to know the terminologies used in SVD and PCA. We start by describing Principal Component. Geometrically, a Principal Component (PC1) is represented by a Principal axis, oriented in the direction that maximizes the variance (given two features). We also term this Principal direction. Data points are projected to this Principal axis. Then, the sum of squared distance (SSD) of all the projected points from the point of origin along the Principal axis is calculated. See below: \\[\\begin{align} \\text{SSD}_{max} = \\underset{ssd}{max} \\left\\{ \\sum_i^n d_i^2 \\right\\}= \\underset{ssd}{max} \\left\\{d_1^2 + d_2^2 + d_3^2 + ... + d_n^2 \\right\\} \\end{align}\\] To get the maximum SSD, we may need to visualize the geometric representation of the Principal Component as it rotates around the point of origin until such that we find the maximum variance. A second Principal Component (PC2) inherently gets created, which is also geometrically represented by a Principal axis that is orthogonal (or perpendicular) to the primary Principal axis (PC1). Equivalently, we get the sum squared distance of the projected points from the point of origin along the orthogonal axis by using the SSD formula. Sixth, let us compute for the unit vectors. Note that the orientation of the segments that make up both PC1 andPC2 is described by the slopes of their respective lines (axis). We know that a slope consists of the rise over the run, which corresponds to the y vector and x vector (also called PC Scores or Loading Scores ). \\[\\begin{align} \\text{slope} = \\frac{\\text{rise}}{\\text{run}} = \\frac{\\text{y vector}}{\\text{x vector}} \\end{align}\\] In this specific case, we only focus on our two dimensions (e.g. two features). colnames(V) = c(&quot;EigenVec for PC1&quot;, &quot;EigenVec for PC2&quot;) rownames(V) = c(&quot;Score 1 (a)&quot;, &quot;Score 2 (b)&quot;) V ## EigenVec for PC1 EigenVec for PC2 ## Score 1 (a) -0.3467 0.9380 ## Score 2 (b) -0.9380 -0.3467 The slope of our linear combinations (our PCs) becomes: pc1.slope = V[1,1] / V[2,1] pc2.slope = V[1,2] / V[2,2] c(&quot;Slope for PC1&quot; = pc1.slope, &quot;Slope for PC2&quot; = pc2.slope) ## Slope for PC1 Slope for PC2 ## 0.3696 -2.7054 Now suppose for PC1, we have a=-0.3467 and b=-0.938; therefore, a classic Pythagorean Theorem gives us the linear combination as \\(c = \\sqrt{a^2 + b^2} =\\) 1. This is the length (or magnitude value) of our Eigenvector for PC1. To compute the unit vector of the x and y vectors, including the Eigenvector, we divide all the vector components by the Eigenvector length. This makes the Eigenvector a Unit Eigenvector equal to 1, e.g. \\(\\text{unit eigenvector } =\\) 1/1. The y and x unit vectors have -0.938 and -0.3467, respectively. It means that for every 0.3467 unit decrease of feature1, we get -0.938 unit increase of feature2. ## X-vec Y-vec Eigenvector ## -0.3467 -0.9380 1.0000 Similarly, for PC2, we have the following: ## X-vec Y-vec Eigenvector ## 0.9380 -0.3467 1.0000 If an Eigenvector is scaled down to a unit vector, we call this Singular vector for the PC1. On the other hand, the Singular value for PC1 is computed by taking the root square of its SSD. \\[\\begin{align} \\text{singular value} = \\sqrt{\\text{SSD}} \\end{align}\\] Below is the implementation of the singular value computation in R code: SSD &lt;- function(a, b, eigenvector) { s = 0 a.unit = a / eigenvector b.unit = b / eigenvector c = sqrt(a.unit^2 + b.unit^2) for (i in 1:N) { s = s + (c[i] - 0)^2 } s } The singular values for PC1 and PC2 are the following: PC1.ssd = SSD(pc1.x.proj, pc1.y.proj, pc1.eigenvector) PC2.ssd = SSD(pc2.x.proj, pc2.y.proj, pc2.eigenvector) SV = data.frame( ssd = rbind(PC1.ssd, PC2.ssd), singular.value = rbind(sqrt(PC1.ssd), sqrt(PC2.ssd)), variance = rbind(PC1.ssd/(N-1), PC2.ssd/(N-1)), std.dev = rbind(sqrt(PC1.ssd/(N-1)), sqrt(PC2.ssd/(N-1))), proportion = pca.model$sdev / sum(pca.model$sdev) * 100 ) rownames(SV) = c(&quot;PC1&quot;, &quot;PC2&quot;) SV ## ssd singular.value variance std.dev proportion ## PC1 25571 159.9 1065.4 32.64 53.28 ## PC2 19662 140.2 819.3 28.62 46.72 We can validate the standard deviation using the PCA.model we generated previously: pca.model$sdev ## [1] 32.64 28.62 For the proportionality of the Principal Components, we see that PC1 has a higher proportion at 53.2796%. See Figure 9.51. PC.proportions = SV$proportion barplot(PC.proportions, las=1, xlab=&quot;Principal Components&quot;, ylab = &quot;Variance&quot;, col=&quot;white&quot;, names.arg=paste(&quot;PC&quot;,1:length(PC.proportions))) Figure 9.51: PC Proportions Seventh, for two features, we can rotate the respective Principal axis of their PCs to align them along the vertical and horizontal axes. The data is also rotated. See Figure 9.52. plot(NULL, xlim=range(-80,80), ylim=range(-100,80), xlab=&quot;feature1 (X1)&quot;, ylab=&quot;feature2 (X2)&quot;, main=&quot;Rotated Principal Components&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) ### Plot the Points points(PC, col=&quot;grey&quot;, pch=16, cex=1.0) points(0, 0, col=&quot;black&quot;, pch=16) abline(h=0, col=&quot;darksalmon&quot;) abline(v=0, col=&quot;navyblue&quot;) ### Legend and PC2 label legend(&quot;bottomright&quot;, legend=c( &quot;Principal Axis (PC1)&quot;, &quot;Principal Axis (PC2)&quot;), col=c(&quot;darksalmon&quot;, &quot;navyblue&quot;), pch=c(16, 20, 20), cex=0.8) text(5, -80, label=&quot;(PC2)&quot;, cex=0.8) text(60, 5, label=&quot;(PC1)&quot;, cex=0.8) Figure 9.52: Rotated Principal Components It is important to note that, for a dataset with three features, we get three Principal axes that are all orthogonal to each other; thus, geometrically, we see a 3D representation of the Principal components. For larger datasets with more features, we can quickly generate a covariance matrix derived from our original dataset to review deviations between features quickly. Additionally, because covariance is commutative, the matrix is therefore symmetric in which the upper and lower triangular areas are equal. Because of this, we can use Cholesky Decomposition, which we introduced in Chapter 2 (Numerical Linear Algebra I) to extract the upper portion of the matrix. # Get the covariance, then transform to cholesky matrix (chol.matrix = chol( cov(dataset)) ) ## feature1 feature2 feature3 feature4 ## feature1 29.14 2.748 -2.249 0.6465 ## feature2 0.00 32.067 3.051 -3.8405 ## feature3 0.00 0.000 30.959 8.1306 ## feature4 0.00 0.000 0.000 27.1806 We can then use eigen(.) function as before. However, here, we show two functions that we can use for PCA, namely prcomp(.) and princomp(.). (pca.model = prcomp(dataset, tol=0.10, center = TRUE, scale = TRUE)) ## Standard deviations (1, .., p=4): ## [1] 1.1305 1.0455 0.9975 0.7962 ## ## Rotation (n x k) = (4 x 4): ## PC1 PC2 PC3 PC4 ## feature1 0.1670 0.46545 -0.8299 -0.2584 ## feature2 0.1668 0.81206 0.3543 0.4327 ## feature3 -0.6672 0.35077 0.2515 -0.6070 ## feature4 -0.7065 -0.02958 -0.3500 0.6144 Below is the summary of the PCA outcome: summary(pca.model) ## Importance of components: ## PC1 PC2 PC3 PC4 ## Standard deviation 1.13 1.046 0.997 0.796 ## Proportion of Variance 0.32 0.273 0.249 0.158 ## Cumulative Proportion 0.32 0.593 0.842 1.000 We also use screeplot(.) function from graphics library to visualize PCA. See Figure 9.53 for the screeplot. require(graphics) screeplot(pca.model, npcs = 5, type = c(&quot;lines&quot;)) Figure 9.53: PC Proportions Alternatively, we can use princomp(.) function. See Figure 9.53 for the screeplot. std.dataset = scale(dataset, center=TRUE, scale=TRUE) (pca.model &lt;- princomp(std.dataset, cor = TRUE)) ## Call: ## princomp(x = std.dataset, cor = TRUE) ## ## Standard deviations: ## Comp.1 Comp.2 Comp.3 Comp.4 ## 1.1305 1.0455 0.9975 0.7962 ## ## 4 variables and 25 observations. screeplot(pca.model, npcs = 5, type = &quot;lines&quot;) Figure 9.54: PC Proportions Independent Component Analysis (ICA) is similar to PCA. We leave readers to investigate further on this topic. 9.6.4 Linear Discriminant Analysis (LDA) We extend the concept of Dimensionality Reduction compared to PCA by introducing Linear Discriminant Analysis (LDA). With LDA, we assume that our dataset is classified (or labeled). Our goal is to determine the maximum separation of the classes. In other words, we discriminate data points such that they are separated into groups relative to their respective moments, e.g., mean (\\(\\mu\\)) and variance (\\(\\sigma\\)). Here, we use Figure 9.55 to illustrate the idea. Figure 9.55: Linear Discriminant Analysis (LDA) The figure shows an Eigenvector representing the space unto which data points are projected. The projected data points are assumed to follow a multimodal distribution with multinomial outcomes. In the case of our figure, we see a projection of two Gaussian distribution with respective means (\\(\\mu_1, \\mu_2\\)) and covariances (\\(\\sigma_1, \\sigma_2\\)). In LDA, similar to PCA, our goal is to find the best fit. However, a best fit represents a line (a projected vector) with maximum separability and a minimum overlap of classes. Note that in PCA, our goal is to derive the Principal components, e.g. PC1 and PC2. In LDA, our goal is to derive the Linear Discriminants, e.g. LD1 and LD2. To illustrate, we use the iris dataset, which is common in explaining LDA. features = c(&quot;Sepal.Length&quot;,&quot;Sepal.Width&quot;,&quot;Petal.Length&quot;,&quot;Petal.Width&quot;) lda.dataset = iris[,features] lda.dataset = as.matrix(lda.dataset) N = nrow(lda.dataset) head(lda.dataset) # show only first 5 observations ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## [1,] 5.1 3.5 1.4 0.2 ## [2,] 4.9 3.0 1.4 0.2 ## [3,] 4.7 3.2 1.3 0.2 ## [4,] 4.6 3.1 1.5 0.2 ## [5,] 5.0 3.6 1.4 0.2 ## [6,] 5.4 3.9 1.7 0.4 Here, our iris dataset is labeled - in other words, each observation is already classified as either Setosa, Versicolor, or Virginica depending on the Sepal and Petal properties. labels = levels(iris[,c(&quot;Species&quot;)]) K = length(labels) labels ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; First, using the Fisher Linear Discriminant method, our initial step is to compute for the mean of each features based on labels: \\[\\begin{align} \\mu_k = \\frac{1}{n_k} \\sum_{x \\in D_k} x \\ \\ \\ \\ \\ \\ \\ \\ where:\\ \\begin{array}{ll} \\mathbf{k} &amp; \\text{kth class}\\\\ \\mathbf{D} &amp; \\text{dataset}\\\\ \\end{array} \\label{eqn:eqnnumber401} \\end{align}\\] mu.k = matrix(0, nrow=K, ncol=length(features), byrow=TRUE) for (k in 1:K) { idx = which(iris[,c(&quot;Species&quot;)] == labels[k]) mu.k[k,] = apply(lda.dataset[idx,] , 2, mean) } colnames(mu.k) = features rownames(mu.k) = labels mu.k ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 Note that the whole dataset assumes to follow a multi-modal Gaussian distribution with three modes, representing three means, e.g., \\(\\mu_1, \\mu_2, \\mu_3\\), and three variances. Our goal is to project the data points to lower-dimensional spaces - a set of projections (represented by eigenvectors). Second, we compute for two types of variances. The first variance is the variance within each class. The second variance is the variance between classes. The idea is to maximize the separation of classes. That can be achieved if the variance between classes is larger than the variance within each class, allowing each data point to be closer together in a class and further away from other classes. Here, we compute for the within-class scatter matrix \\(\\mathbf{S_w}\\) like so: \\[\\begin{align} \\mathbf{S_w} = \\sum_{k=1}^{K} S_k \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ S_k = \\sum_{x \\in D_k} \\left(x - \\mu_k\\right)\\left(x - \\mu_k\\right)^T \\end{align}\\] within.class &lt;- function(x, labels) { s_k = 0 for (k in labels) { idx = which(iris[,c(&quot;Species&quot;)] == k) mu = apply(x[idx,] , 2, mean) s_i = 0 for (i in idx) { s_i = s_i + (x[i,] - mu) %*% t(x[i,] - mu) } s_k = s_k + s_i } s_k } S_w = within.class(lda.dataset, labels) rownames(S_w) = features S_w ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 38.956 13.630 24.625 5.645 ## Sepal.Width 13.630 16.962 8.121 4.808 ## Petal.Length 24.625 8.121 27.223 6.272 ## Petal.Width 5.645 4.808 6.272 6.157 Third, we compute for the between-class scatter matrix \\(\\mathbf{S_b}\\) with the given total mean per class. \\[\\begin{align} \\mathbf{S_b} = \\sum_{k=1}^{K} n_k \\left(\\mu_k - \\mu\\right)\\left(\\mu_k - \\mu\\right)^T \\ \\ \\ \\ \\ \\ where: \\ \\ \\mu\\ \\text{is total mean per class} \\end{align}\\] (mu = apply(lda.dataset, 2, mean)) # mean(lda.dataset)) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## 5.843 3.057 3.758 1.199 between.class &lt;- function(x, labels) { s_k = 0 for (k in labels) { idx = which(iris[,c(&quot;Species&quot;)] == k) n.k = length(idx) mu.k = apply(lda.dataset[idx,] , 2, mean) s_k = s_k + n.k * (mu.k - mu) %*% t(mu.k - mu) } s_k } S_b = between.class(lda.dataset, labels) rownames(S_b) = features S_b ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 63.21 -19.95 165.25 71.28 ## Sepal.Width -19.95 11.34 -57.24 -22.93 ## Petal.Length 165.25 -57.24 437.10 186.77 ## Petal.Width 71.28 -22.93 186.77 80.41 Fourth, to derive the best fit, we need to minimize the following objective function: \\[\\begin{align} \\mathcal{J}(\\mathbf{\\bar{v}}) = \\frac{(\\bar{\\mu}_1 - \\bar{\\mu}_2)^2}{s_1^2 + s_2^2} = \\frac{\\mathbf{\\vec{v}}^T S_b \\mathbf{\\vec{v}}}{\\mathbf{\\vec{v}}^T S_w \\mathbf{\\vec{v}}} \\label{eqn:eqnnumber150a} \\end{align}\\] For a good understanding of Equation (\\(\\ref{eqn:eqnnumber150a}\\)) and to explain its formulation, let us consider discussing projections and how to find the best projection. For that, we start with the idea that any data point can be projected in the direction of a vector, e.g., \\(\\mathbf{\\vec{v}}\\), using the following formula: \\[\\begin{align} \\tilde{x}_i^{(\\text{projected point)}} = \\mathbf{\\vec{v}}^T x_i \\end{align}\\] A class of data points has a mean that can also be projected like so: \\[\\begin{align} \\tilde{\\mu}_k^{(\\text{projected mean)}} = \\mathbf{\\vec{v}}^T \\mu_k \\end{align}\\] where k is a class and \\(\\mathbf{\\vec{v}}\\) is the projected line. We can compare such projected mean \\(\\tilde{\\mu}_1\\) of a class k=1 with the projected mean \\(\\tilde{\\mu}_2\\) of another class k=2 in terms of distance. The best projection is based on the largest distance between classes (with respect to their means). Next, we also account for the variance of each class using the following formula (with basic derivation included): \\[\\begin{align} \\tilde{s}_k^2 &amp;= \\sum_{x \\in D_k} \\left(\\tilde{x}_i - \\tilde{\\mu}_k\\right)^2\\\\ &amp;= \\sum_{x \\in D_k} \\left(\\mathbf{\\vec{v}}^T x_i - \\mathbf{\\vec{v}}^T \\mu_k\\right)^2\\\\ &amp;=\\sum_{x \\in D_k} \\mathbf{\\vec{v}}^T \\left(x - \\mu_k\\right)\\left(x - \\mu_k\\right)^T \\mathbf{\\vec{v}}\\\\ &amp; = \\mathbf{\\vec{v}}^T S_k \\mathbf{\\vec{v}} \\end{align}\\] Therefore: \\[\\begin{align} \\sum \\tilde{s}_k^2 = \\mathbf{\\vec{v}}^T S_w \\mathbf{\\vec{v}} \\end{align}\\] Similarly, starting with a 2 class configuration, we can derive the following: \\[\\begin{align} \\left(\\tilde{\\mu}_1 - \\tilde{\\mu}_2\\right)^2 = \\left( \\mathbf{\\vec{v}}^T \\mu_1 - \\mathbf{\\vec{v}}^T\\tilde{\\mu}_2\\right)^2 = \\mathbf{\\vec{v}}^T S_b \\mathbf{\\vec{v}} \\end{align}\\] Now to minimize the objective function, namely \\(\\mathbf{\\mathcal{J}}(\\mathbf{\\vec{v}})\\), with respect to vector \\(\\mathbf{\\vec{v}}\\), we use partial derivatives: \\[\\begin{align} \\frac{ \\partial \\mathbf{\\mathcal{J}}(\\mathbf{\\vec{v}}) } {\\partial \\mathbf{\\vec{v}}} = \\frac{ \\partial } {\\partial \\mathbf{\\vec{v}}} \\left[\\frac{\\mathbf{\\vec{v}}^T S_b \\mathbf{\\vec{v}}}{\\mathbf{\\vec{v}}^T S_w \\mathbf{\\vec{v}}}\\right] \\end{align}\\] Setting the derivative of the objective function with respect to \\(\\mathbf{\\vec{v}}\\) to zero, we get: \\[\\begin{align} \\underbrace{S_b \\mathbf{\\vec{v}} = \\lambda S_w \\mathbf{\\vec{v}} }_{\\text{Generalized Eigenvalue Problem}} \\ \\ \\ \\ \\leftarrow \\ \\ \\ \\ \\ \\ S_b \\mathbf{\\vec{v}} - \\lambda S_w \\mathbf{\\vec{v}} = 0 \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\lambda = \\frac{\\mathbf{\\vec{v}}^T S_b \\mathbf{\\vec{v}}}{\\mathbf{\\vec{v}}^T S_w \\mathbf{\\vec{v}}} \\end{align}\\] For a two-class configuration, the derived projection becomes: \\[\\begin{align} \\mathbf{\\vec{v}} = S_w^{-1} \\left(\\mu_1 - \\mu_2\\right) \\end{align}\\] For multi-class configuration, recall in Chapter 2 (Numerical Linear Algebra I) the Eigen equation and the derivation of the Eigenvalue \\(\\lambda\\) using determinant and echelon form: \\[\\begin{align} (A - \\lambda I) = 0\\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ A \\mathbf{\\vec{v}} = \\lambda \\mathbf{\\vec{v}} \\end{align}\\] Equivalently, we have the following expression: \\[\\begin{align} \\underbrace{(S_w^{-1}S_b )\\mathbf{\\vec{v}} = \\lambda \\mathbf{\\vec{v}}}_{A \\mathbf{\\vec{v}} = \\lambda \\mathbf{\\vec{v}}} \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ S_b \\mathbf{\\vec{v}} = \\lambda S_w \\mathbf{\\vec{v}} \\end{align}\\] Fifth, to then find the projection of datasets to the new axis, we use the following expression: \\[\\begin{align} y = \\mathbf{\\vec{v}}^T x \\end{align}\\] Let us compute for the eigenvalues and corresponding eigenvectors (we choose the two highest eigenvalues): e = eigen(solve(S_w) %*% S_b) idx = sort( e$values, index.return = TRUE, decreasing=TRUE) (eigenvalues = idx$x) ## [1] 3.219e+01 2.854e-01 -2.801e-15 -4.881e-15 (eigenvectors = e$vectors[idx$ix,]) ## [,1] [,2] [,3] [,4] ## [1,] -0.2087 -0.006532 0.850126 -0.6859 ## [2,] -0.3862 -0.586611 -0.373448 0.4384 ## [3,] 0.7074 -0.769453 0.002199 -0.3476 ## [4,] 0.5540 0.252562 -0.371237 0.4653 Finally, to plot, let us compute the projections of the datasets (see Figure 9.56). coeff = eigenvectors LD1 = coeff[1,1] * iris[,c(&quot;Sepal.Length&quot;)] + coeff[2,1] * iris[,c(&quot;Sepal.Width&quot;)] + coeff[3,1] * iris[,c(&quot;Petal.Length&quot;)] + coeff[4,1] * iris[,c(&quot;Petal.Width&quot;)] LD2 = coeff[1,2] * iris[,c(&quot;Sepal.Length&quot;)] + coeff[2,2] * iris[,c(&quot;Sepal.Width&quot;)] + coeff[3,2] * iris[,c(&quot;Petal.Length&quot;)] + coeff[4,2] * iris[,c(&quot;Petal.Width&quot;)] plot(NULL, xlim=range(LD1), ylim=range(LD2), ylab=&quot;LD2&quot;, xlab=&quot;LD1&quot;, main=&quot;Linear Discriminant Analysis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) col = rep(&quot;&quot;, length(LD1)) col[ which(iris[,c(&quot;Species&quot;) ] == &quot;setosa&quot;) ] = &quot;red&quot; col[ which(iris[,c(&quot;Species&quot;) ] == &quot;versicolor&quot;) ] = &quot;blue&quot; col[ which(iris[,c(&quot;Species&quot;) ] == &quot;virginica&quot;) ] = &quot;green&quot; points(LD1,LD2, pch=20, col=col) legend(-0.5, -5, c( &quot;Setosa&quot;, &quot;Versicolor&quot;, &quot;Virginica&quot; ), col=c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), horiz=FALSE, cex=0.8, lty=c(1,2,2)) Figure 9.56: Linear Discriminant Analysis Sixth, to validate, we use lda(.) from library MASS: library(MASS) (lda.model = lda(Species ~ ., data=iris, prior=c(1,1,1)/3, method=&quot;moment&quot;)) ## Call: ## lda(Species ~ ., data = iris, prior = c(1, 1, 1)/3, method = &quot;moment&quot;) ## ## Prior probabilities of groups: ## setosa versicolor virginica ## 0.3333 0.3333 0.3333 ## ## Group means: ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## setosa 5.006 3.428 1.462 0.246 ## versicolor 5.936 2.770 4.260 1.326 ## virginica 6.588 2.974 5.552 2.026 ## ## Coefficients of linear discriminants: ## LD1 LD2 ## Sepal.Length 0.8294 0.0241 ## Sepal.Width 1.5345 2.1645 ## Petal.Length -2.2012 -0.9319 ## Petal.Width -2.8105 2.8392 ## ## Proportion of trace: ## LD1 LD2 ## 0.9912 0.0088 The result gives us the Coefficients of the linear discriminants and the LD1 and LD2. The calculations of LD1 and LD2 are based on the Coefficients: coeff = lda.model$scaling LD1 = coeff[1,1] * iris[,c(&quot;Sepal.Length&quot;)] + coeff[2,1] * iris[,c(&quot;Sepal.Width&quot;)] + coeff[3,1] * iris[,c(&quot;Petal.Length&quot;)] + coeff[4,1] * iris[,c(&quot;Petal.Width&quot;)] LD2 = coeff[1,2] * iris[,c(&quot;Sepal.Length&quot;)] + coeff[2,2] * iris[,c(&quot;Sepal.Width&quot;)] + coeff[3,2] * iris[,c(&quot;Petal.Length&quot;)] + coeff[4,2] * iris[,c(&quot;Petal.Width&quot;)] We can then use the result to plot a 2D representation of our dataset (see Figure 9.57): plot(NULL, xlim=range(LD1), ylim=range(LD2), ylab=&quot;LD2&quot;, xlab=&quot;LD1&quot;, main=&quot;Linear Discriminant Analysis&quot;) grid(lty=3, col=&quot;lightgrey&quot;) col = rep(&quot;&quot;, length(LD1)) col[ which(iris[,c(&quot;Species&quot;) ] == &quot;setosa&quot;) ] = &quot;red&quot; col[ which(iris[,c(&quot;Species&quot;) ] == &quot;versicolor&quot;) ] = &quot;blue&quot; col[ which(iris[,c(&quot;Species&quot;) ] == &quot;virginica&quot;) ] = &quot;green&quot; points(LD1,LD2, pch=20, col=col) legend(-1, 9, c( &quot;Setosa&quot;, &quot;Versicolor&quot;, &quot;Virginica&quot; ), col=c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;), horiz=FALSE, cex=0.8, lty=c(1,2,2)) Figure 9.57: Linear Discriminant Analysis Note that LDA, as shown in the figure, is also used as a basic classifier. For non-linear discriminants, we leave readers to investigate Quadratic Discriminant Analysis (QDA). 9.6.5 Feature Construction Feature Construction is a method to create or filter features. It helps to discuss this in the context of pattern detection and pattern recognition, especially if working with images, texts, sounds, and signals. In this literature, we may also regard this Feature Engineering component as Feature Extraction by Construction. To illustrate, let us use the MNIST database made available by Y. LeCun, C. Cortes, and C. Burges, which contains over 60,000 grayscale images of handwritten digits. The database comes with four files with the following contents: a training set of images, a training set of labels, a test set of images, and a test set of labels. In our case, we use the training set to exemplify. First, we read the file that contains the digit labels. minst_file = paste0(dir, &quot;train-labels-idx1-ubyte&quot;) minst.handler = file( minst_file, &quot;rb&quot;) magic.number = readBin(minst.handler, integer(), n=1, endian=&quot;big&quot;) image.count = readBin(minst.handler, integer(), n=1, endian=&quot;big&quot;) image.labels = readBin(minst.handler, integer(), size=1, n= image.count, endian=&quot;big&quot;) close(minst.handler) c(&quot;Magic Number&quot; = magic.number, &quot;Number of Images&quot; = image.count) ## Magic Number Number of Images ## 2049 60000 We display the first 20 digit labels: image.labels[1:20] ## [1] 5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9 Because the digit images are collected randomly, let us find the first occurrence of digits 0 through 9 in ascending order. digits = seq(0, 9) ( indices.of.1st.occurrences = match(digits, image.labels) ) ## [1] 2 4 6 8 3 1 14 16 18 5 Second, the number of digit images and the constant size of the images are encoded into the first few bytes of the MNIST database. Therefore, we need to extract the information as so: minst_file = paste0(dir, &quot;train-images-idx3-ubyte&quot;) minst.handler = file( minst_file, &quot;rb&quot;) # Get metadata ( number of images, rows, columns) magic.number = readBin(minst.handler, &#39;integer&#39;, n = 1, size = 4, endian = &#39;big&#39;) image.count = readBin(minst.handler, &#39;integer&#39;, n = 1, size=4, endian=&quot;big&quot;) image.rows = readBin(minst.handler, &#39;integer&#39;, n = 1, size=4, endian=&quot;big&quot;) image.columns = readBin(minst.handler, &#39;integer&#39;, n = 1, size=4, endian=&quot;big&quot;) c( &quot;Number of Images&quot; = image.count, &quot;Number of Rows&quot; = image.rows, &quot;Number of Columns&quot; = image.columns) ## Number of Images Number of Rows Number of Columns ## 60000 28 28 Third, for our purpose, we read only the first 25 28x28 digit images and filter the first occurrence of digits 0 through 9 into a list structure: images = list() for (idx in 1:25) { img.hex.vector = readBin(minst.handler, what = &quot;raw&quot;, n = 784 , endian=&quot;big&quot;) if (idx %in% indices.of.1st.occurrences) { img.dec.vector = as.integer(img.hex.vector ) images[[idx]] = matrix(img.dec.vector, nrow=28, ncol=28) } } close(minst.handler) Now, herein lies the construction of features. We intentionally show 25 iterations, each reading 784 bytes and storing them into a 784-length vector. The entire 784 bytes of data represent one image of a digit. In other words, each byte is a feature. The representation of a digit depends upon the value of each byte. Therefore, it becomes more apparent to store the vector in a 28x28 matrix for visualization. Each cell in a matrix represents a grayscale code between 0-255 (bytesize) that is mapped to one pixel. However, for classification, a vector of features is more convenient for computation. Certain image perturbations (such as thinning, swelling, thickening, and fractures) of each digit are then evaluated for feature matching (for whatever intended purpose). Fourth, for visualization, we use a 3rd-party R package called imager and use its built-in function as.cimg(.) to convert the image list into cimage format so that we can plot the actual image (see Figure 9.58). Note that each image in the figure represents 784 features. library(imager) digits = c() for (n in digits.index) { digits = rbind(digits, rep(255, 28), images[[n]]) } plot(as.cimg(digits), axes=FALSE) Figure 9.58: MNIST digits Other types of datasets, such as the one used by Natural Language Processing (NLP), use word embeddings generated by tools such as word2vec using CBOW and Skip-gram. We defer this topic towards the end around Attention and Deep Learning. Gesture and Speech Recognition are two other areas that demand different representations and interpretations of their datasets and corresponding features. For example, Speech Recognition uses spectrogram. Datasets that do not fall under the category of images, sound, signals, or texts have simpler features. For example, the dataset mtcars deals with tuples that are transactional and uses the following simpler features (e.g., cyl, disp, hp, drt, wt, asec, and so on ): options(width=70) attributes(mtcars)$names ## [1] &quot;mpg&quot; &quot;cyl&quot; &quot;disp&quot; &quot;hp&quot; &quot;drat&quot; &quot;wt&quot; &quot;qsec&quot; &quot;vs&quot; &quot;am&quot; ## [10] &quot;gear&quot; &quot;carb&quot; As we have shown, Feature Construction demands some expert knowledge in a specific domain. We need to understand the structure and mechanics involved in translating and converting inputs to their corresponding interpretable representation for a specific domain to construct features. In the next section, we expand the knowledge of Feature Engineering from the concept of construction to the concept of selection. 9.6.6 Feature Selection Feature Selection is a method of selecting salient features based on the strength of relevance. We favor relevantly strong features over those that are relevantly weak. There are three common categories: Filter-based: This category selects features based on chosen scoring and ranking system. Scoring (or rating) and ranking may come in many shapes and flavors. Data is given a score and ordered in rank, after which a recommendation is carried out. We favor top-ranked features over low-ranks in the case of Filter-based feature selection. The computation of the result is based upon which result is required. Therefore, we can use a combination of the following measures to compare the outcome. Some measures are already discussed in the Distance Metrics section. Others are discussed in Chapter 8 (Bayesian Computation II) under the Information Theory Section. Precision Support Distance Recall Error Rate Loss vs Gain Sensitivity Accuracy Entropy (Mutual Information) Specificity Similarity Correlation (Collinearity) For selection, one common comparison of features is using Correlation which we cover under linear regression analysis in Chapter 6 (Statistical Computation). We also extend the concept of Correlation or Collinearity under the Exploratory Data Analysis section. Additionally, we have a long discussion on ANOVA, Chi-Square, and RMSE in Chapter 6. More importantly, it helps to understand Statistical Interaction under the Linear Regression Modeling section in Chapter 6. The idea is to evaluate the additive and interactive strengths of features when combined, along with the interaction of dummy or indicator variables. Here, we use AUC on ROC as one of the many tools that can be used as a scoring mechanism for Filter-based Feature Selection. We use an actual ML training set based on using one of the five folds (5 randomly generated samples from the BreastCancer dataset from mlbench library). library(caret) library(mlbench) data(BreastCancer) BC = na.omit( BreastCancer ) # 5-fold (creates 5 groups) fold.indices = createFolds(BC$Class, k = 5, returnTrain = FALSE) # choose the first fold for our test group. test = BC[fold.indices$Fold1,] # choose the other folds for training group. train = BC[-fold.indices$Fold1,] # preserve test label for comparison later test.class = test$Class test$Class = NULL We have the following Training set attributes: options(width=70) attributes(train)$names ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; ## [4] &quot;Cell.shape&quot; &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; ## [7] &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; &quot;Normal.nucleoli&quot; ## [10] &quot;Mitoses&quot; &quot;Class&quot; We also have the Test set attributes (omitting the Label or Class): options(width=70) attributes(test)$names ## [1] &quot;Id&quot; &quot;Cl.thickness&quot; &quot;Cell.size&quot; ## [4] &quot;Cell.shape&quot; &quot;Marg.adhesion&quot; &quot;Epith.c.size&quot; ## [7] &quot;Bare.nuclei&quot; &quot;Bl.cromatin&quot; &quot;Normal.nucleoli&quot; ## [10] &quot;Mitoses&quot; Then we use a randomForest algorithm to model the fit with the multinomial dataset (in this case, we are using only the binomial dataset). library(randomForest) # Random Forest Model model.rf &lt;- randomForest(Class ~ ., data = train, importance=TRUE, ntree=3) Finally, we make a prediction and generate the AUC score using the performance(.) function from the ROCR library. For the TEST set, we use the following R code: library(ROCR) # Calculate AUC for test set test.pred = stats::predict(model.rf, type=&quot;prob&quot;,newdata = test)[,2] test.pred = prediction(test.pred, test.class) (test.auc = performance(test.pred, measure = &quot;auc&quot;)@y.values[[1]] ) ## [1] 0.9631 We calculate the performance of prediction (in the context of predicting the class) by comparing its Sensitivity and Specificity corresponding to their true-positive rates and false-positive rates. See Figure 9.59. Figure 9.59: Prediction Performance We also calculate the accuracy of the prediction. See Figure 9.60. Figure 9.60: Prediction Performance The AUC score and accuracy show that using the training set for the Random Forest algorithm makes a strong prediction. Here, our training and test sets contain relevant features. One has to manually mix and match the different combinations of the features to rank predictions and select the combination with higher AUC and Accuracy scores than other combinations. In a previous section, we introduce the concept of AUC on ROC with details on comparing AUC of each feature (or their corresponding predictions). It helps to review the section on how we did a simple feature selection using AUC on ROC. Lastly, we use dataset iris as an example to illustrate Feature Selection. Here, we use Random Forest to model our classification and use a few measures of importance such as MSE and Gini Index. options(width=70) library(randomForest) library(caret) data(iris) model.rf = randomForest(Species ~ ., data = iris, importance = TRUE ) pred.rf = stats::predict(model.rf) confusionMatrix(pred.rf, iris$Species) ## Confusion Matrix and Statistics ## ## Reference ## Prediction setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 3 ## virginica 0 3 47 ## ## Overall Statistics ## ## Accuracy : 0.96 ## 95% CI : (0.915, 0.985) ## No Information Rate : 0.333 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.94 ## ## Mcnemar&#39;s Test P-Value : NA ## ## Statistics by Class: ## ## Class: setosa Class: versicolor Class: virginica ## Sensitivity 1.000 0.940 0.940 ## Specificity 1.000 0.970 0.970 ## Pos Pred Value 1.000 0.940 0.940 ## Neg Pred Value 1.000 0.970 0.970 ## Prevalence 0.333 0.333 0.333 ## Detection Rate 0.333 0.313 0.313 ## Detection Prevalence 0.333 0.333 0.333 ## Balanced Accuracy 1.000 0.955 0.955 To extract importance of features from random Forest, we use a function called varImpPlot(.) and rank features based on Mean Decrease Accuracy and Mean Decrease Gini. See Figure 9.61. Figure 9.61: Variable Importance Mean Decrease in Accuracy (MDA), also called Percent increase in MSE, as measured by Mean Squared Error (MSE), ranks features based on how worst if variable is present. For example, Petal.width is important (or significant), Sepal.Width is worst in importance. Equivalently, Mean Decrease in Gini (MDG) ranks features based on how worst if variable is not present. For example, Petal.width is important, Sepal.Width is worst. The importance(.) function shows the importance of features. importance.model = importance(model.rf) colnames(importance.model) = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;, &quot;MDAccuracy&quot;, &quot;MDGini&quot;) importance.model ## setosa versicolor virginica MDAccuracy MDGini ## Sepal.Length 6.169 8.087 7.862 10.78 9.557 ## Sepal.Width 4.619 1.322 4.584 5.16 2.299 ## Petal.Length 20.785 32.482 27.886 32.89 42.613 ## Petal.Width 23.520 32.978 30.729 33.69 44.785 The varUsed(.) function shows how frequently each variable is used. Notice below an imbalance of feature usage. varUsed(model.rf) ## [1] 786 567 1255 1222 Both MDA and MDG are farther discussed under Decision Trees subsection in Regression and Classification sections respectively. Wrapper-based: This category starts with a subset of features evaluated against a model based on performance measures. In Chapter 6 (Statistical Computation), we have discussed Aikike Information Criterion (AIC) and Bayesian Information Criterion (BIC) in great detail. We encourage readers to review the algorithm in that Chapter and carefully review the forward, backward, and exhaustive steps and bidirectional steps. While the topic focuses on model selection, the underlying concept and method altogether apply to feature selection in that the best model chosen is the result of mixing and matching different combinations of features based on the AIC score. Below is a summarized outcome of AIC-based scoring. We start with a formula that includes the following features: disp, hp, drat, wt, qsec. Then, after performing a bidirectional step for our Feature Selection, the final formula lists a reduced number of features, namely drat, wt, qsec. initial.model = lm(mpg ~ disp + hp + drat + wt + qsec , data = mtcars) (selected.model = step(initial.model, direction = &quot;both&quot;, k=2, trace = FALSE)) ## ## Call: ## lm(formula = mpg ~ drat + wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) drat wt qsec ## 11.394 1.656 -4.398 0.946 Given our formula’s initial number of features, we start with a high AIC score of 65.4663, After the bidirectional AIC-based selection, the above formula with a reduced list of features gives us the lowest AIC score of 63.8911. See below: selected.model$anova$AIC ## [1] 65.47 64.21 63.89 Embedded-based: The feature selection in this category is part of model fitting. There is no need to subset features or to score features. Features are instead rewarded or penalized. That involves tuning coefficients, which gives weights to features - we call this Regularization. A detailed discussion of Regularization is discussed in a few sections ahead when we cover General Modeling in which we introduce Ridge, Lasso, and Elasticnet Regularization. The idea is that features are weighted and dropped from the list of relevant features as their weights reach zero. The discussion of Embedded-based feature selection is covered primarily in Chapter 13 (Computational Deep Learning II). 9.6.7 Feature Transformation Feature Transformation is a method to improve the interpretability of features by transforming features into a somewhat more interpretable synthetic form. In this literature, we regard this Feature Engineering component as Feature Extraction by Transformation. Among many other transformations, we give about six transformations that may help us to improve interpretability. In fact, a few of them are derived from Econometrics dealing with Pooled Models and Panel Models, including homogeneity and heterogeneity of observations which we introduced in Chapter 6 (Statistical Computation). We wrap the models around the context of Feature Transformation in a sense because certain raw data can be merged cross-sectionally or cross-sequentially and arrived at a newly formed synthetic set of features. Synthesized features may undergo different modeling altogether. That brings us to models such as Pooled OLS model, Fixed Effect Model (FEM), and Random Effect Model (REM), which we introduce under the Time-Series Forecasting section in Chapter 11 (Computational Learning III). One Hot Encoding: This transformation allows the creation of a new separate feature for each category in a feature expressed in the form of a one-hot vector. We have covered One Hot encoding in detail in the Exploratory Data Analysis section. Bucketize Feature Columns: This transformation allows assigning feature values into corresponding buckets. That is almost equivalent to binning; however, instead of discretizing numerical values into discrete values, we put values (whether numerical or discrete) into bounded buckets - or a set of ranges. For example, one good feature to extract from a date feature and then bucketize is the year. Our example below shows that essential events are categorized into two buckets based on dates representing a decade between 2001-2010 and another decade between 2011-2020 (note that the events are fictitious except for the COVID-19 pandemic). Here, a new feature called Decade is added. Table 9.35: Bucketize Feature (Yearly) Important Event Date Decade Discovery of Time Machine 01-Jan-2009 2001-2010 First Encounter of the 4th kind 23-Feb-2010 2001-2010 Covid Pandemic 11-Mar-2020 2011-2020 Crossed Feature Columns: This transformation allows multiple features to be combined into one feature. Each feature crosses the other to form one new feature. That is different from correlation in which we drop duplicates. That is also different from interactive of multiple features as discussed in Statistical Computation in which we multiply the weights of two features. An example of a crossed feature is when combining longitude and latitude to develop a new feature called global position. The value of the new feature can be represented by hashing the longitude and latitude together as an example, and then perhaps implement bucketize feature transformation to further process the feature into buckets - such as into regions, zones, or hemispheres. Cross-Sectional vs Longitudinal Columns: Both Cross-Sectional Feature Columns (CSFC) and Longitudinal Feature Columns (LFC), also called Panel Feature Columns (PFC), do extract data from time-series data. CSFC differs from LFC in that CSFC analyzes subjects of varying categories at only one moment (a snapshot), whereas LFC analyzes subjects in the same category across the varying period. See Figure 9.62. For cross-sectional analysis, the data of interest contains subjects under age brackets 20, 30, 40, 50, and 60 in 2016. Moreover, for longitudinal analysis, the data of interest contains subjects under Cohorts D across the years 2015, 2016, 2017, and 2018. Figure 9.62: Cross-Sectional, Longitudinal, Cross-Sequential Cross-Sequential Feature Columns: Cross-Sequential Features combines both Cross-Sectional and Longitudinal features in that, for example, subjects under age 50 across Cohorts A through D is being analyzed across the year 2015, 2016, 2017, 2018. We may also perform the same analysis for subjects under age 40 and so on. Cross-Sectional Windowing: Now, in terms of transformation, the tables in Figure 9.62 are assumed to be coming from time-series data. The transformation of CSFC from time-series data into another synthetic form relies on Windowing. As an example, in Figure 9.63, if we are to account for seasonality on monthly sales, perhaps we can stretch an overlapping 4-month window that covers Holiday sales and see if sales are higher than non-Holiday sales. Figure 9.63: Windowing (Cross-Sectional) Pooled Data Model vs Panel Data Model: Pooled Model and Panel Model are terms that can be interchanged with Cross-Sectional Model and Longitudinal Model. In concept, Pooled model refers to sampling different subjects of the same population that are independently collected at different time intervals and pooled together, whereas Panel model refers to sampling the same set of subjects collected at different times intervals. One reason for analyzing the panel dataset is to determine resiliency and sustainability. 9.6.8 Model Specification In Chapter 6 (Statistical Computation), we introduce the concept of Statistical Model Specification, or Model Specification in general. In specifying a model, we need to be able to describe and define the relationships of observations and features and what they represent. Not having a correct specification may result in an imbalance of bias and variance. In fitting a poorly specified model, we may consequently obtain inaccurate results. We can say that inaccurate results can either be underspecified or overspecified. To mitigate such situations, part of the goal of feature engineering as discussed in the previous section and which we discuss next in subsequent sections around model building is to understand how different machine learning algorithms introduce regularization as a way to measure the importance of observations and features. The existence of a feature that may be deemed not necessary or relevant may create a misrepresentation of our model. Similarly, a missing feature may create data shift or data perturbation - observations that are not accounted for yet necessary or relevant. In the context of anomalies and outliers, it helps to identify, distinguish and describe the existence of such observations - such rare phenomena - and specify their requirements, representations, and relationships. 9.7 General Modeling This section introduces a few important operations behind Computational learning; in particular, we cover the Learning, Testing, and Validation operations. 9.7.1 Training (Learning) Training a model starts with a dataset - we call this fitting the model to data. The model can be a linear model such that we fit a line through the data points. In Machine Learning (ML), it is common to split a dataset into subsets and use a portion of the split for training while the rest are held out for validation and test. The portion of the split for training is called training set. We use the training set to train or fit a model. An example of fitting a model is illustrated in Ordinary Least Square (OLS). Geometrically, we randomly place a line through a random set of data points and adjust the line until it meets the minimum computed Least-Squares - a measure for the goodness of fit. The fitted line is a geometric representation of an ML model in the form of the following equation - this accounts for noise denoted by the symbol epsilon \\(\\epsilon\\), which also stands for error: \\[\\begin{align} h_{\\theta}(X) = \\hat{f}(x) = \\sum_{i=1}^n \\theta x_i + \\epsilon, \\end{align}\\] where h(.) stands for hypothetical value - the \\(\\mathbf{yhat}\\ (\\hat{y})\\) and the \\(\\theta\\) symbol stands for coefficient - the weight multiplied to features. Without noise, the data points align perfectly well such that a line can go through all of them. Such a case can then be expressed as a linear expression like so (in its most simple linear form): \\[\\begin{align} y = mx + b \\end{align}\\] However, an ML model does not just consist of a linear expression. The linear expression above comes with two model parameters: m for slope and b for intercept. We also treat these parameters as weights or coefficients - in some cases, it is denoted by the symbol theta \\(\\theta\\). When we say fit the line or fit the model or train the model, we mean to use the linear expression while adjusting the weights. For every adjustment of the weights, we measure and take note of all the distances of the data points to the fitted line - such distance is called residual. The one adjustment that renders the least square distance effectively determines the final ML model. For example: \\[\\begin{align} \\epsilon = y - \\hat{y}\\ \\ \\rightarrow \\ \\ \\ MSE = \\frac{1}{n}\\sum \\epsilon^2 \\end{align}\\] The final ML model in terms of linear regression comes with the following sufficient ML artifacts (other implementation of models come with additional supplementary artifacts, e.g. for ML model provenance): The linear equation, e.g. \\(\\hat{y} = \\beta_0 + \\beta_1 x\\) The coefficient values, e.g. \\(\\beta_0 = 30, \\beta_1 = 50\\) The training set: \\(\\{x, y\\}_{i=1}^n\\) The features: x The fitted values: (\\(\\hat{y}\\)) The residual: \\(\\epsilon\\) Note that by running the following R code below, a linear model in R using the function lm(.) captures the following content (preserved in a list structure): coefficients, residuals, effects, rank, fitted.values, qr, df.residual, xlevels, call, formula, terms, and others. # Using mtcars dataset to illustrate linear modeling my.model = lm(formula = mpg ~ cyl + am, data = mtcars) saveRDS(my.model, &quot;./my_model.rds&quot;) # save our model my.model = readRDS(&quot;./my_model.rds&quot;) # retrieve our model names(my.model) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; Our definition of a model should be all-encompassing to include all necessary ingredients to reconstruct our model. The list of artifacts above exemplifies that. However, Chapter 14 (Distributed Computation) covers Open Standards, which describe standard model specifications that guide how models can be portable and sharable. At the very least, a model should meet the minimum required content for inferencing. 9.7.2 Validation (Tuning) In fitting a model, it is not unusual to encounter overfitting. That means the model is too tailored to fit the training set perfectly well but loses generality; thus, it cannot be used for new unseen data. If this happens, additional adjustment is needed to the model to minimize the overfit. This adjustment is called tuning the model. Tuning is essentially needed for optimization. Primarily, we tune hyperparameters. A combination of cross-validation and calibration of hyperparameters helps improve the performance of our models. In two separate sections a little later ahead, we discuss more of cross-validation and tuning by using regularization techniques. Now, one way to overcome overfitting is to use cross-validation. The idea is to split the dataset into k sets (see Figure 9.64). We then perform model fitting by holding out the first set and using the rest of the sets for training. After which, we perform another model fitting by holding out the second set and using the rest of the sets. We continue to do so iteratively until completing several iterations, e.g., five iterations. Figure 9.64: 5-Fold Cross-Validation After the last iteration, we then analyze the result. We choose the model that renders the best result. In this case, for linear regression, we choose one with least error using \\(\\mathbf{\\text{RMSE}, \\text{R}^2,}\\text{ and }\\mathbf{\\text{MAE}}\\). 9.7.3 Testing (Assessing) After a model is trained, our next goal is to assess the performance (e.g., predictive power) of the model. Here, we treat the holdout set as a testing set, which contains a list of predictive variables only - the features. Unlike the training set, we do not account for the response variable in the testing set. Ideally, the values in the response variables in the testing set are preserved as ground truth for later evaluation. For example, we have covered Auc on ROC under the Feature Selection section as a method we can use to compare the predicted result of our test over the actual values. In terms of the proportionality of our testing set over training set, it may be safe to assume that there is no specific hard rule in the manner in which we split our dataset, but it seems sensical to assume (in a general sense, but not always) that smaller datasets may not have to be split into many folds. A simple 2-fold split helps when the first fold becomes the training set and the second fold becomes a holdout set. The proportion may be arbitrary depending on necessity, e.g., training set may accommodate 75% of the dataset, and 25% goes into a holdout set. The question now comes when we use the holdout set. If the set is used to assess our model’s performance (e.g., predictive power) and shows high results (e.g., high accuracy or low error), it may not necessarily signify a good thing. Overfitting is a good example. To ensure that accuracy holds, we perform multiple tests using cross-validation. Below is an example of splitting the dataset BreastCancer in 5-fold using createFolds(.) function. require(caret) require(mlbench) n=80 X = seq(0, 1, length.out= n) # note that the output is the location # (index of corresponding data points) (fold.indices = createFolds(X, k = 5, returnTrain=FALSE)) ## $Fold1 ## [1] 2 8 11 14 22 24 25 40 44 45 51 59 62 65 76 78 ## ## $Fold2 ## [1] 4 7 9 10 21 26 29 37 46 49 52 60 67 70 72 75 ## ## $Fold3 ## [1] 1 3 15 16 23 28 33 38 41 48 56 57 61 63 64 79 ## ## $Fold4 ## [1] 5 6 13 18 30 31 35 36 47 53 55 58 68 69 77 80 ## ## $Fold5 ## [1] 12 17 19 20 27 32 34 39 42 43 50 54 66 71 73 74 Given the 5-folds, we can use the 1st fold for our test set and the rest of the folds for our training set like so: # choose the first fold for our holdout set. holdout.set = X[fold.indices$Fold1] # choose the other folds for our training set training.set = X[-fold.indices$Fold1] c(&quot;HoldOut&quot;=length(holdout.set), &quot;Training&quot;=length(training.set)) ## HoldOut Training ## 16 64 9.7.4 Cross-Validation (CV) Let us further extend the concept of Cross-Validation (CV). There are a few other reasons why we need CV of which we mention the following three reasons: Model Selection and Tuning Hyperparameter Selection and Tuning Algorithm Selection and Tuning Here, we discuss two CV techniques. The first CV technique is called the K-fold Cross-Validation. We split the dataset into k-folds. See Figure 9.64 which shows a 5-fold split. There is no hard rule when choosing k for splitting the dataset. However, care must be considered to avoid data leakage - a topic introduced in the EDA section. It is easy to contaminate our dataset, especially when using cross-validation. This happens if the algorithms and hyperparameters are not well-tuned. To tune our algorithm and hyperparameter and select the best combination, it is ideal for performing a repeated K-fold CV. For example, Figure 9.65 shows a 1-iteration evaluation of five different attempts to tune a model, e.g., given some set of tunable parameters. In practice, we can use multiple iterations across several different configurations (e.g., one combination of a chosen model and hyperparameters). Figure 9.65: Repeated 5-Fold Cross-Validation (Model Selection) On the other hand, we are not limited to only one algorithm for evaluation. Figure 9.66 illustrates an example of evaluating the performance of different algorithms using a repeated K-fold CV. Figure 9.66: Repeated 5-Fold Cross-Validation (Algorithm Selection) The second CV technique is called the Leave One Out Cross Validation (LOOCV). It is also referred to as the Jackknife approach. The idea is simply to hold out a data point and use the rest of the dataset for training. Recall in linear regression how we geometrically fit a line through data points. Such a line is fitted by adjusting its slope and intercept. To derive a good fit, we use the Jackknife approach in which we perform the following steps: Using the entire dataset, we compute the initial values of the slope and intercept. We then hold out one data point and use the rest of the dataset to compute the slope and intercept values. We repeat step 2 by holding out the next data point until all the data points have taken turns being held out. Compute the average of all the computed slopes and intercepts. Such an approach aims to balance bias and variance. 9.7.5 Bias and Variance Let us use a popular diagram used in discussing bias and variance. See Figure 9.67. Figure 9.67: Variance and Bias In fitting a model, we rely on measuring the error of our fit against some true value. A breakdown (or decomposition) of the error is essential in understanding how we may deal with underfitting or overfitting a model. To compute for the total error (e.g., MSE), we use the following decomposition: \\[\\begin{align} \\text{Total Error}(x) = Bias^2 + Variance + \\text{Irreducible Error} \\end{align}\\] Bias and Variance are derived from the following equations: \\[\\begin{align} Bias^2 = \\left[\\mathbb{E}[\\hat{f}(x)] - f(x) \\right]^2 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ Variance = \\mathbb{E}\\left[ \\left(\\hat{f}(x) - \\mathbb{E}[\\hat{f}(x)]\\right)^2\\right] \\end{align}\\] where: \\[ \\underbrace{\\hat{y} = \\hat{f}(x)}_{ \\begin{array}{l} \\text{Estimated Response} \\\\ \\text{Learned from X} \\end{array} } \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{y = {f}(x) }_{\\text{Actual Response}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{Irreducible Error} = \\sigma^2_e}_{Noise} \\] Let us design a simple dataset and create a 3-fold dataset to illustrate bias and variance. require(caret) set.seed(2019) n = 10; k = 3 f &lt;- function(X) { sqrt(X) } X = seq(0, 1, length.out= n * k) E = rnorm(n = n * k, 0, 0.1) # Irreducible Error Y = f(X) # Actual Values without Noise Y.observed = f(X) + E # Observed Values with Noise fold.indices = createFolds(X, k = k, returnTrain=FALSE) Given the 3 folds, we have the plot in Figure 9.68: require(ModelMetrics) Y.bias = rep(0, k) Y.var = rep(0, k) Y.mse = rep(0, k) plot(NULL, xlim=range(0,1), ylim=range(0,1.3), xlab=&quot;Predictor&quot;, ylab=&quot;Response&quot;, main=&quot;Cross Validation&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) for (j in 1:k) { indices = fold.indices[[j]] # choose the j-fold for our holdout set. X.holdout.set = X[indices]; Y.holdout.set = Y.observed[indices] # choose the other folds for our training set X.training.set = X[-indices]; Y.training.set = Y.observed[-indices] # fit a linear model Y.model = lm(Y.training.set ~ X.training.set) Y.predicted = predict.lm(Y.model, newdata=data.frame(X.training.set = X.holdout.set)) Y.bias[j] = mean( ( mean(Y.predicted) - Y.holdout.set )^2 ) Y.var[j] = sum( (Y.predicted - mean(Y.predicted))^2 ) * 1/(n-1) Y.mse[j] = mean((Y.predicted - Y.holdout.set)^2) points(X.holdout.set, Y.holdout.set, col=j) lines(X.training.set, Y.model$fitted.values, col=j) } legend(&quot;bottomright&quot;, legend=c( &quot;Train 1&quot;, &quot;Train 2&quot;, &quot;Train 3&quot;), col=seq(1,k), pch=c(1), cex=0.8) Figure 9.68: Cross Validation There are situations in which we find that no matter how good we fit our model, we cannot reduce the error. This so-called irreducible error could indicate a dataset with a certain amount of noise - some perturbation that could be the norm in a system. Perhaps we may acknowledge that additional data processing, adjustment, or transformation may be required before fitting a model - all that with one thing in mind. That is to do our best to improve our model’s goodness of fit by balancing bias and variance - a trade-off. That is only as long as we do not fall into underfitting or overfitting conditions. (measure = data.frame( Train = seq(1,k), bias = Y.bias, variance = Y.var, mse = Y.mse)) ## Train bias variance mse ## 1 1 0.05536 0.07679 0.02174 ## 2 2 0.08065 0.08024 0.01423 ## 3 3 0.10337 0.05376 0.01895 When it comes to losing generality, there is a correlation between generalizability and variance. If the dataset deviates or varies a lot, we may still be able to come up with a model that can predict. Otherwise, perhaps we can split the dataset into groups and use different modeling algorithms for each group such that when we merge the models into an aggregated model - an ensemble model, it will be able to predict. In a linear regression, if we start to fit a line in such a manner that the line starts to curve or adjust towards every data point (basically introducing a higher degree of freedom), then it can be guaranteed that our model will just not fit any new unseen data. The model loses generalizability. See Figure 9.69. fit = function(x) { x^2 } set.seed(2020) N = 40 x = seq(-1, 1, length.out=N) e = rnorm(n=N, mean=0, sd=0.1) # Noise y = fit(x) # f(x) &lt;- Actual Response y.hat = y + e plot(NULL, xlim=range(-1,1), ylim=range(0,1), xlab=&quot;Predictor&quot;, ylab=&quot;Response&quot;, main=&quot;Goodness of Fit&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0.5, col=&quot;green&quot;, lwd=2) lines(x, y, lwd=2, col=&quot;navyblue&quot;) lines(lowess(x, y.hat, f=0.02), lwd=1, lty=1, col=&quot;darksalmon&quot;) points(x, y.hat, col=&quot;darksalmon&quot;, pch=20) ### Legend legend(-0.2, 1, legend=c( &quot;Fitted Line (Optimal)&quot;, &quot;Underfit (High Bias)&quot;, &quot;Overfit (High Variance)&quot;), col=c(&quot;navyblue&quot;, &quot;green&quot;, &quot;darksalmon&quot;), lty=c(1,1,1), lwd=2, cex=0.8) Figure 9.69: Goodness of Fit Similarly, in a case where a decision tree has too many branches or as the tree depth increases, there is a tendency to overfit since every leaf tends toward every data point. Once again, this indicates that the tree model will not fit any new (unseen) set of data points. We continue to discuss Overfitting in subsequent chapters. We will introduce hyperparameters and network layers starting with Chapter 12 (Computational Deep Learning I). The more we begin to tailor the network against many layers where each layer filters toward every data point, the more we overfit. 9.7.6 Loss and Cost Functions When we calculate our estimates, intuitively, we quantify the error in our estimates and adjust our operations as necessary. Additionally, we define functions that allow us to measure the difference of our estimates based on error, adjust as necessary, and numerically determine how much of an error we can tolerate. Loss (Error) function Loss function measures the difference between our estimated and actual values. The larger the difference, the larger the error. We use an error function to compute our error. Squared Error Loss (e.g. SSE, MSE, MAE) and Huber Loss are examples of Loss functions for regression. A general formula for a Loss function in regression is expressed as such: \\[\\begin{align} \\underbrace{Lik(\\theta) = \\sum(y - \\hat{y})^2}_{\\text{squared error loss}} \\end{align}\\] Note that classification problems introduce other forms of measures for Loss functions based on Information Theory and Bayesian Theory. Examples of measures are the Gini Index, Cross-Entropy, and Information Gain, which are used for tree classifications, Logit Loss for binary classification, and Hinge Loss for SVM. We cover this in the Classification section later. Below are examples of Hinge Loss function and Logit Loss function respectively: \\[\\begin{align} \\underbrace{Lik(\\theta) = max\\left\\{0, 1 - f(x)\\right\\}}_{\\text{hinge loss}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{Lik(\\theta) = max\\left\\{1 + exp(- f(x))\\right\\}}_{\\text{log loss}} \\end{align}\\] Objective (Cost) function Objective function measures the Cost of fitting a model to data. Intuitively, our goal is to reduce the Cost. In this respect, it is understandable that we determine our tolerance level - how much Cost is acceptable. Cost is a function of the loss that we incur and the complexity of our model to fit. If our model is too complex, there is a chance to overfit. As shown in the previous section, we can use Lasso or Ridge regularization for linear regression to balance complexity. A general formula for the objective function is expressed as such: \\[\\begin{align} J(\\theta) = \\theta^{\\text{*}} = \\text{arg}\\ \\underset{\\theta}{\\text{min}}\\ Lik(\\theta) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ J(\\theta)_R = \\theta^{\\text{*}}_R = \\underbrace{ \\text{arg}\\ \\underset{\\theta}{\\text{min}} \\left\\{Lik(\\theta) + \\lambda(\\theta)\\right\\} }_{\\text{with regularization}} \\end{align}\\] where \\(\\lambda\\) is a regularization function Stopping Criterion function We use an objective function to find the minimum error. However, we might wonder why we have to find a minimum error when we can go straight to zero error, meaning we just get the estimated value equal to the actual value. Indeed, that would be ideal if we knew the actual value. Unfortunately, in some cases, we do not have the actual value. All we have are data points, and the most basic method we can use is to average the data points and estimate the actual value based on the average. However, we know that average will not always give accurate results when our data points are inherently skewed. In this sense, we need better methods to find a minimum error that is optimal enough to describe the accuracy of our estimate. That said, either we keep looking for that optimal minimum error, or at some point, we have to stop. Furthermore, this is where we need to determine the exit criterion to stop? We use a stopping criterion function to determine our stopping point - which we also term our tolerance level. Note that the three general functions ( loss, cost, stopping criterion) are commonly used and emphasized in Machine Learning. Most algorithms are readily available in many languages and are implemented in different ways. Here, we use a library called caret to illustrate two functions, namely trainControl(.) and train(.). The former is used to control cross-validation training. The latter is used to train a model by injecting a selection of algorithms and corresponding hyperparameter settings. The trainControl(.) function allows us to set parameters that control how we train our model. Such parameters include the validation method, number of splits to a dataset, and others. library(caret) tr.control = caret::trainControl(method = &quot;cv&quot;, number = 5) We then plug the control parameters to train(.). The function allows us to set the algorithm to use for the training such as general linear model (glm), gradient boosting machine (gbm), regression forest (rf), etc. The tr.tuneGrid is a configuration of regularization choices and a selection of lambdas as discussed in previous chapter. We then use train(.) to train our model with centering and scaling for pre-processing. Notice the use of expand.grid(.). That allows multiple hyperparameters to produce different unique combinations. In our case, we use alpha and lambda. We then train the model and determine which combination renders the optimal fit. library(glmnet) ## Loaded glmnet 3.0-2 set.seed(2020) predict = stats::predict tr.tune = expand.grid(alpha = c(0,0.5,1), lambda = c(0,0.5,1)) (tr.model = caret::train(mpg ~ wt + hp + cyl, data=mtcars, preProcess = c(&quot;scale&quot;, &quot;center&quot; ), method=&quot;glmnet&quot;, tuneGrid = tr.tune, trControl = tr.control)) ## glmnet ## ## 32 samples ## 3 predictor ## ## Pre-processing: scaled (3), centered (3) ## Resampling: Cross-Validated (5 fold) ## Summary of sample sizes: 25, 27, 25, 27, 24 ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared MAE ## 0.0 0.0 2.599 0.8681 2.164 ## 0.0 0.5 2.600 0.8682 2.165 ## 0.0 1.0 2.614 0.8692 2.168 ## 0.5 0.0 2.619 0.8650 2.192 ## 0.5 0.5 2.625 0.8659 2.198 ## 0.5 1.0 2.670 0.8662 2.213 ## 1.0 0.0 2.621 0.8647 2.195 ## 1.0 0.5 2.670 0.8618 2.236 ## 1.0 1.0 2.808 0.8580 2.326 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0 and lambda = 0. The output shows an optimal model based on the lowest RMSE (e.g., the choice of performance metric) using ridge regression (alpha=0.0) and lambda=0.0. We can then use the model for prediction. tr.predict = stats::predict(tr.model, newx = x) 9.7.7 Global and Local Minima Now, in the context of a Loss function, when we deal with data, we need to perform the estimation in most cases. We try our best to predict an estimated value of our data close to an actual value. Because we are estimating, our result will come with some inaccuracies or errors. Intuitively, especially in cases where we cannot afford errors, we need to minimize these errors. Errors can be costly and so when we say minimize error, we may also be minimizing cost. We will be covering Error functions and Loss functions in later chapters. However, for now, let us use Figure 9.70 to talk about the basic concepts of Global and Local Minima. The local minima is at \\(x = 1\\). Figure 9.70: Global and Local Minima In training a good model, we notice that the result of our Loss function for every iteration during training begins to follow a convex curve when charted. We hope to find the minimum point somewhere along the convex curve - that is our minimum error. However, there may be cases when the curve does not necessarily form a single convex curve. It may form multiple convex shapes or saddle areas, each with multiple minimum points - we call these points our local minima. Our hope in this situation is to find the global minima - the smallest minima. We shall see how we minimize Loss functions and avoid local minima when we discuss Neural Network, which involves using gradient descent and all other optimization strategies. 9.7.8 Regularization Regularization is mostly used as an added tuning technique during learning. The idea is to calibrate hyperparameters and thus tune modeling algorithms. In doing so, we effectively optimize our models and reduce model complexity. It can, therefore, also effectively disqualify features by penalizing coefficients (a.l.a embedded-based feature selection in essence). Recall that Regularization is a way to reward or penalize. In linear regression, we use regularization to reward or penalize coefficients through the Loss functions, either diminishing or amplifying the effect of the Loss. Our goal is to guide coefficients, so the model fits even more optimal among data points. Note that OLS fits a model by adjusting two model parameters, also called coefficients, namely slope and interface. We add weight to the two coefficients by introducing a tunable regularization hyperparameter, namely the lambda \\(\\lambda\\). We then use an updated Loss function that accommodates the hyperparameter to specifically minimize error, e.g., MSE (mean square error); hence, calibration is about minimizing our objective (or loss) function. Let us review three regularization techniques in this section, namely ridge, lasso, and elastic net. First, let us mention a couple of loss functions we use during cross-validation. L1-norm vs L2-norm The least absolute deviation (LAD) is referred to as L1-norm and is expressed mathematically as: \\[\\begin{align} LS_{L1-norm} = \\sum_{i=1} |y_{i} - f(x_{i})| \\end{align}\\] The least squared error (LSE) is referred to as L2-norm and is expressed mathematically as: \\[\\begin{align} LS_{L2-norm} = \\sum_{i=1} (y_{i} - f(x_{i}))^2 \\end{align}\\] The difference in the formula is between using absolute difference versus using squared difference. L1-loss vs L2-loss The mean absolute error (MAE) is referred to as L1-loss and is expressed mathematically as: \\[\\begin{align} MAE_{L1} = \\frac{LS_{L1-norm}}{N} \\end{align}\\] The mean squared error (MSE) is referred to as L2-loss and is expressed mathematically as: \\[\\begin{align} MSE_{L2} = \\frac{LS_{L2-norm}}{N} \\end{align}\\] As for Regularization, we focus on using lambda as a tuning hyperparameter. The first to cover is Ridge regularization. Ridge Regularization (alpha=0) With Ridge regularization, we have an updated formula for coefficients \\(\\hat{\\beta}\\). \\[\\begin{align} \\hat{\\beta}_{ridge} = \\underbrace{(X^{T}X + \\lambda I)^{-1}X^{T}y}_{\\text{Regularized OLS thru Ridge}} \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ \\underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS} \\end{align}\\] Our least square objective \\(LS_{(objective)}\\) is expressed as: \\[\\begin{align} LS_{(objective)} = | y - X\\beta|^2 = \\sum_{i=1}^n (y_{i} - x_{i}\\beta)^2, \\end{align}\\] We modify the function to accommodate the Ridge regularization like so: \\[\\begin{align} RSS_{(\\beta_{ridge})} = |y - X\\beta|^2 + \\lambda \\beta^2 = \\underbrace{\\sum_{i=1}^n (y_{i} - x_{i}\\beta)^2}_{\\text{RSS}} + \\underbrace{\\lambda \\sum_{i=1}^n \\beta_i^2}_{\\text{Penalty Term}} \\end{align}\\] Notice the addition of lambda denoted as \\(\\lambda I\\). This tuning hyperparameter allows shrinking the coefficients, \\(\\beta s\\), making Ridge coefficients lesser than Least-Square coefficients. Our objective function for Ridge regression is therefore expressed as such: \\[\\begin{align} \\hat{\\beta}_{ridge} = \\text{arg}\\ \\underset{\\beta}{\\mathrm{min}}\\ RSS_{(\\beta_{ridge})} \\end{align}\\] To illustrate, let us use the glmnet(.) function to generate a General Linear Regression model. We provide alpha \\((\\alpha)\\) equal to 0 to indicate we are using ridge regression (see Figure 9.71). library(glmnet) x = data.matrix( mtcars[,!(names(mtcars) %in% c(&quot;mpg&quot;))] ) y = mtcars[, c(&quot;mpg&quot;)] ridge.model = glmnet(x, y, alpha=0) plot(ridge.model, xvar=&quot;lambda&quot;, label=TRUE, col=1:ncol(x)) legend(&quot;bottomright&quot;, lty=1, legend=colnames(x), col = 1:ncol(x)) grid(lty=3, col=&quot;lightgrey&quot;) Figure 9.71: Ridge Regularization Figure 9.71 shows a plot of coefficients, each corresponding to a feature variable in the mtcars dataset. Each coefficient converges toward zero from left to right as log lambda increases, virtually eliminating the features. If we settle the log lambda at 4, we can see that am, drat, vs, gear, disp, cyl, wt, and qsec features all have coefficients that are non-zero. On the other hand, qsec is close to being zero. In other words, it seems only eight out of ten are important features just based on observing the plot. Let us perform cross-validation using cv.glmnet(.) function to confirm our assessment of the eight coefficients. Here, we split the mtcars dataset into 5 sets (see Figure 9.72). cv.ridge = cv.glmnet(x, y, nfolds=5, type=&quot;mse&quot;, alpha=0) plot(cv.ridge) grid(col=&quot;lightgrey&quot;) Figure 9.72: Ridge Regularization (CV) In Figure 9.72, there are two vertical dotted lines. The first dotted line represents the minimum lambda, and the second dotted line represents one standard error (1se) from the minimum lambda. It also shows that the range between the minimum lambda and the lambda is one standard error away, covering a minimum MSE, making any lambdas within the range candidates for regularization. c(&quot;Minimum Lambda&quot;= (lambda.min = cv.ridge$lambda.min), &quot;1se Lambda&quot; = (lambda.1se = cv.ridge$lambda.1se)) ## Minimum Lambda 1se Lambda ## 2.747 9.206 Note that as we use cross-validation, we need to compute the MSE between training sets. The equivalent specific lambdas for min and 1se are computed using the following: idx.min = which(cv.ridge$lambda == lambda.min) idx.1se = which(cv.ridge$lambda == lambda.1se) c(&quot;MSE@lambda.min&quot; = cv.ridge$cvm[idx.min], &quot;MSE@lambda.1se&quot; = cv.ridge$cvm[idx.1se], &quot;1se&quot; = cv.ridge$cvsd[idx.min], &quot;MSE(our 1se)&quot; = cv.ridge$cvm[idx.min] + cv.ridge$cvsd[idx.min] ) ## MSE@lambda.min MSE@lambda.1se 1se MSE(our 1se) ## 6.881 7.965 1.228 8.109 If we choose lambda.1se (9.2061), then we have the following coefficients for the features: coef(cv.ridge, s = &quot;lambda.1se&quot;) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 19.85948 ## cyl -0.36149 ## disp -0.00524 ## hp -0.00962 ## drat 0.98359 ## wt -0.86947 ## qsec 0.15136 ## vs 0.87153 ## am 1.17967 ## gear 0.49498 ## carb -0.36970 Equivalently, we can arbitrarily plug the value of lambda.1se and run regression like so: ridge.model = glmnet(x, y, alpha=0, lambda=lambda.1se) coef(ridge.model) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 19.862051 ## cyl -0.361608 ## disp -0.005244 ## hp -0.009626 ## drat 0.984155 ## wt -0.869465 ## qsec 0.151322 ## vs 0.870986 ## am 1.179181 ## gear 0.494695 ## carb -0.369557 Afterwhich, we can then perform prediction using the ridge model: ridge.predict = predict.glmnet(ridge.model, newx = x) As one can see, based on our cross-validation, it seems that our cross-validation does show that all ten coefficients are used as long as our lambda between the range lambda.min and lambda.1se, which fall under a minimum MSE. It does not force the coefficients to become zero. Therefore, we still see that all ten coefficients are kept. We will show LASSO regularization next, which forces coefficients to become zero. A couple of notes to be aware of: Increase in ridge lambda decreases variance but increases bias. Ridge avoids overfitting by penalizing coefficients. It means it shrinks beta coefficients reducing MSE and predicted error. Lasso Regularization (alpha=1) LASSO stands for Least Absolute Selection and Shrinkage Operation. This regularization technique also decreases variance but increases bias. However, unlike Ridge regularization, it forces coefficients to become zero. Because of that, coefficients that are not significant tend toward zero and are eliminated, so in a way, this removes unwanted input variables. Similarly, with Lasso regularization, we have an updated formula for coefficients \\(\\hat{\\beta}\\). \\[\\begin{align} \\hat{\\beta}_{lasso} = \\underbrace{(X^{T}X + \\lambda I)^{-1}X^{T}y}_{\\text{Regularized OLS thru Lasso}} \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ \\underbrace{(X^{T}X)^{-1}X^{T}y}_{OLS} \\end{align}\\] Our least square objective \\(LS_{(objective)}\\) is expressed as: \\[\\begin{align} LS_{objective} = | y - X\\beta|^2 = \\sum_{i=1}^n (y_{i} - x_{i}\\beta)^2, \\end{align}\\] We modify the function to accommodate the ridge regularization like so: \\[\\begin{align} RSS_{(\\beta_{lasso})} = |y - X\\beta|^2 + \\lambda |\\beta| = \\underbrace{\\sum_{i=1}^n (y_{i} - x_{i}\\beta)^2}_{\\text{RSS}} + \\underbrace{\\lambda \\sum_{i=1}^n |\\beta_i|}_{\\text{Penalty Term}}, \\end{align}\\] Our objective function for Ridge regression is therefore expressed as such: \\[\\begin{align} \\hat{\\beta}_{lasso} = \\text{arg}\\ \\underset{\\beta}{\\mathrm{min}}\\ RSS_{(\\beta_{lasso})}. \\end{align}\\] To illustrate, we provide alpha \\((\\alpha)\\) equal to 1 to indicate we are using LASSO regression (see Figure 9.73). library(glmnet) x = data.matrix( mtcars[,!(names(mtcars) %in% c(&quot;mpg&quot;))] ) y = mtcars[, c(&quot;mpg&quot;)] lasso.model = glmnet(x, y, alpha=1) plot(lasso.model, xvar=&quot;lambda&quot;, label=TRUE, col=1:ncol(x)) legend(&quot;bottomright&quot;, lty=1, legend=colnames(x), col = 1:ncol(x)) grid(lty=3, col=&quot;lightgrey&quot;) Figure 9.73: Lasso Regularization Figure 9.73 shows a plot of coefficients, each corresponding to a feature variable in the mtcars dataset. From left to right, each coefficient converges toward zero as log lambda increases, virtually eliminating the feature. Unlike ridge, we can see in the plot that given a log lambda equal to zero, the number of coefficients (see top labels) is equal to three. Just by observation, the three features corresponding to the coefficients are wt, cyl, and hp. Let us perform cross-validation using cv.glmnet(.) function to confirm our assessment of the three coefficients. Here, as before, we split the mtcars dataset into 5 sets (see Figure 9.74). cv.lasso = cv.glmnet(x, y, nfolds=5, type=&quot;mse&quot;, alpha=1) plot(cv.lasso) grid(col=&quot;lightgrey&quot;) Figure 9.74: Lasso Regularization (CV) In Figure 9.74, we see the range between the minimum lambda and the lambda that is one standard error away, which covers a minimum MSE, making any lambdas within the range candidates for regularization. c(&quot;Minimum Lambda&quot;= (lambda.min = cv.lasso$lambda.min), &quot;1se Lambda&quot; = (lambda.1se = cv.lasso$lambda.1se)) ## Minimum Lambda 1se Lambda ## 0.1246 1.1617 It can be observed that the vertical dotted line for the lambda.1se points towards a lesser number of coefficients. Note that as we are using cross-validation, we need to compute the MSE between training sets. The equivalent specific lambdas for min and 1se are computed using the following: idx.min = which(cv.lasso$lambda == lambda.min) idx.1se = which(cv.lasso$lambda == lambda.1se) c(&quot;MSE@lambda.min&quot; = cv.lasso$cvm[idx.min], &quot;MSE@lambda.1se&quot; = cv.lasso$cvm[idx.1se], &quot;1se&quot; = cv.lasso$cvsd[idx.min], &quot;MSE(our 1se)&quot; = cv.lasso$cvm[idx.min] + cv.lasso$cvsd[idx.min] ) ## MSE@lambda.min MSE@lambda.1se 1se MSE(our 1se) ## 8.7300 9.5551 0.8342 9.5642 If we choose lambda.1se (1.1617), then we have the following coefficients for the features: coef(cv.lasso, s = &quot;lambda.1se&quot;) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## 1 ## (Intercept) 34.758210 ## cyl -0.860295 ## disp . ## hp -0.008835 ## drat . ## wt -2.501672 ## qsec . ## vs . ## am . ## gear . ## carb . Equivalently, we can arbitrarily plug the value of lambda.1se and run regression like so: lasso.model = glmnet(x, y, alpha=1, lambda=lambda.1se) coef(lasso.model) ## 11 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s0 ## (Intercept) 34.757287 ## cyl -0.859728 ## disp . ## hp -0.008845 ## drat . ## wt -2.502011 ## qsec . ## vs . ## am . ## gear . ## carb . Notice that we only see three features listed, namely cyl, hp, and wt. We can then perform prediction using the lasso model: lasso.predict = predict.glmnet(lasso.model, newx = x) Elastic Net Regularization (alpha=0.5) We leave readers to investigate Elastic Net. This technique meets in between both Ridge and LASSO regularization. It accepts alpha equal to 0.5, e.g. (see Figure 9.75). library(glmnet) x = data.matrix( mtcars[,!(names(mtcars) %in% c(&quot;mpg&quot;))] ) y = mtcars[, c(&quot;mpg&quot;)] elastic.model = glmnet(x, y, alpha=0.5) plot(elastic.model, xvar=&quot;lambda&quot;, label=TRUE, col=1:ncol(x)) legend(&quot;bottomright&quot;, lty=1, legend=colnames(x), col = 1:ncol(x)) grid(lty=3, col=&quot;lightgrey&quot;) Figure 9.75: Elastic Net Regularization And for cross-validation of Elastic Net, see Figure 9.76. cv.elastic = cv.glmnet(x, y, nfolds=5, type=&quot;mse&quot;, alpha=0.5) plot(cv.elastic) grid(col=&quot;lightgrey&quot;) Figure 9.76: Elastic Net Regularization (CV) We also leave readers to investigate Coordinate Descent for Lasso, which may be helpful for multivariate multinomial predictors. 9.8 Supervised vs. Unsupervised Learning In Computational Learning, there are two common types of learning discussed in other literature, namely supervised versus unsupervised learning. Our simple definition of supervised learning falls under the premise that a model can be trained if labels exist in a dataset being used for training and that the dataset is well labeled. However, if the dataset is not equipped with labels, then learning becomes unsupervised. In the following chapter, we cover Clustering as an unsupervised learning category. This section relies on two supervised learning categories when dealing with predictions. If our dataset consists of a continuous response variable, we use regression to train the model. On the other hand, if our response variable is categorical, we use classification to train the model. Also, if labels are dichotomous (binary) in form, we can use logistic regression for classification. There are pieces of literature that cover dichotomizing continuous labels or even performing quantile regression; however, this is outside the scope of our discussions, and thus we leave readers to investigate the subject further. In terms of predictor variables (features), some ML algorithms may prefer features to be either all continuous or all categorical but not mixed. If our dataset is mixed with both continuous and categorical features, it may be best to perform transformation to have a dataset with only all categorical or all continuous features. The choice of transformation depends upon necessary considerations. Some features may correlate well with the response variable in their transformed version. Some categorical features may have very high cardinality - many distinct values. The use of One Hot Encoding may not be practical as a transformation method for some features as this creates a high number of dummy variables. A good approach is perhaps to use stratification by grouping categories into strata. Using weight of evidence (WoE) and Information Value (IV), we may be able to group categories into their corresponding WoE or IV values in continuous form. On the other hand, some continuous features can be discretized into bins if the goal is to have a dataset with only categorical features. 9.9 Summary Having gone through Exploratory Data Analysis, let us now move on to discuss supervised learning. "],["machinelearning2.html", "Chapter 10 Computational Learning II 10.1 Regression (Supervised) 10.2 Binary Classification (Supervised) 10.3 Multi-class Classification (Supervised) ", " Chapter 10 Computational Learning II In this chapter, we continue to discuss Computational Learning, emphasizing Regression and Classification. Note that in Learning, we emphasize the Generalization Ability of our models (Vapnik V. 2000). 10.1 Regression (Supervised) Our simple definition of Regression is finding an estimated value closest to the actual value. With that definition, the Gauss-Markov theorem is worth mentioning, which stresses the idea of finding the most minimum variance to produce the best-unbiased estimation (Halliwell L. J. 2015; Ang A. 2014). It may be fair to say that Regression intends to achieve the same basic goal - the so-called bias-variance trade-off. In previous chapters, we discuss the use of Ordinary Least Square as a classic linear regression solution. We also discuss using Polynomial Regression as a solution for low and high-degree polynomials. Then we cover Kernel functions and Splines as additional methods to solve irregular non-linear cases. All that illustrates Regression in detail for linear, regular non-linear, and irregular non-linear cases. In the following few sections, we delve into the inner workings of a few popular regression algorithms. The emphasis is on the strategies used by the algorithms to balance bias and variance such that regression models are generalized to a certain degree. 10.1.1 Regression Trees It becomes a challenge to use simple or multiple linear regression to predict an outcome for our response variable, especially when we begin to form and encounter highly complex interactions of a large number of features. In this circumstance, a Regression Tree, also called Decision Tree, is one of many other better ML algorithmic alternatives to use. A Regression Tree allows for handling irregular non-linear complex datasets and allows better interpretability. In this section, we discuss one classic tree algorithm used to build a regression, namely Classification and Regression Tree (CART). While CART can handle both classification and regression, let us discuss Decision Trees for regression only. For classification, we mention three other classic tree algorithms, namely, ID3, C4.5, and C5.0. We cover them in a later section ahead under Classification Trees. Figure 10.1 illustrates a Regression Tree modeled using dataset mtcars to perform prediction. Here, we use a function called rpart(.) to model the tree and another function called rpart.plot to plot the tree (Therneau T. et al. 2019). library(rpart); library(rpart.plot) set.seed(142) mtcars2 = mtcars[, c(&quot;mpg&quot;, &quot;cyl&quot;, &quot;disp&quot;, &quot;hp&quot;, &quot;drat&quot;, &quot;wt&quot;, &quot;qsec&quot;)] datacars = mtcars2 = within(mtcars2, { cyl &lt;- as.factor(cyl) }) rpart.control = rpart.control(minsplit=2, maxdepth=5, minbucket = 2, cp=0, xval=10) tree.model = rpart(mpg ~ ., data = datacars, control = rpart.control) rpart.plot(tree.model) Figure 10.1: Regression Tree A tree consists of two types of nodes, namely decision nodes and leaf or terminal nodes. In our example, the tree is a CART tree - a binary tree that starts with a root node - the first decision node that splits into two child nodes. A child node can be another decision node or a terminal node that does not branch further. Note that CART is a greedy algorithm using a recursive binary splitting technique. Here, we discuss the algorithm of splitting a tree and pruning a tree. Note that we utilize a few helper functions for constructing a regression tree such as split.input(.), cost.rule(.), split.goodness(.), rank.importance(.), and my.tree(.). However, we naively implement them to focus more on certain main points of the algorithm. Such helper functions do not account for optimization, and thus they are limited in terms of recognizing only the following model parameters: minbucket - minimum observation of a node. maxdepth - maximum depth of a tree. The rpart(.) uses a separate parameter called minsplit that controls the minimum number of observations allowed in a decision node. Our implementation uses only minbucket for both decision and terminal nodes. Splitting a Tree Node and Building a Tree First, suppose we use any arbitrary single input variable, namely wt in this case, and also choose any arbitrary location to split the data. For example, suppose we split the input data evenly so that the split is located between a chosen pair of input points, namely the sixth and seventh input points. Note that duplicated data points are skipped along the way. options(width=60) wt = datacars[[&quot;wt&quot;]] (sorted.input = sort(wt, index.return = TRUE))$x ## [1] 1.513 1.615 1.835 1.935 2.140 2.200 2.320 2.465 2.620 ## [10] 2.770 2.780 2.875 3.150 3.170 3.190 3.215 3.435 3.440 ## [19] 3.440 3.440 3.460 3.520 3.570 3.570 3.730 3.780 3.840 ## [28] 3.845 4.070 5.250 5.345 5.424 options(width=60) split.index = 6 c(&quot;point.1&quot; = sorted.input$x[split.index], &quot;point.2&quot; = sorted.input$x[split.index + 1], &quot;left.obs&quot; = length(sorted.input$x[1:split.index]), &quot;right.obs&quot; = length(sorted.input$x [(split.index + 1):length(sorted.input$x)]), &quot;split.avg&quot; = (sorted.input$x[split.index] + sorted.input$x[split.index + 1])/2) ## point.1 point.2 left.obs right.obs split.avg ## 2.20 2.32 6.00 26.00 2.26 The average of the two input points is 2.26. We then use this average as a decision criterion to decide which group to assign for a data point. Any input less than the average goes to the left group, and the rest to the right group. Below is our implementation of the splitting criterion. What we get are the indices of the left and right groups. options(width=60) split.input &lt;- function(sorted.input, idx, is.factor = FALSE) { n = length(sorted.input$x) left.indices = sorted.input$ix[1:idx] right.indices = sorted.input$ix[(idx+1):n] list(&quot;left&quot; = left.indices, &quot;right&quot; = right.indices ) } (split = split.input(sorted.input, split.index )) ## $left ## [1] 28 19 20 26 27 18 ## ## $right ## [1] 3 21 1 30 32 2 9 29 8 4 23 5 10 11 6 22 7 31 ## [19] 13 14 24 25 12 15 17 16 We can use the indices to split our target (or response) variable like so: options(width=60) # all target values belonging to the left side (left.data = datacars[split$left, c(&quot;mpg&quot;)]) ## [1] 30.4 30.4 33.9 27.3 26.0 32.4 # all target values belonging to the right side (right.data = datacars[split$right, c(&quot;mpg&quot;)]) ## [1] 22.8 21.5 21.0 19.7 21.4 21.0 22.8 15.8 24.4 21.4 15.2 ## [12] 18.7 19.2 17.8 18.1 15.5 14.3 15.0 17.3 15.2 13.3 19.2 ## [23] 16.4 10.4 14.7 10.4 Second, we use Figure 10.2 as reference to measure the goodness of a split. Figure 10.2: Cost Computation We explain classification tree in Chapter , but here, we focus on Regression tree. In the case of regression, we use the sum squared error (SSE), also called sum squared residual (SSR) to measure the goodness of fit - this is our splitting criterion. \\[\\begin{align} SSE_{(split)}= SSE_{(left)} + SSE_{(right)} = \\sum_{i:L.split}^n\\left( y_i - \\bar{y}_L\\right)^2 + \\sum_{i:R.split}^m\\left( y_i - \\bar{y}_R\\right)^2 \\end{align}\\] where \\(L.split = x \\in L_{(split)}\\) and \\(R.split = x \\in R_{(split)}\\). We have the following implementation of measuring SSE for our simple split: left.sse = sum((left.data - mean(left.data))^2) right.sse = sum((right.data - mean(right.data))^2) (split.sse = left.sse + right.sse) ## [1] 391.1 Ultimately, our goal is to minimize SSE such that the optimal split renders the least SSE. Here, we use an example implementation of our loss function for the split. The object of interest is the computation of SSE and improvement of deviance - to be discussed further. sample = nrow(datacars) split.loss &lt;- function(loss, sort.input, output, left, right, avg, minbucket ) { cs = loss n = length(output) parent.mean = mean(output) parent.sse = sum((output - parent.mean)^2) if (length(left) &gt;= minbucket &amp;&amp; length(right) &gt;= minbucket ) { left.idx = sort.input$ix[left] right.idx = sort.input$ix[-left] o1 = output[left.idx]; o2 = output[right.idx] m1 = mean(o1); m2 = mean(o2) if (parent.mean != m1 || parent.mean != m2) { child.sse = sum((o1 - m1)^2) + sum((o2 - m2)^2) child.improve = 1 - (child.sse / parent.sse) o.len = n; l.len = length(o1); r.len = length(o2) cs$split = c(cs$split, avg) cs$sse = c(cs$sse, round(child.sse,4)) cs$improve = c(cs$improve, round(child.improve,6)) cs$obs = c(cs$obs, o.len); cs$l.ymean = c(cs$l.ymean, m1) cs$r.ymean = c(cs$r.ymean, m2) cs$l.son = c(cs$l.son, l.len) cs$r.son = c(cs$r.son, r.len) cs$left.indices[[as.character(avg)]] = left.idx cs$right.indices[[as.character(avg)]] = right.idx } } cs } We use the following rules to implement minimization. Additionally, we use the averages of each split to determine the direction (whether towards the left or right). Subsequently, we swap the direction to ensure the split with a lesser average goes towards the left. Here, we optimize by minimizing loss: minimum.loss &lt;- function(feature, loss, output, data, factor) { cs = loss indices = data$indices data = data$dataset[indices,] parent.mean = mean(output) parent.sse = sum((output - parent.mean)^2) parent.mse = mean((output - parent.mean)^2) if (!is.null(cs$sse) &amp;&amp; length(output) &gt; 1) { min.idx = which.min(cs$sse) avg = as.character(cs$split[min.idx]) perc = round(cs$obs[min.idx] / sample * 100, 0) lson = cs$l.son[min.idx] rson = cs$r.son[min.idx] lindices = cs$left.indices[[avg]] lindices = indices[lindices] rindices = cs$right.indices[[avg]] rindices = indices[rindices] node = list(&quot;feature&quot; = feature, &quot;split&quot; = avg, &quot;obs&quot; = cs$obs[min.idx], &quot;left&quot; = lson, &quot;right&quot; = rson, &quot;sse&quot; = cs$sse[min.idx], &quot;ymean&quot; = round(parent.mean,2), &quot;mse&quot; = round(parent.mse,6), &quot;improve&quot; = cs$improve[min.idx], &quot;perc&quot; = perc, &quot;left.indices&quot; = lindices, &quot;right.indices&quot; = rindices, &quot;indices&quot; = indices, &quot;response&quot; = output, &quot;ntype&quot; = &quot;node&quot;) } else { node = list(&quot;feature&quot; = feature, &quot;split&quot; = &quot;.&quot;, &quot;obs&quot; = length(output), &quot;left&quot; = &quot;.&quot;, &quot;right&quot; = &quot;.&quot;, &quot;sse&quot; = round(parent.sse,4), &quot;ymean&quot; = round(parent.mean,2), &quot;mse&quot; = round(parent.mse, 6), &quot;improve&quot; = &quot;.&quot;, &quot;perc&quot; = round( length(output) / sample * 100, 0), &quot;indices&quot; = indices, &quot;response&quot; = output, &quot;ntype&quot; = &quot;leaf&quot;) } node } optimizer &lt;- minimum.loss Let us use the cost function to get details of the split at index 6: feature = &quot;wt&quot;; response = &quot;mpg&quot; split.index = i = 6 input = datacars[,c(feature)] output = datacars[,c(response)] sort.input = sort(input, index.return = TRUE) avg = (sort.input$x[i] + sort.input$x[i+1]) / 2 left = which( sort.input$x &lt; avg ) right = which( sort.input$x &gt;= avg ) loss = split.loss( list(), sort.input, output, left, right, avg, 2) loss$left.indices = NULL; loss$right.indices = NULL t(as.matrix(loss)) ## split sse improve obs l.ymean r.ymean l.son r.son ## [1,] 2.26 391.1 0.6527 32 30.07 17.79 6 26 Here, SSE is the criterion for the goodness of split, which gives us: 391.1199. However, we may not know if the obtained SSE concludes an optimal split. Ideally, we need to know all the possible splits and thus compare all the corresponding SSEs. The idea is to find the minimum SSE, which becomes the choice of split to use. To do that, let us use two cost functions, namely split.continuous(.) to handle continuous input variables and split.categorical(.) to handle categorical input variables. For continuous input, we step through each pair of sorted input values, computing for the combined SSE of the binary splits. split.continuous &lt;- function(loss, feature, target, data, minbucket) { indices = data$indices input = data$dataset[indices,c(feature)] output = data$dataset[indices,c(target)] n = length(input) sort.input = sort(input, index.return = TRUE) if (n == 1) { avg = sort.input$x loss = split.loss( loss, sort.input, output, NULL, NULL, avg, minbucket) } else for (i in 1:(n-1)) { if (sort.input$x[i] != sort.input$x[i+1]) { avg = (sort.input$x[i] + sort.input$x[i+1]) / 2 left = which( sort.input$x &lt; avg ) right = which( sort.input$x &gt;= avg ) loss = split.loss( loss, sort.input, output, left, right, avg, minbucket) } } optimizer(feature, loss, output, data, factor=FALSE) } For n-observations of a continuous input variable, there are n-1 splits to consider. However, if the number of continuous observations is large, it helps to discretize, e.g., use percentile (5%, 10%, 15%, and on ). For categorical input, we can choose to consider all possible splitting permutations for the categories so that for our three levels (A, B, C), we can have the following splits: LRR, RLR, RRL, LLR, RLL, LRL. That is \\(P(n,n) = 6\\) permutations. However, imagine if we have a larger category, for example, zip codes, then it becomes computationally expensive to compute the goodness of fit for nominal categories. One way to reduce computation is to group the categorical input by the average of the output and then map the averages back to the corresponding input values (e.g., transforming to continuous) like so: set.seed(220) (input = sample(c(&#39;A&#39;,&#39;B&#39;,&#39;C&#39;), size=10, replace=TRUE)) ## [1] &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot; (output = round(runif( n = 10, min=20, max=25),1)) ## [1] 20.1 24.9 20.8 24.4 22.8 20.9 21.8 20.8 20.7 24.3 # group by average (output.grp.mean = tapply(output, input, mean)) ## A B C ## 21.76 24.30 22.10 # map the averages back to the input (transform to continuous) output.grp.mean[input] ## A C C A A C C A A B ## 21.76 22.10 22.10 21.76 21.76 22.10 22.10 21.76 21.76 24.30 We then sort the newly transformed input accordingly and step through the splitting. Before running a categorical split, it helps to acquire a complete list of all categories for all categorical inputs. The implementation below allows us to do just that: get.categories &lt;- function(target, dataset) { categories = list() features = !(names(dataset) %in% c(target)) features = names(dataset)[features] for (feature in features) { f = is.factor(dataset[1,c(feature)]) if (f == TRUE) { categories[[feature]] = levels(dataset[,c(feature)]) } } categories } get.categories(&quot;mpg&quot;, datacars) ## $cyl ## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot; We then use the complete list of categories as a reference when splitting categorical inputs. See our example implementation below: split.categorical &lt;- function(loss, feature, target, data, minbucket, categories) { indices = data$indices input = data$dataset[indices,c(feature)] output = data$dataset[indices,c(target)] n = length(input) # group and average by category output.grp.mean = tapply(output, input, mean) sort.input = sort( output.grp.mean[input], index.return = TRUE) if (n == 1) { avg = sort.input$x loss = split.loss( loss, sort.input, output, NULL, NULL, avg, minbucket) } else for (i in 1:(n-1)) { if (sort.input$x[i] != sort.input$x[i+1]) { avg = (sort.input$x[i] + sort.input$x[i+1]) / 2 left = which( sort.input$x &lt; avg ) right = which( sort.input$x &gt;= avg ) l = unique( names( which( sort.input$x &lt; avg ) ) ) r = unique( names( which( sort.input$x &gt;= avg ) ) ) catg = replace(categories, categories %in% l, &#39;L&#39;) catg = replace(catg, catg %in% r, &#39;R&#39;) avg = paste0(replace(catg, catg %in% setdiff(categories, c(l,r)), &#39;-&#39;), collapse=&quot;&quot;) loss = split.loss( loss, sort.input, output, left, right, avg, minbucket) } } optimizer(feature, loss, output, data, factor=TRUE) } Finally, we compute the goodness of fit for each feature using the following implementation: sp.model &lt;- function() { cs = list() cs$split = cs$sse = cs$obs = cs$improve = NULL cs$l.ymean = cs$r.ymean = cs$l.son = cs$r.son = NULL cs$left.indices = list(); cs$right.indices = list() cs } split.goodness &lt;- function(features, target, data, minbucket = 2, categories) { goodness = list() cs = sp.model() for (f in features) { input = data$dataset[data$indices,c(f)] if (is.factor(input)) { cat = categories[[f]] goodness[[f]] = split.categorical(cs, f, target, data, minbucket, cat) } else { goodness[[f]] = split.continuous(cs, f, target, data, minbucket) } } goodness } For continuous input, we show the list of SSEs derived by iteratively splitting the data points starting from the first pair to the last pair for our wt feature. Note here that we allow a minimum of two observations per node. features = c(&quot;wt&quot;, &quot;cyl&quot;) target = c(&quot;mpg&quot;) datacars = mtcars2[, c(target, features) ] data = list(&quot;indices&quot; = seq(1, nrow(datacars)), &quot;dataset&quot; = datacars) categories = get.categories(target, datacars) ft = split.goodness(features, target, data, minbucket = 2, categories)$wt ft$left.indices = ft$right.indices = ft$indices = ft$response = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right sse ymean mse ## [1,] &quot;wt&quot; &quot;2.26&quot; 32 6 26 391.1 20.09 35.19 ## improve perc ntype ## [1,] 0.6527 100 &quot;node&quot; The minimum SSE from the list is as follows: ft$sse ## [1] 391.1 For categorical input, we use cyl feature as an example which has only three levels: 4, 6, 8. Similarly, we limit the number of observations to two per node. ft = split.goodness(features, target, data, minbucket = 2, categories)$cyl ft$left.indices = ft$right.indices = ft$indices = ft$response = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right sse ymean mse improve ## [1,] &quot;cyl&quot; &quot;RLL&quot; 32 21 11 401.9 20.09 35.19 0.6431 ## perc ntype ## [1,] 100 &quot;node&quot; The cyl is categorical with three sorted levels (4, 6, 8). The second column describes a split in which level 4 is assigned to the Right, and levels 6 and 8 are assigned to the Left. The minimum SSE from the list is as follows: ft$sse ## [1] 401.9 Third, it is necessary to recall that the wt feature is arbitrarily chosen for the root node. That is indeed not ideal and not optimal. To determine which feature to use for a node split, we rank each feature by importance using the least SSE. The most important feature becomes the candidate to use for the split. Otherwise, we can also use a similar formula called improvement of deviance from rpart(.), and it is expressed like so (Therneau T.M. et al. 2019): \\[\\begin{align} improve = 1 - \\frac{(SS_{(left)} + SS_{(right)})}{SS_{(parent)}} \\end{align}\\] where: \\(SS_{(left)}\\) is the SSE from the left split, \\(SS_{(right)}\\) is the SSE from the right split, \\(SS_{(parent)}\\) is the SSE from the data points prior to split. ft$improve ## [1] 0.6431 The improvement of deviance is implemented in our split.cost(.). Note that the obtained improvement is computed only for the wt feature. We need to compute the improvements of the other features to compare and rank them. To do that, we use our example implementation called rank.importance(.). rank.importance &lt;- function(features, target, data, minbucket, categories) { goodness = split.goodness(features, target, data, minbucket, categories) ranks = NULL for (f in features) { r = goodness[[f]] r$left.indices = r$right.indices = r$response = r$indices = NULL r = data.frame(r, stringsAsFactors = FALSE) ranks = rbind(ranks, r) } ordered.idx = order(as.numeric(ranks[,c(&quot;sse&quot;)]), stringr::str_detect(ranks[,c(&quot;split&quot;)], &quot;[LR-]&quot;), ranks[,c(&quot;feature&quot;)] ) ranks = data.frame(ranks[ordered.idx,], stringsAsFactors = FALSE) feature = ranks[1,c(&quot;feature&quot;)] top = goodness[[feature]] list(&quot;ranks&quot; = ranks, &quot;top&quot; = top) } Below, we use the rank.importance(.) function to obtain the ranks of features given minbucket equal to two. Note that our implementation breaks the tie so that if multiple features have the same SSE, we order the features based on type (category vs. continuous) and feature name (by alphabet). Other implementations such as rpart(.) may use other means to break the tie. See the list of features below ranked based on the least SSE. features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) datacars = mtcars2 categories = get.categories(target, datacars) data = list(&quot;indices&quot; = seq(1, nrow(datacars)), &quot;dataset&quot; = datacars) r = rank.importance(features, target, data, minbucket=2, categories) as.data.frame(r$ranks) ## feature split obs left right sse ymean mse improve ## 5 wt 2.26 32 6 26 391.1 20.09 35.19 0.6527 ## 1 cyl RLL 32 21 11 401.9 20.09 35.19 0.6431 ## 2 disp 163.8 32 14 18 435.7 20.09 35.19 0.6130 ## 3 hp 118 32 15 17 449.2 20.09 35.19 0.6011 ## 4 drat 3.75 32 18 14 654.6 20.09 35.19 0.4187 ## 6 qsec 18.41 32 20 12 749.9 20.09 35.19 0.3340 ## perc ntype ## 5 100 node ## 1 100 node ## 2 100 node ## 3 100 node ## 4 100 node ## 6 100 node The top-rank feature for the node is: ft = r$top ft$left.data = ft$right.data = ft$left.indices = ft$right.indices = ft$response = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right sse ymean mse ## [1,] &quot;wt&quot; &quot;2.26&quot; 32 6 26 391.1 20.09 35.19 ## improve perc indices ntype ## [1,] 0.6527 100 Integer,32 &quot;node&quot; The output shows that our choice of feature to use for the root node is wt, which is ranked at the top, based on the least SSE (or based on the largest improvement of deviance). The left and right columns indicate the number of observations corresponding to the split. To validate, we capture a summary of our previous tree model generated by previously running the rpart(.) function. summ = capture.output(summary(tree.model)) Then we use our simple show.node(.) function to parse the summary and extract a specific node summary. The root node is our first node to show. show.node(summ, 1) ## [ 23 ] Node number 1: 32 observations, complexity param=0.6527 ## [ 24 ] mean=20.09, MSE=35.19 ## [ 25 ] left son=2 (26 obs) right son=3 (6 obs) ## [ 26 ] Primary splits: ## [ 27 ] wt &lt; 2.26 to the right, improve=0.6527, (0 missing) ## [ 28 ] cyl splits as RLL, improve=0.6431, (0 missing) ## [ 29 ] disp &lt; 163.8 to the right, improve=0.6131, (0 missing) ## [ 30 ] hp &lt; 118 to the right, improve=0.6011, (0 missing) ## [ 31 ] drat &lt; 3.75 to the left, improve=0.4187, (0 missing) ## [ 32 ] Surrogate splits: ## [ 33 ] disp &lt; 101.6 to the right, agree=0.969, adj=0.833, (0 split) ## [ 34 ] hp &lt; 92 to the right, agree=0.938, adj=0.667, (0 split) ## [ 35 ] drat &lt; 4 to the left, agree=0.906, adj=0.500, (0 split) ## [ 36 ] cyl splits as RLL, agree=0.844, adj=0.167, (0 split) The output confirms that wt is the top-ranking feature for the split with improvement of deviance equals 0.6527. The root node is split into two decision nodes with the one child node receiving 6 observations and the other child node receiving 26 observations. The split happens between a pair of input points averaging 2.26. When a feature has missing observations, rpart(.) uses a Surrogate split by splitting the observations of another feature (acting as the surrogate). Our simple implementation does not account for missing observations; thus, we leave readers to investigate further handling missing observations for regression trees. Fourth, let us finally try to implement a regression tree. Our example implementation of Regression Tree uses a simple level-ordered algorithm, and it is flat with no additional complex rules. Our example tree implementation uses a third-party library called dequer to handle queues and help achieve a level-ordered tree. library(dequer) my.regression.tree &lt;- function(features, target, dataset, minbucket = 1, maxdepth=50) { split.child &lt;- function(indices) { data = list(&quot;indices&quot; = indices, &quot;dataset&quot; = dataset) rank.importance(features, target, data, minbucket, categories) } build.level.tree &lt;- function(my.stack, top.count = 0, level = 1, model = list()) { child.stack = queue() while(length(my.stack)) { node = pop(my.stack) top = node$top top.count = top.count + 1 model[[top.count]] = node if (top$ntype == &quot;leaf&quot; || level &gt; maxdepth) { model[[top.count]]$top$split = &quot;.&quot; model[[top.count]]$top$ntype = &quot;leaf&quot; next } if (!is.null(top$left.indices)) { left = split.child(top$left.indices) left$parent = top.count pushback(child.stack, left) } if (!is.null(top$right.indices)) { right = split.child(top$right.indices) right$parent = top.count pushback(child.stack, right) } } if (length(child.stack) &amp;&amp; level &lt;= maxdepth) { model = build.level.tree(child.stack, top.count, level + 1, model) } model } my.stack = queue() indices = seq(1, nrow(dataset)) categories = get.categories(target, dataset) root = split.child(indices); root$parent = 0 pushback(my.stack, root) my.model = build.level.tree(my.stack) list(&quot;model&quot; = my.model, &quot;categories&quot; = categories) } my.tree = my.regression.tree For displaying the table form of the tree, we use the following function: my.table.tree &lt;- function(my.model, display_mode = FALSE) { mlen = length(my.model) tree = NULL for (m in 1:mlen) { r = my.model[[m]]$top r$left.indices = r$right.indices = r$response = r$indices = NULL if (display_mode == TRUE) { if (r$ntype == &quot;leaf&quot;) { r$feature = &quot;&lt;leaf&gt;&quot; } r$ntype = NULL } r = data.frame(r, stringsAsFactors = FALSE) r = cbind(N=as.numeric(m), P=my.model[[m]]$parent, r) tree = rbind(tree, r) } tree } Below, we use minbucket equal to 2 and maxdepth equal to 5 to generate a tree model. my.model = my.tree(features, target, datacars, minbucket=2, maxdepth=5) my.tree.model = my.table.tree(my.model$model, display_mode=TRUE) ## N P feature split obs L R SSE ymean MSE ## 1 1 0 wt 2.26 32 6 26 391.12 20.09 35.19 ## 2 2 1 qsec 19.185 6 4 2 16.03 30.07 7.43 ## 3 3 1 disp 266.9 26 12 14 127.32 17.79 13.33 ## 4 4 2 wt 1.775 4 2 2 0.84 28.52 3.73 ## 5 5 2 &lt;leaf&gt; . 2 . . 1.12 33.15 0.56 ## 6 6 3 wt 3.3275 12 9 3 15.94 20.93 3.51 ## 7 7 3 disp 450 14 12 2 33.66 15.10 6.09 ## 8 8 4 &lt;leaf&gt; . 2 . . 0.00 30.40 0.00 ## 9 9 4 &lt;leaf&gt; . 2 . . 0.84 26.65 0.42 ## 10 10 6 hp 96 9 3 6 3.97 21.78 1.65 ## 11 11 6 &lt;leaf&gt; . 3 . . 1.09 18.37 0.36 ## 12 12 7 drat 3.18 12 7 5 19.98 15.88 2.80 ## 13 13 7 &lt;leaf&gt; . 2 . . 0.00 10.40 0.00 ## 14 14 10 &lt;leaf&gt; . 3 . . 1.71 23.33 0.57 ## 15 15 10 qsec 16.74 6 2 4 0.99 21.00 0.38 ## 16 16 12 disp 339 7 5 2 3.47 16.79 2.37 ## 17 17 12 hp 254.5 5 3 2 1.36 14.62 0.68 ## 18 18 15 &lt;leaf&gt; . 2 . . 0.84 20.35 0.42 ## 19 19 15 &lt;leaf&gt; . 4 2 2 0.08 21.32 0.04 ## 20 20 16 &lt;leaf&gt; . 5 3 2 2.27 15.92 0.67 ## 21 21 16 &lt;leaf&gt; . 2 . . 0.12 18.95 0.06 ## 22 22 17 &lt;leaf&gt; . 3 . . 1.04 14.10 0.35 ## 23 23 17 &lt;leaf&gt; . 2 . . 0.32 15.40 0.16 ## improve perc ## 1 0.652661 100 ## 2 0.64015 19 ## 3 0.632617 81 ## 4 0.943317 12 ## 5 . 6 ## 6 0.621527 38 ## 7 0.604969 44 ## 8 . 6 ## 9 . 6 ## 10 0.732984 28 ## 11 . 9 ## 12 0.40646 38 ## 13 . 6 ## 14 . 9 ## 15 0.560841 19 ## 16 0.790639 22 ## 17 0.598583 16 ## 18 . 6 ## 19 0.423729 12 ## 20 0.323477 16 ## 21 . 6 ## 22 . 9 ## 23 . 6 The column N represents the node number, and P represents the parent number of a node. The columns L and R represent the number of nodes in the left and right splits, respectively. We previously validated the root node. Let us validate the two children (nodes 2 and 3). Moreover, let us show the following summary of the left child node (node 2). show.node(summ, 2) ## [ 38 ] Node number 2: 26 observations, complexity param=0.1947 ## [ 39 ] mean=17.79, MSE=13.33 ## [ 40 ] left son=4 (14 obs) right son=5 (12 obs) ## [ 41 ] Primary splits: ## [ 42 ] disp &lt; 266.9 to the right, improve=0.6326, (0 missing) ## [ 43 ] cyl splits as RRL, improve=0.6326, (0 missing) ## [ 44 ] hp &lt; 136.5 to the right, improve=0.5804, (0 missing) ## [ 45 ] wt &lt; 3.325 to the right, improve=0.5393, (0 missing) ## [ 46 ] qsec &lt; 18.15 to the left, improve=0.4211, (0 missing) ## [ 47 ] Surrogate splits: ## [ 48 ] hp &lt; 136.5 to the right, agree=0.962, adj=0.917, (0 split) ## [ 49 ] wt &lt; 3.49 to the right, agree=0.885, adj=0.750, (0 split) ## [ 50 ] qsec &lt; 18.15 to the left, agree=0.885, adj=0.750, (0 split) ## [ 51 ] drat &lt; 3.58 to the left, agree=0.846, adj=0.667, (0 split) Below, we show a summary of the right child node (node 3): show.node(summ, 3) ## [ 53 ] Node number 3: 6 observations, complexity param=0.02533 ## [ 54 ] mean=30.07, MSE=7.426 ## [ 55 ] left son=6 (4 obs) right son=7 (2 obs) ## [ 56 ] Primary splits: ## [ 57 ] qsec &lt; 19.18 to the left, improve=0.6402, (0 missing) ## [ 58 ] disp &lt; 78.85 to the right, improve=0.6322, (0 missing) ## [ 59 ] wt &lt; 1.885 to the right, improve=0.3030, (0 missing) ## [ 60 ] hp &lt; 65.5 to the right, improve=0.2923, (0 missing) ## [ 61 ] drat &lt; 4.325 to the right, improve=0.2346, (0 missing) ## [ 62 ] Surrogate splits: ## [ 63 ] disp &lt; 78.85 to the right, agree=0.833, adj=0.5, (0 split) Pruning a Tree In essence, the formation of a tree is driven by a few adjustable parameters, namely the allowable depth of the tree and the allowable number of observations per node. One of the reasons to adjust the shape of a tree is to offset overfitting and underfitting. In the previous discussion, we used maxdepth to control the depth of a tree. In a way, this is a straightforward approach to trimming a tree. Moreover another approach is to prune a tree. Similar to LASSO and Ridge regularization in linear regression discussion in which we use a parameter \\(\\lambda\\) to find an optimal value to penalize or reward coefficients, we use a complexity parameter (CP) (Therneau T. M. et al. 1997) in regression trees to prune a branch (node) of a tree - we call this cost complexity pruning or weakest link pruning. The idea is to find an optimal tree - meaning that we do not want a tree to grow too complex in the same way that we do not want our regression line in linear regression to start following every point, resulting in an overfit. The intuition is to prune a tree if \\(R^2\\) increases by at least a factor of CP size. If \\(R^2\\) does not increase by at least the amount of a CP value, then no pruning happens. In linear regression, we use adjusted \\(R^2\\) to evaluate the significance of a predictor variable; but to the point, CP is a way to measure tree complexity and help determine which tree branch to prune. \\[\\begin{align} \\alpha_{(cp)} = \\frac{R(T) - R(T_t)}{|T_t| - |T| } \\end{align}\\] where \\(\\alpha\\) is the complexity parameter (CP) \\(\\mathbf{R(T)}\\) is the risk (computed SSE) for the entire T tree. \\(\\mathbf{R(T_t)}\\) is the risk for a t subtree. \\(\\mathbf{|T|}\\) is the number of splits in the entire tree. \\(\\mathbf{|T_t|}\\) is the number of splits in the t subtree. The cp value is computed as part of our example implementation of Pruning a tree: tree.loss &lt;- function(model) { nodes = model$N parents = unique(model$P) nsplit = length(parents) - 1 diff = setdiff(nodes, parents) idx = which( nodes %in% diff ) leafs = data.frame(model[idx,], stringsAsFactors = FALSE) mse = as.numeric(leafs$mse) obs = as.numeric(leafs$obs) sum(mse * obs) } my.prune.tree &lt;- function(my.model, cp = 0.0 ) { prune.cost &lt;- function(model, cptable = NULL, subtrees = list(), cnt = 0) { nodes = model$N parents = unique(model$P) nsplit = length(parents) - 1 diff = setdiff(nodes, parents) idx = which( nodes %in% diff ) leafs = data.frame(model[idx,], stringsAsFactors = FALSE) candidate.branch = cost = pruned.model = NULL min.tcost = min.branch = Inf for (p in parents) { children = model[ which(model$P == p), ] nchildren = sum(leafs$N %in% children$N) if (nchildren == 2) { idx = which(model$N %in% children$N) # exclude pruned children for loss compute new.model = model[-idx,] tc = tree.loss(new.model) if (min.tcost &gt; tc) { new.model[which(new.model$N == p),]$split = &#39;.&#39; new.model[which(new.model$N == p),]$ntype = &quot;leaf&quot; pruned.model = new.model min.tcost = tc; min.branch = p } } } subtrees[[as.character(cnt)]] = model cptable = rbind(cptable, c(min.branch, min.tcost)) if (nrow(pruned.model) == 1 ) { cptable = apply(cptable, 2, as.numeric ) cptable = apply(cptable, 2, rev) base.err = min.tcost rel.err = cptable[,2] / base.err cptable = data.frame( cp = -c(diff(rel.err),0), nsplit = seq(0, nrow(cptable) - 1), rel.error = rel.err, sse = cptable[,2], branch = cptable[,1] ) m = nrow(cptable) cptable = cptable[ which( cptable$cp &gt;= cp ),] n = nrow(cptable) res = list(&quot;subtree&quot; = subtrees[[as.character(m - n)]], &quot;cptable&quot; = cptable ) return(res) } prune.cost(pruned.model, cptable, subtrees, cnt + 1) } model = my.table.tree(my.model$model) treeloss = tree.loss(model) prune.cost(model, c(0, treeloss)) } Finally, let us show the cptable. Note that the branch column indicates the branch (or decision node) to prune given its corresponding cp value. See below: (cptable = my.prune.tree(my.model, cp = 0.0)$cptable) ## cp nsplit rel.error sse branch ## 1 0.652661 0 1.000000 1126.05 1 ## 2 0.194702 1 0.347339 391.12 3 ## 3 0.045774 2 0.152636 171.88 7 ## 4 0.025328 3 0.106863 120.33 2 ## 5 0.023250 4 0.081534 91.81 6 ## 6 0.012488 5 0.058285 65.63 4 ## 7 0.012149 6 0.045796 51.57 12 ## 8 0.011647 7 0.033648 37.89 16 ## 9 0.009670 8 0.022000 24.77 10 ## 10 0.001801 9 0.012330 13.88 17 ## 11 0.001126 10 0.010529 11.86 15 ## 12 0.000000 11 0.009404 10.59 0 The table can be validated using a similar cp table generated by rpart(.). tree.model$cptable # generated by rpart() ## CP nsplit rel error xerror xstd ## 1 0.652661 0 1.000000 1.0646 0.25211 ## 2 0.194702 1 0.347339 0.6490 0.10462 ## 3 0.045774 2 0.152636 0.3922 0.06825 ## 4 0.025328 3 0.106863 0.3582 0.06410 ## 5 0.023250 4 0.081534 0.2996 0.05976 ## 6 0.012488 5 0.058285 0.2803 0.06724 ## 7 0.012149 6 0.045796 0.2803 0.06724 ## 8 0.011647 7 0.033648 0.2803 0.06724 ## 9 0.009670 8 0.022000 0.2788 0.06672 ## 10 0.001801 9 0.012330 0.2322 0.06739 ## 11 0.001126 10 0.010529 0.2457 0.07379 ## 12 0.000000 11 0.009404 0.2476 0.07793 There is a slight discrepancy between the two results; in particular, we notice that rpart(.) is built with cross-validation. Thus, we see two other measures: the cross-validation error and standard deviation (xerror and xstd). \\[\\begin{align} rel. error = xerror + xstd \\end{align}\\] Additionally, the xerror is based on the Predictive Residual Sum Square Error (PRESS) statistic (Terry M Therneau et al. 2019). As for deriving the value of cp, let us use cp equal to 0.1947 from the second row in the table, which involves a tree with one split. That is calculated as such: \\(\\alpha_{(cp)}\\) = \\(\\frac{R(T) - R(T_t)}{|T_t| - |T| }\\) = (0.3473 - 0.1526) / (2 - 1) = 0.1947. Now, to demonstrate pruning using a choice of cp from the cp table, let us first create a tree with minsplit and minbucket equal to five and maxdepth equal to two. That gives us a simple and decent tree (see Figure 10.3): library(rpart); library(rpart.plot) set.seed(142) datacars = mtcars2 rpart.control = rpart.control(minsplit=2, maxdepth=2, minbucket = 5, cp = 0) tree.model = rpart(mpg ~ ., data = datacars, control = rpart.control) rpart.plot(tree.model, main=paste0(&quot;cp = 0&quot;)) Figure 10.3: Regression Tree The tree gives us three terminal nodes with two splits. Suppose we trim node number two of the decision tree. Node number two has two terminal nodes. To confirm that, let us show the node like so: print(tree.model) ## n= 32 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 32 1126.00 20.09 ## 2) wt&gt;=2.26 26 346.60 17.79 ## 4) disp&gt;=266.9 14 85.20 15.10 * ## 5) disp&lt; 266.9 12 42.12 20.92 * ## 3) wt&lt; 2.26 6 44.55 30.07 * Let us show a list of cp choices and choose one to allow us to trim a tree. tree.model$cptable ## CP nsplit rel error xerror xstd ## 1 0.6527 0 1.0000 1.0646 0.25211 ## 2 0.1947 1 0.3473 0.6490 0.10462 ## 3 0.0000 2 0.1526 0.5074 0.09457 We use the cp value equal to 0.1947 (see Figure 10.4). rpart.control = rpart.control(minsplit=2, maxdepth=2, minbucket = 5, cp = tree.model$cptable[2,1]) cp.tree.model = rpart(mpg ~ ., data = datacars, control = rpart.control) rpart.plot(cp.tree.model, main=paste0(&quot;cp = &quot;, tree.model$cptable[2,1])) Figure 10.4: Regression Tree In the figure, node two becomes a terminal node. We can validate below: cp.tree.model ## n= 32 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 32 1126.00 20.09 ## 2) wt&gt;=2.26 26 346.60 17.79 * ## 3) wt&lt; 2.26 6 44.55 30.07 * A better way to achieve the same tree configuration is to use the prune(.) function - the idea is to avoid regenerating a tree; instead, perform pruning. See below: chosen.cp = tree.model$cptable[2,1] (pruned.tree.model = prune(tree.model, cp = chosen.cp)) ## n= 32 ## ## node), split, n, deviance, yval ## * denotes terminal node ## ## 1) root 32 1126.00 20.09 ## 2) wt&gt;=2.26 26 346.60 17.79 * ## 3) wt&lt; 2.26 6 44.55 30.07 * If we now get the summary, we see that Node number 2 is pruned - it becomes a terminal node. (pruned.tree = as.party(pruned.tree.model)) ## ## Model formula: ## mpg ~ cyl + disp + hp + drat + wt + qsec ## ## Fitted party: ## [1] root ## | [2] wt &gt;= 2.26: 18 (n = 26, err = 347) ## | [3] wt &lt; 2.26: 30 (n = 6, err = 45) ## ## Number of inner nodes: 1 ## Number of terminal nodes: 2 We also can show a box plot per node using two libraries called party and partykit. See Figure 10.5. library(party); library(partykit) plot(pruned.tree,paste0(&quot;cp = &quot;, tree.model$cptable[2,1])) Figure 10.5: Pruned Tree Prediction and Cross-Validation In splitting or pruning a tree, we mostly rely on at least four parameters: minsplit, minbucket, maxdepth, and complexity. These four knobs allow us to build and optimize a tree model as best we can; however, with a high number of possible permutations, it takes a bit of effort to arrive at a model with optimal parameters. Therefore, to better train a model without manually tweaking knobs, we rely on cross-validation. First, our strategy is to perform a leave-one-out cross-validation given a dataset. Here, we use createFolds(.) function to derive several folds. We treat the first fold as our test dataset and the rest of the dataset as our training set. Labels (or response variables) are excluded from the test set. set.seed(142) features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) datacars = mtcars2 fold.indices = createFolds(datacars$mpg, k=nrow(datacars), returnTrain=FALSE) # choose the first fold for our test group. testset = datacars[fold.indices$Fold01,] # choose the other folds for training group. trainset = datacars[-fold.indices$Fold01,] # test target test.target = testset$mpg # test predictors with no labels test.predictors = testset test.predictors$mpg = NULL Second, we need to be able to measure the performance of our model using a prediction function. A predicted value corresponding to an observation in our test dataset is obtained from one of the tree leaves, which holds the average target value. We use the average target value as the predicted value. To reach the leaf, we start from the root of the tree and navigate towards the target leaf by following the decision rules enforced at the decision nodes. To do that, we use the following formula below: \\[\\begin{align} \\overline{Y}_{(target\\ leaf)} = \\sum_{l\\ \\in\\ L} \\overline{Y}_l \\times \\prod_{d\\ \\in\\ D} I\\left(X_v^{(a)}, D_v^{(a)}\\right) \\label{eqn:eqnnumber402}\\\\ I\\left(X_v^a{(a)}, D_v^{(a)}\\right) = \\begin{cases} 1 &amp; X_v^{(a)} &lt; D_v^{(a)} \\\\ 0 &amp; otherwise\\end{cases} \\label{eqn:eqnnumber403} \\end{align}\\] where: \\(\\mathbf{I}\\) is an indicator function \\(\\mathbf{l}\\) is a leaf node L is a set of leaf nodes d is a decision node D is a set of ancestral decision nodes (ancestors of l, including direct parents) a is a feature in X input and corresponding decision node v is the value of the feature in X and the value of the split in the decision node \\(\\mathbf{\\overline{Y}}\\) is the average value of a leaf (for continuous output) Let us review an example implementation of our prediction function for a model tree. Note that we skip the use of indicator function and matrix manipulation. On the other hand, our implementation uses coefficient of determination (\\(\\mathbf{R^2}\\)), RMSE, MSE, and MAE for performance measurements. Refer to Residual Sum of Squares (RSS) derived from Simple Linear Regression discussed in Statistical Computation. prediction.score &lt;- function(y, y.hat) { RMSE = MSE = MAE = RMSLE = R2 = Bias = Variance = 0 RSS = sum( (y - y.hat)^2 ) # residual sum of squares (RSS) ESS = sum( (y.hat - mean(y))^2 ) # explained sum of squares (ESS) TSS = RSS + ESS # total sum of squares # (TSS) = RSS + ESS if (TSS) { R2 = 1 - RSS/TSS # R squared RMSE = sqrt(RSS / length(y)) # Root mean square error MSE = mean( (y - y.hat)^2 ) # Mean square error MAE = sum( abs(y - y.hat) ) # mean absolute error RMSLE = ModelMetrics::rmsle(y, y.hat) # Root Mean square log error } result = data.frame( &quot;Rsquare&quot; = R2, &quot;MSE&quot; = MSE, &quot;MAE&quot; = MAE, &quot;RMSE&quot; = RMSE, &quot;RMSLE&quot; = RMSLE) list(&quot;fitted.values&quot; = y.hat, &quot;loss&quot; = round(result,4)) } my.predict &lt;- function(my.model, x, y, resid = NULL, tendency = function(top, resid = NULL) { mean(top$response) }, method = prediction.score, tabletree = FALSE) { n = nrow(x) responses = rep(0, n) model = my.model$model categories = my.model$categories if (tabletree == FALSE) { model = my.table.tree(my.model$model) } for (i in 1:n) { node = model[1,] while (TRUE) { if (node$ntype == &quot;leaf&quot;) { if (tabletree == FALSE ) { top = my.model$model[[node$N]]$top responses[i] = tendency( top, resid ) } else { responses[i] = node$ymean } break } children = model[which( model$P == node$N ),] cat = categories[[node$feature]] split = node$split val = x[i, c(node$feature)] if (is.factor(val)) { s = base::strsplit(split,NULL)[[1]] direction = s[which(cat %in% val )] direction = ifelse(direction == &#39;L&#39;, 1, 2) } else { split = as.numeric(split) val = as.numeric(val) direction = ifelse(val &lt; split, 1, 2) } node = children[direction,] } } method(y, responses) } Let us use the training set to build our tree model with minbucket=5 and maxdepth=3 like so: features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) my.tree.model = my.tree(features, target, trainset, minbucket=5, maxdepth=3) Then, we perform prediction using the test set: (predicted = my.predict(my.tree.model, test.predictors, test.target))$loss ## Rsquare MSE MAE RMSE RMSLE ## 1 0.5 2.151 1.467 1.467 0.069 Let us train our tree model using minbucket=5 and maxdepth=3 like so: my.tree.model = my.tree(features, target, trainset, minbucket=2, maxdepth=5) Then, we perform prediction using the test set the second time: (predicted = my.predict(my.tree.model, test.predictors, test.target))$loss ## Rsquare MSE MAE RMSE RMSLE ## 1 0.5 0.4225 0.65 0.65 0.03 Compared to the previous result, we get a larger RMSE this time around. The predicted values from the last prediction are as follows: predicted$fitted.values ## [1] 20.35 compared to the observed values: test.target ## [1] 21 Note that the predicted value is not as close to the observed value as we expect. That is because prediction performance may not always suffice using only a single tree. As we will see in later sections, there are other ways to obtain optimal performance, such as Bagging and Boosting. Third, let us continue our discussion on optimizing our tree by using cross-validation (cv) to select an optimal cp. Note that this is not necessarily intended to boost performance; rather, we use cv to balance bias and variance (e.g., avoid overfitting). Here, we use our example implementation of k-fold cross-validation for a tree model. While our cv function uses prediction to measure model performance, we evaluate which cost complexity (cp) parameter value best supports our model tree. cross.validate &lt;-function (features, target, data, k=10, minbucket=1, maxdepth=50, cp.list) { fold.indices = createFolds(data[,c(target)], k = k, returnTrain = FALSE) cv.result = matrix(0, length(cp.list), 6) models = list(); tests = list() # Build a set of large trees for (foldname in names(fold.indices)) { fold = fold.indices[[foldname]] test = data[fold,] train = data[-fold,] models[[foldname]] = my.tree(features, target, train, minbucket=minbucket, maxdepth=maxdepth) tests[[foldname]] = test } # Prune trees up to cost complexity for (i in 1:length(cp.list)) { fold.predictions = NULL for (foldname in names(fold.indices)) { # cross-validate model = models[[foldname]] test = tests[[foldname]] x.label = test[,c(target)] x.predictors = test[,c(features)] pruned.tree = my.prune.tree(model, cp = cp.list[i]) pruned.model = list(&quot;model&quot; = pruned.tree$subtree, &quot;categories&quot; = model$categories) predicted = my.predict(pruned.model, x.predictors, x.label, tabletree=TRUE) fold.predictions = rbind(fold.predictions, c(cp.list[i], as.matrix(predicted$loss))) } cv.result[i,] = apply( fold.predictions , 2, mean) } colnames(cv.result) = c(&quot;cp&quot;, &quot;Rsquare&quot;, &quot;RMSE&quot;, &quot;MSE&quot;, &quot;MAE&quot;, &quot;RMSLE&quot; ) cv.result } We use 10-fold cross-validation, fixing our parameters to the default values, namely minbucket=1 and maxdepth=50 to achieve the largest tree. Then initialize a set of cp values. The idea is to feed our cv function arbitrarily with a set of cp and see if any of the cp renders the lowest performance metrics (of our choice) for each corresponding pruned subtree. set.seed(145) features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) datacars = mtcars2 cp.list = seq(from=0.0, to=0.01, length.out=10) # crude but adjustable cvalid = cross.validate(features, target, datacars, k=10, cp.list = cp.list) Using the code below, we can plot our chosen performance metrics against the set of cp. See Figure 10.6. Figure 10.6: Cost Complexity The four plots show an optimal cp somewhere below 0.006. As an experiment, it may help to re-run the code with a new list of cp range that is more granular to see what the plot may show in terms of a possible lower cp. 10.1.2 Ensemble Methods In place of pruning and cross-validation of decision trees, an alternative method to use is called ensemble. Primarily, the goal is to balance the trade-off between bias and variance. There are three Ensemble methods used for decision trees, namely Bagging, Boosting, and Stacking. In this section, we discuss the basic concept of the three methods. Then, in separate sections ahead, we cover Random Forest, AdaBoost, and XBoost to illustrate their application. Bagging The first method is called Bagging, which is a popular ensemble method introduced by Leo Breiman (1994). The name stands for Bootstrap Aggregation. The idea is to perform random selection with replacement against our data set. We bundle data points randomly selected into a sample set called bootstrap sample, which is used to construct a tree model. We repeat this random selection several times to build a list of bootstrap samples that become the basis for constructing a set of tree models used to perform average computation against all regression (or predictions) of the tree models. In effect, we can balance well the spread of our data points across several sample sets (trees) - we compensate for the high variance of individual trees by averaging. Though we lose interpretability, by this random selection with replacement, the combined models show reduced variance and low bias with higher prediction accuracy. To illustrate, let X be a vector of data points. \\[\\begin{align} X = (x_{1}, x_{2}, ..., x_{n}) \\end{align}\\] Let S be bootstrap samples from which trees (T) are modeled : \\[\\begin{align*} S_{1} \\in \\{x_{1}, x_{2}, ..., x_{n}\\} \\to T_{1} \\\\ S_{2} \\in \\{x_{1}, x_{2}, ..., x_{n}\\} \\to T_{2} \\\\ \\vdots \\\\ S_{b} \\in \\{x_{1}, x_{2}, ..., x_{n}\\} \\to T_{b} \\end{align*}\\] Let \\(T_{i}(X^{(test)}, Y^{(test)})\\) be the tree model function to compute the average weight of a tree, where X represents predictors for the test set, and Y represents target labels for the test set. The goal is to predict Y by aggregating output from all the bagged trees. \\[\\begin{align} Y_{(aggregate)} = \\frac{1}{k}\\sum_{i=1}^{k} T_{i}(X^{(test)}_i, Y^{(test)}_i) \\end{align}\\] First, we show our example implementation of our Bagging algorithm. Here, we introduce one parameter, namely b, which indicates the number of tree models we want to generate. my.bagging &lt;- function(features, target, data, minbucket=1, maxdepth=50, b=5, size=5) { models = list() s = sample.int(size, b * size, replace = TRUE ) m = matrix(s, b, size, byrow=TRUE) for (i in 1:b) { sample.indices = m[i,] samples = data[sample.indices, ] model = h.learner(features, target, samples) models[[i]] = model } models } Second, we generate predictions for each ensemble model. For prediction, we use RMSE as our measure of performance. We also use the apply(.) function to average the derived fitted values from each tree. my.bagging.predict &lt;- function(ensemble.model, x, y, b=5) { predictions = NULL for (i in 1:b) { model = ensemble.model[[i]] predicted = h.score(model, x, y) predictions = rbind(predictions, predicted$fitted.values) } y.hat = apply(predictions,2,mean) prediction.score(y, y.hat) } Third, our choice of base learner is our regression tree, namely my.tree(.). Also, our choice of score function corresponds to my.predict(.). In other literature, such base learners produce estimates (deemed hypothetical); thus, we tend to see learner functions denoted as h(.). Let us follow such convention, which becomes apparent in our discussion of the different ensemble methods. h.learner &lt;- my.tree h.score &lt;- my.predict Fourth, we divide our dataset into train set and test set as before. We use our previous train set to generate multiple resamples for our Bagging method - that is, we resample 100 times (b=100) with replacement, given a fixed sample size that is equal to the size of the train set. set.seed(2020) b=100 h.model = my.bagging(features, target, trainset, b=b, size=nrow(trainset)) h.pred = my.bagging.predict(h.model, test.predictors, test.target, b=b) c(&quot;Predicted&quot; = h.pred$fitted.values, &quot;Actual&quot; = test.target) ## Predicted Actual ## 20.75 21.00 h.pred$loss ## Rsquare MSE MAE RMSE RMSLE ## 1 0.5 0.065 0.255 0.255 0.0117 As we can see, we have a predicted value closer to the actual value using Bagging. In the next section, we cover Random Forest, a variant of Bagging. Boosting The second ensemble method is called Boosting, introduced by Y.Freund and R. Schapire (1999). It is considered a stagewise additive modeling technique because it trains an ensemble of models sequentially, boosting performance one model after the next. For each iteration, each model calculates residuals as output which get passed on as weights to boost the performance of the next learning model. In later sections ahead, we cover three variants of Boosting, namely AdaBoost, Gradient Boost, and XGBoost (R. Schapire 1999). Stacking Finally, the third ensemble method is called Stacking, and it is a Meta-Learning technique because we stack different heterogeneous modeling techniques such as Random Forest, SVM, and XGBoost, allowing these chosen models to learn independently, providing their corresponding predictions which we then evaluate to determine the order of accuracy. 10.1.3 Random Forest Random Forest is a refinement to Bagging introduced by Leo Breiman (2001). There are two concepts introduced. For one, random forest performs a random selection of samples (called Bootstrap samples) and a random selection of the input variables (features). Using a subset of features for every node bifurcation adds uniqueness to the underlying tree; thus, the number of correlations among a sampled set (or trees) is reduced. Additionally, this handles high dimensionality. The number of features in a subset is controlled by a parameter (m) when there are (p) parameters (or variables). \\[\\begin{align} m = \\sqrt{p}\\ \\ \\ \\ \\ \\ \\text{where m is a subset of p} \\end{align}\\] For another, it also introduces out-of-bag (OOB). Figure 10.7 best illustrates the idea of Random Forest with OOB. Note that while the figure shows as if a flask is pouring data points into a bag, pretend for a moment that the flask ‘magically’ replaces a data point that leaves the flask. Figure 10.7: Random Forest (Out-Of-Bag) In the figure, one point to emphasize is that each data point from the out-of-bag is used to test all tree models (in the ensemble) that are not trained using the data point. In essence, this cross-validation saves us from allocating a separate validation set. It becomes apparent in the implementation of our validation function later. So how does Random Forest work? First, let us implement a variant of our base learner, my.tree(.), and name it as my.rf.tree(.). We then include a helper function to generate a subspace for our features. From this subspace, we randomly sample a subset of all the features (to be used by each split). We do not track re-used features in subsets; though, perhaps we leave this as an exercise. From here, we have a new regression tree function for our random forest: library(dequer) my.rf.tree &lt;-function(features, target, dataset, minbucket = 1, maxdepth=50){ random.subspace &lt;- function(features) { p = length(features) subspaces = combn(p,sqrt(p)) # m = sqrt(p) n.sub = ncol(subspaces) sub.indices = sample(seq(1:n.sub), size=1, replace=TRUE) features[subspaces[,sub.indices]] } split.child &lt;- function(indices) { sub.features = random.subspace(features) data = list(&quot;indices&quot; = indices, &quot;dataset&quot; = dataset) rank.importance(sub.features, target, data, minbucket, categories) } build.level.tree &lt;- function(my.stack, top.count = 0, level = 1, model = NULL) { ################################################# ### Similar content as my.regression.tree(...) ################################################# } my.stack = queue() indices = seq(1, nrow(dataset)) categories = get.categories(target, dataset) root = split.child(indices); root$parent = 0 pushback(my.stack, root) my.model = build.level.tree(my.stack) list(&quot;model&quot; = my.model, &quot;categories&quot; = categories) } h.learner &lt;- my.rf.tree Second, our example implementation of Random Forest uses the same implementation as my.bagging(.) function; however, we keep track of data points excluded from each sample. Note that there is a tendency for data points to be selected multiple times in random sampling with replacement. On the hand, there is also a tendency for data points to be not selected. Data points that are not selected are tracked and stored in an out-of-bag structure. my.random.forest &lt;- function(features, target, data, minbucket=1, maxdepth=50, b=5, size=5) { models = list(); oobs = list() data.indices = seq(1, nrow(data)) sample.subspace = sample.int(size, b * size, replace = TRUE ) m = matrix(sample.subspace, b, size, byrow=TRUE) for (i in 1:b) { sample.indices = m[i,] samples = data[sample.indices, ] oob.indices = which(!(data.indices %in% sample.indices)) model = h.learner(features, target, samples) models[[i]] = model oobs[[i]] = oob.indices # out of bag indices } list(&quot;models&quot; = models, &quot;oobs&quot; = oobs ) } Here, our base learner for h.learner(.) references my.rf.tree(.). Now, to use our function, we resample from the mtcars train set about 100 times (b=100) with replacement, given a fixed sample size equal to the size of the train set. A unique regression tree is modeled for each sample. The output is a set of 100 uniquely trained regression trees. This set is our Random Forest ensemble model. set.seed(2020) features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) b=100 my.rf.model = my.random.forest(features, target, trainset, b=b, size=nrow(trainset)) Third, we run validation against the ensemble using our implementation, namely my.rf.validate(.). Our predictor uses h.score(.) which references our original my.predict(.) function. The prediction is measured using either SSE, MSE, or RMSE. In our case, we use MSE. my.rf.validate &lt;- function(ensemble, features, target, data, b=5) { predictions = loss = NULL n = nrow(data) train.indices = seq(1, nrow(data)) y = data[,c(target)] oob.yhat = list() oob.error = list() m = matrix(NA, n, b, byrow=TRUE) for (i in 1:n) { ic = as.character(i) for (j in 1:b) { model = ensemble$models[[j]] oob.indices = ensemble$oobs[[j]] # The existence of the data point (i) in the out-of-bag means # that the data point is not used to train the model. Thus, we # use the data point as test for prediction using the model. if (sum(oob.indices %in% i)) { test = data[i,] predicted = h.score(model, test[,c(features)], test[,c(target)]) m[i, j] = predicted$fitted.values } } } mse = rmse = sse = rep(0, b) for (j in 1:b) { tree.yhat = m[,1:j] if (!is.null(nrow(tree.yhat))) { tree.yhat = apply(tree.yhat, 1, mean, na.rm = TRUE) } idx = which(!is.na(tree.yhat)) y.pred = tree.yhat[idx] y.true = y[idx] sse[j] = sum((y.true - y.pred)^2) mse[j] = mean((y.true - y.pred)^2) rmse[j] = sqrt(mse[j]) } list(&quot;sse&quot; = sse, &quot;mse&quot; = mse, &quot;rmse&quot; = rmse) } The implementation of the validation above ensures that data points from out-of-bag are used only for tree models that are not trained with the data points. Let us use the function to validate our ensemble model. Our goal is to determine an optimal number of trees based on the least MSE. features = names(mtcars2)[which(!names(mtcars2) %in% &quot;mpg&quot;)] target = c(&quot;mpg&quot;) my.rf.cv.result = my.rf.validate(my.rf.model, features, target, trainset, b=b) Assume that we limit our ensemble with a minimum number of trees to 10. min.tree = 10 If we then plot, we see that the optimal number of trees is around 62. See Figure 10.8. cp = seq(min.tree, b) par(mfrow=c(1,2)) mse = my.rf.cv.result$mse; sse = my.rf.cv.result$sse mse = mse[(min.tree):b]; sse = sse[(min.tree):b] plot(NULL, main=&quot;My Forest Model&quot;, xlim = range(cp), ylim= range(mse), xlab=&quot;trees&quot;, ylab=&quot;MSE&quot;, col=&quot;navyblue&quot;) lines(cp, mse, col=&quot;navyblue&quot;) plot(NULL, main=&quot;My Forest Model&quot;, xlim = range(cp), ylim= range(sse), xlab=&quot;trees&quot;, ylab=&quot;SSE&quot;, col=&quot;navyblue&quot;) lines(cp, sse, col=&quot;navyblue&quot;) Figure 10.8: Weights Using our test set to predict using the optimal number of trees in the ensemble, we get a prediction even closer to the actual value. my.rf.predict &lt;- function(ensemble, x.feature, x.label, b=5) { predictions = loss = NULL for (i in 1:100) { model = ensemble$models[[i]] predicted = h.score(model, x.feature, x.label) predictions = rbind(predictions, predicted$fitted.values) } y.hat = apply(predictions,2,mean) prediction.score(x.label, y.hat) } min.mse.b = which.min(my.rf.cv.result$mse[5:100]) + 6 my.rf.result = my.rf.predict(my.rf.model, test.predictors, test.target, b=min.mse.b) c(&quot;Predicted&quot; = my.rf.result$fitted.values, &quot;Actual&quot; = test.target ) ## Predicted Actual ## 20.83 21.00 my.rf.result$loss ## Rsquare MSE MAE RMSE RMSLE ## 1 0.5 0.0295 0.1718 0.1718 0.0078 We can also use a 3rd-party library to validate our result, namely randomForest(.). Figure 10.9 shows an optimal number of trees based on least MSE. Also, the node impurity plot shows that wt, disp, and hp are top in terms of importance. library(randomForest) set.seed(2020) formula = as.formula(paste(target, &quot;~&quot;, paste(features, collapse=&quot;+&quot;))) forest.model = randomForest(formula, data=trainset, ntree=b) y = forest.model$mse; x = seq(1, length(y)) par(mfrow=c(1,2)) plot(NULL, main=&quot;Forest Model&quot;, xlim = range(x), ylim= range(y), xlab=&quot;trees&quot;, ylab=&quot;MSE&quot;, col=&quot;navyblue&quot;) lines(x, y, col=&quot;navyblue&quot;) varImpPlot(forest.model, pch=16, col=&quot;navyblue&quot;) Figure 10.9: Weights Additionally, the final model calculates and shows the importance of the features using importance(.) and varUsed(.) functions. cbind( importance(forest.model), &quot;varUSed&quot; = varUsed(forest.model)) ## IncNodePurity varUSed ## cyl 187.45 60 ## disp 243.61 201 ## hp 280.76 185 ## drat 65.19 122 ## wt 249.21 171 ## qsec 29.69 128 As shown above, we leave readers to improve my.rf.tree(.) as an exercise to generate the equivalent order of feature importance. Additionally, it helps to investigate if the Node Purity (IncNodePurity) can serve as a guide to know if we need to cut the number of features upfront before passing to randomForest(.). Finally, to validate our previous prediction, we get an almost similar result: y.hat = stats::predict(forest.model, test.predictors ) c(&quot;Predicted&quot; = y.hat, &quot;Actual&quot; = test.target ) ## Predicted.Mazda RX4 Actual ## 21 21 10.1.4 AdaBoost AdaBoost is short for Adaptive Boosting developed by Yoav Freund and Robert Schapire (1999). This Boosting ensemble algorithm introduces a few concepts, namely weighted errors, weak learners, and decision stumps. Figure 10.10: AdaBoost Unlike Bagging, there does not have to be bootstrapping of samples in Boosting. Instead, the entire train set is used to build decision stumps, which are one-split trees, each consisting of a root node with two children - using binary split. So how does AdaBoost work? The intuition behind Adaboost is commonly explained for classification. However, in regression, let us later reference a couple of papers that address regressors for boosting. The important emphasis in regression is on calculating loss function and weight. Our loss function continues to measure SSE. Additionally, it helps to salvage a few of our familiar R functions discussed in the Regression Trees section also to emphasize the process motivated by Drucker H. (1997) on the use of CART for regression. First, we use the mtcars dataset, splitting the dataset into a training set and a test set. Here, we use the entire training set as our sample set. We also generate the initial distribution: sample.set = trainset n = nrow(sample.set); w = 1/n sample.weight = rep(w, n) # Initial Proportionality # only for display purpose, see output: head(cbind(sample.set[,c(&quot;mpg&quot;, &quot;wt&quot;, &quot;disp&quot;, &quot;hp&quot;)], sample.weight)) ## mpg wt disp hp sample.weight ## Mazda RX4 Wag 21.0 2.875 160 110 0.03226 ## Datsun 710 22.8 2.320 108 93 0.03226 ## Hornet 4 Drive 21.4 3.215 258 110 0.03226 ## Hornet Sportabout 18.7 3.440 360 175 0.03226 ## Valiant 18.1 3.460 225 105 0.03226 ## Duster 360 14.3 3.570 360 245 0.03226 Also, notice that we use a separate vector to keep track of sample weights, namely sample.weight. sample.weight ## [1] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 ## [8] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 ## [15] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 ## [22] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 ## [29] 0.03226 0.03226 0.03226 Second, we build decision stumps. Each decision stump is built based on a chosen feature in our sample set. In the case of mtcars, we have ten features, namely wt, disp, hp, and on. Therefore, we expect around ten decision stumps. We evaluate each feature using the same criterion we used for the split function, split.goodness(.), to split the data points for the feature - based on the least SSE value. For the wt feature, the sample set is split into the following: categories = get.categories(&quot;mpg&quot;, datacars) data = list(&quot;indices&quot;=seq(1, nrow(sample.set)), &quot;dataset&quot;=sample.set) ft = split.goodness(features, target, data, minbucket = 2, categories)$wt ft$left.data = ft$right.data = ft$left.indices = ft$right.indices = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right sse ymean mse improve ## [1,] &quot;wt&quot; &quot;2.26&quot; 31 6 25 380.4 20.06 36.3 0.6619 ## perc indices response ntype ## [1,] 97 Integer,31 Numeric,31 &quot;node&quot; Note that the indices and response columns contain an array of 31 integers. For cyl feature, the sample set is split into the following: ft = split.goodness(features, target, data, minbucket = 2, categories)$cyl ft$left.data = ft$right.data = ft$left.indices = ft$right.indices = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right sse ymean mse improve ## [1,] &quot;cyl&quot; &quot;RLL&quot; 31 20 11 382 20.06 36.3 0.6605 ## perc indices response ntype ## [1,] 97 Integer,31 Numeric,31 &quot;node&quot; If we have to cheat a little bit, we might be able to reuse our my.regression.tree(.) function to mimic creating a one-split decision stump for each feature like so: my.wt.stump = my.tree(&quot;wt&quot;, target, sample.set , minbucket=1, maxdepth=1) my.tree_.model = my.table.tree(my.wt.stump$model ) ## N P feat split obs L R SSE ymean MSE improve perc ## 1 1 0 wt 2.26 31 6 25 380.39 20.06 36.30 0.6619 97 ## 2 2 1 wt . 6 3 3 31.05 30.07 7.43 0.3030 19 ## 3 3 1 wt . 25 9 16 159.61 17.66 13.43 0.5247 78 ## type ## 1 node ## 2 leaf ## 3 leaf my.cyl.stump = my.tree(&quot;cyl&quot;, target, sample.set , minbucket=1, maxdepth=1) my.tree_.model = my.table.tree(my.wt.stump$model) ## N P feat split obs L R SSE ymean MSE improve perc ## 1 1 0 wt 2.26 31 6 25 380.39 20.06 36.30 0.6619 97 ## 2 2 1 wt . 6 3 3 31.05 30.07 7.43 0.3030 19 ## 3 3 1 wt . 25 9 16 159.61 17.66 13.43 0.5247 78 ## type ## 1 node ## 2 leaf ## 3 leaf Third, we rank each decision stump and determine the stump with the least SSE. Our my.tree(.) is already built with the algorithm to determine which feature has the least SSE. It uses rank.importance(.) to do just that. See below: r = rank.importance(features, target, data, minbucket=1, categories) as.data.frame(r$ranks) ## feature split obs left right sse ymean mse improve ## 5 wt 2.26 31 6 25 380.4 20.06 36.3 0.6619 ## 1 cyl RLL 31 20 11 382.0 20.06 36.3 0.6605 ## 2 disp 153.35 31 12 19 415.2 20.06 36.3 0.6310 ## 3 hp 118 31 14 17 432.2 20.06 36.3 0.6159 ## 4 drat 3.75 31 18 13 641.8 20.06 36.3 0.4296 ## 6 qsec 18.41 31 19 12 736.5 20.06 36.3 0.3454 ## perc ntype ## 5 97 node ## 1 97 node ## 2 97 node ## 3 97 node ## 4 97 node ## 6 97 node As shown, wt is the top-ranked feature for the first decision stump because it has the least SSE. The other stumps are discarded. Fourth, calculate the error rate. Because stumps are one-split trees, they do not train well. For this reason, they are also called weak learners. The core premise around Boosting is to measure the errors corresponding to data points incorrectly predicted by weak learners and give a more favorable chance for these data points (once weighted) to be re-trained in the next iteration. In classification, it is easy to determine which data points are correctly performing because the target variable (response variable) is more categorically deterministic than if the target variable is continuous. That is where we introduce two Adaboost Regressors. The first one is called AdaBoost.R2, for which we reference H. Drucker (1997) on improving regressors using boosting techniques. The second one is called AdaBoost.RT for which we reference D.P. Solomatine and D. Shrestha (2004) on AdaBoost.RT - a boosting algorithm for regression problems. Here, we cover AdaBoost.R2. In H. Drucker’s paper, for AdaBoost.R2, there are three candidate choices for a loss function, conditioned on the range [0, 1]. For illustration, we use the exponential law as our loss function: \\[\\begin{align} \\epsilon_i = L_i^{(t)} = 1 - exp\\left[\\frac{-|\\hat{y_i} - y_i|}{D} \\right]\\ \\ \\ \\ \\ \\ \\ \\ where\\ D\\ = sup|\\hat{y_i} - y_i|,\\ \\ \\ \\ i\\ in\\ 1\\ ..\\ n \\end{align}\\] Here, let us use our my.predict(.) function from regression tree for h.score(.). Our example implementation of the AdaBoost.R2 Loss function is shown below. We have to predict our fitted values against the stump using the same mtcars train set as the sample set. h.score &lt;- my.predict adaboost.loss &lt;- function(features, target, stump, sample.set) { x = sample.set[,c(features)] y = sample.set[,c(target)] predicted = h.score(stump, x, y) yhat = predicted$fitted.values e = abs( yhat - y ) loss = 1 - exp( -e / max(e) ) list(&quot;yhat&quot; = yhat, &quot;loss&quot; = loss ) } (Lt = adaboost.loss(features, target, my.wt.stump, sample.set )) ## $yhat ## [1] 17.66 17.66 17.66 17.66 17.66 17.66 17.66 17.66 17.66 ## [10] 17.66 17.66 17.66 17.66 17.66 17.66 17.66 30.07 30.07 ## [19] 30.07 17.66 17.66 17.66 17.66 17.66 30.07 30.07 30.07 ## [28] 17.66 17.66 17.66 17.66 ## ## $loss ## [1] 0.36875 0.50737 0.40259 0.13346 0.05881 0.37049 0.60480 ## [8] 0.50737 0.19113 0.01910 0.15933 0.04838 0.28741 0.63212 ## [15] 0.63212 0.33483 0.27486 0.04488 0.41022 0.41076 0.25734 ## [22] 0.28741 0.45149 0.19113 0.31688 0.42887 0.04488 0.22601 ## [29] 0.24497 0.30677 0.40259 Fifth, calculate the average loss using the following equation (note that this is a weighted expectation - thus, the use of the summation): \\[\\begin{align} \\overline{L^{(t)}} = \\sum_i^n L_i^{(t)} p_i^{(t)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ p_i^{(t)}\\text{ = probability distribution.} \\end{align}\\] (Lt.avg = sum(Lt$loss * sample.weight)) ## [1] 0.3083 Sixth, our next step is to use the average weight to evaluate the confidence level of our stump. In other literature, this is also described as the amount of say, the voting power, or the weight of a stump which is expressed in the following equation: \\[\\begin{align} \\beta^{(t)} = \\left( \\frac{\\overline{L^{(t)}}}{1 - \\overline{L^{(t)}}} \\right)\\ \\ \\ \\ \\ \\text{where } \\mathbf{t} \\text{ corresponds to the iteration index} \\end{align}\\] ( Bt = Lt.avg / (1 - Lt.avg) ) ## [1] 0.4457 Seventh, we update our sample weights using the following equation. The new sample weights follow a new probability distribution (P) for the data points. \\[\\begin{align} w_i^{(t+1)} = \\frac{w_i}{z_t}^{(t)}\\beta^{(t)}{}^{\\left( 1 - L_i^{(t)}\\right)} \\ \\ \\ \\ \\ where\\ z_t\\ \\text{is a normalizer} \\end{align}\\] new.sample.weight = sample.weight * Bt * exp( 1 - Lt$loss) (normalized.weight = P = new.sample.weight / sum(new.sample.weight)) ## [1] 0.02993 0.02605 0.02893 0.03787 0.04080 0.02988 0.02363 ## [8] 0.02605 0.03574 0.04245 0.03690 0.04123 0.03246 0.02300 ## [15] 0.02300 0.03096 0.03287 0.04137 0.02871 0.02870 0.03345 ## [22] 0.03246 0.02755 0.03574 0.03152 0.02818 0.04137 0.03452 ## [29] 0.03387 0.03184 0.02893 Eight, we use this new distribution to resample our data set. To do that, we create a cumulative version to map our data points to the distribution. (cumulative.weight = P = cumsum(normalized.weight)) ## [1] 0.02993 0.05598 0.08491 0.12278 0.16358 0.19346 0.21709 ## [8] 0.24314 0.27889 0.32134 0.35824 0.39947 0.43194 0.45493 ## [15] 0.47793 0.50889 0.54177 0.58314 0.61185 0.64055 0.67400 ## [22] 0.70647 0.73402 0.76976 0.80128 0.82946 0.87084 0.90536 ## [29] 0.93923 0.97107 1.00000 We collect a new sample set based on the proportionality, using random numbers between 0 and 1. The idea is to draw N random numbers, each mapped according to the corresponding cumulative weights of the data point. Because some weights are larger than the others, some random numbers have a higher probability of falling under the larger weights. In essence, this is how we favor samples that are performing poorly in the sample set and thus give them a higher probability of being re-sampled in the next iteration. set.seed(142) rnum = sort(runif(n=n, min=0, max=1)) head(cbind(sample.set[,c(&quot;mpg&quot;, &quot;wt&quot;, &quot;disp&quot;, &quot;hp&quot;)], cumulative.weight, rnum)) ## mpg wt disp hp cumulative.weight ## Mazda RX4 Wag 21.0 2.875 160 110 0.02993 ## Datsun 710 22.8 2.320 108 93 0.05598 ## Hornet 4 Drive 21.4 3.215 258 110 0.08491 ## Hornet Sportabout 18.7 3.440 360 175 0.12278 ## Valiant 18.1 3.460 225 105 0.16358 ## Duster 360 14.3 3.570 360 245 0.19346 ## rnum ## Mazda RX4 Wag 0.0509 ## Datsun 710 0.1469 ## Hornet 4 Drive 0.1766 ## Hornet Sportabout 0.2804 ## Valiant 0.3359 ## Duster 360 0.4323 Notice above that the rnum is not correctly mapped yet to the corresponding cumulative weights. To do that, we use the below example implementation as part of resampling the distribution: sampling.distribution &lt;- function(sample.set, sample.weight, seed=142) { n = nrow(sample.set) indices = rep(0, n) cumulative.weight = cumsum(sample.weight) set.seed(seed) random.number = runif(n=n, min=0, max=1) for (i in 1:n) { indices[i] = which(random.number[i] &lt; cumulative.weight)[1] } sample.set[indices,] } head(sampling.distribution(sample.set, normalized.weight, 1)) ## mpg cyl disp hp drat wt qsec ## Merc 280 19.2 6 167.6 123 3.92 3.440 18.30 ## Merc 450SL 17.3 8 275.8 180 3.07 3.730 17.60 ## Honda Civic 30.4 4 75.7 52 4.93 1.615 18.52 ## Ferrari Dino 19.7 6 145.0 175 3.62 2.770 15.50 ## Merc 240D 24.4 4 146.7 62 3.69 3.190 20.00 ## Ford Pantera L 15.8 8 351.0 264 4.22 3.170 14.50 Ninth, we repeat the process starting from step two by processing the newly re-sampled sample set with new weights and building the next decision stump. Alternatively, we can use our example implementation of AdaBoost Regressor, pulling together all the steps above into one function. Our stopping criterion is based on an average loss larger than 0.5 (Drucker H. 1997). h.learner &lt;- my.tree my.adaboost.R &lt;- function(features, target, sample.set, estimators=10) { stumps = list(); betas = list(); yi = list() n = nrow(sample.set) w = 1/n sample.weight = rep(w, n) # Proportionality for (t in 1:estimators) { data = sampling.distribution(sample.set, sample.weight) my.stump = h.learner(features, target, data, minbucket=1, maxdepth=1) predicted = adaboost.loss(features, target, my.stump, data) et = sum(predicted$loss * sample.weight) # avg/weighted loss Bt = et / (1 - et) if (is.nan(Bt) || et &gt;= 0.5) break stumps[[t]] = my.stump betas[[t]] = Bt yi[[t]] = predicted$yhat sample.weight = sample.weight * Bt^(1 - predicted$loss) sample.weight = sample.weight / sum(sample.weight) sample.set = data } list(&quot;stumps&quot; = stumps, &quot;betas&quot; = betas, &quot;yi&quot; = yi) } Here, our base learner h.learner(.) references our original regression function, namely my.tree(.). Note in boosting that we have a choice of resampling from our original sample set or a choice of re-weighing our original sample set instead. In our implementation, we have chosen to perform resampling. The choice of re-weighing is illustrated in a later section in the context of AdaBoost Classifier. set.seed(142) sample.set = trainset my.adaboost.model = my.adaboost.R(features, target, sample.set, estimators=100) # example of first stump my.table.model = my.table.tree(my.adaboost.model$stumps[[1]]$model) ## N P feat split obs L R SSE ymean MSE improve % ## 1 1 0 wt 2.3925 31 10 21 308.30 19.86 56.54 0.8241 97 ## 2 2 1 disp . 10 8 2 17.59 29.75 8.91 0.8027 31 ## 3 3 1 disp . 21 16 5 71.23 15.15 10.44 0.6750 66 ## type ## 1 node ## 2 leaf ## 3 leaf my.adaboost.model$betas[[1]] # example of corresponding beta ## [1] 0.3368 my.adaboost.model$yi[[1]] # example of corresponding y (ith=1) ## [1] 15.15 15.15 15.15 29.75 29.75 29.75 15.15 15.15 15.15 ## [10] 15.15 29.75 15.15 15.15 29.75 15.15 29.75 15.15 29.75 ## [19] 29.75 15.15 15.15 15.15 29.75 15.15 15.15 15.15 15.15 ## [28] 29.75 15.15 15.15 15.15 Finally, for a model prediction for Adaboost Regression, we use weighted median (Drucker H. 1997). \\[\\begin{align} h_f = inf\\left\\{y \\in Y : \\sum_{t:h_t \\le y} \\log_e\\left(\\frac{1}{\\beta_t}\\right) \\ge \\frac{1}{2} \\sum_t \\log_e \\left( \\frac{1}{\\beta_t}\\right) \\right\\} \\end{align}\\] Our example implementation of the weighted median is like so: ln &lt;- function(x) { log(x, exp(1))} # exp(1) = 2.718282 my.adaboost.predict &lt;- function( features, target, adaboost.model, sample.set) { n = nrow(sample.set); b = 0; hf = rep(0, n) estimators = length(adaboost.model$stumps) B = adaboost.model$betas for (t in 1:estimators) { b = b + ln (1/ B[[t]]) } b = 1/2 * b for (i in 1:n) { for (t in 1:estimators) { stump = adaboost.model$stumps[[t]] yi = adaboost.model$yi[[t]] # predicted y-hat (yi) predicted = adaboost.loss(features, target, stump, sample.set[i,]) ht = predicted$yhat s = 0 for (p in 1:length(yi)) if (ht &lt;= yi[p]) s = s + ln(1/B[[t]]) if (s &gt;= b) hf[i] = ht } } hf } yhat = my.adaboost.predict(features, target, my.adaboost.model, testset) cbind(&quot;Predicted&quot; = yhat, &quot;True&quot; = test.target) ## Predicted True ## [1,] 21.42 21 As an exercise, we leave readers to play around with different random seeds for the random re-sampling, namely sampling.distribution(.), and see how it affects prediction performance. We leave readers to investigate G. Ridgeway et al.’s paper in which a variant of the Adaboost regressor uses probabilistic classifier and probabilistic prediction, allowing the transformation of a sample set into one with a categorical target variable. Let us discuss Gradient Boost next which provides an alternative technique to regression. 10.1.5 Gradient Boost Gradient Boost, also called Gradient Boost Machine (GBM), was developed by Jerome H. Friedman (1999b). The idea originated from Leo Breiman (1999), who came to realize, by observing Adaboost, that the algorithm characterizes gradient descent in function space, particularly in the loss function space, e.g.: \\[\\begin{align} \\underbrace{\\nabla_{F(x)} Lik(y, F(x))}_{\\text{negative gradient}} = -\\left[\\frac{\\partial Lik(y, F(x))}{\\partial F(x)}\\right] \\end{align}\\] \\[\\begin{align} where\\ \\ \\ \\underbrace{Lik(y, F(x))}_{\\text{loss function}} = \\frac{1}{2}\\left(y -\\hat{y}\\right)^2 \\ \\ and\\ \\ \\underbrace{F(x) = \\hat{y}}_{\\text{y-hat}}. \\end{align}\\] Similar to Adaboost, Gradient Boost is also sequential in processing an ensemble of tree models as it learns from one model to the next. In addition, it introduces a coordinate-wise gradient-based learning technique using a learning rate denoted by the eta symbol \\(\\eta\\). See Figure 10.11. Figure 10.11: Gradient Learning The idea is to start with an arbitrary initial value (usually the sample average). Then, the goal is to calculate an optimal path to the target value such that the initial value eventually reaches (or reasonably matches) the target value iteratively at step-wise increments. So how does Gradient Boost work? First, let us use the same mtcars training set as before in which mpg is our dependent variable (our target variable), and then generate our initial predicted value. Here, our initial estimate is merely the average of our target values derived from the following equation: \\[\\begin{align} \\begin{array}{ll} F_0(X) &amp;= \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{i=1}^n Lik(y_i, \\gamma) \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ \\ Lik(y_i, \\gamma)= \\frac{1}{2} \\left(y_i - \\gamma\\right)^2\\\\ &amp;= \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{i=1}^n \\frac{1}{2} \\left(y_i - \\gamma\\right)^2 = \\frac{1}{n}\\sum_{i=1}^n y_i = \\bar{y} \\end{array} \\label{eqn:eqnnumber404} \\end{align}\\] The symbol gamma \\((\\gamma)\\) represents the predicted value. At the same time, the \\(\\mathbf{F_m(x)}\\) represents the best weak-learner model by finding the minimal gamma. The loss function is required to be differentiable such that by derivation, we can calculate the value of gamma. For example, given target values (mpg) from the mtcars set, we perform a derivative calculation to get the following equivalent average: \\[\\begin{align*} \\frac{1}{2}(21.0 - \\gamma)^2\\ +\\ ...\\ +\\ \\frac{1}{2}(21.4 - \\gamma)^2 &amp;= 0&amp;\\\\ (21.0 - \\gamma)(-1)\\ +\\ ...\\ +\\ (21.4 - \\gamma)(-1) &amp;= 0&amp;\\leftarrow \\text{(by derivation)}\\\\ \\gamma = (21.0\\ +\\ ...\\ +\\ 21.4)/32 &amp;= 20.06&amp;\\leftarrow\\text{(base model)}\\ F_0(x) = \\bar{y}\\\\ \\end{align*}\\] sample.set = trainset sample.predictors = sample.set[, c(features)] sample.target = sample.set[, c(target)] (target.mean = mean(sample.target)) ## [1] 20.06 Second, we calculate the error between our initial estimated target and the actual target. To calculate the error, we use the following formula, for \\(i = 1\\ ..\\ n\\): \\[\\begin{align} r_{im} &amp;= - \\left[\\frac{\\partial Lik(y_i, F_{m-1}(x_i))}{\\partial F_{m-1}(x_i)}\\right] = - \\frac{\\partial}{\\partial(\\hat{y})} \\left[\\frac{1}{2}\\left(y -\\hat{y}\\right)^2 \\right] = (y - \\hat{y})\\\\ \\nonumber \\\\ &amp;where\\ \\ \\ \\ F_{m-1}(x_i) \\equiv F_0(x_i) \\equiv \\hat{y} \\equiv \\gamma \\end{align}\\] By partial derivation of the loss function with respect to \\(\\hat{y}\\), the equation is simplified to the following simple residual equation: \\(r_{im} = (y - \\hat{y})\\). (target.residual = sample.target - target.mean) # rim = (y - y.hat) ## [1] 0.9387 2.7387 1.3387 -1.3613 -1.9613 -5.7613 4.3387 ## [8] 2.7387 -0.8613 -2.2613 -3.6613 -2.7613 -4.8613 -9.6613 ## [15] -9.6613 -5.3613 12.3387 10.3387 13.8387 1.4387 -4.5613 ## [22] -4.8613 -6.7613 -0.8613 7.2387 5.9387 10.3387 -4.2613 ## [29] -0.3613 -5.0613 1.3387 Let us call this error our target residual or pseudo residual. In the context of gradient descent in function space, this initial target residual is our starting point, and the expectation is that the residual decreases as we fit more regression models. Fitting regression to residual is indeed a concept we have not discussed thus far - an idea introduced in boosting in the context of regression. It becomes apparent next. Third, note that boosting is agnostic in the choice of base regressor to use. In our case, like Adaboost.R2, we continue to choose our implementation of a regression tree for Gradient Boost, namely my.tree(.). Unlike Adaboost.R2, we do not have to build stumps. Our trees can grow to decent depths. There is no need to grow the trees to the maximum. A decent depth in our case is around five - but this is not to be taken as a hard rule. comprehensive.dataset = datacars sample.set = cbind(&quot;mpg&quot; = target.residual, sample.predictors) my.gbm.model = my.tree(features, target, sample.set, minbucket=1, maxdepth=5) We use the target residual as our dependent variable in building a tree model. Here, we fit our tree model to the target residuals. In the process, the leaf nodes are formed containing the corresponding target residuals for this model. For every leaf that contains multiple target residuals, we take the average of those residuals, which becomes the target value of each leaf. While that is already done in our my.tree(.), it helps to show and explain the equation in the context of the GBM algorithm: \\[\\begin{align} \\gamma_{jm} = \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{xi \\in L_{j}} Lik(y_i, F_{m-1}(x_i) + \\gamma), \\end{align}\\] Note that \\(\\mathbf{F_{m-1}(x)}\\) represents our regressor model - in this early step, it is the base model, namely \\(\\mathbf{F_0(x)}\\), which also happens to be our initial target residual. If we follow the same idea discussed in the first step by derivation and minimization with respect to gamma, we find that each leaf (indexed by j) in the model (indexed by m) produces an average gradient. It is thus essential to note that, in our specific implementation of the regression tree, our my.tree(.) function indeed already performs a more straightforward process of averaging residuals local to a leaf (or a region). For example, assume that a couple of residuals happen to fall under the leaf (j) of a tree model (m); therefore, we have the following: \\[\\begin{align} \\frac{1}{2}(y_1 - (F_0(x_1) + \\gamma_{jm} ))^2\\ + \\frac{1}{2}(y_2 - (F_0(x_2) + \\gamma_{jm} ))^2&amp;= 0&amp;\\\\ \\frac{1}{2}(y_1 - (20.06 + \\gamma_{jm} ))^2\\ + \\frac{1}{2}(y_2 - (20.06 + \\gamma_{jm} ))^2&amp;= 0&amp;\\\\ (y_1 - 20.06 - \\gamma_{jm})(-1)\\ + (y_2 - 20.06 - \\gamma_{jm})(-1) &amp;= 0&amp;\\leftarrow \\text{(by derivation)}\\\\ \\gamma_{jm} = [(y_1\\ - \\ 20.06) + (y_2\\ - \\ 20.06)]/2 &amp;&amp;\\leftarrow\\text{(score = leaf average)} \\end{align}\\] Each leaf across all models thus holds such corresponding \\(\\gamma_{jm}\\) to represent the score as a base learner in ensemble models. Otherwise, it represents the predicted/estimated value so that if we are to predict an unseen data, we flow in the direction of the average gradient from the root node of the tree model, producing a predicted pseudo-residual like so: # leaf average (gamma) (residual.output1 = my.predict(my.gbm.model, sample.predictors, target.residual)$fitted.values) ## [1] 1.2637 2.7387 1.2637 -1.1113 -1.9613 -5.9613 4.3387 ## [8] 2.7387 -0.8613 -2.2613 -4.1413 -4.1413 -4.1413 -9.6613 ## [15] -9.6613 -5.9613 12.3387 10.3387 13.8387 1.2637 -4.1413 ## [22] -4.1413 -5.9613 -1.1113 7.2387 5.9387 10.3387 -4.6613 ## [29] -0.3613 -4.6613 1.2637 We use the gradient boost tree model, namely my.gbm.model, to predict the next residual. Note hereafter that we expect the predicted output from the fit to be an estimated residual - and that is true for all subsequent gradient boost trees that we build. We use the my.predict(.) function to produce the expected residual output. Alternatively, we can combine all that and formulate our example implementation of a weak-learner regressor. We use the new target residual to re-construct our sample set, replacing the previous target residual with the new residual as the next target. Our base learner uses my.tree(.) and my.predict(.) from Regression Tree fitting a model to residual. h.learner &lt;- my.tree h.score &lt;- my.predict weak.learner &lt;- function(features, target, x, y) { data = cbind(y, x) # target residual (y) names(data)[1] = target model = h.learner(features, target, data, minbucket=1, maxdepth=2) (pseudo.res = h.score(model, x, y)$fitted.values) list(&quot;model&quot; = model, &quot;predicted.residual&quot; = pseudo.res) } h = weak.learner # hypohetical estimator function wk.model = h(features, target, sample.predictors, target.residual) residual.output1 = wk.model$predicted.residual # leaf average (gamma) Fourth, with our predicted residual, we generate the next new model (\\(F_{m}(x)\\)) for our ensemble prediction using the following equation: \\[\\begin{align} F_1(X) = F_0(X) + \\eta \\times r_1\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\eta \\text{ is learning rate, and } r_1 = h_1(X) \\end{align}\\] For illustration, we choose our learning rate to be 0.01. l.r = 0.01 # learning rate (scaling constant) (predicted.target = target.mean + l.r * residual.output1) ## [1] 20.07 20.07 20.07 20.01 20.07 20.01 20.07 20.07 20.07 ## [10] 20.07 20.01 20.01 20.01 20.01 20.01 20.01 20.19 20.15 ## [19] 20.19 20.07 20.01 20.01 20.01 20.01 20.15 20.15 20.15 ## [28] 20.01 20.07 20.01 20.07 Fifth, we then calculate the next target residual as before. Here, we repeat the process from step 2. (target.residual = sample.target - predicted.target) ## [1] 0.9301 2.7301 1.3301 -1.3117 -1.9699 -5.7117 4.3301 ## [8] 2.7301 -0.8699 -2.2699 -3.6117 -2.7117 -4.8117 -9.6117 ## [15] -9.6117 -5.3117 12.2078 10.2541 13.7078 1.4301 -4.5117 ## [22] -4.8117 -6.7117 -0.8117 7.1541 5.8541 10.2541 -4.2117 ## [29] -0.3699 -5.0117 1.3301 Sixth, we build our next weak learner model using our new target residual, similar to step 3. wk.model = h(features, target, sample.predictors, target.residual) residual.output2 = wk.model$predicted.residual Seventh, and just as before, we calculate our new estimated target following step 4, but this time, we add the first predicted residual, scaled using a learning rate, with the second predicted residual, also scaled with the same learning rate. With that, we obtain the following: l.r = 0.01 # learning rate (scaling constant) (predicted.target = target.mean + l.r * residual.output1 + l.r * residual.output2) ## [1] 20.05 20.10 20.05 19.99 20.05 19.95 20.10 20.10 20.05 ## [10] 20.05 19.99 19.99 19.99 19.95 19.95 19.95 20.30 20.25 ## [19] 20.30 20.10 19.99 19.99 19.95 19.99 20.25 20.18 20.25 ## [28] 19.95 20.05 19.95 20.10 The predicted.target is our ensemble prediction, derived as the summation of the residual outputs of our weak learners and is expressed as such: \\[\\begin{align} F_2(X) = F_1(X) + \\eta \\times r_2\\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\eta \\text{ is learning rate, and } r_2 = h_2(X) \\end{align}\\] Notice that each estimated target gets closer in the direction of the true target values. Finally, we iterate the entire process starting from step 3 until a stopping criterion. All that can be summarized using the below Gradient Boost algorithm (Friedman J. 1999b): \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in \\mathbb{R}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{number of machines}: M\\\\ \\ \\ \\ \\text{a loss function}: Lik(y, \\hat{y}) = \\frac{1}{2}(y - \\hat{y})^2\\\\ \\ \\ \\ \\text{a weak-learner base regressor}: h(x)\\\\ \\ \\ \\ \\text{a learning rate}: \\eta\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ F_0(X) = \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{x=1}^n Lik(y_i, \\gamma ) \\ \\ \\ &amp;\\text{(initialize)}\\\\ \\ \\ \\ \\text{loop}\\ m\\ in\\ 1:\\ M \\\\ \\ \\ \\ \\ \\ \\ \\ r_{im} = -\\left[\\frac{\\partial Lik(y_i, F(x_i))}{\\partial F(x_i)}\\right]_{F(X) = F_{m-1}(X)} = y - \\hat{y}&amp;\\text{(pseudo-residuals)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{fit model using } h_m(\\{x_i,r_{im}\\}_{i=1}^n)\\ &amp;\\text{(fit model to pseudo-residual)} \\\\ \\ \\ \\ \\ \\ \\ \\ \\text{for each}\\ (L)_j \\in (T)_m\\ &amp;\\text{where (L)eaf and (T)ree} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\gamma_{jm} = \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{x \\in L_{jm}} Lik(y, F_{m-1}(x) + \\gamma) &amp; \\text{(leaf residual)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\ \\ \\ \\ F_m(x) = F_{m-1}(x) + \\begin{cases} \\eta\\ h_m(x) \\\\ \\gamma_{m}\\ h_m(x) \\end{cases} &amp; \\begin{cases} \\text{regression tree} \\\\ \\text{otherwise, other regressors} \\end{cases} \\\\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\text{Output }F_m(x) \\end{array} \\] And below is the example implementation of the algorithm: my.gbm &lt;- function(features, target, data, machines=5, learning.rate=0.01) { mse &lt;- function(x) { sum(x^2) } x = data[,c(features)] y = data[,c(target)] eta = learning.rate F0 = y.hat = y.mean = mean(y) # Initial residual (target.mean) tot.predicted = matrix(0, length(y), machines, byrow=TRUE) residuals = rep(0, machines) old.mse = Inf; model = NULL for (m in 1:machines) { residual = y - y.hat # negative gradient (r.im) (target.residual) wk.model = h(features, target, x, residual) tot.predicted[,m] = wk.model$predicted.residual hms = apply(tot.predicted, 1, sum) y.hat = y.mean + eta * hms # Fm = F0 + n * h_1(x) ... if (old.mse == mse( y - y.hat )) break residuals[m] = mse( y - y.hat ) old.mse = residuals[m] } # final predicted target ( y.hat is Fm ) list(&quot;mse&quot; = residuals, &quot;y.hat&quot; = y.hat, &quot;model&quot; = wk.model$model ) } Let us use the implementation to learn a trainset. library(rpart) sample.set = trainset ensemble.model = my.gbm(features, target, sample.set, machine=50, learning.rate=1) Figure 10.12: Gradient Boost (Regression) If we compare the variance between the target and the estimate, we obtain the following: summary(sample.target) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.4 15.3 19.2 20.1 22.8 33.9 summary(ensemble.model$y.hat) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 10.4 15.3 19.2 20.1 22.8 33.9 In our implementation of Gradient Boost, our weak learner uses CART - regression tree. However, we have not allowed a way to control the minbucket and maxdepth hyperparameters, let alone calibrate the complexity parameter. They are made constant. The following section introduces an algorithm that allows us to adjust knobs to calibrate our models. Note that there are other variants of Boosting techniques, namely LogitBoost, BrownBoost, CatBoost, LightGBM, and LPBoost. We leave readers to investigate other boosting variants. 10.1.6 XGBoost To recognize XGBoost as the dream algorithm for most ML enthusiasts is probably a true statement - after all, it is also recognized to win competitions. XGBoost stands for Extreme Gradient Boost, and Tianqi Chen developed it to assess every nook and cranny of Gradient Boost. The idea is to implement a roster of tools and techniques to address optimizing the Gradient Boost algorithm at scale, along with many other features it offers. For example, we use a CART booster (or learner) for Gradient Boost such that the basic hyperparameters to tune are minbucket, minsplit, maxdepth, complexity parameter, and many others, including a choice of sampling (and sub-sampling). XGBoost finds the means to calibrate those hyperparameters and use Regularization such as LASSO or Ridge regularization in Linear Regression. For another example, Gradient Boost relies on sequentially boosting weak learners. XGBoost finds the means to parallelize the process. On top of that, it also looks into efficiently utilizing computational resources by handling caching optimization and parallel processing in a distributed manner. To illustrate, let us use a third-party R implementation of the XGBoost called xgboost(.) made available through open source. library(xgboost) m.trainset = apply( as.matrix(trainset), 2, as.numeric) xgboost.model = xgboost(data = m.trainset, label = trainset$mpg, verbose = 0, max.depth = 2, eta = 1, nthread = 2, nrounds = 50, objective = &quot;reg:squarederror&quot;, eval_metric=&quot;rmse&quot;) The algorithm offers a list of objective options (based on xgboost R documentation): \\[ \\begin{array}{lll} \\text{reg:squarederror} &amp; \\text{reg:squaredlogerror} &amp; \\text{reg:logistic} \\\\ \\text{binary:logistic} &amp; \\text{binary:hinge} &amp; \\text{count:poisson} \\\\ \\text{survival:cox} &amp; \\text{survival:aft} &amp; \\text{multi:softmax} \\\\ \\text{multi:softprob} &amp; \\text{rank:pairwise} &amp; \\text{rank:ndcg} \\\\ \\text{rank:map} &amp; \\text{reg:gamma} &amp; \\text{reg:tweedie} \\\\ \\end{array} \\] Correspondingly, it also offers the following evaluation metrics, depending upon the chosen objective function: \\[ \\begin{array}{lll} \\text{rmse} &amp; \\text{rmsle} &amp; \\text{mae} \\\\ \\text{mape} &amp; \\text{logloss} &amp; \\text{mlogloss} \\\\ \\text{error} &amp; \\text{auc} &amp; \\text{ndcg} \\\\ \\text{map} &amp; \\text{poisson-nloglik} &amp; \\text{gamma-nloglik} \\\\ \\text{cox-nloglik} &amp; \\text{gamma-deviance} &amp; \\text{tweedie-noglik} \\\\ \\end{array} \\] See Figure 10.13 for the plot of the RMSE result. Figure 10.13: XGBoost (Regression) We leave readers to investigate the many features and combinations of hyperparameters used by XGBoost to understand its opportunities and potential. The key takeaway in this section is that XGBoost can be viewed as an implementation of some of the theories we covered in previous chapters, namely Numerical Linear Algebra, Probability and Distribution, Statistical Computation, and Bayesian Computation. Additionally, we leave readers to investigate and evaluate LightGBM. 10.1.7 Generalized Linear Modeling (GLM) In conventional linear regression, it is common to fit models on datasets in which the continuous response variable, namely y, is characterized by a linear function. Such a response variable commonly follows a normal distribution: \\[\\begin{align} f(x) = y = x^T\\beta\\ \\ \\ \\ \\rightarrow where\\ y \\ \\sim\\ \\mathcal{N}(\\mu = x^TB, \\text{sd} =\\sigma)\\ \\ \\ \\ \\end{align}\\] where x represents dependent covariates and \\(\\beta\\) represents weights or coefficients. In other cases, the response variable may be characterized by some non-linear function following an exponential family of distributions, e.g., Binomial, Poisson, Bamma, and on. To accommodate such cases, McCullagh P. and Nelder J.A. (1983) popularized the concept of General Linear Models (GLM). For example, GLM supports a response variable that may follow a Binomial distribution like so: \\[\\begin{align} f(x) = \\mu = x^T\\beta\\ \\ \\ \\ \\rightarrow where\\ \\mu \\ \\sim\\ \\mathcal{Bin}(n, \\rho)\\ \\ \\ \\ \\end{align}\\] or it may follow an Exponential distribution like so: \\[\\begin{align} f(x) = \\mu = x^T\\beta\\ \\ \\ \\ \\rightarrow where\\ \\mu \\ \\sim\\ \\mathcal{Exp}(\\lambda)\\ \\ \\ \\ \\end{align}\\] or even follow a Poission distribution like so: \\[\\begin{align} f(x) = \\mu = x^T\\beta\\ \\ \\ \\ \\rightarrow where\\ \\mu \\ \\sim\\ \\mathcal{Pois}(\\lambda)\\ \\ \\ \\ \\end{align}\\] Each of these exponential distributions tends towards a central tendency. Our expressions above use the mean (\\(\\mu\\)) to indicate that every point in y represents the center or the mean (\\(\\mu\\)) of a family of distribution. For example, the mean (\\(\\mu\\)) of Normal distribution in Linear Regression is simply the fitted values (\\(\\hat{y_i}\\)). See Point-Estimate vs Stochastic Estimate Figure in Chapter 7 (Bayesian Computation I) under Bayesian Inference Section. On the other hand, the mean of Binomial distribution in Logistic Regression is formed by the following sigmoid equation: \\[\\begin{align} \\mu = P(y=1|x) = \\frac{exp(y)}{1 + exp(y)} \\end{align}\\] This mean (\\(\\mu\\)) creates what we see as the Sigmoid curve, which is non-linear. GLM transforms the non-linear relationship between the independent variables and the mean (\\(\\mu\\)) into one that is linear by using a link function denoted as g(.). In the case of logistic regression, the link function is expressed as: \\[\\begin{align} g(\\mu) = \\frac{\\mu}{1 - \\mu} \\end{align}\\] For other exponential distributions, see Figure 10.14. Figure 10.14: Generalized Linear Models Let us list a few exponential distributions that are common in many literatures. See Table 10.1. Table 10.1: General Linear Model Family Link Name Mean Function Link Function Support Gaussian Identity \\(\\mu = y = x^T\\beta\\) \\(\\mu\\) \\((-\\infty, +\\infty)\\) Inverse Gaussian Inverse Squared \\(\\mu = (x^T\\beta)^{-1/2}\\) \\(\\mu^{-2}\\) \\((0, +\\infty)\\) Bernoulli Logit (Logistic Unit) \\(\\mu = \\frac{exp(x^T\\beta)}{1+exp(x^T\\beta)}\\) \\(\\log_e(\\mu/(1 - \\mu)\\) [0,1] Binomial Logit \\(\\mu = \\frac{exp(x^T\\beta)}{1+exp(x^T\\beta)}\\) \\(\\log_e(\\mu/(1 - \\mu)\\) [0,N] Binomial Probit (Probab. Unit) \\(\\mu = \\phi(x^T\\beta)\\) \\(\\phi^{-1}(\\mu)\\) [0,1] Poisson Natural Log \\(\\mu=exp(x^T\\beta)\\) \\(\\log_e(\\mu)\\) \\([0,+\\infty)\\) Exponential Negative Inverse \\(\\mu=-(x^T\\beta)^{-1}\\) \\(-\\mu^{-1}\\) \\((0,+\\infty)\\) In the next two sections, we demonstrate the use of GLM using the glm(.) function with which we model a family of distributions specific to Exponential Distributions such as Binomial and Poisson Distribution. We also introduce Binary Classification through Logistic Regression and Poisson Regression. 10.1.8 Logistic Regression (GLM) In Linear Regression, when fitting a linear model, we optimize the \\(\\beta\\)s parameters such that the corresponding response variable, the fitted (\\(\\hat{y}\\)) values, are derived - forming a fitted line. We commonly use Sum Squared Error (SSE) as our loss function. That is because the response variable is expected to be continuous. We fit a linear model to find the least square - we call this minimizing the loss function. In Logistic Regression (also called Logit Regression), when fitting a logistic model, similar to linear regression, we also optimize the \\(\\beta\\) parameters such that the corresponding response variable, the fitted \\(\\hat{y}\\) values (also called logit values), are derived - also forming a fitted line. Note that it may look as if the response variable is linearly associated with the predictor variables. This is made possible only because the response variable holds the logit-transformed values (logit values) of its binomial probabilities. In other words, the response variable - assumed to be dichotomous - goes into two transformations to achieve a linear association with the predictor variables. For example, we start with the common equation below: \\[\\begin{align} Y = f(X) = \\beta^TX \\end{align}\\] We then transform Y into a binomial probability with range [0, 1] using the inverse logit equation: \\[\\begin{align} \\text{logit}^{-1}\\left(\\beta^TX\\right) = \\frac{exp(\\beta^TX)}{1 + exp(\\beta^TX)} \\end{align}\\] That is derived from the fact that the invert logit is a binomial probability of observing a successful event and can also be expressed as such: \\[\\begin{align} \\mu = P(y = 1|x) = \\frac{exp(y)}{1 + exp(y)} \\ \\ \\ \\ \\ \\ \\text{where } \\mu \\sim Bin(n,p)\\ \\ \\ and\\ \\ \\ \\ Y = f(X) = \\beta^TX \\end{align}\\] Here, the rho (\\(\\rho\\)) symbol indicates the probability of success, given n number of trials. We see a sigmoid curve if we plot the binomial probability. Because a sigmoid curve remains to be non-linear in form, our next step is to use log-odds to transform the probability into log-odds unit using the following equation: \\[\\begin{align} g(\\mu) = \\text{logit}(\\mu) = \\log_e \\left(\\frac{\\mu}{1 - \\mu}\\right) = \\beta^TX = \\beta_0 + \\beta_1 x_1 + ... + \\beta_n x_n \\end{align}\\] where \\(g(\\mu)\\) is a link function. In terms of goodness of fit, it is worth noting that when we fit a logistic model, we use the following inverse logit function below, in which we obtain a p-hat (\\(\\hat{p}\\)) for logistic regression similar to the common y-hat (\\(\\hat{y}\\)) for linear regression. Note that our p-hat (\\(\\hat{p}\\)) is also equivalent to the mean (\\(\\mu\\)), pointing to the fact that our binomial distribution - geometrically presented as a sigmoid curve - has a central tendency such that every point along the curve ends up to be a mean (\\(\\mu_i\\)), center to the noise model. \\[\\begin{align} \\hat{p} = \\mu = \\text{logit}^{-1}\\left(\\beta^TX\\right) \\end{align}\\] Additionally, let us not forget that \\(\\hat{p}\\) is the estimated probability of an event being successful. Therefore, to account for events that are not successful, we have the following equation: \\[\\begin{align} \\hat{q} = 1 - \\hat{p} \\end{align}\\] It becomes apparent when we compute for the MLE for our loss function. As always, our goal in fitting a model is to optimize the \\(\\beta\\) parameters. It is achieved by using maximum likelihood estimate (MLE) as our loss function because of the non-linearity of our logistic model. However, we use its counterpart negative log-likelihood (NLL) instead to keep the common notion of minimizing a loss function. When using MLE, we tend to maximize the likelihood estimate - similar to maximizing the loss function, which may not be sensible. If we use NLL, it is natural to aim for the least NLL; thus, minimizing the loss function. \\[\\begin{align} \\hat{\\beta} = \\text{arg}\\ \\underset{\\beta}{\\text{min}}\\ \\underbrace{\\mathcal{L}\\text{oss}\\left\\{ - \\sum_{i=1}^n \\left[ \\underbrace{y_i \\log_e \\left(\\mu\\right)}_{\\text{if Y=1}} + \\underbrace{ (1 - y_i) \\log_e \\left( \\text{logit}\\left(\\frac{\\mu}{ 1 - \\mu}\\right)\\right)}_{\\text{if Y=0}} \\right] \\right\\} }_{\\text{NLL}} \\end{align}\\] where: \\(\\mu = \\text{logit}^{-1}\\left(\\beta^TX\\right)\\). The equation for the loss function above can be written in a simpler form: \\[\\begin{align} \\hat{\\beta} = \\text{arg}\\ \\underset{\\beta}{\\text{min}}\\ \\mathcal{L}\\text{oss}\\left\\{ - \\sum_{i=1}^n \\left[ y_i \\log_e \\left(\\mu\\right) + (1 - y_i) \\log_e \\left( 1 -\\mu \\right) \\right] \\right\\} \\end{align}\\] As long as the loss function is differentiable, we should be able to perform partial derivatives with respect to each \\(\\beta_j\\) parameter like so: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\beta_j} =- \\sum_{i=1}^n \\left[ y_i \\log_e \\left(\\mu\\right) + (1 - y_i) \\log_e \\left( 1 -\\mu \\right) \\right] = 0 \\end{align}\\] We leave readers to investigate the use of grid search, or Monte Carlo, to simulate different combinations of the \\(\\beta\\) parameters. In reference to Bayesian Computation and Deep Neural Network chapters, we can also look for other optimization strategies, namely Laplace Approximation and Gradient Descent or Newton Raphson from the Linear Algebra chapter. Note that the NLL Loss function is also termed the Cross-Entropy Loss Function in Deep Neural Network. In fitting a model, each combination of \\(\\beta\\) parameters corresponds to a line geometrically. As we adjust our \\(\\beta\\) parameters, the line that is formed becomes a reference model to obtain our yhat (\\(\\hat{y}\\)) - we can call this logit-hat. However, we do not use the logit-hat (\\(\\hat{y}\\)) to calculate the Least-Squares. Instead, we convert the logit-hat to its equivalent estimated probability - the p-hat - using the inverse logit function. It is this probability for which we minimize our NLL loss function. Once we find the least NLL, then the corresponding \\(\\beta\\) parameters are the model components for the final fit. Here we use plogis(.) for the conversion. See also the equivalent inverse.logit(.) in Statistical Computation chapter. Now, to do that, the following helper functions are required: ln &lt;- function(x) { log(x, exp(1))} mle &lt;- function(x) { sum(ln(x)) } nll &lt;- function(x) { -mle(x) } We can then convert the logit-hat (\\(\\hat{p}\\)) to its inverse-logit form - the p-hat (\\(\\hat{p}\\)) values - to which we apply NLL. loss &lt;- function(logit, target) { p.hat = logit prob = ifelse(target == 1, p.hat, (q.hat = 1 - p.hat)) nll(prob) # Negative Log Likelihood } Let us now use a simple mtcars dataset to illustrate logistic regression. Here, we discuss Binary Logistic regression because our dependent variable is binary (e.g. vs). We fit a model using glm(.) for logistic regression — note that glmnet(.) can be used which includes Regularization: logit.model = glm(vs ~ wt + hp + disp, data=trainset, family=binomial(link=logit)) See Inverse Logit (Sigmoid) Figure in Chapter 6 (Statistical Computation) under Logistic Regression Section for a sample of the fitted sigmoid curve. With the fitted model, we should be able to obtain the fitted values. The fitted values are the log odds of our p-hat \\(\\hat{p}\\) probabilities. logit.hat = fitted(logit.model) round(logit.hat,3) ## [1] 0.631 0.742 0.878 0.500 0.003 0.806 0.000 0.994 0.951 ## [10] 0.771 0.771 0.039 0.020 0.022 0.003 0.002 0.001 0.983 ## [19] 0.979 0.971 0.857 0.043 0.046 0.000 0.004 0.971 0.821 ## [28] 0.317 0.039 0.000 0.836 The logit-hat is already based on the optimized model. We can see the final MLE and NLL obtained from this fit. c(&quot;NLL&quot; = loss(logit.hat, trainset$vs), &quot;MLE&quot; = -loss(logit.hat, trainset$vs)) ## NLL MLE ## 7.493 -7.493 We can validate the loss using the logLik(.) function against the model like so: c(&quot;NLL&quot; = -logLik(logit.model), &quot;MLE&quot; = logLik(logit.model)) ## NLL MLE ## 7.493 -7.493 Now, assume that we have optimized \\(\\beta\\) parameters to represent our model after fitting the logistic model (after minimizing our loss function) - that also means obtaining the fitted (\\(\\hat{y}\\)) values. The fitted binomial model (also called the error model or noise model) gives us the following coefficients (along with the confidence interval). coefficients(logit.model) ## (Intercept) wt hp disp ## 5.32430 2.04414 -0.06624 -0.01787 confint.default(logit.model) ## 2.5 % 97.5 % ## (Intercept) -2.25902 12.907619 ## wt -1.20904 5.297312 ## hp -0.13836 0.005882 ## disp -0.05265 0.016913 Note that the coefficients are in the log-odds unit. To get the odds ratio (OR), we just exponentiate the logit coefficients like so: (OR = exp(coefficients(logit.model))) ## (Intercept) wt hp disp ## 205.2647 7.7225 0.9359 0.9823 This means that if hp and disp are constant, then wt is 7.7225 times more likely to be in the S (Straight) engine class. In terms of optimizing specific parameters to fit a model, the goodness of fit in Logistic Regression does not use the common R-squared. Instead, we use a variant of it called Pseudo R-squared. There are a few variants that we can investigate, of which we list only three variants from the following contributors, namely McFadden, Cox &amp; Snell, and NagelKerke (Cragg-Uhler). In our discussion, let us use McFadden’s R-squared expressed like so: \\[\\begin{align} R^2 = 1 - \\frac{\\log_e Lik(M_{Full})}{\\log_e Lik(M_{Null})} \\ \\ \\ \\ \\ \\ where \\ \\ \\ \\ \\ \\begin{array}{l} \\text{M}_{(Full)}\\text{ = model with predictors}\\\\ \\text{M}_{(Null)}\\text{ = model without predictors} \\end{array} \\label{eqn:eqnnumber405} \\end{align}\\] A model without predictors represents a horizontal line intercepting the y-axis. In other words, such a model makes all the logit-hat values correspond to the \\(\\beta_0\\) coefficient (the intercept) values. null.model = glm(vs ~ 1, data=trainset, family=binomial(link=logit)) logit.null = fitted(null.model) We now calculate the McFadden’s R-squared: (R.squared = c( 1 - logLik(logit.model) / logLik(null.model) ) ) ## [1] 0.6489 and to validate, we have: model.full = -loss(logit.hat, trainset$vs) # MLE for the fit. model.null = -loss(logit.null, trainset$vs) # MLE for the mean. (R.squared = 1 - (model.full) / model.null) ## [1] 0.6489 As for the three \\(\\mathbf{R^2}\\) variants, let us use a 3rd party package called rcompanion and use the function nagelkerke(.) like so: library(rcompanion) nagelkerke(logit.model)$Models ## ## Model: &quot;glm, vs ~ wt + hp + disp, binomial(link = logit), trainset&quot; ## Null: &quot;glm, vs ~ 1, binomial(link = logit), trainset&quot; nagelkerke(logit.model)$Pseudo.R.squared.for.model.vs.null ## Pseudo.R.squared ## McFadden 0.6489 ## Cox and Snell (ML) 0.5908 ## Nagelkerke (Cragg and Uhler) 0.7902 As in the case of \\(\\mathbf{Pseudo\\ R^2}\\), a value of one indicates the best fit; otherwise, a value of zero indicates the worst fit. Our result above is a good fit at 0.6489. In terms of using Logistic Regression as Binary Classification, we can convert the p-hat into its dichotomous form based on a 0.5 threshold like so: \\[ h(z) = \\begin{cases} 1 &amp; \\text{if}\\ \\hat{p} &gt;= 0.5\\ \\ \\ \\ \\text{when z &gt;= 0} \\\\ 0 &amp; \\text{if}\\ \\hat{p} &lt; 0.5\\ \\ \\ \\ \\ \\ \\ \\text{when z &lt; 0} \\end{cases} \\] where: \\(z = \\text{logit}\\) and \\(\\hat{p} \\text{= inverse-logit values (probabilities)}\\). h &lt;- function(logit, target) { p.hat = plogis(logit) prob = ifelse(target == 1, p.hat, 1 - p.hat) ifelse(prob &gt;=0.5, 1, 0 ) } Using our previous model generated from glm(.), we can then classify the fitted logit values like so: (fitted.class = h(logit.hat, trainset$vs)) ## [1] 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1 ## [29] 0 0 1 Apart from using \\(\\mathbf{Pseudo\\ R^2}\\) to measure goodness of fit, it also helps to measure the strength of association not only based on model performance (e.g., McFadden’s full model vs. null model) but also based on predicted and observed outcomes. To test if our model performs accurately, we can use techniques described in the Statistical Computation chapter under the Goodness of Fit section, namely Wald test, Likelihood Ratio test, and Lagrange Multiplier test. Additionally, we also introduce three measures. Deviance test The Deviance test is a measure between the fitted model and the saturated model. Whereas a null model is the most restrictive model (See Chapter 6 (Statistical Computation) under Inference for Regression Section), a saturated model is more general in that it has n parameters equivalent to the sample size with a likelihood equal to 1. \\[\\begin{align} G^2 = 2\\sum_{i=1}^n O_i \\log_e \\left(\\frac{O_i}{E_i}\\right) = -2 \\log_e \\mathcal{L} \\left(\\hat{\\beta}\\right) \\end{align}\\] Using this test, small deviance suggests a good fit. For why we use -2 in the equation, refer to Wilk’s Theorem. Pearson’s Chi-square test The Pearson’s Chi-square test is a widely used test of statistical significance between observed (y) and expected (\\(\\mathbf{\\hat{y}}\\)) models. \\[\\begin{align} X^2_{(Pearson)} = \\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i} \\end{align}\\] Using this test, a small Chi-square with p-value closer to one suggests a good fit. Otherwise, a large Chi-square with p-value less than 0.05 suggests a poor fit. Hosmer-Lemeshow test The Hosmer-Lemeshow is expressed like so: \\[\\begin{align} X^2_{(HL)} = \\sum_{i=1}^n \\frac{(O_i - E_i)^2}{E_i(1 - \\frac{E_i}{n_i})} \\end{align}\\] This test is known to render a good fit if the group is set to ten (default). We can use a third-party library called ResourceSelection and use the hoslem.test(.) function to run the test. See below: library(ResourceSelection) hl = hoslem.test(trainset$vs, fitted(logit.model), g=10) cbind(hl$expected, hl$observed) ## yhat0 yhat1 y0 y1 ## [3.23e-07,0.00106] 3.99880 0.001197 4 0 ## (0.00106,0.00344] 2.99161 0.008386 3 0 ## (0.00344,0.0219] 2.95444 0.045563 3 0 ## (0.0219,0.0432] 2.87855 0.121455 3 0 ## (0.0432,0.5] 2.13627 0.863735 1 2 ## (0.5,0.771] 1.08517 2.914832 2 2 ## (0.771,0.821] 0.37354 1.626456 1 1 ## (0.821,0.878] 0.42893 2.571072 0 3 ## (0.878,0.971] 0.10823 2.891768 0 3 ## (0.971,0.994] 0.04446 2.955536 0 3 Stukel test and Osius-Rojek test are recent alternatives that offer benefits over Hosmer-Lemeshow test. We leave readers to investigate the two tests. As for Deviance test and Pearson’s Chi-square test, the next section uses both tests in the context of Poisson regression. Finally, in terms of Prediction for Logistic Regression classification, let us use the model we trained previously. Our response variable (vs) represents types of engine car, which is a binomial target having only two values, namely V-shaped engine or Straight engine. We can predict and determine the probability of a given new car having an S engine given our fitted model. logit.predicted = predict.glm(logit.model, test.predictors, type=&quot;response&quot;) prob.predicted = P = plogis(logit.predicted) car = names(P); names(P) = NULL; est.class = ifelse(P &gt;=0, &quot;S Engine&quot;, &quot;V Engine&quot;) c(&quot;Car&quot; = car, &quot;Probability&quot; = round(P,2), &quot;Predicted Class&quot; = est.class) ## Car Probability Predicted Class ## &quot;Ford Pantera L&quot; &quot;0.5&quot; &quot;S Engine&quot; Before we go to SVM classification, let us extend (perhaps complete) our understanding of GLM by introducing Poisson Regression. While the section should fall under Regression, let us focus on the loss function as this topic becomes a more critical subject in subsequent classification discussions. 10.1.9 Poisson Regression (GLM) Poisson Regression models events that occur at a certain rate and is used to forecast counts. Alternatively, we also can use Negative Binomial Regression to model the same. As is the case, our goal is to optimize \\(\\beta\\) parameters to achieve a good fit. To do that, we also start with the common equation below: \\[\\begin{align} Y = f(X) = \\beta^TX \\end{align}\\] We then transform Y into a Poisson probability with range [0, \\(+\\infty\\)), using the exponential equation: \\[\\begin{align} \\mu = exp(\\beta^TX)\\ \\ \\ \\ \\ where\\ \\mu\\ \\sim Pois(\\lambda) \\end{align}\\] Each point in the probability model (the curve line) corresponds to the mean (\\(\\mu\\)) - the central tendency of the Poisson distribution - which indicates the probability of observing the number of events occurring at a given period. To achieve linearity of the relationship between independent variables and the dependent variable, we convert the mean (\\(\\mu\\)) to natural log like so: \\[\\begin{align} g(\\mu) = \\log_e(\\mu) = \\beta^TX \\end{align}\\] where \\(g(\\mu)\\) is a link function, also called the natural link. To now optimize the fit, we use MLE similar to Logistic Regression. Here, we express the log-likelihood using the below expression: \\[\\begin{align} Lik(\\beta) = \\prod_{i=1}^n \\frac{\\lambda_i^{Y_i} e^{-\\lambda_i} }{Y_i!} \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ \\log_e Lik(\\beta) = \\sum_{i=1}^n y_i \\log_e \\lambda_i - \\log_e y_i! - \\lambda_i \\end{align}\\] Our example implementation of the log-likelihood is as follows: ln &lt;- function(x) { log(x, exp(1))} poisson.likelihood &lt;- function(x, lambda) { (lambda^x * exp(-lambda)) / factorial(x) } poisson.loglik &lt;- function(x,lambda) { sum(x * ln(lambda) - ln(factorial(x)) - lambda) } Our loss function becomes the following equation below. Here we use NLL to minimize the loss: \\[\\begin{align} \\hat{\\beta_j} = \\text{arg}\\ \\underset{\\beta_j}{\\text{min}}\\ \\underbrace{\\mathcal{L}\\text{oss}\\left\\{ \\lambda_{i=1}^n x_{ij} \\left[Y_i - exp\\left(\\beta^TX\\right)\\right] \\right\\} }_{\\text{NLL}} \\end{align}\\] As long as the loss function is differentiable, we should be able to perform partial derivatives with respect to each \\(\\beta_j\\) parameter like so: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\beta_j} = \\lambda_{i=1}^n x_{ij} \\left[Y_i - exp\\left(\\beta^TX\\right)\\right] = 0 \\end{align}\\] Only if differentiation cannot solve the optimization can we resort to the use of other iterative algorithms such as Gradient Descent or Newton Raphson. Now to illustrate Poisson Regression, let us simulate a dataset. We start with the following equation: \\[\\begin{align} \\log_e(y) = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\mu = \\exp(\\beta_0 + \\beta_1 x_1 + \\beta_2 x_2) \\end{align}\\] set.seed(2020) N = 100 beta = c(1,2,3) x1 = runif(n = N, min = 0, max = 1) x2 = runif(n = N, min = 0, max = 1) mu = exp(beta[1] + beta[2] * x1 + beta[3] * x2) Using the central tendency denoted as mu (\\(\\mu\\)), we generate a random outcome that follows a Poisson distribution like we use rpois(.) function so that our link function is g(.) = rpois(.): set.seed(2020) y = rpois(n=N, lambda = mu) Then, we fit three models. Note that one property of a Poisson distribution is to have the mean equal to the variance. However, as we cannot guarantee from our sample dataset. Let us use quasi-Poisson to handle overdispersion or underdispersion concerns - See Chapter 6 (Statistical Computation) under Dispersion Subsection under Regression Analysis Section. pois.model = glm(y ~ x1 + x2, family=quasipoisson(link=log)) pois1.model = glm(y ~ x1, family=quasipoisson(link=log)) pois2.model = glm(y ~ x2, family=quasipoisson(link=log)) We are now ready to plot. See Figure 10.15. plot(NULL, xlim=range(x1), ylim=range(y), ylab=&quot;Frequency&quot;, xlab=&quot;Interval&quot;, main=&quot;Poisson Regression&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x1,y, pch=20, col=&quot;navyblue&quot;) points(x2,y, pch=20, col=&quot;darksalmon&quot;) n = length(pois.model$fitted.values) y.hat = pois.model$fitted.values y.hat1 = pois1.model$fitted.values y.hat2 = pois2.model$fitted.values x = seq(0,1, length.out = n) pois = smoothingSpline = smooth.spline(x, sort(y.hat), spar=1.00) pois1 = smoothingSpline = smooth.spline(x, sort(y.hat1), spar=1.00) pois2 = smoothingSpline = smooth.spline(x, sort(y.hat2), spar=1.00) lines( pois1 , col=&quot;navyblue&quot;, lwd=2) lines( pois2 , col=&quot;darksalmon&quot;, lwd=1) lines( pois , col=&quot;black&quot;, lwd=1) legend(0, 250, c( &quot;pois.model (y ~ x1 + x2)&quot;, &quot;pois1.model (y ~ x1)&quot;, &quot;pois2.model (y ~ x2)&quot; ), col=c(&quot;black&quot;, &quot;navyblue&quot;, &quot;darksalmon&quot;), horiz=FALSE, cex=0.8, lty=c(1,2,2)) Figure 10.15: Poisson Regression Let us analyze the pois.model: (summary.pois = summary(pois.model)) ## ## Call: ## glm(formula = y ~ x1 + x2, family = quasipoisson(link = log)) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.152 -0.841 0.083 0.575 3.431 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.0151 0.0564 18.0 &lt;2e-16 *** ## x1 1.9878 0.0532 37.4 &lt;2e-16 *** ## x2 2.9702 0.0589 50.4 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for quasipoisson family taken to be 1.052) ## ## Null deviance: 5347.02 on 99 degrees of freedom ## Residual deviance: 102.47 on 97 degrees of freedom ## AIC: NA ## ## Number of Fisher Scoring iterations: 4 Note that the summary shows the Dispersion measure as 1.0521. That is the adjusted Dispersion because of Quasi-Poisson distribution; otherwise, the value stays at 1. The Deviance Residual shows values close to zero, indicating a balance bias, stretching evenly towards opposite directions. On the other hand, the unit variance for Poisson Distribution is as follows (A. Colin Cameron et al. 1995; Zhirui Ye et al. 2012): \\[\\begin{align} D(y, u) = 2\\left(y \\log_e\\frac{y}{\\mu} - y + \\mu\\right) \\end{align}\\] ln &lt;- function(x) { log(x, exp(1))} deviance.test &lt;- function(y,mu) { 2 * (y * ln(ifelse(y == 0, 1, y/mu)) - y + mu ) } residual.dev = sqrt(deviance.test(y, y.hat)) * ifelse(y &gt; y.hat, 1, -1) summary(residual.dev) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -2.152 -0.841 0.083 -0.040 0.575 3.431 The R-squared based on Pearson’s residual shows the following computation: \\[\\begin{align} R^2_{(chi-square)} = \\sum_{i=1}^n \\frac{(y_i - \\hat{y}_i)^2}{\\hat{y}_i}\\ \\ \\ \\ \\text{(Pearson&#39;s Chi-square)} \\end{align}\\] pearson.rsquare1 = residuals(pois.model, type=&quot;pearson&quot;) pearson.rsquare2 = (y - y.hat) / sqrt(y.hat) all.equal(pearson.rsquare1, pearson.rsquare2) ## [1] TRUE Additionally, all coefficients in the summary indicates statistical significance. We can use Chi-square test with the null model to validate that further. null.model = glm(y ~ 1, family=quasipoisson(link=log)) anova(null.model, pois.model, test=&quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: y ~ 1 ## Model 2: y ~ x1 + x2 ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 99 5347 ## 2 97 102 2 5245 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The result indicates that x1 and x2 are statistically significant predictors of y. In terms of prediction, we use predict.glm(.) to predict the frequency outcome for the response variable. (count = predict.glm(pois.model, newdata = data.frame(x1 = 0.6, x2=0.6), type=&quot;response&quot;)) ## 1 ## 54.05 Alternatively, we can also obtain the link outcome; otherwise, the log of the frequency outcome. log.count = predict.glm(pois.model, newdata = data.frame(x1 = 0.6, x2=0.6), type=&quot;link&quot;) c(&quot;link&quot; = log.count, &quot;log(response)&quot; = log(count)) ## link.1 log(response).1 ## 3.99 3.99 10.2 Binary Classification (Supervised) For Regression - specifically Linear Regression, we tend to use sum squared error (SSE) to minimize our loss function and use Mean Square Error (MSE) or Root Mean Square Error (RMSE) to evaluate performance for which the response variable is continuous. For Classification, we minimize our loss (or gain) function based on measures derived from Information Theory such as Gini Index (GI) and Entropy (H) and use confusion matrix as a reference to evaluate performance for which the response variable is categorical. We score the performance based on accuracy, sensitivity, specificity, and F1 score. 10.2.1 Linear SVM (SGD/PEGASOS) Support Vector Machine (SVM) is a classic Computational Learning technique used for classification. As long as data points are linearly separable, we can use a simple linear SVM (LSVM) to perform classification. If that is not the case, then perhaps there are other ways. Figure 10.16 shows example techniques of how to separate data points. Figure 10.16: Support Vector Machine Modulus transformation is a good illustration of how one can use even basic mathematics to separate data points. For example, in separating even numbers from odd numbers, we use modulus operations. Additionally, Figure 10.17 shows SVM techniques in separating data points. Figure 10.17: Support Vector Machine In this section, let us discuss classification using linear SVM. Note that LSVM is about modeling an optimal decision boundary and not about modeling probability in the same way we do with General Linear Models. It is different from linear regression in which we measure the minimum horizontal distance - the residual - from the data point to the line. It is also far different from the Principal Component Analysis (PCA) in which we measure the maximum distance - the variance - from the projected data point to the point of origin along the Principal Component axis (see Figure 9.49). In linear SVM, we measure the minimum orthogonal distance - a normal line - from the data point to a hyperplane. See Figure 10.18. Figure 10.18: Linear SVM Note that LSVM relies on concepts we cover in linear algebra around vector calculation. Understanding Euclidian norm, Dot Product, Magnitude, Direction, Projection, and many others are essential in building our intuition on SVM. To understand the concept, we start with the standard linear equation (where m is the slope and b is the intercept) with a slight modification (where \\(\\mathbf{x_2}\\) takes the place of y): \\[\\begin{align} y = mx + b\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ x_2 = m x_1 + b \\end{align}\\] In an alternative format, we can re-arrange the equation by moving \\(\\mathbf{x_2}\\) to the right side and, at the same time, show a generalized equation for the line like so: \\[\\begin{align} 0 = mx_1 - x_2 + b \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\mathbf{w}^T\\mathbf{x} + b = 0}_{\\text{1-D hyperplane}} \\end{align}\\] where \\(\\mathbf{w}\\) - norm to the hyperplane - and \\(\\mathbf{x}\\) are vectors such that: \\[ \\mathbf{w} = \\left[\\begin{array}{r} m \\\\ -1 \\end{array}\\right] \\ \\ \\ \\ \\text{and} \\ \\ \\ \\ \\mathbf{x} = \\left[\\begin{array}{r} x_1 \\\\ x_2 \\end{array}\\right]. \\] The b term becomes a bias coefficient that displaces the hyperplane from point of origin , namely O = \\(\\mathbf{[0,0]}^T\\), if b &gt; 0. Assume b=0 and the slope is m=2 based on rise=2 and run=1. We then have the following: b = 0; m = 2 x1 = seq(-4,4, length.out=10) # some continuous obs for feature x1 x2 = m * x1 + b The equation should equal to zero: \\(m x1 - x2 = \\mathbf{w}^T\\mathbf{x} = 0\\) w = c(m, -1); x = rbind(x1, x2) v1 = m * x1 - x2 + b v2 = c( t(w) %*% x + b ) data.frame(&quot;sum v1&quot; = sum(v1), &quot;sum v2&quot; = sum(v2), &quot;equal&quot; = all.equal(v1, v2)) ## sum.v1 sum.v2 equal ## 1 0 0 TRUE Based on the equation above, we can see the plotted hyperplane in Figure 10.19. Figure 10.19: Hyperplane As we can see, the vector w points in the direction denoted by (\\(w_1\\),\\(w_2\\)) = (2, -1). To derive the size (magnitude) of w, we use the following euclidean formula (recall euclidean norm): \\[\\begin{align} \\|\\mathbf{w}\\| = \\sqrt{w_1^2 + w_2^2 + w^2_3 +\\ ...\\ + w^2_n} \\\\ = \\sqrt{(2)^2 +(-1)^2} = \\sqrt{5} \\end{align}\\] Given a vector, namely p (4,2), let us compute the magnitude of the projection of p into the subspace w. Let us denote the projection of p as q. Here, we use the formula: \\[\\begin{align} c_{(scale)} = \\frac{\\mathbf{w} \\cdot \\mathbf{p}}{\\| \\mathbf{w} \\|^2} = \\frac{ \\left[\\begin{array}{r} 2\\\\-1 \\end{array}\\right] \\cdot \\left[\\begin{array}{r} 4\\\\2 \\end{array}\\right] \\label{eqn:eqnnumber406} } {(\\sqrt{5})^2} = \\frac{2\\times4 + (-1)\\times 2}{5} = 1.2 \\end{align}\\] \\[\\begin{align} \\mathbf{q}_{(proj)} = c_{(scale)} \\times \\mathbf{w} = 1.2 \\times \\left[\\begin{array}{r} 2\\\\-1 \\end{array}\\right] = \\left[\\begin{array}{r} 2.4\\\\-1.2 \\end{array}\\right] \\label{eqn:eqnnumber407} \\end{align}\\] proj &lt;- function(p, v) { v.size = sqrt(sum(v^2)) # magnitude c.scale = (t(v) %*% p) / v.size^2 c(c.scale) * v } p = c(4,2); (q = proj(p, w)) ## [1] 2.4 -1.2 Using similar computation, we can obtain the projection of r, denoted as s, like so: \\[ \\mathbf{s}_{(proj)} = \\left[\\begin{array}{r} -0.8\\\\ 0.4 \\end{array}\\right] \\] r = c(1,4); (s = proj(r, w)) ## [1] -0.8 0.4 Let us see the plot in Figure 10.20. Figure 10.20: Projections and Magnitude The magnitude of the projection (q) of p and project (s) of r are: \\[\\begin{align*} \\|\\mathbf{q}\\| = \\sqrt{2.4^2 + (-1.2)^2} = 2.683282\\ \\ \\ \\ \\ \\ \\ \\|\\mathbf{s}\\| = \\sqrt{(-0.8)^2 + (0.4)^2} = 0.8944272 \\end{align*}\\] In other words, the orthogonal distance from point p to the hyperplane equals \\(\\|\\mathbf{q}\\|\\). Similarly, the orthogonal distance from point r to the hyperplane equals \\(\\|\\mathbf{s}\\|\\). Therefore, to note, the distance from any random point to the hyperplane can be computed based on its projection to w. Equivalently, \\(\\|\\mathbf{q}\\|\\) = \\(\\|\\mathbf{p}\\|\\) and \\(\\|\\mathbf{s}\\|\\) = \\(\\|\\mathbf{r}\\|\\). We can calculate the margin of each point across a hyperplane. For example: \\[\\begin{align*} \\text{M}_{p} = 2 \\| \\mathbf{p}\\| = 5.366563\\ \\ \\ \\ \\ \\ \\text{M}_{r} = 2 \\| \\mathbf{r}\\| = 1.788854\\ \\ \\ \\ \\ \\ where: \\text{M = margin} \\end{align*}\\] See Figure 10.21 for the plot of the margins. Figure 10.21: Margins and Hyperplanes Given that we can calculate distance and margin, our next goal for LSVM is to find the optimal decision boundary - the \\(H_0\\) hyperplane. It can be achieved based on the following steps: First, data points are classified based on the following formula: \\[\\begin{align} g(x) = \\mathbf{w}^Tx + b \\end{align}\\] \\[ where: g(x) = \\begin{cases} \\ge +1 &amp; \\text{(+) data points on or above the }H_{(+)} \\\\ 0 &amp; \\text{the }H_0 \\text{ hyperplane} \\\\ \\le-1 &amp; \\text{(-) data points on or below the }H_{(-)} \\end{cases} \\] Any data point classified as positive (+) respects the following equation: \\[\\begin{align} \\mathbf{w}^Tx + b \\ge +1 \\end{align}\\] Any data point classified as negative (-) respects the following equation: \\[\\begin{align} \\mathbf{w}^Tx + b \\le -1 \\end{align}\\] For mathematical convenience, we combine the two equations into one constraining equation by introducing an extra variable, namely y where \\(y \\in (-1,+1)\\) like so: \\[\\begin{align} y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\ge 1\\ \\ \\ \\equiv\\ \\ \\ y_i (\\mathbf{w}^T\\mathbf{x_i} + b) -1 \\ge 0 \\end{align}\\] The equation above imposes a hard margin constraint such that it allows us to classify any x data points so that if \\(\\mathbf{w}^T\\mathbf{x} + b\\) is positive and \\(y = +1\\), then x is correctly classified. Similarly if \\(\\mathbf{w}^T\\mathbf{x} + b\\) is negative and \\(y = -1\\), then x is correctly classified. However, there may be cases when x data points do not respect the equation above and thus may render misclassification, which happens when such data points fall between the \\(H_{(+)}\\) and \\(H_{(-)}\\) hyperplanes. For example: \\[\\begin{align} y_i (\\mathbf{w}^T\\mathbf{x_i} + b) &lt; 1\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\text{mis-classified} \\end{align}\\] Such data points may cause the entire dataset not to be linearly separable. To mitigate the situation, we can relax the constraint by softening the margin allowing a few data points to cross over to the other side of the \\(H_{(+)}\\) and \\(H_{(-)}\\) boundaries. For example, positive (+) data points can cross over to the other side of the \\(H_{(+)}\\) hyperplane in the same way that negative (-) data points can cross over to the other side of the \\(H_{(-)}\\) hyperplane. In doing so, we end up with a soft margin expressed like so: \\[\\begin{align} y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\ge 1 - \\xi_i \\ \\ \\ \\equiv\\ \\ \\ y_i (\\mathbf{w}^T\\mathbf{x} + b) -(1 - \\xi_i) \\ge 0 \\end{align}\\] We introduce a slack variable denoted by the xi (\\(\\xi\\)) symbol. The slack variable serves as a measure of crossover error - not necessarily a misclassification error because, for example, positive (+) data points may still find themselves between the decision boundary and the \\(H_{(+)}\\) boundary in that they are therefore still correctly classified. However, as data points get farther away from the borders and deeper to the other side, the error progresses to a misclassification error. Under such conditions, each data point that crosses over incurs some loss - calculated through Hinge Loss. We continue this discussion further in step four. Second, data points that are located along the \\(H_{(+)}\\) and \\(H_{(-)}\\) hyperplanes are called the support vectors because they support the decision boundary and they help to dictate the separability distance between two classes. The idea, therefore, is to identify those support vectors. To do that, we need to rely on distance measurement, which requires us to review three equations derived from the original equation, namely \\(g(x) =\\mathbf{w}^T\\mathbf{x} + b\\). The three equations are expressed as such: \\[\\begin{align} \\underbrace{\\mathbf{w}^Tx + b = +1}_{\\text{equation for the }H_{(+)}\\text{ hyperplane}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\mathbf{w}^Tx + b = -1}_{\\text{equation for the }H_{(-)}\\text{ hyperplane}} \\end{align}\\] \\[\\begin{align} \\underbrace{\\mathbf{w}^Tx + b = 0}_{\\text{equation for the }H_{0}\\text{ hyperplane}} \\end{align}\\] We have shown earlier how to project a vector to w and derive the magnitude of the vector. Alternatively, we also can use the following equation (based on Pythagoras) to compute the distance of any point to a hyperplane directly. \\[\\begin{align} D(x, w, b) = \\frac{|\\mathbf{w}^T\\mathbf{x} + b|}{\\|\\mathbf{w}\\|} \\ \\ \\ \\ \\leftarrow \\text{based on}\\ \\ \\ \\ \\ \\ \\underbrace{\\frac{|ax +bx + c|}{\\sqrt{a^2 + b^2}}}_{\\text{euclidean distance}} \\end{align}\\] For example, we can solve for the distance of both \\(\\|\\mathbf{p}\\|\\) and \\(\\|\\mathbf{r}\\|\\) like so: Distance &lt;- function(x, w, b) { numer = abs( t(w) %*% x + b ) denom = sqrt(sum(w^2)) numer / denom } c(&quot;dist(p)&quot; = Distance(p, w, b), &quot;dist(r)&quot; = Distance(r, w, b)) ## dist(p) dist(r) ## 2.6833 0.8944 Now, using the same distance equation, we can determine the distance of both \\(H_{(+)}\\) and \\(H_{(-)}\\) hyperplanes to the \\(H_0\\) hyperplane. For \\(H_{(+)}\\) hyperplane, we have the following distance from which our (+) support vectors are found: \\[\\begin{align} D(x, w, b) = \\frac{|\\mathbf{w}^T\\mathbf{x} + b|}{\\|\\mathbf{w}\\|} = \\frac{|+1|}{\\|\\mathbf{w}\\|} \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\underbrace{\\mathbf{w}^Tx + b = +1}_{\\text{equation for the }H_{(+)}\\text{ hyperplane}} \\end{align}\\] For \\(H_{(-)}\\) hyperplane, we have the following distance from which our (-) support vectors are found: \\[\\begin{align} D(x, w, b) = \\frac{|\\mathbf{w}^T\\mathbf{x} + b|}{\\|\\mathbf{w}\\|} = \\frac{|-1|}{\\|\\mathbf{w}\\|} \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\underbrace{\\mathbf{w}^Tx + b = -1}_{\\text{equation for the }H_{(-)}\\text{ hyperplane}} \\end{align}\\] From here, it is easy to prove that the margin - the distance between \\(H_{(+)}\\) and \\(H_{(-)}\\) hyperplanes - of our hyperplane is equal to the following: \\[\\begin{align} \\text{margin} = D_{H_{(+)}} + D_{H_{(-)}} = \\frac{|+1|}{\\|\\mathbf{w}\\|} + \\frac{|-1|}{\\|\\mathbf{w}\\|} = \\frac{2}{\\|\\mathbf{w}\\|} \\end{align}\\] In other words, our (+) support vectors are a margin distant-away from (-) support vectors. Alternatively, the width of the margin is calculated based on subtracting the closest opposing support vectors. See Figure 10.22. \\[\\begin{align} \\text{width}_{\\left(v_{(+)} - v_{(-)}\\right)} = \\frac{\\mathbf{w}}{\\|\\mathbf{w}\\|} \\cdot \\left(v_{(+)} - v_{(-)}\\right) = \\frac{2}{\\|\\mathbf{w}\\|} \\end{align}\\] For example: width &lt;- function(v.pos, v.neg, w) { u = w / sqrt(sum(w^2)) abs(c(u %*% ( v.pos - v.neg ))) } w.norm = c(1, -1); v.pos = c(2,1); v.neg = c(1,2) all.equal( width(v.pos, v.neg, w.norm), 2 / sqrt(sum(w.norm^2)) ) ## [1] TRUE Also, notice in the equation above that if we minimize w, the margin is effectively maximized. It is our optimization problem, expressed like so: \\[\\begin{align} \\text{max}\\ \\frac{2}{\\|\\mathbf{w} \\|} \\equiv \\text{min}\\ \\|\\mathbf{w} \\| \\end{align}\\] Third, we need to find the optimal decision-boundary. To do that, all we need to do is perform the optimization equation below (with a caveat such that minimizing the w is subject to a constraint): \\[\\begin{align} \\text{min}\\ \\|\\mathbf{w}\\|\\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\ \\text{subject to:}\\ \\ \\ \\ y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\ge 1 \\end{align}\\] For mathematical convenience, we can perform mathematical manipulation using other variants of the minimization term, keeping the proportionality intact. \\[\\begin{align} \\text{min} \\left\\{\\|\\mathbf{w}\\| \\propto \\|\\mathbf{w}\\|^2 \\propto \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\right\\} \\equiv \\text{min} \\left\\{ \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\right\\} \\end{align}\\] Here, without losing proportionality, we transform the equation into a convex function expressed as the square of \\(\\|\\mathbf{w}\\|\\), divided by 2. This transformation allows us an easy way to work with lagrangian multiplier optimization, which we will cover later. Also, we can adjust the constraint to address scale variant concerns by dividing it by the w norm so that we have the following: \\[\\begin{align} y_i \\left[D(x_i, w, b)\\right] \\ge 1 \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ y_i \\left(\\frac{\\mathbf{w}^T\\mathbf{x_i} + b}{\\|\\mathbf{w}\\|}\\right) \\ge 1 \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\ge 1 \\end{align}\\] With all that said, our minimization formula for a perfectly separable dataset becomes: \\[\\begin{align} \\text{arg}\\ \\underset{\\mathbf{w},b}{\\text{min}} \\left\\{ \\frac{1}{2}\\|\\mathbf{w}\\|^2 \\right\\}\\ \\ \\ \\ \\mathbf{s.t.}\\ \\ \\ y_i \\left[D(x_i, w, b)\\right] \\ge 1,\\ \\ \\ \\ \\ where\\ \\forall i = 1,..,n. \\end{align}\\] Note that the distance formula depends upon the values of w and b - they are the parameters of our \\(H_0\\) hyperplane. As long as we know that the data points are linearly separable, we can generate a list of arbitrary \\(H_0\\) hyperplanes described by their corresponding set of w and b values. However, only one of the \\(H_0\\) hyperplanes can be a qualified candidate based upon the margin that ensures the maximum separability of the (+) and (-) support vectors. Here, by minimizing the value of w (and b), we maximize the margin. See Figure 10.22. Figure 10.22: Optimal Hyperplane Fourth, recall in step one that if our dataset is not linearly separable, perhaps we can relax the hard margin a bit; instead, we can impose a softer margin to permit data points to cross over the decision boundary. Consequently, however, by crossing over to the other side, the data points are forced to incur some loss, represented by a slack variable (\\(\\xi\\)). Now, a slack variable (\\(\\xi_i\\)) acts as a loss function - specifically, the Hinge Loss function - that allows a cross-over violation such that the deeper the cross-over to the wrong side (meaning, the larger the violation), the more significant the loss. For a good intuition, it helps to show two loss functions, namely logistic loss function and hinge loss function. See Figure 10.23. Figure 10.23: Loss Functions Here, we use Hinge Loss for SVM, expressed bellow, to measure the amount of loss for data points. \\[\\begin{align} \\xi_i = \\underbrace{\\text{max} \\left\\{0, 1 - y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\right\\}}_{\\text{hinge loss function}} \\ \\ \\ \\ \\text{based on}\\ \\ \\ y_i (\\mathbf{w}^T\\mathbf{x_i} + b) &lt; 1\\ \\ \\leftarrow\\ \\text{(mis-classified)} \\end{align}\\] A simple intuition behind this is to equate (\\(\\mathbf{w}^T\\mathbf{x_i} + b\\)) with a y-hat (\\(\\mathbf{\\hat{y}}\\)) outcome such that we can then formulate the outcome this way: \\[\\begin{align} y_i (\\hat{y_i}) = \\begin{cases} \\ge +1 &amp; \\text{if}\\ (y_i = +1, \\hat{y_i} \\ge +1) \\ or\\ (y_i=-1,\\hat{y_i} \\le -1) \\\\ \\le -1 &amp; \\text{if}\\ (y_i = -1, \\hat{y_i} \\ge +1) \\ or\\ (y_i=+1,\\hat{y_i} \\le -1) \\\\ \\end{cases} \\label{eqn:eqnnumber408} \\end{align}\\] Therefore: \\[\\begin{align} \\xi_i = \\text{max} \\left\\{0, 1 - y_i (\\hat{y}_i) \\right\\} \\begin{cases} =0 &amp; \\text{max} \\left\\{0, 1 - (\\ge+1) \\right\\}\\ \\ \\leftarrow \\text{(zero loss)}\\\\ &gt;0 &amp; \\text{max} \\left\\{0, 1 - (\\le-1) \\right\\}\\ \\ \\leftarrow \\text{(with loss)}\\\\ \\end{cases} \\label{eqn:eqnnumber409} \\end{align}\\] We now go back to our optimization equation and plug in the slack variable (\\(\\xi_i\\)), which gets minimized also. See below: \\[\\begin{align} \\text{arg}\\ \\underset{\\mathbf{w},b, \\xi_i}{\\text{min}} \\left\\{ \\frac{1}{2}\\|\\mathbf{w}\\|^2 + \\sum_{i=1}^n\\xi_i^p\\right\\} \\ \\ \\ \\mathbf{\\text{s.t}}\\ \\ \\text{const.} \\begin{cases} y_i\\left[D(x_i, w, b)\\right] \\ge 1 - \\xi_i\\\\ \\xi_i \\ge 0 \\end{cases} \\ \\ \\ \\ \\forall i = 1,..,n \\label{eqn:eqnnumber410} \\end{align}\\] Notice the inclusion of an exponent p to the Hinge Loss. It takes care of outliers by increasing the loss exponentially, thus pushing them further away from the border. Now, because we use a softer margin, in the process of maximizing the margin, there may be chances that more data points may forcibly get trapped between the borders, accumulating a larger number of misclassified data points than necessary. A way to avoid that is to have a trade-off between how much misclassification we can take versus how large our margin should be. We apply a cost as a penalty represented by C or by the lambda \\(\\lambda\\) parameter to regulate the trade-off. We have two choices like so. \\[\\begin{align} \\underbrace{\\text{arg}\\ \\underset{\\mathbf{w},b, \\xi_i}{\\text{min}} \\left\\{ \\frac{1}{2}\\|\\mathbf{w}\\|^2 + C\\sum_{i=1}^n\\xi_i^p\\right\\}}_{\\text{1st choice}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\text{arg}\\ \\underset{\\mathbf{w},b, \\xi_i}{\\text{min}} \\left\\{ \\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2 + \\sum_{i=1}^n\\xi_i^p\\right\\}}_{\\text{2nd choice}} \\end{align}\\] Here, we choose to add lambda (\\(\\lambda\\)) to our equation, still subject to the aforementioned constraints. The final equation for our objective (cost) function below is what we call a solution to the primal optimization problem. By primal, it means that the original variables (e.g. w and b) continue to be used for optimization. Also, recall Ridge Regression under the General Modeling section in relation to the equation below: \\[\\begin{align} \\mathcal{J}(\\mathbf{w}) = \\text{arg}\\ \\underset{\\mathbf{w},b, \\xi_i}{\\text{min}} \\left\\{ \\underbrace{\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2}_{ \\begin{array}{l} \\text{ridge-like} \\\\ \\text{regularizer} \\end{array} } + \\underbrace{\\sum_{i=1}^n \\overbrace{\\text{max} \\left\\{0, 1 - y_i (\\mathbf{w}^T\\mathbf{x_i} + b) \\right\\}^p}^{ \\text{hinge }(\\xi_i)}}_{\\text{loss}} \\right\\} \\label{eqn:eqnnumber411} \\end{align}\\] \\[\\begin{align} \\ \\ \\ \\mathbf{\\text{subject to }}\\ \\ \\begin{cases} y_i\\left[D(x_i, w, b)\\right] \\ge 1 - \\xi_i\\\\ \\xi_i \\ge 0 \\end{cases} \\ \\ \\ \\ \\forall i = 1,..,n \\label{eqn:eqnnumber412} \\end{align}\\] On the other hand, we can rewrite the same problem into another form called the dual optimization formulation. We use such formulation in line with the optimization method using Lagrange multiplier (See Chapter 3 (Numerical Linear Algebra II). This topic will be more discussed when dealing with kernel non-linear SVM. Fifth, for primal optimization problem, we can use Gradient Descent or Stochastic Gradient Descent (SGD), or both. Given the non-differentiable nature of the Hinge Loss, we need to use sub-gradient instead. We use one called PEGASOS,, an acronym for Primal Estimated sub-GrAdient SOlver for SVM. For the update rule using the PEGASOS algorithm, we have the following: \\[\\begin{align} \\mathbf{w}^{(t+1)} &amp;= \\mathbf{w}^{(t)} - \\eta^{(t)} \\nabla_{\\mathbf{w}} \\mathcal{J}(\\mathbf{w}^{(t)}) \\\\ &amp;= (1 - \\eta^{(t)} \\lambda)\\mathbf{w}^{(t)} + \\eta^{(t)}\\ \\mathbf{1}[\\ y^{(t)}_i \\mathbf{w}^T \\mathbf{x}^{(t)}_i]\\ y^{(t)}_i x^{(t)}_i \\end{align}\\] \\[\\begin{align*} \\text{where: }\\\\ \\ \\ \\ \\eta \\text{ is learning rate }\\\\ \\ \\ \\ \\lambda \\text{ is a regularization parameter }\\\\ \\end{align*}\\] with an indicator function being: \\[\\begin{align} \\mathbf{1}[\\ y^{(t)}_i \\mathbf{w}^T \\mathbf{x}^{(t)}_i]= \\begin{cases} 1 &amp; y^{(t)}_i \\mathbf{w}^T \\mathbf{x}^{(t)}_i &lt; 1\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\label{eqn:eqnnumber413} \\end{align}\\] Here, we take the gradient (\\(\\nabla\\)) of the cost function \\(\\mathcal{J}(\\mathbf{w})\\) with respect to w. Then we multiply the gradient by the learning rate (\\(\\eta\\)). We then update \\(\\mathbf{w}\\) and repeat the process until convergence. The algorithm eventually convergences because our objective function is convex. Below are two juxtaposed algorithms for review. One is with a generic gradient descent algorithm, and the other is the PEGASOS algorithm introduced by Shalev-Shwartz S. et al. (2007). \\[ \\begin{array}{lll} \\mathbf{\\text{Gradient Descent Algorithm}} \\\\ \\\\ \\text{Initialize } \\mathbf{w}_0\\ \\text{ and } \\eta \\\\ \\text{loop}\\ t\\ in\\ 1:\\ \\text{T} \\\\ \\ \\ \\ \\text{Calculate}\\ \\mathbf{gradient } (\\nabla_\\mathbf{w})\\\\ \\ \\ \\ \\ \\ \\ \\ of\\ \\mathcal{J}(\\mathbf{w}^{(t)}) \\\\ \\ \\ \\ \\text{Update}\\ \\mathbf{w}\\\\ \\ \\ \\ \\ \\ \\ \\ e.g.\\\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta \\nabla_{\\mathbf{w}} \\mathcal{J}(\\mathbf{w}^{(t)})\\\\ \\text{end loop} \\\\ \\text{Ouput}\\ \\mathbf{w}^{(T)} \\\\ {} \\\\ {} \\\\ {} \\end{array} \\left| \\begin{array}{ll} \\mathbf{\\text{PEGASOS Algorithm}} \\\\ \\text{(Shai Shalev-Shwartz et al, 2007)}\\\\ \\\\ \\text{Input:}\\ \\ S, \\lambda, \\text{T} \\\\ \\text{Set}\\ \\mathbf{w}_1 = 0\\\\ \\text{loop}\\ t\\ in\\ 1,2,...,\\text{T} \\\\ \\ \\ \\ \\ \\text{Choose}\\ i^{(t)}\\ \\in \\{1,...,|S|\\}\\ \\ \\text{(unif. random)} \\\\ \\ \\ \\ \\ \\text{Set}\\ \\eta^{(t)} = \\frac{1}{t\\lambda}\\\\ \\ \\ \\ \\ \\text{if}\\ \\ y_{i_t} (\\mathbf{w}^T x_i^{(t)}) &lt; 1\\ \\text{then} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{w}^{(t+1)} = (1 - \\eta^{(t)} \\lambda) \\mathbf{w}^{(t)} + \\eta^{(t)} y_i^{(t)} x_i^{(t)}\\\\ \\ \\ \\ \\ \\text{else}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{w}^{(t+1)} = (1 - \\eta^{(t)} \\lambda) \\mathbf{w}^{(t)} \\\\ \\ \\ \\ \\ \\text{end if}\\\\ \\text{end loop} \\\\ \\text{Ouput}\\ \\mathbf{w}^{(T+1)} \\end{array} \\right. \\] PEGASOS performs sub-gradient descent stochastically as it randomly selects an observation from the dataset. Here, we use sample.int(.) to simulate random sampling of the index (\\(i_t\\)). Below is an example implementation of PEGASOS for SVM: my.pegasos.svm &lt;- function(data, lambda=0.01, limit=15000) { x = cbind(1, data[,c(1,2)]) y = data[,c(3)] n = nrow(x); p = ncol(x) w = rep(0, p) for (t in 1:limit) { i = sample.int(n, size=1) xi = x[i,]; yi = y[i] eta = 1 / (t * lambda) # learning rate if ( yi * (t(w) %*% xi) &lt; 1) { # s.t. (1 - yi * (t(w) %*% xi) &gt;= 0) w = ( 1 - eta * lambda) * w + eta * yi * xi } else { w = ( 1 - eta * lambda) * w } } w } With the optimized w coefficients (or weights), we can implement the hyperplanes like so: svm.hyperplanes &lt;- function(svm.model) { b = svm.model[1]; rise = svm.model[2]; run = svm.model[3] H0.int = -b / run Hp.int = (1 - b) / run Hn.int = -(1 + b) / run list(&quot;slope&quot; = -rise / run, &quot;H0.int&quot; = H0.int, &quot;Hp.int&quot; = Hp.int, &quot;Hn.int&quot; = Hn.int) } Let us use the implementation to learn the classifier. Below is a sample dataset to use: set.seed(152) N = 20; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v); y1 = rep( 1, N) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v); y2 = rep(-1, N) x = cbind(c(x1.blue, x1.red), c(x2.blue, x2.red)) y = cbind(c(y1, y2)) data = cbind(x, y) svm.pegasos.model = my.pegasos.svm(data) hplanes = svm.hyperplanes(svm.pegasos.model) We now plot and see Figure 10.24. plot(NULL, xlim=range(x[,1]), ylim=range(x[,2]), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;LSVM (Sub-Gradient Descent)&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, pch=20, col=ifelse(y == -1, &quot;darksalmon&quot;, &quot;navyblue&quot;)) abline(a=hplanes$H0.int, b=hplanes$slope, lty=1, col=&quot;black&quot;, lwd=2) abline(a=hplanes$Hp.int, b=hplanes$slope, lty=2, col=&quot;blue&quot;) abline(a=hplanes$Hn.int, b=hplanes$slope, lty=2, col=&quot;red&quot;) Figure 10.24: LSVM (Sub-Gradient Descent) Sixth, for prediction, we can use the model to classify any new or missing data. \\[\\begin{align} h(x; \\mathbf{w}) = \\text{sign}(g(x)) = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b) \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\begin{cases} +1 &amp; \\mathbf{w}^T\\mathbf{x} \\ge 0\\\\ -1 &amp; \\mathbf{w}^T\\mathbf{x} &lt;0 \\end{cases} \\label{eqn:eqnnumber414} \\end{align}\\] Below is an example of predicting the class of a given set of data. x.new = rbind( c(&quot;intercept&quot; = 1, &quot;x1&quot; = -2, &quot;x2&quot; = 2), c(&quot;intercept&quot; = 1, &quot;x1&quot; = 2, &quot;x2&quot; = -2)) my.svm.prediction &lt;- function(w, x) { c(sign(w %*% t(x))) } h = my.svm.prediction(svm.pegasos.model, x.new) pred = ifelse(h == 1, &quot;positive&quot;, &quot;negative&quot;) print(cbind(x.new, pred), quote=FALSE, right=TRUE) ## intercept x1 x2 pred ## [1,] 1 -2 2 positive ## [2,] 1 2 -2 negative Finally, to evaluate performance, we can also use Chi-squared test in the same way we did with Poisson Regression. See the previous section for the method of evaluation. 10.2.2 Kernel SVM (SMO) This section (SMO-based SVM) and the next section (SDCA-based SVM) may require an understanding of the theory of duality starting with simplex method, dual simplex, and primal and dual formulations from Chapter 3 (Numerical Linear Algebra II) under Subsections Simplex Method, Dual Method, and Primal Dual respectively under Polynomial Regression Section. In this section, we take the case in which our optimization problem is written into a different formulation - the dual optimization formulation. Here, we cover two complementing algorithms to solve for SVM, namely Lagrangian Multiplier method and Sequential Minimal Optimization (SMO). First, we begin our discussion by introducing the Lagrangian duality, a variant of the Fenchel duality for optimization problems. Based on Duality Theory and Lagrange Multiplier in Chapter 3 (Numerical Linear Algebra II), we obtain the following equation: \\[\\begin{align} Lik(X, \\alpha) = f(X) - \\sum_{i=1}^n \\alpha_i \\times (g_i(X) - c_i) \\end{align}\\] Now, if only the dataset is perfectly separable with a hard margin, then our equation from the primal formulation in the previous section equates to the following primal Lagrangian formulation: \\[\\begin{align} Lik(w, b, \\alpha_i) = \\underbrace{\\frac{1}{2} \\|\\mathbf{w}\\|^2}_{f(\\mathbf{w},b)} - \\sum_{i=1}^n \\alpha_i \\underbrace{\\left(y_i \\left( \\mathbf{w}^T\\mathbf{\\vec{x}_i} + b\\right) - 1\\right)}_{g(w,b) - c_i} \\end{align}\\] Second, if we then relax the hard margin by using a penalty (\\(\\xi_i\\)) where p=1, then our primal Lagrangian formulation becomes: \\[\\begin{align} Lik(w, b, \\xi_i, \\alpha_i) = \\left[\\frac{1}{2}\\|\\mathbf{w}\\|^2 + C \\sum_{i=1}^n \\xi_i \\right] - \\sum_{i=1}^n\\left[ \\alpha_i \\left\\{ y_i \\left( \\mathbf{w}^T\\mathbf{\\vec{x}_i} + b\\right) - (1 - \\xi_i )\\right\\} \\right] - \\sum_{i=1}^n \\alpha_i \\left( \\xi_i \\right) \\end{align}\\] Here, we use C as our regularization parameter. In this formulation, the alpha (\\(\\alpha\\)) symbol represents a set of Lagrange multipliers. We can then take the partial derivative of the function with respect to w, b, and \\(\\xi_i\\) respectively: \\[\\begin{align} {}&amp;\\nabla_w Lik(w, b, \\xi_i, \\alpha_i) = \\frac{\\partial L}{\\partial \\mathbf{w}} = \\mathbf{w} - \\sum_{i=1}^n \\alpha_i y_i \\mathbf{\\vec{x}_i} = 0 \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\mathbf{w} = \\sum_{i=1}^n \\alpha_i y_i \\mathbf{\\vec{x}_i} \\\\ \\nonumber \\\\ &amp;\\nabla_b Lik(w, b, \\xi_i, \\alpha_i) = \\frac{\\partial L}{\\partial b} = - \\sum_{i=1}^n \\alpha_i y_i = 0\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\sum_{i=1}^n \\alpha_i y_i = 0 \\nonumber \\\\ &amp;\\nabla_{\\xi_i} Lik(w, b, \\xi_i, \\alpha_i) = \\frac{\\partial L}{\\partial \\xi_i} = C - \\alpha_i = 0\\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ C = \\alpha_i \\end{align}\\] By substituting the derived w to the Lagrangian formulation (dropping constants) where w=\\(\\left(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x}_i\\right)\\), we get the following: \\[\\begin{align} Lik(w, b, \\alpha_i) {}&amp;= \\frac{1}{2} \\left(\\sum_{i=1} \\alpha_i y_i \\mathbf{\\vec{x}_i}\\right) \\cdot \\left(\\sum_{j=1} \\alpha_j y_j \\mathbf{\\vec{x}_j}\\right) \\\\ &amp;- \\left(\\sum_{i=1} \\alpha_i y_i \\mathbf{\\vec{x}_i}\\right) \\cdot \\left(\\sum_{j=1} \\alpha_j y_j \\mathbf{\\vec{x}_j}\\right) - \\sum \\alpha_i y_i b + \\sum \\alpha_i. \\end{align}\\] where \\(\\sum \\alpha_i y_i b = 0\\) Simplifying the equation, we end up with the Dual Lagrangian formulation. By Dual, we disregard the use of the original variables (e.g., w and b) as minimized in the primal optimization formulation and use a dot product of two vectors for optimization instead in which we have the following: \\[\\begin{align} Lik(w, b, \\alpha_i) = \\sum \\alpha_i - \\frac{1}{2} \\sum_{i=1} \\sum_{j=1} \\alpha_i \\alpha_j y_i y_j (\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j}) \\end{align}\\] conditioned on the following Karush-Khun-Tucker (KKT) constraints where p=1: \\[\\begin{align} \\text{* Optimality (Stationarity) } &amp; \\nabla_w Lik(w, b, \\xi_i, \\alpha_i) = 0, &amp; i=1,...,n\\\\ &amp; \\nabla_b Lik(w, b, \\xi_i, \\alpha_i) = 0, &amp; i=1,...,n\\\\ &amp; \\nabla_{\\xi_i} Lik(w, b, \\xi_i, \\alpha_i) = 0, &amp; i=1,...,n\\\\ \\text{* Primal Feasibility } &amp; y_i(\\mathbf{w}^T \\mathbf{\\vec{x}_i} + b) \\ge 1, &amp; i=1,...,n \\\\ &amp; \\xi_i \\ge 0, &amp; i=1,...,n \\\\ \\text{* Dual Feasibility } &amp; \\alpha_i \\ge 0, &amp; i=1,...,n \\\\ &amp; C - \\alpha_i \\ge 0, &amp; i=1,...,n \\\\ \\text{* Complementary Slackness } &amp; \\alpha_i\\left(y_i(\\mathbf{w}^T \\mathbf{\\vec{x}_i} + b) - 1 + \\xi_i\\right) = 0, &amp; i=1,...,n \\\\ &amp; (C - \\alpha_i)\\ \\xi_i = 0, &amp; i=1,...,n \\end{align}\\] Third, with all that, the final equation for our objective (cost) function below is what we call solution to the dual optimization problem. The dual formulation tells us to maximize the dot product of the vectors instead, namely \\((\\mathbf{x_i} \\cdot \\mathbf{x_j})\\): \\[\\begin{align} \\mathcal{J}(\\alpha) = \\text{arg}\\ \\underset{\\alpha_i \\ge 0}{\\text{max}} \\left\\{\\sum_{i=1}^n \\alpha_i- \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\left(\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j}\\right) \\right\\} \\end{align}\\] \\[\\begin{align*} \\ \\ \\mathbf{subject\\ to }\\ \\ \\begin{cases} 0 \\le \\alpha_i \\le C \\\\ \\sum_{i=1}^n \\alpha_i y_i = 0 \\end{cases} ,\\ \\forall i=1,...,n \\end{align*}\\] For classification, we can use the below prediction function. \\[\\begin{align} h\\left(\\mathbf{x}\\right) = \\underbrace{\\sum_{i=1}^n \\alpha_i y_i \\left(\\mathbf{x} \\cdot \\mathbf{\\vec{x}_i} \\right) + b}_{\\text{dual}} \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\underbrace{\\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)}_{\\text{primal }} \\end{align}\\] Fourth, we get to the point in which we modify the objective function and prediction function such that they support Kernel functions. The transformation of the equation is indeed straightforward. We pass the dot-product of the two vectors to a Kernel function like so: \\[\\begin{align} \\mathcal{J}(\\alpha) = \\text{arg}\\ \\underset{\\alpha_i \\ge 0}{\\text{max}} \\left\\{\\sum_{i=1}^n \\alpha_i- \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\ \\mathbf{K}\\left(\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j}\\right) \\right\\} \\end{align}\\] \\[\\begin{align} h\\left(\\mathbf{x}\\right) = \\underbrace{\\text{sign}\\left(\\sum_{i=1}^n \\alpha_i y_i\\ \\mathbf{K}\\left( \\mathbf{x} \\cdot \\mathbf{\\vec{x}_i} \\right) + b\\right)}_{\\text{dual}} \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ \\underbrace{\\text{sign}(\\mathbf{w}^T \\mathbf{K}(\\mathbf{x}) + b)}_{\\text{primal }} \\end{align}\\] The choice of Kernel function is based on the nature of the dataset. If the data follows some linear characteristics, perhaps a Linear Kernel is appropriate. Below are three Kernels used for SVM: Linear Kernel - if data linearly demonstrates separability, then we use the kernel below: \\[\\begin{align} \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j}\\right) = \\left(\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j} \\right) \\end{align}\\] Polynomimal Kernel - if data demonstrates separability in a polynomial fashion (given d degrees), then we use the kernel below: \\[\\begin{align} \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j}\\right) = \\left(\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j} + c \\right)^d \\ \\ \\ \\ \\ \\text{where }\\ \\mathbf{c}\\ \\text{is constant} \\end{align}\\] Radial Basis Kernel (RBF) - if data demonstrates separability in a gaussian fashion, then we can use the kernel below: \\[\\begin{align} \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j}\\right) = exp\\left( - \\gamma \\| \\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_j}\\|^2 \\right) \\ \\ \\ \\ \\ where\\ \\ \\gamma = \\frac{1}{2\\sigma^2}\\ and\\ \\sigma\\ \\text{is variance} \\end{align}\\] Laplacian Kernel - this kernel is a variant of RBF kernel. \\[\\begin{align} \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j}\\right) = exp\\left( - \\gamma \\| \\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_j}\\| \\right) \\ \\ \\ \\ \\ where\\ \\ \\gamma = \\frac{1}{\\sigma}\\ and\\ \\sigma\\ \\text{is variance} \\end{align}\\] Exponential Kernel - this kernel is another variant of RBF kernel. \\[\\begin{align} \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j}\\right) = exp\\left( - \\gamma \\| \\mathbf{\\vec{x}_i} - \\mathbf{\\vec{x}_j}\\| \\right) \\ \\ \\ \\ \\ where\\ \\ \\gamma = \\frac{1}{2\\sigma^2}\\ and\\ \\sigma\\ \\text{is variance} \\end{align}\\] Fifth, we now come down to the algorithm. Here, let us discuss an algorithm called Sequential Minimal Optimization (SMO) introduced by John C. Platt (1998). A simplified variant of SMO is found in a Stanford CS229 course (2009). We start with the objective function \\(\\mathcal{J}(\\lambda)\\) in the fourth step and the two constraints in the third step. The intent is to solve for the lagrangian multipliers, e.g. (\\(\\alpha_i, \\alpha_j)\\), so that given \\(\\alpha = (\\alpha_1, \\alpha_2, \\alpha_3,\\ ...\\ \\alpha_n)\\), the objective function allows us to solve only one pair of elements at a time; meaning, the goal is to heuristically pick an arbitrary pair of \\(\\alpha\\) elements, e.g. \\(\\alpha_i\\) and \\(\\alpha_j\\), such that in solving for their values using SMO, we respect the following KKT constraints: \\[\\begin{align} \\alpha_i = 0\\ \\ \\ \\ &amp;\\rightarrow y_i f(\\mathbf{\\vec{x}_i}) \\ge 1 \\\\ 0 &lt; \\alpha_i &lt; C\\ \\ \\ \\ &amp;\\rightarrow y_i f(\\mathbf{\\vec{x}_i}) = 1 \\\\ \\alpha_i = C\\ \\ \\ \\ &amp;\\rightarrow y_i f(\\mathbf{\\vec{x}_i}) \\le 1 \\end{align}\\] To illustrate, we use Figure 10.25. Figure 10.25: Dual Constraint The first constraint (inequality constraint), namely \\(0 \\le \\alpha_i \\le C\\) and \\(0 \\le \\alpha_j \\le C\\), bounds the diagonal lines within the boxes shown in the figure. The second constraint (linear equality constraint), namely \\(\\sum_{i=1}^n \\alpha_i y_i = 0\\), limits us along the diagonal lines in the figure. To enforce the first constraint, we start with the following expression: \\[\\begin{align} \\gamma = \\alpha_i + s\\ \\alpha_j \\end{align}\\] Here, let \\(s = y_i y_j\\) and let the labels \\(y_i\\) and \\(y_j\\) to have values between (-1,1) respectively. Therefore, \\(s = -1\\) if \\(y_i \\ne y_j\\) and \\(s = +1\\) if \\(y_i = y_j\\). So that: \\[\\begin{align} \\text{if}\\ s = -1 \\text{ and } \\gamma &gt; 0, &amp;\\text{then} &amp;min(\\alpha_j) = 0, &amp;max(\\alpha_j) = C - \\gamma \\\\ \\text{if}\\ s = -1 \\text{ and } \\gamma &lt; 0, &amp;\\text{then} &amp;min(\\alpha_j) = -\\gamma, &amp;max(\\alpha_j) = C\\\\ \\text{if}\\ s = +1 \\text{ and } \\gamma &lt; C, &amp;\\text{then} &amp;min(\\alpha_j) = 0, &amp;max(\\alpha_j) = \\gamma\\\\ \\text{if}\\ s = +1 \\text{ and } \\gamma &gt; C, &amp;\\text{then} &amp;min(\\alpha_j) = \\gamma - C, &amp;max(\\alpha_j) = C \\end{align}\\] Combining the conditions above, we obtain the following upper (H) and lower (L) bounds: \\[\\begin{align} If\\ y_i \\ne y_j,&amp; L = max(0, \\alpha_j^{(t)} - \\alpha_i^{(t)}),&amp; H=min(C, C+ \\alpha_j^{(t)} - \\alpha_1^{(t)})\\\\ If\\ y_i = y_j,&amp; L = max(0, \\alpha_i^{(t)} + \\alpha_j^{(t)} -C),&amp; H=min(C, \\alpha_i^{(t)} + \\alpha_j^{(t)}) \\end{align}\\] To enforce the second constraint along the diagonal line, we update the second alpha (\\(\\alpha_j\\)) using the following formula: \\[\\begin{align} \\alpha_j^{(t+1)} = \\alpha_j^{(t)} - \\frac{y_j\\left(E_i - E_j\\right)}{\\eta} \\end{align}\\] where: \\[\\begin{align} E_i &amp;= f(\\mathbf{\\vec{x}}_i) - y_i \\\\ \\eta &amp;= 2\\ \\mathbf{K}\\left( \\mathbf{\\vec{x}}_i, \\mathbf{\\vec{x}}_j \\right) - \\mathbf{K}\\left( \\mathbf{\\vec{x}}_i, \\mathbf{\\vec{x}}_i \\right) - \\mathbf{K}\\left( \\mathbf{\\vec{x}}_j, \\mathbf{\\vec{x}}_j \\right) \\end{align}\\] Note that the eta (\\(\\eta\\)) symbol represents second derivative of the objective function along the diagonal line (Platt J.C. 1998). Here, we incorporate the use of Kernel function. Also, note that \\(\\mathbf{E_i}\\) is the error between the true label and the estimated output on the ith observation. See prediction function discussed above, namely \\(h(\\mathbf{x})\\) for \\(f(\\mathbf{\\vec{x}}_i)\\). The next step is to get the new value of the second alpha (\\(\\alpha_j\\)) by clipping it within the range [L, H]. \\[\\begin{align} \\alpha_j^{(t+1)} = \\begin{cases} H &amp; if\\ \\alpha_j^{(t+1)} &gt; H\\\\ \\alpha_j^{(t+1)} &amp; if\\ L \\le \\alpha_j^{(t+1)} \\le H\\\\ L &amp;\\ if\\ \\alpha_j^{(t+1)} &lt; L \\end{cases} \\label{eqn:eqnnumber415} \\end{align}\\] Equivalently, to get the new value of the first alpha (\\(\\alpha_i\\)), we use the y labels along with the second alpha (\\(\\alpha_i\\)): \\[\\begin{align} \\alpha_i^{(t+1)} = \\alpha_i^{(t)} + s\\left(a_j^{(t)} - a_j^{(t+1)}\\right) \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ s = y_i y_j \\end{align}\\] Now, to classify labels, we also need to calculate the intercept b. Here, we first compute for the \\(\\mathbf{b}_1\\) and \\(\\mathbf{b}_2\\) thresholds. Let the following be: \\[\\begin{align} \\mathbf{w}_i = y_i\\left(\\alpha_i^{(t+1)} - \\alpha_i^{(t)}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{w}_j = y_j\\left(\\alpha_j^{(t+1)} - \\alpha_j^{(t)}\\right) \\end{align}\\] therefore: \\[\\begin{align} b_1 = E_i + \\mathbf{w}_i \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_i} \\right) + \\mathbf{w}_j \\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j} \\right) + b^{(t)} \\end{align}\\] \\[\\begin{align} b_2 = E_j + \\mathbf{w}_i\\mathbf{K}\\left( \\mathbf{\\vec{x}_i}, \\mathbf{\\vec{x}_j} \\right) + \\mathbf{w}_j \\mathbf{K}\\left( \\mathbf{\\vec{x}_j}, \\mathbf{\\vec{x}_j} \\right) + b^{(t)} \\end{align}\\] then, we clip our intercept b like so: \\[\\begin{align} b^{(t+1)} = \\begin{cases} b_1 &amp; if\\ 0\\ &lt; \\alpha_i &lt; C \\\\ b_2 &amp; if\\ 0\\ &lt; \\alpha_j &lt; C \\\\ (b_1 + b_2)/2 &amp; otherwise \\end{cases} \\label{eqn:eqnnumber416} \\end{align}\\] Finally, we also need to handle our error cache required during enforcing the second constraint above, taking care of non-bound multipliers (\\(\\alpha_k\\)) such that \\(k \\ne i\\) and \\(k \\ne j\\). \\[\\begin{align} E^{(t+1)}_k = E^{(t)}_k + \\mathbf{w}_i\\mathbf{K}(\\mathbf{\\vec{x}}_i, \\mathbf{\\vec{x}}_k) + \\mathbf{w}_j\\mathbf{K}(\\mathbf{\\vec{x}}_j, \\mathbf{\\vec{x}}_k) + b^{(t)} - b^{(t+1)} \\end{align}\\] Below is our example implementation of SMO in R code following the pseudo-code in Platt J.C. (1998) and motivated by a python code from jonchar.net with modification. The implementation has three procedures, namely take.step(.), examine.example(.), and a main function - in our case, we have my.smo.svm(.) for main. Our implementation starts with a helper function, namely roll(.), for the random starting point of a vector. roll &lt;- function(x, start) { n = length(x) if (start &lt; 1 || start &gt;= n) return(x) x.e = x[(n - start + 1):n] x.b = x[1:(n - start)] c(x.e, x.b) } Then, we have two basic kernel functions - a linear kernel and a Gaussian kernel, respectively. linear.kernel &lt;- function(x1, x2, b = 1) { x1 %*% t(x2) + b } radial.kernel &lt;- function(x1, x2, sigma=1) { # Gaussian Kernel if (is.null(nrow(x1)) || dim(x1) == 1 ) { z = -sweep(x2, 2, x1, &#39;-&#39;) x.norm = apply( z , 1, function(x) { sqrt(sum(x^2)) }) } else if (is.null(nrow(x2)) || dim(x2) == 1 ) { z = -sweep(x1, 2, x2, &#39;-&#39;) x.norm = apply( z , 1, function(x) { sqrt(sum(x^2)) }) } else { pu = c() N = nrow(x1) for (i in 1:N) { u = matrix(rep(x1[i,],N),N, byrow=TRUE) a = array( as.matrix(u - x2), c(N,2,1)) pu = c(pu, a) } pu = array(pu, c(N,2,N)) x.norm = apply(pu, c(3,1), function(x) { sqrt(sum(x^2)) }) } exp( - (x.norm^2) / (2 * sigma^2) ) } More importantly, we implement our objective function and prediction (decision) functions: J = objective.function &lt;- function(alphas, kernel, x, y) { a = alphas; sum(a) - 0.5 * sum((a*a) * (y*y) * kernel(x,x)) } h = prediction.function &lt;- function(alphas, kernel, x1, x2, y, b) { (alphas * y) %*% kernel( x1, x2) - b } f = evaluate &lt;- function(i) { h(alphas, kernel, x, t(c(x[i,])), y, b) } The take.step(.) procedure implements the update rules for \\(\\alpha_i\\) and \\(\\alpha_j\\), including the b threshold and error cache. take.step &lt;- function(i1, i2, E2) { if (i1 == i2) { return(0) } alph1 = alphas[i1]; alph2 = alphas[i2] y1 = y[i1]; y2 = y[i2] s = y1 * y2 # Compute L, H if (y1 != y2) { L = max(0, alph2 - alph1); H = min(C, C + alph2 - alph1) } else if (y1 == y2) { L = max(0, alph1 + alph2 - C); H = min(C, alph1 + alph2) } if (L == H) { return(0) } if (eps &lt; alph1 &amp;&amp; alph1 &lt; C - eps) { E1 = errors[i1] } else { E1 = f(i1) - y1 } # Computer kernels k11 = kernel(t(x[i1,]), t(x[i1,])) k12 = kernel(t(x[i1,]), t(x[i2,])) k22 = kernel(t(x[i2,]), t(x[i2,])) eta = 2 * k12 - k11 - k22 if (eta &lt; 0) { a2 = alph2 - y2 * (E1 - E2) / eta if (a2 &lt; L) { a2 = L } else if (a2 &gt; H) { a2 = H } } else { adj.alphas = alphas adj.alphas[i2] = L L.obj = J(adj.alphas, kernel, x, y) adj.alphas[i2] = H H.obj = J(adj.alphas, kernel, x, y) if (L.obj &gt; H.obj + eps) { a2 = L } else if (L.obj &lt; H.obj - eps) { a2 = H } else { a2 = alph2 } } if (a2 &lt; 1e-8) { a2 = 0 } else if (a2 &gt; C - 1e-8) { a2 = C } if (abs(a2 - alph2) &lt; eps * (a2 + alph2 + eps)) { return(0) } # update alpha_i a2 = c(a2) a1 = c(alph1 + s * (alph2 - a2)) # update new b threshold w1 = c(y1 * (a1 - alph1)); w2 = c(y2 * (a2 - alph2)) b1 = c(E1 + w1 * k11 + w2 * k12 + b) b2 = c(E2 + w1 * k12 + w2 * k22 + b) if ( 0 &lt; a1 &amp;&amp; a1 &lt; C) { b.new = b1 } else if ( 0 &lt; a2 &amp;&amp; a2 &lt; C) { b.new = b2 } else { b.new = (b1 + b2) * 0.5 } # update alphas alphas[i1] &lt;&lt;- a1 alphas[i2] &lt;&lt;- a2 # update error cache if (0 &lt; a1 &amp;&amp; a1 &lt; C) { errors[i1] &lt;&lt;- 0 } if (0 &lt; a2 &amp;&amp; a2 &lt; C) { errors[i2] &lt;&lt;- 0 } alph.indices = which( !(seq(n) %in% c(i1,i2)) ) errors[alph.indices] &lt;&lt;- errors[alph.indices] + w1 * kernel(t(x[i1,]), x[alph.indices,]) + w2 * kernel(t(x[i2,]), x[alph.indices,]) + b - b.new # update b threshold b &lt;&lt;- c(b.new) return (1) } As the function’s name implies, we examine a given observation (or example) based on the second index. examine.example &lt;- function(i2) { y2 = y[i2] alph2 = alphas[i2] if (eps &lt; alph2 &amp;&amp; alph2 &lt; C - eps) { E2 = errors[i2] } else { E2 = f(i2) - y2 } r2 = E2 * y2 if ((r2 &lt; -tol &amp;&amp; alph2 &lt; C) || (r2 &gt; tol &amp;&amp; alph2 &gt; 0)) { i1.indices = which( alphas != 0 &amp; alphas != C ) m = length(i1.indices) # Consider heuristics in terms of max error deltas if (m &gt; 1) { if (E2 &gt; 0) { i1 = which.min(errors) } else if (E2 &lt;= 0) { i1 = which.max(errors) } if (take.step(i1, i2, E2)) { return(1) } } # Consider heuristics in terms of non-zero and non-C alphas if (m &gt; 0) { rand.i = sample.int(m, size=1) rolled.indices = roll(i1.indices, rand.i) for (i1 in rolled.indices) { if (take.step(i1, i2, E2)) { return(1) } } } # Otherwise, consider all observations rand.i = sample.int(n, size=1) for (i1 in roll(seq(n), rand.i)) { if (take.step(i1, i2, E2)) { return(1) } } } return(0) } Our training function is my.smo.svm(.) and it learns data for classification. train = my.smo.svm &lt;- function(x, C=1, tol=10e-3, eps=10e-3) { n = nrow(x) num.changed = 0 examine.all = 1 while (num.changed &gt; 0 || examine.all ) { num.changed = 0 if (examine.all) { for (i in 1:n) { num.changed = num.changed + examine.example(i) } } else { for (i in which( alphas != 0 &amp; alphas != C )) { num.changed = num.changed + examine.example(i) } } if (examine.all == 1) { examine.all = 0 } else if (num.changed == 0) { examine.all = 1 } } list(&quot;alphas&quot; = alphas, &quot;kernel&quot; = kernel, &quot;x&quot; = x, &quot;y&quot; = y, &quot;b&quot; = b) } Then, we have a function to plot the result of our SVM: plot.svm &lt;- function(title, model) { alphas = model$alphas; kernel = model$kernel x = model$x; y = model$y; b = model$b x.test = seq(min(x[,1]), max(x[,1]), length.out=100) y.test = seq(min(x[,2]), max(x[,2]), length.out=100) grid = matrix(rep(0, 100 * 100), 100, byrow=TRUE) for (j in 1:length(x.test)) { for (i in 1:length(y.test)) { grid[j,i] = c(h(alphas, kernel, x, t(c(x.test[j],y.test[i])), y, b)) } } plot(NULL, xlim=range(x[,1]), ylim=range(x[,2]), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=title) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0,v=0,lty=3) points(x[,1],x[,2], pch=20, col=ifelse(y == -1, &quot;darksalmon&quot;, &quot;navyblue&quot;)) no_opt = which(abs(alphas) &gt; eps) for (i in 1:length(no_opt)) { points(x[no_opt[i],1], x[no_opt[i],2], pch=1, cex=1.2, col=&quot;black&quot;) } contour(x = x.test, y = y.test, method = &quot;edge&quot;, lwd=c(1,2,1), grid, levels=c(-1, 0, 1), lty=c(2,1,2), col=c(&quot;red&quot;, &quot;black&quot;, &quot;blue&quot;), add=TRUE) } Also, for fix parameters, let us have the following global variables: tol = 10e-3 # tolerance eps = 10e-3 # constraint limit Now, to use our implementation, let us use the following dataset: set.seed(152) N = 100; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v); y1 = rep( 1, N) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v); y2 = rep(-1, N) x = cbind(c(x1.blue, x1.red), c(x2.blue, x2.red)) y = c(cbind(c(y1, y2))) Finally, let us run our implementation and plot. See Figure 10.26. set.seed(152) n = nrow(x) kernel = linear.kernel alphas = rep(0, n) b = 0 C = 5 errors = h(alphas, kernel, x, x, y, b) - y my.smo.model = my.smo.svm(x) plot.svm(&quot;SMO (linear)&quot;, my.smo.model) Figure 10.26: SMO (Linear) Below, we show learning SVM-SMO using radial kernel. See Figure 10.27. set.seed(152) n = 100 pi.set = seq(0, 2 * pi, length.out=n) x1.outer.rand = 2 * cos(pi.set) + runif(n, -0.5, 0.5) x2.outer.rand = 2 * sin(pi.set) + runif(n, -0.5, 0.5) x1.inner.rand = 0.5 * cos(pi.set) + runif(n, -0.5, 0.5) x2.inner.rand = 0.5 * sin(pi.set) + runif(n, -0.5, 0.5) x1.outer.line = 2 * cos(pi.set) x2.outer.line = 2 * sin(pi.set) x1.inner.line = 0.5 * cos(pi.set) x2.inner.line = 0.5 * sin(pi.set) x.line = rbind(cbind(x1.inner.rand, x2.inner.rand), cbind(x1.inner.rand, x2.inner.rand)) x = rbind(cbind(x1.outer.rand, x2.outer.rand), cbind(x1.inner.rand, x2.inner.rand)) y = c(rep(1, n), rep(-1, n)) set.seed(2020) n = nrow(x) kernel = radial.kernel alphas = rep(0, n) b = 0 C = 5 errors = h(alphas, kernel, x, x, y, b) - y my.smo.model = my.smo.svm(x) plot.svm(&quot;SMO (Radial)&quot;, my.smo.model) Figure 10.27: SMO (Radial) Below, we show another SVM-SMO learning using radial kernel. See Figure 10.28. set.seed(2020) n = 100 pi.set = seq(0, pi, length.out=n) x1.outer.rand = 2 * cos(pi.set) + runif(n, -0.5, 0.5) x2.outer.rand = 2 * sin(pi.set) + runif(n, -0.5, 0.5) x1.inner.rand = -2 * cos(pi.set) + runif(n, -0.5, 0.5) + 2 x2.inner.rand = -2 * sin(pi.set) + runif(n, -0.5, 0.5) + 1 x1.outer.line = 2 * cos(pi.set) x2.outer.line = 2 * sin(pi.set) x1.inner.line = -2 * cos(pi.set) + 2 x2.inner.line = -2 * sin(pi.set) + 1 x.line = rbind(cbind(x1.inner.rand, x2.inner.rand), cbind(x1.inner.rand, x2.inner.rand)) x = rbind(cbind(x1.outer.rand, x2.outer.rand), cbind(x1.inner.rand, x2.inner.rand)) y = c(rep(1, n), rep(-1, n)) set.seed(152) n = nrow(x) kernel = radial.kernel alphas = rep(0, n) b = 0 C = 5 errors = h(alphas, kernel, x, x, y, b) - y my.smo.model = my.smo.svm(x) plot.svm(&quot;SMO (Radial)&quot;, my.smo.model) Figure 10.28: SMO (Radial) We leave readers to adjust the random seed, number of samples, and the C parameter as an exercise. 10.2.3 SDCA-based SVM Another algorithm to mention for dual optimization is the Stochastic Dual Coordinate Ascent (SDCA) analyzed by Shai Shalev-Shwartz et al. (2008) along with reference to DCA as proposed by Hsieh C. et al. (2008). There are variants of SDCA, all intended to enhance the algorithm in parallel processing, distributed processing, and using mini-batch. In relation to the Dual Lagrangian formulation, we rewrite the second term of the objective function in the fourth step of SMO: \\[\\begin{align} \\mathcal{J}(\\alpha) = \\text{arg}\\ \\underset{\\alpha_i \\ge 0}{\\text{max}} \\left\\{\\sum_{i=1}^n \\alpha_i- \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j \\left(\\mathbf{\\vec{x}_i} \\cdot \\mathbf{\\vec{x}_j}\\right) \\right\\} \\end{align}\\] such that we have the following: \\[\\begin{align} \\frac{1}{2} \\sum_{i=1}^n \\sum_{j=1}^n \\alpha_i \\alpha_j y_i y_j (\\mathbf{x_i} \\cdot \\mathbf{x_j}) &amp;= \\frac{1}{2} \\left(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x_i} \\right) \\left( \\sum_{j=1}^n \\alpha_j y_j \\mathbf{x_j} \\right)\\\\ &amp;= \\frac{1}{2} \\mathbf{w} \\cdot \\mathbf{w}\\\\ &amp;= \\frac{1}{2}\\|\\mathbf{w}\\|^2_2 \\end{align}\\] where \\(\\frac{1}{2} \\left(\\sum_{i=1}^n \\alpha_i y_i \\mathbf{x_i} \\right)\\) is based on result of the partial derivative of the dual form with respect to w. Our objective function is therefore rewritten this way (note here that we are using the \\(\\lambda &gt; 0\\) as our regularizer): \\[\\begin{align} \\mathcal{J}(\\alpha) = \\text{arg}\\ \\underset{\\alpha_i \\ge 0}{\\text{max}} \\left\\{\\sum_{i=1}^n \\alpha_i- \\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 \\right\\} \\end{align}\\] Now consider the delta of \\(\\alpha_i\\) denoted as \\(\\Delta^{(\\alpha)}_i\\) so that \\(\\alpha^{(t+1)}_i = \\alpha_i^{(t)} + \\Delta\\alpha_i\\), we then have the dual form for the ith observation: \\(\\mathbf{D(\\alpha_i + \\Delta^{(\\alpha)}_i)}\\). Equivalently, our primal form is updated with the following delta \\((\\lambda n)^{(-1)}\\Delta^{(\\alpha)} x_i\\) so that we therefore have \\(\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + (\\lambda n)^{(-1)}\\Delta^{(\\alpha)} x_i\\). We then update our objective function: \\[\\begin{align} \\mathcal{J}(\\alpha) = \\text{arg}\\ \\underset{\\alpha_i \\ge 0}{\\text{max}} \\left\\{\\sum_{i=1}^n \\alpha_i + \\Delta^{(\\alpha)}) - \\frac{\\lambda}{2} \\|\\mathbf{w} + (\\lambda n)^{-1} \\Delta^{(\\alpha)} x_i\\|^2 \\right\\} \\end{align}\\] With all that, we now have our update rules for SDCA like so: \\[\\begin{align} \\Delta_i^{(\\alpha)} {}&amp;= y_i\\ \\underbrace{ \\text{max} \\left(0, \\text{min} \\left(1, \\frac{1 - y_i x_i^T \\mathbf{w}^{(t-1)}}{\\|x_i\\|^2_2/(\\lambda n)}\\right) + y_i \\alpha_i^{(t-1)} \\right) }_{\\text{hinge-loss in closed form}} - \\alpha_i^{(t-1)}\\\\ \\alpha_{i}^{(t+1)} &amp;= \\alpha_i^{(t)} + \\Delta_i^{(\\alpha)}\\ \\ \\ \\ \\ \\ \\ \\leftarrow \\text{(dual update)} \\\\ \\mathbf{w}^{(t+1)} &amp;= \\mathbf{w}^{(t)} + \\frac{\\Delta_i^{(\\alpha)}}{\\lambda n} x_i\\ \\ \\ \\leftarrow \\text{(primal update)} \\end{align}\\] For the algorithm, we focus on the SDCA-Perm variant from Shai Shalev-Shwatz and Tong Zhang (2013) in solving the dual problem of SVM using the hinge-loss and random option. Below is the algorithm for review. \\[ \\begin{array}{ll} \\mathbf{\\text{SDCA Algorithm}} \\\\ \\text{Shalev-Schwartz et al. (arxiv 1209.1873v2 2013)}\\\\ \\\\ \\text{Input:}\\ \\ S, \\lambda, \\text{T} \\\\ \\text{Set}\\ \\mathbf{w}_0 = z + \\frac{1}{\\lambda n} \\sum_{i=1}^n \\alpha^{0} x_i\\\\ \\text{loop}\\ t\\ in\\ 1,2,...,\\text{T} \\\\ \\ \\ \\ \\ \\text{Choose}\\ i_t\\ \\in \\{1,...,|S|\\}\\ \\ \\leftarrow \\ \\ \\text{(uniformly random)}\\\\ \\ \\ \\ \\ \\Delta_i^{(\\alpha)} = y_i\\ \\text{max} \\left(0, \\text{min} \\left(1, \\frac{1 - y_i x_i^T \\mathbf{w}^{(t-1)}}{\\|x_i\\|^2_2/(\\lambda n)}\\right) + y_i \\alpha_i^{(t-1)} \\right) - \\alpha_i^{(t-1)}\\\\ \\ \\ \\ \\ \\alpha_i^{(t+1)} = \\alpha_i^{(t)} + \\Delta_i^{(\\alpha)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\leftarrow \\text{(dual update)} \\\\ \\ \\ \\ \\ \\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} + \\frac{\\Delta_i^{(\\alpha)}}{\\lambda n} x_i\\ \\ \\ \\ \\ \\ \\leftarrow \\text{(primal update)} \\\\ \\text{end loop} \\\\ \\text{Ouput}\\ \\mathbf{w}^{(T+1)} \\end{array} \\] Below is an example implementation of SDCA for SVM. Here, we use sample.int(.) to simulate random sampling of the index (\\(i^{(t)}\\)). my.sdca.svm &lt;- function(x, y, lambda=0.01, limit=15000) { n = nrow(x); x = cbind(rep(1, n), x) p = ncol(x) w = rep(0, p) alphas = rep(0, n) for (t in 1:limit) { i = sample.int(n, size=1) xi = x[i,]; yi = y[i] norm_x = sqrt(sum(xi^2)) delta.alpha_i = yi * max(0, min(1, ( (1 - yi * t(xi) %*% w) * lambda * n/ norm_x )) + yi * alphas[i]) - alphas[i] alphas[i] = alphas[i] + delta.alpha_i w = w + ( delta.alpha_i * xi) / (lambda * n ) } list(&quot;alphas&quot; = alphas, &quot;w&quot; = w) } Let us use the same linear dataset as we did in SMO previously to use our implementation. set.seed(152) eps = 10e-3 N = 20; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v); y1 = rep( 1, N) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v); y2 = rep(-1, N) x = cbind(c(x1.blue, x1.red), c(x2.blue, x2.red)) y = cbind(c(y1, y2)) Moreover, because our algorithm is stochastic, let us use a random seed for our implementation. set.seed(152) svm.sdca.model = my.sdca.svm(x,y) hplanes = svm.hyperplanes(svm.sdca.model$w) We then plot. See Figure 10.29. Figure 10.29: SVM (Stochastic Dual Coordinate Ascent) Finally, for prediction, we can use the model to classify any new or missing data. \\[\\begin{align} h(x; \\mathbf{w}) = \\text{sign}(g(x)) = \\text{sign}(\\mathbf{w}^T \\mathbf{x} + b)\\ \\ \\ \\leftarrow\\ \\ \\ \\ \\begin{cases} +1 &amp; \\mathbf{w}^T \\mathbf{x} \\ge 0\\\\ -1 &amp; \\mathbf{w}^T \\mathbf{x} &lt; 0\\\\ \\end{cases} \\label{eqn:eqnnumber417} \\end{align}\\] x.new = rbind( c(&quot;intercept&quot; = 1, &quot;x1&quot; = -2, &quot;x2&quot; = 2), c(&quot;intercept&quot; = 1, &quot;x1&quot; = 2, &quot;x2&quot; = -2)) my.sdca.prediction &lt;- function(w, x) { c(sign(w %*% t(x))) } h = my.sdca.prediction(svm.sdca.model$w, x.new) pred = ifelse(h == 1, &quot;positive&quot;, &quot;negative&quot;) print(cbind(x.new, pred), quote=FALSE, right=TRUE) ## intercept x1 x2 pred ## [1,] 1 -2 2 positive ## [2,] 1 2 -2 negative We showed a sample Kernel functions in our SMO-based SVM. We leave readers to investigate Kernel functions in use with SDCA-based SVM and as an exercise, modify our example implementation above. Also, there are variants of SDCA worth investigating with the introduction of mini-batching, along with properties that make the algorithm adaptive and distributed. Also, for other variants of SVM, we leave readers to investigate Top-K multiclass SVM for a high accuracy multiclass SVM algorithm with top k error prioritization, and Ensemble SVM to support ensemble models for SVM models. Note that an example application for Top-K multi-classification is around image/object classification and image/object segmentation. As for prediction performance for SVM methods, we can reference ConfusionMatrix for metrics such as specificity, sensitivity, accuracy, F1 score, and others. 10.3 Multi-class Classification (Supervised) Most of our discussions are around binary classifications in previous sections. Our goal now is to demonstrate multivariate classification. Here, we revisit Decision Trees and review goodness of split based on the purity (or impurity) of nodes. Our goal is to group observations of variables into homogeneous instances (or nodes) which have low degrees of impurity. A starting point is to discuss Naive Bayes as a simple classifier. 10.3.1 Bayesian Classification Let us start the discussion of multi-classification with Naive Bayes as a simple classifier. Recall the Bayes formula derived from Chapter 7 (Bayesian Computation I): \\[\\begin{align} P(\\text{Class|Features}) = \\frac{P(\\text{Features|Class}) P(\\text{Class}) }{P(\\text{Features}) } \\ \\ \\rightarrow \\ \\ P(Y|X) = \\frac{P(X|Y)P(Y)}{P(X)} \\end{align}\\] We can then expand the formula to accommodate a case in which our posterior distribution is based on the condition of multiple distributions. \\[\\begin{align} P(y_k|x_{1}, x_{2}, x_{3},...,x_{n}) = \\frac{\\left[\\prod_{i=1}^{n}P(x_{i}|y_k)\\right]\\cdot P(y_k)} {\\prod_{i=1}^{n}P(x_{i})} \\end{align}\\] Expanding the likelihood and prior, we get the following: \\[\\begin{align} \\underbrace{P(x_i = j|y_i = k) = \\frac{\\sum_i^N\\mathbf{I}(x_i=j, y_i = k)}{\\sum_i^N\\mathbf{I}(y_i = k) }}_{\\text{likelihood}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{P(y_i = k) = \\frac{\\sum_i^N\\mathbf{I}(y_i = k)}{N}}_{\\text{prior}} \\end{align}\\] where: \\(X\\) represents a set of features that are continuous or discrete y being the class or label k is a specific value of a class in Y. j is a specific value of a feature in X. For a toy example, let us use the common frequency (probabilities) table to illustrate naive bayesian classification. Suppose we are looking to form a new basketball team to represent a national basketball competition. To form the team, we need quality players with specific requirements. - the idea is to classify whether a candidate is a Class A caliber player, a Class B caliber player, and so on, based on the following features: SPG - Scores Per Game RPG - Rebounds Per Game APG - Assists Per Game FTPG - Free Throws per Game A qualified candidate is with high SPG, high RPG, or high APG if one gets double-digit statistics, e.g., 20 points per game and ten assists per game. Table 10.2 shows 200 observed candidates with the qualifications we seek: Table 10.2: National Basketball Competition Player (y) FTPG (\\(x_{1}\\)) APG (\\(x_{2}\\)) RPG (\\(x_{3}\\)) SPG (\\(x_{4}\\)) Total Class A 5 (0.50) 9 (0.90) 7 (0.70) 10 (1.00) \\(\\mathbf{10}\\) (0.05) Class B 10 (0.20) 40 (0.80) 25 (0.50) 25 (0.50) \\(\\mathbf{50}\\) (0.25) Class C 28 (0.20) 70 (0.50) 84 (0.60) 14 (0.01) \\(\\mathbf{140}\\) (0.70) Total 43 (0.22) 119 (0.60) 116 (0.58) 49 (0.24) \\(\\mathbf{200}\\) (1.00) Of the overall 200 candidates in Table 10.2, we see only 5% are Class A caliber players. Of the 10 Class A caliber players, only 5 achieve high FTPG. With reference to Table 10.2, we can formulate the probabilities. It can be shown that the probability of getting players with high free throws per game (FTPG) is 22% and that the probability of players with high scores per game (SPG) is 24%. They are expressed accordingly - they form the marginal probability, namely \\(P(X) \\equiv P(Features)\\): \\[ P(x_1 = FTPG) = \\frac{43}{200} = 22\\%\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(x_4 = SPG) = \\frac{49}{200} = 24\\% \\] We can also show that the likelihood of a high FTPG given Class A caliber player is 50% and that the likelihood of a high SP given Class A caliber player is 100%. They are also expressed accordingly - and they form the likelihood, namely \\(P(x|y) \\equiv P(Features|\\text{Class})\\): \\[\\begin{align*} P(x_1 = FTPG | y = \\text{Class A}) = \\frac{5}{10} = 50\\% \\\\ P(x_4 = SPG | y = \\text{Class A}) = \\frac{10}{10} = 100\\% \\end{align*}\\] Now the probability of getting a Class A player given all the features is expressed as such: \\[\\begin{align} P(\\text{Class A}&amp;|FTPG, APG, RPG, SPG) \\nonumber \\\\ &amp;=\\frac{P(FTPG|A)\\ P(APG|A)\\ P(RPG|A) \\ P(SPG|A)\\ P(A)} {P(FTPG)\\ P(APG)\\ P(RPG)\\ P(SPG)} \\\\ &amp;= \\frac{0.50\\times 0.90\\times 0.70\\times 1.00\\times 0.05}{0.22\\times 0.60\\times 0.58\\times 0.24} \\nonumber \\\\ &amp;= 0.8571708 \\nonumber \\end{align}\\] If we are looking for Class B players, we will be estimating it this way: \\[\\begin{align} P(\\text{Class B}&amp;|FTPG, APG, RPG, SPG) \\nonumber \\\\ &amp;=\\frac{P(FTPG|B)\\ P(APG|B)\\ P(RPG|B)\\ P(SPG|B)\\ P(B)} {P(FTPG)\\ P(APG)\\ P(RPG)\\ P(SPG)} \\\\ &amp;= \\frac{0.20 \\times 0.80\\times 0.50\\times 0.50\\times 0.25}{0.22\\times 0.60\\times 0.58\\times 0.24} \\nonumber \\\\ &amp;= 0.5442355 \\nonumber \\end{align}\\] If we are looking for Class C players, we will be estimating it this way: \\[\\begin{align} P(\\text{Class C}&amp;|FTPG, APG, RPG, SPG) \\nonumber \\\\ &amp;=\\frac{P(FTPG|C)\\ P(APG|C)\\ P(RPG|C)\\ P(SPG|C)\\ P(C)} {P(FTPG)\\ P(APG)\\ P(RPG)\\ P(SPG)} \\\\ &amp;= \\frac{0.20\\times 0.50\\times 0.60\\times 0.01\\times 0.70}{0.22\\times 0.60\\times 0.58\\times 0.24} \\nonumber \\\\ &amp;= 0.02285789 \\nonumber \\end{align}\\] Therefore, it can be shown that with the 200 available candidates, we have about 85.72% probability of getting a Class A caliber player given the presented statistics, about 54.42% probability of getting a Class B caliber player, and about 2.29% probability of getting a Class C caliber player. Now, assume a training set claims to have the following probabilities for high FTPG, high APG, and high RPG, but zero probability for having low in SPG. \\[ \\begin{array}{rl} P(x_1 = FTPG | \\text{A}) = 0.50\\\\ P(x_1 = APG | \\text{A}) = 0.90\\\\ P(x_1 = RPG | \\text{A}) = 0.70\\\\ P(x_1 = SPG | \\text{A}) = 0.00\\\\ \\end{array}\\ \\ \\begin{array}{rl} P(x_1 = FTPG | \\text{B}) = 0.20\\\\ P(x_1 = APG | \\text{B}) = 0.80\\\\ P(x_1 = RPG | \\text{B}) = 0.50\\\\ P(x_1 = SPG | \\text{B}) = 0.00\\\\ \\end{array}\\ \\ \\begin{array}{rl} P(x_1 = FTPG | \\text{C}) = 0.20\\\\ P(x_1 = APG | \\text{C}) = 0.50\\\\ P(x_1 = RPG | \\text{C}) = 0.60\\\\ P(x_1 = SPG | \\text{C}) = 0.00\\\\ \\end{array} \\] If even one of the high statistics per game has zero frequency, then the probability of predicting such a class is always zero. It is a disadvantage for Bayes Classification. Here, we use Laplacian Smoothing as a solution to correct such a disadvantage. Basically, for Naive Bayes which has the form \\(P(y_i = k|x_i) \\propto P(x_i|y_i = k)P(y_i = k)\\), we add \\(\\lambda\\) like so: \\[\\begin{align} \\underbrace{P(x_i = j|y_i = k) = \\frac{\\sum_i^N\\mathbf{I}(x_i=j, y_i = k) + \\lambda}{\\sum_i^N\\mathbf{I}(y_i = k) + J\\lambda}}_{\\text{likelihood}} \\ \\ \\ \\ \\ \\underbrace{P(y_i = k) = \\frac{\\sum_i^N\\mathbf{I}(y_i = k) + \\lambda}{N + K\\lambda}}_{\\text{prior}} \\end{align}\\] where: J is the number of categorical values for \\(x_i\\). Here, we only have two (low or high). K is the number of categorical values for \\(y_i\\). Here, we have three (A, B, C). So that if \\(\\lambda = 1\\) (arbitrarily choose a small number) \\[ \\begin{array}{lrr} P(x_1 = FTPG | \\text{A}) &amp;= \\frac{5 + 1}{10 + 2\\times1} = 0.50\\\\ P(x_1 = APG | \\text{A}) &amp;= \\frac{9 + 1}{10 + 2\\times1} = 0.83\\\\ P(x_1 = RPG | \\text{A}) &amp;= \\frac{7 + 1}{10 + 2\\times1} = 0.67\\\\ P(x_1 = SPG | \\text{A}) &amp;= \\frac{0 + 1}{10 + 2\\times1} = 0.08\\\\ \\end{array}\\ \\ \\ \\ \\ \\ \\begin{array}{lrr} P(y_k = \\text{A}) = \\frac{10 + 1}{200 + 3\\times1} = 0.0541872\\\\ P(y_k = \\text{B}) = \\frac{50 + 1}{200 + 3\\times1} = 0.2512315\\\\ P(y_k = \\text{C}) = \\frac{140 + 1}{200 + 3\\times1} = 0.6945813\\\\ \\end{array} \\] Notice now that we have a small probability assigned to \\(P(x_1 = SPG | \\text{A})\\). We leave readers to compute the other probabilities. 10.3.2 Classification Trees In this section, it helps to refresh our understanding of Decision Trees. Recall in Regression trees the use of SSE as basis for our loss function to determine goodness of fit. We now extend the concept by using other splitting criteria based from probability theory and information theory. In the context of Classification Trees when dealing with categorical target variables, we consider basic metrics such Entropy, Gini Index, Gini Impurity, and Information Gain for our loss (and gain) function. In the same context, we use classification error rate to validate performance. CHAID, ID3, C4.5, C5.0 Apart from CART developed in 1984, other decision tree algorithms are available for review. Three such algorithms were introduced by Ross Quinlan (1986, 1996). The algorithms differ in addressing the following core criteria and strategy, among other considerations: splitting criterion, stopping criterion, and pruning strategy. Note that splitting criterion is based on measures of node impurity. Also, note in the Decision Tree section that we put a discussion of CART in the context of Regression and bifurcation (binary split); however, in the context of Classification, the implementation of Cart supports multiway split along with the use of Gini Index (also called Gini Impurity) to determine the split. Here, instead of detailing classic classification tree algorithms, let us briefly introduce them (Hssina B. et al. 2014): CHAID is called Chi-square automatic interaction detector and was introduced by Gordon Kass in 1980. The algorithm uses Chi-square as splitting criterion. ID3 is called Iterative Dichotomiser 3 introduced by R. Quinlan in 1983-86. The algorithm uses Information gain as the splitting criterion and supports multiway split. C4.5 is a successor of ID3 introduced by R. Quinlan in 1993. The algorithm uses Gain Ratio as the splitting criterion. It enforces a rule-based algorithm that supports continuous and categorical attributes, missing data, post-pruning, and multiway split. C5.0 is a successor of C4.5 introduced by R. Quinlan R. in 1994. The algorithm uses Information Gain as the splitting criterion. It comes with improved speed and memory efficiency, among other enhancements. It supports discrete, continuous, date and time, timestamp, and categorical attributes. In our discussions ahead, we illustrate the use of Gini Index and Information Gain as we build our classification tree. Recall under the Regression Trees Subsection under Regression Section the use of CART to perform tree regression. Here, we use the same method with the intent to perform tree classification. A simple tree classification using CART is shown in Figure 10.30 using an R dataset called iris. pacman::p_load(rpart,rpart.plot) set.seed(142) data(iris) rpart.control = rpart.control(minsplit=2, maxdepth=15, minbucket = 2) tree.model = rpart(Species ~ ., data = iris, control = rpart.control) rpart.plot(tree.model) Figure 10.30: Classification Tree To illustrate tree classification, we start by building a tree and splitting tree nodes. First, let us view the structure of our dataset, namely iris. Here, we have three independent variables, namely Sepal.Length, Sepal.Width, Petal.Length, and Petal.Width. Our target (or dependent) variable is Species. See below: options(width=56) str(iris, width=56, strict.width=&quot;wrap&quot;) ## &#39;data.frame&#39;: 150 obs. of 5 variables: ## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ## ... ## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 ## 3.1 ... ## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 ## 1.5 ... ## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 ## 0.1 ... ## $ Species : Factor w/ 3 levels ## &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... Second, we also use Figure 10.2 similar to Regression Trees under Regression Section. Note that our target variable is categorical in this case. Also, to illustrate, let us choose one of the features, namely Sepal.Length and arbitrarily split the observations perhaps somewhere in the middle (say at index 70). options(width=70) datacars = iris sepal.length = datacars[[&quot;Sepal.Length&quot;]] (sorted.input = sort(sepal.length, index.return = TRUE))$x ## [1] 4.3 4.4 4.4 4.4 4.5 4.6 4.6 4.6 4.6 4.7 4.7 4.8 4.8 4.8 4.8 4.8 ## [17] 4.9 4.9 4.9 4.9 4.9 4.9 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 ## [33] 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.2 5.2 5.2 5.2 5.3 5.4 5.4 ## [49] 5.4 5.4 5.4 5.4 5.5 5.5 5.5 5.5 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6 ## [65] 5.6 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.8 5.8 5.8 5.8 5.8 5.8 5.8 ## [81] 5.9 5.9 5.9 6.0 6.0 6.0 6.0 6.0 6.0 6.1 6.1 6.1 6.1 6.1 6.1 6.2 ## [97] 6.2 6.2 6.2 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.4 6.4 6.4 6.4 ## [113] 6.4 6.4 6.4 6.5 6.5 6.5 6.5 6.5 6.6 6.6 6.7 6.7 6.7 6.7 6.7 6.7 ## [129] 6.7 6.7 6.8 6.8 6.8 6.9 6.9 6.9 6.9 7.0 7.1 7.2 7.2 7.2 7.3 7.4 ## [145] 7.6 7.7 7.7 7.7 7.7 7.9 options(width=70) split.input &lt;- function(sorted.input, idx, is.factor = FALSE) { n = length(sorted.input$x) left.indices = sorted.input$ix[1:idx] right.indices = sorted.input$ix[(idx+1):n] list(&quot;left&quot; = left.indices, &quot;right&quot; = right.indices ) } split.index = 70 (split = split.input(sorted.input, split.index )) ## $left ## [1] 14 9 39 43 42 4 7 23 48 3 30 12 13 25 31 46 ## [17] 2 10 35 38 58 107 5 8 26 27 36 41 44 50 61 94 ## [33] 1 18 20 22 24 40 45 47 99 28 29 33 60 49 6 11 ## [49] 17 21 32 85 34 37 54 81 82 90 91 65 67 70 89 95 ## [65] 122 16 19 56 80 96 ## ## $right ## [1] 97 100 114 15 68 83 93 102 115 143 62 71 150 63 79 84 ## [17] 86 120 139 64 72 74 92 128 135 69 98 127 149 57 73 88 ## [33] 101 104 124 134 137 147 52 75 112 116 129 133 138 55 105 111 ## [49] 117 148 59 76 66 78 87 109 125 141 145 146 77 113 144 53 ## [65] 121 140 142 51 103 110 126 130 108 131 106 118 119 123 136 132 The left split has a total of 70 observations, and the right split has a total of 80 observations. Third, we determine the number of classes in our target variable. Below, we have three classes in the Species target variable, namely Setosa, Versicolor, and Virginica. levels(iris$Species) ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Fourth, we then determine the distribution of the classes between the two splits. library(plyr) left = iris[split$left,] right = iris[split$right,] iris.dist = cbind(count(left, &#39;Species&#39;), count(right, &#39;Species&#39;), count(iris, &#39;Species&#39;))[,c(1,2,4,6)] colnames(iris.dist) = c(&quot;Class&quot;, &quot;Left Count&quot;, &quot;Right Count&quot;, &quot;Total Count&quot;) iris.dist ## Class Left Count Right Count Total Count ## 1 setosa 49 1 50 ## 2 versicolor 19 31 50 ## 3 virginica 2 48 50 Fifth, to know if the split is good, we need to use specific metrics. For Regression, we use SSE. For classification, let us illustrate the use of Gini Index (GI) and Gini Information Gain (IG). \\[\\begin{align} GI_{\\text{(gini index)}} &amp;=\\sum_{k=1}^K P_k \\times ( 1 - P_k) &amp;\\ \\ \\ \\ \\ \\text{where K = number of classes} \\\\ &amp;=1 - \\sum_{k=1}^K\\left[P(Y=y_k)\\right]^2 \\end{align}\\] For a perfect split, we have the following Gini Index: \\[ \\begin{array}{ll} GI_{(perfect)} &amp;= 1 - \\left[ P(Y=\\text{setosa})^2 + P(Y=\\text{versicola})^2 + P(Y=\\text{virginica})^2\\right] \\\\ &amp;= 1 - \\left[ \\left(\\frac{50}{150}\\right)^2 + \\left(\\frac{50}{150}\\right)^2 + \\left(\\frac{50}{150}\\right)^2\\right]\\\\ &amp;= 0.6666667 \\end{array} \\] For each split in our example above, we have the following Gini Index: \\[\\begin{align} GI_{(left)} &amp;= 1 - [ P(Y=\\text{setosa}|X_{(left)})^2 + \\nonumber\\\\ &amp;\\ \\ \\ \\ P(Y=\\text{versicola}|X_{(left)})^2 + P(Y=\\text{virginica}|X_{(left)})^2 ] \\\\ &amp;= 1 - \\left[ \\left(\\frac{49}{70}\\right)^2 + \\left(\\frac{19}{70}\\right)^2 + \\left(\\frac{2}{70}\\right)^2\\right] \\nonumber \\\\ &amp;= 0.4355102 \\nonumber \\end{align}\\] \\[\\begin{align} GI_{(right)} &amp;= 1 - [ P(Y=\\text{setosa}|X_{(right)})^2 + P(Y=\\text{versicola}|X_{(right)})^2 + \\nonumber \\\\ &amp;\\ \\ \\ \\ P(Y=\\text{virginica}|X_{(right)})^2 ] \\\\ &amp;= 1 - \\left[ \\left(\\frac{1}{80}\\right)^2 + \\left(\\frac{31}{80}\\right)^2 + \\left(\\frac{48}{80}\\right)^2\\right] \\nonumber\\\\ &amp;= 0.4896875 \\nonumber \\end{align}\\] To now get the goodness of split, we use the following formula: \\[\\begin{align} IG_{(gini)} = GI_{(perfect)} - \\left[ P(left) \\times GI_{(left)} + P(right) \\times GI_{(right)} \\right] \\end{align}\\] where \\(P(left)\\) is the weight of impurity of the left split and \\(P(right)\\) is the weight of impurity of the right split. \\[ \\begin{array}{lll} P(left) = \\frac{49 + 19 + 2}{150} = \\frac{70}{150} &amp;\\ \\ \\ \\ \\ &amp;P(left) = \\frac{1 + 31 + 48}{150} = \\frac{80}{150} \\end{array} \\] Therefore, our Gini Gain which calculates the amount of impurity removed is: \\[ \\begin{array}{ll} IG_{(gini)} &amp;= 0.6666667 - \\left[ \\frac{70}{150}\\times(0.4355102) + \\frac{80}{150} \\times (0.4896875) \\right]\\\\ &amp;= 0.2022619 \\end{array} \\] Below, we have our example implementation of Gini Gain: gini = gini.index &lt;- function(y, lev) { n = length(y) gi = 0 for (k in lev) { cl.n = sum(y == k) gi = gi + (cl.n / n)^2 } 1 - gi } gain = gini.gain &lt;- function(parent, left, right, lev) { left.n = length(left) right.n = length(right) total.n = left.n + right.n p.left = left.n / total.n p.right = right.n / total.n gini(parent, lev) - (p.left * gini(left, lev) + p.right * gini(right, lev)) } y = iris$Species lev = levels(y) print(paste0(&quot;Gini Gain: &quot;, gain (y, y[split$left], y[split$right], lev))) ## [1] &quot;Gini Gain: 0.202261904761905&quot; Sixth, alternatively, we can also use Entropy (H) and Entropy Information Gain (IG) to measure the goodness of fit. \\[\\begin{align} H_{\\text{(entropy)}} = - \\sum_{i=1}^K P_k \\times\\ \\log_e(P_k) \\end{align}\\] For a perfect split, we have the following Entropy: \\[\\begin{align*} \\begin{array}{rl} H{\\text{(perfect)}} &amp;= - \\left[ \\begin{array}{ll} P(Y = setosa) \\times\\ \\log_e P(Y = setosa) +\\\\ P(Y = versicola) \\times\\ \\log_e P(Y = versicola) +\\\\ P(Y = virginica) \\times\\ \\log_e P(Y = virginica) \\end{array} \\right]\\\\ \\\\ &amp;= - \\left[ \\frac{50}{150} \\log_e \\frac{50}{150} + \\frac{50}{150} \\log_e \\frac{50}{150}+ \\frac{50}{150} \\log_e \\frac{50}{150} \\right]\\\\ &amp;= - \\left[-1.098612\\right] = 1.098612 \\end{array} \\end{align*}\\] For each split in our example above, we have the following Entropy: \\[\\begin{align*} \\begin{array}{rl} H{\\text{(left)}} &amp;= - \\left[ \\begin{array}{ll} P(Y = setosa|X_{(left)}) \\times\\ \\log_e P(Y = setosa|X_{(left)}) +\\\\ P(Y = versicola|X_{(left)}) \\times\\ \\log_e P(Y = versicola|X_{(left)}) +\\\\ P(Y = virginica|X_{(left)}) \\times\\ \\log_e P(Y = virginica|X_{(left)}) \\end{array} \\right]\\\\ \\\\ &amp;= - \\left[ \\frac{49}{70} \\log_e \\frac{49}{70} + \\frac{19}{70} \\log_e \\frac{19}{70}+ \\frac{2}{70} \\log_e \\frac{2}{70} \\right]\\\\ &amp;= -\\left[-0.705212\\right] = 0.705212 \\\\ \\\\ H{\\text{(right)}} &amp;= - \\left[ \\begin{array}{ll} P(Y = setosa|X_{(right)}) \\times\\ \\log_e P(Y = setosa|X_{(right)}) +\\\\ P(Y = versicola|X_{(right)}) \\times\\ \\log_e P(Y = versicola|X_{(right)}) +\\\\ P(Y = virginica|X_{(right)}) \\times\\ \\log_e P(Y = virginica|X_{(right)}) \\end{array} \\right]\\\\ \\\\ &amp;= - \\left[ \\frac{1}{80} \\log_e \\frac{1}{80} + \\frac{31}{80} \\log_e \\frac{31}{80}+ \\frac{48}{80} \\log_e \\frac{48}{80} \\right]\\\\ &amp;= -\\left[-0.728636\\right] = 0.728636 \\end{array} \\end{align*}\\] To now get the goodness of split, we use the following formula: \\[\\begin{align} IG_{(entropy)} &amp;= H_{(perfect)} - \\left[ P(left) \\times H_{(left)} + P(right) \\times H_{(right)} \\right]\\\\ &amp;= 1.098612 - \\left[ \\frac{70}{150} (0.705212) + \\frac{80}{150} (0.728636) \\right] \\nonumber \\\\ &amp;= 0.3809072 \\nonumber \\end{align}\\] Below, we have our example implementation of Entropy Gain: entropy &lt;- function(y, lev) { n = length(y) e = 0 for (k in lev) { cl.n = sum(y == k) p = cl.n / n e = e + p * log(p, exp(1)) } -e } entropy.gain &lt;- function(parent, left, right, lev) { left.n = length(left) right.n = length(right) total.n = left.n + right.n p.left = left.n / total.n p.right = right.n / total.n entropy(parent, lev) - (p.left * entropy(left, lev) + p.right * entropy(right, lev)) } y = iris$Species lev = levels(y) print(paste0(&quot;Entropy Gain: &quot;, entropy.gain (y, y[split$left], y[split$right], lev))) ## [1] &quot;Entropy Gain: 0.38090751345448&quot; Seventh, let us now revisit our implementation of Regression Trees and modify a few of the functions we used to build a Regression Tree. For intuition of the few functions, reference our detailed discussion in the Regression Trees Section. The first function to modify is the split.loss(.) function. The modification replaces the loss function that uses the SSE measure with the Gini Index and Gini Gain measure. Also, the minimum bucket applies not only to the tree node but also to the individual classes so that if only one class has observations greater than minbucket, then no further split is needed. The measure of improvement is measured using the Gini Gain of parents and children like so: \\[\\begin{align} improve = \\frac{IG_{(splits)}}{IG_{(parent)}} \\end{align}\\] class.probs &lt;- function(y) { K = length(levs) total = length(y) probs = rep(0, K); i = 0 for (lev in levs) { i = i + 1; probs[i] = sum(y == lev) / total } probs } split.loss &lt;- function(loss, sort.input, output, left, right, avg, minbucket ) { cs = loss n = length(output) probs = class.probs(output) parent.gain = gini(output, levs) if (length(left) &gt;= minbucket &amp;&amp; length(right) &gt;= minbucket ) { # no split if only one class has &gt; minbucket if (sum( probs * n &gt; minbucket ) &gt; 1) { left.idx = sort.input$ix[left] right.idx = sort.input$ix[-left] o1 = output[left.idx]; o2 = output[right.idx] child.probs = round(probs[which.max(probs)]*100,0) child.class = levs[which.max(probs)] child.gain = gain(output, o1, o2, levs) child.improve = child.gain / parent.gain o.len = n; l.len = length(o1); r.len = length(o2) cs$split = c(cs$split, avg) cs$Pr = c(cs$Pr, child.probs) cs$class = c(cs$class, child.class) cs$gain = c(cs$gain, round(child.gain,4)) cs$improve = c(cs$improve, round(child.improve*100,3)) cs$obs = c(cs$obs, o.len); cs$l.son = c(cs$l.son, l.len); cs$r.son = c(cs$r.son, r.len) cs$left.indices[[as.character(avg)]] = left.idx cs$right.indices[[as.character(avg)]] = right.idx } } cs } The second function to modify is our optimizer function which references the minimum.loss(.) function in Regression. Here in Classification, we replace the optimizer with maximum.gain(.) because our measure is now to maximize the gain. maximum.gain &lt;- function(feature, loss, output, data, factor) { cs = loss indices = data$indices data = data$dataset[indices,] parent.gain = gini(output, levs) if (!is.null(cs$gain) &amp;&amp; length(output) &gt; 1) { max.idx = which.max(cs$gain) avg = as.character(cs$split[max.idx]) perc = round(cs$obs[max.idx] / sample * 100, 0) lson = cs$l.son[max.idx] rson = cs$r.son[max.idx] lindices = cs$left.indices[[avg]]; lindices = indices[lindices] rindices = cs$right.indices[[avg]]; rindices = indices[rindices] node = list(&quot;feature&quot; = feature, &quot;split&quot; = avg, &quot;obs&quot; = cs$obs[max.idx], &quot;left&quot; = lson, &quot;right&quot; = rson, &quot;Pr&quot; = cs$Pr[max.idx], &quot;class&quot; = cs$class[max.idx], &quot;gain&quot; = cs$gain[max.idx], &quot;improve&quot; = cs$improve[max.idx], &quot;perc&quot; = perc, &quot;left.indices&quot; = lindices, &quot;right.indices&quot; = rindices, &quot;indices&quot; = indices, &quot;response&quot; = as.character(output), &quot;ntype&quot; = &quot;node&quot;) } else { probs = class.probs(output) parent.probs = round(probs[which.max(probs)]*100,0) parent.class = levs[which.max(probs)] node = list(&quot;feature&quot; = feature, &quot;split&quot; = &quot;.&quot;, &quot;obs&quot; = length(output), &quot;left&quot; = &quot;.&quot;, &quot;right&quot; = &quot;.&quot;, &quot;Pr&quot; = parent.probs, &quot;class&quot; = parent.class, &quot;gain&quot; = round(parent.gain,4), &quot;improve&quot; = &quot;.&quot;, &quot;perc&quot; = round( length(output) / sample * 100, 0), &quot;indices&quot; = indices, &quot;response&quot; = as.character(output), &quot;ntype&quot; = &quot;leaf&quot;) } node } optimizer &lt;- maximum.gain The third function to modify is the split.continuous(.), using the maximum.gain(.) function above. split.continuous &lt;- function(loss, feature, target, data, minbucket) { ################################################################# ### Similar content as split.continuous (...) in Regression Tree ################################################################# } The fourth function to modify is the split.categorical(.), using the maximum.gain(.) function above. This requires the get.categories(.) function to derive the full categorical list. split.categorical &lt;- function(loss, feature, target, data, minbucket, categories) { ################################################################# ### Similar content as split.categorical(...) in Regression Tree ################################################################# } The fifth function to modify is the split.goodness(.) function. sp.model &lt;- function() { cs = list() cs$split = cs$gain = cs$obs = cs$improve = NULL cs$Pr = cs$class = cs$l.son = cs$r.son = NULL cs$left.indices = list(); cs$right.indices = list() cs } split.goodness &lt;- function(features, target, data, minbucket = 2, categories) { ################################################################# ### Similar content as split.goodness(...) in Regression Tree ################################################################# } To test the goodness of split, we apply the function split.goodness(.) like so: features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = &quot;Species&quot; levs = levels(iris[,target]) datacars = iris sample = nrow(datacars) categories = get.categories(target, datacars) data = list(&quot;indices&quot; = seq(1, sample), &quot;dataset&quot; = datacars) ft = split.goodness(features, target, data, minbucket=2, categories)$Petal.Length ft$left.indices = ft$right.indices = ft$indices = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right Pr class gain improve ## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150 50 100 33 &quot;setosa&quot; 0.3333 50 ## perc response ntype ## [1,] 100 Character,150 &quot;node&quot; Then finally, the last function to modify is the rank.importance(.) function to support the ranking of improvement based on Gini Gain. rank.importance &lt;- function(features, target, data, minbucket, categories) { goodness = split.goodness(features, target, data, minbucket, categories) ranks = NULL for (f in features) { r = goodness[[f]] r$left.indices = r$right.indices = r$response = r$indices = NULL r = data.frame(r, stringsAsFactors = FALSE) ranks = rbind(ranks, r) } ordered.idx = order(ranks[,c(&quot;improve&quot;)], stringr::str_detect(ranks[,c(&quot;split&quot;)], &quot;[LR-]&quot;), decreasing=TRUE ) ranks = data.frame(ranks[ordered.idx,], stringsAsFactors = FALSE) feature = ranks[1,c(&quot;feature&quot;)] top = goodness[[feature]] list(&quot;ranks&quot; = ranks, &quot;top&quot; = top) } Using the modified implementation, we can rank our classification such that we get the top feature to use for the optimal split: r = rank.importance(features, target, data, minbucket=2, categories) my.rank = as.data.frame(r$ranks) ## feature split obs L R Pr class gain improve % ntyp ## 3 Petal.Length 2.45 150 50 100 33 setosa 0.333 50.00 100 node ## 4 Petal.Width 0.8 150 50 100 33 setosa 0.333 50.00 100 node ## 1 Sepal.Length 5.45 150 52 98 33 setosa 0.228 34.16 100 node ## 2 Sepal.Width 3.35 150 113 37 33 setosa 0.127 19.04 100 node ft = r$top ft$left.indices = ft$right.indices = ft$indices = NULL print(t(as.matrix(ft)), right=TRUE, quote=FALSE) ## feature split obs left right Pr class gain improve ## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150 50 100 33 &quot;setosa&quot; 0.3333 50 ## perc response ntype ## [1,] 100 Character,150 &quot;node&quot; To validate, let us print a summary of the result from our previous tree.model: summ = capture.output(summary(tree.model)) show.node(summ,1, lnum=FALSE) ## Node number 1: 150 observations, complexity param=0.5 ## predicted class=setosa expected loss=0.6667 P(node) =1 ## class counts: 50 50 50 ## probabilities: 0.333 0.333 0.333 ## left son=2 (50 obs) right son=3 (100 obs) ## Primary splits: ## Petal.Length &lt; 2.45 to the left, improve=50.00, (0 missing) ## Petal.Width &lt; 0.8 to the left, improve=50.00, (0 missing) ## Sepal.Length &lt; 5.45 to the left, improve=34.16, (0 missing) ## Sepal.Width &lt; 3.35 to the right, improve=19.04, (0 missing) ## Surrogate splits: ## Petal.Width &lt; 0.8 to the left, agree=1.000, adj=1.00, (0 split) ## Sepal.Length &lt; 5.45 to the left, agree=0.920, adj=0.76, (0 split) ## Sepal.Width &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split) Let us now implement a new base learner for our classification tree. Here, we use my.regression.tree(.) with no modification. We only rename the function as my.classification.tree(.), but uses rank.importance(.) as modified above for classification. library(dequer) my.classification.tree &lt;- function(features, target, dataset, minbucket = 1, maxdepth=50) { ################################################################# ### Similar content as my.regression.tree (...) in Regression Tree ################################################################# } features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = &quot;Species&quot; levs = levels(iris[,target]) datacars = iris my.model = h.learner(features, target, datacars, minbucket=2, maxdepth=5) # See Regression Tree Section for my.table.tree(.) my.tree.model = my.table.tree(my.model$model, display_mode=TRUE) ## N P feature split obs L R Pr class gain improv perc ## 1 1 0 Petal.Length 2.45 150 50 100 33 setosa 0.333 50 100 ## 2 2 1 &lt;leaf&gt; . 50 . . 100 setosa 0.000 . 33 ## 3 3 1 Petal.Width 1.75 100 54 46 50 versicolor 0.390 77.939 67 ## 4 4 3 Petal.Length 4.95 54 48 6 91 versicolor 0.082 49.031 36 ## 5 5 3 &lt;leaf&gt; . 46 . . 98 virginica 0.042 . 31 ## 6 6 4 &lt;leaf&gt; . 48 . . 98 versicolor 0.041 . 32 ## 7 7 4 &lt;leaf&gt; . 6 . . 67 virginica 0.444 . 4 In terms of prediction, one can construct the same prediction function, namely my.predict(.), which we implemented for Regression Trees. A modified version of the function is implemented in a section covering AdaBoost for classification. 10.3.3 Ensemble Methods Using Classification Trees, similar to Regression, we have ensemble methods available for classification, namely Random Forest, Adaboost, and Gradient Boost. For a review of the methods, recall the use of MSE for regression to evaluate individual trees of an ensemble. For classification, in our case, we can reference a Confusion Matrix to evaluate models based on specificity, sensitivity, accuracy, F1 score, and others. To illustrate, let us continue to use the iris train set and test set. set.seed(142) features = names(iris)[which(!names(iris) %in% &quot;Species&quot;)] target = c(&quot;Species&quot;) datacars = iris fold.indices = createFolds(datacars$Species, k = 10, returnTrain = FALSE) # choose the first fold for our test group. test = datacars[fold.indices$Fold01,] # choose the other folds for training group. train = datacars[-fold.indices$Fold01,] # test target test.class = test$Species test$Species = NULL Now, recall our discussion of ensemble methods under Ensemble Methods Subsection under Regression Section. The concept of bagging and boosting remains the same for classification. Learning a tree model by random sampling - so-called bootstrapping - and then testing using out of bag applies to classification also. Similarly, learning a tree model by boosting weak learners, e.g., AdaBoost, or using the gradient method to speed up the learning process, e.g., Gradient Boost, also applies to classification. The next few sections review how ensemble methods use the train set and test set for classification. Note that details of the ensemble methods are already covered in regression; thus, the following sections complement the intuition with the use of 3rd-party R packages and emphasize the classification performance of the methods. 10.3.4 Random Forest We reference Random Forest Subsection under Regression Section for a detailed intuition of Bagging using Random Forest as our Ensemble algorithm. With modification to our implementation, we replace the regressor with our own implementation of Random Forest classifier. For example, we use both my.random.forest(.) and my.rf.tree(.) functions; however, content of build.level.tree(.) uses a classifier, namely my.classification.tree(.). In this section, we extend our discussion of Random Forest by covering the model’s performance. Using our train set and test set, we learn the model like so: library(randomForest) ntree = 5 rf.model = randomForest(Species ~ ., data = train, importance=TRUE, ntree=ntree, mtry=2) rf.model$confusion ## setosa versicolor virginica class.error ## setosa 43 0 0 0.0000 ## versicolor 0 36 5 0.1220 ## virginica 0 5 31 0.1389 Based on the model, our confusion matrix above indicates that the model fits well for setosa class, given a classification error of 0. The virginica class shows a 0.1389 classification error. Then, we plot the variable importance. It is worth mentioning that if the dataset is for regression, the importance of a variable is measured based on the increase of Node Impurity denoted by IncNodePurity; however, for classification, it is measured based on the mean decrease of Gini Index and Accuracy denoted by MeanDecreaseGini and MeanDecreaseAccuracy respectively. cbind(importance(rf.model), &quot;varUsed&quot; = varUsed(rf.model)) ## setosa versicolor virginica MeanDecreaseAccuracy ## Sepal.Length 0.000 -0.3128 0.8773 1.02317 ## Sepal.Width 0.000 -1.1180 1.1180 -0.08649 ## Petal.Length 1.118 3.1824 1.6423 2.03300 ## Petal.Width 4.372 3.7990 3.8253 4.58091 ## MeanDecreaseGini varUsed ## Sepal.Length 11.602 11 ## Sepal.Width 1.091 4 ## Petal.Length 17.058 9 ## Petal.Width 59.251 17 varImpPlot(rf.model) Figure 10.31: Variable Importance It also helps to mention that the measures above explain how variables may contribute in importance the lower the values. In practice, however, the interpretation of such values gets lost in the complexity of how Random Forest builds trees, especially if we deal with thousands of trees. Instead, we can look into the out-of-bag error. Note that it is not necessary for us to have a separate train set and test set because we are setting aside a portion of the dataset (called out-of-bag) for validation. We can use the entire dataset to fit our model while, internally, the algorithm sets aside an out-of-bag set to test the model. The error from the test is called the out-of-bag error rate or out-of-bag estimate. It reflects an estimate of what a prediction error may look like should we predict a class given new data. rf.model$err.rate ## OOB setosa versicolor virginica ## [1,] 0.02381 0 0.06667 0.0000 ## [2,] 0.10667 0 0.12500 0.1923 ## [3,] 0.13000 0 0.20588 0.1875 ## [4,] 0.11504 0 0.20000 0.1471 ## [5,] 0.08333 0 0.12195 0.1389 We also use OOB error rate to evaluate the performance of different configurations of our random Tree, e.g., using different mtry - this is the number of variables to try and test at split when building our random forest tree. rf.model1 = randomForest(Species ~ ., data = train, importance=TRUE, ntree=ntree, mtry=1) rf.model2 = randomForest(Species ~ ., data = train, importance=TRUE, ntree=ntree, mtry=3) (OOB.rates = cbind(&quot;OOB (mtry=1)&quot; = rf.model1$err.rate[,1], &quot;OOB (mtry=2)&quot; = rf.model$err.rate[,1], &quot;OOB (mtry=3)&quot; = rf.model2$err.rate[,1])) ## OOB (mtry=1) OOB (mtry=2) OOB (mtry=3) ## [1,] 0.06667 0.02381 0.06000 ## [2,] 0.08974 0.10667 0.04706 ## [3,] 0.10526 0.13000 0.06000 ## [4,] 0.10619 0.11504 0.05405 ## [5,] 0.09677 0.08333 0.06667 min.rate.idx = which.min(OOB.rates[ntree,]) It shows that tuning the tree with 3 variable(s) (e.g. mtry=3) renders the least error at 6.67%. Lastly, to complement our evaluation, we can still split our dataset and use a separate test set to measure the performance of predictions. Here, we make a prediction and generate the AUC score using the performance(.) function from the ROCR library. For prediction using our test set, we use the following code: test.pred = stats::predict(rf.model, newdata=test, type=&quot;response&quot;) table(observed=test.class,predicted=test.pred) ## predicted ## observed setosa versicolor virginica ## setosa 5 0 0 ## versicolor 0 5 0 ## virginica 0 0 5 We then evaluate the prediction performance by plotting and reviewing the AUC. See Figure 10.32. library(ROCR) color = c(&quot;red&quot;, &quot;blue&quot;, &quot;black&quot;) test.votes = stats::predict(rf.model, newdata=test, type=&quot;prob&quot;) plot(NULL, xlim=range(0,1), ylim=range(0,1), xlab=&quot;False Positive Rate&quot;, ylab=&quot;True Positive Rate&quot;, main=&quot;AUC (Random Forest Performance)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline( a=0, b=1, lty=2 ) auc = rep(0, length(levs)) for (k in 1:length(levs)) { truth = ifelse(test.class == levs[k], 1, 0) pred = prediction(test.votes[,k], truth) perf = performance(pred, &quot;tpr&quot;, &quot;fpr&quot;) auc[k] = performance(pred, measure = &quot;auc&quot;)@y.values plot(perf, col=color[k], add=TRUE) } legend(0.38, 0.28, legend=c( paste0(levs[1],&quot; (auc=&quot;,auc[1],&quot;)&quot;), paste0(levs[2],&quot; (auc=&quot;,auc[2],&quot;)&quot;), paste0(levs[3],&quot; (auc=&quot;,auc[3],&quot;)&quot;) ), col=color, pch=20, cex=0.8) legend(0.65, 0.70, legend=c( &quot;0.90 - 1.00 = excellent&quot;, &quot;0.80 - 0.90 = good&quot;, &quot;0.70 - 0.80 = fair&quot;, &quot;0.60 - 0.70 = poor&quot;, &quot;0.50 - 0.60 = fail&quot; ), col=c(&quot;black&quot;), pch=c(16,16,16), cex=0.8) Figure 10.32: AUC (Random Forest Performance) The figure shows that the model predicts excellently, having AUC values within the range 0.90 - 1.00. 10.3.5 AdaBoost &amp; SAMME We continue to discuss classification using one of the Ensemble methods called Boosting. Our next introductory classification algorithm is AdaBoost which we also covered under AdaBoost Subsection under Regression Section. Recall in AdaBoost Regression the introduction of AdaBoost.R2 (Drucker H. 1997). For Regression, we also can use SAMME.R as an alternative algorithm which is not covered under Regression in Computational Learning I; however, in this section, we discuss SAMME for classification instead. Also, we introduce AdaBoost.M2 (adam2) for classification. Though we do not cover AdaBoost.M1 (adam1) in this section, it helps to be aware of such precursors. Here, we compare the two multi-classification algorithms, namely AdaBoost.M2 and SAMME. Below is the AdaBoost.M2 algorithm (Yoav Freund, Robert E Schapire 1996). \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,K\\}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{Let} B = \\{{(i, y)}:i\\ \\in\\ \\{1,...,n\\}, y \\ne y_i\\}\\\\ \\ \\ \\ \\text{number of machines}: M\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ weights: D^{(0)}_i = \\frac{1}{n},\\ i=1,2,...,n\\ \\ \\ \\ \\ \\ \\ \\text{(initialize distribution)}\\\\ \\ \\ \\ \\text{loop}\\ m\\ in\\ 1:\\ M \\\\ \\ \\ \\ \\ \\ \\ \\ S^{(m)} \\leftarrow \\text{Resample from original training set } X^{(0)} \\text{ with }D^{(m)}\\\\ \\ \\ \\ \\ \\ \\ \\ h^{(m)}\\ \\leftarrow \\text{Train a weak classifier on } S^{(m)} \\text{ then }\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{calculate the pseudo-loss of } h^{(m)}: \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\epsilon^{(m)} = \\frac{1}{2} \\sum_{(i,y) \\in B}^n D^{(m)}_{(i,y)} \\left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\\right)\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Compute contribution for this classifier}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta^{(m)} = \\epsilon^{(m)}/(1 - \\epsilon^{(m)}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(Voting Power)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Update weights on training points}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\alpha^{(m)} = (1/2) \\left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\\right)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ D^{(m+1)}_{(i,y)} \\leftarrow D^{(m)}_{(i,y)} \\cdot (\\beta^{(m)})^{\\alpha^{(m)}}, i=1,2,...,n\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Normalize weights such that } \\sum_{i=1}^n D^{(m+1)}_{(i,y)} = 1 \\\\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\text{Output }H(\\mathbf{x}) = \\text{arg}\\ \\underset{y \\in Y}{\\text{max}}\\left(\\sum_{m=1}^M \\log_e \\frac{1}{\\beta^{(m)}}h^{(m)}(x, y)\\right) \\end{array} \\] Moreover, below is the SAMME algorithm. The name is an acronym for Stagewise Additive Modeling Using Multi-class Exponential loss function. It is also a multi-classification algorithm (Ji Zhu et al. 2006). \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,K\\}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{number of machines}: M\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ weights: w^{(0)}_i = \\frac{1}{n},\\ i=1,2,...,n \\ \\ \\ \\ \\ \\ \\ \\ \\text{(initialize equal weights)}\\\\ \\ \\ \\ \\text{loop}\\ m\\ in\\ 1:\\ M \\\\ \\ \\ \\ \\ \\ \\ \\ S^{(m)} \\leftarrow \\text{Resample from original training set } X^{(0)} \\text{ with }w^{(m)}\\\\ \\ \\ \\ \\ \\ \\ \\ h^{(m)}\\ \\leftarrow \\text{Train a weak classifier on } S^{(m)} \\text{ then }\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{compute the following pseudo-loss of } h^{(m)}: \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\epsilon^{(m)} = \\sum_{i=1}^n w^{(m)}_i \\mathbf{1}\\{h^{(m)}(x_i) \\ne y_i \\} / \\sum_{i=1}^n w_i^{(m)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Compute contribution for this classifier}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta^{(m)} = \\log_e \\frac{1 - \\epsilon^{(m)}}{\\epsilon^{(m)}} + \\log_e(K-1) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(Voting Power)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Update weights on training points}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\alpha^{(m)} = \\beta^{(m)} \\cdot \\mathbf{1}\\{h^{(m)}(x_i) \\ne y_i \\}) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w^{(m+1)}_i \\leftarrow (w^{(m)}_i)^{\\alpha^{(m)}}, i=1,2,...,n\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Normalize weights such that } \\sum_{i=1}^n w^{(m+1)}_i = 1 \\\\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\text{Output }H(\\mathbf{x}) = arg\\ \\underset{k}{max}\\left(\\sum_{m=1}^M \\beta^{(m)} \\cdot \\mathbf{1}\\{h^{(m)}(x_i) = k \\}\\right) \\end{array} \\] One important note is that a base learner’s choice for h(x) is allowed for both algorithms. Any base learner classifier is supported, namely, Decision Trees, SVM, Logistic Regression, and so on. The idea is to use the base learner to fit models that are not required to be well fitted - hence, a weak fit - and we use our algorithm to boost for a better fit. In our case, we use our Classification Tree that we previously implemented and introduced as our base learner. First, we start with the idea that Boosting for Classification Trees uses weak base learners called Stumps. A stump is a tree with only one split and can be built using our classifier as an illustration. Note that our classifier uses Gini Index and Gini Gain to evaluate goodness of split for a stump. Alternatively, we can use Entropy and Entropy Gain, detailed under the Classification Trees section. library(dequer) h.learner = my.classification.tree # our base learner classifier For example, we write the following code to generate a stump considering only the Petal.Length features using the iris dataset: my.petal.stump = h.learner(&quot;Petal.Length&quot;, &quot;Species&quot;, train, minbucket=1, maxdepth=1) my.tree.model = my.table.tree(my.petal.stump$model, display_mode=TRUE) ## N P feature split obs L R Pr class gain improve perc ## 1 1 0 Petal.Length 2.45 135 45 90 33 setosa 0.333 50 90 ## 2 2 1 &lt;leaf&gt; . 45 . . 100 setosa 0.000 . 30 ## 3 3 1 &lt;leaf&gt; . 90 40 50 50 versicolor 0.361 72.2 60 In terms of ranking the top feature to use for the split of a stump, we use the same rank.importance(.) function: features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = &quot;Species&quot; levs = levels(iris[,target]) datacars = iris categories = get.categories(target, datacars) data = list(&quot;indices&quot; = seq(1, sample), &quot;dataset&quot; = datacars) r = rank.importance(features, target, data, minbucket=1, categories) my.rank = as.data.frame(r$ranks) ## feature split obs L R Pr class gain improve perc ntyp ## 3 Petal.Length 2.45 150 50 100 33 setosa 0.333 50.00 100 node ## 4 Petal.Width 0.8 150 50 100 33 setosa 0.333 50.00 100 node ## 1 Sepal.Length 5.45 150 52 98 33 setosa 0.228 34.16 100 node ## 2 Sepal.Width 3.35 150 113 37 33 setosa 0.127 19.04 100 node The result shows Petal.length feature as the top choice based on the Gini Gain metrics. Alternatively, we can use the improve metrics. Second, for prediction, we continue to use my.predict(.) with slight modification. Here, we expect a comparison of categorical output between the estimate and true value. prediction.score &lt;- function(y, y.hat) { result = ifelse(y == y.hat, 1, -1) list(&quot;fitted.values&quot; = y.hat, &quot;result&quot; = result) } my.predict &lt;- function(my.model, x, y, resid = NULL, tendency = function(top, resid = NULL) { top$class }, method = prediction.score, tabletree = FALSE) { n = nrow(x) responses = rep(0, n) model = my.model$model categories = my.model$categories if (tabletree == FALSE) { model = my.table.tree(my.model$model) } for (i in 1:n) { node = model[1,] while (TRUE) { if (node$ntype == &quot;leaf&quot;) { if (tabletree == FALSE ) { top = my.model$model[[node$N]]$top responses[i] = tendency( top, resid ) } else { responses[i] = node$class } break } children = model[which( model$P == node$N ),] cat = categories[[node$feature]] split = node$split val = x[i, c(node$feature)] if (is.factor(val)) { s = base::strsplit(split,NULL)[[1]] direction = s[which(cat %in% val )] direction = ifelse(direction == &#39;L&#39;, 1, 2) } else { split = as.numeric(split) val = as.numeric(val) direction = ifelse(val &lt; split, 1, 2) } node = children[direction,] } } method(y, responses) } h.score = my.predict To test our prediction function, we use one of our stump trees like so: options(width=70) h.score(my.petal.stump, test, test.class) ## $fitted.values ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ## [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; ## [11] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; ## ## $result ## [1] 1 1 1 1 1 1 1 1 1 1 -1 -1 -1 -1 -1 Third, we also use sampling.distribution(.). Here, we pick a new set of samples from our original dataset; but the sample is a weighted sample based on the calculated distribution. sampling.distribution &lt;- function(sample.set, sample.weight, seed=142) { n = nrow(sample.set) indices = rep(0, n) cumulative.weight = cumsum(sample.weight) # Distribution set.seed(seed) random.number = runif(n=n, min=0, max=1) for (i in 1:n) { indices[i] = which(random.number[i] &lt; cumulative.weight)[1] } sample.set[indices,] } Our sample weight, denoted as w, is used eventually to construct our distribution based on the cumulative weight with which we sample our original train set for the next fit. sample.set = train n = nrow(sample.set); w = 1/n sample.weight = rep(w, n) sampled.set = cbind(sample.set, sample.weight) ## Sepal.Len Sepa.Width Petal.Len Petal.Width Species Sampl.Weight ## 1 5.1 3.5 1.4 0.2 setosa 0.007407 ## 2 4.9 3.0 1.4 0.2 setosa 0.007407 ## 3 4.7 3.2 1.3 0.2 setosa 0.007407 ## 4 4.6 3.1 1.5 0.2 setosa 0.007407 ## 5 5.0 3.6 1.4 0.2 setosa 0.007407 ## 6 5.4 3.9 1.7 0.4 setosa 0.007407 Fourth, calculate the pseudo-loss of our hypothesis, namely \\(\\mathbf{h^{(t)}}\\). For AdaBoost.M2, we have: \\[\\begin{align} \\epsilon^{(m)} = \\frac{1}{2} \\sum_{(i,y) \\in B}^n D^{(m)}_{(i,y)} \\left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\\right) \\end{align}\\] For SAMME, we have: \\[\\begin{align} \\epsilon^{(m)} = \\sum_{i=1}^n w^{(m)}_i \\mathbf{1}\\{h^{(m)}(x_i) \\ne y_i \\} / \\sum_{i=1}^n w_i^{(m)} \\end{align}\\] Fifth, calculate the contribution (or power of say). For AdaBoost.M2, we have: \\[\\begin{align} \\beta^{(m)} = \\epsilon^{(m)}/(1 - \\epsilon^{(m)}) \\end{align}\\] For SAMME, we have: \\[\\begin{align} \\beta^{(m)} = \\frac{1}{2} \\log_e \\frac{1 - \\epsilon^{(m)}}{\\epsilon^{(m)}} + \\log_e(K-1) \\end{align}\\] Sixth, we now update the weights. For AdaBoost.M2, we have: \\[\\begin{align} \\alpha^{(m)} = (1/2) \\left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\\right)\\\\ D^{(m+1)}_{(i,y)} \\leftarrow D^{(m)}_{(i,y)} \\cdot (\\beta^{(m)})^{\\alpha^{(m)}}, i=1,2,...,n \\end{align}\\] For SAMME, we have: \\[\\begin{align} \\alpha^{(m)} = \\beta^{(m)} \\cdot \\mathbf{1}\\{h^{(m)}(x_i) \\ne y_i \\}) \\\\ w^{(m+1)}_i \\leftarrow (w^{(m)}_i)^{\\alpha^{(m)}}, i=1,2,...,n \\end{align}\\] Finally, we calculate our prediction output. For AdaBoost.M2, we have: \\[\\begin{align} H(\\mathbf{x}) = \\text{arg}\\ \\underset{y \\in Y}{\\text{max}}\\left(\\sum_{m=1}^M \\log_e \\frac{1}{\\beta^{(m)}}h^{(m)}(x, y)\\right) \\end{align}\\] For SAMME, we have: \\[\\begin{align} H(\\mathbf{x}) = arg\\ \\underset{k}{max}\\left(\\sum_{m=1}^M \\beta^{(m)} \\cdot \\mathbf{1}\\{h^{(m)}(x_i) = k \\}\\right) \\end{align}\\] Now, for illustration, let us use SAMME as our algorithm for an example implementation of multi-classification. my.adaboost.SAMME &lt;- function(features, target, D, estimators = 100) { n = nrow(D) w = rep(1/n, n) levs = levels(D[,c(target)]) K = length(levs) K.log = log(K - 1, exp(1)) # 2.718282 model = list() for (m in 1:estimators) { sample.set = sampling.distribution(D, w, 1) x = sample.set[, c(features)] y = as.character(sample.set[, c(target)]) h.model = h.learner(features, target, sample.set, minbucket=1, maxdepth=1) h.pred = h.score(h.model, x, y) errors = (h.pred$fitted.values != y) * 1 epsilon = sum(w * errors) / sum(w) beta = log( (1 - epsilon) / epsilon, exp(1)) * K.log w = w^(beta * errors) w = w / sum(w) # normalize model[[m]] = list(&quot;learner&quot; = h.model, &quot;beta&quot; = beta) } list(&quot;levs&quot; = levs, &quot;model&quot; = model) } features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = &quot;Species&quot; my.model = my.adaboost.SAMME(features, target, train, estimators = 5) Let us also write our example implementation of a prediction function using SAMME. H = my.predict.SAMME &lt;- function(models, x, y) { n = length(y) y.max = rep(0, length(y)) levs = models$levs K = length(levs) class = matrix(0, n, K, byrow=TRUE) for (model in models$model) { pred = h.score(model$learner, x, y) beta.I = ifelse(pred$result == 1, model$beta, 0) for (k in 1:K) { h = (levs[k] == pred$fitted.values) class[,k] = class[,k] + beta.I * h } } y.pred = apply(class, 1, which.max) list(&quot;y.hat&quot; = levs[y.pred], &quot;classes&quot; = levs) } (predicted = H(my.model, test, test.class)) ## $y.hat ## [1] &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; &quot;setosa&quot; ## [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; ## [11] &quot;virginica&quot; &quot;virginica&quot; &quot;virginica&quot; &quot;virginica&quot; &quot;virginica&quot; ## ## $classes ## [1] &quot;setosa&quot; &quot;versicolor&quot; &quot;virginica&quot; Then, to evaluate using confusion matrix, we have the following implementation: library(caret) my.evaluate.SAMME &lt;- function(target, predicted) { y.hat = as.factor(predicted$y.hat) levels(y.hat) = predicted$classes conf.tab = table(y.hat, as.factor(target)) accuracy = sum(diag(conf.tab)) / sum(conf.tab) list(&quot;conf.table&quot; = conf.tab, &quot;accuracy&quot; = round(accuracy * 100,2)) } (my.outcome = my.evaluate.SAMME(test.class, predicted)) ## $conf.table ## ## y.hat setosa versicolor virginica ## setosa 5 0 0 ## versicolor 0 5 0 ## virginica 0 0 5 ## ## $accuracy ## [1] 100 where row-vectors are the predictions and column-vectors are the reference. Our metrics are based on using a confusion matrix to derive our calculation for accuracy per class in a multi-classification setting. A multiclass accuracy is formulated below: \\[\\begin{align} Accuracy_{(avg)} = \\frac{1}{K} \\sum_{k=1}^K \\frac{TP_k + TN_k}{TP_k + TN_k + FP_k + FN_k} \\ \\ \\ \\ \\ \\text{where K is the number of classes} \\end{align}\\] The formula is calculated based on the confusion matrix above. Also, a simple way to calculate accuracy using a matrix is shown below: cmat = my.outcome$conf.table accuracy = sum(diag(cmat)) / sum(cmat) c(&quot;accuracy&quot; = round(accuracy * 100, 2), &quot;error&quot; = 1 - accuracy) ## accuracy error ## 100 0 Note that the accuracy above may seem too accurate if it is 100%. Overfitting can be avoided using regularization. Other score metrics to consider for evaluation include precision and recall. precision = diag(cmat) / rowSums(cmat) recall = diag(cmat) / colSums(cmat) rbind(precision, recall) ## setosa versicolor virginica ## precision 1 1 1 ## recall 1 1 1 As an exercise, we leave readers to play around with different random seeds for the random resampling, namely sampling.distribution(.), and see how it affects prediction performance. Under the Regression Section for AdaBoost, the choice of sampling set is based on resampling. Here, we demonstrate the use of re-weighing. 10.3.6 LogitBoost (J Classes) Let us extend our discussion of Logistic Regression to cover Multi Classification. Here, we introduce LogitBoost as an ensemble method using Logistic Regression for Multi Classification. We tailor our discussion based on the LogitBoost (J classes) algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani (2012) - note here that we use K for the number of classes. \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,K\\}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{number of machines}: M \\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ F_{0,k}(x_i) = 0, P_{0,k}(x_i) = 1/K\\ for\\ k=1,...,K\\ and\\ i=1,...,n\\ \\ \\ &amp;\\text{(initialize)} \\\\ \\ \\ \\ \\text{loop}\\ m\\ in\\ 1:\\ M \\\\ \\ \\ \\ \\ \\ \\ \\text{loop}\\ k\\ in\\ 1:\\ K \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Compute working responses and weights}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ z_{ik} = \\frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )} &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Fit model using } f_{m,k}(\\{x_i,z_{ik}, w_{ik}\\}_{i=1}^n)\\ \\text{by a weighted} &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{least-squares regression of}\\ z_{ik}\\ \\text{to}\\ x_i\\ \\text{with weights}\\ w_{ik}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\ \\ \\ \\ \\text{Set}\\ f_{m,k}(x_i)\\ = \\frac{K-1}{K} \\left(f_{m-1,k}(x_i) - \\frac{1}{K}\\sum_{l=1}^K f_{m-1,l}(x_i)\\right) &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ F_{m,k}(x_i)\\ = F_{m-1,k}(x_i) + f_{m,k}(x_i) &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\ \\ \\ \\ P_{m,k}(x_i) = exp(F_{m,k}(x_i))/\\sum_{l=1}^K exp(F_{m,l}(x_i)) &amp; \\text{i = 1,...,n}\\\\ \\ \\ \\ \\text{end loop}\\\\ \\ \\ \\ \\text{Output}\\ \\text{arg}\\ \\underset{k}{\\text{max}}\\ F_{M,k}(x) \\end{array} \\] The algorithm is based on Newton method for optimization. An alternative method called Gradient Descent is covered in the next section under Gradient Boost. Here, we work on the basis of probabilistic estimates expressed like so: \\[\\begin{align} \\hat{p}_k = P(y_i=k|x_i) = \\frac{exp(F_k(x_i))}{1 + exp(F_k(x_i))}, \\end{align}\\] so that given a dataset, namely \\(\\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,K\\}\\}_{i=1}^n\\) where K is the number of classes, our goal is to compute the probability of each of the K classes, e.g. \\(\\hat{p}_k \\in \\mathbb{R}^k\\), such that one of them is the most likely to be the k class for \\(y_i\\). That is expressed as such: \\[\\begin{align} \\hat{y}_i|x_i = \\text{arg}\\ \\underset{k}{\\text{max}}\\ \\hat{p}_k(x_i) \\ \\ \\ \\ \\ where\\ \\sum_{k=1}^K \\hat{p}_k = 1 \\end{align}\\] For the LogitBoost Loss Function, we use negative log likelihood (NLL): \\[\\begin{align} \\mathcal{L(y, F(x))} = \\sum_{i=1}^n \\left\\{ - \\sum_{k=1}^K y_{ik}\\ \\log_e \\hat{p}_{k}(x_i)\\right\\} \\end{align}\\] We minimize the Loss Function by taking the 1st (Jacobian) and second (Hessian) derivatives of the loss function with respect to \\(\\mathbf{F_k(x_i)}\\). \\[\\begin{align} \\underbrace{\\nabla_{F_{m,l}(x_i)} Lik(\\{y_{il}, F_{m,l}(x_i)\\}^K_{l=1})}_{\\text{negative gradient}} &amp;= -\\left[\\frac{\\partial Lik(\\{y_{il}, F_{m,l}(x_i)\\}^K_{l=1})}{\\partial F_{m,l}(x_i)}\\right]_{\\{F_{m,l}(x) = F_{m-1,l}(x)\\}^K_{l=1}} \\\\ &amp;= y_{ik} - P_{m-1,k}(x_i) \\end{align}\\] \\[\\begin{align} \\underbrace{\\nabla_{F_{m,l}(x_i)}^2 Lik(\\{y_{il}, F_{m,l}(x_i)\\}^K_{l=1})}_{\\text{negative gradient}} &amp;= -\\left[\\frac{\\partial^2 Lik(\\{y_{il}, F_{m,l}(x_i)\\}^K_{l=1})}{\\partial F_{m,l}(x_i)^2}\\right]_{\\{F_{m,l}(x) = F_{m-1,l}(x)\\}^K_{l=1}} \\\\ &amp;= P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) \\end{align}\\] Then we formulate our working response, namely \\(\\mathbf{z_{ik}}\\), and weight, namely \\(\\mathbf{w_{ik}}\\), based on the result of the derivatives: \\[\\begin{align} z_{ik} = \\frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) \\end{align}\\] Note that \\(w_{ik}\\) can become numerically unstable as it approaches zero. In our example implementation below, our quick patch is to apply the following condition: \\[\\begin{align*} w_{ik}\\ \\ \\ \\rightarrow \\begin{cases} \\text{1e-15} &amp; if\\ w_{ik} = 0\\\\ w_{ik} &amp; if\\ w_{ik} \\ne 0 \\end{cases} \\end{align*}\\] Finally, LogitBoost is a greedy stage-wise additive model such that in terms of our base learner, namely \\(\\mathbf{f_{k,m}(x_i)}\\), we perform the following expressions below iteratively: \\[\\begin{align} f_{m,k}(x_i)\\ &amp;= &amp;\\frac{K-1}{K} \\left(f_{m-1,k}(x_i) - \\frac{1}{K}\\sum_{l=1}^K f_{m-1,l}(x_i)\\right)\\\\ F_{m,k}(x_i) &amp;= &amp;F_{m-1,k}(x_i) + f_{m,k}(x_i) \\end{align}\\] Below is an example implementation of LogitBoost (J classes). We use rpart(.) for the fit - a regression tree for the base learner. library(rpart) my.logitboost &lt;- function(features, target, data, machines=5) { x = as.matrix(data[,c(features)]) y = data[,c(target)] n = nrow(x) M = machines classes =levels(y) K = length(classes) F_k = matrix(0, n, K, byrow=TRUE) p_k = matrix(1/K, n, K, byrow=TRUE) y_ik = matrix(0, n, K, byrow=TRUE) h = matrix(0, n, K, byrow=TRUE) cntrl = rpart.control(cp=0, maxdepth=1, minsplit=1) for (k in 1:K) { y_ik[,k] = as.numeric(y == classes[k]) } for (m in 1:M) { for (k in 1:K) { w_ik = ( p_k * ( 1 - p_k)) z_ik = (y_ik - p_k) / ifelse( w_ik &gt; 0, w_ik, 1e-15) f_model = rpart(z_ik[,k] ~ x, weights = w_ik[,k], control = cntrl) h[,k] = stats::predict(f_model) } f_mk = (K-1)/K * ( h - 1/K * apply(h, 1, sum)) F_k = F_k + f_mk p_k = exp(F_k) / apply(exp(F_k), 1, sum) } apply(F_k, 1, which.max) } Let us use the implementation and compare the difference between the actual target values and the fitted values generated by logitBoost using m equal to 1, 5, and 10. features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = c(&quot;Species&quot;) y = as.numeric(train[,c(target)]) fitted.values = my.logitboost(features, target, train, machines=1) all.equal(y, fitted.values) ## [1] &quot;Mean relative difference: 0.3529&quot; fitted.values = my.logitboost(features, target, train, machines=5) all.equal(y, fitted.values) ## [1] &quot;Mean relative difference: 0.5&quot; fitted.values = my.logitboost(features, target, train, machines=10) all.equal(y, fitted.values) ## [1] &quot;Mean relative difference: 0.3333&quot; As an experiment, we leave readers to tune maxdepth and maxsplit. Moreover, we leave readers to modify the implementation to preserve the weak learner models, e.g., f_model, and use them to predict using the test set. From there, we use Confusion Matrix to evaluate the performance of the logitBoost ensemble itself. There are other variants of LogitBoost that address numerical stability. One is called Adaptive Base Class (ABC) LogitBoost formulated by Ping Li (from Cornell University). We leave readers to investigate ABC-LogitBoost and its application to image classification (Ping Li 2012). 10.3.7 Gradient Boost We continue to extend our concept of Boosting by using Gradient Boost. A detailed intuitive view of the concept of Gradient Boosting is covered under the Regression Section. The same intuition applies to Classification. Recall in Gradient Boosting for Regression that we have the following square error in our loss function: \\[ \\underbrace{Lik(y, F(x))}_{\\text{loss function}} = \\frac{1}{2}\\left(y -\\hat{y}\\right)^2 \\ \\ and\\ \\ \\underbrace{F(x) = \\hat{y}}_{\\text{y-hat}}. \\] In this section, we discuss K-class Logistic Gradient Boost introduced by Jerome Friedman (1999a, 2001, 2002) in the context of Multi-Classification. Here, we start with our loss function based on cross-entropy in the form below where we measure the loss for each k class - hence, it is notable to point out the notation: \\(\\{y_k, F_k(x)\\}^K_{k=1}\\). Our loss function is thus written as: \\[\\begin{align} \\underbrace{Lik(\\{y_k, F_k(x)\\}^K_{k=1})}_{\\text{loss function}} = - \\sum_{k=1}^K y_k\\log_e P_k(x) \\end{align}\\] where K is the number of classes and \\(P_k\\) is a softmax function expressed as: \\[\\begin{align} \\underbrace{P_k(x) = \\frac{e^{o_k}}{\\sum_{l=1}^K e^{o_l}}}_{\\text{softmax}} \\ \\ \\ \\ \\ \\ \\ \\leftarrow\\ \\ \\ \\ \\ \\ F_{(k\\ or\\ l)}(x) = \\underbrace{o_{(k\\ or\\ l)} = \\frac{P(y_{(k\\ or\\ l)} = 1|x)}{P (y_{(k\\ or\\ l)} = 0|x)}}_{\\text{log odds}} \\end{align}\\] Expanding our loss function, we get: \\[\\begin{align} \\underbrace{Lik(\\{y_k, F_k(x)\\}^K_1)}_{\\text{loss function}} = - \\sum_{k=1}^K y_k \\log_e \\frac{e^{o_k}}{\\sum_{l=1}^K e^{o_l}} = \\sum_{i=1}^n \\underbrace{ \\left(-\\sum_{k=1}^K \\underbrace{\\mathbf{1}\\{class = k\\}}_{y_k} \\log_e \\underbrace{ \\frac{e^{o_k}}{\\sum_{l=1}^K e^{o_l}}}_{\\text{softmax}}\\right)}_{\\begin{array}{c}\\text{negative log-likelihood loss}\\\\ \\text{(cross-entropy)}\\end{array}} \\label{eqn:eqnnumber419} \\end{align}\\] Note that our log-odds undergo the following logistic transformation in reference to another Boosting method called LogitBoost, which uses Quasi-Newton (or Newton Raphson) method along with Hessian calculation (Friedman J. 1999a). The multinomial log-likelihood loss is generalized into LogitBoost for multi-classification. \\[\\begin{align} F_k(x) = \\log_e p_k(x) - \\frac{1}{K}\\sum_{l=1}^K \\log_e p_l(x) \\end{align}\\] Using the loss function, we derive its negative gradient. Let us first take the derivative of our softmax with respect to log odds. \\[\\begin{align} \\text{if }i=j\\ \\ \\ \\ \\rightarrow \\frac{\\partial P_i}{\\partial o_k} = \\frac{\\partial \\frac{e^{o_k}}{\\sum_{j=1}^K e^{o_j}}}{\\partial o_k} &amp;= \\frac{e^{o_k} \\left(\\sum_{k=1}^K e^{o_k} - e^{o_j} \\right)}{(\\sum_{k=1}^K)^2} = P_i \\left(1 - P_j\\right)\\\\ \\text{if }i\\ne j\\ \\ \\ \\ \\rightarrow \\frac{\\partial P_i}{\\partial o_k} = \\frac{\\partial \\frac{e^{o_k}}{\\sum_{j=1}^K e^{o_j}}}{\\partial o_k} &amp;= \\frac{0 - e^o{_j} e^{o_i}}{\\left(\\sum_{k-1}^K e^{o_k}\\right)} = - P_j P_i \\end{align}\\] therefore: \\[\\begin{align} \\frac{\\partial P_i}{\\partial o_k} = \\begin{cases} P_i( 1 - P_i) &amp; if\\ i = j\\\\ - P_i P_j &amp; if\\ i \\ne j \\end{cases} \\label{eqn:eqnnumber420} \\end{align}\\] We then take the negative derivative of our loss function with respect to log-odds. \\[\\begin{align} r_{ik} = \\underbrace{\\nabla_{F_l(x_i)} Lik(\\{y_{il}, F_l(x_i)\\}^K_{l=1})}_{\\text{negative gradient}} &amp;= -\\left[\\frac{\\partial Lik(\\{y_{il}, F_l(x_i)\\}^K_{l=1})}{\\partial F_l(x_i)}\\right]_{\\{F_l(x) = F_{l,m-1}(x)\\}^K_{l=1}} \\\\ &amp;= -\\sum_{k=1}^K y_k(x) \\left(\\frac{ \\partial \\log_e P_k(x) }{\\partial o_k}\\right)\\\\ &amp;= - y_i ( 1- P_i) - \\sum_{k\\ne i} y_k \\frac{1}{P_k} ( - P_i P_j )\\\\ &amp;= y_{ik} - P \\left(\\sum_k y_k\\right)\\ \\ \\ \\ \\leftarrow\\ \\left(\\sum_k y_k\\right) = 1\\\\ &amp;= y_{ik} - P_{k,m-1}(x_i)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(residual)} \\end{align}\\] where \\(F_{l,m-1}(x_i) \\equiv F_{l,0}(x_i) \\equiv \\hat{y}_l \\equiv \\gamma_l\\). Let us implement the log-odds function and softmax function. We perform a logistic transform, a.l.a softmax: softmax &lt;- function(F_k) { n = nrow(F_k) K = ncol(F_k) p_k = matrix( 0, nrow=n, ncol=K, byrow=TRUE) for (k in 1:K) { p_k[,k] = exp(F_k[,k]) / apply(exp(F_k), 1, sum) } p_k } In the case of using our classification tree as our base learner, each leaf (or region) produces the following score transformation: \\[ \\begin{array}{lrr} \\ \\ \\ \\ \\ \\ \\ \\text{for each}\\ (L)_j \\in (T)_m\\ &amp;\\text{where (L)eaf and (T)ree} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\gamma_{jkm} = \\frac{K - 1}{K} \\frac{\\sum_{x_i} \\in {L_{jkm}}^{rim}}{\\sum_{x_i} \\in {L_{jkm}}^{|r_{im}|(1 - |r_{im}|)}} \\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\end{array} \\] The calculation of the score is implemented as such: transform &lt;- function(my.model, prob = NULL) { J = length(my.model$model) numer = 0; denom = 0 for (j in 1:J) { top = my.model$model[[j]]$top if (top$ntype == &quot;leaf&quot;) { indices = top$indices y_j = top$response p_j = prob[indices] gamma = (J-1) / J * ( sum(y_j) / sum (abs(p_j) * abs(1 - p_j))) my.model$model[[j]]$top$ymean = gamma } } my.model } Finally, Gradient Boost is a greedy stage-wise additive model such that in terms of our base learner, namely \\(\\mathbf{\\gamma_{jkm}(x_i)}\\), we perform the following expressions iteratively: \\[\\begin{align} F_{k,m}(x) = F_{k,m-1}(x) + \\sum_{j=1}^J \\gamma_{jkm}\\mathbf{1}\\{x \\in L_{jkm}\\} \\ \\ \\ \\ \\ where\\ \\ \\text{(where J is number of leafs)} \\end{align}\\] The algorithm below is a variant of Gradient Boosting with modification to allow for Multi-Classification (See Algorithm 6: \\(L_k\\)_TreeBoost, J. Friedman (1999a)). Here, we use the term Leaf referring to Region for a J-terminal to emphasize using decision trees for base learners. \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,K\\}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{number of machines}: M\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ F_0(X) = \\text{arg}\\ \\underset{\\gamma}{min} \\sum_{x=1}^n Lik(y_i, \\gamma ) \\ \\ \\ \\text{(initialize)}\\\\ \\ \\ \\ \\text{loop}\\ m\\ in\\ 1:\\ M \\\\ \\ \\ \\ \\ \\ \\ P_k(x) = \\frac{e^{F_k}}{\\sum_{l=1}^K e^{F_l}},\\ \\ \\ \\ for\\ k = 1,..,K \\\\ \\ \\ \\ \\ \\ \\ \\text{loop}\\ k\\ in\\ 1:\\ K\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ r_{im} = y_{ik} - P_{k,m-1}(x_i)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{fit model using } h_m(\\{x_i,r_{im}\\}_{i=1}^n)\\ \\ \\ \\ \\ \\ \\ \\text{(fit model to pseudo-residuals)} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{for each}\\ (L)_j \\in (T)_m\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{where (L)eaf and (T)ree} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\gamma_{jkm} = \\frac{K - 1}{K} \\frac{\\sum_{x_i \\in L_{jkm}}{rim}}{\\sum_{x_i \\in L_{jkm}}{|r_{im}|(1 - |r_{im}|)}} \\ \\ \\ \\ \\text{(leaf residual)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ F_{k,m}(x) = F_{k,m-1}(x) + \\sum_{j=1}^J \\gamma_{jkm}\\mathbf{1}\\{x \\in L_{jkm}\\} \\ \\ \\ \\ \\ \\ \\text{(J is number of leafs)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\text{end loop}\\\\ \\ \\ \\text{Output } F_{k,m}(x) \\end{array} \\] Below is our example implementation of Algorithm 6: \\(L_k\\)_TreeBoost (J. Friedman 1999a): # Here, we use regression tree and prediction # from Computational Learning I chapter. h.learner &lt;- my.regression.tree h.score &lt;- my.predict tendency &lt;- function(top, resid) { top$ymean } my.gradientboost &lt;- function(features, target, data, machines=50, minbucket=1, maxdepth=4) { x = data[,c(features)] y = data[,c(target)] n = length(y) classes = levels(y) K = length(classes) M = machines y_ik = matrix(0, n, K, byrow=TRUE) y.hat = matrix(0, n, K, byrow=TRUE) F_k = matrix( 0, nrow=n, ncol=K, byrow=TRUE) f_model = list() names(F_k) = classes for (k in 1:K) { y_ik[,k] = as.numeric(y == classes[k]) } for (m in 1:M) { p_k = softmax(F_k) for (k in 1:K) { y.hat[,k] = residual = y_ik[,k] - p_k[,k] data[,c(target)] = residual f_model[[k]] = h.learner(features, target, data, minbucket, maxdepth) f_model[[k]] = transform(f_model[[k]], p_k[,k]) gamma_jkm = h.score(f_model[[k]], x, y.hat[,k] )$fitted.values F_k[,k] = F_k[,k] + gamma_jkm } } p_k = softmax(F_k) apply(p_k, 1, which.max) } Let us use the implementation and compare the difference between the actual target values and the fitted values generated by Gradient Boost using m equal to 1. features = c(&quot;Sepal.Length&quot;, &quot;Sepal.Width&quot;, &quot;Petal.Length&quot;, &quot;Petal.Width&quot;) target = c(&quot;Species&quot;) y = as.numeric(train[,c(target)]) fitted.values = my.gradientboost(features, target, train, machines=1) all.equal(y, fitted.values) ## [1] TRUE We leave readers to enhance the implementation with emphasis on fixing the overfitting. 10.3.8 K-Next Neighbors (KNN) Before we jump to a new topic, it helps to also finally give merit to the K-Next Neighbors (KNN) classification method being one of the traditional classification algorithms. KNN may be confused with K-means, which are two separate methods. The former is a classification method, and the latter is a clustering method. KNN has the following simple algorithm, which allows for unknown data, e.g., test data, to be classified given already learned or classified data. In the algorithm below, we are asked to determine the class to which the new data belongs. Note here that C is the number of categories (or classes) and K is the K nearest neighbors. \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i,y_i)}:x_i\\ \\in\\ X, y_i \\in Y = \\{1,...,C\\}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{unknown dataset}: \\{{(z_i)}:z_i\\ \\in\\ Z \\}_{i=1}^m\\\\ \\ \\ \\ \\text{number of nearest data points}: K\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ \\text{loop}\\ j\\ in\\ 1:\\ m\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{loop}\\ i\\ in\\ 1:\\ n\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Calculate:}\\ Distance(x_i, z_j) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Sort the calculated distances in ascending order} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Choose the top K least distances (nearest neighbors)} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{The most frequent class, e.g. c, in the K nearest}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{neighbors gets assigned to}\\ z_j \\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop}\\\\ \\ \\ \\ \\text{end loop}\\\\ \\end{array} \\] In the algorithm, any data measurement as necessary can be used, e.g. Euclidean, Manhattan, etc. "],["machinelearning3.html", "Chapter 11 Computational Learning III 11.1 Clustering (Unsupervised) 11.2 Meta-Learning 11.3 Natural Language Processing (NLP) 11.4 Time-Series Forecasting 11.5 Recommender Systems ", " Chapter 11 Computational Learning III In this chapter, we continue to discuss Computational Learning, emphasizing Clustering. We then cover a few applications of Computational Learning. 11.1 Clustering (Unsupervised) Here, we switch context to discuss datasets that are unlabeled. Our goal is to determine similarities among data points. The idea is to group similar (or homogeneous) data points to form clusters and hopefully be able to annotate and profile such discovered clusters (or structures). The similarity of data points is measured depending on the domain, which we expound on further. For our case, let us narrow down our discussion to only three types of clustering methods, though other literature may have other types of Clustering: Partition-based Clustering - this method of Clustering requires an input of K number of clusters. The algorithm then splits the dataset into K number of clusters. The core idea of this method is to then optimize each cluster by computing the distance of the local data points in each cluster based on a chosen metric criterion (e.g., Euclidean distance measurement). Until clusters are optimized, data points are iteratively relocated from cluster to cluster. Then the clusters are re-evaluated. An example of such a method is K-mean. Hierarchical-based Clustering - this method does not require an input of K number of clusters. There are two types of Hierarchical Clustering: Divisive Clustering - the idea is to group all data points into one cluster and divide the cluster into smaller clusters as necessary. We repeat the process until a stopping criterion. This Clustering is also called the top-bottom approach. Agglomerative Clustering - this is a bottom-up approach in which clusters are merged until a stopping criterion. From the onset, it treats each data point as a cluster. It then compares such small clusters based on chosen metric criteria and merges clusters to form larger clusters. The idea is to have a stopping criterion to determine the optimal number of clusters formed. Density-based Clustering - this method does not also require an input of K number of clusters. Rather it calculates the density of data points in terms of closeness. Data points that are closer to each other form a cluster. The Closeness of data points is measured with chosen distance metrics. An example of such a method is DBSCAN. Note that it helps to perform Exploratory Data Analysis (EDA) against the dataset first. This is to scale data points and deal with outliers as they tend to affect distance or similarity measures. In Chapter 9 (Computational Learning I), we have a few lists of Distance Metrics, which cover Euclidean, Manhattan, and Minkowski distance measurements. For specific domains, we leave readers to investigate other relevant distance measurements. 11.1.1 K-means (clustering) K-means is a partition-based clustering method formulated by Stuart P Lloyd in 1957. It is also regarded as a centroid clustering method because it finds the most representative of all data points within each cluster and designates it to be the centroid. In this section, we follow Lloyd’s k-means algorithm (here, we can use a multi-dimensional dataset): \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i^{(1)},...,x_i^{(D)})}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{number of clusters}: K\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ \\mu_1,...,\\mu_k \\leftarrow \\text{initial centroids} &amp; \\text{(randomly chosen)}\\\\ \\ \\ \\ \\text{while stopping criterion not met} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega_1, ..., \\omega_n = 0 &amp; \\text{(initialize cluster)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{loop i in 1 : N} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ k = \\text{arg} \\underset{k&#39;}{\\text{min}} \\|x_i - \\mu_{k&#39;} \\|^2_2 &amp; \\text{(euclidean distance)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega_i \\leftarrow k &amp; \\text{(relocation of clusters)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{loop k in 1 : K} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_k = \\frac{1}{|\\omega_{i..n}=k|} \\sum_{i,\\omega_i = k} x_i &amp; \\text{(recomputation of centroids)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop} \\\\ \\ \\ \\ \\text{end while loop}\\\\ \\ \\ \\ \\text{Output: } C = \\{\\omega_1,...,\\omega_n\\} \\end{array} \\] Here, our algorithm uses a vector denoted by \\(\\omega\\) such that its length equals the number of data points. Each element in the vector corresponds to the k cluster in which the corresponding data point belongs. The relocation of each data point to the corresponding k cluster is determined by choosing the distance closest to a k-centroid. When all data points are relocated, a new set of k-centroids is recalculated. The process repeats until such that the recalculated k-centroids do not change. The key to achieving a good cluster is to provide the number of K clusters and their corresponding centroids. Awareness of the domain helps to correctly (though arbitrarily) choose the K clusters and the centroids needed to optimally perform the K-means method. Let us now review our example implementation of K-means algorithm: my.kmeans &lt;- function(x, K=3, limit=50, tol=1e-5) { N = nrow(x); D = ncol(x) compute.centroid &lt;- function(omega) { centroid = matrix(0, nrow=K, ncol=D, byrow=TRUE) for (k in 1:K) { idx = which(omega == k) centroid[k,] = apply(x[idx,], 2, mean) } centroid } initialize &lt;- function(x) { omega = sample(K, size=N, replace=TRUE) compute.centroid(omega) } distance &lt;- function(x, centroid) { sqrt(sum((x - centroid)^2)) # euclidean } stopping.criterion &lt;- function(centroid, old.centroid) { abs( sum(centroid - old.centroid) ) } algorithm &lt;- function(x, K, limit, tol) { iter = 0 old.centroid = matrix(0, nrow=K, ncol=D, byrow=TRUE) centroid = initialize(x) # initialize centroid while (stopping.criterion(centroid, old.centroid) &gt; tol) { omega = rep(0, N) for (i in 1:N) { closest = rep(0, K) for (k in 1:K) { closest[k] = distance(x[i,], centroid[k,]) } k = which.min(closest) omega[i] = k } old.centroid = centroid centroid = compute.centroid(omega) iter = iter + 1 if (iter &gt;= limit) break # hard stop } omega } algorithm(x, K, limit, tol) } To illustrate, let us generate two clusters using the following dataset. set.seed(142) N = 200; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v) x = cbind(c(x1.blue, x1.red), c(x2.blue, x2.red)) Let us now plot the result of my.kmean(.) (see Figure 11.1). set.seed(2021) K = 2 clusters = my.kmeans(x, K=K, limit=15) plot(NULL, xlim=range(x[,1]), ylim=range(x[,2]), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;K-mean Clustering&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0,v=0,lty=3) color = c(&quot;red&quot;, &quot;navyblue&quot;, &quot;green&quot;) for (k in 1:K) { idx = which(clusters == k) points(x[idx,1], x[idx,2], col=color[k], pch=20) } Figure 11.1: K-mean Clustering We leave readers to also investigate the Hartigan K-means method. 11.1.2 Hierarchical (clustering) Hierarchical Clustering is a data mining technique to group similar data points to form a clustr. We begin this discussion with an illustration using the following data set. set.seed(2020) v = sample(seq(50, 100), 8, replace=TRUE) x = matrix(v, nrow=4, ncol=2, byrow=TRUE) rownames(x) = paste0(&quot;data.point&quot;, seq(1,4)) colnames(x) = paste0(&quot;feature&quot;, seq(1,2)) x ## feature1 feature2 ## data.point1 82 70 ## data.point2 81 74 ## data.point3 56 53 ## data.point4 56 70 The first step in clustering is to determine similarities. In our discussion, we use distance to measure the proximity (closeness) of data points. In particular, we use Euclidean distance. As an illustration, an approach to computing the distance from any data point to any other data point is to plug the data points into a diagonal distance matrix like so: (dist.matrix = dist(x, diag=TRUE, method = &quot;euclidean&quot;)) ## data.point1 data.point2 data.point3 data.point4 ## data.point1 0.000 ## data.point2 4.123 0.000 ## data.point3 31.064 32.650 0.000 ## data.point4 26.000 25.318 17.000 0.000 Each entry is calculated using the Euclidean distance (albeit other distance measurements can be tried such as Manhattan, Minkowski, Correlation, and so on): \\[\\begin{align} Dist_i = \\sqrt{\\sum^D_{d=1}\\left(x_1^{(d)} - x_2^{(d)}\\right)^2}\\ \\ \\ \\ \\ \\text{where D is dimension (no. of features)} \\end{align}\\] For example, the distance between data points 1 and 2 is: sqrt(sum((x[1,] - x[2,])^2)) ## [1] 4.123 Note that building the hierarchical cluster starts with a pair-wise approach. We pair data points based on which pairs have the closest distance (in increasing order). The decision to merge (or cluster) a pair of data points is based on the above distance matrix. Notice that the smallest (closest) distance in the matrix is 4.1231 between data.point2 and data.point1. Therefore, they are merged first. min.dist = max(dist.matrix) (lab = rownames(which(as.matrix(dist.matrix)==min.dist, arr.ind=TRUE))) ## [1] &quot;data.point3&quot; &quot;data.point2&quot; ## 1 6 5 3 2 4 ## 4.123 17.000 25.318 26.000 31.064 32.650 In R, we can use the function order(.) to order the distances in increasing order. So then, the next smallest distance is 17 between data.point4 and data.point3. They are merged next. o = order(dist.matrix); d = dist.matrix[o]; names(d) = o; d ## 1 6 5 3 2 4 ## 4.123 17.000 25.318 26.000 31.064 32.650 next.min.dist = d[2] (lab = rownames(which(as.matrix(dist.matrix)==next.min.dist, arr.ind=TRUE))) ## [1] &quot;data.point4&quot; &quot;data.point3&quot; All pairs that are formed are considered clusters. The next step is to merge the pairs (clusters) to form larger clusters. When merging, we have three methods: Single - each data point from one cluster is paired with data points from other clusters. We then evaluate the distance between pairs with the closest distance. Finally, we merge the two clusters having the closest pair. Complete - each data point from one cluster is paired with data points from other clusters. We then evaluate the distance between pairs with the farthest distance. Finally, we merge the two clusters having the farthest pair. Average - the centroid of one cluster is paired with the centroid of another cluster. We then evaluate which distance between centroids has the closest distance. Finally, we merge the two clusters having the closest centroids. Let us see the plot for the formation of two clusters using a Dendrogram for the formation of two clusters. See Figure 11.2). In the figure, we use Complete as a method to merge clusters. library(dplyr) my.hierarchy.cluster = hclust(dist.matrix, method=&quot;complete&quot;) plot(my.hierarchy.cluster, ylab=&quot;Height&quot;, xlab=&quot;Clusters&quot;, main=&quot;Hierarchical Clustering (Dendogram)&quot;) Figure 11.2: (Complete) Hierarchical Clustering (Dendogram) Let us plot using Average as method to merge clusters (see Figure 11.2). library(dplyr) my.hierarchy.cluster = hclust(dist.matrix, method=&quot;average&quot;) plot(my.hierarchy.cluster, ylab=&quot;Height&quot;, xlab=&quot;Clusters&quot;, main=&quot;Hierarchical Clustering (Dendogram)&quot;) Figure 11.3: (Average) Hierarchical Clustering (Dendogram) For other variants of Hierarchical Clustering, we leave readers to investigate Balanced Iterative Reducing and Clustering (BIRCH) method, Clustering Using Representatives (CURE) method, and Chameleon method. 11.1.3 DBSCAN (clustering) DBSCAN, introduced by Martin Ester et al. (1996), is short for Density-based spatial clustering of applications with noise and is a density-based clustering algorithm. The algorithm works by clustering data points using two parameters: minpts - This number determines the minimum allowable density of a region. If a region contains minpts=5, then the region is considered dense. radius\\((\\epsilon)\\) - This number determines the neighborhood proximity of a selected data point to other data points. Any data point is considered a neighbor if it is reachable by at least an \\(\\epsilon\\) closer to a selected data point. There are three distinct types of points: Core point - A point that has at least minpts neighbors, including itself, within an \\(\\epsilon\\) distance from itself. Border point - A point that belongs to a neighborhood but is not a core point. Noise point - A point that does not belong to a neighborhood; thus, it is neither a core point nor a border point. In Figure 11.4, we see seven core points forming a more extensive cluster with two border points. Also, there are three outliers in the figure. Figure 11.4: DBSCAN DBSCAN has the following algorithm: \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{{(x_i^{(1)},...,x_i^{(D)})}\\}_{i=1}^n\\\\ \\ \\ \\ \\text{radius}: \\epsilon\\\\ \\ \\ \\ \\text{minimum points to form dense cluster: } \\mathbf{minpts}\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ \\mathbf{\\text{foreach}}\\ \\text{unvisited point }x \\in \\mathbf{X}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Mark }x\\text{ as }\\mathbf{ VISITED}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{N }\\leftarrow \\text{Neighbors}(x,\\epsilon)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{if sizeof(N) &lt; }\\mathbf{minpts}\\text{ then}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Mark } x\\text{ as }\\mathbf{NOISE}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{else}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega \\leftarrow \\{x\\} &amp; \\text{(add point to new cluster)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{ExpandCluster}(x, N, \\omega, \\epsilon, \\mathbf{minpts})\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end if}\\\\ \\ \\ \\ \\text{end loop}\\\\ \\ \\ \\ \\text{Output: } C = \\{\\omega_1,...,\\omega_k\\}\\\\ \\end{array} \\] where: \\[ \\begin{array}{ll} \\text{ExpandCluster}(x, N, \\omega, \\epsilon, \\mathbf{minpts}):\\\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{foreach}}\\text{ point x}&#39; \\in N\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{if }}x&#39;\\text{ is not visited then}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{Mark }x&#39;\\text{ as }\\mathbf{ VISITED}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{N}&#39;\\leftarrow \\text{Neighbors}(x&#39;,\\epsilon)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{if }}\\text{ sizeof}(N&#39;) \\ge\\mathbf{minpts}\\text{ then}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ N \\leftarrow N \\cup N&#39; &amp; \\text{(merge neighbors)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end if}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end if}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{if }}x&#39;\\text{ is not a cluster member}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega \\leftarrow \\omega \\cup\\{x&#39;\\} &amp; \\text{(add point to cluster)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{end if}\\\\ \\ \\ \\ \\ \\ \\ \\ \\text{end loop}\\\\ \\end{array} \\] Below is our example implementation of DBSCAN: library(dequer) my.dbscan &lt;- function(x, eps, minpts) { n = nrow(x); K = 0 noise = NULL; visited = rep(FALSE, n) omega = rep(0, n) is.member &lt;- function(i) { omega[i] != 0 } distance &lt;- function(x, y) { sqrt(sum((x - y)^2)) } # euclidean neighbors &lt;- function(i, eps) { N = queue() for (j in 1:n) { if (j != i) { proximity = distance(x[i,], x[j,]) if (proximity &lt;= eps) { pushback(N, j) } } } N } merge.neighbors &lt;- function(N1, N2) { N = queue(); p = NULL; q = NULL while (length(N1)) { p = c(p, pop(N1)) } while (length(N2)) { q = c(q, pop(N2)) } p = union(p, q) # eliminate duplicates for (i in p) pushback(N, i) # form one queue N } expandCluster &lt;- function(p, K, N) { omega[p] &lt;&lt;- K while (length(N)) { j = pop(N) if (visited[j] == FALSE) { visited[j] &lt;&lt;- TRUE N.j = neighbors(j, eps) if (length(N.j) &gt;= minpts) { N = merge.neighbors(N, N.j) } } if (!is.member(j)) { omega[j] &lt;&lt;- K } } } algorithm &lt;- function() { for (p in 1:n) { if (visited[p] == FALSE) { visited[p] &lt;&lt;- TRUE N = neighbors(p, eps) if (length(N) &lt; minpts) { noise &lt;&lt;- c(noise, p) } else { K &lt;&lt;- K + 1 # new cluster expandCluster(p, K, N) } } } omega } algorithm() } Let us use the dataset below for our DBSCAN implementation: set.seed(142) N = 200; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v) x = cbind(c(x1.blue, x1.red), c(x2.blue, x2.red)) clusters = my.dbscan(x, eps = 1, minpts = 15) Figure 11.5 shows the plot of the cluster. K = length(unique(clusters)) - 1 plot(NULL, xlim=range(x[,1]), ylim=range(x[,2]), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;DBSCAN Clustering&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0,v=0,lty=3) color = c(&quot;green&quot;, &quot;navyblue&quot;, &quot;red&quot;) for (k in 0:K) { idx = which(clusters == k) points(x[idx,1], x[idx,2], col=color[k + 1], pch=20) } Figure 11.5: DBSCAN Clustering 11.1.4 Quality of Clustering One of the challenges in using a clustering method such as K-means is determining the optimal number of clusters to input. In this section, let us discuss three methods to evaluate the goodness of a cluster. Elbow Method - this method is one of the common measures to determine the quality of clusters. The name elbow becomes apparent once we plot the result of our evaluation. Here, we use WSS metrics to measure the sum of squares within clusters. For example, in Figure 11.6, the elbow shape is found at cluster equal to 2. Therefore, the optimal number of clusters is 2. elbow.method &lt;-function(x, K) { clusters = kmeans(x, centers=K, nstart=25) clusters$tot.withinss } K = 10 wss = rep(0, K) for (k in 1:K) { wss[k] = elbow.method(x, k) } plot(NULL, xlim=range(1:K), ylim=range(wss), ylab=&quot;within clusters sum of squares&quot;, xlab=&quot;number of clusters (k)&quot;, main=&quot;Elbow Method&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0,v=0,lty=3) points(seq(1,K), wss, pch=20) lines(seq(1,K), wss, type=&quot;b&quot;) abline(v=2, lty=2, col=&quot;red&quot;) Figure 11.6: Elbow Method Silhouette Method - the quality of clustering can be measured using the Silhouette method which measures a Silhouette Coefficient. There are two average distances to consider here and are called Cohesion and Separation respectively denoted by \\(\\mathbf{a(x)}\\) and \\(\\mathbf{b(x)}\\) where: \\(\\mathbf{a(x)}\\) refers to Cohesion taking the average distance of x to all other vectors within clusters. \\(\\mathbf{b(x)}\\) refers to Separation taking the average distance of x to the vectors between clusters. The Silhouette width, namely s(x) is expressed as such: \\[\\begin{align} S(x) = \\frac{b(x) - a(x)}{max\\left(a(x), b(x)\\right)}\\ \\ \\ \\ \\leftarrow \\begin{cases} -1 &amp; \\text{bad}\\\\ 0 &amp; \\text{indifferent}\\\\ 1 &amp; \\text{good} \\end{cases} \\label{eqn:eqnnumber502} \\end{align}\\] We then take the average, resulting in our Silhouette Coefficient: \\[\\begin{align} SC = \\frac{1}{N}\\sum_{i=1}^N S(x) \\end{align}\\] The goal is to find the minimum coefficient. To illustrate, let us review Figure 11.7. Notice that the minimum coefficient is at 3. library(cluster) silhouette.score &lt;-function(x, K) { clusters = kmeans(x, centers=K, nstart=25) ss = silhouette(clusters$cluster, as.matrix(dist(x))) mean(ss[, 3]) } K = 10 sc = rep(0, K ) for (k in 1:K) { sc[k] = silhouette.score(x, k + 1) } plot(NULL, xlim=range(1:K), ylim=range(sc), ylab=&quot;Silhouette Coefficient&quot;, xlab=&quot;number of clusters (k)&quot;, main=&quot;Silhouette Method&quot;) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0,v=0,lty=3) points(seq(1,K), sc, pch=20) lines(seq(1,K), sc, type=&quot;b&quot;) abline(v=3, lty=2, col=&quot;red&quot;) Figure 11.7: Silhouette Method Calinski-Harabasz Index - this measure is also called Variance Ratio Criterion which calculates the ratio of dispersion (Baarsch J. 2012). The formula is as follows: \\[ s(k) = \\frac{tr(B_k)}{tr(Wk)} \\times \\frac{\\left( N -k\\right)}{\\left(k-1\\right)} ,\\ \\ \\ \\ \\ \\ \\text{k = number of clusters} \\] where: \\[ \\underbrace{Bk = \\sum_{j=1}^k n_j\\left(\\mu_j - \\mu\\right) \\left(\\mu_j- \\mu\\right)^\\text{T}}_{\\text{Between Clusters}} \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{Wk = \\sum_{j=1}^{k} \\sum_{i=1}^{n_j} \\left(x_i - \\mu_j\\right) \\left(x_i - \\mu_j\\right)^\\text{T}}_{\\text{Within Cluster j}} \\] and where \\(N\\) is the number of samples in the dataset, \\(\\mu\\) is the center of the dataset, \\(n_j\\) is the number of samples in cluster j, and \\(\\mu_j\\) is the center of the cluster j. The \\(tr(.)\\) is a function that yields a trace matrix. A higher score indicates a better clustering. We leave readers to investigate other methods of evaluating clustering algorithms such as Bayesian Information Criterion (BIC), Davies-Bouldin Index (DBI), and Dunn’s index. 11.2 Meta-Learning Meta-Learning is the process of learning how to learn. In other words, the idea of being adaptive to changes goes beyond just being able to identify regressions or being able to recognize patterns and classify them. Self-adaptation, self-preservation, and self-healing are a few of many things that allow a system to advance farther into being autonomous. This book does not cover such topics; however, they are essential ingredients in advancing fundamental knowledge of computational learning. We thus leave readers to investigate them. In the following three sections, we introduce use cases commonly discussed in Machine Learning. Let us start with NLP. 11.3 Natural Language Processing (NLP) Natural Language Processing (NLP) is covered in Information Retrieval and Text Mining Theory. However, other literature covers NLP in Artificial Intelligence that deals with the computational processing of human languages based on Word Embeddings - a topic worth covering in the last chapter. A better way to explain NLP is to start with a simple list of sample documents - consider this as our corpus for now: D1 = c(&quot;There is a big animal in my house.&quot;) D2 = c(&quot;The animal has great big ears, great big eyes, great big teeth.&quot;) D3 = c(&quot;A wolf is an animal.&quot;) D4 = c(&quot;Little Red Riding Hood met a Wolf.&quot;) (documents = c(D1, D2, D3, D4)) ## [1] &quot;There is a big animal in my house.&quot; ## [2] &quot;The animal has great big ears, great big eyes, great big teeth.&quot; ## [3] &quot;A wolf is an animal.&quot; ## [4] &quot;Little Red Riding Hood met a Wolf.&quot; In our sample list of documents, we extract words and convert the list into a vector of words - we call this vector as bag of words. It is raw and needs cleanup. Our goal is to perform pre-processing which includes dealing with tokenization, case-sensitivity, stopwords, N-grams, stemming, and lemmatization to name a few. 11.3.1 Pre-Processing Texts Part of the pre-processing text is to tokenize terms in documents. Tokenization is extracting units of text information called tokens. A token is represented as a word (or a term) in a document. Below, we extract words and map them into a vector. (words2vector = unlist( strsplit(documents, split=&#39; &#39;))) ## [1] &quot;There&quot; &quot;is&quot; &quot;a&quot; &quot;big&quot; &quot;animal&quot; &quot;in&quot; ## [7] &quot;my&quot; &quot;house.&quot; &quot;The&quot; &quot;animal&quot; &quot;has&quot; &quot;great&quot; ## [13] &quot;big&quot; &quot;ears,&quot; &quot;great&quot; &quot;big&quot; &quot;eyes,&quot; &quot;great&quot; ## [19] &quot;big&quot; &quot;teeth.&quot; &quot;A&quot; &quot;wolf&quot; &quot;is&quot; &quot;an&quot; ## [25] &quot;animal.&quot; &quot;Little&quot; &quot;Red&quot; &quot;Riding&quot; &quot;Hood&quot; &quot;met&quot; ## [31] &quot;a&quot; &quot;Wolf.&quot; Now, there is pre-processing to be made when tokenizing documents. Let us cover a few of them: Case-Sensitivity If we do not care much about case sensitive characters, we may choose to set all words in small cases: (documents = tolower(documents)) ## [1] &quot;there is a big animal in my house.&quot; ## [2] &quot;the animal has great big ears, great big eyes, great big teeth.&quot; ## [3] &quot;a wolf is an animal.&quot; ## [4] &quot;little red riding hood met a wolf.&quot; Punctuations Note that documents have punctuations or special whitespaces that we need to remove. (documents = gsub(&quot;[[:punct:]]&quot;, &quot;&quot;, documents)) ## [1] &quot;there is a big animal in my house&quot; ## [2] &quot;the animal has great big ears great big eyes great big teeth&quot; ## [3] &quot;a wolf is an animal&quot; ## [4] &quot;little red riding hood met a wolf&quot; StopWords Apparently, we also notice that certain terms such as (is, a, an, in, the) are common in documents that may give little to no information. We can treat them as stopwords and can be removed. For stopwords, let us use anti_join(.) function from dplyr library. Also, let us use tibble(.) to convert the documents to tibble data frame. Here, we use unnest_tokens(.) from tidytext library to tokenize. library(tidytext) library(dplyr) stopwords = c(&quot;is&quot;, &quot;a&quot;, &quot;an&quot;, &quot;has&quot;, &quot;in&quot;, &quot;the&quot;, &quot;there&quot;) stopwords.frame = tibble( term = stopwords) doc.frame = tibble(docid = c(&#39;D1&#39;,&#39;D2&#39;,&#39;D3&#39;,&#39;D4&#39;), term = documents) tokenized.table = doc.frame %&gt;% unnest_tokens(term, term) %&gt;% anti_join(stopwords.frame, by = c(&quot;term&quot; = &quot;term&quot; )) We now have the final pre-processed list of words above. preprocessed.doc &lt;- function(t) { n = length(tokenized.table) ids = c(&quot;D1&quot;, &quot;D2&quot;, &quot;D3&quot;, &quot;D4&quot;) docs = rep(&quot;&quot;, n) for (i in 1:length(ids)) { idx = which(tokenized.table$docid == ids[i]) docs[i] = paste0(tokenized.table$term[idx], sep=&quot; &quot;, collapse=&quot;&quot;) } docs } (documents = preprocessed.doc(tokenized.table)) ## [1] &quot;big animal my house &quot; ## [2] &quot;animal great big ears great big eyes great big teeth &quot; ## [3] &quot;wolf animal &quot; ## [4] &quot;little red riding hood met wolf &quot; N-Gram At times, in Text Mining, it helps to consider also a group of words and not just individual words. For example, a sequence of one, two, and three words is called a unigram, bigram, and trigram, respectively. For example, below, we generate a sequence of bigram tokens. tokens = doc.frame %&gt;% unnest_tokens(term, term, token = &quot;ngrams&quot;, n = 2) %&gt;% dplyr::count(term, sort=TRUE) c(tokens) ## $term ## [1] &quot;great big&quot; &quot;a wolf&quot; &quot;a big&quot; &quot;an animal&quot; ## [5] &quot;animal has&quot; &quot;animal in&quot; &quot;big animal&quot; &quot;big ears&quot; ## [9] &quot;big eyes&quot; &quot;big teeth&quot; &quot;ears great&quot; &quot;eyes great&quot; ## [13] &quot;has great&quot; &quot;hood met&quot; &quot;in my&quot; &quot;is a&quot; ## [17] &quot;is an&quot; &quot;little red&quot; &quot;met a&quot; &quot;my house&quot; ## [21] &quot;red riding&quot; &quot;riding hood&quot; &quot;the animal&quot; &quot;there is&quot; ## [25] &quot;wolf is&quot; ## ## $n ## [1] 3 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 Stemming and Lemmatization In the English language, we use inflection forms for parts of speech. For example, the adjective form of beauty is beautiful. The past participle of run is running. The plural form of chocolate is chocolates. Now, it may be necessary to reduce such terms into their stem (or root) form, e.g. beauty, run, and chocolate. This method is called Stemming and Lemmatization. Stemming cuts the suffix and prefix of words. For example, stemming cuts the suffix ing from the term cooking. A good choice of stemmer is important to avoid over stemming or under stemming. For example, a poor stemmer may cut driving into driv. Lemmatization takes into account the morphological structure of words. For example, if we simply cut the prefix ing for running, we may end up with runn. Lemmatization allows us to transform running into run, swimming into swim, driving into drive. Also, words such as am, is, are get converted into be. To illustrate, let us use the quanteda library to perform all the pre-processing methods we discussed so far, including stemming and lemmatization. Here, we use dfm(.) function. First, we generate a document corpus using corpus(.) function from quanteda library: library(quanteda) doc.frame = tibble(docid = c(&#39;D1&#39;,&#39;D2&#39;,&#39;D3&#39;,&#39;D4&#39;), term = c(D1, D2, D3, D4)) doc.corpus = corpus(doc.frame, docid_field = &quot;docid&quot;, text_field = &quot;term&quot;) summary(doc.corpus) ## Corpus consisting of 4 documents, showing 4 documents: ## ## Text Types Tokens Sentences ## D1 9 9 1 ## D2 10 15 1 ## D3 6 6 1 ## D4 8 8 1 Now, we can perform the pre-processing: doc.tokens = tokens(doc.corpus, remove_punct = TRUE, remove_numbers = TRUE) doc.tokens = tokens_select(doc.tokens, stopwords, case_insensitive = TRUE, selection=&#39;remove&#39;) doc.tokens = tokens_wordstem(doc.tokens) doc.tokens ## Tokens consisting of 4 documents. ## D1 : ## [1] &quot;big&quot; &quot;anim&quot; &quot;my&quot; &quot;hous&quot; ## ## D2 : ## [1] &quot;anim&quot; &quot;great&quot; &quot;big&quot; &quot;ear&quot; &quot;great&quot; &quot;big&quot; &quot;eye&quot; &quot;great&quot; ## [9] &quot;big&quot; &quot;teeth&quot; ## ## D3 : ## [1] &quot;wolf&quot; &quot;anim&quot; ## ## D4 : ## [1] &quot;Littl&quot; &quot;Red&quot; &quot;Ride&quot; &quot;Hood&quot; &quot;met&quot; &quot;Wolf&quot; One point to make is that stemming results in the term hous for house and anim for animal, which may be correct but not as expected. Another point to make is that quanteda has its list of stopwords: head(stopwords(source = &quot;smart&quot;), n=25) # listing only 25 stopwords ## [1] &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; ## [5] &quot;above&quot; &quot;according&quot; &quot;accordingly&quot; &quot;across&quot; ## [9] &quot;actually&quot; &quot;after&quot; &quot;afterwards&quot; &quot;again&quot; ## [13] &quot;against&quot; &quot;ain&#39;t&quot; &quot;all&quot; &quot;allow&quot; ## [17] &quot;allows&quot; &quot;almost&quot; &quot;alone&quot; &quot;along&quot; ## [21] &quot;already&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; ## [25] &quot;am&quot; 11.3.2 Ranking and Scoring One of the simplest pieces of information we can gather from a sequence of pre-processed words is the corresponding ranks based on frequency. For example, we show that the terms big and great are ranked at the top below. words = unlist( strsplit(documents, split=&#39; &#39;)) (ranked.words = sort(table(words), decreasing=TRUE)) ## words ## big animal great wolf ears eyes hood house little met ## 4 3 3 2 1 1 1 1 1 1 ## my red riding teeth ## 1 1 1 1 We can also just quickly validate the result by using count(.). library(tidytext) library(dplyr) stopwords.frame = tibble( term = stopwords) tokens = tokenized.table %&gt;% dplyr::count(term, sort=TRUE) c(tokens) ## $term ## [1] &quot;big&quot; &quot;animal&quot; &quot;great&quot; &quot;wolf&quot; &quot;ears&quot; &quot;eyes&quot; &quot;hood&quot; ## [8] &quot;house&quot; &quot;little&quot; &quot;met&quot; &quot;my&quot; &quot;red&quot; &quot;riding&quot; &quot;teeth&quot; ## ## $n ## [1] 4 3 3 2 1 1 1 1 1 1 1 1 1 1 Ranking based on TF-IDF Ranking can be achieved using TF-IDF, short for Term Frequency - Inverse Document Frequency. It is a method to rank relevant words (or terms) found in documents. A simple ranking method is to count for the occurrence of terms - the frequency - which we demonstrated in the previous section. The result only provides the frequency as the basis for our ranking. However, it does not necessarily provide accurate relevance of the terms to the four documents from where they come. TF-IDF uses a formula to compute for relevance: \\[\\begin{align} \\text{score}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t,D) \\end{align}\\] where: \\[\\begin{align} \\text{TF}(t, d) &amp;= \\frac{\\text{count}(t\\ \\in\\ d)}{\\text{count}(d)} = \\frac{\\text{Freq of the term }\\mathbf{t}\\ \\text{ in d}}{\\text{No of words in d}} \\\\ \\text{IDF}(t,D) &amp;= \\log_e \\left(\\frac{N}{df_t \\text{ = count}(d\\ \\in\\ D:\\ t\\ \\in\\ d)}\\right)\\\\ &amp;= \\log_e \\left( \\frac{\\text{Total No of Documents}}{\\text{No of Documents containing the term }\\mathbf{t}} \\right) \\end{align}\\] and where: \\(\\mathbf{\\text{TF}(t,d)}\\) is the frequency of word (t) found in document (d). \\(\\mathbf{\\text{IDF}(t,D)}\\) is the number of documents containing word (t). Its intent is to give high score (relevance) to rare words. Note that stopwords such as the, a, and an are high in frequency and are not rare. In TF-IDF, the inclusion of the IDF formula in the equation allows us to give low scores (or low relevance) to stopwords; thus, we choose not to remove the stopwords in our corpus as they get pushed to lower relevance. To illustrate TF-IDF, let us calculate the TF-IDF weight for the terms big in D1 versus in D2 using the pre-processed documents. \\[ \\begin{array}{ll} \\text{score}(big, D1) &amp;= TF(big, D1) \\times IDF(big,D) \\\\ &amp;= \\frac{\\text{Freq of the term }\\mathbf{big}\\ \\text{ in D1}}{\\text{No of words in D1}} \\times \\log_e \\left( \\frac{\\text{Total No of Documents}}{\\text{No of Docs containing the term }\\mathbf{big}} \\right) \\\\ &amp;= \\frac{1}{4} \\times \\log_e \\left( \\frac{4}{2} \\right) \\\\ &amp;= 0.25 \\times \\log_e(2)\\\\ &amp;= 0.25 \\times 0.6931472 = 0.1732868\\\\ \\\\ \\text{score}(big, D2) &amp;= \\frac{3}{10} \\times \\log_e \\left( \\frac{4}{2} \\right) \\\\ &amp;= 0.30 \\times \\log_e(2)\\\\ &amp;= 0.30 \\times 0.6931472 = 0.2079442 \\end{array} \\] The calculations show that the term big has a 17.3% relevance in D2 and has an 8.66% relevance in D1, indicating that the term is more relevant in D2 than in D1. An enhanced variance of TF-IDF called Okapi BM25 (S.E. Robertson et al. 1994) has the following formula below that accounts for other considerations when ranking terms: \\[\\begin{align} score(D,Q)_{(BM25)} = \\underbrace{ \\sum_{t\\ \\in\\ Q} \\frac{TF_{t,d} \\times \\left(k + 1\\right)}{TF_{t,d} + k \\times \\left(1-b+b \\times \\frac{dl}{adl}\\right)}}_{\\text{TF}} \\times \\underbrace{\\log_e \\left(\\frac{N - df_t + 0.5}{df_t + 0.5}\\right)}_{\\text{IDF}} \\end{align}\\] The first consideration is to account for Term Saturation, which modifies the TF into the following formula with the idea that as the frequency of a given term fully reaches some maximum level, any further increase in frequency may not provide any further benefit. Therefore the k parameter controls the effect of TF. \\[\\begin{align} TF(t, d)\\ \\text{modified into} \\rightarrow \\frac{TF(t,d) \\times (k + 1)}{TF(t,d) + k} \\end{align}\\] Figure 11.8 illustrates the effect of different values of k: Figure 11.8: TF (Term Saturation The second consideration accounts for Document Length. The TF term gets modified further like so: \\[\\begin{align} TF(t, d)\\ \\text{modified into} \\rightarrow \\frac{TF(t,d) \\times (k + 1 ) }{TF(t,d) + k \\times \\frac{dl}{adl}} \\end{align}\\] where dl is the document length and adl is the average document length. The relevance of a term in a document is much higher if the length is shorter than if it is longer; thus, a higher weight is given to terms in shorter documents. On the other hand, if document length is not as important, we can disable or reduce the weight using the b parameter, giving a value between [0,1]. \\[\\begin{align} TF(t, d)\\ \\text{modified into} \\rightarrow \\frac{TF(t,d) \\times (k + 1)}{TF(t,d) + k \\times \\left(1 - b + b \\times\\frac{dl}{adl}\\right)} \\end{align}\\] The last consideration is the adjustment of IDF. This adjustment can be regarded as an additional tune-up for term relevance from Lucene variance. \\[\\begin{align} IDF = \\log_e\\left(\\frac{N}{df_t}\\right) \\propto \\log_e \\left( \\frac{1+ N - df_t + 0.5}{df_t + 0.5} \\right) \\end{align}\\] Let us apply Okapi BM25 to our previous example (assume k=2 and b=0.70): \\[ \\begin{array}{ll} \\text{score}(big, D1) &amp;= TF(big, D1) \\times IDF(big,D) \\\\ &amp;= \\frac{\\frac{1}{4} \\times (2 + 1)}{ \\frac{1}{4} + 2 \\times \\left(1 - 0.70 + 0.70 \\times \\frac{4}{5.5}\\right)} \\times \\log_e \\left( \\frac{1 + 4 - 2 + 0.5}{2 + 0.5} \\right) \\\\ &amp;= \\frac{0.75}{1.868182} \\times \\log_e(\\frac{3.5}{2.5})\\\\ &amp;= 0.1350801 \\\\ \\text{score}(big, D2) &amp;= \\frac{\\frac{3}{10} \\times (2 + 1)}{ \\frac{3}{10} + 2 \\times \\left(1 - 0.70 + 0.70 \\times \\frac{10}{5.5}\\right)} \\times \\log_e \\left( \\frac{1 + 4 - 2 + 0.5}{2 + 0.5} \\right) \\\\ &amp;= \\frac{0.9}{3.445455} \\times \\log_e(\\frac{3.5}{2.5})\\\\ &amp;= 0.08789115 \\end{array} \\] The document lengths for d1 and d2 are 4 and 10 respectively. The average length is 5.5 based on the computation below: D = documents dl = function (d) { length( unlist(strsplit(d, &quot; &quot;)) ) } (adl = ( dl(D[1]) + dl(D[2]) + dl(D[3]) + dl(D[4]) ) / 4) ## [1] 5.5 Let us now re-evaluate our original ranked words and rank them using additional functions from tidytext library. Here, we use bind_tf_idf(.) function to calculate TF-IDF score. library(tidytext) library(dplyr) tokens = tokenized.table %&gt;% group_by(docid, term) %&gt;% tally() %&gt;% arrange(desc(n)) head( tokens %&gt;% bind_tf_idf(term, docid, n) ) # display top 5 ## # A tibble: 6 x 6 ## # Groups: docid [2] ## docid term n tf idf tf_idf ## &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 D2 big 3 0.3 0.693 0.208 ## 2 D2 great 3 0.3 1.39 0.416 ## 3 D1 animal 1 0.25 0.288 0.0719 ## 4 D1 big 1 0.25 0.693 0.173 ## 5 D1 house 1 0.25 1.39 0.347 ## 6 D1 my 1 0.25 1.39 0.347 We leave readers to evaluate other variants of BM25 and its other counterparts. Scoring Based on Cumulative Gain While TF-IDF provides us the ability to rank words, we use two popular ranking measures to determine the quality of ranking, namely mean Average Precision (mAP) and Normalized Discounted Cumulative Gain (nDCG). In this section, let us cover nDCG and defer the discussion of mAP in the Recommender System (Image Classification) section. We begin with the idea that a search engine retrieves a list of documents based on a given user query. In search engines, we rank retrieved articles based on relevance to our query. The relevance of each retrieved documents can be categorically labeled as: 1 - not relevant, 2 - fairly relevant, 3 - quite relevant. The value given to a document is called the Gain. However, we are after the cumulative gain to score the effectiveness of the retrieval, for which we use the following nDCG Scoring: \\[\\begin{align} nDCG = \\frac{DCG_{(n)}}{IDCG_{(n)}}\\ \\ \\ \\ \\ \\ \\text{where n is number of retrieved documents} \\end{align}\\] The score depends on the Cumulative Gain and Discounted Cumulative Gain and Ideal Discounted Cumulative Gain (IDC) expressed respecitvely below: \\[\\begin{align} \\mathbf{CG} = \\sum_{i=1}^n G_i\\ \\ \\ \\ \\ \\mathbf{DCG} = \\sum_{i=1}^n \\frac{G_i}{\\log_e(i + 1)}\\\\ \\mathbf{IDCG} = \\sum \\left[\\text{sort}\\_\\text{terms} \\left\\{\\frac{G_i}{\\log_e(i + 1)}\\right\\}_{i=1}^n\\right] \\end{align}\\] To illustrate, suppose we have the following documents and their relevance (the Gain): \\[ D_{1} = 3\\ \\ \\ \\ \\ \\ D_{2} = 1\\ \\ \\ \\ \\ \\ D_{3} = 2\\ \\ \\ \\ \\ \\ D_{4} = 0\\ \\ \\ \\ \\ \\ D_{5} = 2\\ \\ \\ \\ \\ \\ \\] The Cumulative Gain (CG) result is: \\[\\begin{align} \\mathbf{CG} = \\sum_{i=1}^n G_i = 3 + 1 + 2 + 0 + 3 = 9 \\end{align}\\] The Discounted Cumulative Gain (DCG) is: \\[\\begin{align} \\mathbf{DCG}_{(5)} &amp;= \\sum_{i=1}^n \\frac{G_i}{\\log_e(i + 1)} \\\\ &amp;=\\frac{3}{\\log_e(2)} + \\frac{1}{\\log_e(3)} + \\frac{2}{\\log_e(4)} \\frac{0}{\\log_e(5)} + \\frac{3}{\\log_e(6)} = 8.355351 \\nonumber \\end{align}\\] The Ideal (sorted) Discounted Cumulative Gain (IDCG) is: \\[\\begin{align} \\mathbf{IDCG}_{(5)} &amp;= \\sum_{i=1}^n \\frac{G_i}{\\log_e(i + 1)}\\\\ &amp;= \\frac{3}{\\log_e(2)} + \\frac{3}{\\log_e(3)} + \\frac{2}{\\log_e(4)} + \\frac{1}{\\log_e(5)} + \\frac{0}{\\log_e(6)} = 9.059608 \\nonumber \\end{align}\\] Finally, the nDCG score is: \\[\\begin{align} \\mathbf{nDCG}_{(5)} = \\frac{DCG_{(5)}}{IDCG_{(5)}} = \\frac{8.355351}{9.059608} = 0.9222641 \\end{align}\\] Based on the score, we can say that the search engine demonstrates 92.23% effectiveness in retrieving the relevant documents. 11.3.3 Document Similarity For Information Retrieval, a classic representation of texts is in the form of a Vector Space Model (VSM). The model is achieved by casting each term in the texts into its corresponding numerical equivalence that becomes helpful in measuring similarity at some point. Our goal is to use VSM to determine term similarities or word relationships. We start with a generic similarity function like so: \\[\\begin{align} \\mathbf{\\text{similarity}}\\left(d^{(1)}, d^{(2)}\\right)\\ \\ \\ \\ \\ \\ \\ \\ \\text{where }\\mathbf{d^{(1)}}\\ \\text{is 1st document and }\\mathbf{d^{(2)}}\\ \\text{is 2nd document.} \\end{align}\\] Note that we can also compare a given query relevant to a retrieved document like so: \\[\\begin{align} \\mathbf{\\text{similarity}}(q, d)\\ \\ \\ \\ \\ \\ \\ \\ \\text{where }\\mathbf{q}\\ \\text{is query and }\\mathbf{d}\\ \\text{is document.} \\end{align}\\] The goal is to determine if the given query is similar (or relevant) to a retrieved document. Here, a good measure of similarity is based on cosine similarity; albeit, we also can use euclidean distance and so on for comparison. \\[\\begin{align} \\mathbf{\\text{similarity}}(q, d) = \\mathbf{\\text{cosine}}(\\vec{q}, \\vec{d}) = \\frac{\\vec{q} \\cdot \\vec{d}}{\\|\\vec{q}\\|\\|\\vec{d}\\|} = \\frac{\\sum_{i=1}^{|v|} q_i d_i}{\\sqrt{\\sum_{i=1}^{|v|}q_i^2} \\sqrt{\\sum_{i=1}^{|v|} d_i^2}} \\end{align}\\] where: \\(|v|\\) represents the number of unique features (unique terms). \\(\\mathbf{q_i}\\) is a calculated numerical value, e.g. tf-idf score for ith term in the query (q). \\(\\mathbf{d_i}\\) is a calculated numerical value, e.g. tf-idf score for ith term in the document (d). To illustrate, let us cast our original document corpus into Document Feature Matrix (DFM), also called Document Term Matrix (DTM) using dfm(.) function from quanteda library. Pre-processing can also be applied through the function (below, we disable stemming): doc.dfm = dfm(doc.corpus, tolower = TRUE, remove_punct = TRUE, remove_numbers = TRUE, stem = FALSE, remove = stopwords ) (dtm.doc = data.frame(as.matrix(doc.dfm)))[,1:8] # display in data frame ## big animal my house great ears eyes teeth ## D1 1 1 1 1 0 0 0 0 ## D2 3 1 0 0 3 1 1 1 ## D3 0 1 0 0 0 0 0 0 ## D4 0 0 0 0 0 0 0 0 We can convert the data frame into a Term Document Matrix (TDM) using transpose (we can come back to this form in a later section for LSA): (tdm.doc = data.frame(t(as.matrix(dtm.doc)))) ## D1 D2 D3 D4 ## big 1 3 0 0 ## animal 1 1 1 0 ## my 1 0 0 0 ## house 1 0 0 0 ## great 0 3 0 0 ## ears 0 1 0 0 ## eyes 0 1 0 0 ## teeth 0 1 0 0 ## wolf 0 0 1 1 ## little 0 0 0 1 ## red 0 0 0 1 ## riding 0 0 0 1 ## hood 0 0 0 1 ## met 0 0 0 1 We can use the topfeatures(.) function with DFM to rank terms. It shows the same result as before. topfeatures(doc.dfm) ## big animal great wolf my house ears eyes teeth little ## 4 3 3 2 1 1 1 1 1 1 For cosine similarity, we can use the dfm_weight(.) function to obtain the same result as dfm(.) as.matrix( dfm_weight(doc.dfm) ) ## features ## docs big animal my house great ears eyes teeth wolf little red riding ## D1 1 1 1 1 0 0 0 0 0 0 0 0 ## D2 3 1 0 0 3 1 1 1 0 0 0 0 ## D3 0 1 0 0 0 0 0 0 1 0 0 0 ## D4 0 0 0 0 0 0 0 0 1 1 1 1 ## features ## docs hood met ## D1 0 0 ## D2 0 0 ## D3 0 0 ## D4 1 1 Using the DTM above, the corresponding Cosine Similarity between the document D1 and D2 is: \\[\\begin{align} \\vec{d^{(1)}} \\cdot \\vec{d^{(2)}} &amp;= \\sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)}\\\\ &amp;=(1\\times 3) + (1 \\times 1) + (1\\times 0) + (1\\times 0) + (0 \\times 3) + \\nonumber \\\\ &amp;\\ ( 0 \\times 1 ) + ( 0 \\times 1 ) + ( 0 \\times 1 ) \\nonumber \\\\ &amp;= 4 \\nonumber \\\\ \\nonumber \\\\ \\sqrt{\\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \\sqrt{1^2 + 1^2 + 1^2 + 1^2} = 2\\\\ \\nonumber \\\\ \\sqrt{\\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \\sqrt{3^2 + 1^2 + 3^2 + 1^2 + 1^2 + 1^2} = 4.690416\\\\ \\nonumber \\\\ \\mathbf{\\text{cosine}}(\\vec{q}, \\vec{d}) &amp;= 4\\ /\\ (\\ 2 \\times 4.690416\\ ) = 0.4264014 \\end{align}\\] A value of one means that the two documents are similar. Alternatively, we can also generate a Document Term Matrix with TF-IDF score: tfidf = dfm_tfidf(doc.dfm, scheme_tf = &quot;prop&quot;, base=exp(1), force=TRUE) round( as.matrix( tfidf ), 2) ## features ## docs big animal my house great ears eyes teeth wolf little red ## D1 0.17 0.07 0.35 0.35 0.00 0.00 0.00 0.00 0.00 0.00 0.00 ## D2 0.21 0.03 0.00 0.00 0.42 0.14 0.14 0.14 0.00 0.00 0.00 ## D3 0.00 0.14 0.00 0.00 0.00 0.00 0.00 0.00 0.35 0.00 0.00 ## D4 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.12 0.23 0.23 ## features ## docs riding hood met ## D1 0.00 0.00 0.00 ## D2 0.00 0.00 0.00 ## D3 0.00 0.00 0.00 ## D4 0.23 0.23 0.23 Using the DTM above, the corresponding cosine similarity between the document D1 and D2 is: \\[\\begin{align} \\vec{d^{(1)}} \\cdot \\vec{d^{(2)}} &amp;= \\sum_{i=1}^{|v|} d_i^{(1)} d_i^{(2)}\\\\ &amp;=(0.17\\times 0.21) + (0.07 \\times 0.03) + (0.35\\times 0) + (0.35\\times 0) \\nonumber \\\\ &amp;\\ \\ \\ + (0 \\times 0.42) + ( 0 \\times 0.14 ) + ( 0 \\times 0.14 ) + ( 0 \\times 0.14 ) \\nonumber \\\\ &amp;= 0.0378 \\nonumber\\\\ \\nonumber \\\\ \\sqrt{\\sum_{i=1}^{|v|} (d^{{(1)}})^2} &amp;= \\sqrt{0.17^2 + 0.07^2 + 0.35^2 + 0.35^2 } = 0.5280152\\\\ \\nonumber \\\\ \\sqrt{\\sum_{i=1}^{|v|} (d^{{(2)}})^2} &amp;= \\sqrt{0.03^2 + 0.42^2 + 0.14^2 + 0.14^2 + 0.14^2} = 0.4859012\\\\ \\nonumber \\\\ \\mathbf{\\text{cosine}}(\\vec{q}, \\vec{d}) &amp;= 0.0378\\ /\\ (\\ 0.5280152\\times 0.4859012\\ ) = 0.1473321 \\end{align}\\] The cosine similarity of the two documents is adjusted using TF-IDF. Notice that with TF-IDF, the similarity measurements drop even lower, indicating dissimilarity between the two documents, though human intuition seems there seems to be some relation between the two documents. We can revisit this in Latent Semantic Analysis (LSA). 11.3.4 Linguistic Analysis When it comes to Speech Recognition, we emphasize Linguistics, defined by Oxford language and Google Dictionary to be the scientific study of language and structure, including the study of morphology (language form), Syntax, Phonetics, and Semantics which includes both Lexical structure (language vocabulary for meaning and relationship) and Conceptual structure (language vocabulary for meaning and cognition). Wikipedia also includes Contextual structure, including social, historical, cultural, and other properties that influence the language. We may not be able to cover them all in this book as they deserve their own merits. Instead, the next few sections will touch only on Lexical and Semantic analysis basics. This, in particular, covers Parts of Speach (POS) and Phrases (also called Chunks). Also, we cover additional insight about Speech Recognition in Chapter 13 (Computational Deep Learning II). 11.3.5 Lexical Analysis For NLP, getting the closest insight about the content (or possibly common key themes) of a set of documents is essential. A straightforward way to determine the closest topic is to count the frequency of all terms found in the documents to rank them accordingly. This section extends our discussion on pre-processing of words and TF-IDF starting with POS tagging - the processes of labeling or annotating words with Parts of Speech. POS Tagging There should be more than 15 to 20 different tags, each corresponding to a particular Part of Speech. The following (POS) tags are common (albeit we only show a few tags): NN - Noun NNS - Noun Plural VBD - Verb JJ - Adjective RB - Adverb PRP - Possessive Pronoun To illustrate, we use treetag(.) function from the koRpus library for POS tagging: library(koRpus) library(koRpus.lang.en) library(tm) library(SnowballC) set.kRp.env(TT.cmd=&quot;~/nlp/cmd/tree-tagger-english&quot; , lang=&quot;en&quot;, preset=&quot;en&quot;, treetagger=&quot;kRp.env&quot;, format=&quot;file&quot;, TT.tknz=TRUE, add.desc = FALSE, encoding=&quot;UTF-8&quot;) tagged.text = treetag( &quot;~/nlp/documents.txt&quot; , stopwords = tm::stopwords(&quot;en&quot;), stemmer = SnowballC::wordStem) tagged.text[1:10, 2:10] # Display the 1st 10 lines ## token tag lemma lttr wclass desc stop stem idx ## 1 There EX0 there 5 existential NA TRUE There 1 ## 2 is VBZ be 2 verb NA TRUE i 2 ## 3 a AT0 a 1 article NA TRUE a 3 ## 4 big AJ0 big 3 adjective NA FALSE big 4 ## 5 animal NN1 animal 6 noun NA FALSE anim 5 ## 6 in PRP in 2 preposition NA TRUE in 6 ## 7 my DPS i 2 determiner NA TRUE my 7 ## 8 house NN1 house 5 noun NA FALSE hous 8 ## 9 . SENT . 1 fullstop NA FALSE . 9 ## 10 The AT0 the 3 article NA TRUE The 10 We can then show the mapping between terms (tokens) and tags like so: pos_tag &lt;- function(tokens, tags) { paste0(tokens, &quot;/&quot;, tags) } tokens = tagged.text@tokens$token tags = tagged.text@tokens$tag pos_tag(tokens, tags) ## [1] &quot;There/EX0&quot; &quot;is/VBZ&quot; &quot;a/AT0&quot; &quot;big/AJ0&quot; &quot;animal/NN1&quot; ## [6] &quot;in/PRP&quot; &quot;my/DPS&quot; &quot;house/NN1&quot; &quot;./SENT&quot; &quot;The/AT0&quot; ## [11] &quot;animal/NN1&quot; &quot;has/VBZ&quot; &quot;great/AJ0&quot; &quot;big/AJ0&quot; &quot;ears/NN2&quot; ## [16] &quot;,/PUN&quot; &quot;great/AJ0&quot; &quot;big/AJ0&quot; &quot;eyes/NN2&quot; &quot;,/PUN&quot; ## [21] &quot;great/AJ0&quot; &quot;big/AJ0&quot; &quot;teeth/NN2&quot; &quot;./SENT&quot; &quot;A/AT0&quot; ## [26] &quot;wolf/NN1&quot; &quot;is/VBZ&quot; &quot;an/AT0&quot; &quot;animal/NN1&quot; &quot;./SENT&quot; ## [31] &quot;Little/DT0&quot; &quot;Red/AJ0&quot; &quot;Riding/NP0&quot; &quot;Hood/NP0&quot; &quot;met/VBD&quot; ## [36] &quot;a/AT0&quot; &quot;Wolf/NP0&quot; &quot;./SENT&quot; There are a few notes to make about the output above. First, the document.txt file contains four sentences, as in the case we have previously. Each term in the output above maps to its corresponding sentence. Secondly, note that we used 3rd-party stopwords and stemmers from tm library and SnowballC library, respectively. We also can use our stopwords as an option. Lastly, note that treetagger is a 3rd-party software externally used by koRpus. Annotation Different structures of a text can be annotated. We can annotate Documents, Paragraphs, Sentences, Phrases, Clauses, Words. Only a few will be explained. Moreover, we use a common library called NLP and openNLP. To illustrate, let us use Annotator functions from openNLP and NLP libraries for annotation. At the same time, we show an alternative way to perform POS tagging, and in addition, to handle Chunking. Below, we annotate texts based on sentence structure, word structure, and POS forms. library(NLP) library(openNLP) library(openNLPmodels.en) text = as.String( doc.corpus ) # as.String(doc.corpus) sentence.annotator = Maxent_Sent_Token_Annotator() word.annotator = Maxent_Word_Token_Annotator() pos.annotator = Maxent_POS_Tag_Annotator(probs=TRUE) annotated.doc = annotate(s = text, list(sentence.annotator, word.annotator)) pos_tagged.doc = annotate(s = text, pos.annotator, annotated.doc) Now, let us show a summary of the annotations for the part of speech - the POS tags: head(pos_tagged.doc, n=10) # display only first 10 lines ## id type start end features ## 1 sentence 1 34 constituents=&lt;&lt;integer,9&gt;&gt; ## 2 sentence 36 98 constituents=&lt;&lt;integer,15&gt;&gt; ## 3 sentence 100 119 constituents=&lt;&lt;integer,6&gt;&gt; ## 4 sentence 121 154 constituents=&lt;&lt;integer,8&gt;&gt; ## 5 word 1 5 POS=EX, POS_prob=0.9517 ## 6 word 7 8 POS=VBZ, POS_prob=0.9993 ## 7 word 10 10 POS=DT, POS_prob=0.9909 ## 8 word 12 14 POS=JJ, POS_prob=0.9979 ## 9 word 16 21 POS=NN, POS_prob=0.9799 ## 10 word 23 24 POS=IN, POS_prob=0.9801 To map tokens (terms) to tags, we code the following: text.string = as.String(doc.corpus) words = subset(pos_tagged.doc, type==&quot;word&quot;) tags = sapply(words$features, &#39;[[&#39;,&quot;POS&quot; ) tokens = text.string[subset(pos_tagged.doc, type==&quot;word&quot;)] pos_tag(tokens, tags) ## [1] &quot;There/EX&quot; &quot;is/VBZ&quot; &quot;a/DT&quot; &quot;big/JJ&quot; &quot;animal/NN&quot; ## [6] &quot;in/IN&quot; &quot;my/PRP$&quot; &quot;house/NN&quot; &quot;./.&quot; &quot;The/DT&quot; ## [11] &quot;animal/NN&quot; &quot;has/VBZ&quot; &quot;great/JJ&quot; &quot;big/JJ&quot; &quot;ears/NNS&quot; ## [16] &quot;,/,&quot; &quot;great/JJ&quot; &quot;big/JJ&quot; &quot;eyes/NNS&quot; &quot;,/,&quot; ## [21] &quot;great/JJ&quot; &quot;big/JJ&quot; &quot;teeth/NNS&quot; &quot;./.&quot; &quot;A/DT&quot; ## [26] &quot;wolf/NN&quot; &quot;is/VBZ&quot; &quot;an/DT&quot; &quot;animal/NN&quot; &quot;./.&quot; ## [31] &quot;Little/NNP&quot; &quot;Red/NNP&quot; &quot;Riding/VBG&quot; &quot;Hood/NNP&quot; &quot;met/VBD&quot; ## [36] &quot;a/DT&quot; &quot;Wolf/NNP&quot; &quot;./.&quot; The output shows that the token big maps to the JJ tag, and the token animal maps to NN. Note that the tag used by openNLP, such as JJ for adjectives, is different from the tag used by treetag, e.g., AJ0. There are a few differences to observe likewise. Chunking and Syntactic Parsing Chunking is extracting and parsing texts to group terms into phrases and is often guided by the corresponding language syntax and structure. For example, a clause may represent a combination of adjective and noun tagged as ‘JJ+NN’ in that order. A phrase, on the other hand, is made up of clauses. Let us show a summary of the Chunks: chunk.annotator = Maxent_Chunk_Annotator(probs = TRUE) chunked.doc = annotate(text, chunk.annotator, pos_tagged.doc) head(chunked.doc, n=10) # display only first 10 lines ## id type start end features ## 1 sentence 1 34 constituents=&lt;&lt;integer,9&gt;&gt; ## 2 sentence 36 98 constituents=&lt;&lt;integer,15&gt;&gt; ## 3 sentence 100 119 constituents=&lt;&lt;integer,6&gt;&gt; ## 4 sentence 121 154 constituents=&lt;&lt;integer,8&gt;&gt; ## 5 word 1 5 POS=EX, POS_prob=0.9517, chunk_tag=B-NP, ## chunk_prob=0.9736 ## 6 word 7 8 POS=VBZ, POS_prob=0.9993, chunk_tag=B-VP, ## chunk_prob=0.9909 ## 7 word 10 10 POS=DT, POS_prob=0.9909, chunk_tag=B-NP, ## chunk_prob=0.9962 ## 8 word 12 14 POS=JJ, POS_prob=0.9979, chunk_tag=I-NP, ## chunk_prob=0.9939 ## 9 word 16 21 POS=NN, POS_prob=0.9799, chunk_tag=I-NP, ## chunk_prob=0.9898 ## 10 word 23 24 POS=IN, POS_prob=0.9801, chunk_tag=B-PP, ## chunk_prob=0.992 Note that Maxent_Chunk_Annotator(.) requires a model file for language en. This function can be installed from https://datacube.wu.ac.at/ like so: install.packages(&quot;openNLPmodels.en&quot;, repos = &quot;http://datacube.wu.ac.at/&quot;, type = &quot;source&quot;) We can begin parsing the text using the annotated chunks and create a parsed tree. However, it is worth mentioning that parsing is an expensive operation and may require CPU and memory resources. For that reason, specifically for the parser used in openNLP, we adjust the memory requirement - being so, the parser uses java virtual machine (JVM). From there, we can invoke the annotator. options(java.parameters = &quot;-Xmx4096m&quot;) parse.annotator = Parse_Annotator() Then we can annotate. Here, we used the annotated chunk texts. parsed.doc = parse.annotator(text, chunked.doc) parsed.texts = sapply(parsed.doc$features, `[[`, &quot;parse&quot;) parsed.tree = lapply(parsed.texts , Tree_parse) The parsed tree of the 1st document has the text format below: parsed.tree[[1]] ## (TOP ## (S ## (NP (EX There)) ## (VP ## (VBZ is) ## (NP ## (NP (DT a) (JJ big) (NN animal)) ## (PP (IN in) (NP (PRP$ my) (NN house))))) ## (. .))) The equivalent graph form is shown in Figure 11.9. Figure 11.9: Parsed Tree 1 Similarly, the parsed tree of the 4th document has the text format below: parsed.tree[[4]] ## (TOP ## (S ## (NP (NNP Little) (NNP Red) (VBG Riding) (NNP Hood)) ## (VP (VBD met) (NP (DT a) (NNP Wolf))) ## (. .))) The equivalent graph form is shown in Figure 11.10. Figure 11.10: Parsed Tree 4 11.3.6 Semantic Analysis In a sense, Semantics Analysis reminds us of Clustering, which depends on the similarities of members in a cluster and the grouping of members in their respective clusters. In Semantic Analysis, we are also interested in determining the relationship of members within and between clusters. In the context of NLP, clusters may represent groups of words (possibly the syntactic structures - e.g., parsed tree - as we have shown in the previous section) and their relationship to the documents. Let us start with one of the first popular classic methods called Latent Semantic Analysis (LSA). Note that we do not Semantic Relationships in this book. We leave readers to investigate the types of Semantic Relationships, such as Synonymy, Antonymy, Homonymy, and Metonymy, which may be covered in the study of Linguistics. Such relationships may require knowledge-based inputs as references for training our models. We may present here basic relationship measures in comparing similarities based on weighted words (or chunks) in documents. Latent Semantic Analysis (LSA) LSA is an NLP technique formulated by Susan T. Dumais et al. (1988) for semantic analysis. It is dependent upon terms found in documents. With LSA, our goal is to determine the relationship between documents. To achieve our goal, let us recall a Matrix Decomposition method in Chapter 2 (Numerical Linear Algebra I) called Singular Value Decomposition (SVD). \\[\\begin{align} \\mathbf{M}_{(\\text{n x m})} = \\mathbf{U}_{(\\text{n x p})}\\mathbf{\\Sigma}_{(\\text{p x p})}\\mathbf{V}^{\\text{T}}_{(\\text{p x m})} \\ \\ \\ \\ \\ \\ where\\ \\mathbf{\\Sigma}\\ \\text{is diagonal matrix} \\end{align}\\] The formula can be interpreted based on Figure 11.11. Figure 11.11: Latent Semantic Analysis Previously, we are able to derive the DTM and TDM representations of our relevant terms. Here, both our DTM and TDM contain TF-IDF weights instead of basic Term Frequency. dtm.doc = tfidf = dfm_tfidf(doc.dfm, scheme_tf = &quot;prop&quot;, base=exp(1), force=TRUE) tdm.doc = data.frame(t(as.matrix(dtm.doc))) tdm.doc ## D1 D2 D3 D4 ## big 0.17329 0.20794 0.0000 0.0000 ## animal 0.07192 0.02877 0.1438 0.0000 ## my 0.34657 0.00000 0.0000 0.0000 ## house 0.34657 0.00000 0.0000 0.0000 ## great 0.00000 0.41589 0.0000 0.0000 ## ears 0.00000 0.13863 0.0000 0.0000 ## eyes 0.00000 0.13863 0.0000 0.0000 ## teeth 0.00000 0.13863 0.0000 0.0000 ## wolf 0.00000 0.00000 0.3466 0.1155 ## little 0.00000 0.00000 0.0000 0.2310 ## red 0.00000 0.00000 0.0000 0.2310 ## riding 0.00000 0.00000 0.0000 0.2310 ## hood 0.00000 0.00000 0.0000 0.2310 ## met 0.00000 0.00000 0.0000 0.2310 Following SVD, we should be able to decompose TDM and obtain the Topic-To-Document matrix. (M = svd(tdm.doc)) ## $d ## [1] 0.5604 0.5391 0.4869 0.3598 ## ## $u ## [,1] [,2] [,3] [,4] ## [1,] 0.47681 -0.05787 0.05537 -0.03755 ## [2,] 0.14739 0.05267 -0.07076 0.37164 ## [3,] 0.43794 -0.04288 -0.49816 -0.06325 ## [4,] 0.43794 -0.04288 -0.49816 -0.06325 ## [5,] 0.51567 -0.07286 0.60890 -0.01185 ## [6,] 0.17189 -0.02429 0.20297 -0.00395 ## [7,] 0.17189 -0.02429 0.20297 -0.00395 ## [8,] 0.17189 -0.02429 0.20297 -0.00395 ## [9,] 0.07005 0.36649 -0.01581 0.84684 ## [10,] 0.03971 0.41202 0.01415 -0.16440 ## [11,] 0.03971 0.41202 0.01415 -0.16440 ## [12,] 0.03971 0.41202 0.01415 -0.16440 ## [13,] 0.03971 0.41202 0.01415 -0.16440 ## [14,] 0.03971 0.41202 0.01415 -0.16440 ## ## $v ## [,1] [,2] [,3] [,4] ## [1,] 0.70809 -0.06671 -0.69989 -0.06566 ## [2,] 0.69480 -0.09445 0.71290 -0.01025 ## [3,] 0.08116 0.24965 -0.03216 0.96439 ## [4,] 0.09632 0.96141 0.02982 -0.25599 We reduce the decomposed matrices such that we only consider the first two columns: reduced.dtm = M$u[,1:2 ] %*% diag(M$d[1:2 ]) %*% t(M$v[,1:2 ]) rownames(reduced.dtm) = rownames(tdm.doc) colnames(reduced.dtm) = colnames(tdm.doc) reduced.dtm ## D1 D2 D3 D4 ## big 0.1912700 0.188585 0.013895 -0.004262 ## animal 0.0565857 0.054700 0.013792 0.035253 ## my 0.1753109 0.172691 0.014145 0.001409 ## house 0.1753109 0.172691 0.014145 0.001409 ## great 0.2072291 0.204479 0.013645 -0.009933 ## ears 0.0690764 0.068160 0.004548 -0.003311 ## eyes 0.0690764 0.068160 0.004548 -0.003311 ## teeth 0.0690764 0.068160 0.004548 -0.003311 ## wolf 0.0146147 0.008612 0.052513 0.193743 ## little 0.0009392 -0.005518 0.057261 0.215703 ## red 0.0009392 -0.005518 0.057261 0.215703 ## riding 0.0009392 -0.005518 0.057261 0.215703 ## hood 0.0009392 -0.005518 0.057261 0.215703 ## met 0.0009392 -0.005518 0.057261 0.215703 With the new coefficients reflected in our TDM obtained through SVD, let us compute for cosine similarity to see any effect. Here, we use a 3rd-party library called lsa for our similarity calculation: library(lsa) original.dtm = as.matrix( dfm_weight(doc.dfm) ) c(&quot;original&quot; = lsa::cosine(original.dtm [1,], original.dtm [2,]), &quot;new&quot; = lsa::cosine(reduced.dtm[1,], reduced.dtm[2,])) ## original new ## 0.4264 0.9016 Notice that with LSA, our two documents (D1 and D2) are similar. Hence, there is a 90.16% relationship between the two documents. However, this latent relationship is presented only as a percentage - the interpretation of such a relationship is unknown. Therefore, let us review pLSA to see how the interpretation is considered. Probabilistic Latent Semantic Analysis (pLSA) pLSA is formulated by Thomas Hofmann (1999), and it introduces the idea of an Aspect model (See Figure 11.12 adapted from David M. Blei (2012)). Figure 11.12: pLSA (Aspect Model) The model can also be represented in the form of a Probabilistic Graphical Model (PGM) (see Figure 11.13), particularly formulation 1. Figure 11.13: pLSA (PGM) If we are to follow the PGM figure, we have two random variables denoted by Z and W. The former follows a multinomial distribution denoted by \\(\\theta_m\\), and the latter follows a second multinomial distribution denoted by \\(\\phi_k\\). Hence, we can express them as such: \\[ Z_{mn} \\sim Multinomial(\\theta_m)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ W_{mn} \\sim Multinomial(\\phi_k) \\] Our goal is to be able to generate the topic proportionality (\\(\\theta_m\\) ) per-document and word distribution (\\(\\phi_k\\)) per-topic (see Figure 11.14) based on our given Corpus. Figure 11.14: pLSA (Multinomial Distribution) We first determine the probability of seeing a document and a word appearing together to achieve our goal. We can fashion this using a joint distribution but with two similar formulations: The first formulation allows us to determine the likelihood of seeing a document in which we can also find a word based on the distribution of topics. \\[\\begin{align} P(d, w) = P(d)\\sum_{z \\in Z} P(w|z)P(z|d) \\end{align}\\] The second formulation follows the LSA SVD-based matrix model such that we have the following: \\[\\begin{align} P(d, w) = \\sum_z \\underbrace{P(z)}_{\\mathbf{\\text{U}}} \\underbrace{P(d|z)}_{\\mathbf{\\text{S}}}\\underbrace{P(w|z)}_{\\mathbf{\\text{V}}^T} \\end{align}\\] Determining the probabilities or their corresponding parameters needs to be done heuristically. The most common method uses Expectation-Maximization (EM); albeit, for LDA in the next section, we can use Gibbs Sampling due to the bayesian a-priori hyperparameters introduced. Let us consider EM based on the two formulations for pLSA. See below (Hong L. 2012) - derivation is not included: \\[ \\begin{array}{l} \\mathbf{\\text{Formulation 1:}}\\\\ \\ \\ \\ \\ P(d, w) = P(d)\\sum_z P(w|z)P(z|d)\\\\ \\mathbf{\\text{E-Step:}}\\\\ P(z|w,d) = \\frac{P(w|z)P(z|d)} {\\sum_z P(w|z)P(z|d)}\\\\ \\mathbf{\\text{M-Step:}}\\\\ P(d) = \\frac{n(d)}{\\sum_d n(d)}\\\\ P(w|z) = \\frac{\\sum_d n(d,w) P(z|w,d)} {\\sum_w \\sum_d n(d,w) P(z|w,d)} \\\\ P(z|d) = \\frac{\\sum_w n(d,w)P(z|w,d)}{n(d)} \\\\ \\\\ \\\\ \\end{array} \\left| \\begin{array}{l} \\mathbf{\\text{Formulation 2:}}\\\\ \\ \\ \\ \\ P(d, w) = \\sum_zP(z) P(d|z)P(w|z)\\\\ \\mathbf{\\text{E-Step:}}\\\\ P(z|w,d) = \\frac{P(w,z,d)}{P(w,d)} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ =\\frac{P(w|z)P(d|z)P(z)} {\\sum_z P(w|z)P(d|z)P(z)} \\\\ \\mathbf{\\text{M-Step:}}\\\\ P(z) = \\frac{\\sum_d \\sum_w n(d,w)P(z|w,d)} {\\sum_d \\sum_w n(d,w)}\\\\ P(w|z) = \\frac{\\sum_d n(d,w)P(z|w,d)} {\\sum_w \\sum_d \\sum_w n(d,w)}\\\\ P(d|z) = \\frac{\\sum_w n(d,w)P(z|w,d)} {\\sum_d \\sum_w \\sum_w n(d,w)}\\\\ \\end{array} \\right. \\] We perform iterative calculations and updates to maximize the log-likelihood. For the first formulation, we use the following log-likelihood: \\[\\begin{align} \\mathcal{L}_1 = \\sum_d \\sum_w n(d, w) \\log_e \\left[P(d) \\sum_z P(w|z) P(z|d)\\right] \\end{align}\\] For the second formulation, we use the following log-likelihood: \\[\\begin{align} \\mathcal{L}_2 = \\sum_d \\sum_w n(d, w) \\log_e \\left[ \\sum_zP(z) P(d|z)P(w|z)\\right] \\end{align}\\] Instead of giving an example to illustrate pLSA, let us rather extend our discussion by introducing Latent Dirichlet Allocation (LDA) which is an enhanced variant of pLSA in that it uses Naive Bayes for which both \\(\\theta_m\\) and \\(\\phi_k\\) follow a Dirichlet distribution using two corresponding Prior hyperparameters, namely alpha (\\(\\alpha\\)) and beta (\\(\\beta\\)). Latent Dirichlet Allocation (LDA) LDA is a popular algorithm commonly used in topic modeling formulated by David M. Blei, Andrew Y. Ng, and Michael I. Jordan (2003). As with pLSA, the grand idea is to classify a bag of words gathered from a collection of documents called corpus into a set of arbitrarily chosen topics. Let us first review the PGM for LDA (see Figure 11.15). Figure 11.15: LDA (PGM) The two Dirichlet parameters are used to provide the initial multinomial distributions, e.g. sampling a Dirichlet distribution as priors for \\(\\theta_m\\) and \\(\\phi_k\\). \\[\\begin{align} \\theta_m \\sim Dir(\\alpha)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\phi_k \\sim Dir(\\beta) \\end{align}\\] To illustrate, let us use icml-nips-iclr-dataset dataset sourced from github/cqql - a note that all contents of the datasets comes from ICML, NIPS, and ICLR 2016-2018. For our dataset, we are only interested in the title for papers published in 2018. Assume for a moment that each title represents one document. options(width=56) icml.nips.iclr = read.csv(file = &quot;~/Documents/nlp/papers.csv&quot;, stringsAsFactors = FALSE) yr.idx = which(icml.nips.iclr[,c(&quot;Year&quot;)] == &quot;2018&quot;) doc.titles = unique(icml.nips.iclr[yr.idx,c(&quot;Title&quot;)]) strwrap(head(doc.titles), width=70, exdent=5) ## [1] &quot;Spline Filters For End-to-End Deep Learning&quot; ## [2] &quot;Non-linear motor control by local learning in spiking neural networks&quot; ## [3] &quot;Implicit Quantile Networks for Distributional Reinforcement Learning&quot; ## [4] &quot;An Inference-Based Policy Gradient Method for Learning Options&quot; ## [5] &quot;Predict and Constrain: Modeling Cardinality in Deep Structured&quot; ## [6] &quot; Prediction&quot; ## [7] &quot;Differentially Private Matrix Completion Revisited&quot; In this illustration, let us use tm to perform the pre-processing, ignoring stemming for a moment. library(tm) no.docs = length(doc.titles) doc_ids = paste0(&quot;D&quot;, seq(1, no.docs)) doc.dfm = data.frame( doc_id = doc_ids, text = doc.titles) doc.dfm.source = DataframeSource(doc.dfm) doc.corpus = Corpus(doc.dfm.source) preprocessed.doc = tm_map(doc.corpus, content_transformer(tolower)) preprocessed.doc = tm_map(preprocessed.doc, removePunctuation, preserve_intra_word_dashes = TRUE) preprocessed.doc = tm_map(preprocessed.doc, removeNumbers) preprocessed.doc = tm_map(preprocessed.doc, removeWords, stopwords(&quot;english&quot;)) #preprocessed.doc = tm_map(preprocessed.doc, stemDocument, language = &quot;en&quot;) preprocessed.doc = tm_map(preprocessed.doc, stripWhitespace) preprocessed.doc ## &lt;&lt;SimpleCorpus&gt;&gt; ## Metadata: corpus specific: 1, document level (indexed): 0 ## Content: documents: 1966 Now, let us introduce the use of the topicmodels package to help us with LDA. In this exercise, we limit our solution to only four topics over words with a minimum frequency of 1. From there, we cast the pre-processed document into DTM format. library(topicmodels) min.Freq = 1; K = 4 DTM = DocumentTermMatrix(preprocessed.doc, control = list(bounds = list(global = c(min.Freq, Inf)))) We can use a couple of methods to meet our goals, as pointed out in the pLSA section. We can use Variational EM (VEM) or Gibbs Sampling. For VEM, recall Variational Inference in Chapter 7 (Bayesian Computation I) under Bayesian Inference Section. Using LDA(.) function, we issue a set of controls, including tolerance levels for convergence and maximum iteration, with the seed being set. tol = 1e-3 em.control = var.control = list(iter.max = 500, tol = tol) lda.vem.control = list( nstart = 1, seed = 2018, verbose=1, var = var.control, em = em.control, initialize = &quot;random&quot;) topic.model = LDA(DTM, K, method=&quot;VEM&quot;, control=lda.vem.control) ## **** em iteration 1 **** ## document 1966 ## new alpha = 13.80105 ## **** em iteration 2 **** ## document 1966 ## new alpha = 15.10332 ## **** em iteration 3 **** ## document 1966 ## new alpha = 16.39318 ## final e step document 1966 Let us output the list of latent topics generated using the VEM method, restricting to only the top 15 words per topic. (topic.matrix = terms(topic.model, 15)) ## Topic 1 Topic 2 Topic 3 ## [1,] &quot;learning&quot; &quot;learning&quot; &quot;learning&quot; ## [2,] &quot;neural&quot; &quot;deep&quot; &quot;neural&quot; ## [3,] &quot;optimization&quot; &quot;networks&quot; &quot;deep&quot; ## [4,] &quot;data&quot; &quot;optimization&quot; &quot;variational&quot; ## [5,] &quot;stochastic&quot; &quot;generative&quot; &quot;adversarial&quot; ## [6,] &quot;efficient&quot; &quot;reinforcement&quot; &quot;networks&quot; ## [7,] &quot;networks&quot; &quot;via&quot; &quot;models&quot; ## [8,] &quot;inference&quot; &quot;adversarial&quot; &quot;recurrent&quot; ## [9,] &quot;network&quot; &quot;bayesian&quot; &quot;via&quot; ## [10,] &quot;via&quot; &quot;training&quot; &quot;reinforcement&quot; ## [11,] &quot;descent&quot; &quot;models&quot; &quot;training&quot; ## [12,] &quot;prediction&quot; &quot;stochastic&quot; &quot;sparse&quot; ## [13,] &quot;unsupervised&quot; &quot;inference&quot; &quot;generative&quot; ## [14,] &quot;models&quot; &quot;model&quot; &quot;linear&quot; ## [15,] &quot;machine&quot; &quot;gradient&quot; &quot;using&quot; ## Topic 4 ## [1,] &quot;networks&quot; ## [2,] &quot;neural&quot; ## [3,] &quot;using&quot; ## [4,] &quot;gradient&quot; ## [5,] &quot;models&quot; ## [6,] &quot;via&quot; ## [7,] &quot;learning&quot; ## [8,] &quot;reinforcement&quot; ## [9,] &quot;adversarial&quot; ## [10,] &quot;data&quot; ## [11,] &quot;model&quot; ## [12,] &quot;estimation&quot; ## [13,] &quot;bayesian&quot; ## [14,] &quot;deep&quot; ## [15,] &quot;fast&quot; For the Gibbs Sampling method, recall the Simulation and Sampling methods in Chapter 8 (Bayesian Computation II). Using LDA(.), we introduce Burn-In and Thinning. Note that we use Gibbs sampling with 600 sampling iterations - including Burn-in - but for simplicity, we omit Thinning for now. library(topicmodels) lda.gibbs.control = list( burnin = 100, iter=500, nstart=1, verbose=100, seed=2018) topic.model = LDA(DTM, K, method=&quot;Gibbs&quot;, control=lda.gibbs.control) ## K = 4; V = 3152; M = 1966 ## Sampling 600 iterations! ## Iteration 100 ... ## Iteration 200 ... ## Iteration 300 ... ## Iteration 400 ... ## Iteration 500 ... ## Iteration 600 ... ## Gibbs sampling completed! Let us output the list of latent topics generated using the Gibbs Sampling method, restricting to only the top 15 words per topic. (topic.matrix = terms(topic.model, 15)) ## Topic 1 Topic 2 Topic 3 ## [1,] &quot;networks&quot; &quot;models&quot; &quot;stochastic&quot; ## [2,] &quot;neural&quot; &quot;adversarial&quot; &quot;via&quot; ## [3,] &quot;optimization&quot; &quot;inference&quot; &quot;using&quot; ## [4,] &quot;bayesian&quot; &quot;generative&quot; &quot;data&quot; ## [5,] &quot;training&quot; &quot;variational&quot; &quot;gradient&quot; ## [6,] &quot;network&quot; &quot;descent&quot; &quot;linear&quot; ## [7,] &quot;convolutional&quot; &quot;gaussian&quot; &quot;distributed&quot; ## [8,] &quot;recurrent&quot; &quot;processes&quot; &quot;sparse&quot; ## [9,] &quot;gradient&quot; &quot;machine&quot; &quot;graph&quot; ## [10,] &quot;policy&quot; &quot;learning&quot; &quot;algorithms&quot; ## [11,] &quot;adaptive&quot; &quot;synthesis&quot; &quot;efficient&quot; ## [12,] &quot;matrix&quot; &quot;information&quot; &quot;estimation&quot; ## [13,] &quot;efficient&quot; &quot;clustering&quot; &quot;unsupervised&quot; ## [14,] &quot;representation&quot; &quot;local&quot; &quot;image&quot; ## [15,] &quot;classification&quot; &quot;search&quot; &quot;approach&quot; ## Topic 4 ## [1,] &quot;learning&quot; ## [2,] &quot;deep&quot; ## [3,] &quot;reinforcement&quot; ## [4,] &quot;model&quot; ## [5,] &quot;structured&quot; ## [6,] &quot;via&quot; ## [7,] &quot;sampling&quot; ## [8,] &quot;transfer&quot; ## [9,] &quot;representations&quot; ## [10,] &quot;graphs&quot; ## [11,] &quot;knowledge&quot; ## [12,] &quot;language&quot; ## [13,] &quot;method&quot; ## [14,] &quot;selection&quot; ## [15,] &quot;active&quot; Note that we leave the choice of methods to the readers. The success of topic modeling comes with proper pre-processing of terms and proper tune-up of each method, among many other considerations. For the computed per-document topic proportion, let us display only the first ten documents and their corresponding topic distribution. Notice that the summation of topic proportion per document is always 1. doc.topic = posterior(topic.model)$topics[1:10,] colnames(doc.topic) = paste0(&quot;T&quot;, seq(1, K)) doc.topic ## T1 T2 T3 T4 ## D1 0.2273 0.2818 0.2273 0.2636 ## D2 0.2500 0.2500 0.2500 0.2500 ## D3 0.2768 0.2232 0.2232 0.2768 ## D4 0.2232 0.2232 0.2411 0.3125 ## D5 0.2368 0.2193 0.2368 0.3070 ## D6 0.2455 0.2818 0.2455 0.2273 ## D7 0.3070 0.2368 0.2368 0.2193 ## D8 0.2547 0.2547 0.2358 0.2547 ## D9 0.2368 0.2368 0.2368 0.2895 ## D10 0.2845 0.2328 0.2328 0.2500 For the computed per-topic word distribution, let us display only three topics and their corresponding word distribution. topic.terms = posterior(topic.model)$terms ttd = NULL; most = 250; topic.term.distribution = list() for (k in 1:(K-1)) { topic.term.distribution[[k]] = sort(topic.terms[k, ], decreasing=TRUE) topics = as.data.frame(topic.term.distribution[[k]][1:most], stringsAsFactors=FALSE) t.names = rownames(topics) p = cbind( t.names, round(topics,3)) colnames(p) = c(paste0(&quot;T&quot;, k), paste0(&quot;Probs&quot;, k)) ttd = cbind(ttd, as.matrix(p)) } rownames(ttd) = NULL # topic.term.distribution, display 1st 15 rows (ttd = as.data.frame(ttd, stringsAsFactors=FALSE))[1:15,] ## T1 Probs1 T2 Probs2 ## 1 networks 0.073 models 0.035 ## 2 neural 0.067 adversarial 0.027 ## 3 optimization 0.032 inference 0.020 ## 4 bayesian 0.020 generative 0.020 ## 5 training 0.018 variational 0.018 ## 6 network 0.016 descent 0.013 ## 7 convolutional 0.013 gaussian 0.010 ## 8 recurrent 0.012 processes 0.009 ## 9 gradient 0.012 machine 0.008 ## 10 policy 0.010 learning 0.008 ## 11 adaptive 0.009 synthesis 0.007 ## 12 matrix 0.009 information 0.007 ## 13 efficient 0.009 clustering 0.007 ## 14 representation 0.007 local 0.007 ## 15 classification 0.007 search 0.007 ## T3 Probs3 ## 1 stochastic 0.025 ## 2 via 0.024 ## 3 using 0.024 ## 4 data 0.021 ## 5 gradient 0.014 ## 6 linear 0.013 ## 7 distributed 0.013 ## 8 sparse 0.012 ## 9 graph 0.012 ## 10 algorithms 0.012 ## 11 efficient 0.010 ## 12 estimation 0.010 ## 13 unsupervised 0.009 ## 14 image 0.009 ## 15 approach 0.009 Let us display wordcloud for the top terms in the first topic (See Figure 11.16). library(wordcloud) # generates wordcloud. library(RColorBrewer) # renders colored texts. set.seed(2020) terms = ttd[,c(&quot;T1&quot;)]; probabilities = as.numeric(ttd[,c(&quot;Probs1&quot;)]) wordcloud(words = terms, freq = probabilities, max.words=150 , random.order=FALSE, rot.per = 0.35, min.freq = 1, colors=brewer.pal(8, &quot;Dark2&quot;)) Figure 11.16: Topic Model (WordCloud) We can readily assume that the first topic suggests a context around neural networks. There are many other considerations when dealing with Topic Modeling. Such considerations cannot all be discussed in one book. However, one such important consideration is choosing a suitable number (K) of topics. There are methods proposed, and the LDA(.) function offers a select number of metrics. We leave readers to experiment on the FindtopicsNumber(.) function from the ldatuning R package for the metrics named after Griffiths 2004, CaoJuan 2009, Arun 2010, and Deveaud 2014. We also encourage exploring the Rate of Perplexity Change (RPC), which helps to measure and cross-validate a suitable number (K) of topics to use. Explicit Semantic Analysis (ESA) Another Semantic Analysis method proposed is Explicit Semantic Analysis (ESA). This method may be regarded as an enhancement to the concept of LSA and LDA in that, instead of topics being latent, it introduces the use of topics that are explicitly pre-determined. In particular, topics are generated from Wikipedia or other sources, e.g. from Encyclopedia. We leave readers to investigate ESA. 11.3.7 Named Entity Recognition (NER) Entity identification or extraction is an essential component of NLP and is a sub-part of text extraction. It helps to recognize Named Entities to add more context to topic modeling. A Named Entity is a real-world proper name given to objects, people, places, dates, events, organizations, and currency. For example, there is only one Mount Everest in the world. Someone’s street address is a named entity. Neil Armstrong is the first man on the moon. Bitcoin is a digital currency. Lastly, Mcdonald’s and Coca-Cola are two world-famous names. Additionally, Named Entities can be categorized accordingly. For example, January 1st is categorized as Date. Likewise, New Year’s Eve is categorized as one of the dates/times. NER becomes apparent for applications around Customer Support, Human Resource, Health Care, and many others. 11.3.8 Sentiment and Opinion Analysis Similar to NER, reading and recognizing sentiments and not just named entities is just as important. It helps to map text to sentiments to interpret and predict sentiments. For this, one of the common Lexicon resources that maps terms to sentiments is called SentiWordNet. There are other resources such as WordNet-Affect, MPQA, and SenticNet to compare (Musto C., Semeraro G., Polignano M., n.d.). Each of the different resources addresses the following sentimental representations: Common Polarity - texts are represented as Positive, Negative, or Neutral. For example, the word good is positive, bad is negative, and ok may be interpreted as Neutral. Intensitiy - this adds granularity in the form of Strong, Medium, Weak or in the form positive and negative scores. For example very good instead of just good. Subjectivity - texts are represented based on human senses, emotions, or opinions. For example, the word happy has a higher positive score, and sad has a lower negative score. A text that says this art looks good has a higher positive score than one that says this art looks terrible. Objectivity - texts are represented based on facts. For example, Earth is round. To illustrate, let us use a 3rd-party library called sentimentr developed by Tyler Rinker that uses SentiWordNet. We feed a sentence to the function sentiment_by(.) that suggests positive sentiment, and we get the following positive score: library(sentimentr) sentiment(&#39;I am very good&#39;) ## element_id sentence_id word_count sentiment ## 1: 1 1 4 0.675 We then feed a sentence that suggests negative sentiment, and we get a positive negative score. sentiment(&#39;I am not very good&#39;) ## element_id sentence_id word_count sentiment ## 1: 1 1 5 -0.06708 Let us feed random positive words and see how the score looks like: terms = c(&quot;happy&quot;, &quot;care&quot;, &quot;great&quot;, &quot;light&quot;, &quot;cheerful&quot; ) sentiment(terms) ## element_id sentence_id word_count sentiment ## 1: 1 1 1 0.75 ## 2: 2 1 1 1.00 ## 3: 3 1 1 0.50 ## 4: 4 1 1 0.00 ## 5: 5 1 1 0.75 Now, let us try with random negative words: terms = c(&quot;sad&quot;, &quot;careless&quot;, &quot;worse&quot;, &quot;dark&quot;) sentiment(terms) ## element_id sentence_id word_count sentiment ## 1: 1 1 1 -0.50 ## 2: 2 1 1 -0.50 ## 3: 3 1 1 -0.75 ## 4: 4 1 1 -0.60 In terms of application, we may find sentiment analysis to be useful as part of a Recommender System, which we discuss further in a section. 11.4 Time-Series Forecasting We now switch context to discuss Time-Series Forecasting - an area that is also equally important. In this context, a probable typical case here is forecasting sales. Note that most - if not all - of our previous discussions underscored static data - a snapshot of an incident. Here, we discuss data that carry changes over time - in a Time-Series fashion. Our goal is to identify patterns (e.g., Trend, Seasonality, among others) that may allow us to forecast accurately. As we tackle other concepts, it is good to be familiar with terms used in Forecasting. Note that when plotting our data, some terms apply to the Time-Series behavior along the Y-axis, e.g. trend, drift, and stationary terms; while the X-axis reflects fix time interval. Period - refers to the duration of one full cycle. Frequency - refers to the number of complete cycles. Time Lag - refers to the elapsed time between two related events. Trend - A series with a trend that follows an increasing or decreasing pattern. The trajectory remains constant, meaning there is no shift in the intercept. Drift - A series in which the trajectory shifts (affecting the intercept). Seasonality - A series has Seasonality if it follows a pattern that repeats, such as weekly, monthly, quarterly, and yearly. Stationary - A stationary series follows a pattern in which the mean (\\(\\mu\\)) and variance (\\(\\sigma^2\\)) along the Y axis are constant over time. This property may be regarded as strongly stationary. If there is an observed change in variance (\\(\\sigma^2\\)) over time while the mean (\\(\\mu\\)) remains constant, this may be regarded as weakly stationary. In the context of seasonality and trend, let us describe stationary to mean a series with no trend nor seasonality patterns. Spike / Peak - follows sudden jumps - almost treated as outliers. Differencing - this method transforms a series from non-stationary to stationary. It also means removing trend and seasonality patterns. Correlation - this is a method of correlating a series between differing time lags. It can serve to identify Stationary vs non-stationary series. If the outcome suggests a non-stationary series, then the series may have a trend, a pattern of Seasonality, or even both. With such terms being described, a good starting point is to construct our data beginning with the idea that our data comes from some random sampling - by that, we mean data with noise. set.seed(2021) # simulate seasonality - 3 cycles (3 years) (x = runif(n=12 * 3, min=0, max=1)) ## [1] 0.45127 0.78378 0.70968 0.38174 0.63632 0.70135 ## [7] 0.64044 0.26668 0.81542 0.98299 0.02727 0.83749 ## [13] 0.60324 0.56745 0.82005 0.25157 0.50549 0.86754 ## [19] 0.95818 0.54570 0.13958 0.95534 0.39249 0.26849 ## [25] 0.57221 0.91214 0.93429 0.88049 0.94569 0.81499 ## [31] 0.03278 0.94271 0.94774 0.90209 0.55227 0.22489 To simulate the idea of running total, let us use the function cumsum(.) to generate our cumulative sum. (x = cumsum(x)) ## [1] 0.4513 1.2350 1.9447 2.3265 2.9628 3.6641 ## [7] 4.3046 4.5713 5.3867 6.3697 6.3969 7.2344 ## [13] 7.8377 8.4051 9.2252 9.4767 9.9822 10.8498 ## [19] 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096 ## [25] 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694 ## [31] 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718 Here, we need to use 3rd-party libraries as listed below to be able to illustrate the idea: library(lubridate) library(tseries) library(forecast) library(fpp) library(caret) Our next step is to convert our Cumulative sum into a Time-Series object using the st(.) function, specifying the total frequency that leads to the accumulation. (TS = ts(x, frequency = 12, start = c(2018, 1))) ## Jan Feb Mar Apr May Jun ## 2018 0.4513 1.2350 1.9447 2.3265 2.9628 3.6641 ## 2019 7.8377 8.4051 9.2252 9.4767 9.9822 10.8498 ## 2020 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694 ## Jul Aug Sep Oct Nov Dec ## 2018 4.3046 4.5713 5.3867 6.3697 6.3969 7.2344 ## 2019 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096 ## 2020 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718 There are two considerations when dealing with Time-Series data, among many others. One consideration is to be able to fill the gap when there is missing data. Another consideration is to perform smoothing. When it comes to missing data, recall our discussion on Missingness and Imputation covered under Exploratory Data Analysis section in Chapter 9 (Computational Learning I). We also discuss Kalman Filter under the Bayesian Models section in Chapter 8 (Bayesian Computation II). We leave readers to review the mentioned sections. For smoothing, e.g., interpolation, the next section introduces the use of STL. 11.4.1 Seasonal Trend Decomposition using LOESS (STL) We now look into smoothing methods using STL. Given the Time-Series object we generated in the previous section, our goal is to see trends and seasonality, including performing differencing to see the delta (or lag) between periods. To illustrate, let us use stl(.) function to plot not just trend and season, but also including the remainder (see Figure 11.17). stl(TS, &quot;periodic&quot;) %&gt;% autoplot() Figure 11.17: Time-Series Plot Notice in the figure that our data shows a constantly increasing trend. The seasonal trend also shows an increase per season — ignore the drops for a moment. If this were a sales trend and we were an investor, we would like what we are seeing. Let us change our random data by introducing a range between -2 and 1. Now, let us review the plot (see Figure 11.18). options(width=70) set.seed(2021) x = cumsum(runif(n=12 * 3, min=-2, max=1)) (TS = ts(x, frequency = 12)) ## Jan Feb Mar Apr May Jun Jul Aug ## 1 -0.6462 -0.2949 -0.1658 -1.0206 -1.1116 -1.0076 -1.0863 -2.2862 ## 2 -2.4870 -2.7846 -2.3245 -3.5698 -4.0533 -3.4507 -2.5761 -2.9390 ## 3 -5.9547 -5.2183 -4.4154 -3.7739 -2.9369 -2.4919 -4.3936 -3.5654 ## Sep Oct Nov Dec ## 1 -1.8399 -0.8910 -2.8092 -2.2967 ## 2 -4.5203 -3.6543 -4.4768 -5.6713 ## 3 -2.7222 -2.0159 -2.3591 -3.6845 stl(TS, &quot;periodic&quot;) %&gt;% autoplot() Figure 11.18: Time-Series Plot By introducing a higher probability of generating negative values, we see a decreasing trend (e.g., more loss in sales). We may not prefer this sales trend. Finally, let us introduce a more significant gap in range and review (see Figure 11.19). set.seed(2021) x = cumsum(runif(n=12 * 3, min=-10, max=10)) (TS = ts(x, frequency = 12)) ## Jan Feb Mar Apr May Jun Jul ## 1 -0.9747 4.7009 8.8946 6.5295 9.2559 13.2829 16.0916 ## 2 26.7534 28.1024 34.5035 29.5349 29.6448 36.9956 46.1592 ## 3 43.6353 51.8781 60.5640 68.1737 77.0876 83.3874 74.0429 ## Aug Sep Oct Nov Dec ## 1 11.4252 17.7337 27.3934 17.9388 24.6886 ## 2 47.0732 39.8648 48.9716 46.8214 42.1911 ## 3 82.8971 91.8519 99.8937 100.9390 95.4367 stl(TS, &quot;periodic&quot;) %&gt;% autoplot() Figure 11.19: Time-Series Plot While the trend may look similar to the first plot, notice that the remainder section of our plot shows some more significant swings or fluctuations. The three plots we reviewed all come from results using the stl(.) function. STL stands for Seasonal Trend Decomposition using LOESS. The idea behind STL is to be able to decompose our raw data using smoothing methods and translate into four meaningful insights, accounting for rolling total, trend, seasonality and remainder. Also, recall our discussion on Scatterplot Smoothing, introducing LOESS for smoothing, in Chapter 3 (Numerical Linear Algebra II). 11.4.2 Forecasting Models Let us cover a few simple classic Forecasting methods we can use, namely: Naïve Forecasting - this is the simplest forecasting in which the forecast is based on the previous (or final) value. For example, based on the recent data in just the previous section, the final sales for Dec 2020 is 95.4367. This value becomes the forecast for the next value for Jan of 2021. Seasonal Naïve Forecasting - this method follows the same as Naïve Forecasting; however, the forecast for Dec 2021 is based on the value taken from the same previous season. Therefore, the forecast for Jan 2021 will follow based on the season value from Jan 2020, which is 43.6353. Linear Trend Projection - this method follows the Least-Squares model in which a straight trend-line stretches in increasing or decreasing fashion fitting through data. The forecast follows the next value that falls on the line. See Least-Squares discussion in Chapter 3 (Numerical Linear Algebra II). Moving Average - this method is mainly used in trading. There are three types of moving average: Simple Moving Average (SMA) - this provides the average of N-periods: \\(SMA = \\frac{1}{N}\\sum_{i=1}^N A_i\\) where A is the average for a period. Weighted Moving Average(WMA) - this provides the average based on the formula: \\(WMA = \\frac{1}{N}\\sum_{i=1}^N A_i \\times W_i\\) where W is a given (or arbitrary) weighting factor. Exponential Moving Average (EMA) - this provides the average based on the formula: \\(\\text{EMA}^{T+1} = (\\text{P} - \\text{EMA}^T) \\times \\left(\\frac{2}{N + 1}\\right) + \\text{EMA}^T\\) where P is previous EMA and P is today’s price. The next few sections cover a couple of advanced methods dealing with Time-Series forecasting using our generated Time-Series object. 11.4.3 Time-Series Linear Model (TSLM) With the given Time-Series object generated using stl(.), let us model our forecast using linear regression. Note here that the tslm(.) function is equivalent to lm(.) for linear regression modeling in which we fit a line. Here, fitting a line considers trends and seasonality in a Time-Series fashion. Moreover, we use the forecast(.) function to forecast, determining future values. There are four ways to fit a model to our data and forecast based on the model. Fitting TS Model with the linear trend only h = 3 # forecast 3 periods ahead in the horizon tslm.model = tslm(TS ~ trend) (forecasts1 = forecast::forecast(tslm.model, h=h)) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 4 93.13 82.69 103.6 76.89 109.4 ## Feb 4 95.85 85.36 106.3 79.54 112.2 ## Mar 4 98.57 88.03 109.1 82.18 115.0 TS Model with seasonality only tslm.model = tslm(TS ~ season) (forecasts2 = forecast::forecast(tslm.model, h=h)) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 4 23.14 -27.46 73.74 -56.11 102.4 ## Feb 4 28.23 -22.37 78.83 -51.02 107.5 ## Mar 4 34.65 -15.95 85.25 -44.59 113.9 TS Linear Model with both linear trend and seasonality tslm.model = tslm(TS ~ trend + season) (forecasts3 = forecast::forecast(tslm.model, h=h)) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 4 87.54 73.85 101.2 66.08 109.0 ## Feb 4 92.63 78.94 106.3 71.17 114.1 ## Mar 4 99.06 85.37 112.7 77.60 120.5 TS Linear Model using fourier series, where K = max. order of fourier terms. tslm.model = tslm(TS ~ forecast::fourier(TS, K=4)) (forecasts4 = forecast::forecast(tslm.model, data.frame(forecast::fourier(TS, K=4, h=h)))) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## Jan 4 27.10 -19.16 73.37 -45.16 99.36 ## Feb 4 25.46 -20.80 71.73 -46.80 97.72 ## Mar 4 35.94 -10.32 82.21 -36.32 108.20 We then plot our forecast corresponding to each model (see Figures and ). library(patchwork) p1 = autoplot(forecasts1, ylab=&quot;Forecast&quot;, main=&quot;Trend&quot;) p2 = autoplot(forecasts2, ylab=&quot;Forecast&quot;, main=&quot;Season&quot;) p1 / p2 # display two plots vertically Figure 11.20: Forecast (using STLM) p3 = autoplot(forecasts3, ylab=&quot;Forecast&quot;, main=&quot;Trend + Season&quot;) p4 = autoplot(forecasts4, ylab=&quot;Forecast&quot;, main=&quot;Fourier(K=4)&quot;) p3 / p4 # display two plots vertically Figure 11.21: Forecast (using STLM) All the plots show our forecast around the fourth period, estimated with confidence levels. 11.4.4 AutoRegressive Integrated Moving Average (ARIMA) ARIMA is also called the Box-Jenkins method formulated by George Box and Gwilym Jenkins in 1976, which allows us to model stationary time-series. ARIMA uses three parameters: p is non-seasonal parameter for AR order d is non-seasonal parameter for differencing (de-trending) q is non-seasonal parameter for MA order Below are ARIMA model examples that can represent different time series: \\[\\begin{align} ARIMA(1, 0, 0) &amp;\\rightarrow &amp;Y_t = \\beta_1 Y_{t-1} + \\mathcal{E}_t\\\\ ARIMA(2, 0, 0) &amp;\\rightarrow &amp;Y_t = \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\mathcal{E}_t\\\\ ARIMA(1, 1, 0) &amp;\\rightarrow &amp;Y_t = \\beta_1 \\Delta Y_{t-1} + \\mathcal{E}_t \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\Delta Y_{t-1} = Y_t - Y_{t-1}\\\\ ARIMA(1, 1, 2) &amp;\\rightarrow &amp;Y_t = \\beta_1 \\Delta Y_{t-1} + \\theta_1 \\mathcal{E}_{t-1} + \\theta_2 \\mathcal{E}_{t-2} + \\mathcal{E}_t \\end{align}\\] To understand ARIMA, we need to break it down into three methods, namely Autoregression (AR), Integration (I), and Moving Average (MA). First, the Autoregression (AR) method allows us to forecast (or predict) the future in reference to the past. Our goal is to achieve a future state that regresses the previous state. We can model that statement based on the following linear equation with an assumed intercept in the form of a mean (\\(\\mu\\)) and a white noise denoted by \\(\\mathcal{E}_t\\): \\[\\begin{align} AR(p) = \\mu + \\underbrace{\\left(\\sum_{i=1}^p \\beta_i y_{t-i}\\right)}_\\text{AR(p)} + \\mathcal{E}_t = \\mu + \\beta_1y_{t-1} + \\beta_2y_{t-2} \\ + ... +\\ \\beta_p y_{t-p} + \\mathcal{E}_t \\end{align}\\] where \\(y_{t - p}\\) is the state value at (t-p) time-period (or the lag) in the past and the mean (\\(\\mu\\)) is the overall mean. The parameter p is the order of terms to use. To illustrate, let us use first-order using the following equation: \\[\\begin{align} AR(p=1) = \\mu + \\beta_1 y_{t-1} + \\mathcal{E}_t \\end{align}\\] Here, for a simple example, assume that our previous state corresponds to a value of 25. Assume that to be more conservative, we weight that value by using a multiplying factor of 0.5 — the \\(\\beta_1\\) coefficient for the first-order term. In addition, the overall average value is 20, which is our starting point or baseline. \\[ y_{t - 1} = 25\\ \\ \\ \\ \\ \\ \\ \\beta_1 = 0.5\\ \\ \\ \\ \\ \\ \\mu = 20 \\] Therefore, our predicted value \\(\\hat{S}\\) becomes: \\[\\begin{align} \\hat{S}(1) = y_t &amp;= \\mu + \\beta_1 y_{t-1} \\\\ &amp;=20 + 0.5\\times 25 \\nonumber \\\\ &amp;=32.5 \\nonumber \\end{align}\\] If only we know the actual state at the period (t), then we should be able to determine the amount of hidden information (whether white noise or any confounding factors) not covered by our prediction. For example, assume that the error is \\(\\mathcal{E}_t = 2\\), then the actual state value computes to about the following: \\[\\begin{align} S(1) = y_t &amp;= \\mu + \\beta_1 y_{t-1} + \\mathcal{E}_t\\\\ &amp;=20 + 0.5\\times 25 + 2 \\nonumber\\\\ &amp;=34.5 \\nonumber \\end{align}\\] In this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as AR(1), meaning that our autoregression model is based on the first-order term. Moreover, if we have to consider two periods (or lags) in the past, we can denote that as AR(2) so that the second-order formula then becomes: \\[\\begin{align} AR(p=2) = = \\mu + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} + \\mathcal{E}_t \\end{align}\\] We can determine the proper order (p) to use for AR(p) by using PACF, which will be covered later. Second, the Moving Average (MA) method allows us to deal with previous state errors instead of state values, as covered in our first step above for AR(p). Our MA equation then looks like so: \\[\\begin{align} MA(q) = \\mu + \\underbrace{\\left(\\sum_{i=1}^q \\theta _i\\mathcal{E}_{t-i}\\right)}_\\text{MA(q)} + \\mathcal{E}_t = \\mu + \\mathcal{E}_t + \\theta_1 \\mathcal{E}_{t-1} + \\theta_2 \\mathcal{E}_{t-2}\\ + ... +\\ \\theta_q \\mathcal{E}_{t-q} \\end{align}\\] where \\(\\mathcal{E}_{t - q}\\) is the error at (t-q) time-period (or the lag) in the past, and the mean (\\(\\mu\\)) is the overall mean. The parameter q is the order of terms to use. To illustrate, let us use first-order using the following equation: \\[\\begin{align} MA(q = 1) = \\mu + \\theta_1 \\mathcal{E}_{t-1} \\end{align}\\] Here, we continue to use our previous example in which we have a previous state corresponding to a value of 25. However, given that our overall average value is 20, we know that there is a 5-point error (or difference) in our previous state. Moreover, we still assume that to be more conservative, we weigh our error (not the state value) by using a multiplying factor of 2.5. \\[ y_{t - 1} = 25\\ \\ \\ \\ \\ \\ \\ \\theta_1 = 2.5\\ \\ \\ \\ \\ \\ \\mu= 20\\ \\ \\ \\ \\ \\ \\ \\ \\mathcal{E}_{t-1} = 5 \\] Therefore, our predicted value \\(\\hat{S}\\) becomes: \\[\\begin{align} \\hat{S}(1) = y_t &amp;= \\mu + \\theta_1 \\mathcal{E}_{t-1} \\\\ &amp;=20 + 2.5\\times 5 \\nonumber \\\\ &amp;=32.5 \\nonumber \\end{align}\\] A similar situation applies should we know the true value. Assume an error difference (white noise) of 2 at period (t); therefore, our true value becomes: \\[\\begin{align} S(1) = y_t&amp;= \\mu+ \\mathcal{E}_t + \\theta_1 \\mathcal{E}_{t-1} \\\\ &amp;=20 + 2 + 2.5\\times 5 \\nonumber \\\\ &amp;=34.5 \\nonumber \\end{align}\\] Also, in this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as MA(1), meaning that our moving average model is based on the first-order. Moreover, if we have to consider two periods in the past, we can denote that as MA(2) so that the second-order formula then becomes: \\[\\begin{align} MA(q = 2) = \\mu + \\mathcal{E}_t + \\theta_1 \\mathcal{E}_{t-1} + \\theta_2 \\mathcal{E}_{t-2} \\end{align}\\] We can determine the proper order (q) for MA(q) by using ACF, which is discussed later. Third, the Integrated (I) method allows us to convert our Time-Series data from non-stationary to stationary series by a simple method called Differencing. Here, we denote \\(\\mathbf{I(d)}\\) as a function that performs differencing for a given Time-Series such that the parameter d indicates the degree of differencing (or how many iterations to take to difference the series). \\[\\begin{align} I(d) = \\{Y_{(t+1)} - Y_t\\}_{t=1}^{(T-1)} \\end{align}\\] To illustrate, the Time-Series data below can transform into the following new vector by first-degree differencing, e.g., I(1): \\[ y = (10,9,5,7,6,4)\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ z^{(1)} = (-1,-4, 2, -1,-2) \\] If it is essential to perform second-degree differencing, e.g. I(2), then we can iterate again: \\[ y = (10,9,5,7,6,4)\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ z^{(1)} = (-1,-4, 2, -1,-2)\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ z^{(2)} = (-3,6, -3, -1) \\] We can use any degree of differencing, e.g., I(d), as we see necessary. Fourth, we can combine all three methods to form ARIMA(p,d,q)with the three given methods. The parameters allow us to fine-tune our formulae for an accurate forecast. \\[\\begin{align} \\text{ARIMA}(p, d, q) = \\mu + \\underbrace{\\left(\\sum_{i=1}^p \\beta_i y_{t-i}\\right)}_\\text{AR(p)} + \\underbrace{\\left(\\sum_{i=1}^q \\theta _i\\mathcal{E}_{t-i}\\right)}_\\text{MA(q)} + \\mathcal{E}_t \\end{align}\\] Note that if we exclude the differencing, e.g., I(d), our model becomes the ARMA(p,q) model. This model assumes that our Time-Series data is already stationary. Finally, to determine the number of orders to use for AR(p) and MA(q), we have to rely on Autocorrelation, which allows us to compare the correlation of a time-series data between two time periods. Here, we introduce AutoCorrelation Function (ACF) and Partial AutoCorrelation Function (PACF). AutoCorrelation Function (ACF) To start, we first determine the value of the q parameter for MA. In this case, we use ACF. We can illustrate this using the equation below: \\[\\begin{align} ACF(k) = \\frac{\\sum_{t=k+1}^T \\left(y_t - \\bar{y}\\right)\\left(y_{t -k} - \\bar{y}\\right)} {\\sum_{t=1}^T \\left(y_t - \\bar{y}\\right)^2} \\end{align}\\] Our example implementation of the ACF equation is as follows: my.acf &lt;- function(y) { N = length(y) y.mean = mean(y) acf = rep(0, N ) for (k in 0: (N-1) ) { numer = 0; denom = 0 for (t in (k+1):N) { numer = numer + (y[t] - y.mean) * (y[t-k] - y.mean) } for (t in 1:N) { denom = denom + (y[t] - y.mean)^2 } acf[k + 1] = numer / denom } acf } To use the function, let us generate some random time-series and plot (see Figure 11.22). set.seed(2020) N = 100 e = rnorm(N, mean=0, sd=1) y = cos( seq(0, N, length.out = N)) + e y = ts (data = y, start = c(2018, 1), frequency=4) y = as.vector(y) Figure 11.22: Stationary (Gaussian Noise) Using our own implementation of ACF, we get the following: acf1 = my.acf(y ); head(acf1, n=30) # display only first 30 values ## [1] 1.00000 0.21814 -0.27968 -0.31197 -0.18164 0.12728 0.18185 ## [8] 0.17614 -0.05514 -0.34622 -0.20759 -0.01356 0.15699 0.14616 ## [15] 0.12170 -0.11613 -0.23486 -0.06625 0.18138 0.22938 -0.02141 ## [22] -0.14976 -0.15499 -0.12128 0.16216 0.17166 0.06235 -0.09066 ## [29] -0.16201 -0.05920 We can validate with acf(.) function from the built-in stats R package: p = stats::acf(y, type=&quot;correlation&quot; , plot=FALSE, lag.max=N - 1) acf2 = as.vector(p$acf); head(acf1, n=30) # display only first 30 values ## [1] 1.00000 0.21814 -0.27968 -0.31197 -0.18164 0.12728 0.18185 ## [8] 0.17614 -0.05514 -0.34622 -0.20759 -0.01356 0.15699 0.14616 ## [15] 0.12170 -0.11613 -0.23486 -0.06625 0.18138 0.22938 -0.02141 ## [22] -0.14976 -0.15499 -0.12128 0.16216 0.17166 0.06235 -0.09066 ## [29] -0.16201 -0.05920 Now to compare if both are equal: all.equal(acf1 , acf2 ) ## [1] TRUE Additionally, we also accompany ACF with an estimation of our confidence level to help with our cut-off. However, first, let us recall the equation and corresponding implementation: \\[\\begin{align} \\pm z \\times \\left(\\sigma / \\sqrt{T}\\right) \\ \\ \\ \\ \\ \\ \\text{where z-score} = 1.96 \\text{ and }\\sigma = 1 \\end{align}\\] conf.interval &lt;- function(y, alpha) { N = length(y); z = alpha; sigma = 1 c(sigma,-sigma) * z/sqrt(N) } A confidence level of 95% has a z=score of 1.96. This maps to the following ACF boundaries. (cl = conf.interval(y, 1.96)) ## [1] 0.196 -0.196 We now plot ACF - see Figure 11.23. Notice that the correlation tails off for one with seasonality. x = seq(1, length(acf1)) plot(NULL, xlim=range(x), ylim=range(-1,1), xaxt=&quot;n&quot;, xlab=&quot;Lag&quot;, ylab=&quot;ACF&quot;, main=&quot;ACF Plot&quot;, frame=TRUE) axis(side = 1, at = seq(0, N), tcl = -0.3, labels = TRUE) grid(lty=3, col=&quot;lightgrey&quot;) abline(h=0, col=&quot;darksalmon&quot;, lwd=2, lty=2) lines(x, acf1, type=&#39;h&#39;, lwd=2, col=&quot;navyblue&quot;) abline(h=conf.interval(y, 1.96), col=&quot;blue&quot;, lty=2) Figure 11.23: ACF Plot With the confidence level drawn horizontally in the figure, any ACF value between \\(\\pm\\) (0.196) is considered as having zero correlation for a lag; and thus gets discarded. Therefore, to finally obtain the order for our q parameter for MA, we get: (MA.q = which(!between(acf1, cl[2], cl[1]))) ## [1] 1 2 3 4 10 11 17 20 47 56 Partial AutoCorrelation Function (PACF) Next, we use PACF to determine the value for the p parameter for AR. The basic idea of PACF starts with the equation below in which the value for \\(Y_t\\) can be explained directly by the pth term, namely \\(Y_{t-k}\\). In other words, we suggest that there is some autocorrelation between \\(Y_t\\) and \\(Y_{t-k}\\). \\[\\begin{align} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\beta_3 Y_{t-3}\\ + ... +\\ \\beta_k Y_{t-k} \\end{align}\\] Therefore, we can discard \\(Y_{t-1},\\ Y_{t-2},\\ Y_{t-3}, ...,\\ Y_{t-k+1}\\) for a Time-Series at lag k. What we are interested is the coefficient at lag k, namely \\(\\beta_k\\) from the term \\(\\beta_k Y_{t-k}\\). The value for PACF is obtained from the coefficient of the terminating term of the following equation. \\[\\begin{align} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\beta_3 Y_{t-3}\\ + ... +\\ \\beta_k Y_{t-k} \\end{align}\\] The terminating term of the equation above is based on a given Kth order. For example, PACF(K=2) equals the \\(\\beta_2\\) coefficient from the following equation: \\[\\begin{align} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} \\end{align}\\] PACF(K=4) equals the \\(\\beta_4\\) coefficient from the following equation: \\[\\begin{align} Y_t = \\beta_0 + \\beta_1 Y_{t-1} + \\beta_2 Y_{t-2} + \\beta_3 Y_{t-3} + \\beta_4 Y_{t-4} \\end{align}\\] Now, let us introduce three ways to calculate PACF for a system of equations. Each equation is generated for each value of K, namely \\(k = 1,2,3,...,N\\). The first way is to use symmetric invertible Toeplitz matrix, which is simply a list of systems of equations — Yule-Walker equations. A generic Toeplitz matrix is shown below: \\[\\begin{align} \\underbrace{ \\left[ \\begin{array}{lllllll} 1 &amp; \\rho_{1} &amp; \\rho_{2 } &amp; \\cdots &amp; \\rho_{k-2} &amp; \\rho_{k-1} \\\\ \\rho_{1} &amp; 1 &amp; \\rho_{1 } &amp; \\cdots &amp; \\rho_{k-3} &amp; \\rho_{k-2} \\\\ \\rho_{2} &amp; \\rho_{1} &amp; 1 &amp; \\cdots &amp; \\rho_{k-4} &amp; \\rho_{k-3} \\\\ \\vdots &amp; \\vdots &amp; \\vdots&amp; \\ddots &amp;\\vdots &amp; \\vdots\\\\ \\rho_{k-2} &amp; \\rho_{k-3} &amp; \\rho_{k-4} &amp; \\cdots &amp; 1 &amp; \\rho_{1} \\\\ \\rho_{k-1} &amp; \\rho_{k-2} &amp; \\rho_{k-3}&amp; \\cdots &amp; \\rho_{1} &amp; 1 \\\\ \\end{array} \\right] }_{\\text{R}} \\underbrace{ \\left[ \\begin{array}{l} \\beta_1 \\\\ \\beta_2 \\\\ \\beta_3 \\\\ \\vdots \\\\ \\beta_{k-1} \\\\ \\beta_k \\\\ \\end{array} \\right]}_{\\beta} = \\underbrace{ \\left[ \\begin{array}{l} \\rho_1 \\\\ \\rho_2 \\\\ \\rho_3 \\\\ \\vdots \\\\ \\rho_{k-1} \\\\ \\rho_k \\\\ \\end{array} \\right]}_{\\rho} \\label{eqn:eqnnumber503} \\end{align}\\] Without going through derivations, we compute for the coefficients like so: \\[\\begin{align} \\beta = R^{-1}\\rho \\end{align}\\] To illustrate, given k=5, we only consider the \\(k \\times x\\) dimension of R and the kth dimension of \\(\\rho\\) to solve for the PACF value at lag 5. \\[\\begin{align} \\underbrace{\\beta}_{(1:k)} = \\underbrace{R^{-1}}_{(1:k,1:k)}\\underbrace{\\rho}_{(1:k)} \\end{align}\\] Let us use the vector of ACF we generated previously to create a Toeplitz matrix given k=5: k=5 R = toeplitz(acf1) (R.kk = R[1:k, 1:k]) # display only 5x5 toeplitz matrix for illustration ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.0000 0.2181 -0.2797 -0.3120 -0.1816 ## [2,] 0.2181 1.0000 0.2181 -0.2797 -0.3120 ## [3,] -0.2797 0.2181 1.0000 0.2181 -0.2797 ## [4,] -0.3120 -0.2797 0.2181 1.0000 0.2181 ## [5,] -0.1816 -0.3120 -0.2797 0.2181 1.0000 Then we use solve(.) to invert the matrix and solve for the coefficients. beta.kk = acf1[2:(k + 1 )] (coeffs = as.vector( solve(R.kk) %*% beta.kk)) ## [1] 0.20877 -0.33304 -0.11718 -0.19982 0.07212 Out of the obtained coefficients, namely \\(\\beta_1\\) = 0.2088, \\(\\beta_2\\) = -0.333, \\(\\beta_3\\) = -0.1172, \\(\\beta_4\\) = -0.1998, and \\(\\beta_5\\) = 0.0721, we consider only PACF(k=5) = \\(\\beta_5\\) = 0.0721. For an example implementation of PACF, we have the following: my.pacf &lt;- function(R, rho) { N = length(rho) pacf = NULL for (k in 1:N) { beta = solve(R[1:k, 1:k]) %*% rho[1:k] pacf = c(pacf, beta[k]) } pacf } P = length(acf1) rho = acf1[2:P] head(my.pacf(R, rho), n=30) # for k=1,2,...,N ## [1] 0.218137 -0.343610 -0.184589 -0.185733 0.072116 -0.017177 ## [7] 0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556 ## [13] -0.062338 0.154348 -0.134410 -0.033726 -0.048870 0.076038 ## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892 0.083693 ## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387 Alternatively, to solve for PACF without using Toeplitz matrix, we can use the Levinson-Durbin Algorithm. The Algorithm comes in two forms, one of which obtains the backward prediction coefficients (Borchers B. 2001). Step 1: \\(\\beta_{1} = \\frac{\\rho_1}{\\rho_0},\\ \\ \\ P_1 = \\beta_{1}\\) Step 2: For \\(n = 2,3,...,p\\), compute for the updates: \\[\\begin{align} \\beta_{nn} &amp;= \\frac{\\rho_n - \\sum_{k=1}^{n-1} \\beta_{n-1,k} \\rho_{n-k}} {1 - \\sum_{k=1}^{n-1} \\beta_{n-1,k} \\rho_k}\\\\ \\beta_{nk} &amp;= \\beta_{n-1} - \\beta_{nn}\\ \\beta_{n-1, n-k},\\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ {k=1,2,...,n-1} \\\\ P_{n} &amp;= \\beta_{nn} \\end{align}\\] We leave readers to investigate the algorithm to obtain the forward prediction coefficients as an exercise. Below is an example implementation of the Levinson-Durbin algorithm (motivated by the original R script from Ross Ihaka’s Statistical 726 Course (n.d.) with modification following the algorithm above): my.Levinson.Durbin = function(rho, lag.max = 2) { N = length(rho) beta = rep(0, N - 1) pacf = beta[1] = rho[2] / rho[1] rho = rho[2:(lag.max+1)] pacf = c(beta[1]) for (n in 2:lag.max) { beta.n = beta[1:(n - 1)] rho.n = rho[1:(n - 1)] beta[n] = ( rho[n] - sum( beta.n * rev(rho.n) )) / ( 1 - sum( beta.n * rho.n)) beta[1:(n - 1)] = beta.n - beta[n] * rev(beta.n) pacf = c(pacf, beta[n]) } pacf } pacf1 = my.Levinson.Durbin(acf1, lag.max = (P-1)) head(pacf1, n=30) ## [1] 0.218137 -0.343610 -0.184589 -0.185733 0.072116 -0.017177 ## [7] 0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556 ## [13] -0.062338 0.154348 -0.134410 -0.033726 -0.048870 0.076038 ## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892 0.083693 ## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387 Alternatively, we can still use pacf(.) function with type equal to partial: pacf2 = as.vector( pacf(y, type=&quot;partial&quot;, lag.max=(P-1), plot=FALSE)$acf) head(pacf2, n=30) ## [1] 0.218137 -0.343610 -0.184589 -0.185733 0.072116 -0.017177 ## [7] 0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556 ## [13] -0.062338 0.154348 -0.134410 -0.033726 -0.048870 0.076038 ## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892 0.083693 ## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387 Let us validate: all.equal(pacf1, pacf2) ## [1] TRUE We now plot PACF - see Figure 11.24. Figure 11.24: PACF Plot Notice that both ACF and PACF plots show some behavior in which the first couple of lags are beyond the confidence levels, indicating a strong correlation of the first lag lags at least with the current time, e.g., \\(Y_t\\). Then the rest of the lags tail off — meaning the correlation diminishes gradually. On the other hand, there are cases in which the plots show a strong correlation at the beginning, then suddenly cuts off. For example, in the ACF plot, if we see a strong correlation at the beginning of the lag, e.g., lag 1, and cuts off for the subsequent lags, then we can interpret that to mean that AR(1) or MA(1) is most likely the configuration we seek in which p for AR or q for MA follows a parameter value of 1. Other literature explains signs of Seasonality or Trend reflected in the plots. As an exercise, we leave readers to investigate other examples of ACF and PACF plots and review their interpretations. 11.4.5 Multiplicative Seasonal ARIMA (SARIMA) ARIMA models are mainly used for stationary time series. To address non-stationary time-series, we can use SARIMA, which is written in the following expression: \\[\\begin{align} \\text{ARIMA}\\underbrace{(p,d,q)}_{\\text{non-seasonal}} \\times \\underbrace{(P,D,Q)}_{\\text{seasonal}} m \\end{align}\\] where: \\[\\begin{align*} \\begin{array}{ll} \\mathbf{p}\\ \\text{is non-seasonal param for AR order} &amp; \\mathbf{P}\\ \\text{is seasonal param for AR order} \\\\ \\mathbf{d}\\ \\text{is non-seasonal param for differencing} &amp; \\mathbf{D}\\ \\text{is seasonal param for differencing} \\\\ \\mathbf{q}\\ \\text{is non-seasonal param for MA order} &amp; \\mathbf{Q}\\ \\text{is seasonal param for MA order} \\end{array}\\\\ \\mathbf{m}\\ \\text{number of periods to cover for a given frequency or season} \\end{align*}\\] To understand SARIMA, let us introduce the Backshift operator denoted simply as B, which shifts a given period one period back. Below are examples of backshifting a given period: \\[\\begin{align} B (Y_t) = Y_{t - 1}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ B (Y_{t-1}) = Y_{t - 2}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ B^2 (Y_t) = B (Y_{t-1}) = Y_{t - 2}\\ \\ \\ \\ \\ \\ \\end{align}\\] Another example is to use the same notation for differencing: \\[\\begin{align} \\underbrace{Y_t - Y_{t - 1} = ( 1 - B) y_t}_{\\text{1st difference}}\\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{Y_t - Y_{t - 1} - Y_{t - 2} = ( 1 - B)^2 y_t}_{\\text{2nd difference}} \\end{align}\\] The last example is to use the same notation to backshift a season to m periods: \\[\\begin{align} (1 - B^m)y_t \\end{align}\\] where m is the number of periods to backshift in a given season. Backshift operator follows both the algebraic and polynomial rules so that given AR(1) and AR(2), respectively, as an example, we start with the usual equations: \\[\\begin{align} y_t = \\mu + \\beta_1 y_{t-1}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ y_t = \\mu + \\beta_1 y_{t-1} + \\beta_2 y_{t-2} \\end{align}\\] We then re-arrange our equation \\[\\begin{align} \\begin{array}{ll} y_t - \\beta_1 y_{t-1} &amp;= \\mu \\\\ (1 - \\beta_1 B )y_t &amp;= \\mu \\end{array}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{ll} y_t - \\beta_1 y_{t-1} - \\beta_2 y_{t-2} &amp;= \\mu \\\\ (1 - \\beta_1 B - \\beta_2 B^2)y_t &amp;= \\mu \\end{array} \\label{eqn:eqnnumber504} \\end{align}\\] The corresponding polynomial for the backshift operation becomes: \\[\\begin{align} \\beta (B) = (1 - \\beta_1 B)\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\beta (B) = (1 - \\beta_1 B - \\beta_2 B^2) \\end{align}\\] so then we get the final polynomial notation (which applies for both AR(1) and AR(2) generically): \\[ \\beta(B) y_t = \\mu \\] In terms of SARIMA modeling, we start with a notation as expressed below: \\[\\mathbf{\\text{SARIMA}(2,1,1)(1,2,1)_{4}}\\]. Here, we break the notation into the following: \\[\\begin{align} \\underbrace{ \\begin{array}{rllll} AR(2)\\ &amp;\\rightarrow\\ y_t &amp;= \\beta_1 y_{t-1} + \\beta_2 y_{t-2}\\\\ I(1)\\ &amp;\\rightarrow\\ \\Delta y_t &amp;= y_t - y_{t-1}\\\\ MA(1)\\ &amp;\\rightarrow\\ \\mathcal{E}_t &amp;= \\theta_1 \\mathcal{E}_{t-1}\\\\ \\end{array}}_{\\text{non-seasonal}} \\left| \\underbrace{ \\begin{array}{rllll} AR(1)\\ &amp;\\rightarrow\\ y_t &amp;= \\alpha_1 y_{t-1} \\\\ I(2)\\ &amp;\\rightarrow\\ \\Delta y_t &amp;= y_t - y_{t-1} - y_{t-2}\\\\ MA(2)\\ &amp;\\rightarrow\\ \\mathcal{E}_t &amp;= \\phi_1 \\mathcal{E}_{t-1} \\\\ \\end{array}}_{\\text{seasonal}} \\right. \\label{eqn:eqnnumber505} \\end{align}\\] Using the backshift notation, we write the following: \\[\\begin{align} \\underbrace{ \\begin{array}{rll} AR(2)\\ &amp;\\rightarrow\\ (1 -\\beta_1 B - \\beta_2 B^2) y_t \\\\ I(1)\\ &amp;\\rightarrow\\ (1 - B)y_t\\\\ MA(1)\\ &amp;\\rightarrow\\ (1 - \\theta_1 B )\\mathcal{E}_t \\\\ \\end{array}}_{\\text{non-seasonal}} \\left| \\underbrace{ \\begin{array}{rll} AR(1)\\ &amp;\\rightarrow\\ (1 - \\alpha B^4)y_t \\\\ I(2)\\ &amp;\\rightarrow\\ (1 - B^4)^2y_t\\\\ MA(2)\\ &amp;\\rightarrow\\ (1 -\\phi_1 B^4) \\mathcal{E}_t \\\\ \\end{array}}_{\\text{seasonal}} \\right. \\label{eqn:eqnnumber506} \\end{align}\\] Therefore, the final SARIMA model, namely \\(\\text{SARIMA}(2,1,1)(1,2,1)_{\\mathbf{4}}\\) corresponds to the following operation. \\[\\begin{align} (1 -\\beta_1 B - \\beta_2 B^2) (1 - B)(1 - \\alpha B^4)(1 - B^4)^2y_t = (1 - \\theta_1 B )(1 -\\phi_1 B^4) \\mathcal{E}_t \\end{align}\\] 11.4.6 Time-Series Decomposition In Chapter 4 (Numerical Calculus)}, we covered Fourier Series and Transformation. We expressed the idea that a large wave can be represented as a convolution of smaller waves such that we can, as an example, mathematically decompose the large wave equation into a series of sinusoidal terms. In this section, the same idea applies. However, the opposite applies in that combining terms is equivalent to applying the effect of smoothing the series. We can use smoothing methods such as splines and local regression to decompose trend, seasonality, and random noise. \\[\\begin{align} yT = f(sT, tT, rT) = sT + tT + rT \\end{align}\\] When components are multiplicative, then use \\(\\log_e(.)\\). \\[\\begin{align} yT = f(sT, tT, rT) = log(sT) + log(tT) + log(rT) \\end{align}\\] To illustrate, let us concoct a dataset that covers 24 periods: set.seed(2020) N = 24 e = rnorm(N, mean=0, sd=1) (y = cos( seq(0, N, length.out = N)) + e ) ## [1] 1.3770 0.8048 -1.5916 -2.1303 -3.3094 1.2044 1.9389 0.2930 ## [9] 1.2851 -0.8821 -1.3850 1.3734 2.1954 0.1697 -0.5775 0.8016 ## [17] 1.1534 -2.5945 -1.2912 0.6182 1.7401 0.1012 -0.2509 0.3510 For a monthly period, we can change the frequency to 12. Since our data set has a stretch of 24 periods, then we cover two years: (ts.data = ts (data = y, start = c(2018, 1), frequency=12)) ## Jan Feb Mar Apr May Jun Jul Aug ## 2018 1.3770 0.8048 -1.5916 -2.1303 -3.3094 1.2044 1.9389 0.2930 ## 2019 2.1954 0.1697 -0.5775 0.8016 1.1534 -2.5945 -1.2912 0.6182 ## Sep Oct Nov Dec ## 2018 1.2851 -0.8821 -1.3850 1.3734 ## 2019 1.7401 0.1012 -0.2509 0.3510 Furthermore, for a quarterly period, we can change the frequency to 4. Similarly, since our data stretches to 24 periods, then we cover six years: (ts.data = ts (data = y, start = c(2018, 1), frequency=4)) ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 1.3770 0.8048 -1.5916 -2.1303 ## 2019 -3.3094 1.2044 1.9389 0.2930 ## 2020 1.2851 -0.8821 -1.3850 1.3734 ## 2021 2.1954 0.1697 -0.5775 0.8016 ## 2022 1.1534 -2.5945 -1.2912 0.6182 ## 2023 1.7401 0.1012 -0.2509 0.3510 Let us now decompose by additive method: (ts.decomposed = decompose( ts.data , &quot;additive&quot;)) ## $x ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 1.3770 0.8048 -1.5916 -2.1303 ## 2019 -3.3094 1.2044 1.9389 0.2930 ## 2020 1.2851 -0.8821 -1.3850 1.3734 ## 2021 2.1954 0.1697 -0.5775 0.8016 ## 2022 1.1534 -2.5945 -1.2912 0.6182 ## 2023 1.7401 0.1012 -0.2509 0.3510 ## ## $seasonal ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 0.6753 -0.4334 -0.5115 0.2695 ## 2019 0.6753 -0.4334 -0.5115 0.2695 ## 2020 0.6753 -0.4334 -0.5115 0.2695 ## 2021 0.6753 -0.4334 -0.5115 0.2695 ## 2022 0.6753 -0.4334 -0.5115 0.2695 ## 2023 0.6753 -0.4334 -0.5115 0.2695 ## ## $trend ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 NA NA -0.97084 -1.50667 ## 2019 -1.01542 -0.27120 0.60603 0.91954 ## 2020 0.24325 -0.03718 0.21165 0.45691 ## 2021 0.68931 0.71877 0.51704 0.04127 ## 2022 -0.39347 -0.50561 -0.45519 -0.04488 ## 2023 0.42212 0.51877 NA NA ## ## $random ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 NA NA -0.10928 -0.89318 ## 2019 -2.96928 1.90897 1.84429 -0.89605 ## 2020 0.36654 -0.41150 -1.08518 0.64700 ## 2021 0.83073 -0.11568 -0.58310 0.49082 ## 2022 0.87151 -1.65549 -0.32457 0.39357 ## 2023 0.64265 0.01586 NA NA ## ## $figure ## [1] 0.6753 -0.4334 -0.5115 0.2695 ## ## $type ## [1] &quot;additive&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;decomposed.ts&quot; Now, let us try to reconstruct by adding: (ts.decomposed$seasonal + ts.decomposed$trend + ts.decomposed$random ) ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 NA NA -1.5916 -2.1303 ## 2019 -3.3094 1.2044 1.9389 0.2930 ## 2020 1.2851 -0.8821 -1.3850 1.3734 ## 2021 2.1954 0.1697 -0.5775 0.8016 ## 2022 1.1534 -2.5945 -1.2912 0.6182 ## 2023 1.7401 0.1012 NA NA Let us also decompose by a multiplicative method and try to reconstruct by multiplying. We should see the same time-series data. ts.decomposed = decompose( ts.data , &quot;multiplicative&quot;) (ts.decomposed$seasonal * ts.decomposed$trend * ts.decomposed$random ) ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 NA NA -1.5916 -2.1303 ## 2019 -3.3094 1.2044 1.9389 0.2930 ## 2020 1.2851 -0.8821 -1.3850 1.3734 ## 2021 2.1954 0.1697 -0.5775 0.8016 ## 2022 1.1534 -2.5945 -1.2912 0.6182 ## 2023 1.7401 0.1012 NA NA We can then plot and see what it looks like. See Figure 11.25. Figure 11.25: Time-Series Decomposition In the next section, let us perform prediction using ARIMA/SARIMA. 11.4.7 STL with AIC/BIC In our recent discussion on Arima and Sarima, we have introduced models in the form of linear equations. Such linear equations, in a Linear Model, as discussed in Chapter 9 (Computational Learning I) under AIC and BIC, can be transformed such that the coefficients are evaluated for relevance. To illustrate, let us use stlf(.) to apply ARIMA method and AIC for information criteria to our recent ts.data time series in the previous section. Here, we try to forecast what it looks like for the first two quarters in the year following the time series, namely 2024. library(forecast) stlf(ts.data, h=2, method=&#39;arima&#39;, ic=&#39;bic&#39;) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2024 Q1 0.7659 -0.9617 2.493 -1.876 3.408 ## 2024 Q2 -0.4565 -2.1840 1.271 -3.099 2.186 Another method of interest is Error Trend Seasonality (ETS). Again, let us use AIC this time with MAE for optimization criteria. stlf(ts.data, h=2, method=&#39;ets&#39;, ic=&#39;aic&#39;, opt.crit=&#39;mae&#39;) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2024 Q1 1.20850 -0.6654 3.082 -1.657 4.074 ## 2024 Q2 -0.01387 -1.8878 1.860 -2.880 2.852 We leave readers to investigate ETS. 11.4.8 Multivariate Time-Series In real-world scenarios, time series can be complex. This section is to account for multivariate features in a time series which we can base on the following linear equation with two independent variables and one dependent variable: \\[\\begin{align} y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 \\end{align}\\] We can concoct a simple multivariate data point (using sine with noise) like so: N = 12 x1 = seq(1,N) + rnorm(N,0,1) x2 = rep( sin(seq(1, 4)), 3) + rnorm(N, 0, 1) beta0 = 0.5; beta1 = 0.7; beta2 = 1.7 y = beta0 + beta1 * x1 + beta2 * x2 (m = as.matrix(cbind(x1,x2, y))) ## x1 x2 y ## [1,] 1.834 0.5559 2.729 ## [2,] 2.199 0.9853 3.714 ## [3,] 4.298 -0.4192 2.796 ## [4,] 4.937 -0.3096 3.429 ## [5,] 4.853 1.7500 6.872 ## [6,] 6.110 0.4042 5.465 ## [7,] 6.187 -0.1599 4.559 ## [8,] 7.256 -1.4828 3.059 ## [9,] 10.095 -0.3386 6.991 ## [10,] 12.435 1.1624 11.181 ## [11,] 11.388 -0.2296 8.081 ## [12,] 12.291 -0.7346 7.855 Then converting to time-series, our time-series starts from the year 2018 and stretches through 2020 (on a quarterly basis): (ts.multiv = ts(m, start =c(2018,1), frequency=4)) ## x1 x2 y ## 2018 Q1 1.834 0.5559 2.729 ## 2018 Q2 2.199 0.9853 3.714 ## 2018 Q3 4.298 -0.4192 2.796 ## 2018 Q4 4.937 -0.3096 3.429 ## 2019 Q1 4.853 1.7500 6.872 ## 2019 Q2 6.110 0.4042 5.465 ## 2019 Q3 6.187 -0.1599 4.559 ## 2019 Q4 7.256 -1.4828 3.059 ## 2020 Q1 10.095 -0.3386 6.991 ## 2020 Q2 12.435 1.1624 11.181 ## 2020 Q3 11.388 -0.2296 8.081 ## 2020 Q4 12.291 -0.7346 7.855 We can then plot and see what it looks like. See Figure 11.26. Figure 11.26: Multivariate Time-Series Let us fit our model to data and predict (forecast what it looks like in the year 2021, covering the first two quarters): ts.fit = tslm(ts.multiv ~ trend + season) (f = forecast(ts.fit, h=2)) ## x1 ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2021 Q1 13.83 11.79 15.86 10.43 17.23 ## 2021 Q2 15.15 13.12 17.19 11.75 18.55 ## ## x2 ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2021 Q1 0.4175 -0.8653 1.700 -1.726 2.561 ## 2021 Q2 0.6124 -0.6704 1.895 -1.532 2.756 ## ## y ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2021 Q1 10.89 8.225 13.56 6.437 15.34 ## 2021 Q2 12.15 9.481 14.81 7.693 16.60 Note that Multivariate Time-Series is different from Multiple Time-Series. The former pertains to dependent variables, namely X, while the latter pertains to independent variables, namely Y. Additionally, the latter considers different time series in different state spaces, depending on different factors and conditions; hence multiple series. We cover that in the next section. 11.4.9 Forecasting Considerations There are many considerations to make when dealing with time-series data and forecasting. Our time-series data is mixed with noise and many other factors in real-world scenarios. Our goal is to perform some level of pre-processing to clean our data. Denoising Time-series One of the essential preparations to make is to reduce the amount of noise from our data. There are many methods of denoising time series. Each method may be more specific and relative to certain domains. In general, however, we are listing below a few suggested methods: Rolling and Expanding Window method Wavelet Transform method Spline Smoothing method Convolution method We leave readers to investigate each of the mentioned denoising methods. Sampling Time-Series Another important step to make is when sampling our data. Take market trading as an example. Certain period is affected by Job Employment status, 10-year notes status, market cycles, industry cycle, fiscal reports, and now including the recent Covid-19 Pandemic. When sampling for data, do we need to skip the year 2020, considering the unique condition compared to any other periods? For this reason, there are cases when we need to sample and model our forecast based on different conditions, multiple series in state space. Lag features Lags in time series can be helpful in some cases with which we can generate new features. Using our original multivariate time series, we can use the Lags from the target variable, namely y, by shifting one lag and using the result as a new feature, namely x3. m ## x1 x2 y ## [1,] 1.834 0.5559 2.729 ## [2,] 2.199 0.9853 3.714 ## [3,] 4.298 -0.4192 2.796 ## [4,] 4.937 -0.3096 3.429 ## [5,] 4.853 1.7500 6.872 ## [6,] 6.110 0.4042 5.465 ## [7,] 6.187 -0.1599 4.559 ## [8,] 7.256 -1.4828 3.059 ## [9,] 10.095 -0.3386 6.991 ## [10,] 12.435 1.1624 11.181 ## [11,] 11.388 -0.2296 8.081 ## [12,] 12.291 -0.7346 7.855 Below is an example of shifting lag: library(data.table) (x3 = shift(m[,3], n=1, fill=NA, type=&quot;lag&quot;) ) ## [1] NA 2.729 3.714 2.796 3.429 6.872 5.465 4.559 3.059 ## [10] 6.991 11.181 8.081 We then incorporate the shifted series back into the matrix in the form of a new feature. as.matrix(data.frame(x1 = m[,1], x2 = m[,2], x3 = x3, y = m[,3])) ## x1 x2 x3 y ## [1,] 1.834 0.5559 NA 2.729 ## [2,] 2.199 0.9853 2.729 3.714 ## [3,] 4.298 -0.4192 3.714 2.796 ## [4,] 4.937 -0.3096 2.796 3.429 ## [5,] 4.853 1.7500 3.429 6.872 ## [6,] 6.110 0.4042 6.872 5.465 ## [7,] 6.187 -0.1599 5.465 4.559 ## [8,] 7.256 -1.4828 4.559 3.059 ## [9,] 10.095 -0.3386 3.059 6.991 ## [10,] 12.435 1.1624 6.991 11.181 ## [11,] 11.388 -0.2296 11.181 8.081 ## [12,] 12.291 -0.7346 8.081 7.855 From there, we can perform forecasting. The number of shifts can be determined perhaps based on cycles or frequencies: cycle(ts.multiv) ## Qtr1 Qtr2 Qtr3 Qtr4 ## 2018 1 2 3 4 ## 2019 1 2 3 4 ## 2020 1 2 3 4 frequency(ts.multiv) ## [1] 4 Panel data and Pooled data We end our discussion of Time-Series in this section by introducing Panel data, also called Longitudinal data, borrowed from Statistics and Econometrics theory. This type of data combines both Time-Series data and Cross-Sectional data. To explain the difference between the two, as an example for the former, Time-Series data is a collection or group of observations for a single variable, e.g., market stock price, over time. As for the latter, we have a collection of observations for multiple variables, e.g., companies and job employment status, over the same period. In some cases, Pooled data is interpreted as Panel data. However, to be more concrete, when we have a collection of cross-sectional data over time, we refer to it as Pooled data. If we have a cross-sectional data sampled multiple times, we regard it as Panel data. For example, it becomes Pooled data for cross-sectional data collected over two years. It then becomes Panel data for cross-sectional data collected every five years - e.g., collecting various factors affecting climate change every five years. Also, in some cases, we find terms such as Stacked Time-Series, Stacked Cross-Section, and Pooled data. As the term implies, multiple Time-Series data can be stacked on top of each other (if visually plotted). Similarly, the same applies to multiple Cross-Sectional data Stacked Time-Series. Fixed Effect Model vs. Random Effect Model It helps to be familiar when dealing with Time-Series, with two models mainly introduced in Statistics and Econometrics. The first is Fixed Effects model, which describes modeling data that are fixed in nature. Examples of such data that are constant and not changing are the sex and nationality of an individual. On the other hand, Random Effect Model changes over time, such as market price. For date utilities that we can use for forecasting, please see the Appendix for the lubridate package. 11.5 Recommender Systems Recommender Systems are systems that filter and suggest the most relevant information to users. When it comes to relevant information, barring privacy and confidentiality, we mine as much valuable and relevant information as we can, such as the following few examples: Customer Information or Profile Customer Choices and Interactions Product Properties Events and Activities Relationships of Information and Interactions Information that signifies popularity, frequency, and trend There are two types of Filtering of information: Collaborative-based Filtering - this Filtering accounts for customer collaboration in reactions. Reactions can be measured based on two types of interaction: Explicit interaction - A user may directly rank, rate, or vote for a product. Implicit interaction - A user may repeatedly visit a product page. Content-based Filtering - this Filtering accounts for product information in customer feedback, reviews, opinions, and suggestions. Through customer feedback, we can determine similar interests in a particular product. In addition, we can compare top product features that overlap with user interests. To illustrate further, Figure 11.27 shows the two types of Information Filtering. Figure 11.27: Information Filtering The first type in the figure shows a table of customer ratings over movies. Here, users seem to have ranked Movie 1 higher than the others. The second type in the figure shows two tables - the first table records movies and corresponding features. The other table records users and their corresponding preferences. Here, two users prefer Drama movies filmed at least since 2005. In real-world scenarios, we deal with a much larger matrix that is highly sparsed, meaning there is a large number of missing data. Below, we simulate a sparsed utility matrix. We use 25% sparsity to simulate missing data. library(Matrix) set.seed(2020) max.rating = 5; items = 5; users = 10; sparsity = 0.25; N = items * users; m.size = (1 - sparsity) * N; X = rep(0, N) cells = sample(N, size=m.size, replace = FALSE) X[cells] = round(runif(m.size, min=1, max=max.rating), 0) A = matrix(X, nrow=users, ncol=items, byrow=TRUE) colnames(A) = paste0(&quot;Item&quot;, seq(1,items)) rownames(A) = paste0(&quot;User&quot;, seq(1,users)) A ## Item1 Item2 Item3 Item4 Item5 ## User1 3 4 3 4 2 ## User2 5 2 0 3 3 ## User3 0 5 0 0 2 ## User4 3 3 1 5 3 ## User5 0 4 5 0 0 ## User6 3 2 3 0 2 ## User7 4 2 5 0 0 ## User8 0 3 4 3 3 ## User9 2 5 3 2 0 ## User10 0 3 4 3 3 Now, let us walk through how a simple Recommender System works. First, given the raw utility matrix above, let us perform normalization to eliminate extreme ratings, e.g., a user giving a rating of five on all items. \\[\\begin{align} \\mathbf{b}_{ij} = \\mu + \\mathbf{b}_{i} + \\mathbf{b}_j \\end{align}\\] where: \\(\\mathbf{u}\\) - global average \\(\\mathbf{b}_i\\) - average rating of a user \\(\\mathbf{b}_j\\) - average rating of an item Let us now normalize our matrix and scale it back to the rating range of [0, 5]. normalize &lt;- function(A) { mu = mean(A) I = nrow(A) J = ncol(A) AA = matrix(0, I, J, byrow=TRUE) for (i in 1:I) { for (j in 1:J) { if (A[i,j] != 0) { bi = A[i,]; bi = mean( bi[ which( bi != 0 ) ] ) bj = A[,j]; bj = mean( bj[ which( bj != 0 ) ] ) AA[i,j] = round( mu + bi + bj, 1) } } } min.A = min(AA) max.A = max(AA) AA = (5 - 0) * (AA - min.A) / (max.A - min.A) + 0 colnames(AA) = colnames(A) rownames(AA) = rownames(A) AA } (A = round( normalize(A), 1)) ## Item1 Item2 Item3 Item4 Item5 ## User1 4.3 4.3 4.4 4.3 3.9 ## User2 4.3 4.3 0.0 4.3 3.9 ## User3 0.0 4.4 0.0 0.0 4.1 ## User4 4.2 4.2 4.3 4.2 3.8 ## User5 0.0 4.9 5.0 0.0 0.0 ## User6 3.9 3.9 4.0 0.0 3.6 ## User7 4.5 4.5 4.6 0.0 0.0 ## User8 0.0 4.3 4.4 4.3 3.9 ## User9 4.2 4.2 4.3 4.2 0.0 ## User10 0.0 4.3 4.4 4.3 3.9 Second, our next goal is to deal with sparsity. Note that a high sparse matrix may not provide correct and accurate recommendations. For this reason, more information may be required, such as relying on hybrid or content-based information filtering. In any case, let us assume some decent amount of sparsity. Here, we need to approximate the values of the missing data. The easiest way is to use SVD. Please refer to our discussion of SVD under Matrix Factorization in Chapter 3 (Numerical Linear Algebra I), our discussion of PCA under Feature Engineering in Chapter 9 (Computational Learning I), and our discussion of Topic Modeling under NLP in Chapter 11 (Computational Learning III). Let us now run svd(.) to obtain U, D, and V. Here, U corresponds to Users and their relationships with the latent factors, namely D. Similarly, V corresponds to Items and their similarities with the latent factors. The D is the singular eigenvalue ordered in increasing order that indicates the strength of the **latent factor. See below: (M = svd(A)) ## $d ## [1] 23.207 7.225 6.333 5.271 2.936 ## ## $u ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.4031 0.07690 0.18355 0.09401 0.26989 ## [2,] -0.3093 0.40194 0.45605 -0.15448 -0.44574 ## [3,] -0.1749 0.30866 -0.25647 -0.63324 -0.32890 ## [4,] -0.3936 0.07411 0.17932 0.09270 0.26242 ## [5,] -0.2277 -0.43402 -0.50546 -0.05542 -0.32551 ## [6,] -0.3017 -0.17395 0.05309 -0.47428 0.52959 ## [7,] -0.2808 -0.57111 0.14391 -0.21386 -0.09417 ## [8,] -0.3348 0.24117 -0.39801 0.25017 0.07550 ## [9,] -0.3323 -0.26578 0.25407 0.39848 -0.38046 ## [10,] -0.3348 0.24117 -0.39801 0.25017 0.07550 ## ## $v ## [,1] [,2] [,3] [,4] [,5] ## [1,] -0.3685 -0.27603 0.8566 -0.1914 0.1327 ## [2,] -0.5738 -0.09535 -0.2531 -0.3634 -0.6823 ## [3,] -0.4946 -0.53375 -0.3922 0.2977 0.4775 ## [4,] -0.3874 0.46061 0.1812 0.7502 -0.2053 ## [5,] -0.3744 0.64627 -0.1246 -0.4242 0.4967 If we regard this from the perspective of PCA, we are looking to see which eigenvalues are relevant based on the latent factors. For example, using K=2, let us reconstruct our matrix by reducing only the first two columns of U, D, and V as so: k = 2 A.hat = M$u[,1:k ] %*% diag(M$d[1:k ]) %*% t(M$v[,1:k ]) rownames(A.hat ) = rownames(A) colnames(A.hat ) = colnames(A) round(A.hat ,0) ## Item1 Item2 Item3 Item4 Item5 ## User1 3 5 4 4 4 ## User2 2 4 2 4 5 ## User3 1 2 1 3 3 ## User4 3 5 4 4 4 ## User5 3 3 4 1 0 ## User6 3 4 4 2 2 ## User7 4 4 5 1 0 ## User8 2 4 3 4 4 ## User9 3 5 5 2 2 ## User10 2 4 3 4 4 By choosing K=2, let us see if we closely match the original matrix. Here, we use SSE for our loss function. Other monotonically equivalent measures can be used, such as MSE or RMSE Additionally, regularization such as L1 and L2 as discussed in Chapter 9 (Computational Learning I) can be added to the loss function: \\[\\begin{align} SSE = \\sum_{ij \\in A} \\left(A_{ij} - \\hat{A}_{ij}\\right)^2 \\end{align}\\] For example: (SSE = sum((A - A.hat)^2)) ## [1] 76.52 If we choose to use K=3, we obtain the following SSE: k = 3 A.hat = M$u[,1:k ] %*% diag(M$d[1:k ]) %*% t(M$v[,1:k ]) (SSE = sum((A - A.hat)^2)) ## [1] 36.41 Notice that the sum squared error (SSE) is lesser, making it appropriate to choose K=3. Below is an example implementation of our cross-validation to determine which K to use based on SSE: xvalidate &lt;- function(A, M) { N = length(M$d) SSE = rep(Inf, N - 1) K = rep(0, N - 1) for (k in 2:N) { A.hat = M$u[,1:k ] %*% diag(M$d[1:k ]) %*% t(M$v[,1:k ]) SSE[k - 1] = sum((A - A.hat)^2) K[ k - 1] = k } list(&quot;SSE&quot; = SSE, &quot;K&quot; = K) } Now, let us plot (See Figure 11.28): Figure 11.28: Cross-Validation for (SVD) Excluding zeroes, the plot clearly shows that K=4 has the least SSE. The reconstructed matrix shows as: k = 4 A.hat = M$u[,1:k ] %*% diag(M$d[1:k ]) %*% t(M$v[,1:k ]) rownames(A.hat ) = rownames(A) colnames(A.hat ) = colnames(A) (A.hat = round(A.hat, 2)) ## Item1 Item2 Item3 Item4 Item5 ## User1 4.19 4.84 4.02 4.46 3.51 ## User2 4.47 3.41 0.62 4.03 4.55 ## User3 0.13 3.74 0.46 -0.20 4.58 ## User4 4.10 4.73 3.93 4.36 3.42 ## User5 0.13 4.25 5.46 -0.20 0.47 ## User6 3.69 4.96 3.26 0.32 2.83 ## User7 4.54 4.31 4.73 -0.06 0.14 ## User8 -0.03 4.45 4.29 4.35 3.79 ## User9 4.35 3.44 4.83 3.97 0.55 ## User10 -0.03 4.45 4.29 4.35 3.79 Now in terms of recommending a list of items, suppose we have a new user with the following rating: new.user = c(0,3,3,0,0) Two common measures are used in Recommender Systems, namely Pearson Correlation and Cosine Similarity. In our case, we use Cosine Similarity to illustrate (recalling the equation below). Here, we compare the similarity of user i and user j for all items. \\[\\begin{align} sim(\\vec{r_i}, \\vec{r_j}) = cos(\\vec{r_i}, \\vec{r_j}) = \\frac{\\vec{r_i} \\cdot \\vec{r_j}}{\\|\\vec{r_i}\\|_2 \\times \\|\\vec{r_j}\\|_2} \\end{align}\\] sim &lt;- function(ri, rj) { sum( ri * rj ) / ( sqrt( sum(ri^2) ) * sqrt( sum(rj^2) ) ) } Let us use our reconstructed matrix to determine if the first user and new user have similar interests based on the result of the Cosine Similarity like so: ri = first.user = A.hat[1,] rj = new.user (cos.sim = sim(ri, rj)) ## [1] 0.6628 We obtain a cosine similarity of 0.6628. A value closer to 1 means strong similarity, a value closer to -1 means opposite similarity, and a zero value indicates no similarity. Now, let us compare all other users against the new user and see which user is likely to have the same ratings as the new user. N = nrow(A.hat) similar = rep(0, N) for (i in 1:N) { similar[i] = sim(A.hat[i,], rj) } names(similar) = rownames(A.hat) sorted.users = sort(similar, decreasing=TRUE, index.return=TRUE) sorted.users$x ## User5 User7 User6 User8 User10 User9 User4 User1 User3 User2 ## 0.9895 0.8146 0.7702 0.7309 0.7309 0.6982 0.6629 0.6628 0.5003 0.3432 The result shows user (User5) as the top user with a cosine similarity of 0.9895 that has a seemingly similar interest as the new user. What could be the possible movies rated by the user? user.index = sorted.users$ix[1] sort(A.hat[user.index,], decreasing = TRUE) ## Item3 Item2 Item5 Item1 Item4 ## 5.46 4.25 0.47 0.13 -0.20 The result shows the items rated by the user (User5) ordered by the highest rating. We can recommend those items to the new user. Note that those ratings come from the reconstructed matrix (A.hat). "],["deeplearning1.html", "Chapter 12 Computational Deep Learning I 12.1 Simple Perceptron 12.2 Adaptive Linear Neuron (ADALINE) 12.3 Multi Layer Perceptron (MLP) 12.4 Convolutional Neural Network (CNN) ", " Chapter 12 Computational Deep Learning I We begin this chapter with an extension of Computational Learning, focusing on Computational Deep Learning. The term Computational is added in this literature to emphasize the computational nature of machine learning. We also now emphasize the term Deep in a new context. While there may be many definitions and descriptions of Deep Learning, the computation involves a Layered Network of nodes. On the other hand, it is unavoidable to encounter the common bigram Neural Network in reference to our biological network structure of neurons. Figure 12.1 shows a picture of a single neuron in the brain. Figure 12.1: Neuron In the figure, a single neuron takes an input as a neural stimulus that stimulates - whether to excite or inhibit. When a neuron is excited around a certain threshold in units of millivolts, it fires an action potential at the axon hillock. This action potential, also called nerve impulse, travels through the neuron to the axon terminals at the synapse - a junction where communication happens between neurons. At the synapse, neurotransmitters are emitted in the form of chemical molecules or electrical ions. The molecules enter receptors of another neuron, which takes them as input. Deep Learning follows a similar concept; thus, the term is associated with Neural network in which a message gets transmitted from neuron to neuron. The most basic premise from which deep learning thrives is the activation function - a function that takes the activation output of a hidden layer and translates it into a bounded output. See Figure 12.2. Figure 12.2: Simple Neuron The term activation is akin to the idea that neurons measure a metric called millivolts. There is a certain threshold to meet such that if the conditions are right, the neuron can trigger an action potential, eventually transmitting an output message to the next neuron. In the next section, let us discuss Neural Network starting with the most basic structure of a simple Perceptron. 12.1 Simple Perceptron A Perceptron is a simple single-layered Neural Network. An intuition behind Perceptron starts with a typical schema shown in Figure 12.3 based on Frank Rosenblatt’s machinery that he built in 1958 called Perceptron. Figure 12.3: Perceptron The schema can be expressed mathematically as a linear equation like so (where we use \\(\\omega_i\\) coefficients as weights in the equation). \\[\\begin{align} z = f(X) = \\sum_{j=1}^p \\omega_j x_j + \\omega_0 = \\omega_0 + \\omega_1 x_1 + \\omega_2 x_2\\ + ...\\ + \\omega_p x_p + \\epsilon \\end{align}\\] The intercept is denoted by the symbol \\(\\omega_0\\), which represents the bias. Disregarding the white noise (\\(\\epsilon\\)), we then have the approximation below: \\[\\begin{align} \\hat{z} = \\hat{f}(X) = \\sum_{j=1}^p \\omega_j x_j + \\omega_0 = \\omega_0 + \\omega_1 x_1 + \\omega_2 x_2\\ + ...\\ + \\omega_p x_p \\end{align}\\] From there, Perceptron feeds the result of the approximation, \\(\\hat{z}\\), into a binary step activation function which enforces the decision boundary or threshold. The schema uses a unit step function as our activation function if our target is binomial in the range (0,1); otherwise, we use a sign function if the target is in the range (-1, 1). \\[\\begin{align} \\hat{y} = \\underbrace{ \\begin{cases} 1 &amp; \\hat{z} &gt; 0\\\\ 0 &amp; otherwise \\end{cases} }_{\\text{heaviside step or unit step function}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{y} = \\underbrace{ sign(\\hat{z}) }_{\\text{sign function}} \\label{eqn:eqnnumber600} \\end{align}\\] Our goal is to be able to predict \\(\\hat{y}\\) as close to the target \\(y\\), which can be achieved if we can optimize the \\(\\omega\\) coefficients. To meet this goal, we train the Perceptron based on the few steps below: First, we initialize the \\(\\omega\\) coefficients with arbitrary values. For our purpose, we choose to start with zeroes: \\[ \\omega_j = 0,\\ \\ \\ \\ \\ \\ \\ for\\ j\\ in\\ 0,..,p \\] Second, we then compute for \\(\\hat{y}\\): \\[\\begin{align} \\hat{y} = step(\\hat{z}) = step\\left(\\sum_{j=1}^p \\omega_j x_j + \\omega_0 \\right) \\end{align}\\] Third, we then determine the close estimate of the coefficients by optimization (minimization) using the Perceptron learning rule expressed below (where \\(\\eta\\) is the learning rate): \\[\\begin{align} \\omega_j = \\omega_j + \\eta \\nabla \\omega_j \\mathcal{L} \\end{align}\\] \\[\\begin{align} where \\ \\ \\ \\ \\nabla \\omega_j \\mathcal{L}= \\begin{cases} (y_i - \\hat{y}_i) (1) &amp; if\\ j = 0,\\ then\\ x_{i,0} = 1 \\ \\ \\ \\ \\text{(bias)} \\\\ (y_i - \\hat{y}_i) (x_{i,j}) &amp; otherwise \\end{cases} \\label{eqn:eqnnumber601} \\end{align}\\] Fourth, we repeat the second and third steps until we reach a given maximum iteration. Below is our example implementation of a simple Perceptron: my.perceptron &lt;- function(x, y, epoch=50, eta = 0.01) { A &lt;- function(h) { ifelse(h &gt;= 0, 1, -1) } H &lt;- function(x, omega) { x %*% omega } # assume x0 = 1 n = length(y) error = rep(0, epoch) for (t in 1:epoch) { for (i in 1:n) { y.hat = A(H(x[i,], omega)) delta = c((y[i] - y.hat) ) omega = omega + eta * delta * x[i,] if (delta != 0.0) { error[t] = error[t] + 1 } } } list(&quot;coefficients&quot; = omega, &quot;error&quot; = error) } We set \\(x_0 = 1\\) and include in \\(X\\) so that we can just conveniently use a simple equation: \\[\\begin{align} \\hat{z} = X \\cdot \\omega\\ \\ \\ \\ \\leftarrow \\hat{z} = \\mathbf{(\\omega_0)} 1 + \\mathbf{(\\omega_1)} x_1 +\\ ... \\mathbf{(\\omega_p)} x_p \\end{align}\\] Before illustrating the use of our Perceptron implementation, let us first recall SVM. In Chapter 10 (Computational Learning II), we use our SVM implementation of PEGASOS to perform a supervised binary classification. We start using the following dataset below: set.seed(152) N = 20; v = 1 # variance x1.blue = rnorm(n=N, -2, v); x2.blue = rnorm(n=N, 2, v); y1 = rep( 1, N) x1.red = rnorm(n=N, 2, v); x2.red = rnorm(n=N, -2, v); y2 = rep(-1, N) x1 = c(x1.blue, x1.red); x2 = c(x2.blue, x2.red) x = cbind(x1, x2) y = cbind(c(y1, y2)) data = cbind(x, y) We then model our dataset using PEGASOS SVM like so: svm.pegasos.model = my.pegasos.svm(data) hplanes = svm.hyperplanes(svm.pegasos.model) The decision boundary along with the hyperplanes are shown in Figure 12.4. plot(NULL, xlim=range(x1), ylim=range(x2), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;PEGASOS SVM&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, pch=20, col=ifelse(y == -1, &quot;darksalmon&quot;, &quot;navyblue&quot;)) abline(a=hplanes$H0.int, b=hplanes$slope, lty=1, col=&quot;black&quot;, lwd=2) abline(a=hplanes$Hp.int, b=hplanes$slope, lty=2, col=&quot;blue&quot;) abline(a=hplanes$Hn.int, b=hplanes$slope, lty=2, col=&quot;red&quot;) Figure 12.4: PEGASOS SVM Now, let us take the same dataset to include the constant 1 for the intercept. At the same time, we initializeour coefficients to an arbitrary value, e.g., using zeroes for now. n = nrow(x) x0 = rep(1, n) # adding constant 1 for the intercept x.p = cbind(x0, x) omega = c(0, 0, 0) # initialize our coefficients head(x.p, n = 10) # display first 10 observations ## x0 x1 x2 ## [1,] 1 0.3216 2.8066 ## [2,] 1 -2.6603 1.4240 ## [3,] 1 -1.6949 3.0092 ## [4,] 1 -3.1667 1.7801 ## [5,] 1 -3.2775 0.4221 ## [6,] 1 -2.7310 2.4981 ## [7,] 1 -1.5136 -0.3908 ## [8,] 1 -2.7902 2.2235 ## [9,] 1 -2.6862 1.8131 ## [10,] 1 -1.7199 3.6344 Next, we then generate our model using our implementation of Perceptron (this gives an approximation of our \\(\\omega\\) coefficients along with the error count per iteration): (model = my.perceptron(x.p, y, epoch=3)) ## $coefficients ## x0 x1 x2 ## 0.00000 -0.04380 0.03507 ## ## $error ## [1] 1 1 0 Let us use the optimized coefficients to visualize the decision boundary. We concoct the points (\\(\\mathbf{x_1, x_2}\\)) that run through the decision boundary like so: p.x1 = seq(-3, 3, length.out=40) cof = model$coefficients b = cof[1]; w1 = cof[2]; w2 = cof[3] p.x2 = ( -w1 / w2 ) * p.x1 + b Finally, we plot the decision boundary based on Perceptron (see Figure 12.5). plot(NULL, xlim=range(x1), ylim=range(x2), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;Perceptron&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, pch=20, col=ifelse(y == -1, &quot;darksalmon&quot;, &quot;navyblue&quot;)) lines(p.x1, p.x2, col=&quot;black&quot;, lwd=2) Figure 12.5: Perceptron Model Notice that the decision boundary closely matches the same boundary line produced by implementing PEGASOS SVM. 12.2 Adaptive Linear Neuron (ADALINE) Bernard Widrow and Ted Hoff, in 1960, formulated a variant of Perceptron called Adaptive Linear Neuron (ADALINE). The schema is shown in Figure 12.6. Figure 12.6: Adaptive Linear Regression The ADALINE schema is almost similar to the Perceptron schema. However, the algorithm is slightly modified: First, we initialize the \\(\\omega\\) coefficients with arbitrary values. For that purpose, we choose to start with zeroes: \\[ \\omega_j = 0,\\ \\ \\ \\ \\ \\ \\ for\\ j\\ in\\ 0,..,p \\] Second, we then compute for \\(\\hat{y}\\). Notice that we omit the step function. Instead, our activation function is linear and thus is still identical to the hypothesis function; thus, our activation function becomes an identity function. \\[\\begin{align} \\hat{y} = \\hat{z} = \\sum_{j=1}^p \\omega_j x_j + \\omega_0 \\end{align}\\] Third, we then determine the close estimate of the coefficients by optimization (minimization) using the ADALINE learning rule expressed below (where \\(\\eta\\) is the learning rate): \\[\\begin{align} \\omega_j = \\omega_j + \\eta \\nabla \\omega_j \\mathcal{L} \\end{align}\\] \\[\\begin{align} where \\ \\ \\ \\ \\nabla \\omega_j \\mathcal{L}= \\begin{cases} (y_i - \\hat{y}_i) (1) &amp; if\\ j = 0,\\ then\\ x_{i,0} = 1 \\ \\ \\ \\ \\text{(bias)} \\\\ (y_i - \\hat{y}_i) (x_{i,j}) &amp; otherwise \\end{cases} \\label{eqn:eqnnumber602} \\end{align}\\] Fourth, we repeat the second and third steps until we reach a given maximum iteration. For each iteration, we calculate the error. In ADALINE, we can use any of the residual measures such as MSE, RMSE, or MAE for our loss function — of which the latter two are based on the sum of squared errors (SSE), also called residual sum of squares (RSS). For example: \\[\\begin{align} \\mathcal{L}\\left\\{ MSE = \\frac{1}{N} \\sum(y - \\hat{y})^2\\ \\ \\ \\ \\ RMSE = \\sqrt{\\frac{1}{N} \\sum (y - \\hat{y})^2}\\ \\ \\ \\ MAE = \\frac{1}{N} \\sum|y - \\hat{y}| \\right\\} \\end{align}\\] Below is our example implementation of ADALINE: my.adaline &lt;- function(x, y, epoch=50, eta = 0.01, tol=1e-3) { A &lt;- function(h) { ifelse(h &gt;= 0, 1, -1) } H &lt;- function(x, omega) { x %*% omega } # assume x0 = 1 n = length(y) y.hat = rep(0, n) mse = NULL for (t in 1:epoch) { sse = 0 for (i in 1:n) { y.hat[i] = H(x[i,], omega) delta = (c(y[i] - y.hat[i])) omega = omega + eta * delta * x[i,] sse = sse + delta^2 } mse[t] = sse / n if (t &gt; 1 &amp;&amp; ( mse[t - 1] - mse[t] &lt;= tol)) break } list(&quot;coefficients&quot; = omega, &quot;error&quot; = mse) } omega = c(0, 0, 0) # initialize our coefficients (model = my.adaline(x.p, y, epoch=50)) ## $coefficients ## x0 x1 x2 ## 0.02996 -0.29061 0.19347 ## ## $error ## [1] 0.2534 0.1191 0.1182 Let us use the optimized coefficients to visualize the decision boundary. We concoct the points (\\(\\mathbf{x_1, x_2}\\)) that run through the decision boundary like so: p.x1 = seq(-3, 3, length.out=40) cof = model$coefficients b = cof[1]; w1 = cof[2]; w2 = cof[3] p.x2 = ( -w1 / w2 ) * p.x1 + b Finally, we plot the decision boundary based on ADALINE (see Figure 12.7). plot(NULL, xlim=range(x1), ylim=range(x2), ylab=&quot;x2&quot;, xlab=&quot;x1&quot;, main=&quot;ADALINE&quot;) grid(lty=3, col=&quot;lightgrey&quot;) points(x, pch=20, col=ifelse(y == -1, &quot;darksalmon&quot;, &quot;navyblue&quot;)) lines(p.x1, p.x2, col=&quot;black&quot;, lwd=2) Figure 12.7: ADALINE Model In the figure, the decision boundary closely matches the same boundary line produced by our simple PERCEPTRON implementation. 12.3 Multi Layer Perceptron (MLP) We now delve deep into MLP, one of the many essential topics in Deep Learning. The fundamental inner workings of MLP serve as a necessary precursor following other advanced neural networks such as Convolutional Neural Network and Recurrent Neural Network. From a simple Perceptron, let us add a bit of complexity using Figure 12.8 to demonstrate a two-layer Neural Network in which each layer is a composite of Simple Perceptrons. Note that whereas Perceptrons use step functions, let us use sigmoid functions. Hereafter, if we use the acronym MLP, we refer to a vanilla Multi-Layer Neural Network. There are two hidden layers in the network, each consisting of two perceptrons, followed by an output layer. Figure 12.8: Multi-Layer Perceptron The schema demonstrates a Multi-Layered Perceptron that has the property of a complete Neural Network. That means that every feature input connects to every neuron in each layer. Moreover, every neuron in each layer connects to all other neurons of the next layer. Therefore, the number of parameters (or \\(\\omega\\) coefficients) of a complete Neural Network is calculated. For example, suppose we have four hundred input features, three hidden layers of five neurons each, and two outputs in the output layer ( granting we consider full bias - each input or hidden layer has a bias), then we are looking at the following computation: \\[ \\begin{array}{ll} \\text{number of }&amp;\\text{parameters (}\\omega\\ \\text{coefficients)} \\\\ &amp;= \\text{no. of features} \\times \\text{no. of neuron in HL1} \\\\ &amp;+ \\text{ no. of neuron in HL1} \\times \\text{no. of neuron in HL2}\\\\ &amp;+ \\text{ no. of neuron in HL2} \\times \\text{no. of neuron in HL3}\\\\ &amp;+ \\text{ no. of neuron in HL3} \\times \\text{no. of output}\\\\ &amp;= 400 \\times 5 + 5 \\times 5 + 5 \\times 5 + 5 \\times 2\\\\ &amp;= 2060\\\\ \\\\ \\text{number of }&amp;\\text{parameters (bias)} \\\\ &amp;= \\text{no. of features} \\times \\text{no. of neuron in HL1} \\\\ &amp;+ \\text{ no. of neuron in HL1} \\times \\text{no. of neuron in HL2}\\\\ &amp;+ \\text{ no. of neuron in HL2} \\times \\text{no. of neuron in HL3}\\\\ &amp;+ \\text{ no. of neuron in HL3} \\times \\text{no. of output}\\\\ &amp;= 1\\times 5 + 1 \\times 5 + 1 \\times 5 +1 \\times 2\\\\ &amp;= 17 \\end{array} \\] The number of parameters becomes too large as we accommodate more input features, more neurons, and more hidden layers. As we recall in Chapter 9 (Computational Learning I), we have gone through a review of Exploratory Data Analysis (EDA) that allows us to reduce the dimensionality of our input. Unfortunately, as we begin to go deeper into neural networks and work on images, feature extraction may not be that simple. Nonetheless, we cover Convolutional Neural Network (CNN) up ahead, which performs feature extraction. There are ways to reduce parameters in CNN, as we shall see later. Using Figure 12.8, let us demonstrate three major aspects of modeling an MLP. The Forward Pass (or Forward Feed) - each input data passes through each layer to the output layer. The BackPropagation - each connection that input data passes through carries an initial arbitrary weight that needs to be optimized. The choice of optimization method is a consideration to make. Here, we use the common Gradient Descent. The result produced by the output layer may not necessarily match the target immediately, and so it takes a sufficient iteration to meet a close match. This mismatch error needs to be propagated back through the network. We can use sum squared error for our loss function to calculate the error that we need to propagate. For this, we require the loss function to be differentiable with respect to each weight. The Backward Pass (or Backward Feed) - for Optimization to happen, we need to re-calculate the weights of all connections. This is where we implement our update rule. 12.3.1 Forward Feed In the Forward Feed (also called Forward Pass) portion, our first hidden layer performs a dot product of the input \\(x_{(i:p)}\\) and the weights \\(\\omega_{(i:p,1:2)}\\), and then it feeds the result to a sigmoid function. Our choice of activation function is scaled between 0 and 1. Here, p represents the dimensionality of our model, e.g., the number of features. \\[\\begin{align*} \\begin{array}{ll} \\hat{z}_1 = \\sum_{i=0}^{p=2} \\omega_{(i,1)} x_{i}\\ \\ \\ \\ \\ z_1 = \\text{sigmoid}\\left( \\hat{z}_1 \\right)\\\\ \\hat{z}_2 = \\sum_{i=0}^{p=2} \\omega_{(i,2)} x_{i}\\ \\ \\ \\ \\ z_2 = \\text{sigmoid}\\left( \\hat{z}_2 \\right)\\\\ \\end{array} \\ \\ \\ where\\ \\ x_0 = 1\\ \\ \\text{(constant used for bias)} \\end{align*}\\] Note that for the sake of notation, let us consider \\(\\hat{z}_1\\) and \\(\\hat{z}_2\\) to be the net input and \\(z_1\\) and \\(z_2\\) to be the activation output. Also, note that for a given constant equal to 1, we have chosen to have a separate bias weight for each connection to a perceptron; otherwise, if we have a fixed weight for bias, we also can use the below formula: \\[\\begin{align*} \\begin{array}{ll} \\hat{z}_1 = \\sum_{i=1}^{p=2} \\omega_{(i,1)} x_{i} + \\omega_0\\ \\ \\ \\ \\ z_1 = \\text{sigmoid}\\left( \\hat{z}_1 \\right)\\\\ \\hat{z}_2 = \\sum_{i=1}^{p=2} \\omega_{(i,2)} x_{i} + \\omega_0\\ \\ \\ \\ \\ z_2 = \\text{sigmoid}\\left( \\hat{z}_2 \\right)\\\\ \\end{array} \\ \\ \\ where\\ \\ x_0 = 1\\ \\ \\text{(constant for bias)} \\end{align*}\\] In this first hidden layer, we have six parameters to optimize. Then, our second hidden layer performs a dot product of the output from the first hidden layer and the weights \\(\\alpha_{(i:p,1:2)}\\), and then it feeds the result to a sigmoid function. \\[\\begin{align*} \\begin{array}{ll} \\hat{h}_1 = \\sum_{i=0}^{p=2} \\alpha_{(i,1)} z_{i}\\ \\ \\ \\ \\ h_1 = \\text{sigmoid}\\left( \\hat{h}_1 \\right)\\\\ \\hat{h}_2 = \\sum_{i=0}^{p=2} \\alpha_{(i,2)} z_{i}\\ \\ \\ \\ \\ h_2 = \\text{sigmoid}\\left( \\hat{h}_2 \\right)\\\\ \\end{array} \\ \\ where\\ \\ z_0 = 1\\ \\ \\text{(constant used for bias)} \\end{align*}\\] Let us also consider \\(\\hat{h}_1\\) and \\(\\hat{h}_2\\) to be the net input and \\(h_1\\) and \\(h_2\\) to be the activation output. In this second hidden layer, we have six parameters to optimize. Finally, our output layer performs a dot product of the output from the second hidden layer and the weights \\(\\varphi_{(i:p,1:2)}\\), and then it feeds the result to a sigmoid function. \\[\\begin{align*} \\begin{array}{ll} \\hat{o}_1 = \\sum_{i=0}^{p=2} \\varphi_{(i,1)} h_{i}\\ \\ \\ \\ \\ o_1 = \\text{sigmoid}\\left( \\hat{o}_1 \\right)\\\\ \\hat{o}_2 = \\sum_{i=0}^{p=2} \\varphi_{(i,2)} h_{i}\\ \\ \\ \\ \\ o_2 = \\text{sigmoid}\\left( \\hat{o}_2 \\right)\\\\ \\end{array} \\ \\ where\\ \\ h_0 = 1\\ \\ \\text{(constant used for bias)} \\end{align*}\\] In this output layer, we have six parameters to optimize. 12.3.2 Backward Feed In the Backward Feed (also called Backward Pass) portion, we formulate our update function for our weight parameters like so (where \\(\\eta\\) is the learning rate between 0 and 1): \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\eta \\nabla \\omega \\mathcal{L} \\end{align}\\] That is true for all weights. We then iterate multiple times until our loss function is minimized. For that to happen, we need to optimize a total of 18 parameters corresponding to the 18 neural connections: \\[ params = \\underbrace{( 2 \\times 2 + 2 \\times 2 + 2 \\times 2 )}_{\\text{feature inputs}} + \\underbrace{( 1 \\times 2 + 1 \\times 2 + 1 \\times 2 )}_{\\text{bias}} = 18 \\] Before we provide an example of optimizing the parameters, let us first discuss BackPropagation in the next section. 12.3.3 BackPropagation To illustrate the concept of Backpropagation, we have to recall Partial Differentiation covered in Chapter 4 (Numerical Calculus) and the concept of Chain rule and Delta rule in the context of Backpropagation. Chain rule Let us use Figure 12.9 to discuss the Chain rule. Figure 12.9: BackPropagation We start with the idea that each neuron in the Neural Network contributes an effect to another Neuron from one layer to the next. The diagram shows how neuron a in layer H1, for example, contributes indirectly to neuron o in the Output Layer. Mathematically, the contribution or effect of a to o can be expressed as \\(\\frac{\\partial o}{\\partial a}\\). To determine the effect of a to o, we first need to know the effect of a to d, the effect of d to g, and the effect of g to o. \\[ a\\rightarrow d\\rightarrow g\\rightarrow o \\] Mathematically, it is written as: \\[\\begin{align} \\frac{\\partial o}{\\partial a} = \\left(\\frac{\\partial o}{\\partial g} \\right) \\left(\\frac{\\partial g}{\\partial d} \\right) \\left(\\frac{\\partial d}{\\partial a} \\right) \\end{align}\\] However, notice in Figure 12.9 that there is no direct connection from a to d nor from d to g. To get to d from a, we can either go to b first or c first. Similarly, to get to g from d, we can either go to e or f first. \\[\\begin{align} \\left(\\frac{\\partial d}{\\partial a} \\right) = \\left(\\frac{\\partial d}{\\partial b} \\right) \\left(\\frac{\\partial b}{\\partial a} \\right) \\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ \\left(\\frac{\\partial d}{\\partial a} \\right) = \\left(\\frac{\\partial d}{\\partial c} \\right) \\left(\\frac{\\partial c}{\\partial a} \\right) \\end{align}\\] \\[\\begin{align} \\left(\\frac{\\partial g}{\\partial d} \\right) = \\left(\\frac{\\partial g}{\\partial e} \\right) \\left(\\frac{\\partial e}{\\partial d} \\right) \\ \\ \\ \\ \\ or \\ \\ \\ \\ \\ \\left(\\frac{\\partial g}{\\partial d} \\right) = \\left(\\frac{\\partial g}{\\partial f} \\right) \\left(\\frac{\\partial f}{\\partial d} \\right) \\end{align}\\] Because we are interested in the effect of a to d and d to g, we can combine the effects of both paths like so: \\[\\begin{align} \\left(\\frac{\\partial d}{\\partial a} \\right) = \\left(\\frac{\\partial d}{\\partial b} \\right) \\left(\\frac{\\partial b}{\\partial a} \\right) + \\left(\\frac{\\partial d}{\\partial c} \\right) \\left(\\frac{\\partial c}{\\partial a} \\right) \\end{align}\\] \\[\\begin{align} \\left(\\frac{\\partial g}{\\partial d} \\right) = \\left(\\frac{\\partial g}{\\partial e} \\right) \\left(\\frac{\\partial e}{\\partial d} \\right) + \\left(\\frac{\\partial g}{\\partial f} \\right) \\left(\\frac{\\partial f}{\\partial d} \\right) \\end{align}\\] Therefore, to get from a to o, we have the following complete Chain rule equation: \\[\\begin{align} \\frac{\\partial o}{\\partial a} = \\left(\\frac{\\partial o}{\\partial g} \\right) \\left(\\frac{\\partial g}{\\partial d} \\right) \\left(\\frac{\\partial d}{\\partial a} \\right) = \\left(\\frac{\\partial o}{\\partial g} \\right) \\left( \\left[ \\frac{\\partial g}{\\partial e} \\frac{\\partial e}{\\partial d} \\right] + \\left[ \\frac{\\partial g}{\\partial f} \\frac{\\partial f}{\\partial d} \\right] \\right) \\left( \\left[ \\frac{\\partial d}{\\partial b} \\frac{\\partial b}{\\partial a} \\right] + \\left[ \\frac{\\partial d}{\\partial c} \\frac{\\partial c}{\\partial a} \\right] \\right) \\end{align}\\] Delta rule Our ultimate goal in Backpropagation is to optimize the weights or parameters of a Neural Network. In Figure 12.9, the Neural Network in the diagram shows that there are 229 parameters to optimize. For optimization, we need to know first the effect of each parameter on the overall performance of the MLP model. This effect can be measured in terms of comparing the output of the model against the actual target using the following error formula denoted by the epsilon symbol (\\(\\epsilon\\)): \\[\\begin{align} \\epsilon = (\\text{o - t}) = -(\\text{t - o})\\ \\ \\ \\ \\ where\\ \\ \\ \\mathbf{t} = target\\ \\ and\\ \\ \\mathbf{o} = output \\end{align}\\] From there, we can formulate our total loss function, which is required to be differentiable. Note that the \\(\\frac{1}{2}\\) is added for mathematical convenience (e.g., ease of differentiation). \\[\\begin{align} \\mathcal{L}_{(total)} = \\frac{1}{2} \\sum_{i=1}^m \\left(t_i - o_i\\right)^2 = \\underbrace{\\frac{1}{2}\\left(t_1 - o_1\\right)^2}_{\\mathcal{L}_{o_1}} + \\underbrace{\\frac{1}{2}\\left(t_2 - o_2\\right)^2 }_{\\mathcal{L}_{o_2}} +\\ ...+\\ \\underbrace{\\frac{1}{2}\\left(t_m - o_m\\right)^2 }_{\\mathcal{L}_{o_m}} \\end{align}\\] Ideally, we want the total loss function to produce zero error. That can only be achieved if we can optimize the effect of each weight to the loss function. To calculate the effect of a weight (\\(\\omega\\)) to the loss function, we use the following notation: \\(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega}\\). For example, to determine the effect of a weight, e.g., \\(\\omega_{10}\\) between layer H5 and the Output layer to the loss function, we can form the following equation: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{10}} = \\sum_{o=i} \\frac{\\partial \\mathcal{L}_{o}}{\\partial \\omega_{10}} \\end{align}\\] To determine the effect to the loss function of a weight, e.g. \\(\\omega_{8}\\) between layer H4 and the Output layer, we can form the following equation: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{8}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_5} \\frac{\\partial H_5}{\\partial \\hat{H}_5} \\frac{\\partial \\hat{H}_5}{\\partial \\omega_{8}} \\end{align}\\] Now, to determine the effect to the loss function with respect to a weight between the Input layer and the Output layer, we can form the following equation: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{1}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_5} \\frac{\\partial H_5}{\\partial \\hat{H}_5} \\frac{\\partial \\hat{H}_5}{\\partial H_4} \\frac{\\partial H_4}{\\partial \\hat{H}_4} \\frac{\\partial \\hat{H}_4}{\\partial H_3} \\frac{\\partial H_3}{\\partial \\hat{H}_3} \\frac{\\partial \\hat{H}_3}{\\partial H_2} \\frac{\\partial H_2}{\\partial \\hat{H}_2} \\frac{\\partial \\hat{H}_2}{\\partial H_1} \\frac{\\partial H_1}{\\partial \\hat{H}_1} \\frac{\\partial \\hat{H}_{1}}{\\partial \\omega_{1}} \\end{align}\\] As we can imagine, the equation gets longer as the number of layers increases. However, algorithmically, there is a better way to express the equation. It is where the Delta rule comes into the picture. In the equation, we notice three contributions. First, let us momentarily use Figure 12.8 to review the contribution of a net input to an activation output. Both net input and activation output are part of a neuron. Each of the neuron in hidden layers comes with two variables, namely the net input and the activation output. In fact, the activation output is called the activation output obtained from the activation function. In our case, we are using the sigmoid function. We can get the contribution of \\(\\mathbf{\\hat{h}_2}\\) to \\(\\mathbf{h_2}\\) by differentiating the sigmoid function. \\[\\begin{align} a(z) = sigmoid(f(x)) = \\frac{1}{1 + exp(-z)}\\ \\ \\ \\rightarrow a&#39;(z) = a(z) ( 1 - a(z)) \\ \\ where\\ z = f(x) \\end{align}\\] Because \\(\\mathbf{h_2}\\) is the activation output of a sigmoid function, we can therefore obtain the derivative of the function with respect to the net input, namely \\(\\mathbf{\\hat{h}_2}\\): \\[\\begin{align} f(x) = \\hat{h}_2 = 1 \\times \\alpha_{0,2} + z_1 \\times \\alpha_{1,2} + z_2 \\times \\alpha_{2,2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathbf{h_2}}{\\partial \\mathbf{\\hat{h}_2}} = \\mathbf{h_2} ( 1 - \\mathbf{h_2} ) \\end{align}\\] Note that the full derivation of the derivative is excluded. Note also that other activation function of choice has different derivatives. Second, we also have the contribution of individual weights to the net input, e.g. \\(\\frac{\\partial \\hat{h_2}}{\\partial \\alpha_{2,2}}\\). Given \\(f(x) = \\hat{h}_2\\), we can obtain the derivative of the net input with respect to a given weight. Similarly, we also can obtain the derivative of the weight given a net input): \\[\\begin{align} \\frac{\\partial \\hat{h_2}}{\\partial \\alpha_{2,2}} = z_2 \\times {\\alpha_{2,2}}^{(1-1)} = \\mathbf{z_2} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\hat{h_2}}{\\partial x_2} = {\\mathbf{z_2}}^{(1-1)} \\times {\\mathbf{\\alpha_{2,2}}} = \\mathbf{\\alpha_{2,2}} \\end{align}\\] Third, we have the contribution of the activation output to the total loss function, e.g. \\(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial o}\\). We can then differentiate the total loss function with respect to a given output so that given the equation below: \\[\\begin{align} \\mathcal{L_{(total)}} = 2 \\frac{1}{2} \\left(t_1 - o_1\\right)^{2-1} + 2 \\frac{1}{2} \\left(t_2 - o_2\\right)^{2-1} +\\ ...\\ +\\ 2 \\frac{1}{2} \\left(t_m - o_m\\right)^{2-1} \\end{align}\\] we obtain the following derivatives: \\[\\begin{align} \\frac{ \\partial \\mathcal{L_{(total)}} } {\\partial o_1}= -\\underbrace{ \\left(t_1 - o_1\\right) }_{\\partial \\mathcal{L_1}} \\ \\ \\ where\\ \\ \\ \\ \\partial \\mathcal{L}_1 = \\epsilon_1 \\end{align}\\] \\[\\begin{align} \\frac{ \\partial \\mathcal{L_{(total)}} } {\\partial o_2} = -\\underbrace{ \\left(t_2 - o_2\\right) }_{\\partial \\mathcal{L_2}} \\ \\ \\ where\\ \\ \\ \\ \\partial \\mathcal{L}_2 = \\epsilon_2 \\end{align}\\] Fourth, the contribution of an activation output to the total loss function, e.g. \\(\\left(\\frac{\\partial L_{(total)}}{\\partial h_2} \\right)\\), can be decomposed into the following: \\[\\begin{align} \\left(\\frac{\\partial L_{(total)}}{\\partial h_2} \\right) = \\left(\\frac{\\partial L_1}{\\partial h_2} + \\frac{\\partial L_2}{\\partial h_2}\\right) \\end{align}\\] where: \\[\\begin{align} \\frac{\\partial L_1}{\\partial h_2} = \\left(\\frac{\\partial L_1}{\\partial o_1}\\right) \\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1}\\right) \\left(\\frac{\\partial \\hat{o}_1}{\\partial h_2}\\right) = \\underbrace{-(t_1 - o_1) \\times o_1 ( 1 - o_1)}_{\\delta_1\\leftarrow \\text{delta rule}} \\times \\varphi_{2,1} \\end{align}\\] \\[\\begin{align} \\frac{\\partial L_2}{\\partial h_2} = \\left(\\frac{\\partial L_2}{\\partial o_2}\\right) \\left(\\frac{\\partial o_2}{\\partial \\hat{o}_2}\\right) \\left(\\frac{\\partial \\hat{o}_2}{\\partial h_2}\\right) = \\underbrace{-(t_2 - o_2) \\times o_2 ( 1 - o_2)}_{\\delta_2\\leftarrow \\text{delta rule}} \\times \\varphi_{2,2} \\end{align}\\] Therefore, \\[\\begin{align} \\left(\\frac{\\partial L_{(total)}}{\\partial h_2} \\right) &amp;= \\underbrace{-(t_1 - o_1) \\times o_1 ( 1 - o_1)}_{\\delta_1\\leftarrow \\text{delta rule}} \\times \\varphi_{2,1} + \\underbrace{-(t_2 - o_2) \\times o_2 ( 1 - o_2)}_{\\delta_2\\leftarrow \\text{delta rule}} \\times \\varphi_{2,2}\\\\ &amp;= \\sum_{y=1}^{Y=2}\\delta_y \\varphi_{2,y} \\end{align}\\] Lastly, if we then combine all three contributions (or derivatives), we end up with the following equation for the example in Figure 12.8. \\[\\begin{align} \\frac{\\partial L_{(total)}}{\\partial \\alpha_{2,2}} = \\left(\\frac{\\partial L_{(total)}}{\\partial h_2} \\right) \\left(\\frac{\\partial h_2}{\\partial \\hat{h}_2} \\right) \\left(\\frac{\\partial \\hat{h}_2}{\\partial \\alpha_{2,2}} \\right) = \\underbrace{\\sum_{y=1}^{Y=2}\\delta_y \\varphi_{2,o} \\times o_2 (1 - o_2)}_{\\delta_h \\leftarrow \\text{delta rule}} \\times z_2 = \\delta_h z_2 \\end{align}\\] Now, back to Figure 12.9 as reference, having the Delta rules in mind, let us figure out how to solve for \\(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{i,q}}\\) where i is the ith feature (including constant) in the Input Layer and q is the qth neuron in the next layer. First, we deal with the effect of a neuron to the total loss function. Consider L=5 such that \\(H_5 = H_{L}\\). We then have: \\[\\begin{align} \\left\\{ \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L,k)}} = \\sum_{y=1} \\frac{\\partial \\mathcal{L}_y}{\\partial H_{(L,k)}} = \\sum_{y=1} \\delta_{y}\\times \\omega_{(L, k)} \\right\\}_{k=1}^K \\end{align}\\] where: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_y}{\\partial H_{(L,k)}}= \\underbrace{ \\frac{\\partial \\mathcal{L}_y}{\\partial O_y} \\frac{\\partial O_y}{\\partial \\hat{O}_y} }_{\\delta_y} \\frac{\\partial \\hat{O}_y}{\\partial H_{(L,k)}} = \\underbrace{-(t_y - o_y)\\times o_y(1 - o_y)}_{\\delta_{y}} \\times \\omega_{(L, k)} \\end{align}\\] Here, L is the H5 layer index with K neurons. The indices are used such that \\(\\mathbf{H_{(5, 3)}}\\) refers to the third neuron in the H5 layer and \\(\\mathbf{\\omega_{(5, 2)}}\\) refers to the second weight from H5 layer. Second, we deal with the effect of the activation output with respect to the net input. \\[\\begin{align} \\left\\{ \\frac{\\partial H_{(L,k)}}{\\partial \\hat{H}_{(L,k)}} = H_{(L,k)} \\left(1 - H_{(L,k)}\\right) \\right\\}_{k=1}^K \\end{align}\\] Third, we deal with the effect of neuron’s output to another neuron’s activation output: \\[\\begin{align} \\left\\{ \\frac{\\partial \\hat{H}_{(L,k)}}{\\partial H_{(L-1,m)}} = \\frac{\\partial \\left(\\sum_{j=1} \\omega_{(L-1, j)} H_{(L-1,j)}\\right)}{\\partial H_{(L-1,m)}} = \\omega_{(L-1, m)} \\right\\}_{m=1}^{M} \\end{align}\\] Assume there are M neurons in H4 layer denoted by \\(H_{(L-1, m)}\\). Fourth, if we combine the first three factors to affect the total loss with respect to the weight (in H4 layer), we get the following: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{(L-1,m)}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L,k)}} \\frac{\\partial H_{(L,k)}}{\\partial \\hat{H}_{(L,k)}} \\frac{\\partial \\hat{H}_{(L,k)}}{\\partial \\omega_{(L-1,m)}} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{(L-1,m)}} = \\underbrace{ \\left[ \\sum_{k=1} \\left(\\sum_{y=1} \\delta_{y}\\times \\omega_{(L, k)} \\right) \\times H_{(L,k)} (1 - H_{(L,k)}) \\right] }_{\\delta_{(L,k)}} \\times H_{(L-1,m)} \\end{align}\\] Fifth, similarly, if we combine the first three factors for the effect to the total loss with respect to the activation output, we get the following equation instead: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-1,m)}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L,k)}} \\frac{\\partial H_{(L,k)}}{\\partial \\hat{H}_{(L,k)}} \\frac{\\partial \\hat{H}_{(L,k)}}{\\partial \\omega_{(L-1,m)}} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-1,m)}} = \\underbrace{ \\left[ \\sum_{k=1} \\left(\\sum_{y=1} \\delta_{y}\\times \\omega_{(L, k)} \\right) \\times H_{(L,k)} (1 - H_{(L,k)}) \\right] }_{\\delta_{(L,k)}} \\times \\omega_{(L-1,m)} \\end{align}\\] Sixth, moving one more layer back (e.g. H3 layer) and using the equation below to expand further: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-1,m)}} = \\delta_{(L,k)} \\times \\omega_{(L-1,m)} \\end{align}\\] we get: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{(L-2,n)}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L,k)}} \\frac{\\partial H_{(L,k)}}{\\partial \\hat{H}_{(L,k)}} \\frac{\\partial \\hat{H}_{(L,k)}}{\\partial H_{(L-1,m)}} \\frac{\\partial H_{(L-1,m)}}{\\partial \\hat{H}_{(L-1,m)}} \\frac{\\partial \\hat{H}_{(L-1,m)}}{\\partial \\omega_{(L-2,n)}} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-2,n)}} = \\underbrace{ \\left[ \\sum_{m=1} \\left(\\sum_{k=1} \\delta_{_{(L,k)}}\\times \\omega_{(L-1, m)} \\right) \\times H_{(L-1,m)} (1 - H_{(L-1,m)}) \\right] }_{\\delta_{(L-1,m)}} \\times \\omega_{(L-2,n)} \\end{align}\\] Assume there are N neurons in H3 layer denoted by \\(H_{(L-2, n)}\\). Seventh, moving even further back, for another layer (e.g. H2 layer) in which: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{(L-3,o)}} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L,k)}} \\frac{\\partial H_{(L,k)}}{\\partial \\hat{H}_{(L,k)}} \\frac{\\partial \\hat{H}_{(L,k)}}{\\partial H_{(L-1,m)}} \\frac{\\partial H_{(L-1,m)}}{\\partial \\hat{H}_{(L-1,m)}} \\frac{\\partial \\hat{H}_{(L-1,m)}}{\\partial H_{(L-2,n)}} \\frac{\\partial H_{(L-2,n)}}{\\partial \\hat{H}_{(L-2,n)}} \\frac{\\partial \\hat{H}_{(L-2,n)}}{\\partial H_{(L-3,o)}} \\end{align}\\] we get: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-3,o)}} = \\underbrace{ \\left[ \\sum_{n=1} \\left(\\sum_{m=1} \\delta_{_{(L-1,m)}}\\times \\omega_{(L-2, n)} \\right) \\times H_{(L-2,n)} (1 - H_{(L-2,n)}) \\right] }_{\\delta_{(L-2,n)}} \\times \\omega_{(L-3,o)} \\end{align}\\] Assume there are O neurons in H2 layer denoted by \\(H_{(L-3, o)}\\). Eight, and for the H1 layer, we get: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial H_{(L-4,p)}} = \\underbrace{ \\left[ \\sum_{p=1} \\left(\\sum_{n=1} \\delta_{_{(L-2,n)}}\\times \\omega_{(L-3, o)} \\right) \\times H_{(L-3,o)} (1 - H_{(L-3,o)}) \\right] }_{\\delta_{(L-3,o)}} \\times \\omega_{(L-4,p)} \\end{align}\\] Assume there are P neurons in H1 layer denoted by \\(H_{(L-4, p)}\\). Equivalently, we get: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{(L-4,p)}} = \\underbrace{ \\left[ \\sum_{p=1} \\left(\\sum_{n=1} \\delta_{_{(L-2,n)}}\\times \\omega_{(L-3, o)} \\right) \\times H_{(L-3,o)} (1 - H_{(L-3,o)}) \\right] }_{\\delta_{(L-3,o)}} \\times H_{(L-4,p)} \\end{align}\\] Simplifying the notation, we get: \\[\\begin{align} \\nabla \\omega_{(L-4, p)} = \\delta_{(L-3, o)} \\times H_{(L-4,p)} \\end{align}\\] Finally, for the Input Layer, having gone through all the example equations above, we get the final effect of the Input Layer to the Output Layer like so: \\[\\begin{align} \\nabla \\omega_{(I, q)} = \\delta_{(L-4, p)} \\times H_{(I,q)} \\end{align}\\] Assume there are Q input features in the Input layer denoted by \\(H_{(I, q)}\\). So far, our equations work only on a single sample. In the next section, we continue to discuss MLP further by example. Then, another section follows with the implementation, focusing on multiple samples. 12.3.4 MLP Example Here, we continue to use Figure 12.8. Suppose we have the following data points, initial weights, and biases (see Figure 12.10): Figure 12.10: Multiple Perceptron (Data Points) Note that our inputs in the table include the biases. Also, our bias constant connects to neurons in different layers with unique weights. Our goal is to update the 18 (weight) parameters. Here, we show the steps in solving only three parameters as an example. First, for the Forward Pass, we calculate the net input and output of each layer. Starting with the Z layer, we have the net input as: \\[\\begin{align*} \\hat{z} = \\left[\\begin{array}{rrr} 1 &amp; 0.12 &amp; 0.18 \\end{array}\\right]_{X} \\cdot \\left[\\begin{array}{rr} 0.05 &amp; 0.40 \\\\ 0.21 &amp; 0.34 \\\\ 0.19 &amp; 0.67 \\end{array}\\right]_{\\omega} = \\left[\\begin{array}{rr} 0.1094 &amp; 0.5614 \\end{array}\\right] \\end{align*}\\] and activation output as: \\[\\begin{align*} z = sigmoid( \\left[\\begin{array}{rr} 0.1094 &amp; 0.5614 \\end{array}\\right]) = \\left[\\begin{array}{rr} 0.52732275 &amp; 0.63677641 \\end{array}\\right] \\end{align*}\\] Note, hereafter, that we use a precision of 8 digits in our examples. For H layer, we have the net input as: \\[\\begin{align*} \\hat{h} &amp;= \\left[\\begin{array}{rrr} 1 &amp; 0.52732275 &amp; 0.63677641 \\end{array}\\right]_{Z} \\cdot \\left[\\begin{array}{rr} 0.18 &amp; 0.27 \\\\ 0.09 &amp; 0.06 \\\\ 0.30 &amp; 0.15 \\end{array}\\right]_{\\alpha}\\\\ &amp;= \\left[\\begin{array}{rr} 0.41849197 &amp; 0.39715583 \\end{array}\\right] \\nonumber \\end{align*}\\] and activation output as: \\[\\begin{align*} h = sigmoid( \\left[\\begin{array}{rr} 0.41849197 &amp; 0.39715583 \\end{array}\\right]) = \\left[\\begin{array}{rr} 0.60312234 &amp; 0.59800413 \\end{array}\\right] \\end{align*}\\] For Output layer, we have the net input as: \\[\\begin{align*} \\hat{o} &amp;= \\left[\\begin{array}{rrr} 1 &amp; 0.60312234 &amp; 0.59800413 \\end{array}\\right]_{H} \\cdot \\left[\\begin{array}{rr} 0.25 &amp; 0.05 \\\\ 0.03 &amp; 0.35 \\\\ 0.04 &amp; 0.40 \\end{array}\\right]_{\\varphi}\\\\ &amp;= \\left[\\begin{array}{rr} 0.29201384 &amp; 0.50029447 \\end{array}\\right] \\nonumber \\end{align*}\\] and activation output as: \\[\\begin{align*} o = sigmoid( \\left[\\begin{array}{rr} 0.29201384 &amp; 0.50029447 \\end{array}\\right]) = \\left[\\begin{array}{rr} 0.57248908 &amp; 0.62252853 \\end{array}\\right] \\end{align*}\\] Second, for the Backpropagation, we calculate our loss function: \\[\\begin{align} \\mathcal{L}_{(total)} &amp;= \\frac{1}{2}\\sum_{i=1}^m( t_i - o_i)^2 \\\\ &amp;= \\frac{1}{2}( t_1 - o_1)^2 + \\frac{1}{2}( t_2 - o_2)^2\\\\ &amp; = \\frac{1}{2} (0.05 - 0.57248908)^2 + \\frac{1}{2} (0.95 - 0.62252853)^2 \\nonumber \\\\ &amp; = 0.1901162 \\nonumber \\end{align}\\] Now, we calculate the gradients starting with the loss function with respect to the activation output of the O layer. \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial o_1} &amp;= \\frac{\\partial L_1}{\\partial o_1} = -(t_1 - o_1) = -(0.05 - 0.57248908) = 0.52248908\\\\ \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial o_2} &amp;= \\frac{\\partial L_2}{\\partial o_2} = -(t_2 - o_2) = -(0.95 - 0.62252853) = -0.32747147 \\end{align}\\] Third, we calculate the gradients of our activation output, \\(\\mathbf{o_1}\\), (from our activation function) with respect to the net input, \\(\\mathbf{\\hat{o}_1}\\), of the O layer : \\[\\begin{align} \\frac{\\partial o_1}{\\partial \\hat{o}_1} &amp;= o_1 \\left( 1 - o_1\\right) = 0.57248908 ( 1 - 0.57248908) = 0.24474533\\\\ \\frac{\\partial o_2}{\\partial \\hat{o}_2} &amp;= o_2 \\left( 1 - o_2\\right) = 0.62252853 ( 1 - 0.62252853) = 0.23498676 \\end{align}\\] Fourth, regarding the weights and biases for the O Layer, we first calculate the gradient of the total loss function with respect to the weights. Starting with (\\(\\varphi\\)) weights, in particular \\(\\varphi_{1,1}\\), we perform the following derivatives: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\varphi_{1,1}} &amp;= \\left(\\frac{\\partial L_{(total)}}{\\partial o_1} \\right) \\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1} \\right) \\left(\\frac{\\partial \\hat{o}_1}{\\partial \\varphi_{1,1}} \\right) \\\\ &amp;= \\underbrace{\\left(\\frac{\\partial L_1}{\\partial o_1} \\right)}_{\\text{1st factor}}\\ \\ \\underbrace{\\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1} \\right)}_{\\text{2nd factor}}\\ \\ \\underbrace{\\left(\\frac{\\partial \\hat{o}_1}{\\partial \\varphi_{1,1}} \\right)}_{\\text{3rd factor}} \\end{align}\\] The first two factors, namely \\(\\left(\\frac{\\partial L_1}{\\partial o_1} \\right)\\) and \\(\\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1} \\right)\\) are already calculated for us above. Both equates to the \\(\\delta_1\\) from our delta.rule. \\[\\begin{align} \\delta_1 = \\left(\\frac{\\partial L_{(total)}}{\\partial o_1} \\right) \\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1} \\right) = (0.52248908)(0.24474533) = 0.12787676 \\end{align}\\] Equivalently, for \\(\\delta_2\\), we have: \\[\\begin{align} \\delta_2 = \\left(\\frac{\\partial L_{(total)}}{\\partial o_2} \\right) \\left(\\frac{\\partial o_2}{\\partial \\hat{o}_2} \\right) = (-0.32747147)(0.23498676) = -0.07695146 \\end{align}\\] We now have to calculate the third factor, namely \\(\\left(\\frac{\\partial \\hat{o}_1}{\\partial \\varphi_{1,1}} \\right)\\): \\[\\begin{align} \\frac{\\partial \\hat{o}_1} {\\partial \\varphi_{1,1}} = \\frac{\\partial \\left(\\varphi_{0,1} \\times h_0 + \\varphi_{1,1} \\times h_1 + \\varphi_{2,1} \\times h_2 \\right) } {\\partial \\varphi_{1,1}} = {\\varphi_{1,1}}^{(1-1)} \\times h_1 = h_1 = 0.60312234 \\end{align}\\] Therefore, the gradient of the total loss function with respect to \\(\\varphi_{1,1}\\) is: \\[\\begin{align} \\nabla \\varphi_{1,1} \\mathcal{L} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\varphi_{1,1}} = \\delta_1 h_1 = (0.12787676)(0.60312234) = 0.077125331 \\end{align}\\] Calculation of gradient for other phi \\((\\varphi)\\) weights follow the same process. Fifth, in terms of the weights and biases for the H Layer, the gradient of the total loss function with respect to \\(\\alpha_{1,1}\\) is written as: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\alpha_{1,1}} = \\underbrace{ \\left(\\frac{\\partial L_{(total)}}{\\partial h_1} \\right) \\left(\\frac{\\partial h_1}{\\partial \\hat{h}_1} \\right)}_{\\delta_{h_1}} \\left(\\frac{\\partial \\hat{h}_1}{\\partial \\alpha_{1,1}} \\right) \\end{align}\\] We can expand the first factor into the following: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\alpha_{1,1}} = \\underbrace{\\left(\\frac{\\partial \\mathcal{L}_1}{\\partial h_1} + \\frac{\\partial \\mathcal{L}_2}{\\partial h_1}\\right) }_{\\text{1st factor}} \\underbrace{\\left(\\frac{\\partial h_1}{\\partial \\hat{h}_1} \\right)}_{\\text{2nd factor}} \\underbrace{\\left(\\frac{\\partial \\hat{h}_1}{\\partial \\alpha_{1,1}} \\right)}_{\\text{3rd factor}} \\end{align}\\] Decomposing the 1st factor into two terms, we have: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\alpha_{1,1}} = \\underbrace{ \\left(\\overbrace{ \\underbrace{\\frac{\\partial \\mathcal{L}_{1}}{\\partial o_1} \\frac{\\partial o_1}{\\partial \\hat{o}_1}}_{\\delta_{1}} \\frac{\\partial \\hat{o}_1}{\\partial h_1}}^{\\partial \\mathcal{L}_1 / \\partial h_1} + \\overbrace{ \\underbrace{ \\frac{\\partial \\mathcal{L}_{2}}{\\partial o_2} \\frac{\\partial o_2}{\\partial \\hat{o}_2} }_{\\delta_{2}} \\frac{\\partial \\hat{o}_2}{\\partial h_1}}^{\\partial \\mathcal{L}_2 / \\partial h_1} \\right) \\frac{\\partial h_1}{\\partial \\hat{h}_1}}_{\\delta_{h_1}} \\frac{\\partial \\hat{h}_1}{\\partial \\alpha_{1,1}} \\end{align}\\] Now, let us calculate the two terms: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_1}{\\partial h_1} = \\underbrace{\\left(\\frac{\\partial \\mathcal{L}_1}{\\partial o_1 }\\right) \\left(\\frac{\\partial o_1}{\\partial \\hat{o}_1 }\\right)}_{\\delta_1} \\left(\\frac{\\partial \\hat{o}_1 }{\\partial h_1}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}_2}{\\partial h_1} = \\underbrace{ \\left(\\frac{\\partial \\mathcal{L}_2}{\\partial o_2 }\\right) \\left(\\frac{\\partial o_2}{\\partial \\hat{o}_2 }\\right)}_{\\delta_2} \\left(\\frac{\\partial \\hat{o}_2 }{\\partial h_1}\\right) \\end{align}\\] The first two factors of the 1st term, namely \\(\\left(\\frac{\\partial \\mathcal{L}_1}{\\partial o_1 }\\right)\\) and \\(\\left(\\frac{\\partial o_1 }{\\partial \\hat{o}_1}\\right)\\) are already solved, denoted by \\(\\delta_1\\). Equivalently, the first two factors of the 2nd term, namely \\(\\left(\\frac{\\partial \\mathcal{L}_2}{\\partial o_2 }\\right)\\) and \\(\\left(\\frac{\\partial o_2 }{\\partial \\hat{o}_2}\\right)\\) are also solved, denoted by \\(\\delta_2\\). The derivatives of the two net inputs with respect to \\(h_1\\) are: \\[\\begin{align} \\frac{\\partial \\hat{o}_1 }{\\partial h_1} = \\frac{\\partial \\left(\\varphi_{0,1} \\times h_0 + \\varphi_{1,1} \\times h_1 + \\varphi_{2,1} \\times h_2 \\right)}{\\partial h_1} = \\varphi_{1,1} \\times {h_1}^{(1-1)} = \\varphi_{1,1} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\hat{o}_2 }{\\partial h_1} = \\frac{\\partial \\left(\\varphi_{0,2} \\times h_0 + \\varphi_{1,2} \\times h_1 + \\varphi_{2,2} \\times h_2 \\right)}{\\partial h_1} = \\varphi_{1,2} \\times {h_1}^{(1-1)} = \\varphi_{1,2} \\end{align}\\] That gives us the following: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_1}{\\partial h_1} = \\delta_1 (\\varphi_{1,1}) = (0.12787676)(0.03) = 0.0038363028 \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_2}{\\partial h_1} = \\delta_2 \\left( \\varphi_{1,2}\\right) = (-0.07695146)(0.35) = -0.026933011 \\end{align}\\] Therefore, the first factor is: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_1} = \\left(\\frac{\\partial \\mathcal{L}_1}{\\partial h_1} + \\frac{\\partial \\mathcal{L}_2}{\\partial h_1}\\right) = 0.0038363028 + -0.026933011 = -0.023096708 \\end{align}\\] Now, let us solve for the second factor (activation output from our sigmoid function). \\[\\begin{align} \\frac{\\partial h_1}{\\partial \\hat{h}_1} = h_1( 1 - h_1) = 0.60312234 ( 1 - 0.60312234) = 0.23936578 \\end{align}\\] At this point, we do not need \\(\\left(\\frac{\\partial h_2}{\\partial \\hat{h}_2}\\right)\\). And for the third factor, we have: \\[\\begin{align} \\frac{\\partial \\hat{h}_1}{\\partial \\alpha_{1,1}} = \\frac{\\partial \\left(\\alpha_{0,1} \\times z_0 + \\alpha_{1,1} \\times z_1 + \\alpha_{2,1} \\times z_2 \\right) } {\\partial \\alpha_{1,1}} = {\\varphi_{1,1}}^{(1-1)} \\times z_1 = z_1 = 0.52732275 \\end{align}\\] As for the delta, e.g. \\(\\delta_{h_1}\\), we have: \\[\\begin{align} \\delta_{h_1} = \\left(\\frac{\\partial L_{(total)}}{\\partial h_1} \\right) \\left(\\frac{\\partial h_1}{\\partial \\hat{h}_1} \\right) = (-0.023096708)(0.23936578) = -0.0055285615 \\end{align}\\] Therefore, the derivative of the total loss with respect to \\(\\alpha_{1,1}\\) is: \\[\\begin{align} \\nabla \\alpha_{1,1} \\mathcal{L} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\alpha_{1,1}} = \\delta_{h_1} z_1 = (-0.0055285615)(0.52732275) = -0.0029153363 \\end{align}\\] Calculation of gradient for other alpha \\((\\alpha)\\) weights follow the same process. Sixth, in terms of the weights and biases for the Z Layer, the derivative of the total loss with respect to \\(\\omega_{1,1}\\) is written as: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{1,1}} = \\underbrace{ \\left(\\frac{\\partial L_{(total)}}{\\partial z_1} \\right) \\left(\\frac{\\partial z_1}{\\partial \\hat{z}_1} \\right)}_{\\delta_{z_1}} \\left(\\frac{\\partial \\hat{z}_1}{\\partial \\omega_{1,1}} \\right) \\end{align}\\] We can expand the first factor into the following: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{1,1}} = \\underbrace{\\left(\\frac{\\partial \\mathcal{L}_1}{\\partial z_1} + \\frac{\\partial \\mathcal{L}_2}{\\partial z_1}\\right) }_{\\text{1st factor}} \\underbrace{\\left(\\frac{\\partial z_1}{\\partial \\hat{z}_1} \\right)}_{\\text{2nd factor}} \\underbrace{\\left(\\frac{\\partial \\hat{z}_1}{\\partial \\omega_{1,1}} \\right)}_{\\text{3rd factor}} \\end{align}\\] Decomposing the 1st factor into two terms, we have: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{1,1}} = \\underbrace{ \\left(\\overbrace{ \\underbrace{\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_1} \\frac{\\partial h_1}{\\partial \\hat{h}_1}}_{\\delta_{h_1}} \\frac{\\partial \\hat{h}_1}{\\partial z_1}}^{\\partial \\mathcal{L}_1 / \\partial z_1} + \\overbrace{ \\underbrace{ \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_2} \\frac{\\partial h_2}{\\partial \\hat{h}_2} }_{\\delta_{h_2}} \\frac{\\partial \\hat{h}_2}{\\partial z_1}}^{\\partial \\mathcal{L}_2 / \\partial z_1} \\right) \\frac{\\partial z_1}{\\partial \\hat{z}_1}}_{\\delta_{z_1}} \\frac{\\partial \\hat{z}_1}{\\partial \\omega_{1,1}} \\end{align}\\] where: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_1} = \\left(\\frac{\\partial \\mathcal{L}_{1}}{\\partial h_1} + \\frac{\\partial \\mathcal{L}_{2}}{\\partial h_1}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_2} = \\left(\\frac{\\partial \\mathcal{L}_{1}}{\\partial h_2} + \\frac{\\partial \\mathcal{L}_{2}}{\\partial h_2}\\right) \\end{align}\\] Note that we have previously solved for the derivative of the total loss with respect to \\(h_1\\): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_1} = -0.02309672 \\end{align}\\] Here, we also have to solve for the derivative of the total loss with respect to \\(h_2\\): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{1}}{\\partial h_2} = \\delta_1 \\left(\\frac{\\partial \\hat{o}_1}{\\partial h_2}\\right) = \\delta_1 \\varphi_{2,1} = (0.12787676)(0.04) = 0.0051150704 \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{2}}{\\partial h_2} = \\delta_2 \\left(\\frac{\\partial \\hat{o}_2}{\\partial h_2}\\right) = \\delta_2 \\varphi_{2,2} = (-0.07695146)(0.40) = -0.030780584 \\end{align}\\] where the derivatives of the two net inputs with respect to \\(h_2\\) are: \\[\\begin{align} \\frac{\\partial \\hat{o}_1 }{\\partial h_2} = \\frac{\\partial \\left(\\varphi_{0,1} \\times h_0 + \\varphi_{1,1} \\times h_1 + \\varphi_{2,1} \\times h_2 \\right)}{\\partial h_2} = \\varphi_{2,1} \\times {h_2}^{(1-1)} = \\varphi_{2,1} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\hat{o}_2 }{\\partial h_2} = \\frac{\\partial \\left(\\varphi_{0,2} \\times h_0 + \\varphi_{1,2} \\times h_1 + \\varphi_{2,2} \\times h_2 \\right)}{\\partial h_2} = \\varphi_{2,2} \\times {h_2}^{(1-1)} = \\varphi_{2,2} \\end{align}\\] Therefore: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_2} = \\left(\\frac{\\partial \\mathcal{L}_{1}}{\\partial h_2} + \\frac{\\partial \\mathcal{L}_{2}}{\\partial h_2} \\right) = 0.0051150704 + -0.030780584 = -0.025665514 \\end{align}\\] Now, as for the H deltas, we already have solved for \\(\\delta_{h_1}\\) in the previous step. We should solve for the \\(\\delta_{h_2}\\) next. For that, we first need to solve for the derivatives of the activation outputs in H layer with respect to their corresponding net inputs. Note that \\(\\frac{\\partial h_1}{\\partial \\hat{h}_1}\\) is already solved for us. We have to solve for \\(\\frac{\\partial h_2}{\\partial \\hat{h}_2}\\). \\[\\begin{align} \\frac{\\partial h_2}{\\partial \\hat{h}_2} = h_2(1 - h_2) = 0.59800413 ( 1 - 0.59800413) = 0.24039519 \\end{align}\\] Therefore, for \\(\\delta_{h_2}\\), we get: \\[\\begin{align} \\delta_{h_2} = \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_2}\\right) \\left(\\frac{\\partial h_2}{\\partial \\hat{h}_2}\\right) = (-0.025665514)(0.24039519) = -0.0061698661 \\end{align}\\] Next, we still have to solve for the derivative of the total loss with respect to \\(\\mathbf{z_1}\\): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial z_1} = \\left(\\frac{\\partial \\mathcal{L}_{1}}{\\partial z_1} + \\frac{\\partial \\mathcal{L}_{2}}{\\partial z_1}\\right) \\end{align}\\] The equation relies on solving for the derivative of net inputs ( \\(\\hat{h}_1\\) and \\(\\hat{h}_2\\)) with respect to the activation output (\\(z_1\\)): \\[\\begin{align} \\frac{\\partial \\hat{h}_1}{\\partial z_1} = \\frac{\\partial (\\alpha_{0,1} \\times z_0 + \\alpha_{1,1} \\times z_1 + \\alpha_{2,1} \\times z_2)} {\\partial z_1} = \\alpha_{1,1} = 0.09 \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\hat{h}_2}{\\partial z_1} = \\frac{\\partial (\\alpha_{0,2} \\times z_0 + \\alpha_{1,2} \\times z_1 + \\alpha_{2,2} \\times z_2)} {\\partial z_1} = \\alpha_{1,2} = 0.06 \\end{align}\\] Finally, we can solve for the derivative of the total loss with respect to \\(z_1\\): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial z_1} &amp;= (\\delta_{h_1} \\alpha_{1,1} + \\delta_{h_2} \\alpha_{1,2}) \\\\ &amp;= (-0.0055285615)(0.09) + (-0.0061698661)(0.06) \\nonumber \\\\ &amp;= -0.00049757053 + -0.00037019197 \\nonumber \\\\ &amp;= -0.0008677625 \\nonumber \\end{align}\\] Next, we solve for the derivative of the activation output (\\(z_1\\)) with respect to its net input (\\(\\hat{z}_1\\)) using sigmoid gradient: \\[\\begin{align} \\frac{\\partial z_1}{\\partial \\hat{z}_1} = z_1(1 - z_1) = 0.52732275(1 - 0.52732275) = 0.24925348 \\end{align}\\] That should suffice to solve for \\(\\delta_{z_1}\\) delta: \\[\\begin{align} \\delta_{z_1} &amp;= \\left[\\left(\\delta_{h_1} \\alpha_{1,1} + \\delta_{h_2} \\alpha_{1,2}\\right) \\right] \\left(\\frac{\\partial z_1}{\\partial \\hat{z}_1}\\right) = (-0.0008677625) (0.24925348) \\\\ &amp;=-0.00021629282 \\nonumber \\end{align}\\] Lastly, we solve for the derivative of the net input (\\(\\hat{z}_1\\)) with respect to the weight (\\(\\omega_{1,1}\\)): \\[\\begin{align} \\frac{\\partial \\hat{z}_1}{\\partial \\omega_{1,1}} = \\frac{\\partial (\\omega_{0,1} \\times x_0 + \\omega_{1,1} \\times x_1 + \\omega_{2,1} \\times x_2)} {\\partial \\omega_{1,1}} = x_1 = 0.12 \\end{align}\\] Therefore: \\[\\begin{align} \\nabla \\omega_{1,1} \\mathcal{L} = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\omega_{1,1}} = \\delta_{z_1} x_1 = (-0.00021629282)(0.12) = \\text{-2.5955138e-5} \\end{align}\\] The gradient calculation for other omega \\((\\omega)\\) weights follows the same process. Seventh, for Backward Pass, we now can perform update to our parameters. Here, we update the parameters we covered in our previous steps, namely \\(\\varphi_{1,1}\\), \\(\\alpha_{1,1}\\), and \\(\\omega_{1,1}\\): \\[\\begin{align} {\\varphi_{1,1}}^{(t+1)} = {\\varphi_{1,1}}^{(t)} - \\eta \\nabla \\omega_{1,1} \\mathcal{L}= 0.03 - 0.01 \\times \\text{0.077125331} = 0.029228747 \\end{align}\\] \\[\\begin{align} {\\alpha_{1,1}}^{(t+1)} = {\\alpha_{1,1}}^{(t)} - \\eta \\nabla \\omega_{1,1} \\mathcal{L}= 0.09 - 0.01 \\times \\text{-0.0029153363} = 0.090029153 \\end{align}\\] \\[\\begin{align} {\\omega_{1,1}}^{(t+1)} = {\\omega_{1,1}}^{(t)} - \\eta \\nabla \\omega_{1,1} \\mathcal{L}= 0.21 - 0.01 \\times \\text{-2.5955138e-5} = 0.21000026 \\end{align}\\] The same update rule applies to all other parameters. Note here that eta \\((\\eta)\\) symbol represents the learning rate set at 0.01 and t serves as an iteration index. Lastly, we iterate through the process until the cost minimizes into a tolerable threshold. We emphasize that our entire MLP example uses sigmoid function and least square loss. However, while it may help showcase the operational and technical aspects of MLP, the choice of the activation function and loss function in our example may not necessarily render the expected result and interpretability. Later, we showcase Cross-Entropy loss as one of the alternative loss functions for Sigmoid. With that in mind, let us introduce alternative Activation Functions commonly used in Neural Networks. 12.3.5 Activation Function From the perspective of an activation function, most of our discussions center around sigmoid function. In this section, we outline other activation functions. We use an activation function mainly for solving non-linear problems; thus, we may often read phrases such as adding nonlinearity to our network, which implies adding activation functions. Choosing an activation function depends on many factors, especially for our output layer. For example, if we are looking to solve a problem based on Linear or Logistic Regression, then perhaps RELU and its variations may apply. If it is based on Binomial Classification, then perhaps Sigmoid and its improved counterparts may apply. Otherwise, perhaps Softmax may apply, generalizing for Multinomial Classification. The list below enumerates just a few of the common activation functions, among many others. We also introduce Swish (Prajit Ramachandran et al. 2017) and Mish (Diganta Misra 2019). \\[ \\begin{array}{ll} \\mathbf{Sigmoid} &amp;= \\frac{1}{1 + \\text{exp}(-x)} \\\\ {} \\\\ \\mathbf{RELU} &amp;= \\text{max}(0, x) \\\\ {} \\\\ \\mathbf{\\text{Softmax}} &amp;= \\frac{exp(x_i - max(x))}{\\sum_j exp(x_j - max(x))} \\\\ {} \\\\ \\mathbf{Mish} &amp;= x \\times \\text{TanH}(\\text{softplus}(x )) \\end{array} \\left| \\begin{array}{ll} \\mathbf{TanH} &amp;= \\frac{\\text{exp}(x) - \\text{exp}(-x)}{\\text{exp}(x) + \\text{exp}(-x)} \\\\ {} \\\\ \\mathbf{Leaky\\ RELU} &amp;= \\begin{array}{l}\\text{max}(\\alpha x, x),\\\\ \\alpha=0.01 - 0.3\\end{array}\\\\ {} \\\\ \\mathbf{Swish} &amp;= x \\times \\text{sigmoid}(x)\\\\ {} \\\\ \\mathbf{Softplus} &amp;= \\log_e( 1 + \\text{exp}(x)) \\end{array} \\right. \\] Other variants of Swish are: \\[ \\begin{array}{ll} \\mathbf{\\text{Switch (RELU)}} &amp;= x \\times \\text{RELU}(x) \\end{array} \\left| \\begin{array}{ll} \\mathbf{\\text{Switch (TanH)}} &amp;= x \\times \\text{TanH}(x) \\end{array} \\right. \\] Note that when handling large numbers, a trick used for softmax - and perhaps for other formulas which rely on exp(.) for that matter - is to subtract the maximum x value from each x value to achieve some level of numerical stability. Here is the implementation of each activation function: linear &lt;- function(x) { x } binary.step &lt;- function(x) { idx = which(x &lt; 0 ); x[idx] = 0; x[-idx] = 1; x } sigmoid &lt;- function(x) { 1 / ( 1 + exp(-x)) } tan.h &lt;- function(x) { (exp(x) - exp(-x)) / ( exp(x) + exp(-x)) } relu &lt;- function(x) { pmax(x, 0) } leaky.relu &lt;- function(x, a=0.01) { pmax( x, a*x) } softplus &lt;- function(x) { log ( 1 + exp(x), exp(1)) } swish.sigmoid &lt;- function(x) { x * sigmoid(x) } swish.relu &lt;- function(x) { x * relu(x) } swish.tanh &lt;- function(x) { x * tan.h(x) } mish.tan.h &lt;- function(x) { x * tan.h(softplus(x))} softmax &lt;- function(x) { p = apply(x, 1, max); x = x - p; p = exp(x) s = apply(p, 1, sum); sweep(p, 1, s, &quot;/&quot;) } Note that there are many other variations of RELU such as Shifted RELU (SRELU), Parameterized RELU (PRELU), Scaled Exponential Linear Unit (SELU), and others. We leave readers to investigate the variations. Now, for every activation function we use, there is a corresponding gradient function and corresponding loss function required for backpropagation. Let us enumerate the gradient functions of a few of the activation functions where \\(\\frac{\\partial\\ a(x)}{\\partial\\ x} = a&#39;(x)\\) (no derivations included): \\[ \\begin{array}{ll} \\mathbf{\\nabla\\ Sigmoid} = \\sigma(x)(1 - \\sigma(x)) \\\\ {} \\\\ \\mathbf{\\nabla\\ RELU} = \\begin{cases} 0 &amp; if\\ x \\le 0\\\\ 1 &amp; if\\ x &gt; 0 \\end{cases} \\\\ {} \\\\ \\mathbf{\\nabla\\ Softmax} = a(x_i)( \\delta_{ij} - a(x_j)) \\\\ {} \\\\ \\mathbf{\\nabla\\ Mish} = \\frac{exp(x) \\times \\omega}{\\delta^2} \\end{array} \\left| \\begin{array}{ll} \\nabla\\ \\mathbf{TanH} = 1 - a(x)^2\\\\ {} \\\\ \\mathbf{\\nabla\\ Leaky\\ RELU} = \\begin{array}{l} \\begin{cases} \\alpha &amp; if\\ x\\le 0,\\\\ 1 &amp; if\\ x &gt; 0 \\end{cases}\\\\ e.g. \\alpha=0.01 \\end{array}\\\\ {} \\\\ \\mathbf{\\nabla\\ Swish} = a(x) + \\text{sigmoid}(x)( 1 - a(x)) \\\\ {} \\\\ \\mathbf{\\nabla\\ Softplus} = \\frac{1}{1 + exp(-x)}\\ \\leftarrow \\text{(sigmoid)} \\end{array} \\right. \\] Note that activation functions have to be differentiable. For RELU and Leaky RELU, we may not have to be strictly mathematical; instead, we can produce a quasi-derivative for the respective functions like so: \\[\\begin{align} a&#39;(x) = \\begin{cases} 0 &amp; if\\ x &lt;= 0\\\\ 1 &amp; if\\ x &gt; 0 \\end{cases} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ a&#39;(x) = \\begin{cases} \\alpha &amp; if\\ x &lt;= 0\\\\ 1 &amp; if\\ x &gt; 0 \\end{cases} \\label{eqn:eqnnumber603} \\end{align}\\] For Mish (Diganta M., 2019), we have the following parameters for its derivatives: \\[\\begin{align} \\omega = 4(x + 1) + 4e^{2x} + e^{3x} + e^x(4x + 6) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta = 2e^x + e^{2x} + 2 \\end{align}\\] For Swish (Prajit R. et. al, 2017), the gradient is \\(a(x) = x \\times \\text{sigmoid}(x)\\). Additionally, Swish also can use an extra parameter, namely \\(\\beta\\). For example: \\[\\begin{align} \\text{Swish (Sigmoid)} = x \\times \\text{sigmoid}(\\beta X) \\end{align}\\] \\[\\begin{align} \\nabla \\text{Swish (Sigmoid)} = \\beta a(\\beta x) + \\text{sigmoid}(\\beta x) \\times ( 1 - \\beta a(\\beta x)) \\end{align}\\] For Softmax (Ludwig Boltzmann, 1868), the derivative optionally uses Kronecker Delta, denoted by the symbol (\\(\\delta_{ij}\\)). Below is an example of applying the Kronecker Delta to a matrix that produces an Identity matrix: \\[ \\left[ \\begin{array}{lll} 1 &amp; 2 &amp; 3\\\\ 4 &amp; 5 &amp; 6\\\\ 7 &amp; 8 &amp; 9 \\end{array} \\right]_X \\times \\left[ \\begin{array}{lll} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 1 &amp; 0\\\\ 0 &amp; 0 &amp; 1 \\end{array} \\right]_\\delta = \\left[ \\begin{array}{lll} 1 &amp; 0 &amp; 0\\\\ 0 &amp; 5 &amp; 0\\\\ 0 &amp; 0 &amp; 9 \\end{array} \\right] \\ \\ \\ \\ where\\ \\ \\ \\ \\delta_{jk} = \\begin{cases} 1 &amp; \\text{if j = k}\\\\ 0 &amp; \\text{if j }\\ne\\text{ k} \\end{cases} \\] More discussion of Softmax to follow later ahead. Below is a list of the implementation of the gradients: gradient.sigmoid &lt;- function(o) { o * (1 - o) } gradient.tan.h &lt;- function(o) { 1 - o^2 } gradient.relu &lt;- function(o) { pmax(sign(o), 0) } gradient.leaky.relu &lt;- function(o, a=0.01) { pmax(sign(o), a) } gradient.softplus &lt;- function(o) { sigmoid(o) } gradient.swish.sigmoid &lt;- function(o) { a = o * sigmoid(o); a + sigmoid(o) * (1 - a) } gradient.mish.tan.h &lt;- function(o) { w = 4*(o + 1) + 4*exp(2*o) + exp(3*o) + exp(o) * (4*o + 6) d = 2*exp(o) + exp(2*o) + 2 (exp(o) * w) / d^2 } gradient.softmax &lt;- function(o) { J = list(); N = nrow(o) for (i in 1:N) { J[[i]] = diag(o[i,]) - kronecker(t(o[i,]), o[i,]) }; J } For a plot of the activation functions and corresponding derivatives, see Figure 12.11. Figure 12.11: Activation Function It is possible to use a different activation function for hidden layers and the output layer. For example, for Logistic Regression, it may be common to use Rectified Linear Unit (RELU) as an activation functions for all hidden layers and opt to use sigmoid in the output layer. And for Multi Classification, we can use softmax function in the output layer. Certain properties of an activation function can be reviewed to see if such a function qualifies to fit one’s purposes: range, monotonicity, boundary (e.g., bounded below, unbounded above), and smoothness. Apart from the usual performance and accuracy goals, some of the activation functions, in certain conditions, may help to avoid the vanishing and exploding gradient conditions; thus, such properties are essential. The former condition, e.g., vanishing gradient, may manifest if the neural network has too many layers. For example, notice in Figure 12.11 that the gradients of both Sigmoid and TanH reduce the effect of gradients as gradients approach zero and as their net inputs stretch outwards, e.g., \\(\\pm \\infty\\). In the figure, all other activation functions (apart from the linear and binary step) have a lower bound (e.g., zero) towards the negative direction and an infinite bound towards the positive direction. The latter condition, e.g., exploding gradient, may manifest if the neural network has connections that accumulate large weights due to large gradients. Whereas vanishing gradients tend towards -infinity (\\(+\\infty\\)), exploding gradients tend towards +infinity (\\(+\\infty\\)). In terms of performance measures, each activation function may work best with its own loss function. For example, for both Relu and Leaky Relu, we have the following Least Squared Error formula: \\[\\begin{align} \\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N \\left(t - o\\right)^2 \\end{align}\\] For both Sigmoid and Softmax, we can use Cross-Entropy (CE) Loss. Generally, we see the following equation, discussed in Chapter 8 (Bayesian Computation II). \\[\\begin{align} \\mathcal{L}^{(CE)} = \\underbrace{-\\sum_x^X P_x \\log_e \\mathcal{Q}_x }_\\text{Cross-Entropy Loss} = - \\sum_x^X \\left[ t_{x} \\log_e \\sigma(o_{(x)})\\right] \\end{align}\\] Note that the idea of Cross-Entropy is about measuring the distance between two distributions - See Chapter 8 (Bayesian Computation II). In essence, for Sigmoid, a Binary CE loss can be translated into a Binomial Logistic Loss like so: \\[\\begin{align} \\mathcal{L}^{(CE)} = \\underbrace{ - \\frac{1}{N} \\sum_{i=1}^N\\left[t_i \\log_e(o_i) + (1 - t_i)\\log_e(1 - o_i) \\right]}_\\text{Binomial Logistic Loss} = \\begin{cases} -\\log_e(o_i) &amp; \\text{if }t_i\\text{ = 1} \\\\ -\\log_e(1 - o_i) &amp; \\text{if }t_i\\text{ = 0} \\\\ \\end{cases} \\label{eqn:eqnnumber604} \\end{align}\\] where N is the number of samples. For Softmax, we may use a Multinomial Logistic Loss or Multiple Cross-Entropy Loss formula for Multi Classification: \\[\\begin{align} \\mathcal{L}^{(CE)} &amp;= \\underbrace{ - \\frac{1}{N} \\sum_{i=1}^N\\sum_{k=1}^K\\left[t_{ik} \\log_e(o_{ik}) + (1 - t_{ik})\\log_e(1 - o_{ik}) \\right]}_\\text{Multinomial Logistic Loss} \\\\ &amp;= -\\frac{1}{N}\\sum_{N=1}^N\\sum_{k=1}^Kt_{(ik)} \\log_e(o_{(ik)}) \\end{align}\\] where N is the number of samples and K is the number of classes. A K-value of 2 makes the loss a Binary CE loss. Note that the target (t) is a one-hot vector, and the output (o) is a vector of probabilities that sums up to 1. Let us give a special look into Softmax in the context of backpropagation. Our discussion focuses on the derivative of CE loss with respect to an activation output - also called activation output in our own context. Sigmoid (Logistic) Loss It helps to start with the more simple Sigmoid activation function for Binomial Classification. Then we can generalize into Softmax for Multi Classification. Using the following sigmoid function, we should be able to get the derivative of the sigmoid function with respect to the raw input (the logits). \\[\\begin{align} s(x) = \\frac{1}{1 + exp(-x)}\\ \\ \\ \\ \\ \\ s&#39;(x) = \\frac{\\partial\\ s(x)}{\\partial x} = s(x)(1 - s(x)) \\end{align}\\] From a Neural Network perspective, we have the equivalent notation in reference to Figure 12.8: \\[\\begin{align} \\frac{\\partial\\ s(x)}{\\partial x} = \\frac{\\partial\\ \\sigma(\\hat{o})}{\\partial \\hat{o}} = \\frac{\\partial o}{\\partial \\hat{o}} = \\sigma(\\hat{o})(1 - \\sigma(\\hat{o})) = o ( 1 - o) \\end{align}\\] where our activation output (activation output) in the output layer is denoted by (o) and our logits input (net input) as (\\(\\mathbf{\\hat{o}}\\)). Additionally, the derivative of a Cross-Entropy Loss as our Loss Function for Sigmoid with respect to an activation output (\\(\\mathbf{o}\\)) is as follows (no derivations provided): \\[\\begin{align} \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o} = - \\frac{t}{o} + \\frac{1 - t}{1 - o} = \\frac{o-t}{o( 1 - o)} \\end{align}\\] If we recall the Delta Rule, the derivative of the Cross-Entropy Loss for Sigmoid function with respect to a net input (\\(\\mathbf{\\hat{o}}\\)) is as follows: \\[\\begin{align} \\delta_{o} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\hat{o}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o}\\right) \\left(\\frac{\\partial o}{\\partial \\hat{o}}\\right) = \\left[\\frac{o-t}{o( 1 - o)}\\right] o ( 1 - o) = (o - t ) \\end{align}\\] Then, the derivative of the Cross-Entropy Loss for Sigmoid function with respect to a weight (\\(\\omega_{(jk)}\\)) is as follows: \\[\\begin{align} \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\omega_{(jk)}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o}\\right) \\left(\\frac{\\partial o}{\\partial \\hat{o}}\\right) \\left(\\frac{\\partial \\hat{o}}{\\partial \\omega_{(jk)}}\\right) = \\delta_{o} \\left(\\frac{\\partial \\hat{o}}{\\partial \\omega_{(jk)}}\\right) = \\delta_{o}(h_j) = (o - t)(h_j) \\end{align}\\] Softmax Loss Now for Softmax, we should be able to get the derivative of the Softmax loss function with respect to the raw input. \\[\\begin{align} s(x_j) = \\frac{exp(x_j)}{\\sum_{k=1}^Kexp(x_k)}\\ \\ \\ \\ \\ \\ s&#39;(x_j) = \\frac{\\partial\\ s(x_j)}{\\partial x_j} = \\begin{cases} s(x_j)(1 - s(x_j)) &amp; \\text{if j = k} \\\\ -s(x_j) s(x_k) &amp; \\text{if j }\\ne \\text{k} \\\\ \\end{cases} \\label{eqn:eqnnumber605} \\end{align}\\] Here, from a Neural Network perspective, we have the equivalent notation in reference to Figure 12.8: \\[\\begin{align} \\frac{\\partial\\ s(x_j)}{\\partial x_j} = \\frac{\\partial\\ \\sigma(\\hat{o}_j)}{\\partial \\hat{o}_j} = \\frac{\\partial o_j}{\\partial \\hat{o}_j} = \\begin{cases} \\sigma(\\hat{o}_j)(1 - \\sigma(\\hat{o}_j)) = o_j ( 1 - o_j) &amp; \\text{if j = k} \\\\ - \\sigma(\\hat{o}_j) \\sigma(\\hat{o}_k) = -(o_j)(o_k) &amp; \\text{if j }\\ne \\text{k} \\\\ \\end{cases} \\label{eqn:eqnnumber606} \\end{align}\\] Figure 12.12 shows a Jacobian matrix as reference when solving for the derivative of the activation output - an output from an activation function - with respect to the net input, e.g. \\(\\frac{\\partial o_j}{\\partial \\hat{o}_j}\\). Figure 12.12: SoftMax Gradient To illustrate, suppose we have the following net input to be fed to a Softmax activation function (assume one sample and three target classes): \\[ \\hat{o}_1 = 3 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{o}_1 = 4 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{o}_1 = 5 \\] Our Softmax yields the following: o.hat = matrix(c(3,4,5), nrow=1) (o = softmax(o.hat)) ## [,1] [,2] [,3] ## [1,] 0.09003 0.2447 0.6652 where sum equals one: \\(\\sum_i o_i =\\) 0.09 + 0.2447 + 0.6652 = 1. Now, the Jacobian matrix representation of the derivatives of activation output with respect to the net input is as follows: \\[\\begin{align} \\sigma(o_j)(\\delta_{jk} - \\sigma(o_k)) = \\underbrace{\\sigma(o_j)\\delta_{jk}}_{\\text{identity matrix}} - \\underbrace{\\sigma(o_j) \\otimes \\sigma(o_k)}_{\\text{kronecker product}} \\ \\ \\ \\ \\ where\\ \\ \\delta_{jk}\\text{ is kronecker delta} \\end{align}\\] (J = diag(o) - kronecker(t(o), o)) ## [,1] [,2] [,3] ## [1,] 0.08193 0.06800 0.03014 ## [2,] 0.06800 0.03014 -0.07277 ## [3,] 0.03014 -0.07277 -0.35251 To validate, let us tackle the first derivative: \\(\\frac{\\partial o_1}{\\partial \\hat{o}_1} = s(\\hat{o}_1)(1 - s(\\hat{o}_1)) = o_1 ( 1 - o_1)\\) = 0.09 \\(\\times (1-\\) 0.09\\()\\) = 0.0819. \\(\\frac{\\partial o_2}{\\partial \\hat{o}_2} = s(\\hat{o}_2)(1 - s(\\hat{o}_2)) = o_2 ( 1 - o_2)\\) = 0.2447 \\(\\times (1-\\) 0.2447\\()\\) = 0.0301. \\(\\frac{\\partial o_3}{\\partial \\hat{o}_3} = s(\\hat{o}_3)(1 - s(\\hat{o}_3)) = o_3 ( 1 - o_3)\\) = 0.6652 \\(\\times (1-\\) 0.6652\\()\\) = -0.3525. \\(\\frac{\\partial o_1}{\\partial \\hat{o}_2} = -s(\\hat{o}_1) s(\\hat{o}_2) = - o_1 \\times o_2\\) = -0.09 \\(\\times\\) 0.2447 = 0.068. \\(\\frac{\\partial o_1}{\\partial \\hat{o}_3} = -s(\\hat{o}_1) s(\\hat{o}_3) = - o_1 \\times o_3\\) = -0.09 \\(\\times\\) 0.6652 = 0.0301. … \\(\\frac{\\partial o_3}{\\partial \\hat{o}_1} = -s(\\hat{o}_3) s(\\hat{o}_1) = - o_3 \\times o_1\\) = -0.6652 \\(\\times\\) 0.09 = 0.0301. \\(\\frac{\\partial o_3}{\\partial \\hat{o}_2} = -s(\\hat{o}_3) s(\\hat{o}_2) = - o_3 \\times o_2\\) = -0.6652 \\(\\times\\) 0.2447 = -0.0728. Next, the derivative of the Cross-Entropy Loss as our Loss Function for Softmax with respect to an activation output (\\(\\mathbf{o_j}\\)) is as follows: \\[\\begin{align} \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_j} = \\frac{\\partial}{\\partial o_j} \\left(-\\sum_{k=1}^Kt_{k} \\log_e(o_{k}) \\right) = \\underbrace{\\frac{\\partial}{\\partial o_j} \\left(- t_j \\log_e(o_j)\\right)}_{\\text{focus on one-hot encoding}} = -\\frac{t_j}{o_j} \\end{align}\\] Note that t is a one-hot vector and only one element is equal to one. Assume therefore that if \\(\\mathbf{t_j= 1}\\), it stands that \\(-\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_j} = -\\frac{\\partial}{\\partial o_j} \\left((1)\\log_e(o_j)\\right)\\), and given that \\(f&#39;(ln(x)) = \\frac{1}{x}\\), therefore \\(-\\frac{\\partial}{\\partial o_j} \\left(\\log_e(o_j)\\right) = -\\frac{1}{o_j} = -\\frac{t_j}{o_j}\\) For the Delta Rule, the derivative of the Cross-Entropy Loss function for Softmax function with respect to a net input (\\(\\mathbf{\\hat{o}_j}\\)) is as follows: \\[\\begin{align} \\delta_{o_j} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\hat{o}_j} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_j}\\right) \\left(\\frac{\\partial o_j}{\\partial \\hat{o}_j}\\right) = \\left(- \\frac{t_k}{o_j} \\right) \\left( \\begin{cases} o_k ( 1 - o_k) &amp; \\text{if j = k}\\\\ - o_j o_k &amp; \\text{if j }\\ne \\text{k} \\end{cases} \\right) = (o_j - t_j) \\label{eqn:eqnnumber607} \\end{align}\\] Finally, the derivative of the Cross-Entropy Loss function for softmax function with respect to a weight (\\(\\omega_{(jk)}\\)) is as follows: \\[\\begin{align} \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\omega_{(jk)}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_j}\\right) \\left(\\frac{\\partial o_j}{\\partial \\hat{o}_k}\\right) \\left(\\frac{\\partial \\hat{o}_k}{\\partial \\omega_{(jk)}}\\right) = \\delta_{o_k} \\left(\\frac{\\partial \\hat{o}_k}{\\partial \\omega_{(jk)}}\\right) = \\delta_{o_k} (h_j) = (o_k - t_k)(h_j) \\end{align}\\] where J and K are the numbers of classes. Therefore, if we carefully follow backpropagation using Figure 12.8, the derivative of the Softmax loss with respect to the weight (\\(\\alpha_{1,1}\\)) is: \\[\\begin{align} \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\alpha_{1,1}} &amp;= \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_1}\\right) \\left(\\frac{\\partial o_1}{\\partial h_1}\\right) \\left(\\frac{\\partial h_1}{\\partial \\alpha_{1,1}}\\right) &amp;+ \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o_2}\\right) \\left(\\frac{\\partial o_2}{\\partial h_1}\\right) \\left(\\frac{\\partial h_1}{\\partial \\alpha_{1,1}}\\right)\\\\ \\\\ &amp;= \\left(-\\mathbf{\\frac{ t_1}{o_1}}\\right) \\left[o_1( 1 - o_1)\\right] h_1 &amp;+ \\left(-\\mathbf{\\frac{ t_2}{o_2}}\\right) \\left(-(o_2)(o_1)\\right) h_1\\\\ &amp;=\\left(t_2 o_1 - t_1 + t_1 o_1 \\right) h_1\\\\ &amp;=\\left((t_2 + t_1) o_1 - t_1 \\right) h_1\\\\ &amp;=(o_1 - t_1) h_1 &amp; \\text{where }\\sum_i t_i = 1 \\end{align}\\] Note that t is a one-hot vector. Therefore, in the case above, either we have (\\(t_1 = 1, t_2 = 0\\)) or (\\(t_1 = 0, t_2 = 1\\)). To illustrate, suppose we have the following target (t) and softmax output (o): t = c(1, 0) o = c(0.49052134, 0.50947866) Below is the result of the unsimplified version: - t[1] / o[1] * o[1] * ( 1 - o[1]) + (-t[2]/o[2]) * ( -o[2] * o[1]) ## [1] -0.5095 And the simplified version: o[1] - t[1] ## [1] -0.5095 Below, we have our implementation of the Loss functions, along with Helper functions: # Helper Functions ln &lt;- function(x) { log(x, exp(1))} relu.loss &lt;- leaky.relu.loss &lt;- function(t, o) { (t-o)^2 } sigmoid.loss &lt;- function(t, o) { eps = 1e-20 # Avoid NaN - (t * ln(o + eps) + (1 - t) * ln( 1 - o + eps))} softmax.loss &lt;- function(t, o) { eps = 1e-20; l = t * ln(o + eps) -apply(l, 1, sum) } gradient.relu.loss &lt;- function(t, o) { o-t } # or -(t-o) gradient.leaky.relu.loss &lt;- gradient.relu.loss gradient.sigmoid.loss &lt;- function(t, o) { (o-t)/(o * (1-o)) } gradient.softmax.loss &lt;- function(t, o) { eps = 1e-20; -t / (o + eps) } activation &lt;- function(x, afunc) { afunc(x) } gradient.activation &lt;- function(o, afunc) { afunc(o) } gradient.loss &lt;- function(t, o, afunc) { afunc(t, o) } For generating the loss function, we have: get.loss &lt;- function(target, layers, afunc=&quot;sigmoid&quot;) { afunc = get(paste0(afunc, &quot;.loss&quot;)) L = length(layers) output = layers[[L]]$output err = afunc(target, output) mean(err) } We now move to the actual implementation of MLP. 12.3.6 MLP Implementation Following an MLP algorithm, we assume a complete Neural Network with the same setup as diagrammed in Figure 12.8. Our supposition is that we have at least one hidden layer labeled as H. Now, in the context of data structures, the implementation of MLP relies heavily on matrix manipulation, especially with multiple samples. We can see in Figure 12.13 how the dot product of an input and its corresponding weight results in an output which is taken by the next layer as an input. In turn, the process continues with the following dot product. Figure 12.13: Forward Pass Below is an MLP pseudocode for the Forward Pass algorithm. Here, we use sigmoid for activation function: \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{dataset}: \\{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\\ \\in\\ \\mathbb{R}^{nxp}\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow \\text{(p is no. of features, n is no. of samples)}\\\\ \\ \\ \\ \\text{parameters}: \\{\\omega_{(1,1)}, \\omega_{(1,2)}, ..., \\omega_{(p,h)}: x\\ \\in\\ \\mathbb{R}^{pxh}\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow \\text{(p is no. of inputs, h is no. of target outputs)}\\\\ \\ \\ \\ \\text{activation function}: f(x) \\rightarrow e.g. \\begin{cases} sigmoid(x) &amp; \\text{if output layer}\\\\ leaky.relu(x) &amp; \\text{if hidden layer} \\end{cases}\\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ net.input = X\\\\ \\ \\ \\ act.output = \\{\\} \\\\ \\ \\ \\ \\text{loop}\\ L\\ in\\ 1:\\ H\\ \\ \\ \\ \\ \\ \\rightarrow \\text{(H is no. of layers excluding input)}\\\\ \\ \\ \\ \\ \\ \\ \\ weights = \\omega^{(L)} \\\\ \\ \\ \\ \\ \\ \\ \\ act.output^{(L)} = f(net.input \\cdot \\omega^{(L)} )\\ \\ \\ \\ \\ (act.output^{(L) } \\in \\mathbb{R}^{nxh})\\\\ \\ \\ \\ \\ \\ \\ \\ net.input = act.output^{(L)}\\\\ \\ \\ \\ \\text{end loop} \\\\ \\mathbf{Output}:\\\\ \\ \\ \\ act.output \\end{array} \\] Recall that our net input in the matrix includes the biases. Also, our bias constant connects to neurons in different layers with unique weights. Note that the gradient of the resulting output can also be obtained in the same forward pass implementation. Next, Figure 12.14 shows the data structure used by BackPropagation and Backward Pass algorithms. Figure 12.14: Backward Pass (Back Propagation) For all layers, excluding the input layer, a corresponding matrix table is produced for the deltas. Next, let us see an MLP algorithm for the BackPropagation that shows the equations used for the deltas: \\[ \\begin{array}{ll} \\mathbf{Input}: \\text{(n is no. of samples)}\\\\ \\ \\ \\ \\text{dataset}: \\{x_{(1,1)}, x_{(1,2)}, ..., x_{(n,p)}: x\\ \\in\\ \\mathbb{R}^{nxp}\\}\\ \\rightarrow \\text{(p is no. of features)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ : \\{y_{(1,1)}, y_{(1,2)}, ..., y_{(n,o)}: y\\ \\in\\ \\mathbb{R}^{nxo}\\}\\ \\rightarrow \\text{(o is no. of neurons)}\\\\ \\ \\ \\ \\text{parameters}: \\{\\omega_{(1,1)}, \\omega_{(1,2)}, ..., \\omega_{(p,h)}: x\\ \\in\\ \\mathbb{R}^{pxh}\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow \\text{(p is no. of inputs, h is no. of target outputs)}\\\\ \\ \\ \\ \\text{forward-pass output}: act.output\\\\ \\ \\ \\ \\text{activation function}: f(x) \\rightarrow e.g. \\begin{cases} sigmoid(x) &amp; \\text{if output layer}\\\\ leaky.relu(x) &amp; \\text{if hidden layer} \\end{cases}\\\\ \\ \\ \\ \\text{gradient function}: g(f(x)) = f(x) (1 - f(x))\\ \\ \\ \\ \\ \\ \\ \\ \\text{(e.g. sigmoid gradient)} \\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ \\text{loop}\\ L\\ in\\ H:1 \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ gradient.output = act.output^{(L)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ net.input =\\ \\ \\ \\begin{cases} X &amp; \\text{(if input layer)}\\\\ act.output^{(L-1)} &amp; \\text{(otherwise)} \\end{cases}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}}{\\partial \\omega} =\\ \\ \\ \\begin{cases} (t - o) \\times act.output^{(L)} &amp; \\text{(if output layer, where L=H)}\\\\ {\\delta}^{(L+1)} \\cdot transpose \\left({\\omega}^{(L+1)}\\right) &amp; \\text{(if hidden layer)}\\\\ \\end{cases}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ {\\delta}^{(L)} =\\ \\ \\frac{\\partial \\mathcal{L}}{\\partial \\omega} \\times g(act.output^{(L)})\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ({\\nabla_{\\omega}}\\mathcal{L})^{(L)} = transpose(net.input) \\cdot \\delta^{(L)}\\\\ \\ \\ \\ \\text{end loop}\\\\ \\mathbf{Output}: {\\nabla_{\\omega}} \\mathcal{L} \\end{array} \\] Note that Kronecker product, denoted by the symbol (\\(\\bigotimes\\)), also applies to \\({\\Delta _w}^{(L)}\\) where every element is multiplied to the matrix: \\[\\begin{align} \\begin{array}{ll} {\\Delta _w}^{(L)}_{\\mathbf{h \\times o}} &amp;= transpose(net.input) \\cdot \\delta^{(L)}\\\\ &amp;= \\sum_i^n \\left\\{reshape\\left(net.input_{(i,)} \\otimes {\\delta}^{(L)}_{(i,)} \\right)\\right\\} \\in \\mathbb{R}^{\\mathbf{h \\times o}} \\end{array} \\label{eqn:eqnnumber608} \\end{align}\\] where “reshape” follows the shape of \\(\\omega\\). For example, using transpose, we have: net.input = matrix(c(1,2,3,4,5,6), nrow=3, byrow=TRUE) delta = matrix(c(0.1,0.2,0.3,0.4,0.5,0.6), nrow=3, byrow=TRUE) t(net.input) %*% delta ## [,1] [,2] ## [1,] 3.5 4.4 ## [2,] 4.4 5.6 Using Kronecker product, we have: reshape &lt;- function(m, r, c) { matrix(m, nrow=r, ncol=c, byrow=TRUE)} s = matrix(rep(0, 4), nrow=2) for (i in 1:nrow(net.input)) { s = s + reshape( kronecker(net.input[i,], delta[i,]), 2, 2) } s ## [,1] [,2] ## [1,] 3.5 4.4 ## [2,] 4.4 5.6 For the Backward Pass, we have the following MLP pseudocode for the Gradient Descent algorithm: \\[ \\begin{array}{ll} \\mathbf{Input}:\\\\ \\ \\ \\ \\text{parameters}: \\{\\omega_{(1,1)}, \\omega_{(1,2)}, ..., \\omega_{(p,h)}: x\\ \\in\\ \\mathbb{R}^{pxh}\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow \\text{(p are inputs, h are target outputs)}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{gradient}: \\{\\Delta \\omega_{(1,1)}, \\Delta \\omega_{(1,2)}, ..., \\Delta \\omega_{(p,h)}: x\\ \\in\\ \\mathbb{R}^{pxh}\\}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\rightarrow \\text{(p are inputs, h are target outputs)}\\\\ \\ \\ \\ \\text{learning rate}: \\eta \\\\ \\mathbf{Algorithm}:\\\\ \\ \\ \\ \\text{loop}\\ L\\ in\\ 1:H \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\omega^{(L)} = \\omega^{(L)} - \\eta {\\nabla_{\\omega}}\\mathcal{L}^{(L)}\\\\ \\ \\ \\ \\text{end loop} \\\\ \\mathbf{Output}:\\\\ \\ \\ \\ \\ \\omega\\ \\ \\ \\ \\ \\ \\text{(updated)} \\end{array} \\] Now, let us review our example implementation of forward.pass(.), backward.pass(.), and back.propagation(.) functions for a vanilla neural network. As for the presence of the batchnorm functions and drop.out(.), a detailed discussion is up ahead in later section. For our Forward Feed, we use forward.pass(.) function implemented like so: forward.pass &lt;- function(X, layers, afunc=&quot;sigmoid&quot;, mode=&quot;train&quot;) { afunc = get(afunc) H = length(layers) act.output = X for (L in 1:H) { # add constant (bias) layer = layers[[L]] net.input = cbind(rep(1, nrow(X)), act.output) net.input = net.input %*% layer$omega$weight if (L == H) { # output layer act.output = activation(net.input, afunc) } else { act.output = net.input if (!is.null(layer$drop)) { act.output = drop.out(act.output, prob=layer$drop) } if (layer$batchnorm == TRUE) { if (mode == &quot;train&quot;) { normalized = batchnorm.forward(act.output, layer) act.output = normalized$act.output layer$moments = normalized$moments } else { # if test normalized = batchnorm.prediction(act.output, layer) act.output = normalized$prediction } } act.output = activation(net.input, leaky.relu) } layer$output = act.output layers[[L]] = layer } list(&quot;layers&quot; = layers) } For our Back Propagation, we use back.propagation(.) function implemented like so: back.propagation &lt;- function(X, Y, model, afunc=&quot;sigmoid&quot;) { afunc.loss = get(paste0(&quot;gradient.&quot;, afunc, &quot;.loss&quot;)) actfunc = get(paste0(&quot;gradient.&quot;, afunc)) H = length(model$layers) delta = list() delta.params = list() layers = model$layers for (L in H:1) { layer = layers[[L]] delta.params[[L]] = list(&quot;omega&quot;= NULL, &quot;gamma&quot;= NULL, &quot;beta&quot;= NULL) act.output = layer$output if (L &gt; 1) { net.input = layers[[L-1]]$output } else { net.input=X } # include constant for bias net.input = cbind(rep(1, nrow(X)), net.input) if (L == H) { # output layer if (afunc == &quot;sigmoid&quot; || afunc == &quot;softmax&quot;) { # the derivation of delta cancels out the activation gradient, # e.g. simplified version: delta = o - t delta[[L]] = gradient.loss(Y, act.output, gradient.relu.loss) } else { gradient.loss = gradient.loss(Y, act.output, afunc.loss) gradient.output = gradient.activation(act.output, actfunc) delta[[L]] = gradient.loss * gradient.output } } else { net.weights = layers[[L+1]]$omega$weight gradient.loss = delta[[L+1]] %*% t(net.weights) gradient.loss = gradient.loss[,-1] # exclude bias gradient.output = gradient.activation(act.output, gradient.leaky.relu) bnorm = FALSE if (layers[[L]]$batchnorm ==TRUE) { normalized = batchnorm.backward( gradient.output, layer$batch.gamma$weight, layer$moments) gradient.output = normalized$gradient.output delta.params[[L]]$gamma = normalized$delta.gamma delta.params[[L]]$beta = normalized$delta.beta } delta[[L]] = gradient.loss * gradient.output } delta.params[[L]]$omega = clipping(t(net.input) %*% delta[[L]]) } list( &quot;delta.output&quot; = delta, &quot;delta.params&quot; = delta.params) } For our updates of parameters as we propagate our gradients backward, we use backward.pass(.) function implemented like so: backward.pass &lt;- function(model, delta.params, eta, t, optimize) { eps = 1e-10 layers = model$layers H = length(layers) for (L in 1:H) { layer = layers[[L]] if (optimize == &quot;sgd&quot;) { layer$omega$weight = layer$omega$weight - eta * delta.params[[L]]$omega } else if (optimize == &quot;adam&quot;) { layer$omega = adam(layer$omega, delta.params[[L]]$omega, eta, t) } if (layer$batchnorm == TRUE) { if (optimize == &quot;sgd&quot;) { layer$batch.gamma$weight = layer$batch.gamma$weight - eta * delta.params[[L]]$gamma layer$batch.beta$weight = layer$batch.betya$weight - eta * delta.params[[L]]$beta } else if (optimize == &quot;adam&quot;) { layer$batch.gamma = adam(layer$batch.gamma, delta.params[[L]]$gamma, eta, t) layer$batch.beta = adam(layer$batch.beta, delta.params[[L]]$beta, eta, t) } layer$moving.mu = layer$moments$moving.mu layer$moving.variance = layer$moments$moving.variance } layers[[L]]$omega = layer$omega layers[[L]]$batch.gamma = layer$batch.gamma layers[[L]]$batch.beta = layer$batch.beta layers[[L]]$moving.mu = layer$moving.mu layers[[L]]$moving.variance = layer$moving.variance } layers } Finally, we come down to the MLP implementation itself. Our implementation uses mini-batch SGD with sgd and adam optimization. Our dataset is split into mini batches using createFolds(.) function from caret library to generate mini batches. library(caret) get.batch.dnn &lt;- function(sample.data, k, t) { set.seed(t) n = nrow(sample.data) shuffle = sample.int(n = n, size=n, replace=FALSE) createFolds(shuffle, k = k, returnTrain = FALSE) } my.MLP &lt;- function(Xset, Yset, layers, afunc=&quot;sigmoid&quot;, console=FALSE, optimize=&quot;sgd&quot;, minibatch = 30, eta = 0.001, epoch=100, tol=1e-10 ) { options(digits = 8) # 8 digits precision for our example eta = c(eta) total.loss = old.delta.params = NULL old.loss = Inf k = ceiling(nrow(Xset) / minibatch) if (console==TRUE) { print(paste0(&quot;Batch Count:&quot;, k)) } n = 0 for (t in 1:epoch) { # one dataset pass per epoch n = n + 1 batch.loss = NULL for (batch in get.batch.dnn(Xset, k, t)) { X = Xset[batch,] Y = Yset[batch,] model = forward.pass(X, layers, afunc) backprop = back.propagation(X, Y, model, afunc ) layers = backward.pass(model, backprop$delta.params, eta, (t - 1) * k + n, optimize) loss = get.loss(Y, layers, afunc) batch.loss = c(batch.loss, loss) } total.loss = c(total.loss, mean(batch.loss)) if (is.na(loss)) { print(paste0(&quot;NaN loss encountered at &quot;, t)); break } if (!is.finite(loss)) { print(paste0(&quot;Infinite Loss:&quot;, loss)); break} if (abs(old.loss - mean(batch.loss)) &lt;= tol) { print(paste0(&quot;Tolerance level reached:&quot;, loss, &quot; at &quot;, t)); break } old.loss = mean(batch.loss) old.delta.params = backprop$delta.params if (n %% 10 == 0 &amp;&amp; console==TRUE) { print(paste0(&quot;epoch:&quot;, t, &quot; loss:&quot;, mean(batch.loss))) flush.console() } } L = length(layers) model = list(&quot;layers&quot; = model$layers, &quot;cost&quot; = total.loss, &quot;delta.params&quot; = old.delta.params, &quot;afunc&quot; = afunc, &quot;last.iteration&quot; = t) } Applying the implementation, we start with a simple dataset. Our dataset assumes having four samples with two features (X) and two targets (Y). X = matrix( c(0.12, 0.18, 0.13, 0.21, 0.15, 0.30, 0.18, 0.40), nrow=4, byrow=TRUE) Y = matrix( c(0.05, 0.95, 0.02, 0.98, 0.03, 0.97, 0.07, 0.92), nrow=4, byrow=TRUE) To be able to feed our data to MLP, we also construct a simple parameter structure using a learnable set of coefficients. omega = list() omega[[1]]=matrix(c(0.05, 0.40, 0.21, 0.34, 0.19, 0.67), nrow=3, byrow=TRUE) omega[[2]]=matrix(c(0.18, 0.27, 0.09, 0.06, 0.30, 0.15), nrow=3, byrow=TRUE) omega[[3]]=matrix(c(0.25, 0.05, 0.03, 0.35, 0.04, 0.40), nrow=3, byrow=TRUE) structure = list() di = dim(omega[[1]]) for (L in 1:3) { params = list(&quot;weight&quot; = omega[[L]], &quot;rho&quot; = array(0, di), &quot;nu&quot; = array(0, di) ) structure[[L]] = list(&quot;size&quot; = 2, &quot;omega&quot; = params, &quot;batchnorm&quot; = FALSE ) } str(structure) ## List of 3 ## $ :List of 3 ## ..$ size : num 2 ## ..$ omega :List of 3 ## .. ..$ weight: num [1:3, 1:2] 0.05 0.21 0.19 0.4 0.34 0.67 ## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0 ## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0 ## ..$ batchnorm: logi FALSE ## $ :List of 3 ## ..$ size : num 2 ## ..$ omega :List of 3 ## .. ..$ weight: num [1:3, 1:2] 0.18 0.09 0.3 0.27 0.06 0.15 ## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0 ## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0 ## ..$ batchnorm: logi FALSE ## $ :List of 3 ## ..$ size : num 2 ## ..$ omega :List of 3 ## .. ..$ weight: num [1:3, 1:2] 0.25 0.03 0.04 0.05 0.35 0.4 ## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0 ## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0 ## ..$ batchnorm: logi FALSE Let us now model an MLP using the data points and parameters above: mlp.model.sigmoid = my.MLP(X, Y, structure, afunc=&quot;sigmoid&quot;, eta=0.1, epoch=100, console=TRUE) ## [1] &quot;Batch Count:1&quot; ## [1] &quot;epoch:10 loss:0.192948734912118&quot; ## [1] &quot;epoch:20 loss:0.181075498552071&quot; ## [1] &quot;epoch:30 loss:0.180865970971357&quot; ## [1] &quot;epoch:40 loss:0.180825075208516&quot; ## [1] &quot;epoch:50 loss:0.180799577608614&quot; ## [1] &quot;epoch:60 loss:0.180776485995595&quot; ## [1] &quot;epoch:70 loss:0.180754172643832&quot; ## [1] &quot;epoch:80 loss:0.180732413713138&quot; ## [1] &quot;epoch:90 loss:0.180711164234054&quot; ## [1] &quot;epoch:100 loss:0.180690402798621&quot; The final optimized parameters are obtained below after 100 epoch: mlp.model = mlp.model.sigmoid mlp.model$layers[[1]]$omega$weight ## [,1] [,2] ## [1,] 0.18386740 0.87451275 ## [2,] 0.22229200 0.38155529 ## [3,] 0.20015635 0.69854820 mlp.model$layers[[2]]$omega$weight ## [,1] [,2] ## [1,] 0.74517894 0.85117204 ## [2,] 0.17553396 0.14730975 ## [3,] 0.69936942 0.55839558 mlp.model$layers[[3]]$omega$weight ## [,1] [,2] ## [1,] -0.84064501 0.69507475 ## [2,] -0.74243477 0.73518925 ## [3,] -0.72260868 0.78560822 Compare that to the original values: omega ## [[1]] ## [,1] [,2] ## [1,] 0.05 0.40 ## [2,] 0.21 0.34 ## [3,] 0.19 0.67 ## ## [[2]] ## [,1] [,2] ## [1,] 0.18 0.27 ## [2,] 0.09 0.06 ## [3,] 0.30 0.15 ## ## [[3]] ## [,1] [,2] ## [1,] 0.25 0.05 ## [2,] 0.03 0.35 ## [3,] 0.04 0.40 Note that the COST decreases as the parameters are optimized for each iteration. head(mlp.model$cost, n=15) ## [1] 0.69242946 0.58055032 0.48232118 0.39731471 0.32746102 0.27462355 ## [7] 0.23823521 0.21510574 0.20115424 0.19294873 0.18815312 0.18533726 ## [13] 0.18366618 0.18266038 0.18204485 Let us plot the COST (see Figure 12.15). x = seq(1, length(mlp.model.sigmoid$cost)) y = mlp.model.sigmoid$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;MLP Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) color = c(&quot;darksalmon&quot;, &quot;brown&quot;) lines(x, y, col=color[1], lwd=2) Figure 12.15: MLP Plot In terms of prediction, we implement prediction(.) function to use the forward.pass(.) function like so: my.predict &lt;- function(X, model) { options(digits = 8) # 8 digits precision for our example response = forward.pass(X, model$layers, afunc=model$afunc, mode=&quot;test&quot;) layers = response$layers L = length(layers) predicted = layers[[L]]$output list(&quot;prediction&quot; = predicted) } Now, for prediction, let us use a new set of data. Here, our MLP implementation requires our samples to be in matrix form: set.seed(2019) new.X = matrix( runif(n=10, min=0, max=1), nrow=5, byrow=TRUE) my.predict(new.X, mlp.model.sigmoid) ## $prediction ## [,1] [,2] ## [1,] 0.024982419 0.97394373 ## [2,] 0.031984152 0.96638168 ## [3,] 0.051672541 0.94489747 ## [4,] 0.039240281 0.95849620 ## [5,] 0.034823196 0.96330124 Comparing the prediction with the target from the training set (to get an idea), we see a close match, though we may tend towards overfitting the closer we get. Y ## [,1] [,2] ## [1,] 0.05 0.95 ## [2,] 0.02 0.98 ## [3,] 0.03 0.97 ## [4,] 0.07 0.92 12.3.7 Deep Neural Network (DNN) So far, we have shown a simple neural network. In this section, let us improve the use of a network structure using our own deep.neural.layers(.) function. Note here that we are merely creating a structure of our network using a sequential list of a randomly generated matrix of weights (with default seed equals 2019). deep.neural.layers &lt;- function(X, ...) { set.seed(2019) K = ncol(X) layers = list(...) parameters = 0 structure = list() l = 0 for (layer in layers) { l = l + 1 if (is.null(layer$batchnorm)) { layer$batchnorm = FALSE } else { layer$batchnorm = TRUE } if (is.null(layer$drop)) { layer$drop = NULL } K = K + 1 # include weights for the bias N = layer$size O = matrix(runif(K * N, min=0, max=1), nrow=K, byrow=TRUE) omega = list(&quot;weight&quot; = O, &quot;rho&quot; = array(0, dim(O)), &quot;nu&quot; = array(0, dim(O)) ) batch.beta = list(&quot;weight&quot; = rep(0, N), &quot;rho&quot; = rep(0, N), &quot;nu&quot; = rep(0, N) ) batch.gamma = list(&quot;weight&quot; = rep(1, N), &quot;rho&quot; = rep(0, N), &quot;nu&quot; = rep(0, N) ) structure[[l]] = list(&quot;size&quot; = layer$size, &quot;batchnorm&quot; = layer$batchnorm, &quot;drop&quot; = layer$drop, &quot;omega&quot; = omega, &quot;batch.gamma&quot; = batch.gamma, &quot;batch.beta&quot; = batch.beta, &quot;moving.variance&quot; = rep(1, N), &quot;moving.mu&quot; = rep(0, N) ) parameters = parameters + K * N K = N } list(&quot;layers&quot; = structure, &quot;parameters&quot; = parameters) } To use the function, consider a simple classification problem. Our recent implementation mainly covers the neural network with the sigmoid activation function for the output layer. Knowing that a sigmoid function squashes the output into a range between 0 and 1 can serve as a probability function for binomial classification. However, to generalize, we can use the softmax function for a multinomial classification. In this section, let us demonstrate softmax for multinomial classification using a simple deep neural network. Our application requires us to convert our dataset labels to a set of one-hot encoded vectors. To help us generate a dataset with a one-hot encoded vector for our target, let us create a function called get.synthetic.samples(.) that can generate some random batch of the dataset with noise: get.synthetic.samples &lt;- function(N, seed=2019) { set.seed(seed) Xset = Yset = list() Xset[[1]] = c(0.10, 0.10); Yset[[1]] = c(1, 0, 0, 0) Xset[[2]] = c(0.90, 0.90); Yset[[2]] = c(0, 1, 0, 0) Xset[[3]] = c(0.10, 0.90); Yset[[3]] = c(0, 0, 1, 0) Xset[[4]] = c(0.90, 0.10); Yset[[4]] = c(0, 0, 0, 1) samples = sample.int(n = 4, size=N, replace=TRUE) X = matrix(0, nrow=N, ncol=2, byrow=TRUE); colnames(X) = c(&quot;X1&quot;, &quot;X2&quot;) Y = matrix(0, nrow=N, ncol=4, byrow=TRUE) colnames(Y) = c(&quot;Y1&quot;, &quot;Y2&quot;, &quot;Y3&quot;, &quot;Y4&quot;) for (i in 1:N) { X[i,] = Xset[[samples[[i]]]] Y[i,] = Yset[[samples[[i]]]] } # Add some decent amount of noise X = X + runif(n=N, min=0.001, max=0.005) list(&quot;X&quot; = X, &quot;Y&quot; = Y) } Barring interpretability of data at the moment while emphasizing the technicality, we concoct a synthetic dataset that can support Softmax. The idea is to have the ability to simulate unique and clear input patterns that can be mapped to unique output patterns and see how our model can correctly fit. Let us split our dataset into train set and test set. Notice the pattern that we explicitly forced into the dataset. train = get.synthetic.samples(N=200) test = get.synthetic.samples(N=20) X = train$X head(cbind(train$X, train$Y), n=10) ## X1 X2 Y1 Y2 Y3 Y4 ## [1,] 0.90179617 0.10179617 0 0 0 1 ## [2,] 0.10355535 0.90355535 0 0 1 0 ## [3,] 0.90421814 0.90421814 0 1 0 0 ## [4,] 0.10473834 0.90473834 0 0 1 0 ## [5,] 0.10198845 0.10198845 1 0 0 0 ## [6,] 0.10436652 0.10436652 1 0 0 0 ## [7,] 0.90298293 0.10298293 0 0 0 1 ## [8,] 0.10399414 0.10399414 1 0 0 0 ## [9,] 0.10132054 0.10132054 1 0 0 0 ## [10,] 0.10137307 0.90137307 0 0 1 0 Next, let us generate our initial neural network structure. Below, we have three layers. Two of the first hidden layers have two neurons, and the last output layer has four neural outputs. dnn = deep.neural.layers(X, list(size=2), list(size=2), list(size=4)) Using our trainset and parameters above, we train our DNN model for a Multi Classification. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, afunc=&quot;softmax&quot;, optimize=&quot;adam&quot;, eta=0.001, epoch=800) Let us compare the prediction and the target using compare.outcome(.): compare.outcome &lt;- function(t, o, n=10) { c = ncol(t) outc = cbind(t, round(o,2)) colnames(outc) = c(paste0(&quot;T&quot;, seq(1,c)), paste0(&quot;O&quot;, seq(1,c))) print(head(outc, n=n)) } output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 row ## T1 T2 T3 T4 O1 O2 O3 O4 ## [1,] 0 0 0 1 0.00 0.00 0.00 1.00 ## [2,] 0 0 1 0 0.00 0.00 1.00 0.00 ## [3,] 0 1 0 0 0.00 1.00 0.00 0.00 ## [4,] 0 0 1 0 0.00 0.00 1.00 0.00 ## [5,] 1 0 0 0 0.97 0.01 0.01 0.01 ## [6,] 1 0 0 0 0.97 0.01 0.01 0.01 ## [7,] 0 0 0 1 0.00 0.00 0.00 1.00 ## [8,] 1 0 0 0 0.97 0.01 0.01 0.01 ## [9,] 1 0 0 0 0.97 0.01 0.01 0.01 ## [10,] 0 0 1 0 0.00 0.00 1.00 0.00 We can see that the trained model matches the target by reviewing the result. We can perform prediction and measure performance using ROC and other related metrics relevant to precision and recall. 12.3.8 Vanishing and Exploding Gradient In this section, let us simulate conditions in which gradients tend to have extremely negative or positive values. Here, we edit get.synthetic.samples(.) function to generate dataset intended for DNN with RELU output. get.synthetic.samples &lt;- function(N, seed=2019) { set.seed(seed) Xset = Yset = list() Xset[[1]] = c(0.10, 0.90); Yset[[1]] = c(0.90, 0.10) Xset[[2]] = c(0.20, 0.80); Yset[[2]] = c(0.80, 0.20) Xset[[3]] = c(0.30, 0.70); Yset[[3]] = c(0.70, 0.30) Xset[[4]] = c(0.40, 0.60); Yset[[4]] = c(0.60, 0.40) Xset[[5]] = c(0.50, 0.50); Yset[[5]] = c(0.50, 0.50) Xset[[6]] = c(0.60, 0.40); Yset[[6]] = c(0.40, 0.60) Xset[[7]] = c(0.70, 0.30); Yset[[7]] = c(0.30, 0.70) Xset[[8]] = c(0.80, 0.20); Yset[[8]] = c(0.20, 0.80) Xset[[9]] = c(0.90, 0.10); Yset[[9]] = c(0.10, 0.90) samples = sample.int(n = 9, size=N, replace=TRUE) X = matrix(0, nrow=N, ncol=2, byrow=TRUE); colnames(X) = c(&quot;X1&quot;, &quot;X2&quot;) Y = matrix(0, nrow=N, ncol=2, byrow=TRUE); colnames(Y) = c(&quot;Y1&quot;, &quot;Y2&quot;) for (i in 1:N) { X[i,] = Xset[[samples[[i]]]] Y[i,] = Yset[[samples[[i]]]] } # Add some decent amount of noise X = X + runif(n=N, min=0.01, max=0.05) Y = Y + runif(n=N, min=0.01, max=0.05) list(&quot;X&quot; = X, &quot;Y&quot; = Y) } train = get.synthetic.samples(N=50) test = get.synthetic.samples(N=10) X = train$X head(cbind(train$X, train$Y), n=10) ## X1 X2 Y1 Y2 ## [1,] 0.72217088 0.32217088 0.32915548 0.72915548 ## [2,] 0.71349361 0.31349361 0.34168717 0.74168717 ## [3,] 0.31142471 0.71142471 0.72626247 0.32626247 ## [4,] 0.62385755 0.42385755 0.44342895 0.64342895 ## [5,] 0.13064818 0.93064818 0.92470781 0.12470781 ## [6,] 0.12493056 0.92493056 0.93566241 0.13566241 ## [7,] 0.84983223 0.24983223 0.21899916 0.81899916 ## [8,] 0.14664758 0.94664758 0.93292818 0.13292818 ## [9,] 0.11212189 0.91212189 0.91777823 0.11777823 ## [10,] 0.61198812 0.41198812 0.41205500 0.61205500 Here, we use a DNN that is wide or shallow - meaning that we have a small number of layers with a large number of neurons. For example, below, we create three hidden layers with 100 neurons for the first two layers and a size of 2 neurons for the third layer. set.seed(2021) layers = list(&quot;X&quot; = X, list(size=100), list(size=100),list(size=2) ) dnn = do.call(deep.neural.layers, layers) The number of parameters generated for our DNN is 10602 parameters. Let us now train our network. Note that we use the whole dataset as our minibatch to force only one iterative pass and be able to capture the error in training. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=50, eta=0.001, epoch=200, afunc=&quot;leaky.relu&quot;) ## [1] &quot;Infinite Loss:Inf&quot; The model results in an early stop and does not complete the iteration. We also notice that COST becomes extremely large. mlp.model.deep$last.iteration ## [1] 4 tail(mlp.model.deep$cost, n=10) ## [1] 6.5924684e+06 7.4100188e+19 1.3648587e+85 Inf Also, we begin to see signs of exploding gradient given the extremely large positive values. See below: mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] ## [,1] [,2] [,3] ## 1.2259206e+72 1.1754450e+72 1.1790274e+72 ## X1 6.0578526e+71 5.8084292e+71 5.8261314e+71 ## X2 6.9268430e+71 6.6416402e+71 6.6618818e+71 If we evaluate the performance of our prediction, the outcome results in NaN. Otherwise, it results in extremely large negative number (e.g. \\(-\\infty\\)). output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 rows ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 -4.6659318e+205 -5.9787324e+205 ## [2,] 0.33325993 0.73325993 -4.6520722e+205 -5.9609733e+205 ## [3,] 0.74788943 0.34788943 -4.6806885e+205 -5.9976410e+205 ## [4,] 0.41684077 0.61684077 -4.6142492e+205 -5.9125084e+205 ## [5,] 0.91527475 0.11527475 -4.7837296e+205 -6.1296736e+205 ## [6,] 0.91044131 0.11044131 -4.7762117e+205 -6.1200405e+205 ## [7,] 0.23370034 0.83370034 -4.5557140e+205 -5.8375039e+205 ## [8,] 0.94793236 0.14793236 -4.7844383e+205 -6.1305817e+205 ## [9,] 0.93496225 0.13496225 -4.7431521e+205 -6.0776792e+205 ## [10,] 0.42822478 0.62822478 -4.6490108e+205 -5.9570505e+205 Similarly, let us now try to use a DNN that is thin or deep - meaning that we have increased the number of layers with a smaller number of neurons. set.seed(2021) layers = list(&quot;X&quot; = X, list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=2)) dnn = do.call(deep.neural.layers, layers) The number of parameters generated for our DNN is 237 parameters. Let us now train our network. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=50, eta=0.001, epoch=200, afunc=&quot;leaky.relu&quot;) ## [1] &quot;Infinite Loss:Inf&quot; The model results in an early stop and does not complete the iteration. We also notice that the COST becomes extremely large. mlp.model.deep$last.iteration ## [1] 3 tail(mlp.model.deep$cost, n=10) ## [1] 2.6917433e+06 4.5512916e+70 Inf Also, we begin to see signs of exploding gradient given the extremely large negative values. See below: mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] ## [,1] [,2] [,3] ## -1.8063676e+67 -2.1878042e+67 -6.3399875e+66 ## X1 -8.9154808e+66 -1.0798094e+67 -3.1291548e+66 ## X2 -1.0217233e+67 -1.2374726e+67 -3.5860435e+66 If we evaluate the performance of our prediction, the outcome results in NaN. Otherwise, it results in an extremely large positive number (e.g. \\(\\infty\\)). output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 rows ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 Inf Inf ## [2,] 0.33325993 0.73325993 Inf Inf ## [3,] 0.74788943 0.34788943 Inf Inf ## [4,] 0.41684077 0.61684077 Inf Inf ## [5,] 0.91527475 0.11527475 Inf Inf ## [6,] 0.91044131 0.11044131 Inf Inf ## [7,] 0.23370034 0.83370034 Inf Inf ## [8,] 0.94793236 0.14793236 Inf Inf ## [9,] 0.93496225 0.13496225 Inf Inf ## [10,] 0.42822478 0.62822478 Inf Inf This section shows that a very thin or very deep network can cause an exploding gradient. However, for vanishing gradient, it helps to use the sigmoid activation function to simulate the phenomenon. We know that a sigmoid output is squashed between 0 and 1. Asymptotically, the output may never reach zero or one. It is this asymptotic nature of sigmoid that the more layers or more neurons we use, the product of such fractions gets smaller to the point that the effect on learning slows down. 12.3.9 Dead Relu We know that a RELU activation function drops to a flat zero towards the right, eventually manifesting the so-called Dying RELU because certain gradients end up with zero values throughout the training. In contrast to the effect of vanishing gradient in which learning slows down, a Dead Relu can completely stop learning. One of the many experiments to try in this section is to simulate a Dying Relu. To do that, we use our original forward.pass(.) function and back.propagation(.) function such that we default the activation functions of the hidden layers to RELU instead of using Leaky RELU. forward.pass &lt;- function(X, layers, afunc=&quot;sigmoid&quot;, mode=&quot;train&quot;) { ... act.output = activation(net.input, relu) ... } back.propagation &lt;- function(X, Y, model, afunc=&quot;sigmoid&quot;) { ... gradient.output = gradient.activation(act.output, gradient.relu) ... } If we then use a very thin or very deep network model, we should begin to see some of our gradients always set to zero throughout the training. mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] We leave readers to experiment on this topic. In the following sections, we discuss some tune-ups to overcome such conditions as vanishing gradients, exploding gradients, and overfitting. 12.3.10 Gradient Clipping (GC) Clipping is a technique that clips a value within a minimum and maximum range. Similar to Batch Normalization, we can use such a technique to avoid exploding gradients as our case allows while at the same time achieving some level of performance improvement. To illustrate, let us review our original back.propagation(.) function with emphasis on clipping: back.propagation &lt;- function(X, Y, model, params, afunc=&quot;sigmoid&quot;) { ... # gradient clipping delta.omega[[L]] = clipping( t(net.input) %*% delta[[L]] ) ... } A simple implementation of clipping is written as such: To disable clipping: clipping &lt;- function(x, x.min=-Inf, x.max=Inf) { pmax( pmin(x, x.max), x.min) } To enable clipping: clipping &lt;- function(x, x.min=-1, x.max=1) { pmax( pmin(x, x.max), x.min) } Using the following layers as before, let us now see if we are able to avoid the exploding gradient condition. set.seed(2021) layers = list(&quot;X&quot; = X, list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=2)) dnn = do.call(deep.neural.layers, layers) Given that, let us train our model: mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=10, eta=0.001, epoch=1800, afunc=&quot;leaky.relu&quot;) The model does not result in an early stop and completes the iteration. We also notice that the COST becomes stable. mlp.model.deep$last.iteration ## [1] 1800 tail(mlp.model.deep$cost, n=10) ## [1] 0.075484215 0.073736657 0.074051346 0.074300869 0.074657774 ## [6] 0.074209654 0.074944662 0.073766241 0.074005804 0.074472293 Also, we do not see extremely large values. See below: mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] ## [,1] [,2] [,3] ## -0.0022213189 -0.0063910013 -0.000016248965 ## X1 -0.0069608285 -0.0201426031 -0.000046744422 ## X2 0.0046131129 0.0133853472 0.000029664716 See a plot of the COST for the model in Figure 12.16. x = seq(1, length(mlp.model.deep$cost)) y = mlp.model.deep$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;DNN Plot (Gradient Clipping)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) color = c(&quot;darksalmon&quot;, &quot;brown&quot;) lines(x, y, col=color[1], lwd=2) Figure 12.16: DNN Plot (Gradient Clipping) Reviewing the performance, we see that the predicted result closely matches the target. output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.56 0.50 ## [2,] 0.33325993 0.73325993 0.56 0.50 ## [3,] 0.74788943 0.34788943 0.56 0.50 ## [4,] 0.41684077 0.61684077 0.56 0.50 ## [5,] 0.91527475 0.11527475 0.56 0.50 ## [6,] 0.91044131 0.11044131 0.56 0.50 ## [7,] 0.23370034 0.83370034 0.56 0.50 ## [8,] 0.94793236 0.14793236 0.56 0.50 ## [9,] 0.93496225 0.13496225 0.56 0.49 ## [10,] 0.42822478 0.62822478 0.56 0.50 The important note is that we can avoid the exploding gradient by clipping our gradients. At the same time, we should still be able to obtain higher performance by adjusting the learning rate, epoch limit, and tolerance level. Additionally, notice that our minimum and maximum thresholds are hand-selected for our clips. The chosen values are heuristically obtained. However, other literature may provide better ways to derive such thresholds automatically. We leave readers to investigate such techniques. 12.3.11 Parameter Initialization Deciding on the appropriate parameter initialization is an essential step in DNN that cannot be taken lightly without leading to failed convergence or very slow convergence. Here, let us introduce three initialization methods used: Xavier (Glorot) Initialization - This initialization is introduced by Xavier Glorot and Yoshua Bengio in (2010). The idea behind the initialization is to generate random values for all weights based on a choice of uniform distribution or normal distribution using the below formulation (derivation is not included): \\[\\begin{align} \\text{glorot.init} = \\frac{1}{n^{[l]} + n^{[l-1]}}, \\ \\ \\ \\ \\ \\sigma = \\sqrt{(2 \\times \\text{glorot.init})} \\ \\ \\ \\ \\ \\ limit = \\sqrt{(6 \\times glorot.init)} \\end{align}\\] \\[\\begin{align} \\mathbf{W}^{(l)} \\sim \\mathcal{N}\\left(\\mu = 0,\\sigma = \\sigma\\right) \\ \\ \\ \\ \\ \\ \\mathbf{W}^{(l)} \\sim \\mathcal{U}\\left(\\text{min = -limit, max = +limit}\\right) \\end{align}\\] where \\(\\mathbf{n^{(l)}}\\) is the number of input neurons, \\(\\mathbf{n^{(l-1)}}\\) is the number of output neurons, and l refers to a layer. The emphasis is on scaling variance to constrain weights from starting with very small or very large values. Such initialization is used for Sigmoid and Softmax activations. Kaiming/He Initialization - the initialization is a variant of Xavier initialization introduced by Kaiming He et al. (2015), which considers only the number of input neurons: \\[\\begin{align} \\text{he.init} = \\frac{1}{n^{[l]} }, \\ \\ \\ \\ \\ \\sigma = \\sqrt{(2 \\times \\text{he.init})} \\ \\ \\ \\ \\ \\ limit = \\sqrt{(6 \\times he.init)} \\end{align}\\] Such initialization is used for Relu activation. LeCun Initialization - the initialization introduced by Yann Lecun has the following variant: \\[\\begin{align} \\text{lecun.init} = \\frac{1}{n^{[l]} }, \\ \\ \\ \\ \\ \\sigma = \\sqrt{(3 \\times \\text{lecun.init})} \\ \\ \\ \\ \\ \\ limit = \\sqrt{(1 \\times lecun.init)} \\end{align}\\] Such initialization is used for TanH activation. Below is our example implementation of the initialization methods: net.initialization &lt;- function(size, ni, no, itype = &quot;glorot&quot;, dist=&quot;normal&quot;) { if (itype == &quot;glorot&quot;) { glorot.init = 1 / (ni + no); sd = sqrt( 2 * glorot.init) lim = sqrt( 6 * glorot.init) } else if (itype == &quot;he&quot;) { he.init = 1 / (ni); sd = sqrt( 2 * he.init) lim = sqrt( 6 * he.init) } else if (itype == &quot;lecun&quot;) { lecun.init = 1 / (ni); sd = sqrt( 3 * lecun.init) lim = sqrt( 1 * lecun.init) } if (dist == &quot;normal&quot;) { init = rnorm(n=size, mean=0, sd = sd) } else if (dist == &quot;uniform&quot;) { init = runif(n=size, min=-lim, max=lim) } init } Let us modify our deep.neural.layers(.) function to use the net.initialization(.) function. To do that, we change from: deep.neural.layers &lt;- function(X, ...) { ... O = matrix(runif(K * N, min=0, max=1), nrow=K, byrow=TRUE) ... } to: deep.neural.layers &lt;- function(X, ...) { ... weights = net.initialization( K * N, K, N, itype=&quot;glorot&quot;, dist=&quot;normal&quot; ) O = matrix(weights, nrow=K, byrow=TRUE) ... } To see the effect of the Glorot initialization scheme, let us follow the same line of thought using a wide DNN to demonstrate the benefit, then followed by using a thin DNN after: set.seed(2021) layers = list(&quot;X&quot; = X, list(size=100), list(size=100), list(size=2)) dnn = do.call(deep.neural.layers, layers) The number of parameters generated for our DNN is 10602 parameters. Let us now train our network. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=50, eta=0.001, epoch=500, afunc=&quot;leaky.relu&quot;) This time, the model does not result in an early stop though it reaches our tolerance level. Furthermore, the COST is tamed. mlp.model.deep$last.iteration ## [1] 500 tail(mlp.model.deep$cost, n=10) ## [1] 0.00017286656 0.00017277647 0.00017268774 0.00017260033 ## [5] 0.00017251487 0.00017243049 0.00017234708 0.00017226500 ## [9] 0.00017218433 0.00017210568 In addition, we find no sign of vanishing gradient. See below: mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] ## [,1] [,2] [,3] ## 1.2549305e-06 0.001745049023 -0.0023383996 ## X1 -1.6533580e-05 0.001748552505 -0.0045640682 ## X2 1.0058240e-05 0.000019985832 0.0017300846 If we evaluate the performance of our prediction, the outcome results in values that marginally match the target. output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 rows ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.33 0.74 ## [2,] 0.33325993 0.73325993 0.33 0.74 ## [3,] 0.74788943 0.34788943 0.73 0.31 ## [4,] 0.41684077 0.61684077 0.42 0.64 ## [5,] 0.91527475 0.11527475 0.93 0.13 ## [6,] 0.91044131 0.11044131 0.93 0.13 ## [7,] 0.23370034 0.83370034 0.23 0.83 ## [8,] 0.94793236 0.14793236 0.93 0.13 ## [9,] 0.93496225 0.13496225 0.93 0.13 ## [10,] 0.42822478 0.62822478 0.43 0.64 On the other hand, let us also try to use a DNN that is thin. set.seed(2021) layers = list(&quot;X&quot; = X, list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=5), list(size=2)) dnn = do.call(deep.neural.layers, layers) The number of parameters generated for our DNN is 237 parameters. Let us now train our network. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=50, eta=0.001, epoch=500, afunc=&quot;leaky.relu&quot;) Similarly, the model does not result in an early stop and the iteration also completes. mlp.model.deep$last.iteration ## [1] 500 tail(mlp.model.deep$cost, n=10) ## [1] 0.074299949 0.074299945 0.074299942 0.074299939 0.074299936 ## [6] 0.074299933 0.074299929 0.074299926 0.074299923 0.074299920 However, while it does show that COST is stable, we start to see the gradients getting much smaller. mlp.model.deep$delta.params[[1]]$omega[1:3, 1:3] ## [,1] [,2] [,3] ## 2.6874065e-09 -2.5275332e-07 -3.9684473e-07 ## X1 2.5275477e-05 -2.3778705e-03 -3.7350164e-03 ## X2 -2.5381638e-05 2.3878567e-03 3.7506992e-03 If we evaluate the performance of our prediction, the outcome results in values that are way off compared to the target. output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 rows ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.56 0.5 ## [2,] 0.33325993 0.73325993 0.56 0.5 ## [3,] 0.74788943 0.34788943 0.56 0.5 ## [4,] 0.41684077 0.61684077 0.56 0.5 ## [5,] 0.91527475 0.11527475 0.56 0.5 ## [6,] 0.91044131 0.11044131 0.56 0.5 ## [7,] 0.23370034 0.83370034 0.56 0.5 ## [8,] 0.94793236 0.14793236 0.56 0.5 ## [9,] 0.93496225 0.13496225 0.56 0.5 ## [10,] 0.42822478 0.62822478 0.56 0.5 This is the effect of having a DNN that is too deep. Apart from merely using initialization to mitigate vanishing and exploding gradient, it should be apparent that a DNN that is too wide or too thin may not give any advantage. In architecting a DNN, it sometimes helps to design a decent and simple one. To illustrate, suppose we adjust the number of layers used in our example above and come up with a decent number of layers, e.g., only two hidden layers instead of the 100 hidden layers we used prior, each corresponding to a select number of neurons: (5,5,2) - the first two layers are the hidden layers while the last layer represents the output layer. set.seed(2021) dnn = deep.neural.layers(X, list(&quot;size&quot; = 5), list(&quot;size&quot; = 5), list(&quot;size&quot; = 2)) Let us continue to use the same configurations as before to train our network. mlp.model.deep= my.MLP(train$X, train$Y, dnn$layers, minibatch=10, eta=0.001, epoch=500, afunc=&quot;leaky.relu&quot;) Here, the training completes with no early stop. mlp.model.deep$last.iteration ## [1] 500 head(mlp.model.deep$cost, n=10) # Reviewing first few MSEs ## [1] 0.35822684 0.35964301 0.35890920 0.35693836 0.35793780 0.35811155 ## [7] 0.36048859 0.35749239 0.35851598 0.35929211 tail(mlp.model.deep$cost, n=10) # Reviewing last few MSEs ## [1] 0.00020791055 0.00020416317 0.00020717125 0.00020579111 ## [5] 0.00020944271 0.00020917991 0.00019817092 0.00020185509 ## [9] 0.00020208305 0.00019340582 Reviewing our gradients, the values are not extreme. See below: mlp.model.deep$delta.params[[1]]$omega ## [,1] [,2] [,3] [,4] ## 0.000055319940 -0.0061976654 -0.0072942866 -0.00254141681 ## X1 -0.000090601529 -0.0025536053 -0.0059808568 -0.00251385332 ## X2 0.000158613103 -0.0053286353 -0.0033576009 -0.00074866972 ## [,5] ## 0.0079363948 ## X1 0.0066501195 ## X2 0.0035133448 Let us plot the error (see Figure 12.17). x = seq(1, length(mlp.model.deep$cost)) y = mlp.model.deep$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;DNN Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) color = c(&quot;darksalmon&quot;, &quot;brown&quot;) lines(x, y, col=color[1], lwd=2) Figure 12.17: DNN Plot Let us now use our trained model to test using our test set: output = my.predict(test$X, mlp.model.deep) compare.outcome(test$Y, output$prediction, n =10) # display first 10 rows ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.34 0.73 ## [2,] 0.33325993 0.73325993 0.34 0.73 ## [3,] 0.74788943 0.34788943 0.72 0.33 ## [4,] 0.41684077 0.61684077 0.43 0.64 ## [5,] 0.91527475 0.11527475 0.92 0.12 ## [6,] 0.91044131 0.11044131 0.92 0.12 ## [7,] 0.23370034 0.83370034 0.23 0.85 ## [8,] 0.94793236 0.14793236 0.92 0.12 ## [9,] 0.93496225 0.13496225 0.92 0.12 ## [10,] 0.42822478 0.62822478 0.44 0.63 We see that the set of predicted outcomes is marginally equivalent to the set of targets. 12.3.12 Regularization by Dropouts Neural Networks are not immune from overfitting. As we recall in Chapter 9 (Computational Learning I), we introduce L1-loss and L2-loss to reduce overfitting by amplifying or diminishing the effect of the loss function on the coefficients (weights). Here, in Neural Network, we introduce Dropout to regularize neural networks (Nitish Srivastava, Geoffrey Hinton, et al., (2014)). The idea is to disconnect incoming and outgoing connections to and from a neuron, effectively dropping out the neuron from the network. For example, in Figure 12.18, there are three dropout neurons in H1 layer, one dropout neuron in H2 layer, two dropout neurons in H3 layer, and so on. Figure 12.18: Dropout To use dropouts, we apply the modification to our forward pass equation: \\[\\begin{align} H = activation( X \\cdotp \\omega + b) \\circ \\mathbf{r}\\ \\ \\ \\ \\ \\leftarrow H = activation( X \\cdotp \\omega + b) \\end{align}\\] where r is a Bernoulli distribution, e.g. \\(r \\sim Bernoulli(p)\\), with p probability set preferably greater than or equal to 0.50. A higher probability indicates a lesser dropout. The random dropout is also scaled by \\(1/p\\) to compensate for dropping some neurons. For example, below is our implementation of the drop.out(.) function motivated by a python code segment from CS231n PPT by Fei Fei et. al. drop.out &lt;- function(data, prob) { v = data di = length(v) is.dim = is.matrix(data) || is.array(data) if (is.dim) { di = dim(data); v = c(data) } len = prod(di) p = (runif(n = len, min=0, max=1) &lt; prob) / prob array(v * p, di) } To illustrate further, we modify our original implementation of forward.pass(.) and my.MLP(.) to accommodate our dropout functionality. All other code and functions in our MLP implementation are unchanged. See below: forward.pass &lt;-function(X, params, afunc=&quot;sigmoid&quot;, drop=NULL, batchnorm=NULL) { ... if (!is.null(drop)) { if (length(drop) &gt; 1) { prob = drop[L]} else { prob = drop } } else ... ... } Let us use some arbitrary number of layers like so without dropouts, then train our model. set.seed(2021) dnn = deep.neural.layers(X, list(&quot;size&quot;=5), list(&quot;size&quot;=5), list(&quot;size&quot;=2)) Here, let us train a model without dropout and another model with a 50% dropout. mlp.model.nodrop = my.MLP(train$X, train$Y, dnn$layers, minibatch=10, optimize=&quot;adam&quot;,eta=0.001, epoch=200, afunc=&quot;leaky.relu&quot;) Now, let us the same number of layers but with 50% dropout, then train our model. set.seed(2021) dnn = deep.neural.layers(X, list(&quot;size&quot; = 5, &quot;drop&quot;=0.50), list(&quot;size&quot; = 5, &quot;drop&quot;=0.50), list(&quot;size&quot; = 2)) mlp.model.withdrop = my.MLP(train$X, train$Y, dnn$layers, eta=0.001, epoch=200, afunc=&quot;leaky.relu&quot;) Using the same trainset as before, we can see that our model with dropout can reach our epoch limit with no early stop. Also, notice how the COST oscillates. c(&quot;Epoch (no Dropouts)&quot; = mlp.model.nodrop$last.iteration, &quot;Epoch (with Dropouts)&quot; = mlp.model.withdrop$last.iteration ) ## Epoch (no Dropouts) Epoch (with Dropouts) ## 200 200 head(mlp.model.nodrop$cost, n=10) ## [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971 ## [7] 0.25912319 0.24184753 0.23954416 0.23575337 head(mlp.model.withdrop$cost, n=10) ## [1] 0.35844878 0.35822198 0.35845522 0.35818881 0.35801963 0.35856555 ## [7] 0.35818907 0.35812269 0.35810622 0.35872444 Let us plot the results in which we can see a change in the landscape for COST and a noticeable oscillation for the model with dropout. See Figure 12.19. x = seq(1, length(mlp.model.nodrop$cost)) par(mfrow=c(1,2)) y = mlp.model.nodrop$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;MLP (No Dropouts)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col=&quot;darksalmon&quot;, lwd=2) x = seq(1, length(mlp.model.withdrop$cost)) y = mlp.model.withdrop$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;MLP (With Dropouts)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col=&quot;darksalmon&quot;, lwd=2) Figure 12.19: MLP (Dropouts) Note that dropouts may best suit fully connected neural networks in which neurons in hidden layers can be turned on and off randomly. This behavior somehow simulates the nature of missing data or perturbation; thus, it generalizes the model, effectively resisting an overfit. Note that other Neural networks may not be that accommodating towards dropouts especially for those that are not fully connected. While this may be true on a case-by-case basis, we leave readers to investigate CNN and RNN (which we cover later) as these Neural networks support datasets with Spatial Relationships or Time-Series properties. 12.3.13 Batch Normalization Internal Covariance shift is a phenomenon noted by Serge Loffe and Christian Szegedy in their (2019) paper. The intuition behind the shift emphasizes the data distribution of the activation output being fed to the next layer. Inconsistent distribution in scale (variance) and shift (mean) may manifest across mini-batches that are processed. Batch Normalization intends to reduce the shifts. In other words, it attempts to compose consistency in the data distribution across mini-batches. On the other hand, a paper published by Shibani Santunkar, Dimitris Tsipras, et al. in (2018) explains that Batch Normalization works because it lays out a smoother landscape in the direction of the gradients traveling from layer to layer instead of a more coarse or rugged terrain. To illustrate, using Figure 12.13, let us take the mean and variance of an activation output as starting point. \\[\\begin{align} \\vec{\\mu}_B - \\frac{1}{m}\\sum_{i=1}^m h_i\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\vec{\\sigma^2}_B = \\frac{1}{m}\\sum_{i=1}^m (h_i - \\vec{\\mu}_B)^2 \\end{align}\\] where: \\(\\vec{\\mu}_B \\in \\mathbb{R}^p\\ and\\ \\vec{\\sigma^2} \\in \\mathbb{R}^p\\). Recall that activation output is an output from an activation function. Here, for now, let us represent the activation output as h and m as the number of samples in a mini-batch. Also, p is represented as number of activation neurons. Now, the normalized version is expressed as: \\[\\begin{align} h^{(norm)}_i = \\frac{h_i - \\vec{\\mu}_B}{\\sqrt{\\vec{\\sigma^2}_B + \\epsilon}} \\end{align}\\] We then use the normalized version and scale it down using two learnable hyperparameters: gamma (\\(\\gamma\\)) and beta (\\(\\beta\\)). \\[\\begin{align} \\hat{h}_i = \\gamma * h^{(norm)}_i + \\beta = BN_{\\gamma,\\beta}(h) \\end{align}\\] where \\(h^{(norm)} \\in \\mathbb{R}^{nxp}\\) and \\(i = 1 ... m\\). In R, the use of var(.) gives sample variance using Bessel’s correction, \\(\\frac{1}{(n-1)}\\). Here, we use our batch variance (a.l.a population variance): # not using the Bessel&#39;s correction ( for sample variance). batch.variance &lt;- function(h) { mean( (h - mean(h))^2 ) } Let us obtain the mean (\\(\\vec{\\mu}_B\\)) and variance (\\(\\vec{\\sigma^2}_B\\)) given a simple dataset like so: options(digits=8) (Input = matrix( c(0.12, 0.18, 0.13, 0.21, 0.15, 0.30, 0.18, 0.40), nrow=4, byrow=TRUE)) ## [,1] [,2] ## [1,] 0.12 0.18 ## [2,] 0.13 0.21 ## [3,] 0.15 0.30 ## [4,] 0.18 0.40 (mu = apply(Input, 2, mean)) ## [1] 0.1450 0.2725 (variance = apply(Input, 2, batch.variance )) ## [1] 0.00052500 0.00736875 Given all that, below is our example implementation of a batchnorm forward function: batchnorm.forward &lt;- function(H, layer, eps=1e-8, momentum = 0.90, rmax=1, dmax=0) { gamma = layer$batch.gamma$weight beta = layer$batch.beta$weight moving.mu = layer$moving.mu moving.variance = layer$moving.variance ## For Training # mu = apply(H, 2, mean) H.mu = sweep(H, 2, mu, &quot;-&quot;) var = apply( H.mu^2 , 2, mean) istd = 1 / sqrt(var + eps) H.norm = sweep( H.mu, 2, istd, &quot;*&quot; ) ## For Inference moving.mu = momentum * moving.mu + (1 - momentum) * mu moving.variance = momentum * moving.variance + (1 - momentum) * var ## Now generate the act.output H.hat = H.norm * gamma + beta moments = list(&quot;H.norm&quot; = H.norm, &quot;H.mu&quot; = H.mu, &quot;istd&quot; = istd, &quot;moving.mu&quot; = moving.mu, &quot;moving.variance&quot; = moving.variance) list(&quot;act.output&quot; = H.hat, &quot;moments&quot; = moments) } batchnorm.prediction &lt;- function(H, layer, eps=1e-8) { gamma = layer$batch.gamma$weight beta = layer$batch.beta$weight moving.mu = layer$moving.mu moving.variance = layer$moving.variance istd = 1/sqrt(moving.variance + eps) H.mu = sweep(H, 2, moving.mu, &quot;-&quot;) H.norm = sweep( H.mu, 2, istd, &quot;*&quot; ) H.hat = (H.norm * gamma + beta) list(&quot;prediction&quot; = H.hat) } Notice the inclusion of three parameters, namely running average, running variance, and momentum. We need these parameters for our inference which we cover in our later discussion. From here, it helps to review forward.pass(.) function in MLP Implementation section once more in order to see how we use the batchnorm.forward(.) function. Note that two learnable parameters are passed to the forward.pass(.) and batchnorm.forward(.) functions, namely one with the symbol gamma (\\(\\gamma\\)) and the other one with symbol beta (\\(\\beta\\)). Both correspondingly represent variance (for scaling) and mean (for shifting) such that their convergence ultimately produces the same original (un-normalized) activation output - in other words, a gamma of 1 and beta of 0 readily dissolve the effect to the original distribution. Note that each activation (each neuron) in each layer requires a pair of these scalar parameters. The following list of derivatives is formulated to obtain the deltas we need for our update rules, especially for the gamma (\\(\\gamma\\)) and beta (\\(\\beta\\)) parameters (note that we are excluding derivations of the derivatives): First, we obtain the derivative of the total loss with respect to the normalized output - or input to a layer: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}} = \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}}\\right) \\left(\\frac{\\partial \\hat{h}}{\\partial h^{(norm)}}\\right) = \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}} \\cdotp \\gamma \\end{align}\\] Second, we obtain the derivative of the total loss with respect to gamma (\\(\\gamma\\)) and beta (\\(\\beta\\)) respectively: \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\gamma} = \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}_i}\\right) \\left(\\frac{\\partial \\hat{h}_i}{\\partial \\gamma}\\right) = \\sum_{i=1}^m\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}_i} \\times h^{(norm)}_i \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\beta} = \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}_i}\\right) \\left(\\frac{\\partial \\hat{h}_i}{\\partial \\beta}\\right) = \\sum_{i=1}^m \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\hat{h}_i} \\ \\ \\ \\ where\\ \\left(\\frac{\\partial \\hat{h}_i}{\\partial \\beta}\\right) = 1 \\end{align}\\] Third, we obtain the derivative of the total loss with respect to variance (\\(\\vec{\\sigma^2}_B\\)): \\[\\begin{align} \\begin{array}{ll} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\sigma}^2_B} &amp;= \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}}\\right) \\left(\\frac{\\partial h^{(norm)}}{\\partial \\vec{\\sigma}^2_B}\\right)\\\\ &amp; = \\sum_{i=1}^m \\left[\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}_i} \\times \\left(h_i - \\vec{\\mu}_B\\right) \\right]\\times \\frac{-1}{2} \\left(\\frac{1}{\\sqrt{\\vec{\\sigma^2}_B + \\epsilon}}\\right)^3 \\\\ \\end{array} \\label{eqn:eqnnumber633} \\end{align}\\] Fourth, we obtain the derivative of the total loss with respect to mean (\\(\\vec{\\mu}_B\\)): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\mu}_B} &amp;= \\left[ \\sum_{i=1}^m\\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}_i}\\right) \\left(\\frac{\\partial h^{(norm)}_i}{\\partial \\vec{\\mu}_B}\\right)\\right] + \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\sigma^2}_B}\\right) \\left(\\frac{\\partial \\vec{\\sigma^2}_B}{\\partial \\vec{\\mu}_B}\\right) \\\\ &amp;= \\left[\\sum_{i=1}^m\\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}_i}\\right) \\left(\\frac{-1}{\\sqrt{\\vec{\\sigma^2}_B + \\epsilon}}\\right)\\right] + \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\sigma^2}_B}\\right) \\left( \\frac{\\sum_{i=1}^m -2 (h_i - \\vec{\\mu}_B)}{m}\\right) \\end{align}\\] Fifth, we obtain the derivative of the total loss with respect to \\(\\mathbf{h_i}\\): \\[\\begin{align} \\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h_i} &amp;= \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}_i}\\right) \\left(\\frac{\\partial h^{(norm)}_i}{\\partial h_i}\\right) + \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\sigma^2}_B}\\right) \\left(\\frac{\\partial \\vec{\\sigma^2}_B}{\\partial h_i}\\right) + \\nonumber \\\\ &amp;\\ \\ \\ \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\mu}_B}\\right) \\left(\\frac{\\partial \\vec{\\mu}_B}{\\partial h_i}\\right) \\\\ &amp;= \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial h^{(norm)}_i}\\right) \\left(\\frac{1}{\\sqrt{\\vec{\\sigma^2}_B + \\epsilon}}\\right) + \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\sigma^2}_B}\\right) \\left(\\frac{2(h_i - \\vec{\\mu}_B)}{m}\\right) + \\nonumber \\\\ &amp;\\ \\ \\ \\left(\\frac{\\partial \\mathcal{L}_{(total)}}{\\partial \\vec{\\mu}_B}\\right) \\left(\\frac{1}{m}\\right) \\end{align}\\] Finally, given all that, we now implement a backward pass for our learnable parameters with the following example implementation of batchnorm.backward(.): batchnorm.backward &lt;- function(Dout, gamma, moments) { H.norm = moments$H.norm H.mu = moments$H.mu istd = moments$istd m = apply(H.norm, 2, length) delta.gamma = apply(Dout * H.norm, 2, sum) delta.beta = apply(Dout, 2, sum) delta.H.norm = Dout * gamma delta.std = apply(delta.H.norm * H.mu, 2, sum) delta.var = delta.std * -0.5 * istd^3 delta.Hmu1 = apply( delta.H.norm * -istd, 2, sum) delta.Hmu2 = delta.var * apply( -2 * H.mu,2, mean) delta.mu = delta.Hmu1 + delta.Hmu2 delta.out = sweep(delta.H.norm * istd + delta.var * (2 * H.mu) / m, 2, (delta.mu / m), &#39;+&#39;) list(&quot;gradient.output&quot; = delta.out, &quot;delta.gamma&quot; = delta.gamma, &quot;delta.beta&quot; = delta.beta) } From here, it also helps to review back.propagation(.) function in the same MLP Implementation section in order to see the use of batchnorm.backward(.) function. Additionally, we may have to review backward.pass(.) function which enforces the update rules for hyperparameters, along with the use of Adam optimization: optimize.adam = adam &lt;- function(param, gradient, eta, t) { beta1 = 0.90; beta2 = 0.999; eps=1e-10 param$rho = beta1 * param$rho + (1 - beta1) * gradient param$nu = beta2 * param$nu + (1 - beta2) * gradient^2 rho.hat = param$rho / (1 - beta1^t) nu.hat = param$nu / (1 - beta2^t) phi = eta / (sqrt(nu.hat) + eps) param$weight = param$weight - phi * rho.hat param } Lastly, the deep.neural.layers(.) function incorporates initialization of the two new hyperparameters, namely gamma (\\(\\gamma\\)) and beta (\\(\\beta\\)) for optimization. To illustrate, let us continue to use the following layers. set.seed(2021) layers = list(&quot;X&quot; = X, list(&quot;size&quot; = 5), list(&quot;size&quot; = 5), list(&quot;size&quot; = 2)) dnn = do.call(deep.neural.layers, layers) That gives us about 57 parameters (including biases). Now, let us try modeling DNN without batch normalization. mlp.model.noBN = my.MLP(train$X, train$Y, dnn$layers, minibatch=10, optimize=&quot;adam&quot;, eta=0.001, epoch=100, afunc=&quot;leaky.relu&quot;) mlp.model.noBN$last.iteration ## [1] 100 head(mlp.model.noBN$cost, n=10) ## [1] 0.35804328 0.34882459 0.32788790 0.30374957 0.28486765 0.26892971 ## [7] 0.25912319 0.24184753 0.23954416 0.23575337 tail(mlp.model.noBN$cost, n=10) ## [1] 0.00023908686 0.00022170241 0.00021220855 0.00020488970 ## [5] 0.00019650187 0.00018757701 0.00018158010 0.00017569945 ## [9] 0.00017344208 0.00016758827 mlp.model.noBN$delta.params[[1]]$omega ## [,1] [,2] [,3] [,4] ## -0.00034719214 0.008882285 0.002632272 0.0027318712 ## X1 -0.00018229795 -0.004817867 -0.011059380 -0.0029795080 ## X2 -0.00017847808 0.012816578 0.012179239 0.0052451620 ## [,5] ## -0.0043827349 ## X1 0.0102768353 ## X2 -0.0131979135 Next, we model DNN with batch normalization. For now, let us keep the same learning rate - eta symbol (\\(\\eta\\)). set.seed(2021) layers = list(&quot;X&quot; = X, list(&quot;size&quot; = 5, &quot;batchnorm&quot; = TRUE), list(&quot;size&quot; = 5), list(&quot;size&quot; = 2)) dnn = do.call(deep.neural.layers, layers) mlp.model.BN = my.MLP(train$X, train$Y, dnn$layers, minibatch=10, optimize=&quot;adam&quot;, eta=0.001, epoch=100, afunc=&quot;leaky.relu&quot;) mlp.model.BN$last.iteration ## [1] 100 head(mlp.model.BN$cost, n=10) ## [1] 0.35807389 0.35782997 0.34387280 0.32203210 0.30398898 0.28888063 ## [7] 0.27927648 0.26171434 0.25589441 0.24918166 tail(mlp.model.BN$cost, n=10) ## [1] 0.051191994 0.050035899 0.050687903 0.052175072 0.050586036 ## [6] 0.050977164 0.050522264 0.049580550 0.051174961 0.050063166 mlp.model.BN$delta.params[[1]]$omega ## [,1] [,2] [,3] [,4] [,5] ## 0.026829669 -1 -1 -0.47626824 1 ## X1 0.011910041 -1 -1 -0.18591176 1 ## X2 0.016637621 -1 -1 -0.32358615 1 We can observe that the model with Batch Normalization seems to have a higher COST at every epoch compared to one without normalization. However, if we see the plot, the landscape of the cost function indeed follows a more comfortable trajectory. See Figure 12.20. x = seq(1, length(mlp.model.noBN$cost)) par(mfrow=c(1,2)) y = mlp.model.noBN$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;MLP (No BatchNorm)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col=&quot;darksalmon&quot;, lwd=2) x = seq(1, length(mlp.model.BN$cost)) y = mlp.model.BN$cost plot(NULL, xlim=range(x), ylim=range(y), xlab=&quot;ITERATION&quot;, ylab=&quot;COST&quot;, main=&quot;MLP (With BatchNorm)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col=&quot;darksalmon&quot;, lwd=2) Figure 12.20: MLP (Batch Normalization) Now, in terms of inference, we use moving average and moving variance to keep track of the population average of all mini-batches throughout the training time - granting the choice of normalization is batch normalization versus layer normalization. Such parameters are then used at test time. See our batchnorm.forward(.) function and batchnorm.prediction(.) function. Also, we use momentum denoted by alpha (\\(\\alpha\\)) which serves as a decaying rate for the moments (\\(\\mu\\) and \\(\\sigma^2\\)) during training. The default we use in our implementation is 0.90. \\[\\begin{align} \\mu_{(moving)} &amp;= \\alpha \\times \\mu_{(moving)} + (1 - \\alpha) \\times \\mu_B\\\\ \\sigma^2_{(moving)} &amp;= \\alpha \\times \\sigma^2_{(moving)} + (1 - \\alpha) \\times \\sigma^2_{B} \\end{align}\\] To illustrate, we compare the result of a model without BN (recall that our dataset is concocted to have a RELU pattern for output): output.noBN = my.predict(test$X, mlp.model.noBN) compare.outcome(test$Y, output.noBN$prediction, n =10) ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.34 0.73 ## [2,] 0.33325993 0.73325993 0.34 0.73 ## [3,] 0.74788943 0.34788943 0.72 0.33 ## [4,] 0.41684077 0.61684077 0.43 0.64 ## [5,] 0.91527475 0.11527475 0.92 0.13 ## [6,] 0.91044131 0.11044131 0.92 0.13 ## [7,] 0.23370034 0.83370034 0.23 0.84 ## [8,] 0.94793236 0.14793236 0.92 0.13 ## [9,] 0.93496225 0.13496225 0.92 0.13 ## [10,] 0.42822478 0.62822478 0.43 0.63 and the result of a model with BN output.BN = my.predict(test$X, mlp.model.BN) compare.outcome(test$Y, output.BN$prediction, n =10) ## T1 T2 O1 O2 ## [1,] 0.32623900 0.72623900 0.53 0.53 ## [2,] 0.33325993 0.73325993 0.53 0.54 ## [3,] 0.74788943 0.34788943 0.59 0.47 ## [4,] 0.41684077 0.61684077 0.54 0.53 ## [5,] 0.91527475 0.11527475 0.63 0.42 ## [6,] 0.91044131 0.11044131 0.63 0.42 ## [7,] 0.23370034 0.83370034 0.50 0.57 ## [8,] 0.94793236 0.14793236 0.63 0.42 ## [9,] 0.93496225 0.13496225 0.62 0.43 ## [10,] 0.42822478 0.62822478 0.54 0.52 Note that while the prediction for the normalization seems a bit off in our particular case, we leave readers to try to use a learning rate of 0.01 for improvement. Additionally, the learning speed for batch normalization is more visible in the next section when we deal with CNN against the cifar-10 dataset. Also, the batch normalization step is positioned before the activation function in our implementation. In a section up ahead, dealing with CNN, we shall try to position the batch normalization after the activation function at the convolution layers. We leave readers to investigate the effectiveness of batch normalization based on whether it should be before or after the activation function. 12.3.14 Optimization Our recent discussions put emphasis on vanishing gradients and exploding gradients. Certain techniques are introduced, such as Parameter initialization, Gradient Clipping, and designing a Decent Architecture in terms of the number of layers and neurons to use. In this section, our emphasis is on overfitting, underfitting, and avoiding local minima. We begin to show the importance of adjusting a combination of some performance knobs such as floating-point precision, learning rate, epoch limit, and tolerance level. If we use a very small learning rate, we may end up with an underfit model. If we use a very large learning rate, we may end up with an overfit model, overshooting the target. If our epoch limit is too small (e.g., inducing an early stop), we may underfit, if it is too large and we do not hit a tolerance threshold, we may overfit. Heuristically, we also learn that Batch Normalization and Gradient Clipping can increase performance. This affects the effectiveness of the hyperparameters. Thus they require adjusting along with the use of the mentioned techniques. For Batch Normalization, it helps to tune the learning rate to balance down the learning to a more optimal speed. So far, what we have done heuristically is to find the best value for the learning rate. However, this trial and error approach may not be as effective if we are not careful. Also, in this section, let us delve deeper into the update rule of the vanilla Gradient Descent algorithm, which is implemented in our backward.pass(.) function. \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\eta\\times g^{(t)} \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\ g^{(t)} = \\nabla \\omega \\mathcal{L} \\end{align}\\] Note that the omega symbol (\\(\\omega\\)) represents coefficients or weights (also called parameters) that dictate the degrees of freedom (df) with which our model is fitted. Such parameters are to be tuned optimally to fit the model generally - we call this training the model (or, in other words, machine learning). One of the important components or elements of all these is the learning rate denoted by the eta symbol (\\(\\eta\\)). Specifically, we aim to find the most optimal learning rate. With that, there are enhancements to optimizing the Gradient Descent algorithm. Here, we list about eight optimizers that are regarded as state-of-art optimizers, whether currently or at one point in time in their rights. Note that, in principle, the aim of the different optimizers is to avoid local minima, avoid overfitting, avoid underfitting, and improve performance. Therefore, without covering the derivations, let us briefly explore and review the equations and implementation of each optimizer. First, let us introduce Momentum which is represented by the nu (\\(\\nu\\)) symbol added to Gradient Descent, in particular, to the Stochastic Gradient Descent (SGD). This is also known as SGD with Momentum. The enhancement is as follows: \\[\\begin{align} \\nu^{(t+1)} = \\gamma \\times \\nu^{(t)} + \\eta \\times g^{(t)} \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} =\\omega^{(t)} - \\nu^{(t+1)} \\end{align}\\] with a corresponding implementation: v = 0 for (t in 1:epoch) { v = gamma * v + eta * gradient( ... ) w = w - v } Here, we use momentum to complement our learning rate as a way to apply moving average to the gradient. Additionally, it also comes with a gamma (\\(\\gamma\\)) hyperparameter, a decaying sum that controls the momentum. A starting value to try for gamma (\\(\\gamma\\)) is 0.99. Second, let us introduce Adaptive Gradient (AdaGrad) optimizer formulated by John Duchi, Elad Hazan, and Yoram Singer in (2011). The enhancement is expressed as follows: \\[\\begin{align} \\nu^{(t+1)} = \\nu^{(t)} + (g^{(t)})^2 \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi \\times g^{(t)} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\sqrt{v^{(t+1)} + \\epsilon}} \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) v = v + g^2 pi = eta / sqrt(v + eps) w = w - pi * g } Similarly here, a momentum hyperparameter is enforced as a decaying rate denoted by the nu (\\(\\nu\\)) symbol to control the Learning Rate (\\(\\eta\\)). It should be apparent that if the denominator is smaller, it makes the Learning Rate larger (thus more aggressively fast). Therefore, there is a risk of overshooting. On the other hand, the further into the iteration we get, the smaller and slower the learning rate becomes. Note that the epsilon (\\(\\epsilon\\)) symbol takes an extremely small number that exists only to prevent the division from zero, e.g., \\(\\epsilon =\\) 1e-10. Third, let us introduce the Root Mean Square Propagation (RMSProp) optimizer lectured by Tijmen Tieleman and Geoffrey Hinton in (2012). The enhancement is expressed as follows: \\[\\begin{align} \\nu^{(t+1)} = \\beta \\times \\nu^{(t)} + (1 - \\beta) (g^{(t)})^2 \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi\\ \\times g^{(t)} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\sqrt{v^{(t+1)} + \\epsilon}} \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) v = B * v + (1 - B) * g^2 pi = eta / sqrt(v + eps) w = w - pi * g } RMSProp is a variant of AdaGrad. The presence of the Beta (\\(\\beta\\)) hyperparameter is to control the decay growth of the denominator in the case the Decay Rate grows exponentially. Fourth, let us introduce the Adaptive Delta (AdaDelta) optimizer formulated by Matthew Zeiler in (2012). This optimizer calculates Root Means Square (RMS). The enhancement is expressed as follows: \\[\\begin{align} s^{(t+1)} = \\rho \\times s^{(t)} + (1 - \\rho)(g^{(t+1)})^2 \\ \\ \\ \\ \\ where\\ s = \\mathbb{E}[g^{(t)})^2] \\end{align}\\] \\[\\begin{align} \\pi = \\frac{\\text{RMS}[\\Delta \\omega^{(t)}]^{(t)}}{\\text{RMS}[g^{(t)}]^{(t+1)}} g^{(t)} \\ \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\ \\ \\ \\text{RMS}[g^{(t)}]^{(t+1)} = \\sqrt{s^{(t+1)} + \\epsilon} \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi \\end{align}\\] \\[\\begin{align} \\Delta \\omega^{(t+1)} = \\rho \\times \\Delta \\omega^{(t)} + ( 1 - \\rho) \\times \\pi^2 \\end{align}\\] where the rho (\\(\\rho\\)) symbol represents the Decay Rate. A good starting point to try for the empirical analysis is a value of 0.90. The corresponding implementation is as such: for (t in 1:epoch) { g = gradient( ... ) s = rho * s + (1 - rho) * g^2 pi = sqrt(delta + eps) * ( g / sqrt(s + eps) ) w = w - pi delta = rho * delta + ( 1 - rho) * pi^2 } Note that s carries the average of the first moment of the gradient, and \\(\\Delta \\omega\\) carries the average of the second moment. Fifth, let us introduce Adaptive Moment Estimation (Adam) optimizer formulated by Diederik P. Kingma and Jimmy Lei Ba in (2015). The enhancement is expressed as follows: \\[\\begin{align} \\tau ^{(t+1)} = \\beta_1 \\times \\tau^{(t)} + (1 - \\beta_1) g^{(t)} \\end{align}\\] \\[\\begin{align} \\nu^{(t+1)} = \\beta_2 \\times \\nu^{(t)}+ (1 - \\beta_2) (g^{(t)})^2 \\end{align}\\] \\[\\begin{align} \\hat{\\tau} = \\frac{\\tau^{(t+1)}}{1 - {(\\beta_1)}^t} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\nu} = \\frac{\\nu^{(t+1)}}{1 - {(\\beta_2)}^t} \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi\\ \\times \\hat{\\tau} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\sqrt{\\hat{v} + \\epsilon}} \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) r = B1 * r + (1 - B1) * g v = B2 * v + (1 - B2) * g^2 r.hat = r / ( 1 - B1^t) v.hat = v / ( 1 - B2^t) pi = eta / sqrt(v.hat + eps) w = w - pi * r.hat } Adam is one of the popular optimizers used in DNN. Apart from using the same concept of moving averages as that of SGD with Momentum, here, we use two moments in the form of the rho (\\(\\rho\\)) and nu (\\(\\nu\\)) symbols. The former is a moving average, and the latter is an exponential moving average for the respective gradients, along with their respective bias corrections denoted by rho-hat (\\(\\hat{\\rho}\\)) and nu-hat (\\(\\hat{\\nu}\\)) symbols. Additionally, the moving averages are controlled by two other extra hyperparameters as Decay Rate for the moments in the form of the beta (\\(\\beta\\)) symbols. Empirically and in typical practice, the (\\(\\beta\\)) values default to 0.90 and 0.999, respectively, with a learning rate (\\(\\eta\\)) of 0.001. Sixth, let us introduce a variant of Adam called AMSGrad optimizer formulated by Sashank J. Reddi, Satyen Kale, and Sanjiv Kumar in (2018). The enhancement is expressed as follows: \\[\\begin{align} \\tau ^{(t+1)} = \\beta_1 \\times \\tau^{(t)} + (1 - \\beta_1) g^{(t)} \\end{align}\\] \\[\\begin{align} \\nu^{(t+1)} = \\beta_2 \\times \\nu^{(t)}+ (1 - \\beta_2) (g^{(t)})^2 \\end{align}\\] \\[\\begin{align} \\hat{\\nu}^{(t+1)} = max \\left(\\nu^{(t+1)} , \\hat{\\nu}^{(t)} \\right) \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi\\ \\times \\tau^{(t+1)} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\sqrt{\\hat{v}^{(t+1)} + \\epsilon}} \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) r = B1 * r + (1 - B1) * g v = B2 * v + (1 - B2) * g^2 v.hat = max(v, v.hat) pi = eta / sqrt(v.hat + eps) w = w - pi * r } Like Adam, two moments are also used by AMSGrad for moving averages; however, unlike Adam, the bias corrections are eliminated. In their place, the maximum between current and past gradients is considered for an update, replacing the original formulation of nu-hat (\\(\\hat{\\nu}\\)). Seventh, let us introduce a variant of Adam called Adaptive Max Pooling (AdaMax) optimizer, also formulated by Diederik P. Kingma and Jimmy Lei Ba in the same work in (2015). The enhancement is expressed as follows: \\[\\begin{align} \\tau ^{(t+1)} = \\beta_1 \\times \\tau^{(t)} + (1 - \\beta_1) g^{(t)} \\end{align}\\] \\[\\begin{align} \\nu^{(t+1)} =\\max\\left(\\beta_2 \\times \\nu^{(t)}, |g^{(t)}|\\right) \\end{align}\\] \\[\\begin{align} \\hat{\\tau}^{(t+1)} = \\frac{\\tau^{(t+1)}}{1 - {(\\beta_1)}^t} \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi\\ \\times \\hat{\\tau}^{(t+1)} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\nu^{(t+1)} + \\epsilon } \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) r = B1 * r + (1 - B1) * g v = max(B^2 * v, abs(g) ) r.hat = r / ( 1 - B1^t) pi = eta / sqrt(v + eps) w = w - pi * r.hat } While Adam optimization uses L2-norm (euclidean), the AdaMax optimization uses max-norm, also called infinity norm. This is apparent in the equation used. Empirically, and in common practice, the (\\(\\beta\\)) values also default to 0.90 and 0.999 respectively with a learning rate (\\(\\eta\\)) of 0.001. Eight, let us introduce Nesterov-Accelerated Adaptive Moment (Nadam) optimizer formulated by Timothy Dozat in (2016). The enhancement is expressed as follows: \\[\\begin{align} \\tau ^{(t+1)} = \\beta_1 \\times \\tau^{(t)} + (1 - \\beta_1) g^{(t)} \\end{align}\\] \\[\\begin{align} \\nu^{(t+1)} = \\beta_2 \\times \\nu^{(t)}+ (1 - \\beta_2) (g^{(t)})^2 \\end{align}\\] \\[\\begin{align} \\hat{\\tau}^{(t+1)} = \\frac{\\tau^{(t+1)}}{1 - {(\\beta_1)}^t} + \\mathbf{\\frac{(1 - \\beta_1) \\times (g)^{(t)}}{1 - {(\\beta_1)}^t}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\hat{\\nu}^{(t+1)} = \\frac{\\nu^{(t+1)}}{1 - {(\\beta_2)}^t} \\end{align}\\] \\[\\begin{align} \\omega^{(t+1)} = \\omega^{(t)} - \\pi\\ \\times \\hat{\\tau}^{(t+1)} \\ \\ \\ \\ \\ \\ \\ \\ where\\ \\pi = \\frac{\\eta}{\\sqrt{\\hat{v}^{(t+1)} + \\epsilon}} \\end{align}\\] with a corresponding implementation: for (t in 1:epoch) { g = gradient( ... ) r = B1 * r + (1 - B1) * g v = B2 * v + (1 - B2) * g^2 r.hat = r / ( 1 - B1^t) + ( 1 - B1) * g / ( 1 - B1^t) v.hat = v / ( 1 - B2^t) pi = eta / sqrt(v.hat + eps) w = w - pi * r.hat } Nadam optimizer expresses identical equations as Adam with one slight change to the rho-hat (\\(\\hat{\\rho}\\)) - the addition of Nesterov Momentum. As stated in the title of Dozat’s paper (2016), the idea is to incorporate Nesterov Momentum into Adam. Finally, apart from the eight optimizers we discussed, other strategies can influence SGD learning speed. Let us present two of them. First, one strategy that allows a non-static learning rate is what we call Step Decay Schedule (Rong Ge et al. 2019). Instead of just being static, the idea here is to reduce progressively (or decay) the learning rate (e.g., by half) at a given interval up to the epoch limit. An example formula for the learning rate is: \\[\\begin{align} \\eta^{(t+1)} = \\eta^{(initial)} \\times DF^{floor\\left(\\frac{t}{step size}\\right)} \\end{align}\\] where t is the epoch and \\(\\mathbf{\\eta}\\) is the learning rate. We cover this more on CNN. Second, another strategy that allows a non-static learning rate is what we call Cyclical Learning Rate (Leslie N. Smith 2017). Instead of a step-wise decay, the idea is to vary the value of the learning rate within a reasonable minimum and maximum boundary. We define a cycle as an interval based on certain choices, e.g., at fix interval or every batch update. We then oscillate the learning rate by allowing it to increase up to the maximum boundary monotonically, and then at the next cycle, we monotonically decrease up to a minimum boundary. Lastly, we now come down to a more general perspective of learning rate. The onus is upon us to find the most optimal learning rate during training. We know that it takes trial and error to find a value for our learning rate if we use a static value throughout the training. Alternatively, we can resort to a more adaptive mechanism by performing simulated annealing (or in the case of DNN, we call it learning rate annealing) - that is to say, not only do we control the speed of the decay but also at which stage or cycle in training to vary the decay. We leave readers to investigate other proposals for optimizing the learning rate. In terms of implementation, we also leave readers to modify our backward.pass(.) function and experiment on the different optimizers recently discussed. 12.3.15 Interpretability The assumption, particularly in our early examples in DNN, is that our two output neurons in the output layer are mutually exclusive (e.g., linearly independent). Particularly for the output layer, the first output neuron produces a sigmoid value ranging between 0 and 1, irrespective of values from the other output neuron. It is essential to note here that if we contrive our samples so that we have a carefully crafted input and target output as shown in Figure 12.21 using a modified version of our get.synthetic.samples(.) function, we hope to see predictable output patterns as a way to validate our DNN. Indeed, Figure 12.21 shows that if we adjust the epoch count or the new X values, the predicted output expectedly follows the intended pattern. Figure 12.21: Interpretability of Predicted Output If we can simulate a synthetic dataset (both input and output) in a very controlled fashion with precise and unique patterns, then we should be able to also yield precise and unique patterns for inference, no matter if we also induce a synthetic perturbation and a set of outliers. However, that is because we understand our data - we crafted it. Likewise, our implementation of DNN using my.MLP(.) makes it possible for us to tailor the complexity of our network (e.g., number of layers, number of neurons). We can make it in such a way as to avoid the perception of a black-box model. However, the context and its interpretation are essential. Somehow, our synthetic data has to simulate a real-world process that produces our samples naturally in the first place, like a toss of a coin. As easy as it may sound, real-world data, in actuality, is a lot more diluted with natural patterns and noise that we may not be able to simulate. We can contrive synthetic samples by any holistic and stochastic means, but it requires domain expertise to qualify a comprehensive sample and representative of a domain’s entire population sample. Nevertheless, the point to take here is that as long as there is indeed a pattern, no matter how inconspicuous it may be, upon which we can infer, then DNN can train. Our mini.batch(.) function can provide such pattern that may simulate a gaussian distribution. This is useful for simple empirical purposes to allow our DNN to yield some expected pattern for training and testing, thereby having the ability to quantify the correctness and mistakes. However, we may also have to account for one intended to prevent DNN from the ability to train rather than make mistakes even. One, in particular, that may offset the ability for DNN to train is the so-called Adversarial Sample, which has no pattern whatsoever. Another challenge that a Neural Network finds difficult to train is the so-called Bongard Problem which roots in puzzle-based samples. Lastly, we also have to account for the data types. For example, data with spatial or sequential relationships calls for a different kind of Neural Network which we cover in the next section. 12.4 Convolutional Neural Network (CNN) In the 1980s, Yann Lecun laid out the underpinnings of what is known today as the Convolutional Neural Network (CNN) used for Image Recognition and Classification. 12.4.1 Computer Graphics The main goal of working on CNN is to process and classify images. However, to classify images, we need to know first the type of computer graphics used. There are essentially two types of computer graphics: vector graphics and raster graphics: Raster graphics are composed of pixels (whether greyscaled or colored). For example, the left image in Figure 12.22 represents the lower right portion of a circle projected to show the individual pixels. Vector graphics are rendered based on vertices and paths. For example, the right image in the figure shows a rectangle with four vertices (points) and four paths (lines) connecting the vertices. Figure 12.22: Raster and Vector Graphics For our purposes, we use a rasterized image in the format of jpeg (png and gif are as equally popular). Here, we use image operations made available in R by third-party libraries such as jpeg. Below is the code to read an image. See Figure 12.23 for the actual image. library(&quot;jpeg&quot;) image = jpeg::readJPEG(source = &quot;appleorange.jpg&quot;) image.copy = image draw.image &lt;- function(image, main=&quot;Apple, Oranges, and Banana&quot; ) { di = dim(image) img = array(image, c(di[1], di[2], di[3])) sz = ncol(img) par(pty=&quot;s&quot;) plot(NA, type=&quot;n&quot;, xlim=c(1, sz), ylim=c(1, sz), xlab=&quot;Image Width&quot;, ylab=&quot;Image Height&quot;, main=main) rasterImage(img,xleft=1, ybottom=sz, xright=sz, ytop=1) } draw.image(image) Figure 12.23: Apple, Oranges, and Banana We use the rasterImage(.) function to project the image to a pixelized format - this is also called rasterization. While our scope does not cover image processing, it helps to leave readers to investigate Graphics Pipelines, which is covered in Data Visualization courses. In addition, the topic touches on vertex processing such as tesselation, transformation, clipping, thresholding, and filtering or masking, which are mostly the processes we may use for illustration purposes when we get to Kernels. Additionally, we know that 3D Games perform heavy image processing and thus rely heavily on Graphics Processing Unit (GPU) for faster processing. Similarly, some, if not most, CNN standard packages are already built to have GPU support. Part of Computer Graphics also touches on Object Segmentation, Object Detection or Localization, and Object Selection, which are essential when dealing with Autonomous Vehicles in a more dynamic time-series fashion (which we cover in the RNN section). In the same line of thinking, recall our discussion on Lidar Sensors and Kalman Filters in Chapter 8 (Bayesian Computation II) for dynamic systems. Now, going back to our image of a handful of fruits, to manipulate the image, we first have to note that the image is stored in a 3D tensor form with a 500x500x3 dimension. It can easily be shown like so: dim(image) ## [1] 500 500 3 The three dimensions correspondingly represent the height, width, and depth of the image. The depth in a JPEG image also represents the three channels of the RGB color scheme. Each element in the tensor represents the color intensity of an RGB color scheme which is normalized by a max value of 255. For example, we can get the hexadecimal equivalent of red color like so: rgb(255/255, 0, 0) ## [1] &quot;#FF0000&quot; To change the color of the image in order to show only the red color scheme, we can manipulate the tensor like so: image.copy = image; image.copy [1:500,1:500,c(2,3)] = 0 # red draw.image(image.copy) We can apply the same changes to isolate the green and blue color schemes of the image. image.copy = image; image.copy [1:500,1:500,c(1,3)] = 0 # green draw.image(image.copy) image.copy = image; image.copy [1:500,1:500,c(1,2)] = 0 # blue draw.image(image.copy) Figure 12.24 gives us the original image and the three images in separate RGB color schemes. Figure 12.24: Apple, Oranges, and Banana Equivalently, Figure 12.25 gives us an enhanced version of the original image and three images in separate grayscale color schemes using the following tricks: image.copy = image image.copy [1:500,1:500,] = ifelse(image.copy [1:500,1:500,] &lt; 0.50, 0, 1) draw.image(image.copy) image.copy = image image.copy [1:500,1:500,] = image.copy [1:500,1:500,1] * ( 250 / 255 ) draw.image(image.copy) image.copy = image image.copy [1:500,1:500,] = image.copy [1:500,1:500,2] * ( 250 / 255 ) draw.image(image.copy) image.copy = image image.copy [1:500,1:500,] = image.copy [1:500,1:500,3] * ( 250 / 255 ) draw.image(image.copy) Figure 12.25: Apple, Oranges, and Banana Given the ability to manipulate the image tensor by hand, there are other more creative ways to apply effects to the image. In the context of CNN, we use Filters to manipulate images. The purpose is to highlight image properties that are readily identifiable and classifiable. This is where we discuss Convolution. 12.4.2 Convolution The convolutional nature of CNN comes with the fact that CNN assumes an input image, performs an element-wise multiplication with a filter, then sums the product to produce a feature map. We demonstrate the operation using Figure 12.26. Figure 12.26: Convolution The idea is to use a Filter, also called Kernel, which is a 2D matrix (in the case of Figure 12.26). We slide the Filter over the input image in the direction from left to right and top to bottom, one step at a time. A step is called a stride, which dictates the pace or number of pixels to skip horizontally and vertically. As the Kernel lands on a patch which has the same dimension as the Kernel, we perform an element-wise multiplication and then sum the product. For example, the first patch (2x2) consists of the following pixels (1,2,5,6). We then multiply the patch with the Kernel like so: \\[ 1 \\times 1 + 2\\times 0 + 5 \\times 0 + 6 \\times 1 = 7 \\] A Patch may also reference the terms receptive field, image patch, and convolution patch. The next patch (2x2), after 1st stride, consists of the following pixels (5,6,9,10). We then multiply the patch with the kernel like so: \\[ 5 \\times 1 + 6 \\times 0 + 9 \\times 0 + 10 \\times 1 = 15 \\] We perform the same operation for subsequent patches until we form a new matrix called Feature Map, also called Convolved Feature. We can repeat the process by performing the same steps, yielding a new Feature Map. For example, in Figure 12.27, our Feature Map becomes an input and goes through another round of convolution using a smaller filter (2x2) with one stride. Figure 12.27: Convolution (Repeated) It is also possible to perform convolution, resulting in a new Feature Map with the same dimension as the original Input Image. To achieve this, where the kernel crosses over the edges of an image, we use padding with zero values. See Figure 12.28. The operation uses one padding and one stride. Figure 12.28: Repeated Convolution with Padding Convolution is expressed using the following general formulation (with padding and a stride of one): \\[\\begin{align} M_{(\\text{feature.map})} = \\sum_d^D \\sum_{(i=-p)}^{(H+p)} \\sum_{(j=-p)}^{(W+p)} I(h_s: h_e, w_s : w_e, d) * K(,,d) \\end{align}\\] where: H, ,W, ,D are the dimensions of an image (e.g. height, width, depth) , k is the size of the kernel, I is the input image with dim. (\\(H \\times W \\times D\\)), K is the kernel with dim. (\\(kr \\times kc \\times D\\)), p is the padding, s is the stride. For the rest of CNN discussions, we assume symmetric or equal paddings along the edges if required. The dimension (\\(O \\times O\\)) of the feature map has the O size calculated as such (Dumoulin V., Visin F. 2018): \\[\\begin{align} O = (W - K + 2p) / s + 1 \\end{align}\\] For our purposes, especially if implemented in R, we assume one (and not zero) as starting index. Therefore, to construct the receptive field, we use the following pseudocode and formulation: \\[\\begin{align} \\{\\ h_i = (s + 1) \\times i \\ \\}_{i=1}^{(H+2p)} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\{\\ w_j = (s + 1) \\times j \\ \\}_{j=1}^{(W+2p)} \\end{align}\\] \\[ \\begin{array}{ll} \\text{for i in }\\{1, ..., O\\}: \\\\ \\ \\ \\ \\ \\ \\text{for j in }\\{1, ..., O\\}: \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ h_s=h_i; \\ \\ \\ \\ he = (hs + r - 1)\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ w_s=w_j;\\ \\ \\ \\ we = (ws + r - 1) \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ ...\\ I[h_s: h_e, w_s, w_e]\\ ... \\end{array} \\] The receptive field is bounded by the four hyperparameters, namely \\(\\mathbf{h_s}\\), \\(\\mathbf{h_e}\\), \\(\\mathbf{w_s}\\), and \\(\\mathbf{w_e}\\) which respectively represent the starting and ending indices of its height and width. Finally, let us review our example implementation of Convolution. Our implementation allows the ability to dilate both the image and the kernel and cache the indices and size that form the receptive fields. This capability is essential during backpropagation in the Pooling Layer, discussed later. library(dequer) convolution &lt;- function(images, filter, cur.fmaps = NULL, dw.kernel = NULL, pw.kernel = NULL, Dout=NULL, stride=1, padding = 0, dil_rate=0, dil_input=0, autopad = NULL, auto.pad=0, normalize=FALSE, clipping=FALSE, afunc=NULL, pool.cache = NULL, pool = NULL, bias = TRUE, dw.bias = NULL, pw.bias = NULL, ptype=&quot;convolv&quot;) { gaps = dil_input d = dim(images) img.h = d[1]; img.w = d[2]; img.d = d[3]; img.s = d[4] padded = pad(dilate(images, dil_input), filter, padding) image = padded$image padding = padded$padding if (!is.null(autopad)) { extra = autopad(image, filter, padding, stride, edge=autopad, auto.pad) image = extra$image auto.pad = extra$pad } di = dim(image) new.h = di[1]; new.w = di[2]; new.d = di[3]; new.s = di[4] K = dilate(filter, dil_rate) r = nrow(K) c = ncol(K) bkprop.cache = NULL O = floor( (new.w - c) / stride + 1 ) if (new.h &lt;= r) error(4) if (O &lt; 1) { return(NULL)} h = seq(from=1, to=new.h, by=stride) w = seq(from=1, to=new.w, by=stride) I = list() n = 0 if (ptype == &quot;maxpool&quot; || ptype == &quot;avgpool&quot;) for (i in 1:O) { for (j in 1:O) { n = n + 1 hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) I[[n]] = array(image[hs:he, ws:we,,], c(r, c, di[3], di[4])) } } if (ptype == &quot;convolv&quot;) { feature.map = convolve.image(image, dw.kernel, pw.kernel, dw.bias, pw.bias, bias, dil_rate, O, h, w, r, c) } else if (ptype == &quot;gradient.I.wrt.K&quot;) { feature.map = gradient.I.wrt.K(image, K, O, h, w, r, c) } else if (ptype == &quot;gradient.K.wrt.I&quot;) { feature.map = gradient.K.wrt.I(image, Dout, cur.fmaps, pw.kernel, dil_rate, O, h, w) } else if (ptype == &quot;gradient.I.wrt.P&quot;) { feature.map = gradient.I.wrt.P(image, Dout, K, O, pool, h, w, pool.cache) } else if (ptype == &quot;maxpool&quot;) { di = dim(I[[1]]) img.s = di[4] im2col = array(unlist(I), c( di[1] * di[2], di[3] * img.s * n)) im2max = apply(im2col, 2, max) pool.cache = apply(im2col, 2, which.max) im2col = array( im2max, c(di[3] * img.s, O, O)) max.channel = list() for (d in 1:(di[3] * img.s)) { max.channel[[d]] = t(im2col[d,,]) } max.channel = array(unlist(max.channel), c(O, O, di[3], img.s)) feature.map = max.channel } else if (ptype == &quot;avgpool&quot;) { di = dim(I[[1]]) img.s = di[4] im2col = array(unlist(I), c( di[1] * di[2], di[3] * img.s * n)) im2mean = pool.cache = colMeans(im2col) im2col = array( im2mean, c(di[3] * img.s, O, O)) mean.channel = list() for (d in 1:(di[3] * img.s)) { mean.channel[[d]] = t(im2col[d,,]) } mean.channel = array(unlist(mean.channel), c(O, O, di[3], img.s)) feature.map = mean.channel } else if (ptype == &quot;mask&quot;) { img.copy = array(0, di) for (i in 1:O) { for (j in 1:O) { hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) img.copy[hs:he, ws:we,,] = sum( array(image[hs:he, ws:we,,], c(r, c, di[3], di[4])) * K) } } feature.map = img.copy } if (normalize==TRUE) { mx = max(feature.map) mn = min(feature.map) feature.map = (feature.map - mn) / (mx - mn) } if (clipping==TRUE) { feature.map = pmax(pmin( feature.map, 1), 0) } if (!is.null(afunc)) { feature.map = activation(feature.map, get(afunc)) } bkprop.cache = list(&quot;stride&quot; = 1, &quot;padding&quot; = max(r - (padding + auto.pad + 1), 0), &quot;dil_rate&quot; = 0, &quot;dil_input&quot; = stride-1, &quot;auto.pad&quot; = auto.pad, &quot;autopad&quot; = &quot;left&quot;, &quot;wstride&quot; = 1, &quot;wpadding&quot; = padding, &quot;wdil_rate&quot; = stride - 1, &quot;wdil_input&quot; = 0, &quot;wauto.pad&quot; = auto.pad, &quot;wautopad&quot; = ifelse(auto.pad &gt; 0, &quot;right.backprop&quot;, &quot;left.backprop&quot;) ) list(&quot;feature.map&quot; = feature.map, &quot;bkprop.cache&quot; = bkprop.cache, &quot;pool.cache&quot; = pool.cache, &quot;auto.pad&quot; = auto.pad, &quot;image&quot; = image) } Note that in the context of graphical images in which we need to respect the color intensity boundary, we implement the option of normalizing or scaling the feature map back to the allowable range between 0 and 1 after convolution and after the option of clipping to a boundary. This option is only for visual or graphical interpretation - for example, if only just because we need to see the visual effect experimentally. On the other hand, when we discuss feed-forward in CNN, we primarily use RELU or Leaky RELU as activation function right after the convolution operation - another way of clipping a value to a lower bound. 12.4.3 Stride and Padding In determining the optimal stride and padding for our CNN, we need to consider how we should avoid losing information which happens in a case in which an input of size \\(4\\times 4\\) and a filter of size \\(3\\times 3\\) force us to drop the right side edge and bottom edge of the input if using stride equal to two. Ideally, we can pad the edges with zeroes. Figure 12.29 shows an example of a convolution with no padding and two convolutions with paddings. Figure 12.29: Stride and Padding Based on the figure, each choice of padding and stride affects the size of the feature map and thus also the information it produces. We may opt to use a stride equal to one with no padding in our case. That gives us a smaller \\(2 \\times 2\\) feature map while preserving information. Below is our example implementation of Convolution with Padding following the equation. In the implementation, we include Auto Padding and Dilation, which we cover in a section up ahead. # Assumes symmetric matrix for the kernels pad &lt;- function(image, filter, padding) { if (padding &lt;= 0) { return ( list(&quot;image&quot; = image, &quot;padding&quot; = padding))} di = dim(image) fi = dim(filter) img.h = di[1]; kernel.h = fi[1] img.w = di[2]; kernel.w = fi[2] img.d = di[3]; kernel.d = fi[3] img.s = di[4] if (padding &gt;= kernel.h) { # limit paddings padding = kernel.h - 1 } h = (img.h + 2 * padding) w = (img.w + 2 * padding) img = array(0, c(h,w, img.d, img.s)) img[(1+padding):(img.h+padding),(1+padding):(img.w+padding),,] = image list(&quot;image&quot; = img, &quot;padding&quot; = padding) } In our implementation, we restrict the number of zero paddings allowed in our CNN such that the number of zero paddings cannot be greater than the size of the kernel size, which is a waste of space. \\[\\begin{align} p = \\begin{cases} kr - 1 &amp; p \\ge kr\\\\ p &amp; otherwise \\end{cases} \\label{eqn:eqnnumber609} \\end{align}\\] Additionally, to preserve information, we automatically add extra paddings to the right and bottom of the image to ensure that we can filter pixels along the edges of the image. In our example implementation below, we allow forcing extra paddings to the right or left edges of the image (as required by backpropagation, for example). Furthermore, if the overall zero paddings exceed the size of the kernel, then we disregard the extra paddings. autopad &lt;- function(image, filter, padding, stride, edge=&quot;right&quot;, auto.pad = 0) { di = dim(image) fi = dim(filter) img.h = di[1]; kernel.h = fi[1] img.w = di[2]; kernel.w = fi[2] img.d = di[3]; kernel.d = fi[3] img.s = di[4]; if (edge == &quot;right&quot;) { extra.pad = (img.h - kernel.h) %% stride extra.pad = ifelse(extra.pad &gt; 0, stride - extra.pad, 0) } else { extra.pad = auto.pad } if (extra.pad == 0 || extra.pad + padding &gt;= kernel.h) { return(list(&quot;image&quot; = image, &quot;pad&quot; = 0)) } h = img.h + extra.pad w = img.w + extra.pad img = array(0, c(h, w, img.d, img.s)) if (edge == &quot;right&quot; || edge==&quot;right.backprop&quot;) { img[1:img.h,1:img.w,,] = image } else { img[(1+extra.pad):(img.h+extra.pad),(1+extra.pad):(img.w+extra.pad),,]= image } list(&quot;image&quot; = img, &quot;pad&quot; = extra.pad) } 12.4.4 Kernels And Filters A filter is a composition of kernels. A filter may only consist of one Kernel, and thus perhaps the terms are interchangeable in that respect based on such cases. Applying a kernel or filter to an image is called masking or filtering. In this section, we cover a few of the masking or filtering tricks based on the type of Kernel. While graphically, we use kernels to mask an image, which results in blurring, smoothing, sharpening, embossing, edge detection, or even noise reduction, among many other effects, in CNN, we use kernels to preserve the spatial relationship of individual pixels, knowing that during convolution, the process may reduce the dimension of the image if we choose to in our architecture design. To illustrate, let us form a list of kernels to use. Below, we list a few examples of a kernel: \\[ \\underbrace{\\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; 1 \\\\ 2 &amp; 0 &amp; -2 \\\\ 1 &amp; 0 &amp; -1 \\end{array} \\right]}_{\\text{Sobel Kernel}} \\ \\ \\ \\ \\ \\underbrace{\\left[ \\begin{array}{rrr} -1 &amp; 0 &amp; -1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; 0 &amp; -1 \\end{array} \\right]}_{\\text{Negative Photo }} \\ \\ \\ \\ \\ \\ \\underbrace{ \\left[ \\begin{array}{rrr} 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\\\ 1 &amp; 0 &amp; -1 \\end{array} \\right]}_{\\text{Embossed (Raised) }} \\ \\ \\ \\ \\ \\ \\underbrace{ \\left[ \\begin{array}{rrr} 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ -1 &amp; -1 &amp; -1 \\end{array} \\right]}_{\\text{Embossed (Recessed) }} \\] We can implement the kernels as follows: sobel.kernel = c(-1, 0, 1, -2, 0, 2, -1, 0, 1) negative.photo = c(-1, 0, -1, 0, 0, 0, -1, 0, -1) raised.emboss = c( 1, 0, -1, 1, 0, -1, 1, 0, -1) recessed.emboss = c( 1, 1, 1, 0, 0, 0, -1, -1, -1) Finally, let us demonstrate the use of the convolution(.) function with a kernel. Below, we perform convolution against the image using different kernels (see the following ). Recall that because convolution may result in a color intensity beyond the range of 0 and 1, we use normalization as an option to scale the feature map back to the same range as required by a JPEG format; otherwise, we also can use clipping. kernel = sobel.kernel filter = array( kernel, c(3,3,3,1)) apple.image = array(image, c(500,500,3,1)) sobel.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map # crude way of converting greyscale to white-black colors. sobel.img [1:500,1:500,,] = ifelse(sobel.img [1:500,1:500,,] &lt; 0.50, 1, 0) kernel = negative.photo filter = array( kernel, c(3,3,3,1)) apple.image = array(image, c(500,500,3,1)) negative.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map kernel = raised.emboss filter = array( kernel, c(3,3,3,1)) raised.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map kernel = recessed.emboss filter = array( kernel, c(3,3,3,1)) recessed.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map See Figure 12.30 for the effects of the kernels ordered as follows: Sobel kernel, negative photo, embossed (raised), embossed (recessed). Figure 12.30: Apple, Oranges, and Banana Notice how the Sobel kernel can be used for edge detection; albeit, there may be other kernels that can mask just the same. For example, test the following kernel: edge.kernel = c(0, -1, 0, -1, 4, -1, 0, -1, 0) One of the important techniques in CNN is the ability to perform multiple convolution operations, each using a different kernel like so: kernel = sobel.kernel filter = array( kernel, c(3,3,3,1)) apple.image = array(image, c(500,500,3,1)) sobel.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map kernel = negative.photo filter = array( kernel, c(3,3,3,1)) negative.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map kernel = edge.kernel filter = array( kernel, c(3,3,3,1)) edge.img = convolution(apple.image , filter = filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map We can then combine the convolved result to form a new input image like so: d = dim(edge.img) new.input.image = array( cbind(sobel.img, negative.img, edge.img), c(d[1], d[2], 3, 1)) which yields the following dimension. dim(new.input.image) ## [1] 500 500 3 1 Along with the new image, we can also concoct another separate filter: pattern = matrix(c( rep(c(0,1), 15), rep(c(1,0), 15)), nrow=30, byrow=TRUE) new.filter = array( cbind(pattern, pattern, pattern), c(30, 30, 3,1)) dim(new.filter) ## [1] 30 30 3 1 Finally, we perform convolution against the new input image with the new filter (see Figure 12.31). new.image = convolution(new.input.image , filter = new.filter, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map draw.image(new.image) Figure 12.31: Apple, Oranges, and Banana Note that while we use a \\(30 \\times 30\\) kernel size for illustration purposes, literatures tend to use \\(3 \\times 3\\), \\(5 \\times 5\\), and \\(7 \\times 7\\) kernel sizes. Determining an optimal kernel size can be done heuristically, or it can be done by automatic or adaptive means. However, a notable emphasis is that as long as the kernel can preserve some level of correlation within local features (e.g., highlighting specific features of an image such as edges and others), then we can perhaps say that we are on the right track, at least for the sake of classification or pattern recognition. Hereafter, let us begin to use highlight to also refer to a feature. On the other hand, we may start losing the highlights if we are not careful in choosing the correct kernel (let alone its size) - as an example of this is shown in Figure 12.31 in which the image starts to blur too much. Irrespective of that, visualization becomes irrelevant as we start training CNN to optimize our kernels. 12.4.5 Dilation Dilation is the method of stretching or inflating a receptive field such that it skips or leaves gaps in between elements. For example, in Figure 12.32, we use a dilate rate of two, resulting in a feature map element of 453 after convolution. In general, a dilate rate of one is equivalent to a normal convolution with no gaps. Moreover, a dilate rate of two corresponds to a one-pixel gap. Figure 12.32: Dilation Below is our example implementation of Dilation. Note that for our own purposes only, we use a dilate rate equal to zero for normal convolution with no gaps. And a rate of one corresponds to a one-pixel gap. dilate &lt;- function(image, dil_rate=0) { if (dil_rate &lt; 1) { return(image) } d = dim(image) img.h = d[1]; img.w = d[2]; img.d = d[3]; img.s = d[4] h = img.h + dil_rate * (img.h - 1) w = img.w + dil_rate * (img.w - 1) img = array(0, c(h,w, img.d, img.s)) for (i in 1:img.h) { for (j in 1:img.w) { img[i*2 - 1,j*2 - 1,,] = image[i,j,,] } } img } dilate.filters &lt;- function(filters, dil_rate = 0) { if (dil_rate &lt; 1) { return(filters) } len = length(filters) for (l in 1:len) { filter = filters[[l]] d = dim(filter) img.h = d[1]; img.w = d[2]; img.d = d[3]; img.s = d[4] h = img.h + dil_rate * (img.h - 1) w = img.w + dil_rate * (img.w - 1) img = array(0, c(h,w, img.d, img.s)) for (i in 1:img.h) { for (j in 1:img.w) { img[i*2 - 1,j*2 - 1,,] = image[i,j,,] } } filters[[l]] = img } filters } A simple example of using the dilate(.) function is shown below: size = 3 m = array(seq(1,size * size), c(size, size, 1, 1)) m[,,,1] ## [,1] [,2] [,3] ## [1,] 1 4 7 ## [2,] 2 5 8 ## [3,] 3 6 9 dilate(m, dil_rate=1) ## , , 1, 1 ## ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 0 4 0 7 ## [2,] 0 0 0 0 0 ## [3,] 2 0 5 0 8 ## [4,] 0 0 0 0 0 ## [5,] 3 0 6 0 9 To illustrate, let us use the edge kernel, but this time, we pass a new hyperparameter to the convolution(.) function called Dilation Rate with a value equal to one. kernel = edge.kernel filter = array( kernel, c(3,3,3,1)) new.image = convolution(apple.image , filter = filter, dil_rate=1, normalize=TRUE, ptype=&quot;mask&quot;)$feature.map Figure 12.33 shows the new image using a dilated filter. draw.image(new.image) Figure 12.33: Apple, Oranges, and Banana 12.4.6 Pooling Pooling is also regarded as SubSampling or DownSampling, of which the primary intent is for dimensionality reduction by summarizing the feature map. Similar to convolution, pooling is an operation that slides a \\(P \\times P\\) filter over a feature map in the direction from left to right and top to bottom. We then perform one of two common pooling methods, namely maximum pooling and average pooling (see Figure 12.34). Figure 12.34: Pooling In Figure 12.34, we compute the value of the first cell in the pool for either the maximum pooling or average pooling. Below is the result: p1 = max(c(224, 161, 325, 83)) p2 = mean(c(224, 161,325, 83)) c(&quot;MaxPool&quot; = p1, &quot;AvePool&quot; = p2) ## MaxPool AvePool ## 325.00 198.25 We then slide the filter to the next patch to perform the same calculation. We repeat the process until we slide to the last patch. p1 = max(c(161, 164, 83, 65)) p2 = mean(c(161, 164, 83, 65)) c(&quot;MaxPool&quot; = p1, &quot;AvePool&quot; = p2) ## MaxPool AvePool ## 164.00 118.25 Notice that the resulting matrix is reduced in size to a smaller dimension. It means a reduction in the number of parameters and computation. Note that for a tensor, the depth is preserved. Our implementation of Pooling piggybacks on Convolution in our case. Therefore, we need a corresponding function, e.g., pooling(.). It is notable to mention that other implementations may avoid overlapping, which can be done by adjusting the strides. pooling &lt;-function(ptype = &quot;maxpool&quot;, ...) { convolution( ptype=ptype, ...)} pool.backprop &lt;- pooling As an example of its application: m = array(seq(1,48), c(4,4,3, 1)) f = array(seq(1,27), c(3,3,3, 1)) dim(m) ## [1] 4 4 3 1 m[,,,1] ## , , 1 ## ## [,1] [,2] [,3] [,4] ## [1,] 1 5 9 13 ## [2,] 2 6 10 14 ## [3,] 3 7 11 15 ## [4,] 4 8 12 16 ## ## , , 2 ## ## [,1] [,2] [,3] [,4] ## [1,] 17 21 25 29 ## [2,] 18 22 26 30 ## [3,] 19 23 27 31 ## [4,] 20 24 28 32 ## ## , , 3 ## ## [,1] [,2] [,3] [,4] ## [1,] 33 37 41 45 ## [2,] 34 38 42 46 ## [3,] 35 39 43 47 ## [4,] 36 40 44 48 p = pooling(image=m, filter=f, stride=1, padding=0) dim(p$feature.map) ## [1] 2 2 3 1 p$feature.map[,,,1] # default: maxpool ## , , 1 ## ## [,1] [,2] ## [1,] 11 15 ## [2,] 12 16 ## ## , , 2 ## ## [,1] [,2] ## [1,] 27 31 ## [2,] 28 32 ## ## , , 3 ## ## [,1] [,2] ## [1,] 43 47 ## [2,] 44 48 There are other types of pooling used in other literature. We leave readers to investigate the others. In the next section, we now discuss how convolution works in a deep convolutional neural network. 12.4.7 CNN Architectures A few CNN architectures have earned their rightful place and can keep their respective prestigious names, namely LeNet-5, AlexNet, VGG-16, and ResNet. Such architectures have become classic models for object and image detection in popular competitions - one competition, in particular, is the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) which promotes annual competition, eyeing for deserving architectures that may find their place in the top ranks based on performance, accuracy, and other criteria. Designing CNN architecture involves determining the number of convolution layers to use, the number of filters to use, the number of pooling layers to use, the size of the minibatch to use, and the use of batch normalization and dropouts, among many other factors. Ultimately, we desire to reduce computation costs while achieving reasonable stability and accuracy across general cases. In this section, let us use a toy architecture to discuss the major components of a CNN architecture as shown in Figure 12.35. Figure 12.35: CNN (Toy Architecture) For CNN to work, we ensure it knows how to learn (to be trained) first. We go through the same methods such as FeedForward and BackPropagation as covered in previous sections about MLP. Moreover, we go through the possible use of optimizers such as AdaGrad, Adam, and Batch Normalization. 12.4.8 Forward Feed We mostly cover the Forward Feed technique in the MLP section. It is indeed a straightforward process. Recall that in the Forward Pass portion of the algorithm, we are given the option to choose which activation function to use. In the context of CNN, we have additional options to choose from, a couple of them being the size and number of kernels. Essentially, this dictates the complexity of our CNN, driven by the number of learnable parameters to use. There are two considerations to note in determining the size and number of kernels: Local Connectivity The core premise of convolution is to form a local grouping of pixels that may carry unique highlights or patterns. Such a group of pixels is projected in a summarized form to what we call activation map; otherwise known as feature map. Each cell in a feature map corresponds to a single neuron due to convolving a portion of an input image using a filter. As the filter slides over the input image, we form a collection of neurons in the form of the activation map. The term local connectivity emphasizes that a neuron is built upon (or is connected to) only a subset (a small region) of an input image. It should be noted that the effect of such local connectivity unavoidably diminishes the quality of the original image as we move deeper down the neural network via feed-forward. This effect can be regarded as smoothing and a way to offset overfitting. On the other hand, local connectivity tries to discover and preserve unique patterns per neuron in a feature map. The entire feature map is a collection of unique local patterns, forming the important, relevant components of a class - possibly a unique unified global information that can be classified. Preserving while siphoning this information from one layer to the next is challenging. For intuition, the ears, eyes, or nose of a facial image are all highlights at a high level, produced by a convolution of the facial image with different kernels. It is an onus upon us to design a CNN that effectively allows the highlights to percolate through the network down a final classification layer without losing relevant information. This final layer is the fully connected (FC) neural network layer, as shown in Figure 12.35 responsible for performing softmax for classification. It is at the output layer of the fully connected neural network where we also connect all highlights in a more global setting to finally classify the image in its entirety based on softmax probability. Weight Sharing Activation maps contain neurons (cells) that share common weights in the form of kernels. Similar to neurons in MLP, activation maps are the net output of a dot product between a set of inputs and weights. Here, kernels - being the weights - hold learnable parameters that need to be optimized during BackPropagation. Therefore, if our kernel size is \\(3 \\times 3\\), we have nine learnable parameters to tune. Likewise, if we build a filter of \\(3 \\times 3\\) kernels with a depth size of 10, we have ninety learnable parameters. Feedforward Now, in terms of FeedForward, we need to be able to construct a set of randomly initialized kernels. Similar to deep.neural.layer(.) based on our MLP implementation, let us also write a CNN version named deep.cnn.layer(.). Both contain a structure of learnable weights, which are trained and updated during Backpropagation and Backward Pass. error &lt;- function(e) { if (e == 1) { stop(paste0(&quot;Feature map convolvs into 1x1.&quot;, &quot; Input size is too small or kernel size is too large&quot;, &quot; or there are too many layers.&quot;)) } else if (e == 2 ) { stop(paste0(&quot;Backprop Convolution does not trace to correct path.&quot;)) } else if (e == 3 ) { stop(paste0(&quot;Full Convolution does not trace to correct path.&quot;)) } else if (e == 4) { stop(paste0( &quot;Size of kernel cannot be the same or greater than input size.&quot;)) } else if (e == 5) { stop(paste0( &quot;Size of Stride cannot be the same or greater than kernel size.&quot;)) } } deep.cnn.layers &lt;- function(X, ...) { di = dim(X) if (length(di) &lt; 4) { di = c(di, 1) X = array(X, di) } input.size = di[1]; img.s = di[4] depth = di[3] layers = list(...) structure = list() l = 0 for (layer in layers) { l = l + 1 if (layer$type == &quot;convolv&quot;) { if (is.null(layer$stride)) { layer$stride = 1 } if (is.null(layer$padding)) { layer$padding = 0 } if (is.null(layer$dil_rate)) { layer$dil_rate = 0 } if (is.null(layer$filters)) { layer$filters = 1 } if (is.null(layer$auto.pad)) { layer$auto.pad = 0 } if (is.null(layer$afunc)) { layer$afunc = NULL } if (is.null(layer$drop)) { layer$drop = NULL } if (is.null(layer$bias)) { layer$bias = TRUE } if (is.null(layer$normalize)) { layer$normalize = FALSE } if (layer$size &lt;= layer$stride) { error(5) } batch.gamma = list(&quot;weight&quot; = 1, &quot;rho&quot; = 0, &quot;nu&quot; = 0) batch.beta = list(&quot;weight&quot; = 0, &quot;rho&quot; = 0, &quot;nu&quot; = 0) layer$batchnorm = list(&quot;gamma&quot; = batch.gamma, &quot;beta&quot; = batch.beta, &quot;moving.mu&quot; = array(0, c(layer$filters,1)), &quot;moving.var&quot; = array(1, c(layer$filters,1)), &quot;normalize&quot; = layer$normalize) # For Depthwise Separable convolution weights = net.initialization(layer$size * layer$size, di[1] * di[2], layer$size * layer$size, itype=&quot;he&quot;, dist=&quot;uniform&quot;) layer$dw.kernel = list( &quot;weight&quot; = array(weights, c(layer$size, layer$size, depth)), &quot;rho&quot; = array(0, c(layer$size, layer$size, depth)), &quot;nu&quot; = array(0, c(layer$size, layer$size, depth)) ) layer$dw.bias = list( &quot;weight&quot; = runif(n=depth, min=0, max=1), &quot;rho&quot; = rep(0, depth), &quot;nu&quot; = rep(0, depth) ) # For Pointwise convolution weights = net.initialization(depth * layer$filters, di[1] * di[2], depth * layer$filters, itype=&quot;he&quot;, dist=&quot;uniform&quot;) layer$pw.kernel = list(&quot;weight&quot; = array(weights, c(depth, layer$filters)), &quot;rho&quot; = array(0, c(depth, layer$filters)), &quot;nu&quot; = array(0, c(depth, layer$filters)) ) layer$pw.bias = list(&quot;weight&quot; = runif(n=layer$filters, min=0, max=1), &quot;rho&quot; = rep(0, layer$filters), &quot;nu&quot; = rep(0, layer$filters) ) input.size = layer$size depth = layer$filters structure[[l]] = list(&quot;stride&quot; = layer$stride, &quot;padding&quot; = layer$padding, &quot;dil_rate&quot; = layer$dil_rate, &quot;auto.pad&quot; = layer$auto.pad, &quot;afunc&quot; = layer$afunc, &quot;istype&quot; = layer$type, &quot;dw.kernel&quot; = layer$dw.kernel, &quot;pw.kernel&quot; = layer$pw.kernel, &quot;dw.bias&quot; = layer$dw.bias, &quot;pw.bias&quot; = layer$pw.bias, &quot;batchnorm&quot; = layer$batchnorm, &quot;drop&quot; = layer$drop, &quot;bias&quot; = layer$bias) } else if (layer$type == &quot;pooling&quot;) { if (is.null(layer$stride)) { layer$stride = 1 } if (is.null(layer$padding)) { layer$padding = 0 } if (is.null(layer$dil_rate)) { layer$dil_rate = 0 } if (is.null(layer$dil_input)) { layer$dil_input = 0 } wsiz = layer$size window = array(rep(1, wsiz * wsiz), c(wsiz, wsiz, 1)) structure[[l]] = list(&quot;window&quot; = window, &quot;stride&quot; = layer$stride, &quot;padding&quot; = layer$padding, &quot;auto.pad&quot; = layer$auto.pad, &quot;dil_rate&quot; = layer$dil_rate, &quot;dil_input&quot; = layer$dil_input, &quot;ptype&quot; = layer$ptype, &quot;istype&quot; = layer$type) } else if (layer$type == &quot;dense&quot;) { structure[[l]] = list( &quot;fc.layers&quot; = layer[layer != layer$type], &quot;init&quot; = TRUE, &quot;istype&quot; = layer$type) } } structure } To illustrate, let us use a synthetic image with the following dimension. options(width=56) size = 7 depth = 3 input = 4 mx = size * size * depth X = array(seq(1,mx), c(size,size,depth, input)) cnn.layers = deep.cnn.layers( X, list( type = &quot;convolv&quot;, size=3, filters=2, normalize=&quot;batch&quot;), list( type = &quot;pooling&quot;, size=2, stride=1, ptype=&quot;maxpool&quot;), list( type = &quot;dense&quot;, list(size=3), list(size=3), list(size=10)) ) str(cnn.layers, strict.width=&quot;wrap&quot;) ## List of 3 ## $ :List of 13 ## ..$ stride : num 1 ## ..$ padding : num 0 ## ..$ dil_rate : num 0 ## ..$ auto.pad : num 0 ## ..$ afunc : NULL ## ..$ istype : chr &quot;convolv&quot; ## ..$ dw.kernel:List of 3 ## .. ..$ weight: num [1:3, 1:3, 1:3] 0.236 0.215 -0.293 ## -0.183 0.326 ... ## .. ..$ rho : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0 ## ... ## .. ..$ nu : num [1:3, 1:3, 1:3] 0 0 0 0 0 0 0 0 0 0 ... ## ..$ pw.kernel:List of 3 ## .. ..$ weight: num [1:3, 1:2] -0.0192 0.0585 -0.1033 ## -0.33 0.3465 ... ## .. ..$ rho : num [1:3, 1:2] 0 0 0 0 0 0 ## .. ..$ nu : num [1:3, 1:2] 0 0 0 0 0 0 ## ..$ dw.bias :List of 3 ## .. ..$ weight: num [1:3] 0.84 0.397 0.393 ## .. ..$ rho : num [1:3] 0 0 0 ## .. ..$ nu : num [1:3] 0 0 0 ## ..$ pw.bias :List of 3 ## .. ..$ weight: num [1:2] 0.552 0.102 ## .. ..$ rho : num [1:2] 0 0 ## .. ..$ nu : num [1:2] 0 0 ## ..$ batchnorm:List of 5 ## .. ..$ gamma :List of 3 ## .. .. ..$ weight: num 1 ## .. .. ..$ rho : num 0 ## .. .. ..$ nu : num 0 ## .. ..$ beta :List of 3 ## .. .. ..$ weight: num 0 ## .. .. ..$ rho : num 0 ## .. .. ..$ nu : num 0 ## .. ..$ moving.mu : num [1:2, 1] 0 0 ## .. ..$ moving.var: num [1:2, 1] 1 1 ## .. ..$ normalize : chr &quot;batch&quot; ## ..$ drop : NULL ## ..$ bias : logi TRUE ## $ :List of 8 ## ..$ window : num [1:2, 1:2, 1] 1 1 1 1 ## ..$ stride : num 1 ## ..$ padding : num 0 ## ..$ auto.pad : NULL ## ..$ dil_rate : num 0 ## ..$ dil_input: num 0 ## ..$ ptype : chr &quot;maxpool&quot; ## ..$ istype : chr &quot;pooling&quot; ## $ :List of 3 ## ..$ fc.layers:List of 3 ## .. ..$ :List of 1 ## .. .. ..$ size: num 3 ## .. ..$ :List of 1 ## .. .. ..$ size: num 3 ## .. ..$ :List of 1 ## .. .. ..$ size: num 10 ## ..$ init : logi TRUE ## ..$ istype : chr &quot;dense&quot; Here, we use a depth size of three, corresponding to the RGB channels. The kernels then inherit the input depth in the first layer in our CNN. Now, suppose that we use only two convolutional layers in building our CNN. For the first layer, we use a kernel size of \\(3 \\times 3\\). We also specify three kernels (based on the kernels parameter). After convolution, it will produce a feature map with a depth of three, based on specifying three kernels. The second layer also uses a kernel size of \\(3 \\times 3\\) but with only two kernels. In other words, the first feature map to be generated will have a depth of 3, and the second feature map will have a depth of two. Let us use our deep.cnn.layers(.) function to construct the structure. Here, we assume a default stride of 1 and padding of 0. The first layer uses Leaky RELU at the end of the convolution. cnn.layers = deep.cnn.layers( X, list( type = &quot;convolv&quot;, size=3, kernels=3), list( type = &quot;convolv&quot;, size=3, kernels=2), list( type = &quot;pooling&quot;, size=3, stride=1, ptype=&quot;maxpool&quot;), list( type = &quot;dense&quot;, list(size=3), list(size=3), list(size=10)) ) With the structure of the CNN layers in place, let us now have our example implementation of forward pass for our CNN, passing the input and the filters, and yielding a flattened vector as output. forward.pass.cnn &lt;- function(X, layers, train=TRUE) { cnn.output = list() cache.output = list() pool.cache = list() feature.map = X H = length(layers) for (L in 1:H) { layer = layers[[L]] if (layer$istype == &quot;convolv&quot;) { fmaps = NULL; v = 0 conv.output = convolution(feature.map, filter = layer$dw.kernel$weight, dw.kernel = layer$dw.kernel$weight, pw.kernel = layer$pw.kernel$weight, dw.bias = layer$dw.bias$weight, pw.bias = layer$pw.bias$weight, bias = layer$bias, stride = layer$stride, padding = layer$padding, dil_rate = layer$dil_rate, autopad = &quot;right&quot;, auto.pad = 1, afunc = NULL) feature.map = conv.output$feature.map$pw.fmap # Dropout Before Activation if (!is.null(layer$drop) &amp;&amp; train==TRUE) { feature.map = drop.out(feature.map, layer$drop) } # Activation Before Normalization if (!is.null(layer$afunc)) { feature.map = activation(feature.map, get(layer$afunc)) } # Normalization After Activation if (layer$batchnorm$normalize != FALSE) { normalize = normalize.forward(feature.map, layer, train) feature.map = normalize$feature.map layer = normalize$layer layers[[L]] = layer } conv.output$featuremap$pw.fmap = feature.map cnn.output[[L]] = conv.output$feature.map cache.output[[L]] = conv.output$bkprop.cache } else if (layer$istype == &quot;pooling&quot;) { conv.output = pooling(feature.map, filter = layer$window, stride = layer$stride, padding = layer$padding, ptype = layer$ptype) pmaps = conv.output$feature.map feature.map = pmaps cnn.output[[L]] = list(&quot;pw.fmap&quot; = feature.map ) cache.output[[L]] = conv.output$bkprop.cache pool.cache[[L]] = conv.output$pool.cache } else if (layer$istype == &quot;dense&quot;) { di = dim(feature.map) feature.map = t(array(feature.map, c( di[1] * di[2] * di[3], di[4]))) if (layer$init == TRUE) { fclayers = layer$fc.layers fclayers[[&quot;X&quot;]] = feature.map dnn = do.call(deep.neural.layers, fclayers) layers[[L]]$fc.layers = dnn$layers layers[[L]]$init = FALSE } fc.layers = layers[[L]]$fc.layers fc.model = forward.pass(feature.map, fc.layers, afunc=&quot;softmax&quot;) } } list(&quot;cnn.output&quot; = cnn.output, &quot;flattened.output&quot; = feature.map, &quot;cache.output&quot; = cache.output, &quot;pool.cache&quot; = pool.cache, &quot;fc.model&quot; = fc.model , &quot;layers&quot; = layers) } Assume a tiny \\(15 \\times 15\\) image of depth 3 (RGB channels). Let us run a forward pass using the generated layers. image = array(runif(675, min=0, max=1), c(15,15,3,1)) dim(image) ## [1] 15 15 3 1 model = forward.pass.cnn(image, cnn.layers) len = length(model$fc.model$layers) str(model$fc.model$layers[[len]]$output) ## num [1, 1:10] 0.0246 0.0714 0.0825 0.3705 0.1133 ... The final set of feature maps is flattened as one vector, and the vector is fed to the fully connected layer. Thus, if we have \\(15 \\times 15 \\times 3\\) feature maps, all three maps are flattened sequentially into one vector. 12.4.9 BackPropagation We start the discussion of CNN backpropagation using Figure 12.36. Note that the diagram uses Max Pooling. Average Pooling is another option to use. Figure 12.36: CNN (BackPropagation) Here, CNN backpropagation consists of two parts. The first part performs backpropagation from a regular MLP (dense network) referred to as a fully connected neural network. The second part takes the gradient output from the dense network and uses that as input to perform the convolutional backpropagation. Let us have a quick review of the first part. First, we take the derivative of the Loss Function with respect to the outputs in the output layer: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial o_1} = o_1 - t_1 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}}{\\partial o_2} = o_2 - t_2 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}}{\\partial o_3} = o_3 - t_3 \\end{align}\\] Second, we then calculate for the deltas (\\(\\delta o\\)): \\[\\begin{align} \\delta o_1 = \\frac{\\partial \\mathcal{L}}{\\partial o_1} \\left( \\frac{\\partial o_1}{\\partial \\hat{o}_1}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta o_2 = \\frac{\\partial \\mathcal{L}}{\\partial o_2} \\left( \\frac{\\partial o_2}{\\partial \\hat{o}_2}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\delta o_3 = \\frac{\\partial \\mathcal{L}}{\\partial o_3} \\left( \\frac{\\partial o_3}{\\partial \\hat{o}_3}\\right) \\end{align}\\] \\[\\begin{align} \\delta_o = \\sum_i \\frac{\\partial \\mathcal{L}}{\\partial o_i} \\left( \\frac{\\partial o_i}{\\partial \\hat{o}_i}\\right) = \\delta o_1 + \\delta o_2 + \\delta o_3 \\end{align}\\] Third, we calculate the derivative of the Loss Function with respect to weights: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\varphi_{1,1}} = \\delta o_1 \\left( \\frac{\\partial \\hat{o}_1}{\\partial \\varphi_{1,1}}\\right) = \\delta {o_1} h_1 \\ \\ \\ \\ \\ \\ \\ ... \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}}{\\partial \\varphi_{2,3}} = \\delta o_3 \\left( \\frac{\\partial \\hat{o}_3}{\\partial \\varphi_{2,3}}\\right) = \\delta {o_3} h_2 \\end{align}\\] Fourth, we calculate the derivative of the Loss Function with respect to activation output: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial h_{1}} = \\sum_i \\left( \\frac{\\partial \\mathcal{L}_i}{\\partial o_i} \\frac{\\partial o_i}{\\partial \\hat{o}_i} \\frac{\\partial \\hat{o}_i}{\\partial h_1} \\right) = \\sum_i \\delta o_i \\varphi_{1,i} = \\delta o_1 \\varphi_{1,1} + \\delta o_2 \\varphi_{1,2} + \\delta o_3 \\varphi_{1,3} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial h_{2}} = \\sum_i \\left( \\frac{\\partial \\mathcal{L}_i}{\\partial o_i} \\frac{\\partial o_i}{\\partial \\hat{o}_i} \\frac{\\partial \\hat{o}_i}{\\partial h_2} \\right) = \\sum_i \\delta o_i \\varphi_{2,i} = \\delta o_1 \\varphi_{2,1} + \\delta o_2 \\varphi_{2,2} + \\delta o_3 \\varphi_{2,3} \\end{align}\\] Fifth, we continue to the next hidden FC layer and perform the same, taking derivative with respect to next previous weight: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial \\omega_{1,1}} = \\delta h_1 \\left(\\frac{\\partial \\hat{h}_1}{\\partial \\omega_{1,1}}\\right ) = \\delta h_1\\ p_1 \\ \\ \\ \\ \\ \\ \\ ... \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial \\mathcal{L}}{\\partial \\omega_{8,2}} = \\delta h_2 \\left(\\frac{\\partial \\hat{h}_2}{\\partial \\omega_{8,2}}\\right ) = \\delta h_2\\ p_8 \\end{align}\\] From: \\[\\begin{align} \\delta {h_1} = \\frac{\\partial \\mathcal{L}}{\\partial h_{1}} \\frac{\\partial h_1}{\\partial \\hat{h}_1} = \\left(\\sum_i \\delta o_i \\varphi_{1,i} \\right) \\left(\\nabla \\text{RELU} = \\begin{cases} 0 &amp; if\\ h_1 \\le 0 \\\\ 1 &amp; if\\ h_1 &gt; 0 \\end{cases} \\right) \\label{eqn:eqnnumber610} \\end{align}\\] \\[\\begin{align} \\delta {h_2} = \\frac{\\partial \\mathcal{L}}{\\partial h_{2}} \\frac{\\partial h_2}{\\partial \\hat{h}_2} = \\left(\\sum_i \\delta o_i \\varphi_{2,i} \\right) \\left(\\nabla \\text{RELU} = \\begin{cases} 0 &amp; if\\ h_2 \\le 0 \\\\ 1 &amp; if\\ h_2 &gt; 0 \\end{cases} \\right) \\label{eqn:eqnnumber611} \\end{align}\\] Sixth, we use the flattened vector - this is the common X input as we know it in MLP. The flattened vector shows eight input features (p1, …, p8) - equivalent to (x1, …, x8). We take the derivative of the Loss function with respect to \\(\\mathbf{x_1}\\). In our case, that is the \\(\\mathbf{p_1}\\). \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial p_1} = \\sum_i \\left( \\delta h_i\\ \\frac{\\partial \\hat{h}_i}{\\partial p_1}\\right) = \\sum_i \\left( \\delta h_i\\ \\omega_{1,i} \\right) = (\\delta h_1 \\omega_{1,1} + \\delta \\hat{h}_2 \\omega_{1,2} ) \\end{align}\\] \\[ ... \\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial p_8} = \\sum_i \\left( \\delta h_i\\ \\frac{\\partial \\hat{h}_i}{\\partial p_8}\\right) = \\sum_i \\left( \\delta h_i\\ \\omega_{8,i} \\right) = (\\delta h_1 \\omega_{8,1} + \\delta \\hat{h}_2 \\omega_{8,2} ) \\end{align}\\] Seventh, we move to the second part of the CNN backpropagation, from the FC layer and into the Max Pooling layer. Here, we take the gradients as output from the FC layer and use them as backpropagation input to the Max Pooling layer. Each gradient output corresponds to a maximum value derived from individual Patches in subsequent previous feature maps. For example, let us consider the first gradient output from the previous layer, e.g. \\(\\frac{\\partial \\mathcal{L}}{\\partial p_1}\\) or \\(\\nabla_{p_1}\\mathcal{L}\\). This gradient represents the effect or influence of \\(\\mathbf{p_1}\\) to the Loss function. At the same time, \\(\\mathbf{p_1}\\) also represents the maximum value, e.g., 409, derived from the 1st Patch of the succeeding previous first feature map produced by the convolution with the first kernel set (still in reference to Figure 12.36). \\[\\begin{align} \\left[ \\begin{array}{ll} f_1 = 307 &amp; f_2 = 226\\\\ f_4 = 409 &amp; f_5 = 170 \\end{array} \\right]\\ \\ \\ \\ \\rightarrow \\left[ \\begin{array}{ll} \\frac{\\partial \\mathcal{L}}{\\partial f_1} = 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_2} = 0\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_4} = \\frac{\\partial \\mathcal{L}}{\\partial p_1} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_5} = 0 \\end{array} \\right] \\rightarrow \\text{(influence to }\\mathcal{L} ) \\label{eqn:eqnnumber612} \\end{align}\\] Let us take another example. We review the 4th Patch of the 2nd feature map produced by the convolution with the second kernel set. It has 158 as the maximum value. \\[\\begin{align} \\left[ \\begin{array}{ll} f_{14} = 4 &amp; f_{15} = 88\\\\ f_{17} = 140 &amp; f_{18} = 158 \\end{array} \\right]\\ \\ \\ \\ \\rightarrow \\left[ \\begin{array}{ll} \\frac{\\partial \\mathcal{L}}{\\partial f_{14}} = 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{15} } = 0\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_{17}} = 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{18}} = \\frac{\\partial \\mathcal{L}}{\\partial p_8} \\end{array} \\right] \\rightarrow \\text{(influence to }\\mathcal{L} ) \\label{eqn:eqnnumber613} \\end{align}\\] We, therefore, should be able to construct the gradients of the Loss Function with respect to all the activation outputs in the two feature maps based on the Max Pool. \\[\\begin{align} \\left[ \\begin{array}{lll} \\frac{\\partial \\mathcal{L}}{\\partial f_{1}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{2} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{3} }\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_{4}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{5} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{6} }\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_{7}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{8} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{9} }\\\\ \\end{array} \\right] = \\left[ \\begin{array}{lll} 0 &amp; 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{2} }\\\\ \\left( \\frac{\\partial \\mathcal{L}}{\\partial p_{1}} + \\frac{\\partial \\mathcal{L}}{\\partial p_{3}}\\right) &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{4} }\\\\ \\end{array} \\right] \\label{eqn:eqnnumber614} \\end{align}\\] Notice that the gradients with respect to \\(\\mathbf{p_1}\\) and \\(\\mathbf{p_3}\\) are overlapping as they both claim the same maximum value for \\(\\mathbf{f_4}\\), but each comes from separate patches. For this case of overlap, we aggregate the gradients by addition. The second feature map has the following corresponding gradients. \\[\\begin{align} \\left[ \\begin{array}{lll} \\frac{\\partial \\mathcal{L}}{\\partial f_{10}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{11} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{12} }\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_{13}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{14} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{15} }\\\\ \\frac{\\partial \\mathcal{L}}{\\partial f_{16}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{17} } &amp; \\frac{\\partial \\mathcal{L}}{\\partial f_{18} }\\\\ \\end{array} \\right] = \\left[ \\begin{array}{lll} 0 &amp; 0 &amp; 0\\\\ 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{5}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{6}} \\\\ 0 &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{7}} &amp; \\frac{\\partial \\mathcal{L}}{\\partial p_{8} }\\\\ \\end{array} \\right] \\label{eqn:eqnnumber615} \\end{align}\\] Overall, the individual contribution for each cell to the Loss function can be expressed in this simple equation: \\[\\begin{align} \\nabla f_i \\mathcal{L} = \\sum_{j} \\begin{cases} \\frac{\\partial \\mathcal{L}}{\\partial p_j} &amp; if\\ f_i = p_j \\\\ 0 &amp; otherwise \\end{cases} \\ \\ \\ \\ \\ \\ \\ where\\ p_j = \\text{max}(\\text{patch}_j\\text{)} \\label{eqn:eqnnumber616} \\end{align}\\] Our implementation of the Pooling Layer uses cached indices from the convolution operation to trace the image pixels (or neurons from previous feature maps) responsible for bearing the influence (see the use of pool.cache from convolution(.) function). Each neuron from the gradient map corresponds to a patch region in the image. Now, it is important to emphasize that if we do not have a Pooling layer between a Convolutional layer and an FC layer, then the gradient with respect to each element in the flattened vector maps directly to the first previous feature map without having to deal with maximum or average values. Nonetheless, the output in CNN from a convolution operation (which may include using an activation function such as RELU or Leaky RELU) maps to a cell in a feature map. Such a cell can also be regarded as neuron receiving an activation output similar to that in MLP. Here, the gradient of the Loss with respect to the activation output is backpropagated. Eight, we then depend on the gradients above to solve for the gradient of the Loss function with respect to the input image (or previous feature maps). Now, when dealing with backpropagation, we need to trace the influence of each component to the Loss Function from layer to layer backward. Sometimes, the needed operations are quite involved, requiring some effort. However, from time to time, a few tricks are discovered to allow certain operations to use computations that are mathematically convenient, especially, as an example, when computing for the gradients of the weights. Here, we can use Full Convolution and \\(\\mathbf{180^{\\circ}}\\) matrix rotation as introduced in other literature. To illustrate, let us use Figure 12.37. The convolution in the figure forms a \\(2 \\times 2\\) feature map. Figure 12.37: CNN (Full Convolution without Padding) Below is the complete influence of I to individual neurons when multiplied to weights: \\[\\begin{align} \\begin{array}{ll} f_{11} &amp;= I_{11} \\times k_1 + I_{12} \\times k_2 + I_{21} \\times k_3 + I_{22} \\times k_4 \\\\ f_{12} &amp;= I_{12} \\times k_1 + I_{13} \\times k_2 + I_{22} \\times k_3 + I_{23} \\times k_4 \\\\ f_{21} &amp;= I_{21} \\times k_1 + I_{22} \\times k_2 + I_{31} \\times k_3 + I_{32} \\times k_4 \\\\ f_{22} &amp;= I_{22} \\times k_1 + I_{23} \\times k_2 + I_{32} \\times k_3 + I_{33} \\times k_4 \\\\ \\end{array} \\label{eqn:eqnnumber617} \\end{align}\\] We can observe in the formulations above that the influence of one specific image feature, namely \\(I_{11}\\), only applies to \\(f_{11}\\). In terms of gradient, we can therefore show that as: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial I_{11}} = \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\frac{\\partial f_{11}}{\\partial I_{11}} = \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_1 \\end{align}\\] The other image features, such as \\(I_{13}\\), \\(I_{31}\\), and \\(I_{33}\\), follow the same gradient formulation. Additionally, \\(I_{12}\\) influences two neurons, and so the gradients are aggregated like so: \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial I_{12}} = \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\frac{\\partial f_{11}}{\\partial I_{12}} + \\frac{\\partial \\mathcal{L}}{\\partial f_{12}} \\frac{\\partial f_{12}}{\\partial I_{12}} = \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_2 + \\frac{\\partial \\mathcal{L}}{\\partial f_{12}} \\times k_1 \\end{align}\\] The others follow the same formulation. Now, to derive them all, we use Full Convolution for the gradients of all activation outputs, e.g. \\(\\frac{\\partial \\mathcal{L}}{\\partial f_{11}}, ..., \\frac{\\partial \\mathcal{L}}{\\partial f_{18}}\\), with the corresponding kernels which are rotated about \\(\\mathbf{180}^{\\circ}\\). From there, we end up with the following: \\[\\begin{align} \\begin{array}{ll} \\frac{\\partial \\mathcal{L}}{\\partial I_{11}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_1 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{13}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{12}} \\times k_2 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{31}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{21}} \\times k_3 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{33}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{22}} \\times k_4 \\\\ \\end{array} \\ \\ \\ \\ \\ \\ \\ \\ \\begin{array}{ll} \\frac{\\partial \\mathcal{L}}{\\partial I_{12}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_2 + \\frac{\\partial \\mathcal{L}}{f_{12}} \\times k_1 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{21}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_3 + \\frac{\\partial \\mathcal{L}}{f_{21}} \\times k_1 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{23}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{12}} \\times k_4 + \\frac{\\partial \\mathcal{L}}{f_{22}} \\times k_2 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial I_{32}} &amp;= \\frac{\\partial \\mathcal{L}}{\\partial f_{21}} \\times k_4 + \\frac{\\partial \\mathcal{L}}{f_{22}} \\times k_3 \\\\ \\end{array} \\label{eqn:eqnnumber618} \\end{align}\\] \\[\\begin{align} \\frac{\\partial \\mathcal{L}}{\\partial I_{22}} = \\frac{\\partial \\mathcal{L}}{\\partial f_{11}} \\times k_4 + \\frac{\\partial \\mathcal{L}}{\\partial f_{12}} \\times k_3 + \\frac{\\partial \\mathcal{L}}{\\partial f_{21}} \\times k_2 + \\frac{\\partial \\mathcal{L}}{\\partial f_{22}} \\times k_1 \\end{align}\\] Notice that the gradients of individual activation output with respect to image features result in individual weights: \\[\\begin{align} \\frac{\\partial f_{11}} {\\partial I_{11}} = k_1 \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial f_{11}} {\\partial I_{12}} = k_2 \\ \\ \\ \\ \\ \\ \\ ... \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial f_{22}} {\\partial I_{33}} = k_4 \\end{align}\\] Equivalently, from individual neuron perspective, the gradient of an activation output, e.g. \\(\\mathbf{f_{11}}\\), with respect to a weight, e.g. \\(\\mathbf{k_1}\\), in the filter is: \\[\\begin{align} \\frac{\\partial f_{11}} {\\partial k_1} = I_{11} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial f_{11}} {\\partial k_2} = I_{12} \\ \\ \\ \\ \\ \\ \\ ... \\ \\ \\ \\ \\ \\ \\ \\frac{\\partial f_{22}} {\\partial k_4} = I_{33} \\end{align}\\] Let us take another case in which our convolution includes padding equal to 1. See Figure 12.38. The convolution in Figure 12.38 forms a \\(4 \\times 4\\) feature map because of the extra padding. Figure 12.38: CNN (Full Convolution with Padding) Notice this time that the Full Convolution does not require padding during backpropagation, whereas during the feedforward, we have padding set to 1. Therefore, we should notice that performing a Full Convolution with a rotated kernel at \\(180^\\circ\\) requires careful tracing of the influence of every neuron and weight to the Loss Function. With the many permutations of image size, kernel size, and the number of stride and paddings, it requires a bit of heuristic iteration to arrive at the correct combination of knobs to use (in terms of stride, padding, dilation, auto padding) for our full convolution. Figure 12.39 illustrates a combination table as a result of heuristically iterating over a few adjustments of the hyperparameters. Figure 12.39: BackProp Combination (Heuristic) For example, if we use stride=1 with padding=0, e.g. 1/0, against an image of size \\(4\\times4\\) with kernel size \\(3 \\times 3\\), e.g. 4/3, then our full convolution should use 1200 (stride=1, padding=2, dilated image=0, auto.pad=0). Programmatically, that is equivalent to the following code: convolution(image = Dout, filter = rotated.filters, stride = 1, padding = 2, dil_input = 0, auto.pad = 0, autopad = &quot;left&quot;, ptype = &quot;gradient.I.wrt.K&quot;) To automatically determine the proper combination, a few patterns in the table expose the below set of formulas: \\[\\begin{align} \\begin{array}{ll} \\text{stride} &amp;= 1\\\\ \\text{padding} &amp;= \\text{max}(\\text{kernel.ht} - (\\text{padding} + \\text{auto.pad} + 1), 0)\\\\ \\text{dilate}\\_\\text{input} &amp;= \\text{stride} - 1 \\\\ \\text{auto.pad} &amp;= \\text{auto.pad} \\\\ \\text{auto.pad} &amp;= \\text{left} \\end{array} \\label{eqn:eqnnumber619} \\end{align}\\] Note here that we can pass dilation rate (e.g., dil_input) to dilate the jacobian matrix of the gradients, and we can also pass dilation rate hyperparameter (e.g., dil_rate) to dilate receptive fields (which is always set to zero). Additionally, auto padding is done to the left and top edges. Also, our brief experimentation does show that an image size of 4 and a kernel size of 3 does not produce a combination for our full convolution if our stride and paddings are set to 2, as shown in Figure 12.39. This experiment tells us that while the formulas above work in reasonably general cases, there are configurations that may not apply. Here, a decent general case implies that a kernel size of 3 and stride of 1 is a good choice versus a stride of 4, which leaves a gap between receptive fields, causing loss of information. Thus, in our later implementation of CNN, we constrain the stride size not to be equal to or greater than the kernel size. We leave readers to investigate this combination and re-evaluate the general case above. Ninth, in terms of solving for the gradient of the Loss function with respect to the individual weights (parameters in the kernels) in a convolutional layer, we take one of the learnable weights as an example, e.g., \\(\\mathbf{k_1}\\), from Figure 12.36. In such a case, we have the following equation: \\[\\begin{align} \\nabla_{k_1} \\mathcal{L} = \\frac{\\partial \\mathcal{L}} {\\partial k_1} = \\left(\\frac{\\partial \\mathcal{L}}{\\partial f_1}\\right) \\left(\\frac{\\partial f_1} {\\partial k_1}\\right) \\end{align}\\] If an activation function such as RELU or Leaky RELU is involved, then we can also use the following equation: \\[\\begin{align} \\nabla_{k_1} \\mathcal{L} = \\frac{\\partial \\mathcal{L}} {\\partial k_1} = \\left(\\frac{\\partial \\mathcal{L}}{\\partial f_1}\\right) \\left(\\frac{\\partial f_1} {\\partial \\hat{f_1}}\\right) \\left(\\frac{\\partial \\hat{f_1}} {\\partial k_1}\\right) \\end{align}\\] where \\(\\hat{f}_i\\) is a net input as result of the convolution prior to invoking an activation function. Here, we use convolution this time to perform backpropagation. Similar to full convolution, we need the correct knobs to use. Below is a set of formulas that can be applied in general cases. \\[\\begin{align*} \\begin{array}{ll} \\text{stride} &amp;= 1\\\\ \\text{padding} &amp;= \\text{padding}\\\\ \\text{dilate}\\_\\text{rate} &amp;= \\text{stride} - 1\\\\ \\text{auto.pad} &amp;= \\text{auto.pad} \\\\ \\text{autapad} &amp;= \\begin{cases} \\text{right} &amp; \\text{auto.pad} &gt; 0 \\\\ \\text{left} &amp; \\text{otherwise} \\end{cases} \\end{array} \\end{align*}\\] For example, suppose we use a stride=2 and padding=0 against an image with size \\(4 \\times 4\\) and a kernel with size \\(3 \\times 3\\), we arrive at five knobs, e.g. (stride=1, padding=0, dilate_rate=1, auto.pad = 0, autopad=right), to adjust for our backpropagation that also goes through its convolution process (which we tag it with type equal to gradient.K.wrt.I in our implementation). Programmatically, that can be written as such: cv = convolution(image=image, filters=Dout, stride=1, padding=0, dil_rate=1, dil_input=0, auto.pad = 1, autopad = &quot;right&quot;, ptype=&quot;gradient.K.wrt.I&quot;) Note here that we can pass the dilation rate parameter (e.g., dil_rate) to dilate receptive fields, and we can also pass dilation rate (e.g., dil_input) to dilate images/feature maps (which is always set to zero). Let us now see our example implementation of CNN Backpropagation. For that, let us show a summarized form of backpropagation (see Figure 12.40). Figure 12.40: CNN (Backpropagation Summary) get.gradient &lt;- function(map, afunc) { # @P1@^P1 if (!is.null(afunc)) { di = dim(map) afunc = get(afunc) gradient = map for (s in 1:di[4]) for (d in 1:di[3]) { gradient[,,d,s] = gradient.activation(map[,,d,s], afunc) } gradient = array(gradient, di) } else { gradient = map } gradient } back.propagation.cnn &lt;-function (X, Y, model) { layers = model$layers H = length(layers) - 1 #remove FC layer and focus on CONV layers fc.layers = model$fc.model$layers backprop = back.propagation(model$flattened.output, Y, model$fc.model, afunc=&quot;softmax&quot;) Dout = backprop$delta.output[[1]] di = dim(model$cnn.output[[H]]$pw.fmap) gradient.loss = Dout %*% t(fc.layers[[1]]$omega$weight) # @L/@P1 gradient.loss = gradient.loss[,-1] #remove bias loss.I = array(t(gradient.loss), di) delta.normparams = list() delta.dws = list() delta.pws = list() delta.dw.biases = list() delta.pw.biases = list() for (L in H:1) { layer = layers[[L]] cache = model$cache.output[[L]] cur.fmaps = model$cnn.output[[L]] if (L &gt; 1) { prev.fmaps = model$cnn.output[[L-1]]$pw.fmap } else if (L==1) { prev.fmaps = X } if (layer$istype == &quot;pooling&quot;) { conv.output = pool.backprop(prev.fmaps, filter = layer$window, Dout = loss.I, stride = layer$stride, pool.cache = model$pool.cache[[L]], ptype = &quot;gradient.I.wrt.P&quot;, pool = layer$ptype) loss.I = conv.output$feature.map } else if (layer$istype == &quot;convolv&quot;) { # activation gradient first before normalization if (!is.null(layer$afunc)) { gradient.I = get.gradient(cur.fmaps$pw.fmap, layer$afunc) Dout = loss.I * gradient.I } else { Dout = loss.I } if (layer$batchnorm$normalize != FALSE) { normalize = normalize.backward(Dout, layer) Dout = normalize$gradient.loss delta.normparams[[L]] = normalize$params } conv.output = convolution(prev.fmaps, filter = Dout, # used only to calculate shape Dout = Dout, cur.fmaps = cur.fmaps, pw.kernel = layer$pw.kernel$weight, stride = cache$wstride, padding = cache$wpadding, dil_rate = cache$wdil_rate, dil_input = cache$wdil_input, auto.pad = cache$wauto.pad, autopad = cache$wautopad, ptype = &quot;gradient.K.wrt.I&quot;) delta.dws[[L]] = conv.output$feature.map$delta.dw delta.pws[[L]] = conv.output$feature.map$delta.pw dw.Dout = conv.output$feature.map$dw.Dout pw.Dout = conv.output$feature.map$pw.Dout # Compute Biases di.dw = dim(dw.Dout) di.pw = dim(pw.Dout) delta.dw.bias = rep(0, di.dw[3]) delta.pw.bias = rep(0, di.pw[3]) for (d in 1:di.dw[3]) { delta.dw.bias[d] = sum(dw.Dout[,,d,]) } for (f in 1:di.pw[3]) { delta.pw.bias[f] = sum(pw.Dout[,,f,]) } delta.dw.biases[[L]] = delta.dw.bias delta.pw.biases[[L]] = delta.pw.bias rotated.kernel = rotate.matrix.180(layer$dw.kernel$weight) conv.output = convolution(dw.Dout, filter = rotated.kernel, stride = cache$stride, padding = cache$padding, dil_rate = cache$dil_rate, dil_input = cache$dil_input, auto.pad = cache$auto.pad, autopad = cache$autopad, ptype = &quot;gradient.I.wrt.K&quot;) loss.I = conv.output$feature.map } } list(&quot;fc.delta.params&quot; = backprop$delta.params, &quot;cnn.delta.normparams&quot; = delta.normparams, &quot;cnn.delta.dws&quot; = delta.dws, &quot;cnn.delta.pws&quot; = delta.pws, &quot;cnn.delta.dw.biases&quot; = delta.dw.biases, &quot;cnn.delta.pw.biases&quot; = delta.pw.biases) } Our implementation of full convolution rotates the filter. Below is a script to perform matrix rotation. rotate.90 &lt;- function(filters) { len = length(filters) for (l in 1:len) { a = filters[[l]] dl = dim(a); h = dl[1]; w = dl[2]; d = dl[3] mx = NULL for (depth in 1:d) { m = a[,,depth] mx = cbind(mx, t(m[nrow(m):1,,drop = FALSE])) } filters[[l]] = array(mx, c(h, w, d)) } filters } rotate.180 &lt;- function(a) { rotate.90(rotate.90(a)) } rotate.matrix.90 &lt;- function(a) { dl = dim(a) mx = NULL for (depth in 1:dl[3]) { m = a[,,depth] mx = cbind(mx, t(m[nrow(m):1,,drop = FALSE])) } array(mx, dl) } rotate.matrix.180 &lt;- function(a) { rotate.matrix.90(rotate.matrix.90(a)) } 12.4.10 Optimization Finally, reviewing Figure 12.36 once again, we come down to the update rules. For the Backward Pass, let us use one of the kernel weights, e.g. \\(k_1\\), to illustrate. \\[\\begin{align} k_1 = k_1 - \\eta \\times \\nabla k_1 \\mathcal{L} \\ \\ \\ \\ \\ \\ \\ ... \\ \\ \\ \\ \\ \\ \\ k_4 = k_4 - \\eta \\times \\nabla k_4 \\mathcal{L} \\end{align}\\] Our implementation of the backward pass is encapsulated into the optimizer(.) function. Recall our discussion of optimization in MLP section. Here, we implement a few of the known optimizers. See below: optimizer &lt;- function(backprop, layers, optimize=&quot;momentum&quot;, t, eta) { optimize.sgd = sgd &lt;- function(param, gradient, eta, t = NULL) { param$weight = param$weight - eta * gradient param } optimize.momentum = momentum &lt;- function(param, gradient, eta, t = NULL) { gamma = 0.90 param$nu = gamma * param$nu + eta * gradient param$weight = param$weight - param$nu param } optimize.rmsprop = rmsprop &lt;- function(param, gradient, eta, t = NULL) { beta1 = 0.90 ; eps=1e-10 param$nu = beta1 * param$nu + (1 - beta1) * gradient^2 phi = eta / (sqrt(param$nu) + eps) param$weight = param$weight - phi * gradient param } optimize.adam = adam &lt;- function(param, gradient, eta, t) { beta1 = 0.90; beta2 = 0.999; eps=1e-10 param$rho = beta1 * param$rho + (1 - beta1) * gradient param$nu = beta2 * param$nu + (1 - beta2) * gradient^2 rho.hat = param$rho / (1 - beta1^t) nu.hat = param$nu / (1 - beta2^t) phi = eta / (sqrt(nu.hat) + eps) param$weight = param$weight - phi * rho.hat param } optimize.adadelta = adadelta &lt;- function(param, gradient, eta, t) { rho = 0.90; eps=1e-10 param$rho = rho * param$rho + (1 - rho) * gradient^2 phi = sqrt(param$nu + eps) * (gradient / sqrt(param$rho + eps)) param$nu = rho * param$nu + (1 - rho) * phi^2 param$weight = param$weight - phi param } optimizing &lt;- function(func, param, gradient, eta, t) { func = get(func) func(param, gradient, eta, t) } delta.params = backprop$fc.delta.params delta.normparams = backprop$cnn.delta.normparams delta.dws = backprop$cnn.delta.dws delta.pws = backprop$cnn.delta.pws delta.dw.biases = backprop$cnn.delta.dw.biases delta.pw.biases = backprop$cnn.delta.pw.biases H = length(layers) for (L in 1:H) { layer = layers[[L]] if (layer$istype == &quot;dense&quot;) { fc.layers = layer$fc.layers len = length(delta.params) for (l in 1:len) { omega = optimizing(optimize, fc.layers[[l]]$omega, delta.params[[l]]$omega, eta, t) layer$fc.layers[[l]]$omega = omega if (fc.layers[[l]]$batchnorm == TRUE) { batch.gamma = optimizing(optimize, fc.layers[[l]]$batch.gamma, delta.params[[l]]$gamma, eta, t) batch.beta = optimizing(optimize, fc.layers[[l]]$batch.beta, delta.params[[l]]$beta, eta, t) layer$fc.layers[[l]]$batch.gamma = batch.gamma layer$fc.layers[[l]]$batch.beta = batch.beta } } } else if (layer$istype == &quot;convolv&quot;) { delta.dw = delta.dws[[L]] delta.pw = delta.pws[[L]] delta.dw.bias = delta.dw.biases[[L]] delta.pw.bias = delta.pw.biases[[L]] layer$dw.kernel = optimizing(optimize, layer$dw.kernel, delta.dw, eta, t) layer$pw.kernel = optimizing(optimize, layer$pw.kernel, delta.pw, eta, t) layer$dw.bias = optimizing(optimize, layer$dw.bias, delta.dw.bias, eta, t) layer$pw.bias = optimizing(optimize, layer$pw.bias, delta.pw.bias, eta, t) if (layers[[L]]$batchnorm$normalize != FALSE) { batchnorm = layer$batchnorm batchnorm$gamma = optimizing(optimize, batchnorm$gamma, delta.normparams[[L]]$gamma, eta, t) batchnorm$beta = optimizing(optimize, batchnorm$beta, delta.normparams[[L]]$beta, eta, t) layer$batchnorm = batchnorm } } layers[[L]] = layer } layers } 12.4.11 Normalization In the MLP section, we introduced the concept of Batch normalization, accompanied by the math behind it and its implementation. In this section, we continue to show Batch normalization for CNN, with a brief introduction to the concept of Layer normalization. While Layer normalization may not be as common in usage for CNN, it helps show that the idea is identical. Furthermore, the implementation is the same. In Batch normalization, we group the operation as a Depthwise operation. Layer normalization groups the operation as a Sample-wise operation. normalize.forward &lt;- function(H, layer, train, eps=1e-10, momentum = 0.90) { ntype = layer$batchnorm$normalize gamma = layer$batchnorm$gamma$weight beta = layer$batchnorm$beta$weight moving.mu = layer$batchnorm$moving.mu moving.variance = layer$batchnorm$moving.var if (ntype == &quot;layer&quot;) { # Layer normalization ( Height, Width, Depth, Sample) # normalize across Samples per Layer mu = apply(H, 4, mean) H.mu = sweep(H, 4, mu, &#39;-&#39;) var = apply(H.mu^2, 4, mean) istd = 1 / sqrt(var + eps) H.norm = sweep(H.mu, 4, istd, &#39;*&#39;) } else if (ntype == &quot;batch&quot;) { # Batch normalization ( Height, Width, Depth, Sample) # normalize across Depth per Layer if (train == TRUE) { mu = apply(H, 3, mean) H.mu = sweep(H, 3, mu, &#39;-&#39;) var = apply(H.mu^2, 3, mean) istd = 1 / sqrt(var + eps) H.norm = sweep(H.mu, 3, istd, &#39;*&#39;) # We can use moving average &amp; variance for this normalization # because normalization is across the entire mini-batch. moving.mu = momentum * moving.mu + (1 - momentum) * mu moving.variance = momentum * moving.variance + (1-momentum)*var layer$batchnorm$moving.mu = moving.mu layer$batchnorm$moving.var = moving.variance } else { H.mu = sweep( H, 3, moving.mu, &#39;-&#39;) istd = 1 / sqrt(moving.variance + eps) H.norm = sweep(H.mu, 3, istd, &#39;*&#39;) } } layer$batchnorm$H.norm = H.norm layer$batchnorm$H.mu = H.mu layer$batchnorm$istd = istd H.hat = H.norm * gamma + beta list(&quot;feature.map&quot; = H.hat, &quot;layer&quot; = layer) } normalize.backward &lt;- function(Dout, layer) { ntype = layer$batchnorm$normalize H.norm = layer$batchnorm$H.norm H.mu = layer$batchnorm$H.mu istd = layer$batchnorm$istd gamma = layer$batchnorm$gamma$weight s = ifelse(ntype == &quot;batch&quot;, 3, 4) # batchnorm (3) or layernorm (4) m = apply(H.norm, s, length) delta.gamma = apply(Dout * H.norm, s, sum) delta.beta = apply(Dout, s, sum) delta.H.norm = Dout * gamma delta.std = apply(delta.H.norm * H.mu, s, sum) delta.var = delta.std * -0.5 * istd^3 delta.Hmu1 = apply( delta.H.norm * -istd, s, sum) delta.Hmu2 = delta.var * apply( -2 * H.mu,s, mean) delta.mu = delta.Hmu1 + delta.Hmu2 delta.out = sweep(delta.H.norm * istd + delta.var * (2 * H.mu) / m, s, (delta.mu / m), &#39;+&#39;) params = list(&quot;gamma&quot; = delta.gamma, &quot;beta&quot; = delta.beta) list(&quot;gradient.loss&quot; = delta.out, &quot;params&quot; = params) } 12.4.12 Step Decay There is one other pointer to consider in CNN, which has to do with adjusting the learning rate during training. While other approaches may use different exponential formulations, here, we recall from MLP the use of the step decay approach using a typical formulation below: \\[\\begin{align} \\eta^{(t+1)} = \\eta^{(initial)} \\times DF^{floor\\left(\\frac{t}{step size}\\right)} \\end{align}\\] where t is the epoch and \\(\\mathbf{\\eta}\\) is the learning rate. The equivalent implementation follows: step.decay &lt;- function(eta, epoch, decay.factor=0.55, step.size=5) { # default decay.factor=0.55 and step.size=5 for cifar-10 dataset eta * (decay.factor ^ floor(epoch/step.size)) } To illustrate the use, below is an example of scheduled decay we use for a cifar-10 dataset of 50,000 images for training. The idea is to try to get the most (fastest) learning during the first five epochs using a large learning rate, then decay at a slightly lower learning rate between 5 and 10 epochs before we conveniently settle below \\(\\text{1e-3}\\) range past 15 epochs. step.decay(0.05, seq(1,20), decay.factor=0.80, step.size=1) ## [1] 0.04000000000 0.03200000000 0.02560000000 ## [4] 0.02048000000 0.01638400000 0.01310720000 ## [7] 0.01048576000 0.00838860800 0.00671088640 ## [10] 0.00536870912 0.00429496730 0.00343597384 ## [13] 0.00274877907 0.00219902326 0.00175921860 ## [16] 0.00140737488 0.00112589991 0.00090071993 ## [19] 0.00072057594 0.00057646075 Before we finally discuss the implementation of the CNN function, let us first cover GEMM and Depthwise Separable Convolution. 12.4.13 GEMM (Matrix Multiplication) Convolution is computationally expensive if we use the common dot-product operation against the raw image input. Doing so is otherwise known as Direct Convolution. Alternatively, we can use what is called GEMM-based Convolution. This convolution involves three consecutive techniques to help boost performance. The first technique is to perform image-to-column (im2col) conversion, which stacks all the receptive fields in a matrix. The second technique GEneral Matrix to Matrix Multiplication (GEMM) operation performs the actual dot-product. The third technique is a column-to-image (col2im) conversion which transforms the resultant Toeplitz Matrix back to its convolution form. We use Figure 12.41 to illustrate a feed-forward convolution using the three methods above. The main idea here is to stack up all the receptive fields into a matrix form and perform the same conversion for the kernels such that the two newly formed matrices can convolve to form the feature map. Note that in the figure, both ker2col and im2col are arranged horizontally to fit our text, labeled as \\(2 \\times 28\\) and \\(9 \\times 28\\), respectively. However, we have it vertically in such an implementation, namely \\(28 \\times 2\\) and \\(28 \\times 9\\). We then convolve so that \\(t(28 \\times 2) * (28 \\times 9) = (2 \\times 9)\\). Also, note that we assume a tensor of (HxWxCxI) dimension. On that note, we intend to reduce the dimension into a 2-dimension matrix. Figure 12.41: CNN (Main Convolution) In terms of backpropagation, we use Figure 12.42 to illustrate the use of a similar GEMM-based method to perform full convolution, computing the gradient with respect to the filters to arrive at our delta gradients for the input. Similarly, the arrangement is plotted horizontally, where our filters and input tensors are converted into 2-dimensional matrices for convolution. Figure 12.42: CNN (Gradient with respect to Filter) Similarly, Figure 12.43 illustrates the use of GEMM-based convolution to compute the gradient with respect to the input to arrive at the delta gradients for our filters. Figure 12.43: CNN (Gradient with respect to Input) Finally, we also can use the GEMM-based method to perform the same operation for our max pool. See Figure 12.44. Figure 12.44: CNN (Gradient with respect to MaxPool) While the use of the GEMM-based method benefits from slow convolution computation, one alternative method for machines or gadgets with low resources is called Depthwise Separable Convolution complemented by Pointwise Convolution. The next section discusses the idea in more detail. 12.4.14 Depthwise Separable Convolution (DSC) Our implementation of CNN uses Depthwise Separable Convolution (DSC) along with Pointwise Convolution (PC) and an intermediate GEMM implementation within the loop. The idea is to separate the convolution operation for individual channels (depthwise) as the first step. Each channel from the Input is separately convolved with each channel from the filter such that we have \\((5\\times5)_{(\\text{input})} * (3\\times3)_{(\\text{kernel})} = (3\\times3)_{(\\text{feature map})}\\) for each Image input. We then conjoin to form a \\((3\\times 3\\times 1)\\) feature map. This is followed by performing pointwise convolution with a set of N \\((1\\times C)\\) filters. See Figure 12.45. Figure 12.45: CNN (Depthwise) Below are the implementations of convolving matrices starting with the convolve.image(.) function, which handles the main convolution using DSC and PC. Our DSC implementation performs the ker2col conversion first as a one-time step, then a set of im2col conversions after accumulating the receptive fields row-wise. We then use the converted im2col matrix for every row to perform GEMM convolution with ker2col. This extra step assumes handling a decent mini-batch size (labeled as I in our text and S in our implementation). The convolution result is conjoined to form an \\((O\\times O\\times C \\times S)\\) matrix that is then convolved with a \\(1\\times C\\) of N filters which are simply represented as \\(C\\times N\\) in our implementation. Looping through S, we end up with the expected \\((O\\times O\\times N \\times S)\\) feature map. convolve.image &lt;- function(image, dw.kernel, pw.kernel, dw.bias, pw.bias, bias, dil_rate, O, h, w, r, c) { dil.filters = dilate.filters(dw.kernel, dil_rate) len = length(dil.filters) di = dim(image) di.k = dim(dw.kernel) len = dim(pw.kernel)[2] ker2col = array(dw.kernel, c(di.k[1] * di.k[2], di.k[3])) #ker2col = rbind(dw.bias, ker2col) # add bias n = 0 I = list() dw.fmap = array(0, c(O, O, di[3], di[4])) for (i in 1:O) { n = 0 I = list() for (j in 1:O) { n = n + 1 hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) I[[n]] = array(image[hs:he, ws:we,,], c(r * c, di[3], di[4])) } D = array(unlist(I), c( r * c, di[3], di[4] * n)) for (d in 1:di[3]) { # Depthwise Separable Convolution (KxKxM filter) im2col = D[,d,] #im2col = rbind(rep(1, di[4] * n), im2col) # Add bias constant conv = t(ker2col[,d]) %*% im2col dw.fmap[i,,d,] = t(array(conv, c(di[4], n))) } } dw.fmap = array(unlist(dw.fmap), c(O, O, di[3], di[4])) # DfxDfxMxS pw.fmap = list() if (bias==TRUE) { pw.kernel = rbind(pw.bias, pw.kernel) } # add bias sz = O * O for (s in 1:di[4]) { # Pointwise Convolution (MxN filter) pw.fmap[[s]] = array( dw.fmap[,,,s], c(sz, di[3])) if (bias==TRUE) { pw.fmap[[s]] = cbind(rep(1, sz), pw.fmap[[s]]) # add bias constant } pw.fmap[[s]] = pw.fmap[[s]] %*% pw.kernel } pw.fmap = array(unlist(pw.fmap), c(O, O, len, di[4])) remove(I, ker2col, D) list(&quot;dw.fmap&quot; = dw.fmap, &quot;pw.fmap&quot; = pw.fmap) } A note to emphasize in the implementation above is the addition of bias only to the Pointwise convolution. We leave readers to investigate the addition of bias to the Depthwise separable convolution. Next, we account for solving the gradient with respect to the filter to generate our Input gradients. Our implementation performs a full convolution by rotating our kernel to 180 degrees. See back.propagation(.). gradient.I.wrt.K &lt;- function(image, dw.kernel, O, h, w, r, c) { di = dim(image) fmap = array(0, c(O, O, di[3], di[4])) ker2col = array(dw.kernel, c(r * c, di[3])) for (i in 1:O) { n = 0 I = list() for (j in 1:O) { n = n + 1 hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) I[[n]] = array(image[hs:he, ws:we,,], c(r * c, di[3], di[4])) } D = array(unlist(I), c( r * c, di[3], di[4] * n)) for (d in 1:di[3]) { # Depthwise Separable Backprop. (KxKxM filter) conv = t(ker2col[,d]) %*% D[,d,] fmap[i,,d,] = t(array(conv, c(di[4], n))) } } remove(I, ker2col, conv) fmap } Next, we also account for solving the gradient with respect to input (or feature map) to generate our filter gradients. Our implementation performs a convolution of the gradient derived from a prior deeper layer (l) with the input of the current layer (l-1). See back.propagation(.). gradient.K.wrt.I &lt;- function(image, Dout, cur.fmaps, pw.kernel, dil_rate, O, h, w) { pw.Dout = Dout dw.fmap = cur.fmaps$dw.fmap pw.fmap = cur.fmaps$pw.fmap di.dw = dim(dw.fmap) di.pw = dim(pw.fmap) delta.pw = 0 dw.Dout = list() for (s in 1:di.dw[4]) { im2col = array( dw.fmap[,,,s], c(di.dw[1] * di.dw[2], di.dw[3])) Dout2col = array( pw.Dout[,,,s], c(di.pw[1] * di.pw[2], di.pw[3])) delta.pw = delta.pw + t(im2col) %*% Dout2col dw.Dout[[s]] = (Dout2col) %*% t(pw.kernel) } #delta.pw = delta.pw / di.pw[4] # take average gradient of minibatch Dout2col = array(unlist(dw.Dout), c(di.pw[1] * di.pw[2], di.dw[3], di.dw[4])) dw.Dout = array(unlist(dw.Dout), c(di.pw[1], di.pw[2], di.dw[3], di.dw[4])) K = dilate(dw.Dout, dil_rate) # Becomes Filter for gradient.I.wrt.K r = nrow(K) c = ncol(K) di.k = dim(K) di = dim(image) n = 0 delta.dw = array(0, c(O, O, di[3])) for (i in 1:O) { for (j in 1:O) { n = n + 1 hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) for (d in 1:di[3]) { Dout2col.sub = array(K[,,d,] , c(r * c, di[4] ) ) im2col.sub = array(image[hs:he, ws:we,d,], c(r * c, di[4])) delta.dw[i,j,d] = sum(Dout2col.sub * im2col.sub) } } } #delta.dw = delta.dw / di[4] # take average gradient of minibatch remove(Dout2col, im2col.sub, Dout2col.sub) list(&quot;dw.Dout&quot; = dw.Dout, &quot;pw.Dout&quot; = pw.Dout, &quot;delta.dw&quot; = delta.dw, &quot;delta.pw&quot; = delta.pw) } A note to emphasize in our implementation is the summation of gradients accumulated from all minibatch samples. We leave readers to investigate the effect of averaging the gradients instead, granting that the average does not swing the value too large or too small enough to offset the trajectory. Lastly, we derive the gradient with respect to a maximum pool. Here, we rely on a cache containing the location of the maximum values. gradient.I.wrt.P &lt;- function(image, Dout, K, O, pool, h, w, pool.cache) { r = nrow(K) c = ncol(K) img.copy = array(0, dim(image)) di = dim(Dout) gr2col = array(Dout, c(1, di[1] * di[2] * di[3] * di[4])) len = length(gr2col) if (pool == &quot;maxpool&quot;) { one.hot = array(0, c(len,r * c)) for (i in 1:len) { one.hot[i, pool.cache[i]] = 1 } } else { all.one = array(1, c(len, r*c)) } n = 0 skip = di[3] * di[4] k = prod(di) for (j in 1:O) { for (i in 1:O) { n = n + 1 hs = h[i]; he = (hs + r - 1) ws = w[j]; we = (ws + c - 1) b = (n-1) * skip + 1 e = n * skip a.idx = seq(from=b, to=e) b.idx = seq(from=n, to=k, by=di[1] * di[2]) if (pool == &quot;maxpool&quot;) { z = sweep(t(one.hot[a.idx,]), 2, gr2col[,b.idx], &#39;*&#39;) } else if (pool == &quot;avgpool&quot;) { z = sweep(t(all.one[a.idx,]), 2, gr2col[,b.idx], &#39;*&#39;) } dimen.pool = array(z, c(r,c, di[3], di[4])) img.copy[hs:he, ws:we,,] = img.copy[hs:he, ws:we,,] + dimen.pool } } img.copy } Other types of Convolution cater to improving the computation of Convolution, of which two of them are necessary for investigation, namely FFT (Fast Fourier Transform) Convolution vs. WinoGrad Convolution. However, we leave readers to investigate these algorithms further, along with how the computations are distributed across a set of GPU processors in a parallel fashion; for example, having each kernel affined to its dedicated GPU unit and performing GEMM. 12.4.15 CNN Implementation Below is a rudimentary implementation of our CNN with the forward feed for CNN and forward feed for MLP (taking part to support fully-connected layers). library(caret) accuracy &lt;- function(t, o) { p1 = apply(t, 1, which.max) p2 = apply(o, 1, which.max) p = p1 == p2 sum(p)/length(p) } get.batch &lt;- function(sample.data, labels, k, t) { set.seed(t) batch.indices = createFolds(sample.data, k = k, returnTrain = FALSE) batches = list() for (i in 1:k) { indices = batch.indices[[i]] X = extract.image(images, indices) Y = extract.label.onehot(images, labels, indices) if (i != k) { batches[[i]] = list( &quot;X&quot; = X, &quot;Y&quot; = Y ) } else { validation = list(&quot;X&quot; = X, &quot;Y&quot; = Y) } } list(&quot;train&quot; = batches, &quot;validation&quot; = validation) } flush.str &lt;- function(...) {str = sprintf(...); print(str); flush.console()} transfer.learning &lt;- function(model.file) { my.cnn.model = readRDS(model.file) my.cnn.model$layers } my.CNN &lt;- function(target.images, labels, layers=NULL, optimize, transfer=NULL, minibatch=40, epoch=100, eta = 0.01) { options(digits = 16) # 16 digits precision eta = c(eta) total.cost = epoch.cost = NULL total.accuracy = epoch.accuracy = fc.params = NULL total.validate = NULL # Target images population = target.images$rgb population.len = length(population) sample.set = 250 # population.len shuffled.data = sample.int(n=population.len, size=population.len, replace=FALSE) sample.size = ifelse(sample.set &lt; population.len, sample.set, population.len) sample.data = sample(shuffled.data, sample.size) k = ceiling(sample.size / minibatch) flush.str( &quot;No of batches: %d&quot;, k ) if (!is.null(transfer)) { layers = transfer.learning(transfer) } if (is.null(layers)) { stop(&quot;No network layers defined.&quot;) } my.time = Sys.time() for (t in 1:epoch) { batch.cost = batch.accuracy = NULL batch.time = Sys.time() step.eta = step.decay(eta, t, decay.factor=1) n = 0 batches = get.batch(sample.data, labels, k, t) for (batch in batches$train) { n = n + 1 X = batch$X Y = batch$Y model = forward.pass.cnn(X, layers) backprop = back.propagation.cnn(X, Y, model) layers = optimizer(backprop, model$layers, optimize, (t - 1) * k + n, step.eta) len = length(model$fc.model$layers) softmax.prob = model$fc.model$layers[[len]]$output loss = softmax.loss(Y, softmax.prob) accurate = accuracy(Y, softmax.prob) batch.cost = c(batch.cost, mean(loss)) batch.accuracy = c(batch.accuracy, accurate) if (n %% 50 == 0) { new.time = Sys.time() lag.time = difftime(new.time, my.time, units=&quot;secs&quot;) my.time = new.time stime = format(Sys.Date(), &quot;%c&quot;) flush.str( &quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lagtime (sec): %5.3f&quot;, n, mean(loss), (t - 1) * k + n, accurate, lag.time) } if (is.na(mean(loss)) || mean(loss) &gt; 40) { flush.str( &quot;loss NaN/increasing at %d epoch.&quot;, (t - 1) * k + n) stop(&quot;&quot;) } } ## Validate batches = batches$validation X = batch$X Y = batch$Y valid.model = forward.pass.cnn(X, layers, train=FALSE) len = length(valid.model$fc.model$layers) softmax.prob = valid.model$fc.model$layers[[len]]$output valid.accurate = accuracy(Y, softmax.prob) new.time = Sys.time() lag.time = difftime(new.time, batch.time, units=&quot;secs&quot;) epoch.cost = c(epoch.cost, mean(batch.cost)) epoch.accuracy = c(epoch.accuracy, mean(batch.accuracy)) total.cost = c(total.cost, batch.cost) total.accuracy = c(total.accuracy, batch.accuracy) total.validate = c(total.validate, valid.accurate) flush.str( &quot;epoch %d: loss %2.3f accuracy %2.3f val %2.3f lag time (sec): %5.3f&quot;, t, mean(batch.cost), mean(batch.accuracy), valid.accurate, lag.time) if (t %% 10 == 0) { gc() } # garbage collection if (valid.accurate &gt;= 0.90) { flush.str( &quot;We have reached a validation accuracy of %2.3f. This is good enough&quot;, valid.accurate) break } } list(&quot;model&quot; = model, &quot;layers&quot; = layers, &quot;eta&quot; = eta, &quot;cost&quot; = epoch.cost, &quot;accuracy&quot; = epoch.accuracy, &quot;total.cost&quot; = total.cost, &quot;total.accuracy&quot; = total.accuracy, &quot;total.valid&quot; = total.validate) } In the following two sections, we discuss the use of GEMM in our implementation and a more efficient convolution algorithm, namely Depthwise Separable Convolution and Pointwise Convolution. 12.4.16 CNN Application In this section, let us perform image classification using CIFAR-10 as our dataset, containing 60000 \\(32 \\times 32\\) tiny images (Krizhevsky A. 2009). Here, we use the binary version of the dataset. At the time of writing, the currently available dataset is broken down into six files: data_batch{1,2,3,4,5}.bin and a test_batch.bin. However, because the dataset is a collection of images instead of one single JPEG image, we have to read differently such that instead of using readJPEG(.), we use readBin(.) to read raw data in binary format. Below is a modified implementation of reading and parsing the CIFAR-10 dataset (motivated by an R script written by an anonymous matt from Stackoverflow (questions/32113942)): getNext &lt;- function(fp, typ = NULL) { if (!is.null(typ) &amp;&amp; typ == &quot;label&quot;) { readBin(fp, integer(), size=1, n=1, endian=&quot;big&quot;) } else { as.integer(readBin(fp, raw(), size=1, n=1024, endian=&quot;big&quot;)) } } readBatch &lt;- function(fn, images, labels) { fp = file(fn, &quot;rb&quot;) i = length(images$lab) for (g in 1:10000) { i = i + 1 images$lab[i] = labels[ getNext(fp, ty = &quot;label&quot;) + 1, ] images$rgb[[i]] = data.frame( r = getNext(fp), g = getNext(fp), b = getNext(fp)) } close(fp) list(&quot;label&quot; = images$lab, &quot;rgb&quot; = images$rgb) } extract.label &lt;- function(images, labels, index) { labels[ images$lab[[index]],] } extract.label.onehot &lt;- function(images, labels, index) { len = length(index) list.label = array(0, c(len,10)) for (i in 1:len) { n = nrow(labels) label = rep(0, n) label[images$lab[[index[i]]]] = 1 list.label[i,] = label } list.label } extract.image &lt;- function(images, index) { len = length(index) list.img = array(0, c(32,32,3, len)) for (i in 1:len) { img = images$rgb[[index[i]]] r = matrix(img$r, ncol=32, byrow = TRUE) g = matrix(img$g, ncol=32, byrow = TRUE) b = matrix(img$b, ncol=32, byrow = TRUE) norm = 255 img = array(cbind(r,g,b) / norm, c(32,32,3)) # normalize list.img[,,,i] = img } list.img } search.image &lt;- function(images, labels, label = &#39;ship&#39;) { idx = which(labels == label) len = length(images$rgb) what = NULL for (i in 1:len) { if (images$lab[[i]] == idx) { what = c(what, i) } } what } draw.image &lt;- function(image, main=&quot;Apple, Oranges, and Banana&quot;) { sz = nrow(image) par(pty=&quot;s&quot;) plot(1, type=&quot;n&quot;, xlim=c(1, sz), ylim=c(1, sz), xlab=&quot;Image Width&quot;, ylab=&quot;Image Height&quot;, main=main) rasterImage(image,xleft=1, ybottom=sz, xright=sz, ytop=1) } We then use our draw.image(.) function like so (for example, reading the 20th image in the dataset): images = list(&quot;label&quot; = list(), &quot;rgb&quot; = list()) labels = read.table(&quot;cifar-10-batches-bin/batches.meta.txt&quot;) for (f in 1:5) { fn = paste0(&quot;cifar-10-batches-bin/data_batch_&quot;, f, &quot;.bin&quot;) images = readBatch(fn, images, labels ) } this.image = extract.image(images,20) this.label = extract.label(images,labels,20) draw.image(array(this.image, c(32,32,3)) , main=this.label) Figure 12.46: CIFAR-10 (20th Image) The image has the following dimension: image = extract.image(images,1) dim(image) ## [1] 32 32 3 1 The corresponding label is translated into a one-hot vector with the 7th index matching the label for a frog: label = extract.label.onehot(images, labels, 1) label ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 0 1 0 0 0 t(labels) ## [,1] [,2] [,3] [,4] [,5] [,6] ## V1 &quot;airplane&quot; &quot;automobile&quot; &quot;bird&quot; &quot;cat&quot; &quot;deer&quot; &quot;dog&quot; ## [,7] [,8] [,9] [,10] ## V1 &quot;frog&quot; &quot;horse&quot; &quot;ship&quot; &quot;truck&quot; With that, let us now design our CNN layers like so: library(dequer) set.seed(2021) size = 32 depth = 3 minibatch = 32 X = array(seq(1, size * size * depth * minibatch), c(size, size, depth, minibatch)) dim(X) ## [1] 32 32 3 32 cnn.layers = deep.cnn.layers( X, list( type = &quot;convolv&quot;, size=3, filters=32, stride=1, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;pooling&quot;, size=2, stride=2, ptype=&quot;maxpool&quot;), list( type = &quot;convolv&quot;, size=3, filters=64, stride=1, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;pooling&quot;, size=2, stride=2, ptype=&quot;maxpool&quot;), list( type = &quot;dense&quot;, list( size=256, drop=0.05 ),list( size=10 )) ) We then invoke our my.CNN(.) function, feeding a minibatch=32 and epoch=10. See plot in Figure 12.47. set.seed(2021) cnn.model = my.CNN(images, labels, layers = cnn.layers, optimize=&quot;adam&quot;, minibatch=minibatch, epoch=10, eta=0.001) ## [1] &quot;No of batches: 8&quot; ## [1] &quot;epoch 1: loss 6.246 accuracy 0.146 val 0.161 lag time (sec): 13.299&quot; ## [1] &quot;epoch 2: loss 3.078 accuracy 0.273 val 0.194 lag time (sec): 12.607&quot; ## [1] &quot;epoch 3: loss 1.599 accuracy 0.508 val 0.000 lag time (sec): 11.835&quot; ## [1] &quot;epoch 4: loss 1.251 accuracy 0.580 val 0.258 lag time (sec): 12.118&quot; ## [1] &quot;epoch 5: loss 0.982 accuracy 0.689 val 0.000 lag time (sec): 11.839&quot; ## [1] &quot;epoch 6: loss 0.842 accuracy 0.736 val 0.125 lag time (sec): 12.420&quot; ## [1] &quot;epoch 7: loss 0.602 accuracy 0.845 val 0.065 lag time (sec): 11.957&quot; ## [1] &quot;epoch 8: loss 0.408 accuracy 0.916 val 0.094 lag time (sec): 11.886&quot; ## [1] &quot;epoch 9: loss 0.307 accuracy 0.959 val 0.194 lag time (sec): 11.908&quot; ## [1] &quot;epoch 10: loss 0.262 accuracy 0.955 val 0.219 lag time (sec): 12.232&quot; x = seq(1, length(cnn.model$cost)) y = cnn.model$cost y1 = (y - min(y))/(max(y) - min(y)) y2 = cnn.model$accuracy plot(NULL, xlim=range(x), ylim=range(0,y1), xlab=&quot;Epoch&quot;, ylab=&quot;Cross-Entropy Loss / Accuracy&quot;, main=&quot;CNN (250, 32 - 32dm, 64dm, fc256)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;, lwd=2) lines(x, y2, col=&quot;navyblue&quot;, lwd=2) Figure 12.47: CNN PLOT Notice in the figure that we trained only 250 images. It is worth noting that our CNN implementation indeed demonstrates the ability to train. We can shoot to 96.8% (even up to 100%) top-1 train accuracy. However, if we perform prediction, we may see our top-1 test accuracy as low as 20%. That is because there is insufficient data to train; thus, we are overfitting the train data. Over time, we performed several iterations to achieve a better train and test accuracy using the entire 50,000 images. Our model was trained with a top-1 train accuracy of 94.20% after epoch 22 for 50000 cifar-10 images, which took 19.8 hours with an average lag time of 3750 seconds per epoch using only four CPUs (from quad-core) with no GPU support. Here, we show how we can use our saved model: transfer.cnn.model = readRDS(&quot;transfer_model.rds&quot;) The model uses the following neural network configuration: set.seed(2021) size = 32 depth = 3 minibatch = 32 X = array(seq(1, size * size * depth * minibatch), c(size, size, depth, minibatch)) dim(X) cnn.layers = deep.cnn.layers( X, list( type = &quot;convolv&quot;, size=3, filters=32, stride=1, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;convolv&quot;, size=3, filters=32, stride=2, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;convolv&quot;, size=3, filters=92, stride=1, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;convolv&quot;, size=3, filters=92, stride=2, padding=1, normalize=&quot;batch&quot;, afunc=&quot;leaky.relu&quot;), list( type = &quot;dense&quot;, list( size=1024, drop=0.95),list( size=10 )) ) The CNN architecture shows only four convolution layers and one dense layer with dropout. Instead of pooling, we adjust the stride to be 2 in the first and fourth convolution layers to reduce dimensionality. With that, the model gives us a plot of the cost and accuracy in Figure 12.48. Note that the line, in increasing direction (and in navy blue color), represents accuracy. Figure 12.48: Transferred Model (CNN) To generate prediction, we use the my.predict.NN(.) function below: get.test.batch &lt;- function(sample.data, labels, k, t) { set.seed(2019) batch.indices = createFolds(sample.data, k = k, returnTrain = FALSE) batches = list() for (i in 1:k) { indices = batch.indices[[i]] X = extract.image(images, indices) Y = extract.label.onehot(images, labels, indices) batches[[i]] = list( &quot;X&quot; = X, &quot;Y&quot; = Y ) } batches } my.predict.CNN &lt;- function(test.images, labels, model, minibatch=32, first_batch_only=FALSE) { options(digits = 16) # 16 digits precision for our example layers = model$layers img.len = length(images$rgb) # Test images population = test.images$rgb population.len = length(population) sample.set = population.len shuffled.data = sample.int(n=population.len, size=population.len, replace=FALSE) sample.size = ifelse(sample.set &lt; population.len, sample.set, population.len) sample.data = sample(shuffled.data, sample.size) k = ceiling(sample.size / minibatch) my.time = Sys.time() flush.str( &quot;No of batches: %d&quot;, k ) n = 0 total.accuracy = 0; total.loss = 0 for (batch in get.test.batch(sample.data, labels, k, t)) { n = n + 1 X = batch$X Y = batch$Y test.model = forward.pass.cnn(X, layers, train=FALSE) len = length(test.model$fc.model$layers) softmax.prob = test.model$fc.model$layers[[len]]$output loss = softmax.loss(Y, softmax.prob) test.accurate = accuracy(Y, softmax.prob) total.loss = c(total.loss, mean(loss)) total.accuracy = c(total.accuracy, test.accurate) if (n %% 10 == 0) { new.time = Sys.time() lag.time = difftime(new.time, my.time, units=&quot;secs&quot;) my.time = new.time stime = format(Sys.Date(), &quot;%c&quot;) flush.str( &quot;batch %d - loss: %2.3f t: %d, accuracy %2.3f lag time (sec): %5.3f&quot;, n, mean(loss), n, test.accurate, lag.time) if (first_batch_only == TRUE) { break } } } list(&quot;accuracy&quot; = mean(total.accuracy), &quot;loss&quot; = mean(total.loss), &quot;prediction&quot; = softmax.prob, &quot;target&quot; = Y) } Along with that, we use the test images from the cifar-10 dataset. test.images = list(&quot;label&quot; = list(), &quot;rgb&quot; = list()) fn = paste0(&quot;cifar-10-batches-bin/test_batch.bin&quot;) test.images = readBatch(fn, test.images, labels ) Now, we run prediction and get the result - let us show the result of the first batch only. result = my.predict.CNN(test.images, labels, transfer.cnn.model, first_batch_only = TRUE) ## [1] &quot;No of batches: 313&quot; ## [1] &quot;batch 10 - loss: 0.170 t: 10, accuracy 0.969 lag time (sec): 6.761&quot; That gives us a top-1 test accuracy of 87.78% and a loss of 0.122853209833548. Below, we see the actual predictions compared to the target: options(digits=8) round(result$prediction[1:5,],2)[1:5,] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0.00 0.00 0 0.00 0 1 0 0.00 0.00 ## [2,] 0 0.00 0.00 0 0.00 0 0 1 0.00 0.00 ## [3,] 0 0.85 0.05 0 0.01 0 0 0 0.00 0.09 ## [4,] 0 0.00 0.99 0 0.00 0 0 0 0.00 0.00 ## [5,] 0 0.02 0.00 0 0.00 0 0 0 0.98 0.00 result$target[1:5,] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 0 1 0 0 0 ## [2,] 0 0 0 0 0 0 0 1 0 0 ## [3,] 0 1 0 0 0 0 0 0 0 0 ## [4,] 0 0 1 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 0 0 0 1 0 Heuristically, we have observed that our implementation reaches a local minima somewhere at epoch 22 with about 96% train accuracy using a learning rate of 0.0001. If we decay the learning rate by 0.10 to 0.00001 at epoch 20, we get a little bit more increase in accuracy. For example, below, we can use a step decay like so: step.decay(0.0001, seq(1,30), decay.factor=0.10, step.size=20) ## [1] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 ## [9] 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 1e-04 ## [17] 1e-04 1e-04 1e-04 1e-05 1e-05 1e-05 1e-05 1e-05 ## [25] 1e-05 1e-05 1e-05 1e-05 1e-05 1e-05 Alternatively, as we have already trained our model, we can instead use the saved transfer.cnn.model to be re-trained via transfer learning and manually adjust the learning rate to 0.00001. system.time((retrained_model = my.CNN(images, labels, layers = cnn.layers, optimize=&quot;adam&quot;, minibatch=minibatch, epoch=10, eta=0.00001, transfer=&quot;transfer_model.rds&quot;))) After a few more epochs during the training, we again try to execute our prediction. Note that because training takes much longer, we are now using the saved model instead, namely retrained.cnn.model, as shown: retrained.cnn.model = readRDS(&quot;retrained_model.rds&quot;) result = my.predict.CNN(test.images, labels, retrained.cnn.model, first_batch_only=TRUE) ## [1] &quot;No of batches: 313&quot; ## [1] &quot;batch 10 - loss: 0.037 t: 10, accuracy 1.000 lag time (sec): 5.961&quot; That gives us a top-1 test accuracy of 89.49% and a loss of 0.0718042660598904. Below, we see the actual predictions compared to the target: options(digits=8) round(result$prediction[1:5,],2)[1:5,] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0.00 0 0.00 0.00 0 0.00 0.00 0.00 0 1 ## [2,] 0.00 0 0.99 0.00 0 0.00 0.01 0.00 0 0 ## [3,] 0.00 0 0.00 0.98 0 0.02 0.00 0.00 0 0 ## [4,] 0.99 0 0.00 0.00 0 0.00 0.00 0.00 0 0 ## [5,] 0.00 0 0.00 0.00 0 0.96 0.00 0.04 0 0 result$target[1:5,] ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10] ## [1,] 0 0 0 0 0 0 0 0 0 1 ## [2,] 0 0 1 0 0 0 0 0 0 0 ## [3,] 0 0 0 1 0 0 0 0 0 0 ## [4,] 1 0 0 0 0 0 0 0 0 0 ## [5,] 0 0 0 0 0 1 0 0 0 0 12.4.17 Summary Convolutional Neural Network is a topic that is wide and deep. There are more than a handful of knobs to cover. The interplay of these knobs is essential to get where we want to be in train and test accuracy. Each knob deserves a section for illustration and discussion. However, one section alone does not give justice to each. Our previous discussions perhaps only scratch the surface. Nevertheless, we leave readers to continue investigating novel tricks and techniques to improve CNN and to read about Data Augmentation and Adversarial Sampling. The former - Data Augmentation - allows us to use newly added datasets for further training based on existing datasets that are synthetically altered (e.g., image transformation). Given an insufficient dataset, the technique is helpful if we need to improve test accuracy. The latter - Adversarial Sampling - performs a similar technique as Augmentation; however, we need to be wary about intentionally tampered datasets that could mislead training. It is also worth mentioning that our CNN implementation is tailored based on a machine with minimal computing capability. However, advanced frameworks can perform distributed (parallel) processing across powerful machines with GPU support, achieving speeds down to seconds instead of hours. We leave readers to explore this next. "],["deeplearning2.html", "Chapter 13 Computational Deep Learning II 13.1 Residual Network (ResNet) 13.2 Recurrent Neural Network (RNN) 13.3 Deep Stacked RNN 13.4 Deep Stacked Bidirectional RNN 13.5 Transformer Neural Network (TNN) 13.6 Applications using TNN (and RNN) 13.7 Generative Adversarial Network (GAN) 13.8 Deep Reinforcement Network (DQN) 13.9 Summary", " Chapter 13 Computational Deep Learning II In this chapter, we continue to build our intuition around Neural Networks by introducing three other types of ANN, namely Residual Network (ResNet), Recurrent Neural Network (RNN), and Transformer Neural Network (TNN) (or simply Transformer). 13.1 Residual Network (ResNet) Recall in MLP and CNN that DropOut is one of the strategies we use to inject regularization into the network. The idea is to push the output of randomly selected neurons to zero, thus negating their effects on subsequent layers. Doing so also helps to generalize our model. In essence, DropOut by its description, is a type of a skip connection strategy. There is another skip connection strategy called residual connection as introduced in ResNet architecture. To illustrate, let us use Figure 13.1. Figure 13.1: Residual Network (ResNet) Unit There are two ways to visualize ResNet. In the figure, the left diagram can be perceived in such a way that a connection skips a network block (or a set of network blocks) and directly adds X to the output, f(X), of the network block(s). The correct diagram can be perceived such that a residual block is added to the network. The residual output, R(X), is added to X, and by doing so, we allow the network to learn the residual. A slightly complex architecture is shown in Figure 13.2. However, the architecture is still simpler than known architectures such as ResNet152. Nevertheless, there is no limit to our creativity as long as we design a neural network architecture that we see fits our purposes and expectations. Figure 13.2: Residual Network (ResNet) Architecture The placement of the skip connection and the residual block size varies depending on the design. For example, extending the connection right after the weight layer is possible but before the activation function. The size of the residual block is dictated by the number of layers we skip. We skip showing an implementation of ResNet in this section. On the other hand, let us discuss RNN next which shows a case of a skip connection, but more importantly, in addition to that, it also introduces the concept of gated connections, which regulate the flow of information apart from only considering the effect of residuals. 13.2 Recurrent Neural Network (RNN) We now switch context to another type of Neural Network that can handle sequential data. In the context of Recurrent Neural Network (RNN), our emphasis is on the recurrence of a sequence of data points. While CNN commonly intends to learn patterns from a static list of images to perform image classification, RNN intends to recognize and learn recurring patterns from a set of sequential data points to perform prediction for the next sequence of patterns. This ability to predict based on a recurring sequence of events showcases the power of RNN, which opens up even wider possible applications such as speech recognition, language translation, self-driving automobiles, and self-serve robots in restaurants. To build up our intuition on RNN, let us first understand the two diagrams shown in Figure 13.3. We illustrate an RNN box known as RNN Cell or RNN Unit. The cell takes X as input and produces \\(\\mathbf{\\hat{Y}}\\) as output. Notice a second output in the form of H comes out of the unit and gets fed back through it. This output is called Hidden State (also called Cell State), and it exists as a memory state preserving information. Figure 13.3: RNN (Unrolled Representation) Here, we note that the X input ideally requires a recurrent sequential pattern. A straightforward example of a recurring input is a sentence formed based on a specific language, e.g., the English language. Because a language follows a certain syntactic and semantic structure, it suggests some level of sequential pattern in that a sentence is made up of a sequence of words as input. Note here that we do not simply consider a bag of words with no order. The sequence affects the semantics or meaning of a sentence if otherwise unordered. For example, let us use one sentence derived from Og Mandino’s The Greatest Salesman in the world: \\[ \\text{Today, I begin a new life.} \\] Our simple goal is to feed the sample sentence into RNN in the form of individual words such that the word Today is fed first through RNN at time \\(\\mathbf{t_0}\\), then the word I is fed next at time \\(\\mathbf{t_1}\\) followed by the word begin at \\(\\mathbf{t_2}\\), and so on. The words are fed through the RNN cell recursively. If we unroll the RNN cell, we see the operations across time as depicted in the unrolled representation in Figure 13.3. The word Today is represented by \\(\\mathbf{X_0}\\) with the corresponding \\(\\mathbf{\\hat{Y}_0}\\) output and \\(\\mathbf{H_0}\\) cell state. The word I is represented by \\(\\mathbf{X_1}\\) with the corresponding \\(\\mathbf{\\hat{Y}_1}\\) output and \\(\\mathbf{H_1}\\) cell state. The word begin is represented by \\(\\mathbf{X_2}\\) with the corresponding \\(\\mathbf{\\hat{Y}_1}\\) output and \\(\\mathbf{H_2}\\) cell state. And so on. We can then stop feeding RNN after the word new and try to allow RNN to predict what is the last word. In our case, the last word is life. Note that we do not feed the exact form of the sentence into RNN. Instead, we take each word in the sentence and convert it into a unique numeric representation. We discuss this technique of casting words to numbers more in the Attention section. Now in RNN, we may encounter four general types of sequence models discussed. Figure 13.4 provides a diagram of each type (Karpathy A. 2015). Figure 13.4: RNN (Sequence Models) The RNN algorithm may differ depending on the sequence (and application) used based on Figure 13.4. For example, using the One to One sequence model for a binary classification of a single image may be appropriate. The One to Many sequence model is appropriate for a single image such as a cifar-10 image, as the case may be in our CNN example. The Many to One sequence model may apply in sentiment analysis in which we feed RNN with a sequence of input (words or images) and determine sentimentally if the sequence exudes a positive vibe. The Many to Many design makes it possible to perform language translation (Karpathy A. 2015). Alternatively (not included in the figure), a combined model of the Many to One and One to Many can be used as encoder and decoder, respectively, as commonly depicted in other literature. In the next few sections, let us introduce three RNN variances, namely Vanilla RNN, LSTM, and GRU. 13.2.1 Vanilla RNN Let us recall that a linear equation is made up of the following general example: \\[\\begin{align} \\hat{y} = f(x) = \\omega_0 + x_1 \\omega_1 + x_2 \\omega_2 +\\ ...\\ + x_p \\omega_p \\end{align}\\] where the omega (\\(\\omega\\)) symbol represents parameters or coefficients required for network learning. Additionally, considering Figure 12.3, we also use an activation function to achieve non-linearity. In this case, we use tanh. \\[\\begin{align} \\hat{y} = f_a(x) = \\text{tanh}(\\omega_0 + x_1 \\omega_1 + x_2 \\omega_2 +\\ ...\\ + x_p \\omega_p) \\end{align}\\] We can easily modify the diagram in Figure 12.3 to reflect an RNN cell with the existence of cell states. See Figure 13.5. Figure 13.5: Vanilla RNN Cell The figure shows that RNN uses two equations. The first equation is expressed as such: \\[\\begin{align} \\mathbf{H}_{nxh}^{(t)} = \\mathbf{f_a}\\left(\\mathbf{X}_{nxp}^{(t)}, \\mathbf{H}_{nxh}^{(t-1)}\\right) = \\mathbf{\\text{tanh}}\\left( \\mathbf{X}_{nxp}^{(t)} \\cdotp \\mathbf{W}_{pxh} + \\mathbf{H}_{nxh}^{(t-1)} \\cdotp \\mathbf{U}_{hxh} + \\mathbf{b}_{1xh}^{(h)} \\right) \\end{align}\\] The cell state (\\(\\mathbf{H^{(t)}}\\)) is preserved as a memory state to be used by the next iteration. Note here that the input \\(\\mathbf{X^{(t)}}\\) and previous cell state \\(\\mathbf{H^{(t-1)}}\\) are multiplied by their respective weights then summed together along with a bias (\\(\\mathbf{b}_{1xh}\\)); after which, the result is then fed through the tanh activation function. The second equation is expressed as such: \\[\\begin{align} \\mathbf{\\hat{Y}}_{nxo}^{(t)} = \\mathbf{f}\\left(\\mathbf{H}_{nxh}^{(t)}\\right) = \\mathbf{\\text{softmax}}\\left(\\mathbf{H}_{nxh}^{(t)} \\times \\mathbf{V}_{hxo} + \\mathbf{b}_{1xo}^{(y)}\\right) \\end{align}\\] This yields the \\(\\mathbf{\\hat{Y}_{nxo}^{(t)}}\\) output produced by a softmax function, in this case, performing classification. In terms of dimensions, Figure 13.6 illustrates how the dot-product and summation operations handle the dimension of the input, output, cell state, and parameters. Figure 13.6: Vanilla RNN Dimensions Given the following activation functions, the structure in Figure 13.5 has the following implementation: rnn.tanh &lt;- function(x) { (exp(x) - exp(-x)) / ( exp(x) + exp(-x)) } rnn.sigmoid &lt;- function(x) { 1 / ( 1 + exp(-x)) } rnn.softmax &lt;- function(x) { p = apply(x, 1, max); x = x - p; p = exp(x) s = apply(p, 1, sum); sweep(p, 1, s, &quot;/&quot;) } Now suppose we have an \\(\\mathbf{n \\times p}\\) input (X) (e.g., it has n samples and p features) with its corresponding \\(\\mathbf{p \\times h}\\) weight (W), a hidden state (H) with \\(\\mathbf{n \\times h}\\) dimension (e.g., h neurons) and its corresponding \\(\\mathbf{h \\times h}\\) ** weight** (U), we need to calculate the \\(\\mathbf{n \\times o}\\) output (\\(\\mathbf{\\hat{Y}}\\)). Below is a sample use of the functions and structure: n = 5 # number of samples p = 30 # number of features per sample (could also mean number of # probabilities of a word embedding) h = 20 # number of neurons in a hidden state o = 3 # number of output neurons in an output layer X = matrix(runif(n * p), nrow=n, ncol=p, byrow=TRUE) H = matrix(runif(n * h), nrow=n, ncol=h, byrow=TRUE) W = matrix(rnorm(p * h), nrow=p, ncol=h, byrow=TRUE) U = matrix(rnorm(h * h), nrow=h, ncol=h, byrow=TRUE) V = matrix(rnorm(h * o), nrow=h, ncol=o, byrow=TRUE) bh = matrix(rnorm(1 * h), nrow=1, ncol=h, byrow=TRUE) by = matrix(rnorm(1 * o), nrow=1, ncol=o, byrow=TRUE) H = rnn.tanh(sweep(X %*% W + H %*% U, 2, bh, &#39;+&#39;)) Y = matrix(rnorm(n * o), nrow=n, ncol=o, byrow=TRUE) Below is the result showing the dimensions of H and Y: str(list(&quot;H&quot; = H, &quot;Y&quot; = Y)) # put in a list for better display ## List of 2 ## $ H: num [1:5, 1:20] 0.968 0.992 -0.88 0.999 0.975 ... ## $ Y: num [1:5, 1:3] 0.13742 2.11004 -0.00548 -0.75795 -0.89994 ... Similar to MLP and CNN, it should be noted that RNN also considers the use of forward feed and backpropagation. Forward Feed Here, we follow a straightforward implementation of RNN forward feed based on the immediate equations above. forward.unit.RNN &lt;- function(X, H, params) { W = params$W U = params$U V = params$V bh = params$bh by = params$by Ht = rnn.tanh(sweep(X %*% W + H %*% U, 2, bh, &#39;+&#39;)) Y.hat = rnn.softmax(sweep(H %*% V, 2, by, &#39;+&#39;)) list(&quot;Ht&quot; = Ht, &quot;Y.hat&quot; = Y.hat) } params = list(&quot;W&quot; = W, &quot;U&quot; = U, &quot;V&quot; = V, &quot;bh&quot; = bh, &quot;by&quot; = by) model = forward.unit.RNN(X, H, params) Below is the result showing the dimension of H and \\(\\mathbf{\\hat{Y}}\\): str(model) ## List of 2 ## $ Ht : num [1:5, 1:20] 1 1 0.845 0.998 0.901 ... ## $ Y.hat: num [1:5, 1:3] 0.895 0.565 0.939 1 0.998 ... Backpropagation We start our discussion of RNN backpropagation by using Figure 13.7. Figure 13.7: RNN (Softmax) Similar to CNN, our implementation of RNN uses Cross-Entropy Loss for Softmax function (which is also the case for LSTM and GRU in next sections ahead). If we recall Delta Rule in MLP section, we start by obtaining Delta o (\\(\\mathbf{\\delta o}\\)) which is the gradient of our loss with respect to the activation function (in this is case, we use softmax). Here, it helps to recall the derivation of the equation under activation function subsection in MLP section. Note that in MLP we use the hat-o (\\(\\hat{\\mathbf{o}}\\)) symbol for the linear function of our output and no-hat (\\(\\mathbf{o}\\)) symbol for our non-linear function. See below: \\[\\begin{align} \\delta_o = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\hat{o}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial o}\\right) \\left(\\frac{\\partial o}{\\partial \\hat{o}}\\right) = (o - t) \\end{align}\\] This should not be confused now about our use of y-hat (\\(\\hat{\\mathbf{y}}\\)) symbol for our softmax and y for our target (t) in RNN; whereas, the h-hat (\\(\\hat{\\mathbf{h}}\\)) symbol represents output of our linear function. Therefore, in our case for RNN, we have the following equation: \\[\\begin{align} \\delta_{y} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\hat{h}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\hat{y}}\\right) \\left(\\frac{\\partial \\hat{y}}{\\partial \\hat{h}}\\right) = (\\hat{y} - y) \\end{align}\\] It should then be straightforward to derive the gradient of \\(\\mathbf{H}_{nxh}^{(t)}\\), \\(\\mathbf{V}_{hxo}\\), and \\(\\mathbf{b}_{1xh}^{(y)}\\) (in the order shown below): \\[\\begin{align} \\delta \\mathbf{H}_{nxh}^{(t)} &amp;= \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{H}_{nxh}^{(t)}} + \\delta \\mathbf{H}_{nxh}^{(t+1)} = \\delta y \\left(\\frac{\\partial \\hat{h}} {\\partial \\mathbf{H}_{nxh}^{(t)}} \\right) + \\delta \\mathbf{H}_{nxh}^{(t+1)} = \\delta y \\cdot \\left(\\mathbf{V}_{hxo}\\right)^{\\text{T}} + \\delta \\mathbf{H}_{nxh}^{(t+1)} \\\\ \\nabla \\mathbf{V_{hxo}} &amp;= \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{V}_{hxo}} = \\left(\\frac{\\partial \\hat{h}} {\\partial \\mathbf{V_{hxo}} } \\right) \\delta y = \\left(\\mathbf{H}_{nxh}^{(t)}\\right)^{\\text{T}} \\cdot \\delta y \\\\ \\nabla \\mathbf{b}_{1xo}^{(y)} &amp;= \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{b}_{1xo}^{(y)} } = \\sum_{column-wise}{\\delta y} \\end{align}\\] Also, notice here the addition of the second term, namely \\(\\left(\\delta \\mathbf{H}_{nxh}^{(t+1)}\\right)\\), which we also refer to as dH.next in our implementation. Because there is no time step t+1 at the start of backpropagation, dH.next is initially zero. Next, we then solve for gradient of our activation function (the tangent) with respect to \\(\\mathbf{H}_{nxh}^{(t)}\\). Here, we know that the first derivative of tangent is written as: \\[\\begin{align} \\mathbf{\\text{tanh}}&#39;(a) = \\left(1 - \\mathbf{tanh}^2(a)\\right) \\end{align}\\] We use the derivative to construct our Delta tanh like so: \\[\\begin{align} \\delta\\ \\mathbf{\\text{tanh}} = ( 1 - \\mathbf{\\text{tanh}}^2(a)) \\odot \\delta a = ( 1 - \\mathbf{\\text{tanh}}^2(\\mathbf{H}_{nxh}^{(t)})) \\odot \\delta \\mathbf{H}_{nxh}^{(t)} \\end{align}\\] where \\(a = \\mathbf{H}_{nxh}^{(t)}\\). From there, we can derive the gradient of our loss with respect to \\(\\mathbf{W}_{pxh}\\), \\(\\mathbf{U}_{hxh}\\), and \\(\\mathbf{b}_{1xh}^{(y)}\\). \\[\\begin{align} \\nabla \\mathbf{W}_{pxh} = \\left(\\mathbf{X}_{nxp}^{(t-1)}\\right)^\\text{T} \\cdotp \\delta \\mathbf{\\text{tanh}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla \\mathbf{U}_{hxh} = \\left( \\mathbf{H}_{nxh}^{(t-1)}\\right)^\\text{T} \\cdotp \\delta \\mathbf{\\text{tanh}} \\end{align}\\] \\[\\begin{align} \\nabla \\mathbf{b}_{1xh}^{(y)} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{b}_{1xh}^{(h)} } = \\delta\\mathbf{\\text{tanh}} \\end{align}\\] For the gradient of the loss with respect to previous X and H, we perform the following equations: \\[\\begin{align} \\underbrace{\\delta \\mathbf{H}_{nxh}^{(t-1)}}_{ \\begin{array}{ll} \\mathbf{\\text{dH.prev becomes}}\\\\ \\mathbf{\\text{new dH.next later}} \\end{array} } = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{H}_{nxh}^{(t-1)}} = \\delta\\mathbf{\\text{tanh}} \\cdotp \\left(\\mathbf{U}_{hxh}\\right)^\\text{T} \\label{eqn:eqnnumber801}\\\\ \\delta \\mathbf{X}_{nxh}^{(t)} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{X}_{nxh}^{(t)}} = \\delta\\mathbf{\\text{tanh}} \\cdotp \\left(\\mathbf{W}_{pxh}\\right)^\\text{T} \\label{eqn:eqnnumber802} \\end{align}\\] Below is our example implementation of RNN backpropagation: backward.unit.RNN &lt;- function(dH.next, X, Y, model, params) { W = params$W U = params$U V = params$V bh = params$bh by = params$by Y.hat = model$Y.hat dy = (Y.hat - Y) dH = dy %*% t(V) + dH.next # dealing with one-hot encoding dV = t(H) %*% dy dby = apply(dy, 2, sum) dtanh = (1 - model$Ht^2) * dH # Gradient of Tanh wrt H dX = dtanh %*% t(W) # delta X for BP through time dH = dtanh %*% t(U) # delta H for BP throught time dW = t(X) %*% dtanh # Gradient of Loss wrt W dU = t(H) %*% dtanh # Gradient of Loss wrt U dbh = apply(dtanh, 2, sum) # Gradient of Loss wrt Bias list(&quot;dX&quot; = dX, &quot;dH&quot; = dH, &quot;dW&quot; = dW, &quot;dU&quot; = dU, &quot;dbh&quot; = dbh, &quot;dV&quot; = dV, &quot;dby&quot; = dby) } params = list(&quot;W&quot; = W, &quot;U&quot; = U, &quot;V&quot; = V) dH.next = matrix(runif(n * h), nrow=n, ncol=h, byrow=TRUE) gradients = backward.unit.RNN(dH.next, X, Y, model, params) str(gradients) ## List of 7 ## $ dX : num [1:5, 1:30] 2.4484 -0.0373 1.2621 4.4449 2.2843 ... ## $ dH : num [1:5, 1:20] -0.795 7.254 -0.411 -3.118 0.463 ... ## $ dW : num [1:30, 1:20] 0.271 0.413 0.219 0.198 0.146 ... ## $ dU : num [1:20, 1:20] -0.339 0.413 0.439 -0.442 0.43 ... ## $ dbh: num [1:20] 0.442 1.185 0.499 1.524 0.403 ... ## $ dV : num [1:20, 1:3] 1.9744 -1.4219 0.0203 -3.8125 3.7388 ... ## $ dby: num [1:3] 3.812 0.142 5.587 Training our RNN iterates between the forward.unit.RNN(.) function and backward.unit.RNN(.) function, along with the usual cost computation and an optimized update of the learnable parameters similar to CNN. See our generic RNN algorithm next. RNN Algorithm It should be noted that the recurrent nature of RNN preserves the previous state at each time step. If we view the unrolled representation of RNN, we should be able to trace back the gradients through time. Below is a generic algorithm for RNN showing the use of forward feed and backpropagation through time. The algorithm is based on the Many to Many sequence model. \\[ \\begin{array}{ll} \\text{W, U, V, bh, by}\\ \\leftarrow \\text{(params)}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ (\\text{randomly generated}) \\\\ \\text{H}_{nxh}, \\text{cost}_{1xepoch}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(initialize to zero)}\\\\ \\text{dH.next} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{(initialize to zero)}\\\\ \\text{dW, dU, dbh, dby}\\ \\leftarrow \\text{(gradients)}\\ \\ \\ \\ \\ \\ \\text{(initialize to zero)}\\\\ \\mathbf{\\text{for}}\\ iter\\ \\text{ in } 1...epoch\\ \\mathbf{\\text{ loop}}\\\\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{for}}\\ t\\ \\text{ in } 1...T\\ \\mathbf{\\text{ loop}}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{model} = \\mathbf{\\text{forward.unit}}(X_{nxp}^{(t)}, H_{nxh}^{(t)}, \\text{params})\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ H_{nxh}^{(t)}, \\hat{Y}_{nxo}^{(t)} = \\text{model}\\\\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{end loop}}\\\\ \\ \\ \\ \\ \\ \\ \\text{cost[iter] } = \\text{mean}\\left(\\mathbf{\\text{ cost.estimate}}(\\hat{Y}_{nxo}, Y_{nxo})\\right) \\ \\ \\ \\ \\ \\ \\ \\ (\\text{cross-entropy cost})\\\\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{for}}\\ t\\ \\text{ in } T...1\\ \\mathbf{\\text{ loop}}\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{gradients} = \\mathbf{\\text{backward.unit}} (\\text{dH.next}, X_{nxh}^{(t)}, \\text{model}, \\text{params})\\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{params[&#39;W&#39;] = params[&#39;W&#39;] + gradients[&#39;dW&#39;]} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{params[&#39;U&#39;] = params[&#39;U&#39;] + gradients[&#39;dU&#39;]} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{params[&#39;bh&#39;] = params[&#39;bh&#39;] + gradients[&#39;dbh&#39;]} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{dH.next = dH} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{params} = \\mathbf{\\text{optimize.update}}(\\text{params, gradients})\\\\ \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{end loop}}\\\\ \\mathbf{\\text{end loop}}\\\\ \\mathbf{\\text{output : }}\\text{ cost, params}\\ \\ \\leftarrow \\text{(trained model)} \\end{array} \\] where cross-entropy cost is calculated as \\(\\left(-\\sum\\left(\\log_e\\left(Y_n^{(t)} \\odot \\hat{Y}_n^{(t)}\\right)\\right)\\right)\\). A common problem with Vanilla RNN is the vanishing or exploding gradient. In such a case, the more critical concern is that RNN, by its very nature, attempts to preserve (in memory) the state of the previous time step in that such state information tends to vanish in time so that the early memory tends to be forgotten as we keep progressing in future time steps. This concern is mitigated using LSTM. 13.2.2 Long Short-Term Memory (LSTM) LSTM is a variant of RNN introduced by Sepp Hochreiter and Jürgen Schmidhuber in (1997). LSTM is designed as a gated RNN with a forget gate, update gate, and output gate. See the design in Figure 13.8. Figure 13.8: LSTM Cell The intuition behind the gates becomes apparent after reviewing the forward feed. LSTM Forward Feed Based on Figure 13.8, the LSTM design uses the below equations where \\(\\odot\\) is an element-wise (Hadamard) multiplication: \\[\\begin{align} [\\mathbf{X,H}]_{nx[p,h]}^{(t)} = \\left[ \\mathbf{X}_{nxp}^{(t)}, \\mathbf{H}_{nxh}^{(t-1)}\\right] &amp; \\\\ A_f^{(t)} = [\\mathbf{X,H}]_{nx[p,h]}^{(t)} \\cdot \\mathbf{W}_{[p,h]xh}^{(f)} + \\mathbf{b}_{1xh}^{(f)} &amp;\\ \\ \\ \\ \\ \\mathbf{F}_{nxh}^{(t)} = \\sigma \\left(A_f^{(t)}\\right) \\\\ A_i^{(t)} = [\\mathbf{X,H}]_{nx[p,h]}^{(t)} \\cdot \\mathbf{W}_{[p,h]xh}^{(i)} + \\mathbf{b}_{1xh}^{(i)} &amp;\\ \\ \\ \\ \\ \\mathbf{I}_{nxh}^{(t)} = \\sigma \\left(A_i^{(t)} \\right) \\\\ A_g^{(t)} = [\\mathbf{X,H}]_{nx[p,h]}^{(t)} \\cdot \\mathbf{W}_{[p,h]xh}^{(g)} + \\mathbf{b}_{1xh}^{(g)} &amp;\\ \\ \\ \\ \\ \\mathbf{G}_{nxh}^{(t)} = \\mathbf{\\text{tanh}} \\left(A_g^{(t)}\\right) \\\\ A_o^{(t)} = [\\mathbf{X,H}]_{nx[p,h]}^{(t)} \\cdot \\mathbf{W}_{[p,h]xh}^{(o)} + \\mathbf{b}_{1xh}^{(o)} &amp;\\ \\ \\ \\ \\ \\mathbf{O}_{nxh}^{(t)} = \\sigma \\left(A_o^{(t)}\\right) \\\\ &amp;\\ \\ \\ \\ \\ \\mathbf{C}_{nxh}^{(t)} = \\mathbf{F}_{nxh}^{(t)} \\odot \\mathbf{C}_{nxh}^{(t-1)} + \\mathbf{I}_{nxh}^{(t)} \\odot \\mathbf{G}_{nxh}^{(t)} \\\\ &amp;\\ \\ \\ \\ \\ \\mathbf{H}_{nxh}^{(t)} = \\mathbf{\\text{tanh}}\\left( \\mathbf{C}_{nxh}^{(t)}\\right) \\odot \\mathbf{O}_{nxh}^{(t)}\\\\ &amp;\\ \\ \\ \\ \\ \\mathbf{\\hat{Y}}_{nxo}^{(t)} = \\mathbf{\\text{softmax}}\\left(\\mathbf{H}_{nxh}^{(t)} \\cdotp \\mathbf{V}_{hxo}+ \\mathbf{b}_{1xo}^{(y)}\\right) \\end{align}\\] Note that the equation for vanilla RNN has the following: \\[\\begin{align} \\mathbf{X}_{nxp}^{(t)} \\cdotp \\mathbf{W}_{pxh} + \\mathbf{H}_{nxh}^{(t-1)} \\cdotp \\mathbf{U}_{hxh} \\end{align}\\] In LSTM, we concatenate X and H instead which is denoted by \\(\\mathbf{\\text{[X, H]}}\\). Then multiplied by an already concatenated weight using the following notation: \\[\\begin{align} [\\mathbf{\\text{X}}_{nxp}^{(t)}, \\mathbf{\\text{H}}_{nxh}^{(t-1)}] \\cdot W_{[p,h]xh} = [\\mathbf{X,H}]_{nx[p,h]}^{(t)} \\cdot \\mathbf{W}_{[p,h]xh}^{(f)} \\end{align}\\] Now, because the forget gate, denoted by \\(\\mathbf{F}_{nxh}^{(t)}\\), gets multiplied by \\(\\mathbf{C}_{nxh}^{(t-1)}\\), mathematically when it comes to the gates, any value of \\(\\mathbf{F}_{nxh}^{(t)}\\), therefore, affects the previous time step. For example, if the value gets closer to zero, then the more limited information (in the form of \\(\\mathbf{C}_{nxh}^{(t-1)}\\)) gets allowed to the next time step. On the other hand, the candidate state, denoted by \\(\\mathbf{G}_{nxh}^{(t)}\\), is negated if the value of the input gate, denoted by \\(\\mathbf{I}_{nxh}^{(t)}\\), becomes zero. In other words, gates in LSTM regulate the flow of information such that a complementary intuition here is to avoid long-term memory, which, in effect, mitigates the vanishing and exploding gradient condition. Here, we have an example implementation of LSTM forward feed based on the above equations: forward.unit.LSTM &lt;- function(X, H, C, params) { Wi = params$Wi$weight; Wf = params$Wf$weight Wg = params$Wg$weight; Wo = params$Wo$weight bf = params$bf$weight; bi = params$bi$weight bg = params$bg$weight; bo = params$bo$weight XH = cbind(X,H) # concatenate Ft = rnn.sigmoid(sweep(XH %*% Wf, 2, bf, &#39;+&#39;)) It = rnn.sigmoid(sweep(XH %*% Wi, 2, bi, &#39;+&#39;)) Gt = rnn.tanh (sweep(XH %*% Wg, 2, bg, &#39;+&#39;)) Ot = rnn.sigmoid(sweep(XH %*% Wo, 2, bo, &#39;+&#39;)) Ct = Ft * C + It * Gt Ht = rnn.tanh(Ct) * Ot list( &quot;Ht&quot; = Ht, &quot;Ct&quot; = Ct, &quot;Ft&quot; = Ft, &quot;It&quot; = It, &quot;Gt&quot; = Gt, &quot;Ot&quot; = Ot ) } LSTM Backpropagation Derived from the cross-entropy loss for softmax to start the backpropagation, we obtain the Delta y (\\(\\delta y\\)) similar to the Vanilla RNN. We also derive the Delta H denoted by \\(\\left(\\delta \\mathbf{H}_{nxh}^{(t)}\\right)\\) which we also reference as dH.next in our implementation. See Figure 13.9 Figure 13.9: RNN (Softmax) We know that the gradient of our loss with respect to \\(\\mathbf{H}_{nxh}^{(t)}\\) is the Delta H denoted by \\(\\left(\\delta \\mathbf{H}_{nxh}^{(t)}\\right)\\). Recalling the example derivation from Vanilla RNN backpropagation, we also add dH.next which is initially zero. \\[\\begin{align} \\delta \\mathbf{H}_{nxh}^{(t)} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{H}_{nxh}^{(t)}} + \\delta \\mathbf{H}_{nxh}^{(t+1)} = \\delta y \\cdot \\left(\\mathbf{V}_{hxo}\\right)^{\\text{T}} + \\underbrace{\\delta \\mathbf{H}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dH.next}}} \\end{align}\\] Our gradients fo the weight V \\(\\left(\\nabla \\mathbf{V_{hxo}}\\right)\\) and bias \\(\\left(\\nabla \\mathbf{b}_{1xh}^{(y)}\\right)\\) follow similar derivation from Vanilla RNN. Now, to calculate \\(\\left(\\delta \\mathbf{C}_{nxh}^{(t)}\\right)\\), let us first obtain the gradient of \\(\\mathbf{H}_{nxh}^{(t)}\\) with respect to \\(\\mathbf{C}_{nxh}^{(t)}\\) using Figure 13.9 as reference: \\[\\begin{align} \\left(\\frac{\\partial \\mathbf{H}_{nxh}^{(t)}} {\\partial \\mathbf{C}_{nxh}^{(t)}}\\right) = \\left(1 - \\mathbf{tanh}^2(\\mathbf{C}_{nxh}^{(t)})\\right) \\odot \\mathbf{O}_{nxh}^{(t)} \\end{align}\\] Therefore, we obtain the following formulation: \\[\\begin{align} \\delta \\mathbf{C}_{nxh}^{(t)} &amp;= \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{C}_{nxh}^{(t)}} = \\left(\\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{H}_{nxh}^{(t)}} \\right) \\left(\\frac{\\partial \\mathbf{H}_{nxh}^{(t)}} {\\partial \\mathbf{C}_{nxh}^{(t)}} \\right) + \\delta \\mathbf{C}_{nxh}^{(t+1)}\\\\ &amp;= \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{tanh}^2(\\mathbf{C}_{nxh}^{(t)})\\right) \\odot \\mathbf{O}_{nxh}^{(t)} + \\underbrace{\\delta \\mathbf{C}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dC.next}}} \\end{align}\\] That becomes our initial Delta C or dC.Next which is used along with the initial dH.next to calculate other gradients for our LSTM. We follow this with the calculation of gradients for the gates: Gradient with respect to \\(\\left(\\mathbf{F}_{nxh}^{(t)}\\right)\\) in the forget gate. \\[\\begin{align} \\delta f = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{F}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{F}_{nxh}^{(t)}} = \\delta \\mathbf{C}_{nxh}^{(t)} \\odot\\mathbf{C}_{nxh}^{(t-1)} \\end{align}\\] The formulation is also equivalent to: \\[\\begin{align} \\delta f = \\left( \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{tanh}^2(\\mathbf{C}_{nxh}^{(t)})\\right) \\odot \\mathbf{O}_{nxh}^{(t)} + \\underbrace{\\delta \\mathbf{C}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dC.next}}}\\right) \\odot\\mathbf{C}_{nxh}^{(t-1)} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{I}_{nxh}^{(t)}\\right)\\) in the input gate. \\[\\begin{align} \\delta i = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{I}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{I}_{nxh}^{(t)}} = \\delta \\mathbf{C}_{nxh}^{(t)} \\odot\\mathbf{G}_{nxh}^{(t)} \\end{align}\\] The formulation is also equivalent to: \\[\\begin{align} \\delta i = \\left( \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{tanh}^2(\\mathbf{C}_{nxh}^{(t)})\\right) \\odot \\mathbf{O}_{nxh}^{(t)} + \\underbrace{\\delta \\mathbf{C}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dC.next}}}\\right) \\odot\\mathbf{G}_{nxh}^{(t)} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{G}_{nxh}^{(t)}\\right)\\) for the candidate state. \\[\\begin{align} \\delta g = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{I}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} = \\delta \\mathbf{C}_{nxh}^{(t)} \\odot\\mathbf{I}_{nxh}^{(t)} \\end{align}\\] The formulation is also equivalent to: \\[\\begin{align} \\delta g = \\left( \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{tanh}^2(\\mathbf{C}_{nxh}^{(t)})\\right) \\odot \\mathbf{O}_{nxh}^{(t)} + \\underbrace{\\delta \\mathbf{C}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dC.next}}}\\right) \\odot\\mathbf{I}_{nxh}^{(t)} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{O}_{nxh}^{(t)}\\right)\\) in the output gate. \\[\\begin{align} \\delta o = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{O}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{O}_{nxh}^{(t)}} = \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\mathbf{tanh}\\left(\\mathbf{C}_{nxh}^{(t)}\\right) \\end{align}\\] Next, we calculate the gradients with respect to the linear functions. Now, in Vanilla RNN, we require the first derivative of the tangent function. Here, we also include the first derivative of the sigmoid function, which is required in calculating gradients for the three gates. \\[\\begin{align} \\mathbf{\\text{tanh}}&#39;(a) = \\left(1 - \\mathbf{tanh}^2(a)\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma&#39;(a) = \\sigma(a)\\left(1 - \\sigma(a)\\right) \\end{align}\\] Gradient with respect to \\(\\left(\\hat{\\mathbf{F}}_{nxh}^{(t)}\\right)\\) in the forget gate. \\[\\begin{align} \\delta \\hat{f} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{\\hat{F}}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{F}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{F}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{F}}_{nxh}^{(t)}} =\\delta f \\odot \\mathbf{F}_{nxh}^{(t)} \\odot ( 1 - \\mathbf{F}_{nxh}^{(t)}) \\end{align}\\] Note that \\(\\delta \\hat{f}\\) can also be represented as: \\[\\begin{align} \\delta \\hat{f} = \\delta f \\odot \\sigma(A_f) \\odot ( 1 - \\sigma(A_f)) \\end{align}\\] Gradient with respect to \\(\\left(\\hat{\\mathbf{I}}_{nxh}^{(t)}\\right)\\) in the input gate. \\[\\begin{align} \\delta \\hat{i} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{\\hat{I}}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{I}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{I}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{I}}_{nxh}^{(t)}} =\\delta i \\odot \\mathbf{I}_{nxh}^{(t)} \\odot ( 1 - \\mathbf{I}_{nxh}^{(t)}) \\end{align}\\] Note that \\(\\delta \\hat{i}\\) can also be represented as: \\[\\begin{align} \\delta \\hat{i} = \\delta i \\odot \\sigma(A_i) \\odot ( 1 - \\sigma(A_i)) \\end{align}\\] Gradient with respect to \\(\\left(\\hat{\\mathbf{G}}_{nxh}^{(t)}\\right)\\) for the candidate state. \\[\\begin{align} \\delta \\hat{g} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{\\hat{G}}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{C}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{C}_{nxh}^{(t)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{G}}_{nxh}^{(t)}} =\\delta g \\odot \\left( 1 - \\left({\\mathbf{G}_{nxh}^{(t)}}\\right)^2\\right) \\end{align}\\] Note that \\(\\delta \\hat{g}\\) can also be represented as: \\[\\begin{align} \\delta \\hat{g} = \\delta g \\odot ( 1 - \\tanh^2(A_g)) \\end{align}\\] Gradient with respect to \\(\\left(\\hat{\\mathbf{O}}_{nxh}^{(t)}\\right)\\) in the output gate. \\[\\begin{align} \\delta \\hat{o} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{\\hat{O}}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{O}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{O}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{O}}_{nxh}^{(t)}} =\\delta o \\odot \\mathbf{O}_{nxh}^{(t)} \\odot ( 1 - \\mathbf{O}_{nxh}^{(t)}) \\end{align}\\] Note that \\(\\delta \\hat{o}\\) can also be represented as: \\[\\begin{align} \\delta \\hat{o} = \\delta o \\odot \\sigma(A_o) \\odot ( 1 - \\sigma(A_o)) \\end{align}\\] Next, we calculate the gradients with respect to weights and biases. Note that, because we concatenated X and H, we can choose to calculate the gradient for each of their weights. In that case, we only use the \\(\\mathbf{p \\times h}\\) (dimension-wise) portion of the concatenated \\(\\mathbf{W_{[p,h]xh}}\\) for the weights for X and \\(\\mathbf{h \\times h}\\) portion for the weights for H. For example: \\[\\begin{align} \\underbrace{\\mathbf{W_{pxh}}}_{\\text{X portion}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\mathbf{W_{hxh}}}_{\\text{H portion}}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\mathbf{W_{[p,h]xh}}}_{\\text{concatenated weights for X and H}} \\end{align}\\] But only for notation convenience, let us use the concatenation format instead: Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(f)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(f)}\\right)\\) in the forget gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(f)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(f)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{f} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(f)} = \\sum_{column-wise}{\\delta \\hat{f}} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(i)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(i)}\\right)\\) in the input gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(i)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(i)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{i} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(i)} = \\sum_{column-wise}{ \\delta \\hat{i}} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(g)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(g)}\\right)\\) for the candidate state. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(g)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(g)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{g} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(g)} = \\sum_{column-wise}{\\delta \\hat{g}} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(o)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(o)}\\right)\\) in the output gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(o)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(o)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{o} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(o)} = \\sum_{column-wise}{\\delta \\hat{o}} \\end{align}\\] Lastly, we calculate gradients with respect to \\(\\left(\\mathbf{C}_{nxh}^{(t-1)}\\right)\\), \\(\\left(\\mathbf{H}_{nxh}^{(t-1)}\\right)\\), and \\(\\left(\\mathbf{X}_{nxh}^{(t)}\\right)\\). \\[\\begin{align} \\nabla \\mathbf{X}_{nxp}^{(t)} &amp;= \\delta f \\cdot \\left(\\mathbf{W_{pxh}^{(f)}}\\right)^{\\text{T}} + \\delta i \\cdot \\left(\\mathbf{W_{pxh}^{(i)}}\\right)^{\\text{T}} + \\delta g \\cdot \\left(\\mathbf{W_{pxh}^{(g)}}\\right)^{\\text{T}} + \\delta o \\cdot \\left(\\mathbf{W_{pxh}^{(o)}}\\right)^{\\text{T}} \\\\ \\underbrace{\\nabla \\mathbf{H}_{nxh}^{(t-1)}}_{\\mathbf{\\text{new dH.next}}} &amp;= \\delta f \\cdot \\left(\\mathbf{W_{hxh}^{(f)}}\\right)^{\\text{T}} + \\delta i \\cdot \\left(\\mathbf{W_{hxh}^{(i)}}\\right)^{\\text{T}} + \\delta g \\cdot \\left(\\mathbf{W_{hxh}^{(g)}}\\right)^{\\text{T}} + \\delta o \\cdot \\left(\\mathbf{W_{hxh}^{(o)}}\\right)^{\\text{T}} \\\\ \\underbrace{\\nabla \\mathbf{C}_{nxh}^{(t-1)}}_{\\mathbf{\\text{new dC.next}}} &amp;= \\delta \\mathbf{C}_{nxh}^{(t)} \\odot F_{nxh}^{(t)} \\end{align}\\] Both \\(\\left(\\nabla \\mathbf{H}_{nxh}^{(t-1)}\\right)\\) and \\(\\left(\\nabla \\mathbf{C}_{nxh}^{(t-1)}\\right)\\) are calculated to be the gradients of previous time step; but they become the new dH.next and new dC.next respectively which we use to propagate back to the next previous time step. \\[\\begin{align} \\text{dH.next} = \\delta \\mathbf{H}_{nxh}^{(t-1)} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\text{dC.next} = \\delta \\mathbf{C}_{nxh}^{(t-1)} \\end{align}\\] Let us now review our example implementation of LSTM backpropagation based on the gradient formulations above (and for additional consistency, we reference an LSTM Python code from Fisseha Berhane (n.d.)): backward.unit.LSTM &lt;- function(dH.next, dC.next, X, Y, H, C, model, params, grad) { p = ncol(X) h = ncol(H) Ft = model$Ft; It = model$It Gt = model$Gt; Ot = model$Ot Ct = model$Ct Ht = model$Ht Wi = params$Wi$weight; Wf = params$Wf$weight Wg = params$Wg$weight; Wo = params$Wo$weight bf = params$bf$weight; bi = params$bi$weight bg = params$bg$weight; bo = params$bo$weight dC = (Ot * (1 - rnn.tanh(Ct)^2) * dH.next) + dC.next dFt = (dC * C) * Ft * (1 - Ft) dIt = (dC * Gt) * It * (1 - It) dGt = (dC * It) * (1 - Gt^2) dOt = dH.next * rnn.tanh(Ct) * Ot * ( 1 - Ot) XH = cbind(X, H) # concatenate # Calculate gradient wrt to shared Weights and Biases dWft = t(XH) %*% dFt; grad$dF = grad$dF + dWft dWit = t(XH) %*% dIt; grad$dI = grad$dI + dWit dWgt = t(XH) %*% dGt; grad$dG = grad$dG + dWgt dWot = t(XH) %*% dOt; grad$dO = grad$dO + dWot dbf = apply(dFt, 2, sum); grad$dbf = grad$dbf + dbf dbi = apply(dIt, 2, sum); grad$dbi = grad$dbi + dbi dbg = apply(dGt, 2, sum); grad$dbg = grad$dbg + dbg dbo = apply(dOt, 2, sum); grad$dbo = grad$dbo + dbo dX = dFt %*% t(Wf[1:p,]) + dIt %*% t(Wi[1:p,]) + dGt %*% t(Wg[1:p,]) + dOt %*% t(Wo[1:p,]) dH = dFt %*% t(Wf[(p+1):(h+p),]) + dIt %*% t(Wi[(p+1):(h+p),]) + dGt %*% t(Wg[(p+1):(h+p),]) + dOt %*% t(Wo[(p+1):(h+p),]) dC = dC * Ft grad$dX = dX grad$dH = dH grad$dC = dC grad } For a simple use, let us concoct a simple dataset and initialize our parameters: set.seed(1) n = 5 # number of samples p = 30 # number of features per sample (could also mean number of # probabilities of a word embedding) h = 20 # number of neurons in a hidden state o = 3 # number of output neurons in an output layer X = matrix(runif(n * p), nrow=n, ncol=p, byrow=TRUE) H = matrix(runif(n * h), nrow=n, ncol=h, byrow=TRUE) C = matrix(runif(n * h), nrow=n, ncol=h, byrow=TRUE) W = matrix(rnorm((p + h) * h), nrow=p+h, ncol=h, byrow=TRUE) # copy same structure for other weights Wf = Wi = Wg = Wo = list(&quot;weight&quot; = W) bias = matrix(rnorm(1 * h), nrow=1, ncol=h, byrow=TRUE) # copy same structure for other biases bf = bi = bg = bo = list(&quot;weight&quot; = bias) We then run our forward feed like so: params = list(&quot;Wf&quot; = Wf, &quot;Wi&quot; = Wi, &quot;Wg&quot; = Wg, &quot;Wo&quot; = Wo, &quot;bf&quot; = bf, &quot;bi&quot; = bi, &quot;bg&quot; = bg, &quot;bo&quot; = bo) model = forward.unit.LSTM(X, H, C, params) str(model, strict.width=&quot;wrap&quot;) ## List of 6 ## $ Ht: num [1:5, 1:20] 3.06e-02 -3.26e-03 -6.87e-05 ## 7.50e-01 -2.87e-07 ... ## $ Ct: num [1:5, 1:20] 0.085496 -0.0067803 -0.0072869 ## 1.0755145 -0.0000626 ... ## $ Ft: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756 ## 0.00459 ... ## $ It: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756 ## 0.00459 ... ## $ Gt: num [1:5, 1:20] -0.5236 -0.0763 -0.9998 0.9939 -1 ## ... ## $ Ot: num [1:5, 1:20] 0.35864 0.48089 0.00942 0.94756 ## 0.00459 ... Afterwhich, we follow that by running our backward pass like so: gradients = list(&quot;dX&quot; = 0, &quot;dH&quot; = 0, &quot;dC&quot; = 0, &quot;dF&quot; = 0, &quot;dI&quot; = 0, &quot;dG&quot; = 0, &quot;dO&quot; = 0, &quot;dbf&quot; = 0, &quot;dbi&quot; = 0, &quot;dbg&quot; = 0, &quot;dbo&quot; = 0 ) dH.next = matrix(rnorm(n * h), nrow=n, ncol=h, byrow=TRUE) dC.next = matrix(rnorm(n * h), nrow=n, ncol=h, byrow=TRUE) gradients = backward.unit.LSTM(dH.next, dC.next, X, Y, H, C, model, params, gradients) str(gradients) ## List of 11 ## $ dX : num [1:5, 1:30] -0.119 2.364 1 -2.511 0.851 ... ## $ dH : num [1:5, 1:20] 0.443 0.249 -0.344 -1.452 0.811 ... ## $ dC : num [1:5, 1:20] -0.05873 0.07422 0.00317 -0.49132 0.00114 ... ## $ dF : num [1:50, 1:20] -0.00558 -0.00869 -0.01672 -0.02838 -0.00533 ... ## $ dI : num [1:50, 1:20] -0.00631 0.00258 -0.00859 -0.00632 -0.0213 ... ## $ dG : num [1:50, 1:20] 0.02282 0.02802 0.00815 -0.03022 0.04779 ... ## $ dO : num [1:50, 1:20] -0.00717 -0.00439 -0.01699 -0.02315 -0.01799 ... ## $ dbf: num [1:20] -0.02811 -0.13134 -0.00873 0.00139 -0.07315 ... ## $ dbi: num [1:20] -0.0131 0.171491 0.041701 0.000973 -0.131988 ... ## $ dbg: num [1:20] 2.52e-02 -4.17e-01 -3.78e-02 -5.07e-09 -7.10e-01 ... ## $ dbo: num [1:20] -2.75e-02 2.11e-01 1.19e-03 7.13e-06 -4.29e-01 ... Note that the calculation of our predicted output (Y.hat) using softmax and the gradients’ calculation are removed from the implementation. The calculations are relocated to another function. The reason becomes apparent in the Deep Stacked Bidirectional RNN section. 13.2.3 Gated Recurrent Units (GRU) GRU is another variant of RNN introduced by Kyunghyun Cho et al. (2014) with an evaluation paper by Junyoung Chung et al. (2014). It is regarded as a simplified version of LSTM. See the design in Figure 13.10. Figure 13.10: GRU Cell In GRU, we do not have a forget gate and an input gate. Instead, it is the update gate, \\(\\mathbf{Z}_{nxh}^{(t)}\\), that regulates the flow of information from state to state or layer to layer. GRU Forward Feed Based on Figure 13.10, the GRU design uses five equations for forward feed like so: \\[\\begin{align} \\mathbf{Z}_{nxh}^{(t)} &amp;= \\sigma \\left(\\mathbf{W}_{[p,h]xh}^{(z)} \\left[ \\mathbf{X}_{nxp}^{(t)}, \\mathbf{H}_{nxh}^{(t-1)} \\right] + \\mathbf{b}_{1xh}^{(z)} \\right) \\\\ \\mathbf{R}_{nxh}^{(t)} &amp;= \\sigma \\left(\\mathbf{W}_{[p,h]xh}^{(r)} \\left[ \\mathbf{X}_{nxp}^{(t)}, \\mathbf{H}_{nxh}^{(t-1)}\\right] + \\mathbf{b}_{1xh}^{(r)}\\right) \\\\ \\mathbf{G}_{nxh}^{(t)} &amp;= \\mathbf{\\text{tanh}} \\left(\\mathbf{W}_{[p,h]xh}^{(g)} \\left[ \\mathbf{X}_{nxp}^{(t)},\\ \\mathbf{R}_{nxh}^{(t)} \\odot \\mathbf{H}_{nxh}^{(t-1)}\\right] + \\mathbf{b}_{1xh}^{(g)}\\right) \\\\ \\mathbf{H}_{nxh}^{(t)} &amp;= \\left(1 - \\mathbf{Z}_{nxh}^{(t)}\\right) \\odot \\mathbf{H}_{nxh}^{(t-1)} + \\mathbf{Z}_{nxh}^{(t)} \\odot \\mathbf{G}_{nxh}^{(t)} \\\\ \\mathbf{\\hat{Y}}_{nxo}^{(t)} &amp;=\\mathbf{\\text{softmax}}\\left(\\mathbf{H}_{nxh}^{(t)} \\cdot \\mathbf{V}_{hxo}+ \\mathbf{b}_{1xo}^{(y)}\\right) \\end{align}\\] While we reference the paper by JunYoung Chung et al (2014) which reflects more of the above formulation, it is notable to mention that other literature may have the following alternative formulation for \\(\\mathbf{H}_{nxh}^{(t)}\\) which references a variant of Figure 13.10: \\[\\begin{align} \\mathbf{H}_{nxh}^{(t)} = \\left(1 - \\mathbf{Z}_{nxh}^{(t)}\\right) \\odot \\mathbf{G}_{nxh}^{(t)} + \\mathbf{Z}_{nxh}^{(t)} \\odot \\mathbf{H}_{nxh}^{(t-1)} \\end{align}\\] We leave readers to investigate the difference in performance, accuracy, etc. That said, we have our example implementation of GRU forward feed based on the equations above but using the latter formulation for \\(\\mathbf{H}_{nxh}^{(t)}\\). Note that our implementation is motivated by a MATLAB code from Minchen Li and partly in reference to our previous implementation of LSTM. forward.unit.GRU &lt;- function(X, H, params) { Wz = params$Wz$weight; Wr = params$Wr$weight; Wg = params$Wg$weight bz = params$bz$weight; br = params$br$weight; bg = params$bg$weight XH = cbind(X,H) # concatenate Zt = rnn.sigmoid(sweep(XH %*% Wz, 2, bz, &#39;+&#39;)) Rt = rnn.sigmoid(sweep(XH %*% Wr, 2, br, &#39;+&#39;)) rXH = cbind(X, Rt * H) # concatenate G.hat = sweep(rXH %*% Wg, 2, bg, &#39;+&#39;) Gt = rnn.tanh(G.hat) Ht = (1 - Zt) * Gt + Zt * H list(&quot;Ht&quot; = Ht, &quot;G.hat&quot; = G.hat, &quot;Zt&quot; = Zt, &quot;Rt&quot; = Rt, &quot;Gt&quot; = Gt) } GRU Backpropagation Similar to the Vanilla RNN and LSTM, we derive our cross-entropy loss for softmax to start the backpropagation. We obtain the Delta y (\\(\\delta y\\)) similar to the Vanilla RNN. We also derive the Delta H denoted by \\(\\left(\\delta \\mathbf{H}_{nxh}^{(t)}\\right)\\) which we also reference as dH.next in our implementation. This has an initial value of zero. \\[\\begin{align} \\delta \\mathbf{H}_{nxh}^{(t)} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{H}_{nxh}^{(t)}} + \\delta \\mathbf{H}_{nxh}^{(t+1)} = \\delta y \\cdot \\left(\\mathbf{V}_{hxo}\\right)^{\\text{T}} + \\underbrace{\\delta \\mathbf{H}_{nxh}^{(t+1)}}_{\\mathbf{\\text{dH.next}}} \\end{align}\\] Our gradients fo the weight V \\(\\left(\\nabla \\mathbf{V_{hxo}}\\right)\\) and bias \\(\\left(\\nabla \\mathbf{b}_{1xh}^{(y)}\\right)\\) follow similar derivation from Vanilla RNN. Next, we calculate the gradients of each gate, namely: Gradient with respect to \\(\\left(\\mathbf{Z}_{nxh}^{(t)}\\right)\\) in the update gate, \\[\\begin{align} \\delta z = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{Z}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{Z}_{nxh}^{(t)}} = \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left( \\mathbf{H}_{nxh}^{(t-1)} - \\mathbf{G}_{nxh}^{(t)} \\right) \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{G}_{nxh}^{(t)}\\right)\\) for the candidate state, \\[\\begin{align} \\delta g = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{G}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} = \\delta \\mathbf{H}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{Z}_{nxh}^{(t)} \\right) \\end{align}\\] and Gradient with respect to \\(\\left(\\mathbf{R}_{nxh}^{(t)}\\right)\\) in the reset gate, \\[\\begin{align} \\delta r = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} = \\delta g \\odot \\left(\\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} \\right) \\end{align}\\] where (Ahlad Kumar 2021): \\[\\begin{align} \\left(\\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} \\right) = \\left(\\mathbf{H}_{nxh}^{(t-1)} \\cdot \\mathbf{W}_{hxh}^{(g)} \\right) \\odot \\left( 1 - \\mathbf{\\text{tanh}}^2 \\left( \\hat{\\mathbf{G}}_{nxh}^{(t)} \\right)\\right) \\end{align}\\] and where: \\[\\begin{align} \\hat{\\mathbf{G}}_{nxh}^{(t)} = \\mathbf{W}_{[p,h]xh}^{(g)} \\left[ \\mathbf{X}_{nxp}^{(t)},\\ \\mathbf{R}_{nxh}^{(t)} \\odot \\mathbf{H}_{nxh}^{(t-1)}\\right] + \\mathbf{b}_{1xh}^{(g)}. \\end{align}\\] For the partial derivative of tanh, let us recall calculus: \\[\\begin{align} \\frac{\\partial\\ tanh(ax + by + z)}{\\partial x} = sec^2(ax + by + z)(a) = a(1 - tan^2(ax + by + z)) \\end{align}\\] such that we have: \\[\\begin{align} a = \\left(\\mathbf{H}_{nxh}^{(t-1)} \\cdot \\mathbf{W}_{hxh}^{(g)} \\right) \\end{align}\\] Also, it is important to note that the input X denoted by \\(\\left(\\mathbf{X}_{nxp}^{(t)}\\right)\\) is canceled out from the equation (see calculus); and therefore, its corresponding weight is also canceled out from the concatenation so that we have: \\[\\begin{align} \\mathbf{W}_{[p,h]xh}^{(g)} \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\mathbf{W}_{hxh}^{(g)} \\end{align}\\] Therefore, our gradient in the reset gate becomes: \\[\\begin{align} \\delta r = \\delta g \\odot \\left(\\mathbf{H}_{nxh}^{(t-1)} \\cdot \\mathbf{W}_{hxh}^{(g)} \\right) \\odot \\left( 1 - \\mathbf{\\text{tanh}}^2 \\left( \\hat{\\mathbf{G}}_{nxh}^{(t)} \\right)\\right) \\end{align}\\] Next, we calculate the gradients with respect to the linear functions, namely: Gradient with respect to \\(\\left(\\mathbf{\\hat{Z}}_{nxh}^{(t)}\\right)\\) in the update gate, \\[\\begin{align} \\delta \\hat{z} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{Z}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{Z}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{Z}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{Z}}_{nxh}^{(t)}} = \\delta z \\odot \\mathbf{Z}_{nxh}^{(t)} \\odot \\left(1 - \\mathbf{Z}_{nxh}^{(t)}\\right) \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{\\hat{G}}_{nxh}^{(t)}\\right)\\) for the candidate state, \\[\\begin{align} \\delta \\hat{g} = \\frac{\\partial \\mathcal{L}^{(CE)}} {\\partial \\mathbf{G}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{H}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{H}_{nxh}^{(t)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{G}}_{nxh}^{(t)}} = \\delta g \\odot \\left(1 - \\left(\\mathbf{G}_{nxh}^{(t)}\\right)^2\\right) \\end{align}\\] and Gradient with respect to \\(\\left(\\mathbf{\\hat{R}}_{nxh}^{(t)}\\right)\\) in the reset gate. \\[\\begin{align} \\delta \\hat{r} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{G}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{G}_{nxh}^{(t)}}{\\partial \\mathbf{R}_{nxh}^{(t)}} \\frac{\\partial \\mathbf{R}_{nxh}^{(t)}}{\\partial \\mathbf{\\hat{R}}_{nxh}^{(t)}} = \\delta r \\odot \\mathbf{R}_{nxh}^{(t)} \\left(1 - \\mathbf{R}_{nxh}^{(t)}\\right) \\end{align}\\] Lastly, let us get the gradients with respect to the weights and biases. Similarly, only for notation convenience, let us use the concatenation format instead: Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(z)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(z)}\\right)\\) in the output gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(z)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(z)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{z} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(z)} = \\sum_{column-wise}{\\delta \\hat{z}} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(g)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(g)}\\right)\\) in the output gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(g)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(g)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{g} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(g)} = \\sum_{column-wise}{\\delta \\hat{g}} \\end{align}\\] Gradient with respect to \\(\\left(\\mathbf{W}_{[p,h]xh}^{(r)}\\right)\\) and \\(\\left(\\mathbf{b}_{1xh}^{(r)}\\right)\\) in the output gate. \\[\\begin{align} \\nabla W_{[p,h]xh}^{(r)} = \\frac{\\partial \\mathcal{L}^{(CE)}}{\\partial \\mathbf{W}_{[p,h]xh}^{(r)}} = \\left([\\mathbf{X,H}]_{nx[p,h]}^{(t)}\\right)^{\\text{T}} \\cdot \\delta \\hat{r} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\nabla b_{1xh}^{(r)} = \\sum_{column-wise}{\\delta \\hat{r}} \\end{align}\\] Lastly, we calculate gradients with respect to \\(\\left(\\mathbf{H}_{nxh}^{(t-1)}\\right)\\), and \\(\\left(\\mathbf{X}_{nxh}^{(t)}\\right)\\). \\[\\begin{align} \\nabla \\mathbf{X}_{nxp}^{(t)} &amp;= \\delta z \\cdot \\left(\\mathbf{W_{pxh}^{(z)}}\\right)^{\\text{T}} + \\delta g \\cdot \\left(\\mathbf{W_{pxh}^{(g)}}\\right)^{\\text{T}} + \\delta r \\cdot \\left(\\mathbf{W_{pxh}^{(r)}}\\right)^{\\text{T}} \\\\ \\underbrace{\\nabla \\mathbf{H}_{nxh}^{(t-1)}}_{\\text{new dH.next}} &amp;= \\delta z \\cdot \\left(\\mathbf{W_{hxh}^{(z)}}\\right)^{\\text{T}} + \\delta g \\cdot \\left(\\mathbf{W_{hxh}^{(g)}}\\right)^{\\text{T}} + \\delta r \\cdot \\left(\\mathbf{W_{hxh}^{(r)}}\\right)^{\\text{T}} \\end{align}\\] The \\(\\left(\\nabla \\mathbf{H}_{nxh}^{(t-1)}\\right)\\) becomes our new dH.next which we use to propagate back to the next previous time step. \\[\\begin{align} \\text{dH.next} = \\delta \\mathbf{H}_{nxh}^{(t-1)} \\end{align}\\] Let us now review our example implementation of GRU backpropagation based on the formulations above: backward.unit.GRU &lt;- function(dH.next, X, Y, H, model, params, grad) { p = ncol(X) h = ncol(H) Ht = model$Ht Zt = model$Zt; Rt = model$Rt; Gt = model$Gt Wz = params$Wz$weight; Wr = params$Wr$weight; Wg = params$Wg$weight bz = params$bz$weight; br = params$br$weight; bg = params$bg$weight G.hat = model$G.hat dH = dH.next dZt = dH * ( Ht - Gt) * Zt * ( 1 - Zt) dGt = dH * ( 1 - Zt) * ( 1 - Gt^2) dRt = ( dGt * (Ht %*% Wr[(p+1):(h+p),]) * (1 - rnn.tanh(G.hat)^2)) * Rt * (1 - Rt) XH = cbind(X,H) # concatenate # Calculate gradient wrt to shared Weights and Biases dWzt = t(XH) %*% dZt; grad$dZ = grad$dZ + dWzt dWgt = t(XH) %*% dGt; grad$dG = grad$dG + dWgt dWrt = t(XH) %*% dRt; grad$dR = grad$dR + dWrt dbz = apply(dZt, 2, sum); grad$dbz = grad$dbz + dbz dbg = apply(dGt, 2, sum); grad$dbg = grad$dbg + dbg dbr = apply(dRt, 2, sum); grad$dbr = grad$dbr + dbr dX = dZt %*% t(Wz[1:p,]) + dGt %*% t(Wg[1:p,]) + dRt %*% t(Wr[1:p,]) dH = dZt %*% t(Wz[(p+1):(h+p),]) + dGt %*% t(Wg[(p+1):(h+p),]) + dRt %*% t(Wr[(p+1):(h+p),]) grad$dX = dX grad$dH = dH grad } For a simple use, let us concoct a simple dataset and initialize our parameters: set.seed(1) n = 5; p = 30; h = 20; o = 3 X = matrix(runif(n * p), nrow=n, ncol=p, byrow=TRUE) H = matrix(runif(n * h), nrow=n, ncol=h, byrow=TRUE) W = matrix(rnorm((p + h) * h), nrow=p+h, ncol=h, byrow=TRUE) Wz = Wg = Wr = list(&quot;weight&quot;=W) #copy same structure for other weights bias = matrix(rnorm(1 * h), nrow=1, ncol=h, byrow=TRUE) bz = bg = br = list(&quot;weight&quot;=bias) #copy same structure for other biases We then run our forward feed like so: params = list(&quot;Wz&quot; = Wz, &quot;Wg&quot; = Wg, &quot;Wr&quot; = Wr, &quot;bz&quot; = bz, &quot;bg&quot; = bg, &quot;br&quot; = br) model = forward.unit.GRU(X, H, params) str(model) ## List of 5 ## $ Ht : num [1:5, 1:20] -0.988 0.474 0.615 0.953 0.513 ... ## $ G.hat: num [1:5, 1:20] -4.472 0.137 0.606 1.184 1.049 ... ## $ Zt : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ... ## $ Rt : num [1:5, 1:20] 0.0071 0.6845 0.4313 0.9389 0.6183 ... ## $ Gt : num [1:5, 1:20] -1 0.136 0.541 0.829 0.782 ... Afterwhich, we follow that by running our backward pass like so: gradients = list(&quot;dX&quot; = 0, &quot;dH&quot; = 0, &quot;dZ&quot; = 0, &quot;dG&quot; = 0, &quot;dR&quot; = 0, &quot;dbz&quot; = 0, &quot;dbg&quot; = 0, &quot;dbr&quot; = 0) dH.next = matrix(rnorm(n * h), nrow=n, ncol=h, byrow=TRUE) gradients = backward.unit.GRU(dH.next, X, Y, H, model, params, gradients) str(gradients) ## List of 8 ## $ dX : num [1:5, 1:30] 1.0933 -0.8241 0.2523 -0.1001 0.0675 ... ## $ dH : num [1:5, 1:20] 1.653 -1.46 -0.101 1.061 0.117 ... ## $ dZ : num [1:50, 1:20] -0.03323 -0.05663 -0.0364 -0.00337 -0.06603 ... ## $ dG : num [1:50, 1:20] 0.6443 0.0415 0.2329 0.233 0.2817 ... ## $ dR : num [1:50, 1:20] 0.0561 0.1294 0.0907 0.0223 0.1606 ... ## $ dbz: num [1:20] -0.073512 0.004256 -0.000381 0.180441 -0.070078 ... ## $ dbg: num [1:20] 0.52703 -0.00109 -0.00139 0.13647 0.09256 ... ## $ dbr: num [1:20] 1.79e-01 -4.04e-07 -1.25e-06 -3.29e-02 -3.01e-02 ... We leave readers to investigate Minimal Gated Unit (MGU), a variant of GRU. 13.3 Deep Stacked RNN Each cell of an RNN forms individual units or components that make up a deep recurrent neural network. This section shows how the cells can stack up on top of each other. Figure 13.11 is a view of a rolled representation of a deep-stacked RNN. Figure 13.11: Deep Stacked RNN To fully implement a deep-stacked RNN, we first need to implement a few helper functions. We start with the optimize.update(.) function that handles optimized updates. This function can be tailored to use sgd, sgd with nesperov momentum, adam, adagrad, adadelta, etc. similar to our implementation of CNN. For our own purposes, we use adam as our optimizer. optimize.adam = adam &lt;- function(param, gradient, eta, t) { beta1 = 0.90; beta2 = 0.999; eps=1e-10 param$rho = beta1 * param$rho + (1 - beta1) * gradient param$nu = beta2 * param$nu + (1 - beta2) * gradient^2 rho.hat = param$rho / (1 - beta1^t) nu.hat = param$nu / (1 - beta2^t) phi = eta / (sqrt(nu.hat) + eps) param$weight = param$weight - phi * rho.hat param } optimize.update &lt;-function(rtype, params, grad,eta=0.001, t) { if (rtype == &quot;lstm&quot;) { Wf= adam(params$Wf, grad$dF, eta,t); bf= adam(params$bf, grad$dbf, eta,t) Wi= adam(params$Wi, grad$dI, eta,t); bi= adam(params$bi, grad$dbi, eta,t) Wg= adam(params$Wg, grad$dG, eta,t); bg= adam(params$bg, grad$dbg, eta,t) Wo= adam(params$Wo, grad$dO, eta,t); bo= adam(params$bo, grad$dbo, eta,t) params = list(&quot;Wf&quot; = Wf, &quot;Wi&quot; = Wi, &quot;Wg&quot; = Wg, &quot;Wo&quot; = Wo, &quot;bf&quot; = bf, &quot;bi&quot; = bi, &quot;bg&quot; = bg, &quot;bo&quot; = bo) } else if (rtype == &quot;gru&quot;) { Wz= adam(params$Wz, grad$dZ, eta,t); bz= adam(params$bz, grad$dbz, eta,t) Wg= adam(params$Wg, grad$dG, eta,t); bg= adam(params$bg, grad$dbg, eta,t) Wr= adam(params$Wr, grad$dR, eta,t); br= adam(params$br, grad$dbr, eta,t) params = list(&quot;Wz&quot; = Wz, &quot;Wg&quot; = Wg, &quot;Wr&quot; = Wr, &quot;bz&quot; = bz, &quot;bg&quot; = bg, &quot;br&quot; = br) } else if (rtype == &quot;vanilla.rnn&quot;) { ############################################ # follow the same code structure as LSTM/GRU ############################################ } params } optimize.output &lt;-function(output, eta=0.001, t) { V = adam(output$V, output$dV, eta,t) by = adam(output$by, output$dby, eta,t) list(&quot;V&quot; = V, &quot;by&quot; = by, &quot;dV&quot; = 0, &quot;dby&quot; = 0) } Additionally, we also require a few other helper functions such as ln(.) for calculating log. We also need softmax.loss(.), and accuracy for calculating our loss and accuracy for metrics. Then the use of flush.str(.) for printing status of our training. See our implementation of CNN for these functions. Next, we then write our example implementation of lstm.fit(.) function. lstm.fit &lt;- function(X, Y, layers, eta, epoch) { dim.x = dim(X) timesteps = dim.x[3] L = length(layers) orig.X = X models = list() for (l in 1:L) { layer = layers[[l]] output = layer$output params = layer$params grad = layer$gradients # reinitialized gradient for every epoch Ht = layer$H[,,1] Ct = layer$C[,,1] model = list() if (l == 1) { X = orig.X } else { X = layers[[l-1]]$H } for (t in 1:timesteps) { Xt = X[,,t] model[[t]] = forward.unit.LSTM(Xt, Ht, Ct, params) Ht = layer$H[,,t] = model[[t]]$Ht Ct = layer$C[,,t] = model[[t]]$Ct } models[[l]] = model; layers[[l]] = layer } # last model stores the output outcome = get.output(X, Y, model, output, actfun = &quot;rnn.softmax&quot;) for (l in L:1) { layer = layers[[l]] params = layer$params grad = layer$gradients dnext = matrix(0, nrow=n, ncol=h, byrow=TRUE) dH.next = dC.next = dnext # copy structure if (l == 1) { X = orig.X } else { X = layers[[l-1]]$H } model = models[[l]] for (t in timesteps:1) { Xt = X[,,t] Ht = model[[t]]$Ht; Ct = model[[t]]$Ct dH.next = outcome$dH + dH.next grad = backward.unit.LSTM(dH.next, dC.next, Xt, Y, Ht, Ct, model[[t]], params, grad) dH.next = grad$dH dC.next = grad$dC } outcome$dH = grad$dX # pass the next Dout to next previous layer output$dV = outcome$dV; output$dby = outcome$dby layer$params = optimize.update(rtype=&quot;lstm&quot;, params, grad, eta, epoch) layer$output = optimize.output(output, eta, epoch) layers[[l]] = layer } list(&quot;loss&quot;= outcome$loss, &quot;accuracy&quot;= outcome$accuracy,&quot;layers&quot;= layers) } We also implement our gru.fit(.) for training GRU models. gru.fit &lt;- function(X, Y, layers, eta, epoch) { dim.x = dim(X) timesteps = dim.x[3] L = length(layers) orig.X = X models = list() for (l in 1:L) { layer = layers[[l]] output = layer$output params = layer$params grad = layer$gradients Ht = layer$H[,,1] model = list() if (l == 1) { X = orig.X } else { X = layers[[l-1]]$H } for (t in 1:timesteps) { Xt = X[,,t] model[[t]] = forward.unit.GRU(Xt, Ht, params) Ht = layer$H[,,t] = model[[t]]$Ht } models[[l]] = model; layers[[l]] = layer } # last model stores the output outcome = get.output(X, Y, model, output, actfun = &quot;rnn.softmax&quot;) output$dV = outcome$dV; output$dby = outcome$dby for (l in L:1) { layer = layers[[l]] params = layer$params grad = layer$gradients dnext = matrix(0, nrow=n, ncol=h, byrow=TRUE) dH.next = dnext # copy structure if (l == 1) { X = orig.X } else { X = layers[[l-1]]$H } model = models[[l]] for (t in timesteps:1) { Xt = X[,,t]; Ht = model[[t]]$Ht dH.next = outcome$dH + dH.next grad = backward.unit.GRU(dH.next, Xt, Y, Ht, model[[t]], params, grad) dH.next = grad$dH } outcome$dH = grad$dX # pass the next Dout to next previous layer layer$params = optimize.update(rtype=&quot;gru&quot;, params, grad, eta, epoch) layer$output = optimize.output(output, eta, epoch) layers[[l]] = layer } list(&quot;loss&quot;= outcome$loss,&quot;accuracy&quot;= outcome$accuracy,&quot;layers&quot;= layers) } For the output, including calculation of the loss, and accuracy, including gradients for the output weight and bias, we implement a separate function called get.output(.): get.output &lt;- function(X, Y, model, output, actfunc) { actfunc = get(actfunc) dim.y = dim(Y) dim.x = dim(X) timesteps = dim.x[3] V = output$V$weight by = output$by$weight Y.hat = array(0, dim.y) if (length(dim.y) == 2) { Y.hat = array(0, c(dim.y, timesteps)) } loss = accurate = rep(0, timesteps) dV = dby = 0 for (t in 1:timesteps) { if (length(dim.y) == 2) { target = Y } else { target = Y[,,t] } Y.hat[,,t] = actfunc(sweep(model[[t]]$Ht %*% V, 2, by, &#39;+&#39;)) loss[t] = mean(softmax.loss(target, Y.hat[,,t])) accurate[t] = accuracy(target, Y.hat[,,t]) L = (Y.hat[,,t] - target) dH = L %*% t(V) dV = dV + t(model[[t]]$Ht) %*% L dby = dby + apply(L, 2, sum) } list(&quot;Y.hat&quot; = Y.hat, &quot;dH&quot; = dH, &quot;dV&quot; = dV, &quot;dby&quot; = dby, &quot;loss&quot; = mean(loss), &quot;accuracy&quot; = mean(accurate)) } Finally, our example implementation of RNN supports LSTM and GRU. The vanilla RNN is left for readers to try to implement. my.RNN &lt;- function(rtype, X, Y, layers, epoch=100, eta=0.001) { options(digits = 16) # 16 digits precision cost = rep(0, epoch) accurate = rep(0, epoch) my.time = Sys.time() for (t in 1:epoch) { if (rtype == &quot;lstm&quot;) { model = lstm.fit(X, Y, layers, eta, t) layers = model$layers cost[t] = model$loss accurate[t] = model$accuracy } else if (rtype == &quot;gru&quot;) { model = gru.fit(X, Y, layers, eta, t) layers = model$layers cost[t] = model$loss accurate[t] = model$accuracy } else if (rtype == &quot;vanilla.rnn&quot;) { ############################################ # follow the same code structure as LSTM/GRU ############################################ } if (t %% (epoch * 0.10) == 0 || t == 1) { new.time = Sys.time() lag.time = difftime(new.time, my.time, units=&quot;secs&quot;) my.time = new.time flush.str( &quot;epoch %d - loss: %2.3f, accuracy %2.3f lag time (sec): %5.3f&quot;, t, cost[t], accurate[t], lag.time) } } list(&quot;cost&quot; = cost, &quot;accuracy&quot; = accurate, &quot;layers&quot; = layers) } To apply our RNN, we need to construct a data structure that keeps track of our parameters and gradients (which also helps to maintain simple metadata and cache for our layers). To illustrate, we implement a network structure for LSTM. deep.lstm.layers &lt;- function(X, Y, ...) { set.seed(1) rinit = rnn.initialize layers = list(...) di = dim(X); n = di[1]; p = di[2]; t = di[3] di = dim(Y); o = di[2] structure = list() l = 0 coef = list(&quot;weight&quot; = 0, &quot;rho&quot; = 0, &quot;nu&quot; = 0) for (layer in layers) { l = l + 1 if (is.null(layer$size)) { layer$size = 30 } if (is.null(layer$bidirectional)) { layer$bidirectional = FALSE } H = array(0, c(n, h, t)) C = array(0, c(n, h, t)) V = Wf = Wi = Wg = Wo = coef # copy the same structure V$weight = rinit(h, o, state=TRUE) Wf$weight = rinit(p + h, h, state=TRUE) Wi$weight = rinit(p + h, h, state=TRUE) Wg$weight = rinit(p + h, h, state=TRUE) Wo$weight = rinit(p + h, h, state=TRUE) by = bf = bi = bg = bo = coef # copy the same structure bf$weight = rinit(1, h, state=FALSE) bi$weight = rinit(1, h, state=FALSE) bg$weight = rinit(1, h, state=FALSE) bo$weight = rinit(1, h, state=FALSE) by$weight = rinit(1, o, state=FALSE) p = h # for next stack output = list(&quot;V&quot; = V, &quot;by&quot; = by, &quot;dV&quot; = 0, &quot;dby&quot; = 0) parms = list(&quot;Wf&quot; = Wf, &quot;Wi&quot; = Wi, &quot;Wg&quot; = Wg, &quot;Wo&quot; = Wo, &quot;bf&quot; = bf, &quot;bi&quot; = bi, &quot;bg&quot; = bg, &quot;bo&quot; = bo) grad = list(&quot;dX&quot; = 0, &quot;dH&quot; = 0, &quot;dC&quot; = 0, &quot;dF&quot; = 0, &quot;dI&quot; = 0, &quot;dG&quot; = 0, &quot;dO&quot; = 0, &quot;dbf&quot; = 0, &quot;dbi&quot; = 0, &quot;dbg&quot; = 0, &quot;dbo&quot; = 0 ) struct = list(&quot;H&quot; = H, &quot;C&quot; = C, &quot;params&quot; = parms, &quot;gradients&quot; = grad) if (layer$bidirectional == TRUE) { structure[[l]] = list( &quot;output&quot; = output, &quot;forward&quot; = struct, &quot;backward&quot; = struct ) } else { structure[[l]] = list(&quot;H&quot; = H, &quot;C&quot; = C, &quot;output&quot; = output, &quot;params&quot; = parms, &quot;gradients&quot; = grad) } } structure } And we also implement another network structure for GRU. deep.gru.layers &lt;- function(X, Y, ...) { set.seed(1) rinit = rnn.initialize layers = list(...) di = dim(X); n = di[1]; p = di[2]; t = di[3] di = dim(Y); o = di[2] structure = list() l = 0 coef = list(&quot;weight&quot; = 0, &quot;rho&quot; = 0, &quot;nu&quot; = 0) for (layer in layers) { l = l + 1 if (is.null(layer$size)) { layer$size = 30 } if (is.null(layer$bidirectional)) { layer$bidirectional = FALSE } H = array(0, c(n, h, t)) V = Wz = Wg = Wr = coef # copy the same structure V$weight = rinit(h, o, state=TRUE) Wz$weight = rinit(p + h, h, state=TRUE) Wg$weight = rinit(p + h, h, state=TRUE) Wr$weight = rinit(p + h, h, state=TRUE) by = bz = bg = br = coef # copy the same structure bz$weight = rinit(1, h, state=FALSE) bg$weight = rinit(1, h, state=FALSE) br$weight = rinit(1, h, state=FALSE) by$weight = rinit(1, o, state=FALSE) p = h # for next stack output = list(&quot;V&quot; = V, &quot;by&quot; = by, &quot;dV&quot; = 0, &quot;dby&quot; = 0) parms = list(&quot;Wz&quot; = Wz, &quot;Wg&quot; = Wg, &quot;Wr&quot; = Wr, &quot;bz&quot; = bz, &quot;bg&quot; = bg, &quot;br&quot; = br ) grad = list(&quot;dX&quot; = 0, &quot;dH&quot; = 0, &quot;dZ&quot; = 0, &quot;dG&quot; = 0, &quot;dR&quot; = 0, &quot;dbz&quot; = 0, &quot;dbg&quot; = 0, &quot;dbr&quot; = 0) struct = list(&quot;H&quot; = H, &quot;params&quot; = parms, &quot;gradients&quot; = grad) if (layer$bidirectional == TRUE) { structure[[l]] = list( &quot;output&quot; = output, &quot;forward&quot; = struct, &quot;backward&quot; = struct ) } else { structure[[l]] = list(&quot;H&quot; = H, &quot;output&quot; = output, &quot;params&quot; = parms, &quot;gradients&quot; = grad) } } structure } An important aspect of training is weight initialization. In CNN, we identified a few initialization methods such as Xavier initialization and He initialization. In RNN, we introduce Orthogonal initialization. Here, we use an rnn.initialize(.) function which uses net.initialization(.) function from our CNN implementation. Also, we introduce the use of our orthogonal.initialization(.) function based on Orthogonal initialization method (Wei Hu (2020); Eugene Vorontsov (2017); Stephen Merity (2015)). orthogonal.initialization &lt;- function(size, row, col) { V = matrix(runif(size, min=0, max=1), nrow=row, ncol=col, byrow=TRUE) sv = svd(V) di = dim(sv$u) if (prod(di) == size) { W = sv$u } else { W = sv$v } array(W, c(row, col)) # reshape } rnn.initialize &lt;- function(ni, no, state=FALSE) { if (state == TRUE) { # variable is concatenated btw input and state p = ni - no; h = no I = net.initialization(p * h, p, h, itype=&quot;glorot&quot;, dist=&quot;uniform&quot;) I = array(I, c(p, h)) S = orthogonal.initialization(h * h, h, h) W = rbind(I, S) } else { W = net.initialization(ni * no, ni, no, itype=&quot;glorot&quot;, dist=&quot;uniform&quot;) W = array(W, c(ni, no)) } W } rnn.initialize(5, 3, state=TRUE) ## [,1] [,2] [,3] ## [1,] -0.31218743 0.91235315 -0.133482087 ## [2,] -0.58272098 -1.02850246 1.037803497 ## [3,] -0.51916059 0.69726644 0.494258841 ## [4,] -0.33635573 0.36494832 -0.868146039 ## [5,] -0.78570803 -0.61695400 0.045062776 Let us see what the structure looks like given the below data points (X, Y). First, our input assumes an embedding using a sequence of three randomly generated numbers. Next, we concoct a single batch of 50 samples. Here, we assume a many-to-one system. set.seed(1) ts = 3; n = 50; p = 30; h = 20; o = 3 X = array(runif(n * p * ts), c(n, p, ts)) Y = array(sample(0, size=n * o , replace=TRUE), c(n, o)) Y.idx = sample.int(o, size=n, replace=TRUE) for (i in 1:n) { Y[i, Y.idx[i]] = 1 } # one-hot encoding For a many-to-many system, e.g. seq2seq, we may try something like so: Y = array(sample(0, size=n * o * ts, replace=TRUE), c(n, o, ts)) for (t in 1:ts) { Y.idx = sample.int(o, size=n, replace=TRUE) for (i in 1:n) { Y[i, Y.idx[i], t] = 1 } # one-hot encoding } Now, let us use deep.lstm.layers(.) to construct the structure. layers = deep.lstm.layers(X, Y, list(size=20), list(size=20)) str(layers[[1]]$params$Wf) ## List of 3 ## $ weight: num [1:50, 1:20] 0.286 -0.143 -0.0284 -0.1161 0.1045 ... ## $ rho : num 0 ## $ nu : num 0 Finally, we use our implementation to train an LSTM model. lstm.model = my.RNN(rtype = &quot;lstm&quot;, X=X, Y=Y, layers, epoch=200, eta=0.01) ## [1] &quot;epoch 1 - loss: 1.243, accuracy 0.287 lag time (sec): 0.141&quot; ## [1] &quot;epoch 20 - loss: 0.898, accuracy 0.553 lag time (sec): 0.140&quot; ## [1] &quot;epoch 40 - loss: 0.540, accuracy 0.760 lag time (sec): 0.153&quot; ## [1] &quot;epoch 60 - loss: 0.518, accuracy 0.800 lag time (sec): 0.151&quot; ## [1] &quot;epoch 80 - loss: 0.480, accuracy 0.807 lag time (sec): 0.152&quot; ## [1] &quot;epoch 100 - loss: 0.439, accuracy 0.813 lag time (sec): 0.180&quot; ## [1] &quot;epoch 120 - loss: 0.357, accuracy 0.853 lag time (sec): 0.149&quot; ## [1] &quot;epoch 140 - loss: 0.266, accuracy 0.913 lag time (sec): 0.145&quot; ## [1] &quot;epoch 160 - loss: 0.161, accuracy 0.940 lag time (sec): 0.151&quot; ## [1] &quot;epoch 180 - loss: 0.102, accuracy 0.967 lag time (sec): 0.175&quot; ## [1] &quot;epoch 200 - loss: 0.042, accuracy 0.993 lag time (sec): 0.162&quot; The Loss and Accuracy are plotted in Figure 13.12. We can see that our model can learn. We see the final loss resulting in 0.042 with a corresponding accuracy of 99.33%. The outcome is a good starting point. x = seq(1, length(lstm.model$cost)) y = lstm.model$cost y1 = (y - min(y))/(max(y) - min(y)) y2 = lstm.model$accuracy plot(NULL, xlim=range(x), ylim=range(0,y1), xlab=&quot;Epoch&quot;, ylab=&quot;Cross-Entropy Loss / Accuracy&quot;, main=&quot;Deep Stacked LSTM Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;, lwd=2) lines(x, y2, col=&quot;navyblue&quot;, lwd=2) Figure 13.12: Deep Stacked LSTM Plot Next, let us use our implementation to also train a GRU model. layers = deep.gru.layers(X, Y, list(size=20), list(size=20)) str(layers[[1]]$params$Wz) ## List of 3 ## $ weight: num [1:50, 1:20] 0.286 -0.143 -0.0284 -0.1161 0.1045 ... ## $ rho : num 0 ## $ nu : num 0 gru.model = my.RNN(rtype = &quot;gru&quot;, X=X, Y=Y, layers, epoch=200, eta=0.01) ## [1] &quot;epoch 1 - loss: 1.516, accuracy 0.280 lag time (sec): 0.094&quot; ## [1] &quot;epoch 20 - loss: 0.861, accuracy 0.607 lag time (sec): 0.111&quot; ## [1] &quot;epoch 40 - loss: 0.651, accuracy 0.747 lag time (sec): 0.121&quot; ## [1] &quot;epoch 60 - loss: 0.744, accuracy 0.693 lag time (sec): 0.117&quot; ## [1] &quot;epoch 80 - loss: 0.541, accuracy 0.740 lag time (sec): 0.120&quot; ## [1] &quot;epoch 100 - loss: 0.430, accuracy 0.813 lag time (sec): 0.119&quot; ## [1] &quot;epoch 120 - loss: 0.447, accuracy 0.847 lag time (sec): 0.124&quot; ## [1] &quot;epoch 140 - loss: 0.258, accuracy 0.907 lag time (sec): 0.116&quot; ## [1] &quot;epoch 160 - loss: 0.360, accuracy 0.840 lag time (sec): 0.144&quot; ## [1] &quot;epoch 180 - loss: 0.271, accuracy 0.907 lag time (sec): 0.175&quot; ## [1] &quot;epoch 200 - loss: 0.214, accuracy 0.920 lag time (sec): 0.119&quot; The Loss and Accuracy are plotted in Figure 13.13. Similarly, our model can train. We see the final loss resulting in 0.214 with an accuracy around 92%. x = seq(1, length(gru.model$cost)) y = gru.model$cost y1 = (y - min(y))/(max(y) - min(y)) y2 = gru.model$accuracy plot(NULL, xlim=range(x), ylim=range(0,y1), xlab=&quot;Epoch&quot;, ylab=&quot;Cross-Entropy Loss / Accuracy&quot;, main=&quot;Deep Stacked GRU Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;, lwd=2) lines(x, y2, col=&quot;navyblue&quot;, lwd=2) Figure 13.13: Deep Stacked GRU Plot 13.4 Deep Stacked Bidirectional RNN Bidirectional RNN was introduced by Mike Schuster and Kuldip K. Paliwal in (1997). It describes the design of RNN in which we calculate our output based on forward feedand use backward feed. Note that backward feed does not refer to backward pass to mean backpropagation. Here, a Bidirectional RNN allows the use of another RNN layer that processes input in the opposite direction. We mean to reverse our input from time step t+1 to t and from t to t-1 and so on. We allow another RNN layer to perform the same forward feed against the reversed input. As we should see in Figure 13.14, the output \\(\\mathbf{\\hat{Y}}\\) is dependent on the combined output of the forward feed layer and backward feed layer in their respective time steps. That allows the output to consider past and future information. Figure 13.14: Deep Stacked Bidirectional RNN In certain frameworks, the term merging or combining the feedforward output and feedbackward output allows the option to add, average, or multiply them in an element-wise manner. Otherwise, concatenation is a more common option. Bidirectional RNN applies to the three variants of RNN, namely Vanilla RNN, LSTM, and GRU; hereafter, we call this simply Bidirectional RNN. To support a working Bidirectional RNN, we should be able to adjust our implementation of lstm.fit(.) and gru.fit(.). Here, we use averaging for the merge. lstm.fit &lt;- function(X, Y, layers, eta, epoch) { d = dim(X) timesteps = d[3] L = length(layers) orig.X = X fmodels = list() bmodels = list() for (l in 1:L) { layer = layers[[l]] flayer = layer$forward blayer = layer$backward output = layer$output fg = flayer$gradients bg = blayer$gradients Ht = flayer$H[,,1]; Hs = blayer$H[,,timesteps] Ct = flayer$C[,,1]; Cs = blayer$C[,,timesteps] fmodel = bmodel = list() if (l == 1) { X = orig.X } else { X = (layers[[l-1]]$forward$H + layers[[l-1]]$backward$H )/2 } for (t in 1:timesteps) { Xt = X[,,t] # forward feed fmodel[[t]] = forward.unit.LSTM(Xt, Ht, Ct, flayer$params) Ht = flayer$H[,,t] = fmodel[[t]]$Ht Ct = flayer$C[,,t] = fmodel[[t]]$Ct s = timesteps - t + 1 # reverses input Xs = X[,,s] # backward feed bmodel[[s]] = forward.unit.LSTM(Xs, Hs, Cs, blayer$params) Hs = blayer$H[,,s] = bmodel[[s]]$Ht Cs = blayer$C[,,s] = bmodel[[s]]$Ct } fmodels[[l]] = fmodel; layers[[l]]$forward = flayer bmodels[[l]] = bmodel; layers[[l]]$backward = blayer } outcome = get.output(X, Y, fmodel, bmodel, output, actfun = &quot;rnn.softmax&quot;) output$dV = outcome$dV; output$dby = outcome$dby for (l in L:1) { layer = layers[[l]] flayer = layer$forward; fmodel = fmodels[[l]] blayer = layer$backward; bmodel = bmodels[[l]] fg = flayer$gradients; bg = blayer$gradients dnext = matrix(0, nrow=n, ncol=h, byrow=TRUE) dH.fnext = dH.bnext = dnext # copy structure dC.fnext = dC.bnext = dnext # copy structure if (l == 1) { X = orig.X } else { X = (layers[[l-1]]$forward$H + layers[[l-1]]$backward$H )/2 } for (t in timesteps:1) { Xt = X[,,t]; Ht = fmodel[[t]]$Ht; Ct = fmodel[[t]]$Ct dH.fnext = outcome$dH + dH.fnext fg = backward.unit.LSTM(dH.fnext, dC.fnext, Xt, Y, Ht, Ct, fmodel[[t]], flayer$params, fg) s = timesteps - t + 1 Xs = X[,,s]; Hs = bmodel[[s]]$Ht; Cs = bmodel[[s]]$Ct dH.bnext = outcome$dH + dH.bnext bg = backward.unit.LSTM(dH.bnext, dC.bnext, Xs, Y, Hs, Cs, bmodel[[s]], blayer$params, bg) dH.fnext = fg$dH; dC.fnext = fg$dC dH.bnext = bg$dH; dC.bnext = bg$dC } # pass the next Dout to next previous layer outcome$dH = fg$dX + bg$dX flayer$params = optimize.update(rtype=&quot;lstm&quot;, flayer$params, fg, eta, epoch) blayer$params = optimize.update(rtype=&quot;lstm&quot;, blayer$params, bg, eta, epoch) layer$output = optimize.output(output, eta, epoch) layers[[l]]$output = layer$output layers[[l]]$forward = flayer layers[[l]]$backward = blayer } list(&quot;loss&quot;= outcome$loss, &quot;accuracy&quot;= outcome$accuracy,&quot;layers&quot;= layers) } Similarly, we modify gru.fit(.): gru.fit &lt;- function(X, Y, layers, eta, epoch) { d = dim(X) timesteps = d[3] L = length(layers) orig.X = X fmodels = list() bmodels = list() for (l in 1:L) { layer = layers[[l]] flayer = layer$forward blayer = layer$backward output = layer$output fg = flayer$gradients bg = blayer$gradients Ht = flayer$H[,,1]; Hs = blayer$H[,,timesteps] fmodel = bmodel = list() if (l == 1) { X = orig.X } else { X = (layers[[l-1]]$forward$H + layers[[l-1]]$backward$H )/2 } for (t in 1:timesteps) { Xt = X[,,t] fmodel[[t]] = forward.unit.GRU(Xt, Ht, flayer$params) # forward feed Ht = flayer$H[,,t] = fmodel[[t]]$Ht s = timesteps - t + 1 # reverses input Xs = X[,,s] bmodel[[s]] = forward.unit.GRU(Xs, Hs, blayer$params) # backward feed Hs = blayer$H[,,s] = bmodel[[s]]$Ht } fmodels[[l]] = fmodel; layers[[l]]$forward = flayer bmodels[[l]] = bmodel; layers[[l]]$backward = blayer } outcome = get.output(X, Y, fmodel, bmodel, output, actfun = &quot;rnn.softmax&quot;) output$dV = outcome$dV; output$dby = outcome$dby for (l in L:1) { layer = layers[[l]] flayer = layer$forward; fmodel = fmodels[[l]] blayer = layer$backward; bmodel = bmodels[[l]] fg = flayer$gradients; bg = blayer$gradients dnext = matrix(0, nrow=n, ncol=h, byrow=TRUE) dH.fnext = dH.bnext = dnext # copy structure if (l == 1) { X = orig.X } else { X = (layers[[l-1]]$forward$H + layers[[l-1]]$backward$H )/2 } for (t in timesteps:1) { Xt = X[,,t]; Ht = fmodel[[t]]$Ht dH.fnext = outcome$dH + dH.fnext fg = backward.unit.GRU(dH.fnext, Xt, Y, Ht, fmodel[[t]], flayer$params, fg) s = timesteps - t + 1 Xs = X[,,s]; Hs = bmodel[[s]]$Ht dH.bnext = outcome$dH + dH.bnext bg = backward.unit.GRU(dH.bnext, Xs, Y, Hs, bmodel[[s]], blayer$params, bg) dH.fnext = fg$dH dH.bnext = bg$dH } # pass the next Dout to next previous layer outcome$dH = fg$dX + bg$dX flayer$params = optimize.update(rtype=&quot;gru&quot;, flayer$params, fg, eta, epoch) blayer$params = optimize.update(rtype=&quot;gru&quot;, blayer$params, bg, eta, epoch) layer$output = optimize.output(output, eta, epoch) layers[[l]]$output = layer$output layers[[l]]$forward = flayer layers[[l]]$backward = blayer } list(&quot;loss&quot;= outcome$loss, &quot;accuracy&quot;= outcome$accuracy,&quot;layers&quot;= layers) } Finally, we modify the get.output(.) function to handle the merge of outputs from both forward and backward feed. Here, we use averaging for the merge. get.output &lt;- function(X, Y, fmodel, bmodel, output, actfunc) { actfunc = get(actfunc) dim.y = dim(Y) dim.x = dim(X) timesteps = dim.x[3] V = output$V$weight by = output$by$weight Y.hat = array(0, dim.y) if (length(dim.y) == 2) { Y.hat = array(0, c(dim.y, timesteps)) } loss = accurate = rep(0, timesteps) dV = dby = 0 for (t in 1:timesteps) { if (length(dim.y) == 2) { target = Y } else { target = Y[,,t] } Ht = (fmodel[[t]]$Ht + bmodel[[t]]$Ht)/2 Y.hat[,,t] = actfunc(sweep(Ht %*% V, 2, by, &#39;+&#39;)) loss[t] = mean(softmax.loss(target, Y.hat[,,t])) accurate[t] = accuracy(target, Y.hat[,,t]) L = (Y.hat[,,t] - target) dH = L %*% t(V) dV = dV + t(Ht) %*% L dby = dby + apply(L, 2, sum) } list(&quot;Y.hat&quot; = Y.hat, &quot;dH&quot; = dH, &quot;dV&quot; = dV, &quot;dby&quot; = dby, &quot;loss&quot; = mean(loss), &quot;accuracy&quot; = mean(accurate)) } Lastly, let us create an alias for our Bidirectional RNN like so (no modification is required for the my.RNN(.) function). my.BiRNN = my.RNN Let us see how the structure looks like given the following data points (X, Y): set.seed(1) t = 5; n = 50; p = 30; h = 20; o = 3 X = array(runif(n * p * t), c(n, p, t)) Y = array(sample(0, size=n * o, replace=TRUE), c(n, o)) Y.idx = sample.int(o, size=n, replace=TRUE) for (i in 1:n) { Y[i, Y.idx[i]] = 1 } # one-hot encoding Let us now use our implementation to train a Bidirectional LSTM model. layers = deep.lstm.layers(X, Y, list(size=20, bidirectional=TRUE), list(size=20, bidirectional=TRUE)) bilstm.model = my.BiRNN(rtype = &quot;lstm&quot;, X=X, Y=Y, layers, epoch=200, eta=0.01) ## [1] &quot;epoch 1 - loss: 1.162, accuracy 0.380 lag time (sec): 0.135&quot; ## [1] &quot;epoch 20 - loss: 0.928, accuracy 0.612 lag time (sec): 0.418&quot; ## [1] &quot;epoch 40 - loss: 0.879, accuracy 0.620 lag time (sec): 0.477&quot; ## [1] &quot;epoch 60 - loss: 0.640, accuracy 0.764 lag time (sec): 0.479&quot; ## [1] &quot;epoch 80 - loss: 0.327, accuracy 0.876 lag time (sec): 0.621&quot; ## [1] &quot;epoch 100 - loss: 0.225, accuracy 0.912 lag time (sec): 0.442&quot; ## [1] &quot;epoch 120 - loss: 0.126, accuracy 0.960 lag time (sec): 0.470&quot; ## [1] &quot;epoch 140 - loss: 0.049, accuracy 0.980 lag time (sec): 0.565&quot; ## [1] &quot;epoch 160 - loss: 0.060, accuracy 0.980 lag time (sec): 0.439&quot; ## [1] &quot;epoch 180 - loss: 0.033, accuracy 0.980 lag time (sec): 0.469&quot; ## [1] &quot;epoch 200 - loss: 0.012, accuracy 1.000 lag time (sec): 0.539&quot; The Loss and Accuracy are plotted in Figure 13.15. We can see that our model can train. We see the final loss resulting in 0.012 with a corresponding accuracy around 100%. x = seq(1, length(bilstm.model$cost)) y = bilstm.model$cost y1 = (y - min(y))/(max(y) - min(y)) y2 = bilstm.model$accuracy plot(NULL, xlim=range(x), ylim=range(0,y1), xlab=&quot;Epoch&quot;, ylab=&quot;Cross-Entropy Loss / Accuracy&quot;, main=&quot;Deep Stacked Bidirectional LSTM Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;, lwd=2) lines(x, y2, col=&quot;navyblue&quot;, lwd=2) Figure 13.15: Deep Stacked Bidirectional LSTM Plot Let us also use our implementation to train a Bidirectional GRU model. layers = deep.gru.layers(X, Y, list(size=20, bidirectional=TRUE), list(size=20, bidirectional=TRUE)) bigru.model = my.BiRNN(rtype = &quot;gru&quot;, X=X, Y=Y, layers, epoch=200, eta=0.01) ## [1] &quot;epoch 1 - loss: 1.406, accuracy 0.380 lag time (sec): 0.090&quot; ## [1] &quot;epoch 20 - loss: 0.973, accuracy 0.424 lag time (sec): 0.328&quot; ## [1] &quot;epoch 40 - loss: 0.972, accuracy 0.472 lag time (sec): 0.404&quot; ## [1] &quot;epoch 60 - loss: 0.788, accuracy 0.616 lag time (sec): 0.341&quot; ## [1] &quot;epoch 80 - loss: 0.664, accuracy 0.704 lag time (sec): 0.348&quot; ## [1] &quot;epoch 100 - loss: 0.387, accuracy 0.856 lag time (sec): 0.410&quot; ## [1] &quot;epoch 120 - loss: 0.431, accuracy 0.796 lag time (sec): 0.336&quot; ## [1] &quot;epoch 140 - loss: 0.253, accuracy 0.896 lag time (sec): 0.344&quot; ## [1] &quot;epoch 160 - loss: 0.222, accuracy 0.928 lag time (sec): 0.354&quot; ## [1] &quot;epoch 180 - loss: 0.139, accuracy 0.956 lag time (sec): 0.423&quot; ## [1] &quot;epoch 200 - loss: 0.098, accuracy 0.964 lag time (sec): 0.344&quot; The Loss and Accuracy are plotted in Figure 13.16. Similarly, our model can train. We see the final loss resulting in 0.098 with an accuracy around 96.4%. x = seq(1, length(bigru.model$cost)) y = bigru.model$cost y1 = (y - min(y))/(max(y) - min(y)) y2 = bigru.model$accuracy plot(NULL, xlim=range(x), ylim=range(0,y1), xlab=&quot;Epoch&quot;, ylab=&quot;Cross-Entropy Loss / Accuracy&quot;, main=&quot;Deep Stacked Bidirectional GRU Plot&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;, lwd=2) lines(x, y2, col=&quot;navyblue&quot;, lwd=2) Figure 13.16: Deep Stacked Bidirectional GRU Plot It is obvious to see that our models perform very well. That is because we iterate through the same dataset. Thus, we may see signs of overfitting to the dataset. While we may not be able to cover many other RNN discussions in this section, it helps to recall a few knobs we highlighted in the CNN section, which can improve the performance of RNN. We mention scheduling, blackout, batch norm, batch size, and many others besides the initialization type and optimization we already covered here. 13.5 Transformer Neural Network (TNN) The Attention mechanism was first introduced in the context of machine translation in a paper published by Dzmitry Bahdanau et al. (2015), followed by another paper published by Minh-Thang Luong et al. (2015) for a variant of the mechanism. Two years later, presented in a paper called Attention is all you need (Vaswani A. et al. 2017), Attention kick-started the emergence of Transformer architectures - also called Transformer Neural Network architectures. 13.5.1 Attention To start this topic, let us first build our fundamental knowledge around Encoder-Decoder architecture, using formulas and referencing diagrams from Bahdanau and Luong (with some modifications for presentation purposes only). As we pointed out in the RNN sections, an Encoder-Decoder model constitutes two separate components: an Encoder and a Decoder. They correspond to the Many-to-One and One-to-Many models. The Encoder component can use an RNN architecture (using LSTM or GRU as an alternative and in a bidirectional fashion) with a series of X as input and a single Encoder Vector as output. The Decoder component takes the Encoder Vector for its initial state and produces a series of Y as output. See Figure 13.17. Figure 13.17: Encoder-Decoder Mathematically, our interest is to produce the final hidden state, e.g. \\(\\mathbf{h_t}\\), from the encoder using the following function: \\[\\begin{align} h_t = f(h_{t-1}, x_t) \\ \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\text{(recall RNN function)} \\end{align}\\] The final hidden state, e.g., \\(\\mathbf{h_t}\\), is then handed over to serve as the initial state \\(\\mathbf{s_0}\\) for the decoder. We use the following function for each decoded output: \\[\\begin{align} y_t = g(s_{t}, y_{t-1}, c)\\ \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\text{(also recall RNN function)} \\end{align}\\] where c is the encoder vector, also called context vector. The symbol \\(\\mathbf{c}\\) should not be confused with the c state of LSTM in case we decide to use that variant of RNN. One known drawback of such a traditional model lies in the fact that the Encoder produces only one single vector representing the entire sentence - sort of a single dot in \\(\\mathbf{d_k}\\) dimensional vector space (Sankar A. 2019). One vector representing a context (or multiple contexts) of the entire sentence may not seem fitting, especially with a very long sentence. So we need a different approach. Take the following simple sentence as an example: \\[\\begin{align*} \\mathbf{\\text{I travel the world in search for the fountain of youth.}} \\end{align*}\\] So instead of feeding the decoder with a single vector representing the entire sentence, a proposed idea is to focus on individual target words, e.g., travel, and figure out which surrounding words are more relevant to the target word travel. For example, the words I, the, and world seem more strongly relevant than the rest. We can then form a vector that may signify some representation about I travel the world for the target word travel. Similarly, we can also form a vector representation of the target word fountain by looking for the closest relevant words such as in, search, for, the, and youth, then feed the decoder with another vector that signifies another kind of representation, this time, about search for the fountain of youth. Thus, herein lies the concept of the Attention mechanism, which is used to draw surrounding words that can pay more attention to target words. We call these surrounding words contextual words, which form contexts of target words. To illustrate the use of the Attention mechanism, we have to modify our diagram in Figure 13.17. The modification is shown in Figure 13.18. Figure 13.18: Encoder-Decoder with Attention Notice that an Attention unit is added to the diagram. The unit forms the core function of the Attention mechanism. Here, all the hidden states {\\(\\mathbf{h_1}, \\mathbf{h_2}, ..., \\mathbf{h_t}\\)} are captured by the Attention unit which are then used to eventually generate distinct contextualized vectors, each denoted by \\(\\mathbf{c_i}\\) representing the context of a selected target word (a timestep in the Encoder). The individual vectors are then fed into their corresponding RNN timesteps in the Decoder. Two popular variants of Attention mechanisms were developed. Bahdanau D. et al. (2015) demonstrated the use of the first Attention mechanism; hereafter, we call this the Bahdanau Attention, regarded as Additive Attention. Luong M. et al. (2015) demonstrated the second variant; hereafter, we call this the Luong Attention, regarded as Multiplicative Attention. We can see the difference between the two mechanisms in Figure 13.18. In the Luong Attention, the individual context vector \\(\\mathbf{c_t}\\) is also added with the output. We show this in the diagram as dashed arrows which are not found in Bahdanau Attention. Additionally, the reason becomes clear why we call the former Additive Attention and the latter as Multiplicative Attention. Notice that the Attention unit takes two inputs, namely, the hidden state \\(\\mathbf{h_t}\\) from the encoder and the hidden state \\(\\mathbf{s_t}\\) from the decoder. The alignment model uses the two states to calculate the attention scores. How we calculate the attention scores depends on the two variants. Let us use Figure 13.19 to illustrate the first variant, Bahdanau attention. Figure 13.19: Attention Unit Mathematically, the use of a single context vector c is replaced by individual distinct context vectors \\(\\mathbf{c_t}\\) like so: \\[\\begin{align} from: y_t = g(s_{t}, y_{t-1}, c) \\ \\ \\ \\ \\ \\ \\ to: y_t = g(s_{t}, y_{t-1}, c_t) \\end{align}\\] It is worth noting that the g(.) function yields the probability of every output word conditioned on both previous outputs and the corresponding context vector as written below: \\[\\begin{align} g(s_{t}, y_{t-1}, c_t) = P(y_t |y_1, .... y_{t-1}, c_t) \\end{align}\\] whereas originally, we are conditioned on a single context vector: \\[\\begin{align} g(s_{t}, y_{t-1}, c) = P(y_t |y_1, .... y_{t-1}, c) \\end{align}\\] We know that the hidden states \\(\\mathbf{s_{t}}\\) are generated by RNN in the Decoder so that each RNN unit feeds from previous hidden states. However, on the other hand, it also feeds from corresponding context vectors \\(\\mathbf{c_i}\\) generated by the Attention unit using the following formula: \\[\\begin{align} c_i = \\sum_{j=1}^t \\alpha_{ij} h_j \\end{align}\\] Here, the symbol alpha (\\(\\alpha\\)) represents the attention weight which is calculated using softmax operation: \\[\\begin{align} \\alpha_{ij} = \\frac{\\mathbf{\\text{exp}}(e_{ij})}{\\sum_k^t \\mathbf{\\text{exp}}(e_{ik})}\\ \\ \\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\text{(softmax)} \\ \\ \\ \\ \\ \\ \\text{where:}\\ \\ \\ \\ \\ \\ \\ \\sum_{j=1} \\alpha_{j} = 1 \\end{align}\\] Now, in Bahdanau Attention, the attention scores denoted by \\(\\mathbf{e_{ij}}\\) are calculated by an alignment function. It is called Additive Attention because both states are parameterized for training, then added together, then tanh is applied before being multiplied by another parameter. \\[\\begin{align} e_{ij} = a(s_{i-1}, h_j) = v_a^\\text{T}\\ \\mathbf{\\text{tanh}}\\left(W_a\\ s_{i-1} + U_a\\ h_j\\right) \\end{align}\\] In Luong Attention, we are given three choices of alignment function to calculate the attention scores: \\[\\begin{align} e_{ij} = score(h_t, \\bar{h}_s) = \\begin{cases} h_t^T \\bar{h}_s &amp; \\text{dot}\\\\ h_t^T W_a \\bar{h}_s &amp; \\text{general}\\\\ v_a^T\\ \\mathbf{\\text{tanh}}\\left(W_a [ h_t; \\bar{h}_s]\\right) &amp; \\text{concat} \\end{cases} \\label{eqn:eqnnumber803} \\end{align}\\] Additionally, Luong Attention also proposes Local Attention in which only a subset of source input is considered for alignment rather than the entire source input. A good example of such implementation is in Skip-gram, which we cover as part of our Word Embedding discussion in a later section in which we discuss the idea of using a window that is controlled by window size and that slides across a series of input throughout the iteration. There are good ideas presented in Luong Attention, such as using monotonic alignment, which considers a few timesteps before and after the target word to construct the window. Alternatively, using predictive alignment based on Gaussian distribution. The idea of predictive alignment is to consider coverage of attentiveness so that we favor neighboring words in proximity and words that are strongly attentive (closer to the mean) though they may be remote. Consider the following sentence: \\[ \\mathbf{\\text{I gave browny, my cute little dog, treats.}} \\] Notice that if our target word is gave, we can form contextual words using I and treats, both of which happen to be not in proximity. \\[ \\mathbf{\\text{I gave treats.}} \\] We leave readers to investigate the below predictive alignment function with gaussian formula: \\[\\begin{align} a_t(s)= \\mathbf{align}(h_t, \\bar{h}_s) \\mathbf{exp}\\left(-\\frac{(s - p_t)^2}{2 \\sigma^2}\\right) \\end{align}\\] We also leave readers to investigate soft and hard Attention mentioned in the Luong M. paper referring to using soft Attention for all patches in an image (see CNN for receptive fields); otherwise, using hard Attention one patch a time. In the next section, we shall use dot product for our alignment function using Self-Attention to demonstrate Global Attention. 13.5.2 Self-Attention and Trainability We saw a small glimpse of how the Attention mechanism works, though that is not the complete picture. For example, we are yet to cover how the mechanism can be trained by introducing learnable parameters - an essential property of Attention. In this section, let us briefly take a closer look at Attention unit under the hood. Afterwhich, we discuss the trainability of Attention units. Let us use Figure 13.19 to illustrate: Figure 13.20: Attention Unit In the figure, we see a layered RNN stack producing a sequence of vectors denoted by \\(\\mathbf{\\vec{h}^{(2)}}\\) from the second RNN layer. This sequence of vectors is fed through an Attention unit. It helps to point out that the diagram reflects a Self-attention unit. By self, each token in the series is paired with the other. The idea is to derive some contextual relationship amongst each other - although being strongly attentive to oneself becomes implicitly part of the computation. Now, in a paper titled Attention is all we need, published by Ashish Vaswani et al. (2017), the trainability of an Attention unit is made possible with the introduction of attention coefficients - learnable parameters (weights) - constructed in the form of three matrices which the paper refers to as Q matrix, K matrix, and V matrix. To illustrate, let us use Figure 13.21. Figure 13.21: Attention with Weights The q, k, and v are instances manifested from the same embedding source that are given roles to play, namely query, keys, values. To give a better picture, let us continue to use our previous example sentence: \\[ \\underbrace{\\mathbf{\\text{I }}}_{\\mathbf{h1}} \\underbrace{\\mathbf{\\text{ travel }}}_{\\mathbf{h2}} \\underbrace{\\mathbf{\\text{ the }}}_{\\mathbf{h3}} \\underbrace{\\mathbf{\\text{ world }}}_{\\mathbf{h4}} \\underbrace{\\mathbf{\\text{ in }}}_{\\mathbf{h5}} \\underbrace{\\mathbf{\\text{ search }}}_{\\mathbf{h6}} \\underbrace{\\mathbf{\\text{ for }}}_{\\mathbf{h7}} \\underbrace{\\mathbf{\\text{ the }}}_{\\mathbf{h8}} \\underbrace{\\mathbf{\\text{ fountain }}}_{\\mathbf{h9}} \\underbrace{\\mathbf{\\text{ of }}}_{\\mathbf{h10}} \\underbrace{\\mathbf{\\text{ youth }}}_{\\mathbf{h11}} \\] Based on Retrieval Information theory, particularly in search engine systems, we supply a query keyword to the engine and expect the engine to respond with a list of relevant documents. In the context of Self-Attention, we supply a query keyword in search for contextual words in a list of words called keys. The idea is to know which of these keys will pay more attention to the query. For a better intuition, let us step through the operations. First, for illustration, let us cut the sentence above to only the subject line, I travel the world, giving us only about four tokens (or word embeddings). Assume for a moment that our embeddings have five dimensions (dk=5). For now, let us randomly assign some continuous numbers. options(digits=8) set.seed(12) t = 4 # number of tokens (embedding) dk = 5 # dimension per token random &lt;- function(dk) { round(runif(n=dk, min=0.100, max=0.900), 3) } # assume hidden states produced by RNN for each word h1 = random(dk) # I h2 = random(dk) # travel h3 = random(dk) # the h4 = random(dk) # world (h = rbind(h1, h2, h3, h4)) ## [,1] [,2] [,3] [,4] [,5] ## h1 0.155 0.754 0.854 0.316 0.235 ## h2 0.127 0.243 0.613 0.118 0.107 ## h3 0.414 0.751 0.401 0.405 0.312 ## h4 0.451 0.466 0.533 0.633 0.190 Second, let us query the keys to obtain scores. Mathematically, we have the following expression, including the normalizer: \\[\\begin{align} \\text{scaled scores} = \\frac{(\\text{query})\\ (\\text{keys}^\\text{T})} {\\sqrt{d_k}} \\end{align}\\] query = keys = h scores = (query) %*% t(keys) scaled.scores = round(scores / sqrt(dk), 3) colnames(scaled.scores) = paste0(&quot;q&quot;, seq(1,t)) rownames(scaled.scores) = paste0(&quot;s&quot;, seq(1,t)) scaled.scores ## q1 q2 q3 q4 ## s1 0.661 0.353 0.525 0.501 ## s2 0.353 0.213 0.251 0.265 ## s3 0.525 0.251 0.518 0.477 ## s4 0.501 0.265 0.477 0.510 Third, we perform softmax for each query like so: \\[\\begin{align} \\alpha_i = \\frac{exp(s_i)} {\\sum_j^t(exp(s_{i,j}))} \\end{align}\\] attention.weights = softmax(scaled.scores) colnames(attention.weights) = paste0(&quot;q&quot;, seq(1,t)) rownames(attention.weights) = paste0(&quot;w&quot;, seq(1,t)) attention.weights ## q1 q2 q3 q4 ## w1 0.289 0.212 0.252 0.246 ## w2 0.271 0.236 0.245 0.248 ## w3 0.270 0.205 0.268 0.257 ## w4 0.265 0.209 0.259 0.267 So that if we add the softmax scores, we should get a 1 for all attention weights. \\[\\begin{align} \\sum_i^t \\left( \\alpha_i\\right) = 1 \\end{align}\\] round(apply(attention.weights, 1, sum), 2) ## w1 w2 w3 w4 ## 1 1 1 1 Finally, we obtain the contextual embeddings using the below formula per embedding. Alternatively, we can perform matrix multiplication instead. \\[\\begin{align} c_i = \\sum \\alpha_i \\cdot v_i \\end{align}\\] values = h context = round(attention.weights %*% (values), 3) colnames(context) = paste0(&quot;f&quot;, seq(1,dk)) rownames(context) = paste0(&quot;c&quot;, seq(1,t)) t(context) ## c1 c2 c3 c4 ## f1 0.287 0.285 0.295 0.295 ## f2 0.573 0.561 0.574 0.570 ## f3 0.609 0.607 0.601 0.601 ## f4 0.374 0.370 0.381 0.382 ## f5 0.216 0.212 0.218 0.216 Using learnable weights, we first initialize three matrices (Q, K, V) using a random uniform distribution. Q = matrix(runif(n=dk * dk, min=-0.5, max=0.5), nrow=dk) K = matrix(runif(n=dk * dk, min=-0.5, max=0.5), nrow=dk) V = matrix(runif(n=dk * dk, min=-0.5, max=0.5), nrow=dk) We then multiply q, k, and v embeddings with the corresponding matrix like so (noting that q, k, and v are sourced from e): q = k = v = h query = q %*% Q keys = k %*% K values = v %*% V Note that we created q, k, v as three separate instances for demonstration. In actual implementation, we may use e for the three computations instead to save memory. We go through the same operations as above to get our context using the query, keys, and values. Here, we try to implement our example of an attention function: attention.unit &lt;- function(query, keys, values) { t = ncol(query) # t-tokens dk = nrow(query) # k-dimension scores = t(query) %*% keys scaled.scores = scores / sqrt(dk) # scaled attention.weights = softmax(scaled.scores) # column-wise context = round(attention.weights %*% t(values), 3) colnames(context) = paste0(&quot;f&quot;, seq(1,dk)) rownames(context) = paste0(&quot;c&quot;, seq(1,t)) t(context) # transpose } attention.unit(query, keys, values) ## c1 c2 c3 c4 c5 ## f1 -0.032 -0.026 -0.075 -0.067 -0.063 ## f2 -0.010 -0.006 -0.031 -0.027 -0.024 ## f3 -0.035 -0.029 -0.079 -0.069 -0.066 ## f4 -0.086 -0.081 -0.123 -0.115 -0.111 We should note that the context produced by our attention function is still raw because we relied on the initial values of the learnable weights. Similar to our discussion on MLP and other types of networks, we need to plug the attention unit into a neural network to allow gradients to flow through the attention layer through backpropagation. We will discuss this further once we get to the Transformer architecture. 13.5.3 Multi-Head Attention Multi-Head Attention emphasizes on having multiple heads of attention units, each head is trained using a set of Q, K, and V matrices containing the trainable parameters. See Figure 13.22. Figure 13.22: Multi-Head Attention Having multi-head attention comes from the fact that a single sentence may contain multiple contexts. So, for example, we can break our example sentence in the previous section into three possible contexts like so: \\[ \\begin{array}{ll} \\mathbf{\\text{context 1}} &amp;\\rightarrow\\ \\text{I travel the world}\\\\ \\mathbf{\\text{context 2}} &amp;\\rightarrow\\ \\text{search for the fountain}\\\\ \\mathbf{\\text{context 3}} &amp;\\rightarrow\\ \\text{fountain of youth} \\end{array} \\] The three contexts may require us to create three attention heads. We can choose to use even more heads if we feel that there are more hidden contexts (perhaps we can do so heuristically). Each context delivers a set of Q, K, V matrices containing learned parameters. In terms of computation, notice in the figure that each of the three instances of the embedding, namely q, k, and v, runs through matrix multiplication with each corresponding matrix. Because we are dealing with three attention heads for the three contexts in our example above, it means that q gets multiplied with Q1, Q2, and Q3 matrices, resulting in three score matrices. Similarly, k gets multiplied with K1, K2, and K3 matrices, resulting in three score matrices. The same applies to v. Next, the scores are scaled using a normalizer, e.g., \\(\\sqrt{d_k}\\), followed by softmax to derive the attention weights - or attention probabilities. The final context is obtained by performing another matrix multiplication between the output of the softmax and the output of the score matrices from v and the three V matrices. For example, in the multi-head attention, the softmax accepts the following input: \\[\\begin{align} \\text{attention}(Q,K,V) = \\text{softmax}\\left(\\frac{QK^\\text{T}}{\\sqrt{d_k}}\\right)\\cdot V \\end{align}\\] To give an example of how we use Multi-head Attention, suppose we have the number of heads equal to 5. Let us randomly initialize the Q, K, and V tensors, noting that the tensors have dimension \\((k \\times k \\times h)\\): h = 5; t = 4 # here, h equals no. of heads, t equals no. of tokens Q = array(runif(n=dk * dk, min=-0.5, max=0.5), c(dk, dk, h)) K = array(runif(n=dk * dk, min=-0.5, max=0.5), c(dk, dk, h)) V = array(runif(n=dk * dk, min=-0.5, max=0.5), c(dk, dk, h)) K[,,1] # display an initialized K matrix for the first head. ## [,1] [,2] [,3] [,4] ## [1,] 0.15782952 0.158294656 0.30144395 0.389498414 ## [2,] 0.38407710 0.078136669 -0.28551430 0.334255804 ## [3,] 0.33999093 0.155190383 0.47465562 0.140585024 ## [4,] 0.44216319 0.297951073 -0.32631462 0.373908170 ## [5,] 0.30393706 0.069033058 -0.48419310 0.094678544 ## [,5] ## [1,] -0.20034764 ## [2,] 0.34011628 ## [3,] 0.09883009 ## [4,] 0.21031208 ## [5,] -0.16618633 We then multiply q, k, and v embeddings with the corresponding matrix like so (noting that q, k, and v are sourced from e: query = keys = values = array(0, c(dk, t, h)) # create the q,k,v structures for (i in 1:h) { query[,,i] = q %*% Q[,,i] keys[,,i] = k %*% K[,,i] values[,,i] = v %*% V[,,i] } Let us now invoke our attention.unit function. options(width=56) contextualized = array(0, c(dk, t, h)) for(i in 1:h) { contextualized[,,i] = attention.unit(query[,,i], keys[,,i], values[,,i]) } The dimension is [5, 4, 5]. Note that the output of the multi-head attention is a list of contextualized matrices. The list has a size equal to the number of heads. To obtain only one contextualized matrix with the same dimension as the original embedding, we need to merge the list. The proposed merge approach is to concatenate and then run through a dense FC layer, reshaping the output in the output layer, then using softmax past the output layer. Multihead Attention is seen in the paper by Ashish Vaswani et al. (2017). It is used as one of the core components of a novel Attention-based architecture called Transformer. Before we jump to Transformers, let us discuss a few essential topics, focusing on sequence-to-sequence applications. Among many others, we need to build our intuition around the following topics: Word Embedding - numerical representation of individual sequence elements (e.g., words), Positional Embedding - numerical representation of the position of sequence elements, and Sequence Alignment - alignment between sequences if we are to translate one sequence to another. 13.5.4 Word Embedding A few concepts are introduced in Chapter 11 (Computational Learning III), particularly in the context of Natural Language Processing (NLP). One of the applications of NLP is around search engines in which we deal with a given query and a list of documents available to search. Here, we start with pre-processing of texts to extract bag of words (BoW) using tokenization, case-sensitivity, stopwords, N-grams, stemming, lemmatization, and so on. While such a bag of words is merely an extracted list of terms, it does not necessarily carry any numerical representation. Instead, we measure the relevance of each term (or relevance of a query made of terms) by way of ranking techniques. Okapi BM25 technique is introduced in the Chapter mentioned, which is a rather advanced variant of the generic TF-IDF technique covered in the field of Information Retrieval. The idea is to be able to rank terms found both in a given query and also in a list of documents. Fundamentally, this is how we score a term, or in other words, how we cast numerical values of terms. Mathematically, we cast words into their numerical representations in the form of Vector Space Model (SVM). From an engineering perspective, we can use Document Term Matrix (DTM) or Term Document Matrix (TDM) interchangeably to compare documents. Both matrices may contain simple counts of terms or TF-IDF results. From there, we use cosine similarity to identify the relationship of documents. See Figure 13.23. Figure 13.23: Vector Space Model (VSM) with TF-IDF scores From the idea of Vector Space Model (VSM), we carry the same intuition for Word Embedding. Perhaps, the fundamental definition of Word Embedding exemplifies the idea that words can be translated to their numeric representation. To further build the intuition around this, imagine a matching site that allows users to enter a list of personality traits and allows its matching engine to pair individual users based on traits. Here, we may compile traits such as athletic, outgoing, adventurous, likes movies, and others. See Figure 13.24. From this perspective, we can readily say that a person is characterized or represented by a vector of trait features. Figure 13.24: Word Embedding) Suppose that, by some algorithm, the matching engine can assign numbers to each trait so that a particular user labeled as You is 0.85 athletic, 0.61 outgoing, and on. Similarly, a particular unknown individual labeled as Person resembles almost the same weights as You in terms of traits. Ideally, using a scoring formula such as cosine similarity, we can suggest that You and Person match by proximity. Moreover, imagine by the same token that if there is another user labeled as Elf whose traits closely match the combined traits of You and Person, we now begin to see a cluster of users with similar characteristics (visualizing by reducing dimensionality to two using PCA). In the context of semantic analysis, the meaning of a word (its context) does not have to be derived only by how the word can be described (based on traits). Instead, it can also be derived from how the word is used in a sequence of words based on its surrounding words. This idea comes from the famous quote below used by other literature: \\[ \\text{&quot;You shall know a word by the company it keeps&quot;, } \\ \\ \\ \\ \\ \\ \\ \\mathbf{\\text{John Rupert Firth.}} \\] In Chapter 11 (Computational Learning III), we introduce two techniques that allow us to capture the semantic essence of texts, namely Probabilistic Latent Semantic Analysis (pLSA) and Latent Dirichlet Allocation (LDA). Mostly, the techniques are used for Topic Modeling relying on probabilistic language modeling to create a contextualized distributed representation of words (in particular using Dirichlet Distributions for LDA) to highlight probabilities of topic words. Both techniques introduce the use of latent parameters that follow certain distributions. We can say that Topic Modeling creates clustered contextualized word representation (Laure Thompson et al. 1997). One important criterion that is lacking when dealing with traditional techniques such as pLSA and LDA is their learnability or trainability. When it comes to that, Neural Probabilistic Language Modeling (NPLM) comes into the picture, introduced by Yoshua Bengio et al. (2003). This model uses Neural Network to learn Word Embeddings. Figure 13.25 shows a neural architecture depicting the original NPLM neural network architecture from Bengio’s paper. Figure 13.25: Neural Probabilistic Language Model The model expects a series of word indexes for which we perform a table lookup against a matrix C which can also serve as a Word Embedding lookup. We expect the lookup to return a set of feature vectors, each vector corresponding to each word index. We then concatenate the set of feature vectors. This embedding gets forwarded to the hidden layer for matrix multiplication with a matrix of learnable weight parameters denoted by (\\(\\omega_h\\)). Then a non-linearity (tanh) is applied, followed by softmax. This architectural process demonstrates a shallow MLP with characteristics of a one hidden-layer fully-connected neural network architecture that performs the usual forward feed and backpropagation. Many other architectural variants are motivated by the NPLM design, laid by Bengio’s paper. Revisiting our CNN, variant designs of NPLM may utilize different combinations of non-linearity, dropouts, layer normalization, optimization, residual nets, and on (Sun S., Iyyer M. 2021). One of the more advanced techniques motivated by NLPM architecture is called Word2vec. It was introduced by Tomas Mikolov et al. (2013). The grand idea follows NLPM in that it associates a word with surrounding words that serve as clues to predict context. Now, the type of Word Embedding created by Word2vec is a set of feature vectors that are learned or trained. The neural architecture of Word2vec is designed to utilize two Word Embedding approaches, namely Continuous Bag of Words (CBOW) and Skip-Gram. A CBOW model is trained to predict the probability of a target word to occur based on a given context. For example, given the context, I travel the world in search for the fountain of [____], the most probable target word is youth from a list of word distributions, e.g., apples, sky, youth. The words apples and sky will receive the lowest probability scores. On the other hand, the Skip-Gram model does the reverse in that it is trained to predict the context (list of contextual words) from a given target word. In other words, in Skip-Gram, it might make sense to predict surrounding words such as I travel the world in search for the fountain of [____], given the target word youth. To illustrate Skip-Gram, let us use Figure 13.26. Figure 13.26: Skip-Gram Architecture The neural architecture of Skip-Gram follows a shallow fully-connected MLP in which we have V-dimension inputs (as one-hot encoding) in the input layer, H neurons in the hidden layer, and O units for the output. Note that a window is used to define the scope of the surrounding words, the target word being at the center. The window is monotonic (see Luong Attention), and a hyperparameter window size can be used to adjust the number of surrounding words. This same number becomes the size of the O units which contains the number of predicted contextual words (the surrounding words). The window slides iteratively across the series of words until the entire series is accommodated. The size of the Hidden layer is defined based on the number of neurons to use for the training. The output of the forward feed at every iteration is a matrix that consists of a predicted set of probability vectors. Each vector represents a predicted contextual word that is then evaluated against the context clues, optimizing the Loss function. The final output of the Skip-Gram itself is a trained model consisting of the optimized weighted matrices (embedding matrix and context matrix). The matrices contain learnable parameters. Additionally, the output layer uses softmax to calculate word probabilities. It helps to note, however, that softmax is known to be computationally expensive because it involves calculating a matrix that carries the vocabulary (V) size. Furthermore, it can get very large. For that reason, Word2vec Skip-gram also demonstrates using negative sampling with sigmoid function as an alternative, switching the classification to binary (second paper published by (Mikolov T. 2013). Other literature emphasizes the same alternative, evaluating the benefits (Long Chen et al. 2018). We leave readers to investigate other tools such as Inpatient2vec, doc2vec, and med2vec intended for specific Word Embeddings while providing a multi-layer medical representation learning for patients, diagnostics, and medical codes, respectively. Such representations provide context to targetted medical terms. Also, it helps to investigate Global Vectors for Word Representation (GloVe). 13.5.5 Positional Embedding With the emergence of Attention, the sequential approach, seen in RNN (e.g., LSTM and GRU), is replaced by an architecture that handles a series of words in parallel. This architecture, however, loses the order information. Therefore, that is where Positional Embedding is required. Positional Embedding is a learned numerical representation of the position and order of words in a given sequence of words. This information is not captured in vanilla Word Embeddings. Thus, we need to add the information to the Word Embedding. \\[ \\mathbf{\\text{Word Embedding}} + \\mathbf{\\text{Positional Embedding}} = \\mathbf{\\text{Position-Aware Word Embedding}} \\] In terms of preserving position information, the first thoughts were using word indices or reducing the word indices to the range 0 and 1 using fractions. However, for the former, a large index at the end of a sequence tends to dominate the value of the Word Embedding when added. For the latter, even though the decimal value is the same, e.g., 0.25, a position in the form 1/4 (position one from a sequence of 4 words) does not equal a position in the form 2/8 (position two from a sequence of 8 words). Therefore, a novel approach introduced is to use Sinusoidal Position Encoding (Vaswani A. et al. 2017). The big idea is to use frequency produced by sine and cosine to yield a position representation. Below is the expression used (derivation not included): \\[\\begin{align} PE(pos, 2i) = sin\\left(\\frac{pos}{10000^{2i/dmodel}}\\right) \\ \\ \\ \\ \\ \\ \\ \\ \\ PE(pos, 2i+1) = cos\\left(\\frac{pos}{10000^{2i/dmodel}}\\right) \\end{align}\\] An example implementation is written below. sinusoid.encoding &lt;- function(npos, emb.dim, pos.range) { angle &lt;- function(pos, i) { pos / 10000^((2 * i) / emb.dim)} pos.seq = seq(0, npos-1, length.out=pos.range) emb.seq = seq(0, emb.dim-1, length.out=emb.dim) pos_embedding = array(0, c(pos.range , emb.dim)) for (p in 1:pos.range) { pos_embedding[p,] = angle(pos.seq[p], emb.seq) } sini = seq(from=0, to=emb.dim-1, by=2) # 2i cosi = seq(from=1, to=emb.dim-1, by=2) # 2i + 1 pos_embedding[,sini] = sin(pos_embedding[,sini]) pos_embedding[,cosi] = cos(pos_embedding[,cosi]) pos_embedding } Assume the number of positions is 20 with an embedding size of 40. We should be able to produce a rotary matrix to be then added to a word embedding of the same dimension. npos = 20 pos.range = 200 # only needed to plot a smoother curve v = sinusoid.encoding(npos = npos, emb.dim=40, pos.range=pos.range) data.frame(v[1:10,1:3]) # display only 10x3 portion of the matrix ## X1 X2 X3 ## 1 1.00000000 0.000000000 1.00000000 ## 2 0.99544550 0.060205727 0.99927770 ## 3 0.98182347 0.120193027 0.99711184 ## 4 0.95925801 0.179744265 0.99350554 ## 5 0.92795465 0.238643386 0.98846403 ## 6 0.88819855 0.296676704 0.98199457 ## 7 0.84035184 0.353633673 0.97410652 ## 8 0.78485036 0.409307652 0.96481128 ## 9 0.72219967 0.463496655 0.95412226 ## 10 0.65297046 0.516004082 0.94205492 A plot of the matrix is demonstrated in Figure . Here, we choose the first four elements (four dimensions) of an embedding. x = seq(0, npos-1, length.out=pos.range) y = v plot(NULL, xlim=range(x), ylim=range(y), ylab=&quot;Embedding Dimension (i)&quot;, xlab=&quot;Position (pos)&quot;, main=&quot;Sinusoidal Position Encoding&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, v[,1], col=&quot;darksalmon&quot;, lwd=2, lty=1) # 2i (sine) lines(x, v[,2], col=&quot;navyblue&quot;, lwd=2, lty=3) # 2i+1 (cosine) lines(x, v[,3], col=&quot;deepskyblue&quot;, lwd=2, lty=3) # 2i (sine) lines(x, v[,4], col=&quot;brown&quot;, lwd=2, lty=3) # 2i+1 (cosine) abline(v=c(0,15), col=&quot;red&quot;, lty=2) points(c(0,0), c(0,1), pch=16, col=&quot;black&quot;) points(c(15,15,15,15), c(-0.75, 0, 0.95, -0.58), pch=16, col=&quot;black&quot;) text(14, -0.75, &quot;i=1&quot;); text(16, 0.00, &quot;i=2&quot;) text(16, 0.92, &quot;i=3&quot;); text(14, -0.58, &quot;i=4&quot;) Figure 13.27: Sinusoidal Position Encoding Notice that the absolute position of a word is reduced to a sinusoidal value that ranges between -1 and 1. For example, in the plot for the first dimension (i=1), the position of the first word at zero renders a frequency value of 1, but the same word at position 15 has a frequency value of -0.75. From here, we can add the derived positional embeddings to the word embeddings, so that assumes we have a word embedding of vector size 40 (40-dimensional features), \\[ \\begin{array}{ll} \\text{W1} &amp;= \\text{[0.734, 0.123, 0.512, ..., 0.111]}\\ \\ \\ \\ (1 \\times 40 )\\\\ \\text{W2} &amp;= \\text{[0.813, 0.022, 0.762, ..., 0.212]}\\ \\ \\ \\ (1 \\times 40 ) \\\\ &amp;... \\\\ \\text{W15} &amp;= \\text{[0.544, 0.653, 0.912, ..., 0.433]}\\ \\ \\ \\ (1 \\times 40 ) \\end{array} \\] we can then, as an example, add the vector of the first dimension like so: \\[\\begin{align*} \\underbrace{\\text{[0.734, 0.123, 0.512, ..., 0.111]}}_{\\text{Word Embedding}} + \\underbrace{\\text{[1.000, 0.653, -0.241, ..., 0.988]}}_{\\text{Positional Embedding}} = \\\\ \\underbrace{[0.734, 0.080, ..., 0.110]}_{\\text{Position-aware Word Embedding}} \\end{align*}\\] Alternatively, instead of dealing with absolute positions such as represented by sinusoidal embeddings, we can deal with relative position embedding. The idea is to represent the position in calculating the location of a word relevant to the location of other neighboring words. In other words, we measure the distance relationship between words. Below is a sample implementation of deriving distances based on the difference between absolute positions. The resulting distances are shown in the Toeplitz matrix form below: distance &lt;- function(w1, w2) { abs(w2) - abs(w1) } w = word.indices = c(0,1,2,3,4,5,6) n = length(word.indices) m = matrix(rep(0, n * n), nrow=n) for (i in 1:n) { m[i, ] = distance(w[i], w) } m ## [,1] [,2] [,3] [,4] [,5] [,6] [,7] ## [1,] 0 1 2 3 4 5 6 ## [2,] -1 0 1 2 3 4 5 ## [3,] -2 -1 0 1 2 3 4 ## [4,] -3 -2 -1 0 1 2 3 ## [5,] -4 -3 -2 -1 0 1 2 ## [6,] -5 -4 -3 -2 -1 0 1 ## [7,] -6 -5 -4 -3 -2 -1 0 The matrix above contains scalar distances. In terms of embeddings or vectors, the resulting distance is illustrated in Figure 13.28. Figure 13.28: Relative Position Embedding Each word embedding is paired to itself and every other word in the sequence in a pair-wise manner for which a relative distance representation is formed. See the table in Figure 13.29. Figure 13.29: Pairwise Relative Relationship Notice that if we pair two embeddings (two vectors), the result is a vector, e.g. (\\(\\vec{\\alpha_{1,1}}\\)), assuming we pair by element-wise multiplication. Hence, each element in the table in Figure 13.29 represents a vector. Furthermore, if we are to derive the relative position embedding for \\(\\mathbf{\\vec{e_1}}\\), we have to stack the vectors \\(\\vec{\\alpha_{1,1}}, \\vec{\\alpha_{2,1}}, ..., \\vec{\\alpha_{t,1}}\\) and add the vectors element-wise to obtain the vector \\(\\vec{\\alpha_1}\\) which we then add to the first word embedding, e.g. \\(\\vec{e_1}\\), to get a position-aware embedding. Here, we choose to use addition for the stacked vectors to demonstrate how to get the final relative position embedding. However, it may not be a good idea to stack multiple vectors and add them to the vanilla embedding - this may dilute the influence of the Word Embedding over the Position Embedding. An alternative approach is to use the Attention mechanism to obtain Relative Position Embedding. This idea is popularized by Peter Shaw et al. (2018) and by Zhiheng Huang et al. (2020). Here, we perform a dot-product between a matrix and its transpose. The matrix, denoted by (\\(\\mathbf{e}\\)), contains the embeddings (in column-wise fashion). See Figure 13.30 Figure 13.30: Relative Position using Attention Mechanism After performing the dot product, the resulting matrix containing the re-weighted values or score values denoted by the symbol alpha (\\(\\alpha\\)) is then multiplied by the original matrix, e.g. (\\(\\mathbf{e}\\)). The operation is also a dot product calculation. Note that our vectors are arranged in a column-wise fashion; thus, the expression below is written as: \\[\\begin{align} \\alpha = e^\\text{T}e\\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ c = \\alpha \\times e^\\text{T} \\end{align}\\] To illustrate, let us create an input matrix with 3 word embeddings that are arranged in column-wise fashion and in sequence, e.g. { \\(\\mathbf{\\vec{e_1}}\\), \\(\\mathbf{\\vec{e_2}}\\), \\(\\mathbf{\\vec{e_3}}\\) }, so that, as an example, the first embedding, e.g. (\\(\\mathbf{\\vec{e_1}}\\)), contains three contextual features (3 dimensions): e = (array(seq(1,9), c(3,3))) colnames(e) = c(&quot;e1&quot;, &quot;e2&quot;, &quot;e3&quot;) rownames(e) = c(&quot;feature1&quot;, &quot;feature2&quot;, &quot;feature3&quot;) e ## e1 e2 e3 ## feature1 1 4 7 ## feature2 2 5 8 ## feature3 3 6 9 We now perform dot product to get some kind of score values like so. alpha = t(e) %*% (e) # dot product colnames(alpha) = c(&quot;a1&quot;, &quot;a2&quot;, &quot;a3&quot;) # scores alpha ## a1 a2 a3 ## e1 14 32 50 ## e2 32 77 122 ## e3 50 122 194 Afterwhich, we then obtain a vector output, e.g., \\(\\mathbf{\\vec{c_1}}\\), by multiplying each word embedding by each scalar score using the below expression: \\[\\begin{align} c_i = \\sum_j \\left(a_{i, j} \\times \\vec{e_j}\\right) \\end{align}\\] We can expand this to show the following solution, along with a simple implementation: \\[\\begin{align} \\vec{c_1} &amp;= \\left(a_{1,1} \\times \\vec{e_1}\\right) + \\left(a_{1,2} \\times \\vec{e_2}\\right) + \\left(a_{1,3} \\times \\vec{e_3}\\right) \\label{eqn:eqnnumber804}\\\\ &amp;= 14 \\times \\left[\\begin{array}{l}1 \\\\ 2 \\\\ 3\\end{array}\\right] + 32 \\times \\left[\\begin{array}{l}4 \\\\ 5 \\\\ 6\\end{array}\\right] + 50 \\times \\left[\\begin{array}{l}7 \\\\ 8 \\\\ 9\\end{array}\\right] \\nonumber \\\\ &amp;= \\left[\\begin{array}{l}492 \\\\ 588 \\\\ 684\\end{array}\\right] \\nonumber \\end{align}\\] a1 = alpha[,1] # scores (c1 = a1[1] * e[,1] + a1[2] * e[,2] + a1[3] * e[,3]) ## feature1 feature2 feature3 ## 492 588 684 We perform the same operations for \\(\\mathbf{\\vec{c_2}}\\), \\[\\begin{align} \\vec{c_2} = \\left(a_{2,1} \\times \\vec{e_1}\\right) + \\left(a_{2,2} \\times \\vec{e_2}\\right) + \\left(a_{2,3} \\times \\vec{e_3}\\right) \\end{align}\\] a2 = alpha[,2] (c2 = a2[1] * e[,1] + a2[2] * e[,2] + a2[3] * e[,3]) ## feature1 feature2 feature3 ## 1194 1425 1656 and the same operations for \\(\\mathbf{\\vec{c_3}}\\). \\[\\begin{align} \\vec{c_3} = \\left(a_{3,1} \\times \\vec{e_1}\\right) + \\left(a_{3,2} \\times \\vec{e_2}\\right) + \\left(a_{3,3} \\times \\vec{e_3}\\right) \\end{align}\\] a3 = alpha[,3] (c3 = a3[1] * e[,1] + a3[2] * e[,2] + a3[3] * e[,3]) ## feature1 feature2 feature3 ## 1896 2262 2628 Alternatively, we can instead use a dot product between the two matrices like so: c = (alpha) %*% t(e) rownames(c) = c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;) colnames(c) = c(&quot;feature1&quot;, &quot;feature2&quot;, &quot;feature3&quot;) t(c) ## c1 c2 c3 ## feature1 492 1194 1896 ## feature2 588 1425 2262 ## feature3 684 1656 2628 The vectors {\\(\\mathbf{\\vec{c_1}}\\), \\(\\mathbf{\\vec{c_2}}\\), \\(\\mathbf{\\vec{c_3}}\\)} represent the final Location-aware word Embeddings. In the examples above, we focused on the operations only, but we can add other improvements such as normalization for numerical stability. \\[\\begin{align} \\alpha = \\text{normalize}(e^\\text{T}e)\\ \\ \\ \\ \\rightarrow \\ \\ \\ \\ \\ c = \\alpha \\times e^\\text{T} \\end{align}\\] 13.5.6 Sequence Alignment Alignment is a consideration that applies mostly to machine translation. For example, take the following sequence-to-sequence translation: \\[ \\begin{array}{ll} \\mathbf{\\text{encoder sequence}} &amp;= \\underbrace{\\text{word}_{(1)}, \\text{word}_{(2)}}_{\\text{context 1}}, \\underbrace{\\text{word}_{(3)}}_{\\text{context 2}}, \\underbrace{\\text{word}_{(4)}, \\text{word}_{(5)}, \\text{word}_{(6)}}_{\\text{context 3}},\\underbrace{\\text{word}_{(7)}}_{\\text{context 4}}\\\\ \\\\ &amp;\\mathbf{\\text{encode-decode (translation)}}\\\\ \\\\ \\mathbf{\\text{decoder sequence}} &amp;= \\underbrace{\\text{word}_{(1)}}_{\\text{context 1}}, \\underbrace{\\text{word}_{(2)}, \\text{word}_{(3)}}_{\\text{context 2}}, \\underbrace{\\text{word}_{(4)}, \\text{word}_{(5)}}_{\\text{context 3}}, \\underbrace{\\text{word}_{(6)}}_{\\text{context 4}}, \\underbrace{\\text{word}_{(7)}}_{\\text{context 5}}\\\\ \\end{array} \\] Note in the example above that words are grouped based on context. In previous discussions, we used alignment scores to identify words that are more aligned to a target word in terms of relevance or attentiveness. Such alignment scores are differently calculated as presented in Luong paper. So far, our discussion around alignment only emphasizes the encoder side. However, we also have to consider alignment on the side of the decoder. We have to be able to map both alignments to help with the translation. It, therefore, makes sense to use the Attention mechanism on both sides. Thus, one that enforces the idea is shown in the Transformer architecture. 13.5.7 Transformer Architectures Like CNN architectures, a few TNN architectures made a name for themselves, namely Bert, RoBERTa, and GPT. However, the one architecture that pioneered it is the Transformer, introduced in the paper titled Attention is all you need. See Figure 13.31 (Vaswani A. et al 2017). Figure 13.31: Transformer Architecture The architecture diagram in the figure exposes an Encoder-Decoder model architecture. The Encoder and Decoder components are enclosed with dashed boxes. The paper treats the two components as layers. So then, we can replicate and stack each layer N times, as shown in the diagram. Additionally, the paper breaks the components into sub-layers, consisting of either a Multi-head Attention piece with Add &amp; Norm piece or a Feed Forward piece with Add &amp; Norm piece. Therefore, there are two sub-layers in the Encoding layer, and there are three sub-layers in the Decoding layer, as pointed out in the paper. A glance at the architecture shows that all the pieces in the architecture are familiar, as we have covered them in previous chapters and previous sections of this chapter. For example, Normalization in Batch or Layer mode is discussed in CNN. Then, Feed Forward is covered in MLP. The discussion extends to CNN, ResNet, and RNN covering Back Propagation for training. We also covered Word Embeddings and Positional Embeddings in the previous sections. The paper concludes that the need for RNN is not required in that Attention is all that is needed. The Transformer architecture re-introduces old concepts and, at the same time, introduces new concepts. Moreover, like CNN and other networks, different pieces can be put together to form all sorts of neural network designs. Therefore, this means that Attention and Positional encoding can be considered another set of new gadgets that we can keep in our toolbox. After all, there is no limit to how far deep we can go with neural networks in terms of architectural design, which requires perhaps just a little touch of creativity, among many others. 13.6 Applications using TNN (and RNN) We know that RNN and TNN are the de facto modeling solutions for many sequence-based applications. However, among many other applications, a few typical applications demonstrate the effectiveness of neural networks, namely Speech recognition, Voice recognition, Emotion and Excitement recognition, Gesture recognition, and Handwriting recognition. This section first covers Speech Recognition, emphasizing feature extraction, which introduces MFCC. Then, it briefly introduces CTC used for alignment as an extension after RNN or TNN modeling. A good source of information for CTC is the HMM algorithms used in the Bayesian model section in Chapter 8 (Bayesian Computation II). 13.6.1 Speech Recognition Smart Assistant applications like Siri and Alexa are known in the consumer world to date as applications that allow consumers to converse with electronic gadgets for assistance. This section describes one feature that is core to Smart Assistant applications, namely, Speech Recognition. We should note that taking the challenge of developing a speech recognition system (or maybe even a voice recognition system) is not a straightforward task. At the very least, an understanding of the science of sounds (Acoustics) and Signal Processing may help. 13.6.2 Mel Coefficients (Feature Extraction) Here, we start with the discussion of Feature Extraction. See Figure 13.32. Figure 13.32: Feature Extraction Method (Speech Recognition) The figure shows two alternative methods of extracting features given raw data in audio format. The first method is called Perceptual Linear Prediction (PLP) introduced by Hynek Hermansky (1990), and the second method is called Mel Frequency Cepstral Coefficients (MFCC). Per Wikipedia, MFC is credited typically to Paul Mermelstein (1980), whereas he gives credits to Bridle and Brown for the idea. Other literature describes hybrid methods as alternatives, such as PLP-MFCC and PLP-RASTA. We leave readers to investigate those methods. In whatever case may be, the function of the two methods in the diagram is to produce what we call a cepstrum - in the form of cepstral coefficients. In simple terms, MFCC produces 39 coefficients equivalent to 39 features (in vector format) that we can use as input into our Encoder-Decoder system for learning. Figure 13.33 illustrates that: Figure 13.33: Spectogram Ideally, when performing feature extraction, the raw data (audio input) is presented in sound waves, continuous in time-domain. Hereafter, let us use the term signal for sound waves in connection to the signal processing field. Now, it helps to review Fourier Transform in Chapter 4 (Numerical Calculus), in which we cover Time-Domain and Frequency-Domain (See Time and Frequency Domain Figure under Approximating using Fourier Series and Transform Section). The goal is to discretize the data using Fast Fourier Transformation (FFT), translating the data into the frequency-domain. First, we discretize the sound wave into DFT-base chunks measured in Hertz, e.g., 20 milliseconds. Then, this discretized spectrum gets processed further via filters. The idea is to use filters and other forms of transformation to distinguish Human voice from the spectrum (e.g., Other literature suggests that the Human voice ranges between 40-60 decibels). Then, once the spectrum is filtered, it gets converted into cepstrum format. Fundamentally, we follow the steps below using MFCC as our choice of feature extraction (Muda L. et al., 2010; D Anggraeni et al. 2018; Maziar Raissi 2021). Note that, for consistency, our implementation compares results from functions such as powspec(.), spec2cep(.), and lifter(.) from tuneR library used in the steps mentioned by Jérôme Sueur (2018). First, let us use a third-party R library called tuneR. Then, we can use one of three popular datasets for our code, namely Timit, Dirha, and LibriSpeech. Timit - a dataset presented as an Acoustic-Phonetic speech corpus which includes multiple dialects. Dirha - a dataset presented as a multi-microphone acoustic corpus. LibriSpeech - a dataset presented as a collection of audio book recordings. As suggested from other literature, a suitable dataset for better training is phonetic-rich. In other words, our overall system needs to recognize phonemes - distinct units of sound (Oxford definition). For our demonstration, let us use the Timit dataset, which comes with sample audio for the train set and a separate set of sample audio for the test set. Each sample comprises a wav file (the actual audio recording), txt file (the text of the audio), and phn file (the corresponding phonemes). The Timit corpus includes time-aligned orthographic, phonetic, and word transcriptions, including a 16-bit, 16kHz speech waveform file for each utterance. An example of the text file follows: txt.file = &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.TXT&quot; \\[\\begin{align} \\mathbf{\\text{0 47104 She had your dark suit in greasy wash water all year.}} \\end{align}\\] That corresponds to the following phonemes phn.file = &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.PHN&quot; \\[ \\begin{array}{lll} 0 &amp; 2798 &amp; \\text{h}\\#\\\\ 2798 &amp; 3960 &amp; \\text{sh}\\\\ 3960 &amp; 4649 &amp; \\text{iy}\\\\ 4649 &amp; 5200 &amp; \\text{hv}\\\\ 5200 &amp; 6477 &amp; \\text{eh}\\\\ 6477 &amp; 6959 &amp; \\text{dcl}\\\\ 6959 &amp; 7265 &amp; \\text{d}\\\\ 7265&amp; 7853 &amp; \\text{y}\\\\ 7853 &amp; 8920 &amp; \\text{er}\\\\ \\text{...}\\\\ 43655 &amp; 47040 &amp; \\text{h}\\# \\\\ \\end{array} \\] Here, we use the corresponding audio. library(tuneR) wav.file = &quot;../../timit/data/TRAIN/DR2/MARC0/SA1.WAV.wav&quot; audio = readWave(wav.file) audio ## ## Wave Object ## Number of Samples: 47104 ## Duration (seconds): 2.94 ## Samplingrate (Hertz): 16000 ## Channels (Mono/Stereo): Mono ## PCM (integer format): TRUE ## Bit (8/16/24/32/64): 16 And with a Mac computer, we can play the audio like so: setWavPlayer(&#39;/usr/bin/afplay&#39;) play(audio) Let us review the structure of the audio data: str(audio) ## Formal class &#39;Wave&#39; [package &quot;tuneR&quot;] with 6 slots ## ..@ left : int [1:47104] -6 -1 2 12 11 2 -1 6 7 6 ... ## ..@ right : num(0) ## ..@ stereo : logi FALSE ## ..@ samp.rate: int 16000 ## ..@ bit : int 16 ## ..@ pcm : logi TRUE In the wav file, we can see that only the left audio channel is recorded, and the right audio channel is empty. Also, the sampling rate per second is 16000 (or 16kHz). signal = audio@left sampling.rate = audio@samp.rate # samp rate in Hz signal.length = length(signal) / audio@samp.rate Based on the structure, the length of our signal is 2.944 seconds based on (47104 \\(/\\) 16000). Second, we perform Pre-emphasis, a pre-processing step to enhance the high-frequency portion of a signal. The following formula is typically used: \\[\\begin{align} y(n) = x(n) - \\alpha x(n-1),\\ \\ \\ \\ \\ \\ 0.90 &lt; \\alpha &lt; 1 \\end{align}\\] where \\(\\mathbf{x(n)}\\) is the signal at time n and \\(\\alpha\\) is a filtering constant. preemphasis &lt;- function(a = 1, x) { N = length(x) y = rep(0, N) for (n in 2:N) { y[n] = x[n] - a * x[n-1] } y } preprocessed.signal = preemphasis(a=0.97, signal) head(signal, n=20) ## [1] -6 -1 2 12 11 2 -1 6 7 6 13 11 13 1 3 1 1 ## [18] 5 3 -2 head(preprocessed.signal, n=20) ## [1] 0.00 4.82 2.97 10.06 -0.64 -8.67 -2.94 ## [8] 6.97 1.18 -0.79 7.18 -1.61 2.33 -11.61 ## [15] 2.03 -1.91 0.03 4.03 -1.85 -4.91 For now, we skip normalization to show the full magnitude of our signal in the plots. normalized.signal = preprocessed.signal / max(abs(preprocessed.signal)) We also can use the preemphasis(.) from seewave library to validate consistency. library(seewave) preprocessed.audio = seewave::preemphasis(audio, alpha=0.97, output=&quot;Wave&quot;) str(preprocessed.audio) ## Formal class &#39;Wave&#39; [package &quot;tuneR&quot;] with 6 slots ## ..@ left : num [1:47104] 0 4.82 2.97 10.06 -0.64 ... ## ..@ right : num(0) ## ..@ stereo : logi FALSE ## ..@ samp.rate: int 16000 ## ..@ bit : num 16 ## ..@ pcm : logi TRUE head(preprocessed.audio@left, n=20) ## [1] 0.00 4.82 2.97 10.06 -0.64 -8.67 -2.94 ## [8] 6.97 1.18 -0.79 7.18 -1.61 2.33 -11.61 ## [15] 2.03 -1.91 0.03 4.03 -1.85 -4.91 Third, we perform Frame Blocking, which splits the signal into frames (or frequency bins) with a fixed frame size from a range of 20ms to 40ms. Other literature regards 25ms as standard. Because our signal is non-stationary, it helps to capture short-time intervals to highlight unique features within such a short period (e.g., harmonics or frequency tone). Here, we assume that given short intervals, we will be able to quantize signals into frames, each achieving some level of stationarity. And then, we also apply frame stride. This idea is similar to filters and strides in a convolutional network in that patches overlap due to strides. Similarly, the stride causes the frames to overlap in the next Windowing step. Other literature may consider a 50% overlap as appropriate (or 12.5ms if the frame size is 25ms). Here, we throw the number as a 10ms frame step. Note that stride size is also called window shift in other literature or hop length in other implementations. Therefore, given that, and assume we have a signal that runs for 2.94 seconds, we should now be able to calculate the following: \\[\\begin{align} \\text{(num of samples per frame)} = N = \\frac{25}{1000 } \\times 16000 = 400 \\text{ sample points}, s(n) \\in \\mathbb{R}^{400} \\end{align}\\] where n is the nth frame in the range \\(0 &lt; n &lt; N - 1\\). \\[\\begin{align} \\text{(sample points per frame step)} = \\frac{12.5}{1000 } \\times 16000 = 200 \\text{ sample points} \\end{align}\\] \\[\\begin{align} \\text{(duration of signal)} = 2.94 \\times 16000 = 47040 \\text{ sample points} \\end{align}\\] \\[\\begin{align} \\text{(num of frames)} = N = \\frac{(47040 - 400)}{200} = 234 \\text{ frames} \\ \\ \\ \\text{(ceiling)} \\end{align}\\] We can pad the last frame with zeroes as a round-off to align to the frame size. Therefore, our frame structure should show as \\(\\mathbf{s} \\in \\mathbb{R}^{234 \\times 400}\\), arranged such that if \\(\\mathbf{s}\\) is a matrix, it has 234 rows (frames) and 400 columns (dimensions). frames &lt;- function(duration.size, duration.stride) { frm.size = duration.size * sampling.rate frm.stride = duration.stride * sampling.rate if (frm.size ) list(&quot;size&quot; = frm.size, &quot;stride&quot; = frm.stride, &quot;duration&quot; = duration.size, &quot;stride.time&quot; = duration.stride) } # 12.5ms = 50% overlap frm = frames(duration.size = 0.025, duration.stride = 0.0125) t(c(frm)) ## size stride duration stride.time ## [1,] 400 200 0.025 0.0125 (N = num_of_frames = round((signal.length * sampling.rate - frm$size) / frm$stride)) ## [1] 234 To achieve total efficiency of FFT calculation later, it helps to choose a frame with a size in multiples of power of two. So the size closer to that is 512. Being so, we adjust the other hyperparameters, therefore: frm$size = 512 frm$stride = 256 frm$duration = frm$size / audio@samp.rate frm$stride.time = frm$stride / audio@samp.rate t(c(frm)) ## size stride duration stride.time ## [1,] 512 256 0.032 0.016 The number of frames is then: (N = num_of_frames = round((signal.length * sampling.rate - frm$size) / frm$stride)) ## [1] 182 Actual implementations may choose to pad the edges of the frame with zero frequencies to align with the length of the signal correctly. Next, we generate a matrix to hold our frames. Each row corresponds to a frame of size 512. s = frm.matrix = matrix(0, nrow=N, ncol=frm$size) for (n in 1:N) { start = (n - 1) * frm$stride + 1 end = start + frm$size - 1 s[n,] = preprocessed.signal[start:end] } Let us plot the Frequency-Time (see Figure 13.34): par(mfrow=c(2,1)) x = seq(1, length(signal)); y = signal plot(NULL, xlab = &quot;Time (Entire Signal)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;Audio Wave (Amplitude-Time)&quot;, type =&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) y = lines(x, y, col = &quot;darkgreen&quot; ) x = seq(1, length(s[1,])); y = s[1,] plot(NULL, xlab = &quot;Time (1st Frame)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;, type=&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col = &quot;darkgreen&quot; ) Figure 13.34: Audio Wave (Amplitude-Time) Fourth, we apply Windowing to prevent FFT from hitting the known spectral leakage or avoid the edge effect issue in which minor frequencies tend to leak to other frames because of the overlap (see strides) (Lyon D. 2009). One of two popular windowing techniques is the Hamming window, which is expressed below (see Figure 13.35): \\[\\begin{align} w[n] = \\frac{25}{46} - \\frac{21}{46} \\ \\mathbf{cos} \\left( \\frac{2\\pi n}{N-1}\\right)= 0.54 - 0.46\\ \\mathbf{cos} \\left( \\frac{2\\pi n}{N-1}\\right), 0 \\le n \\le N- 1 \\end{align}\\] where N = window size. hamming.window &lt;-function(N) { n = seq(0, N-1) 0.54 - 0.46 * cos( ( 2 * pi * n ) / (N - 1) ) } w = window = hamming.window(N = frm$size) f1 = seq(0, frm$size-1) f2 = f1 + frm$stride f3 = f2 + frm$stride plot(NULL, xlim=range(f1,f3), ylim=range(w), xlab=&quot;Samples&quot;, ylab=&quot;Amplitude&quot;, main=&quot;Overlapping Hamming Window with Stride&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(f1, w, col=&quot;navyblue&quot;, lwd=2) lines(f2, w, col=&quot;navyblue&quot;, lwd=2) lines(f3, w, col=&quot;navyblue&quot;, lwd=2) Figure 13.35: Overlapping Hamming Window with Stride Note that each Window shows a bell-shaped curve, a smoothed representation of its main lobe and side lobes of the Window. The other Window is called Hanning Window with the following formula: \\[\\begin{align} w[n] = 0.50 - 0.50\\ \\mathbf{cos} \\left( \\frac{2\\pi n}{N-1}\\right), 0 \\le n \\le N - 1 \\end{align}\\] Note that we skip derivations of those formulas - we leave readers to investigate the use of 0.54 and 0.46 constants for Hamming Window compared to other windows. We also leave readers to investigate other types of windowing, such as Blackman window. We then multiply our signal with the Window in a row-wise fashion like so (see Figure 13.36): windowed.frm = sweep(s, 2, w, &#39;*&#39;) str(windowed.frm) ## num [1:182, 1:512] 0 0.0288 -0.4512 -0.3296 0.4944 ... par(mfrow=c(2,1)) x = seq(1, length(s[1,])); y = s[1,] plot(NULL, xlab = &quot;Time (1st Unwindowed Frame)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;, type=&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col = &quot;darkgreen&quot; ) x = seq(1, length(windowed.frm[1,])); y = windowed.frm[1,] plot(NULL, xlab = &quot;Time (1st Windowed Frame)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;Pre-emphasized Audio Wave (Amplitude-Time)&quot;, type=&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col = &quot;darkgreen&quot; ) Figure 13.36: Audio Wave (Amplitude-Time) Fifth, we then calculate the DFT (Discrete Fourier Transform) for each frame using FFT. Recall our example implementation of FFT using the Cooley-Tukey algorithm in Chapter 4 (Numerical Calculus). It helps to revisit the topic around Time-Domain and Frequency-Domain in the Chapter mentioned, which covers the equation below to convert our signal to the frequency in the frequency domain: \\[\\begin{align} \\tilde{s}(k) = \\sum_{n=0}^{N-1} s_k(n) h(n)e^{-j2\\pi nk / N}, \\ \\ \\ \\ \\ \\ 0 \\le k &lt; K - 1 \\end{align}\\] where \\(\\mathbf{s_k}(n) \\in \\mathbb{R}^{400}\\) which is the kth frame (row-wise) in our frame matrix and \\(\\mathbf{h}(n) \\in \\mathbb{R}^{400}\\) which is our window. Note that we have already calculated our windowed frames, each denoted as: \\[\\begin{align} w(n) = s_k(n) h(n)\\ \\ \\ \\leftarrow\\ \\ \\ \\ \\text{(windowed frame)} \\end{align}\\] Our goal now is to derive a spectrum of frequencies per frame denoted by \\(\\tilde{s}(k) \\in \\mathbb{C}^K\\) where K is the N-point FFT, also called DFT coefficients, which is in complex-number format. Other implementations may denote this as nfft or n_fft which is typically set either as 256 or 512. Here, we calculate DFT using the fft(.) function in R. It helps to note that the first element in the vector produced by the calculation is the DC component (refer to signal processing) which is the average of the entire signal (or the samples in a frame). In considering the DC component (without disregarding it), other literature may suggest subtracting each sample from the average mean before taking the DFTs. NFFT = frm$size frame1 = windowed.frm[1,] frame1 = frame1 - mean(frame1) # Zero-out DC Component to center data However, we skip this extra step to conform with the powspec(.) function. We also calculate the amplitude or magnitude of the frequency. Because the result of fft(.) is a set of complex numbers, we show four different ways to calculate the magnitude - given the four equations below: \\[\\begin{align} \\text{mag}(x) = \\sqrt{Real(x)^2 + Imaginary(x)^2} = |x| = Mod(x) = \\sqrt{x} \\times \\text{Conj}(\\sqrt{x}) \\end{align}\\] Note that the symbol \\(|.|\\) is either interpreted as the absolute value or the modulus of the complex value. For example: cplx.num = 24 + 3i magnitude &lt;- function(x) { sqrt(Re(x)^2 + Im(x)^2) } c(&quot;this&quot; = magnitude(cplx.num), &quot;this&quot; = abs(cplx.num), &quot;this&quot; = Mod(cplx.num), &quot;or this&quot; = sqrt(cplx.num)*Conj(sqrt(cplx.num))) ## this this this or this ## 24.186773+0i 24.186773+0i 24.186773+0i 24.186773+0i Note that if we exclude the DC component, then the first half of the FFT vector is symmetric to the second half of the vector, just by comparing the raw magnitude. We can show this by comparing the symmetry. See the following: NFFT = frm$size frame1 = windowed.frm[1,] dft.coeff1 = fft(frame1) dft.mag1 = magnitude(dft.coeff1) half = floor(NFFT / 2 + 1) first.half = dft.mag1[2:half] second.half = rev(dft.mag1[half:NFFT]) # reverse the second half rbind(first.half[1:5], second.half[1:5]) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 7.7292003 0.8443712 1.7062433 7.6443928 3.8443819 ## [2,] 7.7292003 0.8443712 1.7062433 7.6443928 3.8443819 setequal(round(first.half,8),round(second.half,8)) ## [1] TRUE See the magnitude plot for the DFT in Figure 13.37. par(mfrow=c(2,1)) x = seq(1, length(dft.mag1)); y = Re(dft.mag1) plot(NULL, xlab = &quot;Coefficients (1st Windowed Frame)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;DFT (Amplitude-Coefficient)&quot;, type=&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col = &quot;darkgreen&quot; ) x = seq(1, length(first.half)); y = Re(first.half) plot(NULL, xlab = &quot;Coefficients (1st Half of the 1st Windowed Frame)&quot;, ylab = &quot;Amplitude&quot;, xlim=range(x), ylim=range(y), main = &quot;DFT (Amplitude-Coefficient)&quot;, type=&quot;l&quot;) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y, col = &quot;darkgreen&quot; ) Figure 13.37: DFT (Amplitude-Coefficient) Next, we calculate the Power Spectrum for each short frame, also called Short-Time Fourier Transform (STFT) using the below formula (without the normalizer N). \\[\\begin{align} p_k = |\\text{FFT}(\\tilde{s}_k)|^2 \\end{align}\\] Let us calculate the power spectrum of each frame in the windowed frames. Here, we take the first half of the spectrum, including the DC component. half = floor(NFFT / 2) P = array(0, c(half, N)) for (n in 1:N) { frame = windowed.frm[n,] frame = fft(frame) P[,n] = frame[1:half] } my.powspectrum = P = magnitude(P)^2 str(my.powspectrum ) ## num [1:256, 1:182] 479.088 59.741 0.713 2.911 58.437 ... We get a matrix with column-vector for the spectra and row-vector for the frequencies per spectrum. Equivalently, we can also just use a function called powspec(.) from tuneR library. Note that powspec(.) already computes for the window. Thus we use the original pre-emphasized signal instead. tuneR.powspectrum = powspec(preprocessed.audio@left, sr = audio@samp.rate, wintime = frm$duration, steptime = frm$stride.time) str(tuneR.powspectrum) ## num [1:256, 1:182] 479.088 59.741 0.713 2.911 58.437 ... We can now plot the periodogram for the frame. See Figure 13.38. We shifted the second spectrum about one step to the right to show that both spectra have the same exact magnitude. y1 = my.powspectrum[,1] # 1st DFT y2 = tuneR.powspectrum[,1] # 1st DFT x = seq(1, length(y1)) plot(NULL, xlim=range(x), ylim=range(y1), xlab=&quot;(Xm) Frequency (Bin)&quot;, ylab=&quot;Amplitude (Magnitude)&quot;, main=&quot;Periodogram (Spectrum)&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;green&quot;, type=&quot;l&quot;) lines(x+1, y2, col=&quot;brown&quot;, type=&quot;l&quot;) legend(170,80500, inset=.02, c(&quot;my.powspectrum&quot;, &quot;tuneR.powspectrum&quot;), fill=c(&quot;green&quot;, &quot;brown&quot;), horiz=FALSE, cex=0.8) Figure 13.38: Periodogram (Spectrum) Sixth, we then use a Filter Bank, which, in Signal Processing, is described as a segmentation of input signals into individual analysis signals, each carrying a band of frequencies. A good illustration can be seen in Figure 13.39. Figure 13.39: Filter Bank Structure There are different types of Filter Banks in Signal Processing namely, Gabor Filter Banks, Polyphase Filter Banks, DCT Filter Banks, Mel Filter Banks, etc. A good reference comes from electrical4u.com, providing introductory concepts of each Filter bank. However, in this section, we focus on Mel Filter Bank, which is tailored more for speech. The core component of MFCC is the Mel Filter Bank. Other literature uses human perception of sound to explain that the Mel Filter Bank mimics how our inner ears consist of a set of hair cells that filter human sound at lower frequencies (e.g., 40-60 dB). Similarly, Mel Filter Bank forms a set of triangular filters for filtering. See Figure 13.41. The idea is to convert the Power Spectrum into Mel scale (Mels) - named by Stevens, Volkman, and Newman in 1937 (Wikipedia). We use the below Mel scale formulas: From Frequency to Mel scale (where f is the frequency in Hz): \\[\\begin{align} m = \\mathbf{mels}(f) = 2595 \\times \\log_{10}\\left( 1 + \\frac{f}{700}\\right) = 1127.01048 \\times \\log_e \\left( 1 + \\frac{f}{700}\\right) \\end{align}\\] From Mel scale to Frequency (where m is the mel scale): \\[\\begin{align} f = \\mathbf{freq}(m) = 700 \\times\\left(pow \\left(10,\\frac{m}{2595}\\right) - 1\\right) = 700 \\times\\left(exp\\left(\\frac{m}{1127.01048}\\right) - 1\\right) \\end{align}\\] As for the Mel constant and logarithm used, we leave readers to see Fitting the Mel scale (Umesh, S., Cohen, L., &amp; Nelson, D. J. 1999). Below is an example implementation of the conversions. hertz.to.mel &lt;- function(f) { 2595 * log10 ( 1 + f/700) } mel.to.hertz &lt;- function(m) { 700 * ( 10^( m / 2595) - 1) } ### Or we can use the alternative formulae #hertz.to.mel &lt;- function(f) { 1127.01048 * log(1 + f / 700, base=exp(1)) } #mel.to.hertz &lt;- function(m) { 700 * (exp(m/1127.01048) - 1) } m = hertz.to.mel(f=512) f = mel.to.hertz(m) c(&quot;m&quot; = m, &quot;f&quot; = f) ## m f ## 618.65988 512.00000 For an illustration, let us review the Mel-to-Frequency plot using Figure 13.40. f = seq(0, 10000) m = hertz.to.mel(f) Figure 13.40: Mel-Frequency Plot A way to look at Mel filter bank is with the plot Figure 13.41 using the below function, namely mel.filterbank(.), to generate the center Mel frequencies (motivated by a Python script from scottlawonbc (github)): mel.filterbank &lt;- function(nbands, freq.min, freq.max) { mel.max = hertz.to.mel(freq.max) # or hz2mel(freq.max, htk=TRUE) mel.min = hertz.to.mel(freq.min) # or hz2mel(freq.min, htk=TRUE) mel.delta = abs(mel.max - mel.min) / (nbands + 1) mel.freq = mel.min + mel.delta * seq(0, nbands + 1) mel.lower = head(mel.freq, -2) mel.upper = tail(mel.freq, -2) mel.center = tail(head(mel.freq,-1),-1) list( &quot;lower&quot; = mel.lower, &quot;center&quot; = mel.center, &quot;upper&quot; = mel.upper, &quot;banks&quot; = mel.freq) } nbands = 7; fmin = 300; fmax = 8000 (melfb = mel.filterbank(nbands=nbands, freq.min=fmin, freq.max=fmax)) ## $lower ## [1] 401.97059 706.72714 1011.48370 1316.24026 ## [5] 1620.99682 1925.75337 2230.50993 ## ## $center ## [1] 706.72714 1011.48370 1316.24026 1620.99682 ## [5] 1925.75337 2230.50993 2535.26649 ## ## $upper ## [1] 1011.4837 1316.2403 1620.9968 1925.7534 2230.5099 ## [6] 2535.2665 2840.0230 ## ## $banks ## [1] 401.97059 706.72714 1011.48370 1316.24026 ## [5] 1620.99682 1925.75337 2230.50993 2535.26649 ## [9] 2840.02305 Below are the corresponding Hz frequencies: fm.prev = mel.to.hertz(melfb$lower) fm.ctr = mel.to.hertz(melfb$center) fm.next = mel.to.hertz(melfb$upper) fm.bins = mel.to.hertz(melfb$banks) (fm = list(&quot;lower&quot; = fm.prev, &quot;center&quot; = fm.ctr, &quot;upper&quot; = fm.next, &quot;bins&quot; = fm.bins)) ## $lower ## [1] 300.00000 610.50869 1017.43304 1550.71093 ## [5] 2249.57624 3165.44531 4365.69968 ## ## $center ## [1] 610.50869 1017.43304 1550.71093 2249.57624 ## [5] 3165.44531 4365.69968 5938.64348 ## ## $upper ## [1] 1017.4330 1550.7109 2249.5762 3165.4453 4365.6997 ## [6] 5938.6435 8000.0000 ## ## $bins ## [1] 300.00000 610.50869 1017.43304 1550.71093 ## [5] 2249.57624 3165.44531 4365.69968 5938.64348 ## [9] 8000.00000 Below is an example implementation, plotting the triangular frequency bands. We use seven bands for demonstration only. len = length(melfb$center) col = seq(1, len); ones = rep(1, len); zeros = rep(0, len) plot(NULL, xlim=range(fmin,fmax), ylim=range(0,1.05), xlab=&quot;(Hz) Frequency&quot;, ylab=&quot;Weights&quot;, main=&quot;Mel Filter Banks&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) segments(fm.prev, zeros, fm.ctr, ones, col=col, lwd=2) segments(fm.ctr, ones, fm.next, zeros, col=col, lwd=2) text(fm.ctr, 1.05, round(melfb$center,0), cex=0.7 ) text(7500, 1.05, &quot;(mel scale)&quot;, cex=0.7) Figure 13.41: Mel Filter Banks Here, we choose lower and upper frequencies and convert those frequencies into Mels. To use the Mel Filter Bank, we need to construct its matrix form such that \\(M \\in \\mathbb{R}^{b \\times \\left(\\frac{K}{2} + 1\\right)}\\). Here, we use \\(\\frac{K}{2} + 1\\) because if we look carefully at the Periodogram in Figure 13.38, the frequencies across the spectrum are symmetrical in that the first frequency equals the last frequency in the K-point spectrum. Now, in terms of the number of rows denoted by b, our choice for the number of bands is 40 for b. Below is the formulation to use in constructing our matrix: \\[\\begin{align} H_m(k) = \\begin{cases} 0, &amp; k &lt; f(m - 1) \\\\ \\frac{(k-f(m-1))}{f(m) - f(m-1)}, &amp; f(m-1) \\le k \\le f(m)\\\\ \\frac{(f(m+1)-k)}{f(m+1)-f(m)}, &amp; f(m) &lt; k \\le f(m+1)\\\\ 0, &amp;k &gt; f(m+1) \\end{cases} \\ \\ \\ \\ \\ \\ \\ \\ where: m \\in \\{0,..., M-1\\} \\label{eqn:eqnnumber805} \\end{align}\\] We then show an example implementation of the formulation above to construct our mel filter bank matrix. mel.matrix &lt;- function(freq.bins, nbands, nfft, sr) { L= floor( nfft / 2 ) H = array(0, c(nbands, L)) # Mel Filter Bank Matrix f = freq.bins freq = c(0:(nfft - 1)) / nfft * sr for (i in 1:L) { for (m in 2:(nbands+1)) { h = 0; k = freq[i] if (k &lt; f[m-1]) { h = 0 } else if (f[m-1] &lt;= k &amp;&amp; k &lt;= f[m]) { h = (k - (f[m-1]))/(f[m] - (f[m-1])) } else if (f[m] &lt; k &amp;&amp; k &lt;= f[m+1]) { h = (f[m+1] - k)/(f[m+1] - f[m])} else if (k &gt; f[m+1]) { h = 0 } H[m - 1, i] = h } } H } The matrix requires us to construct the frequency bins like so: nbands = 40; fmin = 300; fmax = 8000 melfb = mel.filterbank(nbands=nbands, freq.min=fmin, freq.max=fmax) fm.ctr = mel.to.hertz(melfb$center) fm.bins = mel.to.hertz(melfb$banks) Finally, we call our mel.matrix(.) function to construct the matrix Mel weights (wts). H = mel.wts = mel.matrix(freq.bins = fm.bins, nbands = nbands, nfft = NFFT, sr = audio@samp.rate) str(mel.wts) ## num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ... Alternatively, we can use fft2melmx(.) function from tuneR library. tuneR.mel = tuneR:::fft2melmx(nfft = NFFT, sr = audio@samp.rate, nfilts = nbands, width = 1, minfreq = fmin, maxfreq = fmax, htkmel = TRUE, constamp = TRUE) H = tuneR.wts = tuneR.mel$wts[,1:256] # if we take only the first half. str(tuneR.wts) ## num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ... Note that the function allows us to use a hyperparameter htkmel, which uses the Mel scale formula if we set it to true, as discussed previously. Let us view the heatmap of our Mel filter bank matrix. See Figure 13.42. library(RColorBrewer) col = colorRampPalette(brewer.pal(8, &quot;Blues&quot;))(800) heatmap(mel.wts, Colv = NA, Rowv = NA, col=col) Figure 13.42: Mel HeatMap Seventh, to get our Mel-filtered power spectrum, we use the following equation: \\[\\begin{align} \\tilde{X}(m) = \\sum_{k=0}^{N-1} \\left(|X(k)|^2 H_m(k)\\right), \\ \\ \\ \\ \\ 0 \\le m \\le M - 1 \\end{align}\\] where \\(\\mathbf{X}(k)\\) is our power spectrum (P) and \\(\\mathbf{\\tilde{X}}(k)\\) is our target Mel power spectrum. H is our Mel weights. The index m is the Mel-filter bank number and k is the DFT bin number. We know that the two matrices have the following dimension; therefore, we can perform matrix manipulation to construct our Mel Spectrum matrix. H.dim = dim(H) # mel filter bank matrix P.dim = dim(P) # power spectrum rbind(&quot;H&quot; = H.dim, &quot;P&quot; = P.dim) ## [,1] [,2] ## H 40 256 ## P 256 182 tilde.X = my.mel.powspectrum = H %*% P str(tilde.X) ## num [1:40, 1:182] 238 212 1430 4381 15900 ... We then take the log of our Mel Spectrum which comes from the idea that the human voice is logarithmic, based on notes from other literature: \\[\\begin{align} S(m) = 20 \\times \\log_{10} \\left(\\tilde{X}(m)\\right) \\end{align}\\] S = my.log.mel.powspectrum = 20 * log10(tilde.X) str(S) ## num [1:40, 1:182] 47.5 46.5 63.1 72.8 84 ... Now, we can plot a heatmap. See Figure 13.43: frames = ncol(S) mel.x = seq(1, signal.length, length.out=frames) mel.y = fm.ctr mel.z = t(S) image(x = mel.x, y = mel.y, z = mel.z, ylab = &#39;Freq [Hz]&#39;, xlab = &#39;Time [s]&#39;, useRaster=FALSE, col = hcl.colors(12, &quot;YlOrRd&quot;, rev = FALSE)) Figure 13.43: Mel Log Spectrum HeatMap Eight, note that other literature may stop from here and use the MFCC spectrogram as input image to CNN. However, for other modeling methods such as RNN (e.g., LSTM/GRU), we can use PLP and LPCC. The difference is that they vary in the number of coefficients (features) generated. In our case, we may be interested in covering the standard 39 features of MFCC per frame (Noughreche A. et al. 2021): \\[ \\begin{array}{ll} \\text{12 MFCC coefficients} &amp; \\text{1 Log Energy coefficient }\\\\ \\text{12 Delta MFCC coefficients} &amp; \\text{1 Delta Log Energy coefficient }\\\\ \\text{12 Delta Delta MFCC coefficients} &amp; \\text{1 Delta-Delta Log Energy coefficient} \\end{array} \\] To construct the 39 features, we start with the first 12 cepstra coefficients. Here, we take the Discrete Cosine Transform (DCT), which is the inverse DFT of the log of the Mel spectrum. It helps to point out as reference only that there are four standard DCT types (Gilbert Strang 1999; Shao X., Johnson S.G 2009): \\[\\begin{align} \\begin{array}{lll} \\mathbf{\\text{DCT Type}} &amp; \\mathbf{\\text{Basis Function (BS)}} &amp; \\mathbf{\\text{Constraint}}\\\\ ------ &amp; ---------- &amp; ---------------\\\\ 1 &amp; \\mathbf{\\text{cos}} \\left[\\left(mn\\right)\\frac{\\pi}{M-1}\\right] &amp; \\text{divide by } \\sqrt{2} \\text{ if m or n = 0 or M - 1}\\\\ 2 &amp; \\mathbf{\\text{cos}}\\left[\\left(n + \\frac{1}{2}\\right) m \\frac{\\pi}{M}\\right] &amp; \\text{divide by } \\sqrt{2} \\text{ if m = 0} \\\\ 3 &amp; \\mathbf{\\text{cos}}\\left[\\left(m + \\frac{1}{2}\\right) n \\frac{\\pi}{M}\\right] &amp; \\text{divide by } \\sqrt{2} \\text{ if n = 0} \\\\ 4 &amp; \\mathbf{\\text{cos}}\\left[\\left(m + \\frac{1}{2}\\right) \\left(n + \\frac{1}{2}\\right) \\frac{\\pi}{M}\\right] \\\\ \\end{array} \\label{eqn:eqnnumber806} \\end{align}\\] For example, using the basis function (BS) of DCT type 3, we get the following equation for DCT. \\[\\begin{align} C_n = \\sum_{m=0}^{M-1} S(m) \\text{BS}_3(n, m), \\ \\ \\ \\ \\ \\ \\text{C = Mel cepstral coefficients} \\end{align}\\] where N is the number of Mel cepstral coefficients, and M is the number of frequencies in the Mel power spectrum. We rely on the basis function to produce a matrix of cosine weights. Based on our convention, each row-vector forms a sinusoidal pattern. It is essential to point out that other literature also demonstrates the use of the following DCT III basis function for MFCC (derivation not included). \\[\\begin{align} \\text{DCT}_{III}^{(mfcc)}(n) = \\sqrt{\\frac{2}{M}} \\times \\mathbf{\\text{cos}}\\left[\\left(m - \\frac{1}{2}\\right)\\frac{n\\pi}{M}\\right], \\ \\ \\ \\ \\ 0 \\le n \\le N-1 \\end{align}\\] where \\(\\sqrt{\\frac{2}{M}}\\) is a scaling factor. And for DCT II to be orthogonal, if n = 0, then we divide by \\(\\sqrt{2}\\) like so: \\[\\begin{align} \\text{DCT}_{II}^{(mfcc)}(0) = \\text{DCT}_{III}^{(mfcc)}(0) \\text{ divide by } \\sqrt{2} \\end{align}\\] For our demonstration purposes, let us use the DCT III formulation instead. To do so, let us show in detail how the matrix for DCT is constructed, starting with the below naive example implementation of DCT II and III: my.dct &lt;- function(N, M, dcttype=3) { raw.dct.matrix &lt;- function(N, M) { n=seq(0, N-1) bs = array(0, c(N, M)) for (m in 1:(M)) { bs[,m] = (n * (m - 1/2)) } bs } rmatrix = raw.dct.matrix(N=N, M=M) dct.matrix = cos( pi * rmatrix / M ) * sqrt(2 / M) if (dcttype == 2) { dct.matrix[1,] = dct.matrix[1,] / sqrt(2) } list(&quot;rmatrix&quot; = rmatrix, &quot;matrix&quot; = dct.matrix) } For a brief example, we show that the raw matrix is constructed as such: dct = my.dct(N=4, M=5, dcttype=3) dct$rmatrix * 2 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 0 0 0 0 0 ## [2,] 1 3 5 7 9 ## [3,] 2 6 10 14 18 ## [4,] 3 9 15 21 27 And our dct matrix generated by our basis functions is thus shown as dct$matrix ## [,1] [,2] [,3] [,4] ## [1,] 0.63245553 0.63245553 6.3245553e-01 0.63245553 ## [2,] 0.60150096 0.37174803 3.8726732e-17 -0.37174803 ## [3,] 0.51166727 -0.19543951 -6.3245553e-01 -0.19543951 ## [4,] 0.37174803 -0.60150096 -1.1618020e-16 0.60150096 ## [,5] ## [1,] 0.63245553 ## [2,] -0.60150096 ## [3,] 0.51166727 ## [4,] -0.37174803 For our 12 Cepstral coefficients, let us construct a 12 x 40 dct matrix: ncepstral = 12 nfilters = nbands BS = my.dct(N=ncepstral, M=nfilters, dcttype=3)$matrix str(BS) ## num [1:12, 1:40] 0.224 0.223 0.223 0.222 0.221 ... Next, we calculate the Log Mel spectrum: S = log(my.mel.powspectrum) Finally, we then obtain our DCT coefficients, switching back from frequency-domain to time-domain by performing matrix manipulation: rbind(&quot;S&quot; = dim(S), &quot;BS&quot; = dim(BS)) ## [,1] [,2] ## S 40 182 ## BS 12 40 my.mel.cepstral.coeffs = BS %*% S str(my.mel.cepstral.coeffs) ## num [1:12, 1:182] 88.916 -6.612 -4.564 0.239 -1.132 ... Ninth, as an alternative to MFCC, we leave readers to investigate the 54 features of PLP and 39 features of LPCC. Other contributions are published for comparative study of different MFCC variants (Ganchev T. et al., 2005; Elharati H. 2019; Joshy J., Sambyo K. 2016). Specifically, one that may stand out is MFCC-HTK, which references Hidden Markov Chain (HMM) ToolKit. In terms of MFCC-HTK implementation, we reference Jérôme Sueur (2018). In his book, he proceeds with three steps to get the Mel coefficients after obtaining the Power Spectrum in which he demonstrates the use of three corresponding functions, namely, audspec(.), spec2cep(.), and lifter(.). We derive our log Mel power spectrum and the 12 cepstral coefficients in step eight. Alternatively, here we use audspec(.) and then spec2cep(.) in next step. Recall that the goal here is to focus on Critical Band Filtering, also called Auditory Filtering (Fletcher 1940), which is a masking process to reduce the Power Spectrum to a set of critical frequency bandwidth relevant to human auditory perceptions. Using the Sueur step, we implement the following to narrow down our spectrum to Auditory Spectrum. tuneR.mel.spectrum = audspec(tuneR.powspectrum, sr = audio@samp.rate, minfreq = fmin, maxfreq = fmax, nfilts=nbands, fbtype=&quot;htkmel&quot;, sumpower=TRUE) str(tuneR.mel.spectrum) ## List of 2 ## $ aspectrum: num [1:40, 1:182] 244 199 1339 4066 16012 ... ## $ wts : num [1:40, 1:256] 0 0 0 0 0 0 0 0 0 0 ... The function audspec(.) calculates NFFT like so: \\[\\begin{align} \\text{nfft} = (\\text{nfreq} - 1) \\times 2 \\end{align}\\] whereas in our case, we use: \\[\\begin{align} \\text{nfft} = \\text{nfreq} / 2 \\ \\ \\ \\ \\ \\text{where nfreq = frame size} \\end{align}\\] If our route is towards using PLP instead of MFCC, then to conform with PLP, we can use postaud(.) function for compression as necessary. mfcc.post.spectrum = postaud(x=tuneR.mel.spectrum$aspectrum, fbtype=&quot;htkmel&quot;, fmax=fmax) str(mfcc.post.spectrum ) ## List of 2 ## $ y : num [1:40, 1:182] 0.179 0.179 0.85 2.089 4.7 ... ## $ eql: num [1:40] 0 0.0000272 0.000457 0.0022935 0.0067969 ... Tenth, we then use spec2cep(.) to convert from Mel Spectrum to Mel Cepstrum. In the process, the function intrinsically performs Discrete Cosine Transform (DCT) to decorrelate the spectrum, then it uses DCT to obtain the cepstral coefficients (Jérôme Sueur 2018). Here, we use DCT type 3. cepstra = spec2cep(tuneR.mel.spectrum$aspectrum, ncep = ncepstral, type=&quot;t3&quot;) tuneR.mel.cepstral.coeffs = cepstra$cep tuneR.mel.cepstral.dctm = cepstra$dctm str(cepstra) ## List of 2 ## $ cep : num [1:12, 1:182] 88.811 -6.682 -4.558 0.272 -1.161 ... ## $ dctm: num [1:12, 1:40] 0.224 0.223 0.223 0.222 0.221 ... As an alternative to the overall MFCC which uses audspec(.) and spec2cep(.), we leave readers to investigate Linear Predictive Cepstral Coefficient (LPCC) which uses dolpc(.) function and lpc2cep(.) correspondingly. Eleventh, apply rescaling to normalize magnitude across coefficients. This is done through liftering (or filtering). \\[\\begin{align} \\mathbf{\\text{liftered.cepstra}} = \\left(1 + \\frac{L}{2} \\mathbf{\\text{sin}} \\frac{\\pi n}{L}\\right) \\mathbf{\\text{cepstra}} \\end{align}\\] Below is a simple example implementation of the equation above: my.htk.lifter &lt;- function(cepstra, lift) { L = lift n = seq(0, nrow(cepstra) - 1) lift = (1 + (L/2) * sin(pi * n/L)) sweep(cepstra, 1, lift, &#39;*&#39;) } mels.coefficients=my.htk.lifter(tuneR.mel.cepstral.coeffs, lift=ncepstral-1) str(mels.coefficients) ## num [1:12, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ... To validate consistency, we use lifter(.). mels.coefficients = lifter(tuneR.mel.cepstral.coeffs, lift=ncepstral-1, htk=TRUE) str(mels.coefficients) ## num [1:12, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ... Twelfth, we compute for the Log Energy Coefficients, which captures the change rate (slope approximation) in cepstral features over time (Jaison Joshy, Koj Sambyo 2016). Here, we capture the 12 Delta MFCC energies as the first-order difference and another 12 Delta Delta MFCC energies as the second-order difference from the first-order difference. These are approximations of first and second derivatives in terms of speed and acceleration of speech. Deltas are expressed as such (Noughreche A. et al 2021): \\[\\begin{align} \\nabla_t = d_t - d_{t-1} \\ \\ \\ \\ \\ \\ \\ \\ \\nabla \\nabla_t = \\nabla_t - \\nabla_{t-1} \\end{align}\\] \\[ where: d_t = \\frac{\\sum_{n=1}^N n\\left(C_{n+t} - C_{n-t}\\right)}{2 \\sum_{n=1}^N n^2}, \\ \\ \\ \\ \\ \\ \\ \\ \\text{C = Mel cepstral coefficients} \\] delta.energies = deltas(tuneR.mel.cepstral.coeffs) str(delta.energies) ## num [1:12, 1:182] -39.19 5.299 0.493 -17.704 1.332 ... delta.delta.energies = deltas(delta.energies) str(delta.delta.energies) ## num [1:12, 1:182] -1046.3 -246.2 279.8 49.5 96.2 ... Finally, to complete the 39 features of MFCC per frame, we also need to calculate the Energy of the signal. Here, we can either consider the Raw Energy directly from the signal or the Cepstral Energy from MFCC coefficients (Korzinek D. 2021). As for the Raw Energy, we can choose the formulation based on which domain the coefficient happens to be. \\[\\begin{align} \\underbrace{\\mathbf{\\log}\\ \\text{E} = \\log\\left(\\sum x^2\\right)}_{\\text{in time-domain}} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\underbrace{\\mathbf{\\log}\\ \\text{E} = \\log\\left(\\frac{|x|^2}{\\text{nfft}}\\right)}_{\\text{in frequency-domain}} \\end{align}\\] get.log.energy &lt;- function(x) { z = log(apply(x^2, 2, sum)); z } get.mfcc.energy &lt;- function(x, nfft) { z = apply(x, 2, sum) * (sqrt(2/nfft)) ; z } To get the energy of the windowed frame, we use the following code: log.energy = get.log.energy(t(windowed.frm)) str(log.energy) ## num [1:182] 8.93 8.86 8.66 8.56 7.97 ... Otherwise, we also can use the MFCC coefficients: log.mfcc.energy = get.mfcc.energy(tuneR.mel.cepstral.coeffs, frm$size) str(log.mfcc.energy) ## num [1:182] 4.25 4.64 4.35 4.19 4.3 ... See the close correlation between log raw energy and MFCC energy in Figure 13.44. len = length(log.energy) x = seq(1, len) y1 = log.energy / max(log.energy) y2 = log.mfcc.energy / max(log.mfcc.energy) ylim = max(y1, y2) plot(NULL, xlim=range(x), ylim=range(0,ylim), xlab=&quot;Frames&quot;, ylab=&quot;Magnitude (Scaled)&quot;, main=&quot;Log Energy vs MFCC Energy&quot;, frame=TRUE) grid(lty=3, col=&quot;lightgrey&quot;) lines(x, y1, col=&quot;darksalmon&quot;) lines(x, y2, col=&quot;darkgreen&quot;) legend(100,0.5, inset=.02, c(&quot;log raw energy&quot;, &quot;mfcc energy&quot;), fill=c(&quot;darksalmon&quot;, &quot;darkgreen&quot;), horiz=FALSE, cex=0.8) Figure 13.44: Log Energy vs MFCC Energy Then, we get the log energy of the deltas: log.delta.energy = get.log.energy(delta.energies) str(log.delta.energy) ## num [1:182] 7.9 8.62 9.55 10.26 10.4 ... log.delta.delta.energy = get.log.energy(delta.delta.energies) str(log.delta.delta.energy) ## num [1:182] 14.1 14.3 13.6 14.3 16 ... We then concatenate all 39 coefficients like so. mfcc.features = array( rbind(mels.coefficients, delta.energies, delta.delta.energies, log.mfcc.energy, log.delta.energy, log.delta.delta.energy), c(39, 182)) str(mfcc.features) ## num [1:39, 1:182] 88.81 -17.04 -18.11 1.4 -6.97 ... We show the long, intricate steps to extract features from individual frames of an entire sound wave. Existing platforms offer an easier way to extract features through APIs. For example, the melfcc(.) function below is enough to extract the 12 Mel coefficients prior to liftering: mfcc.cepstra = melfcc(audio, sr = audio@samp.rate, # Signal Rate wintime = frm$duration, # Window length hoptime = frm$stride.time, # Successive windown inbetween numcep = 12, # By default it will be 12 features sumpower = TRUE, # frequence scale transformation nbands = 40, # Number of spectra bands, filter banks bwidth = 1, # Width of spectral bands preemph = 0.97, # pre Emphasis minfreq = fmin, maxfreq = fmax, fbtype = &quot;htkmel&quot;, dcttype = &quot;t3&quot;, dither = FALSE, frames_in_rows = FALSE ) str(mfcc.cepstra) ## num [1:12, 1:182] 88.803 -6.689 -6.914 0.525 -2.67 ... Other literature may find it helpful to extract other features related to pitch, intensity, zero-cross rate, and on. We leave readers to investigate such features and their necessity. Having extracted the features, we can now rely on modeling techniques such as LSTM and Transformer Neural Network (specifically around Attention) to encode and decode features. However, one of the challenges of sequence-based applications, especially in Handwriting recognition, is alignment. This is where CTC comes to play. 13.6.3 Connectionist Temporal Classification (CTC) CTC was introduced by Alex Graves et al. (2006; 2013). The former paper showcases a hybrid architecture, combining HMM and Bidirectional LSTM for sequence-based applications such as Speech Recognition. It explains the idea of a dynamic programming algorithm in reference to the approach used by HMM forward and backward algorithms and in further reference to the Viterbi (Decoding) Algorithm. We, therefore, encourage readers to revisit the three algorithms we discussed under the Bayesian Model section in Chapter 8 (Bayesian Computation II). If we are to describe CTC, it is a technique primarily associated with the alignment of sequence-based input in that it collapses repeated tokens so that: \\[ A, B, B, C\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ \\ A,B,C \\] Additionally, it also recognizes that certain sequence of tokens, such as the word too, should be kept intact. \\[ T, O, O,\\ \\ \\ \\ \\ \\rightarrow\\ \\ \\ \\ \\ \\ T, O, O \\] In Handwriting recognition, repeated tokens occur when some handwritten tokens get wide enough to extend beyond a segment’s boundaries. This is granting we evenly partition or segment a handwritten text into frames. However, a portion of a token becomes visible across frames such that during the segmentation process, tokens may be interpreted as belonging to two frames - thus, there is duplication of tokens. CTC removes these duplicated tokens. Moreover, it does it through a modified version of the HMM forward and backward algorithms in which we add an extra blank token represented by (“-”). To visualize the algorithm, we use a graph as shown in Figure 13.45 (Waseem Gharbieh 2018). Figure 13.45: CTC Forward and Backward Method In the figure, we construct a graph using the classic forward and modified forward methods. The difference between the two graphs is that the latter has extra rows containing blank nodes added in between rows containing token nodes. Additionally, blank rows are added at the beginning and end rows. Each column represents a timestep. Each row represents the target tokens in their correct order from the ground truth. We also include edges that traverse the graph from the left-most token node at the top to the right-most token node at the bottom. The edges follow a monotonic path such that a token node connects to a token node at the same level or next level in the next timestep. For the modified version, a token node creates an extra edge to another token node across a blank node in the next timestep (only if both token nodes do not have the same token symbols). Now using the former method, to get from token node (a) at \\((y_0, x_0)\\) to token node (d) at \\((y_3, x_6)\\), we can travel using the path below: \\[\\begin{align*} \\text{seq}(aaabccd) = a(y_0, x_0)\\rightarrow a(y_0, x_1)\\rightarrow a(y_0, x_2)\\rightarrow b(y_1, x_3)\\rightarrow \\\\ c(y_2, x_4)\\rightarrow c(y_2, x_5)\\rightarrow d(y_3, x_6) \\end{align*}\\] Here, we can score the path by calculating the product of the probabilities of all nodes along the path such that we have: \\[\\begin{align} \\pi = P(path) = P(\\text{seq}(aaabcdd)) = P(a(y_0, x_0))\\times P(a(y_0, x_0))\\times ... \\times P(d(y_3, x_6)) \\end{align}\\] The same calculations apply to the modified version with blank rows. For example, we have the following path: \\[\\begin{align} \\text{seq}(\\text{aa-bc-d}) = a(y_0, x_0)\\rightarrow a(y_0, x_1)\\rightarrow \\text{&quot;-&quot;} \\rightarrow b(y_1, x_3)\\rightarrow c(y_2, x_4)\\rightarrow \\text{&quot;-&quot;} \\rightarrow d(y_3, x_6) \\end{align}\\] And its probabilities: \\[\\begin{align} \\pi &amp;= P(path) = P(\\text{seq}(\\text{aa-bc-d})) \\\\ &amp;= P(a(y_0, x_0))\\times P(a(y_0, x_1))\\times P(\\text{&quot;-&quot;}) \\times ... \\times P(d(y_3, x_6)) \\end{align}\\] We notice that we can construct multiple paths going from (a) to (d). Therefore, we should also consider the paths generated by the backward methods (especially the modified version for CTC) as means to validate. Ultimately, our goal is to find the best (cost-effective) path, possibly one with the highest probability. This is where we can use the Beam Search method with the Viterbi Algorithm. We leave readers to investigate Beam Search and how it relates to Breadth-First Search (BFS) and Best First Search (BeFS) algorithms. 13.6.4 Model Evaluation To close this section around Speech Recognition and its equivalent applications, such as Text Summarization and Machine Translation, we cover three evaluation metrics that help evaluate our models’ effectiveness. WER stands for Word Error Rate, which is based on Levenstein distance, and it is simply a measure of the rate of error. The equation is written below: \\[\\begin{align} \\mathbf{\\text{WER}} = \\frac{S + D + 1}{N} \\end{align}\\] where: S - number of subsitutions D - number of deletions I - number of insertions N - number of words in the reference (ground truth) Given the following texts: \\[ \\begin{array}{ll} \\mathbf{\\text{Ground Truth:}}&amp;\\text{I travel the world in search for the fountain of youth}\\\\ \\\\ \\mathbf{\\text{Predicted Text:}}&amp;\\text{I travel the world in search for the highest mountain} \\end{array} \\] We note that the word highest is inserted when comparing the two texts. Furthermore, the word fountain is substituted by the word mountain, and two words are deleted, namely of and youth. Therefore, we can calculate WER like so: \\[ \\mathbf{\\text{WER}} = \\frac{1 + 2 + 1}{11} = 0.3636364 = 36\\% \\] Note that WER relies only on word operations; thus, if we pre-process the texts such that we account for lemmatization and stemming, we may be able to reduce the error rate, granting our application necessitates to use those pre-processing methods. Otherwise, we begin to see the limitation of WER. Perplexity is a measure of randomness and is somewhat related to entropy which we covered in the Information Theory section under Bayesian Computation. Other literature commonly describes this metric as to how well a probability distribution can predict a sample (Wikipedia). We use the following equation. \\[\\begin{align} \\mathbf{Perplexity}(W) = P(w_1 \\times w_2 \\times ... \\times w_n)^{-\\frac{1}{N}} \\end{align}\\] Suppose we have a unique set of only 10 unigrams in our vocabulary, and we calculate the probability of each unigram to occur (non-replaceable). The assumption is that the sum probability of all the unigrams combined should be 1. If each unigram has an equal probability of occurring in the set, we are looking at a 0.01 probability. Now, if we then calculate the Perplexity, we see the following: \\[\\begin{align} \\mathbf{Perplexity}(w_1) = \\left(\\mathcal{P_{w_1}}\\right)^{-\\frac{1}{N}} = (0.01)^{-\\frac{1}{2}} = 100 \\end{align}\\] A perplexity of 100 indicates low probability. The same is true for any combination of the words: (x = rep(0.01, 10)) ## [1] 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 perplexity &lt;- function(x) { N = length(x); round(prod(x)^(-1/N))} perplexity(x[1]) ## [1] 100 perplexity(c(x[1], x[3], x[5])) ## [1] 100 Now, assume we change the probability distribution such that the first unigram has a 0.90 probability of occurring while the rest have an equal probability at 0.01. (x = c(0.90, rep(0.01, 9))) ## [1] 0.90 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 perplexity(x[1]) # this has the 0.90 probability ## [1] 1 perplexity(c(x[1], x[3], x[5])) # this has 0.90, 0.01, 0.1 probabilites (mix) ## [1] 22 perplexity(c(x[2], x[4], x[6])) # this has 0.01, 0.01, 0.01 probabilities ## [1] 100 A perplexity of 1 indicates high probability. Note that N-grams also apply in this case. BLEU stands for BiLingual Evaluation Understudy introduced by Kishore Papineni et al. (2002). It is a measure between 0 and 1 that evaluates machine translation quality. Below, we reference the formulations from the paper. \\[\\begin{align} P_n = \\frac{\\sum_{C\\in \\{Candidates\\}} \\sum_{\\text{n-gram}\\in C} \\mathbf{Count}(clip(\\text{n-gram}))} {\\sum_{C&#39;\\in \\{Candidates\\}} \\sum_{\\text{n-gram&#39;}\\in C&#39;} \\mathbf{Count}(\\text{n-gram&#39;})} \\end{align}\\] where \\(P_n\\ \\text{is N-gram Precision}\\) Note that the equation above is a modified formulation for calculating an N-gram Precision because of clipping, eliminating redundant N-grams. \\[\\begin{align} BP \\times \\exp\\left[\\frac{1}{N}\\sum_{n=1}^N \\left(\\log P_n\\right)\\right], \\ \\ \\ \\ \\ BP =\\begin{cases} 1 &amp; \\text{if c} &gt; r\\\\ exp(1 - \\frac{r}{c}) &amp; \\text{if c } \\le r \\end{cases} \\label{eqn:eqnnumber808} \\end{align}\\] where BP is Brevity Penality, r represents reference corpus length, c is the length of candidate translation, and N is the N-gram to consider. Here, we use N=4 (up to 4-grams). To use this metric, we need to have a list of references and candidates. For example: \\[ \\begin{array}{ll} \\mathbf{\\text{Candidate 1}}:\\text{I wish to travel the the world}\\ \\ \\ \\ (generated)\\\\ \\\\ \\mathbf{\\text{Reference 1}}:\\text{I did travel the world}\\\\ \\mathbf{\\text{Reference 2}}:\\text{I like traveling the world}\\\\ \\mathbf{\\text{Reference 3}}:\\text{I really wish to travel the world}\\\\ \\end{array} \\] Here, candidate refers to the text generated from our model. We start by calculating the N-gram Precision, a way to count the number of words that match texts. Let us now reference the table below for the four texts in our example: \\[ \\begin{array}{lllll} \\mathbf{\\text{N-Grams}} &amp; \\mathbf{\\text{Unigram}} &amp; \\mathbf{\\text{Bi-Gram}} &amp; \\mathbf{\\text{Tri-Gram}} &amp; \\mathbf{\\text{4-Gram}}\\\\ \\mathbf{\\text{Candidate 1}} &amp; 7 &amp; 6 &amp; 5 &amp; 4 \\\\ \\mathbf{\\text{Reference 1}} &amp; 5 &amp; 4 &amp; 3 &amp; 2 \\\\ \\mathbf{\\text{Reference 2}} &amp; 5 &amp; 4 &amp; 3 &amp; 2\\\\ \\mathbf{\\text{Reference 3}} &amp; 7 &amp; 6 &amp; 5 &amp; 4\\\\ \\end{array} \\] Comparing candidate one and reference one, we see that the words I, travel, the, and world match; therefore, using the following formula, we have a precision score of: \\[\\begin{align} \\text{Unigram Precision Score} (P_1) = \\frac{\\text{clip(Num of matching 1-gram)}}{\\text{Num of 1-gram Generated}} = \\frac{4}{7} = 0.5714286 \\end{align}\\] Notice that, because of clipping, the word the that occurs twice in our generated text only receives a count of 1. Let us calculate the precision for the 2-gram, 3-gram, and 3-gram. Assume that our model has not gone through lemmatization and stemming. We compare the bare N-grams: \\[ \\text{2-gram Precision Score} (P_2) = \\frac{2}{6} = 0.33 \\] Here, the 2-grams that match are {travel the} and {the world} between candidate 1 and reference 1 texts. \\[ \\text{3-gram Precision Score} (P_3) = \\frac{0}{5} = 0 \\] Here, there are no 3-grams that match. \\[ \\text{4-gram Precision Score} (P_4) = \\frac{0}{4} = 0 \\] Here, there are no 4-gram combinations that match. Now, to get the BLUE score, we take the average of the log of \\(P_n\\). Notice below that if there are no matching grams, we obtain the log of zero, which renders \\(-\\infty\\). Therefore, it may help mathematically to add \\(\\text{eps}=1e^-100\\) (our choice). \\[ \\begin{array}{ll} \\text{BLEU} &amp;= BP \\times \\exp\\left(\\frac{1}{4}\\left[ \\log\\left(\\frac{4}{7}\\right) + \\log\\left(\\frac{2}{6}\\right) + \\log\\left(\\frac{0}{5}\\right) + \\log\\left(\\frac{0}{4}\\right)\\right]\\right)\\\\ &amp;=BP \\times \\text{3.1239399e-51} = 0 \\end{array} \\] eps = 1e-100; N = 4 exp(1/N * (log(4/7) + log(2/6) + log(eps/5) + log(eps/4))) ## [1] 3.1239399e-51 The BP is calculated based on r = 5 and c = 7. There are 7 words in candidate 1 and 5 words in reference 1. Therefore, obtain the below BP for our case: r = 5; c = 7 (BP = ifelse(c &gt; r, 1, exp(1 - r/c))) ## [1] 1 13.7 Generative Adversarial Network (GAN) GAN was introduced by Ian J. Goodfellow et al. in (2014). The basic concept of GAN is simple. We model two deep neural networks that pit against each other (thus adversarial). One model is trained by one network to generate synthetic data (also known as augmentation), and the other is trained by the other network to discriminate against the generated data. Throughout this training process, the ultimate goal is to have both models from the two networks, if trained and optimized well, ironically work together (in an adversarial fashion) to ultimately generate the most plausible data, rendering the data as though it is produced realistically. On the other hand, the discriminative nature of the other network makes it possible to allow the model to be powerful enough to discriminate against spam as an example. Additionally, ideas around GAN extend to the design of games. Two key architectural components that make GAN effective are the generator and discriminator. In image processing, each component can be architected using CNN, e.g., Deep Convolutional GAN (DCGAN). In terms of a generator, there are methods proposed by papers on the best way to generate data. Among many methods, below lists the fundamental methods that can be applied. They are covered in Bayesian Computation. Rejection sampling Metropolis-Hasting algorithm As for GAN models, there are evaluation methods and metrics proposed by papers to evaluate the quality of the image produced by these models. However, it seems other literature agrees that there is no consensus as to which evaluation method is accepted as standard. Therefore, we only mention two common evaluation methods for GAN, namely Inception Score (IS) and Fréchet Inception Distance (FID). The latter is an enhanced variant of the former in terms of coverage. While IS focuses on evaluation against the generated images, FID also accommodates authentic images. Both evaluation metrics sit upon the fundamental idea of KL Divergence in which probability distributions are compared and measured. Now, in terms of discriminator, this component acts as a critic. Its role is to distinguish fake data from actual data. Therefore, it becomes beneficial for the discriminator to maximize the distance between the two data. That is where we cover WGAN. A variant of GAN called Wasserstein GAN (WGAN) was introduced by Martin Arjovsky et al. (2014). WGAN covers the idea of Wasserstein distance metrics introduced by Leonid Vaserštein in 1969. Like KL Divergence, Wasserstein (W) metric compares probability distributions. However, unlike KL divergence, W is symmetric. For example, KL divergence follows the condition below (which is not symmetric): \\[\\begin{align} P(q||p) \\ne P(p||q) \\end{align}\\] Additionally, WGAN is also called Earth Mover’s distance because, in a way, it measures distance based on how far to transport pieces of distribution - the distribution mass - from one region to another. In other words, we measure the distance between two regions instead of between specific points of two regions. In doing so, we allow much broader coverage of distance measurement, and it helps the discriminator identify fake data, especially if the type of data is an image. We leave readers to investigate other distance measurements such as Kolmogorov-Smirnov distance and Jensen-Shannon distance. 13.8 Deep Reinforcement Network (DQN) DQN was introduced by DeepMind in 2015. What separates DQN from vanilla DNN architectures using MLP, CNN, RNN, TNN, and GAN is the use of reward and penalty concept. If we can design a DNN to be motivated or encouraged to learn (also called Reinforcement Learning), that pushes one more step closer to our goal of an Artificial Intelligence (AI) system. In designing DQN, other literature introduces an agent that gets rewarded for doing a good job and gets penalized for a bad job. Therefore, it is sensible to assume that the agent should at least try to remember (having an associative memory) for what makes a job rewarding. This cumulative reward encourages the agent to move in that direction every single time. Now, how the agent is penalized is based on specific rules or policies. An agent should abide by such restrictions to avoid penalties. One of the tricks used is a trial and error approach. By attempting different actions, the agent can adapt. It accumulates these long-term actions to be able to survive in any situational conditions. Applications that may benefit from DQN are self-healing and self-adaptive applications. For example, anomaly detection also falls under this category. 13.9 Summary We end this chapter keeping in mind the different Deep Neural Network that we introduced, starting with MLP followed by CNN, ResNet, RNN and its variants LSTM, GRU, BiLSTM, BiGRU, then TNN. Almost all DNN architectures rely on a combination of CNN, RNN, and TNN stacked or layered one over the other. It is indeed just a matter of creativity and sound design to produce an architecture with promising results. The problems that such DNN architectures try to solve rely on heavy computation so using GPUs and cloud-based distributed systems also needs to be considered. If such architecture can be designed more granularly and in a more distributed parallel fashion, then the needed speed of training and inference can be achieved. "],["distributedcomputation.html", "Chapter 14 Distributed Computation 14.1 Integration and Interoperability 14.2 ML Pipelines 14.3 Open Standards 14.4 General Summary", " Chapter 14 Distributed Computation By going through data processing and modeling, the realization of our efforts is reflected by a well-trained ML model. The next step to consider is how to manage this ML model. So far, we have not covered the concept of Model Management in detail, and it is crucial that models go through some automated, distributed, and collaborative life-cycle management. While this topic requires an understanding of technology, we try to avoid highlighting any specific technology. Instead, let us focus on what is available in R. We know that technology comes and goes. So the takeaway in this chapter is to build some foundation on how to manage models. 14.1 Integration and Interoperability ML models are used primarily for inferencing. Our goal is to use ML models to predict based on new data. However, there are cases when one model needs to be shared across frameworks and platforms. This is where we need to consider the Portability of models. In this section, we shall introduce the idea of Portability to address Integration and Interoperability of ML systems. We rely on ML frameworks and ML libraries instead of building our ML algorithms from scratch in any practical sense. However, working in more complex systems and more prominent organizations, we see three challenges. The first challenge is that not all organizations are locked into only one framework or only one programming language. Thus, while there are too many ML frameworks and ML libraries available across a variety of programming languages, it is crucial to recognize that when using any specific framework, the generated ML model results in a format that is different from one generated by another framework. The second challenge is that while certain frameworks may serialize the format into a standard format such as XML and JSON, the structure or schema layout may be different. Each framework would have to agree upon a common structure or a schema layout in what we call a contract, accompanied by rules and restrictions. The third challenge is that scoring a prediction in ML models may rely upon the data and how the data is trained - the key is the set of operators that will help a scoring engine perform correct inference. For example, it may be easy for a simple linear regression model to perform certain fundamental statistical and mathematical functions - such functions can be operationalized into the schema. Different frameworks can implement such basic operations. However, for models trained by layered and nested methods such as Deep Learning, we have to be able to allow extensibility. Therefore, to develop an ML application relying on any framework that can interoperate with others, the three solutions need to be in place: a standard format, a standard schema specification, and a standard operation with the ability to accommodate other operations. This is where Pipelines and API end-points are needed in terms of integration. Therefore, this chapter will introduce three Open Standards, namely PMML, PFA, and ONNX. The discussion of the concept follows next. Note that we do not cover data serialization systems in this book, such as Avro and ProtoBuf. We encourage readers to investigate the two systems and their specifications, however. 14.2 ML Pipelines One would wonder how members of a Data Science team can work together in an environment with an ecosystem of many moving parts and many stakeholders. A strategy to adopt is to rely on ML Pipelines. Model Pipelining is the method of identifying atomic units of work and organizing them into stages, allowing input to be pipelined from one stage to the next in an automated manner. While there may be variations of an ML Pipeline in terms of the modules and stages, let us stick to the generic variant, which comprises four main stages, namely Cleaning, Preprocessing, Training, and Evaluation. See Figure 14.1. Figure 14.1: ML Pipelines In the figure, we see an abstract of modularized pipelines. If we allow flexibility, in a microservice fashion perhaps where each module may be a microservice, we can pipeline the input from stage 1 module 1, stage 2 module 6, stage 3 module 10, and stage 4 modules 12 and 14. For example: input -&gt; stage 1 (module 1) -&gt; stage 2 (module 6) -&gt; stage 3 (modules 10) -&gt; stage 4 (module 12, 14) -&gt; output Note that the output can further be pipelined to yet another stage (e.g., stage 5) for deployment. Note that modules here may not necessarily imply a unit code; instead, they may consist of an interacting group of code aimed for one purpose, for example, a variant of preprocessing or training. And they may be fronted by a microservice via REST. While pipelines are composed of many units of work that are stitched in different ways, such abstraction can be implemented through Big Data - a system designed to handle a massive amount of data. Indeed, it takes a team to design and architect such a system. On the other hand, such resources are already made available through commercial cloud-based services. 14.3 Open Standards Having an open standard format is to allow models to be portable and sharable. The format produced by each algorithm implemented by each framework needs to be converted into a common standard format. To become part of a common standard, we need converters that support a wide range of ML algorithms for every available framework. Once the formats are standardized, we then need an engine to consume the standardized models to perform scoring and inferencing. We need to use producers and consumers in that respect. For example, standards are being developed along with a runtime to consume a model for inference. We shall come to this concept when we talk about PFA and ONNX. Note that in our Machine Learning chapters, we have demonstrated the use of saveRDS(.) and readRDS(.) functions to preserve our models so that we can reuse them as we need without having to go through the training process. However, the two functions store our model in Rdata format and thus may not be portable across other platforms. Nevertheless, let us illustrate how our Rdata can be transformed into a standard format that we can use to transport our models. 14.3.1 Predictive Model Markup Language (PMML) PMML is an XML-based open-standard format championed by the Data Mining Group (DML). To illustrate, let us use pmml R package (Bolotov D. et al 2021). library(pmml) ## Loading required package: XML We first generate a simple linear regression model using mtcars dataset like so: (lm.model = lm(mpg ~ disp + hp + wt + qsec, data = mtcars)) ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) disp hp wt ## 27.3296380 0.0026664 -0.0186662 -4.6091226 ## qsec ## 0.5441603 Let us take a summary of the model: summary(lm.model) ## ## Call: ## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.86642 -1.58194 -0.37882 1.17125 5.64685 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.3296380 8.6390322 3.1635 0.003834 ** ## disp 0.0026664 0.0107377 0.2483 0.805762 ## hp -0.0186662 0.0156130 -1.1956 0.242267 ## wt -4.6091226 1.2658513 -3.6411 0.001134 ** ## qsec 0.5441603 0.4664932 1.1665 0.253616 ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.6221 on 27 degrees of freedom ## Multiple R-squared: 0.83514, Adjusted R-squared: 0.81072 ## F-statistic: 34.195 on 4 and 27 DF, p-value: 3.3106e-10 Now, we convert our model into PMML format like so: options(width=60) pmml.model = pmml(lm.model) Below is the XML-based schema of the model which can then be used by other applications. &lt;PMML version=&quot;4.4.1&quot; xmlns=&quot;http://www.dmg.org/PMML-4_4&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://www.dmg.org/PMML-4_4 http://www.dmg.org/pmml/v4-4/pmml-4-4.xsd&quot;&gt; &lt;Header copyright=&quot;Copyright (c) 2022 author&quot; description=&quot;Linear Regression Model&quot;&gt; &lt;Extension name=&quot;user&quot; value=&quot;author&quot; extender=&quot;SoftwareAG PMML Generator&quot;/&gt; &lt;Application name=&quot;SoftwareAG PMML Generator&quot; version=&quot;2.5.1&quot;/&gt; &lt;Timestamp&gt;2022-02-26 20:38:27&lt;/Timestamp&gt; &lt;/Header&gt; &lt;DataDictionary numberOfFields=&quot;5&quot;&gt; &lt;DataField name=&quot;mpg&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt; &lt;DataField name=&quot;disp&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt; &lt;DataField name=&quot;hp&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt; &lt;DataField name=&quot;wt&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt; &lt;DataField name=&quot;qsec&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot;/&gt; &lt;/DataDictionary&gt; &lt;RegressionModel modelName=&quot;lm_Model&quot; functionName=&quot;regression&quot; algorithmName=&quot;least squares&quot;&gt; &lt;MiningSchema&gt; &lt;MiningField name=&quot;mpg&quot; usageType=&quot;predicted&quot; invalidValueTreatment=&quot;&quot;/&gt; &lt;MiningField name=&quot;disp&quot; usageType=&quot;active&quot; invalidValueTreatment=&quot;&quot;/&gt; &lt;MiningField name=&quot;hp&quot; usageType=&quot;active&quot; invalidValueTreatment=&quot;&quot;/&gt; &lt;MiningField name=&quot;wt&quot; usageType=&quot;active&quot; invalidValueTreatment=&quot;&quot;/&gt; &lt;MiningField name=&quot;qsec&quot; usageType=&quot;active&quot; invalidValueTreatment=&quot;&quot;/&gt; &lt;/MiningSchema&gt; &lt;Output&gt; &lt;OutputField name=&quot;Predicted_mpg&quot; optype=&quot;continuous&quot; dataType=&quot;double&quot; feature=&quot;predictedValue&quot;/&gt; &lt;/Output&gt; &lt;RegressionTable intercept=&quot;27.3296379667147&quot;&gt; &lt;NumericPredictor name=&quot;disp&quot; exponent=&quot;1&quot; coefficient=&quot;0.002666431092&quot;/&gt; &lt;NumericPredictor name=&quot;hp&quot; exponent=&quot;1&quot; coefficient=&quot;-0.018666202487&quot;/&gt; &lt;NumericPredictor name=&quot;wt&quot; exponent=&quot;1&quot; coefficient=&quot;-4.609122616961&quot;/&gt; &lt;NumericPredictor name=&quot;qsec&quot; exponent=&quot;1&quot; coefficient=&quot;0.544160312015&quot;/&gt; &lt;/RegressionTable&gt; &lt;/RegressionModel&gt; &lt;/PMML&gt; Note that the invalidValueTreatment attribute is left empty to fit the text in this book. Nonetheless, consumers of the PMML-based model can use the MiningSchema to evaluate fields in the schema for validity. For example, if values are invalid or missing or data types are incorrect, the application can prescribe remedies (or treatments). Now, it is essential to point out that PMML-based model producers have to strictly follow the PMML standard to be genuinely portable while avoiding unnecessary efforts for the consumers of the schema to patch additional workarounds. However, a Document Type definition (DTD) for PMML can be downloaded and used by consumers of the model schema to validate that PMML rules are correctly followed. Note that the PMML specification includes a DataDictionary structure, MiningField structure, an Output structure, RegressionTable structure, and the RegressionModel structure. The RegressionModel describes the algorithm used (which is Least-Squares in this case). The RegresssionTable structure keeps a record of the model’s coefficients to be used for inference. The DataDictionary and MiningField structures describe a model’s dataset and data type (e.g., continuous and double). While not seen in our example, also note that PMML specification includes data transformations. For example, such XML-based transformations include normalization, discretization, aggregation, and other transformation functions. However, PMML is not extensible enough, as pointed out by Jim Pivarski in his PFA paper, as it does not allow accommodating other functions or operations. Let us discuss PFA next. 14.3.2 Portable Format for Analytics (PFA) PFA is a JSON-based format developed by Jim Pivarski et al. (2016), and the Data Mining Group is championing the standard. To illustrate, let us use the aurelius R package (Mortimer S. and Collin Bennett C. 2017). library(aurelius) Similarly, we can generate a simple linear regression model and ingest the model into the converter like so: options(width=60) pfa.model = pfa(lm.model) str(pfa.model) ## List of 4 ## $ input :List of 3 ## ..$ type : chr &quot;record&quot; ## ..$ fields:List of 4 ## .. ..$ :List of 2 ## .. .. ..$ name: chr &quot;disp&quot; ## .. .. ..$ type: chr &quot;double&quot; ## .. ..$ :List of 2 ## .. .. ..$ name: chr &quot;hp&quot; ## .. .. ..$ type: chr &quot;double&quot; ## .. ..$ :List of 2 ## .. .. ..$ name: chr &quot;wt&quot; ## .. .. ..$ type: chr &quot;double&quot; ## .. ..$ :List of 2 ## .. .. ..$ name: chr &quot;qsec&quot; ## .. .. ..$ type: chr &quot;double&quot; ## ..$ name : chr &quot;Input&quot; ## $ output: chr &quot;double&quot; ## $ action:List of 3 ## ..$ :List of 1 ## .. ..$ let:List of 1 ## .. .. ..$ glm_input:List of 2 ## .. .. .. ..$ type:List of 2 ## .. .. .. .. ..$ type : chr &quot;array&quot; ## .. .. .. .. ..$ items: chr &quot;double&quot; ## .. .. .. ..$ new :List of 4 ## .. .. .. .. ..$ : chr &quot;input.disp&quot; ## .. .. .. .. ..$ : chr &quot;input.hp&quot; ## .. .. .. .. ..$ : chr &quot;input.wt&quot; ## .. .. .. .. ..$ : chr &quot;input.qsec&quot; ## ..$ :List of 1 ## .. ..$ let:List of 1 ## .. .. ..$ pred:List of 1 ## .. .. .. ..$ model.reg.linear:List of 2 ## .. .. .. .. ..$ : chr &quot;glm_input&quot; ## .. .. .. .. ..$ :List of 1 ## .. .. .. .. .. ..$ cell: chr &quot;glm_model&quot; ## ..$ : chr &quot;pred&quot; ## $ cells :List of 1 ## ..$ glm_model:List of 5 ## .. ..$ type :List of 3 ## .. .. ..$ type : chr &quot;record&quot; ## .. .. ..$ fields:List of 2 ## .. .. .. ..$ :List of 2 ## .. .. .. .. ..$ name: chr &quot;const&quot; ## .. .. .. .. ..$ type: chr &quot;double&quot; ## .. .. .. ..$ :List of 2 ## .. .. .. .. ..$ name: chr &quot;coeff&quot; ## .. .. .. .. ..$ type:List of 2 ## .. .. .. .. .. ..$ type : chr &quot;array&quot; ## .. .. .. .. .. ..$ items: chr &quot;double&quot; ## .. .. ..$ name : chr &quot;gaussianRegression&quot; ## .. ..$ init :List of 2 ## .. .. ..$ const: num 27.3 ## .. .. ..$ coeff:List of 4 ## .. .. .. ..$ : num 0.00267 ## .. .. .. ..$ : num -0.0187 ## .. .. .. ..$ : num -4.61 ## .. .. .. ..$ : num 0.544 ## .. ..$ source : chr &quot;embedded&quot; ## .. ..$ shared : logi FALSE ## .. ..$ rollback: logi FALSE Notice that the PFA model is converted into a list-list structure in R. We can recognize four main parts: a sublist of input, a sublist of output, a sublist of action, and a sublist of cells. Similar to PMML, the cell sublist contains the model’s coefficients used for inference. The input sublist contains the dataset structure used by the mode. The structure follows the Avro specification. Moreover, the action describes a linear regression used for inference. We can then serialize the structure into a JSON format using toJSON(.) function or serializeJSON(.) function: library(jsonlite) pfa.json.model = toJSON(pfa.model, pretty=TRUE, auto_unbox=TRUE) pfa.json.model = serializeJSON(pfa.model) 14.3.3 Open Neural Network Exchange (ONNX) Another open standard format that supports the standard format for deep learning models is ONNX. The standard format follows the acyclic graph concept in which nodes represent the operators, and edges represent the tensors. These tensors can be treated as messages passed from node to node (as shown in CNN). Therefore, it is natural to see tools such as Netron that can graphically present ONNX models. It is worth noting that while PFA follows Avro specification for data structures, ONNX follows Protocol Buffer specification. ONNX may continue to receive wide acceptance and may continue to evolve in terms of supporting other frameworks. At this point, we encourage readers to investigate any available support for such frameworks in R. One may have to investigate the onnx R package (Yuan T. 2021). library(onnx) A scoring engine called ONNX runtime is available for inference. ONNX models are ingested into the runtime to perform prediction. In terms of resources, it helps to explore the devices that support the ONNX runtime. One of the goals is to be able to deploy the ONNX runtime for Edge computing to extend the ability of Edge devices to perform distributed ML computing, at least for inferencing. 14.4 General Summary Throughout this book, we have shown, time and time again, the power and art of approximation, solving problems from the most straightforward root-finding problems to the more complex machine translations that require deep learning. Because we deal with approximations on a daily basis, it is just natural to see the continuous discovery of new scientific and artful tricks and techniques. Other existing tricks and techniques continue to evolve as well. As long as we continue to improve the accuracy of our approximations, there is no telling where we go from here. We know that data is an asset, and as Data Scientists, our contribution to society is to provide meaningful information that creates a positive impact, not just for monetary impact. If we can affect the lives of everyday people through perhaps smart automated assistance, that is already a good first step. From there, we can only envision this - it would seem that we may, one day in the future, find ourselves guided and assisted by consumer-based conversational (perhaps even sentient) devices equipped with artificial intelligence (with LLMs (Large Language Models) or GPTs (Generative Pretrained Transformers) as a starting point), all because of the power and art of approximation. "],["appendix.html", "Chapter 15 Appendix 15.1 Appendix A 15.2 Appendix B 15.3 Appendix C 15.4 Appendix D", " Chapter 15 Appendix 15.1 Appendix A This appendix presents short references of some concepts in trigonometry, logrithms, and category theory. 15.1.1 Trigonometry Trigonometry is about triangles, and angles. Multiple triangles can form other shapes such as squares and rectangles. Figure 15.1: Triangles Pythagorean Theorem Figure 15.2: Pythagorean Triangle \\[ c^2 = a^2 + b^2 \\] \\[\\begin{align*} SOH: sin\\ \\beta &amp; = \\frac{opposite}{hypotenuse} \\\\ CAH: cos\\ \\alpha &amp; = \\frac{adjacent}{hypotenuse} \\\\ TOA: tan\\ \\theta &amp; = \\frac{opposite}{adjacent} \\\\ sec\\ \\theta &amp; = \\frac{hypotenuse}{adjacent} \\\\ csc\\ \\theta &amp; = \\frac{hypotenuse}{opposite} \\\\ co\\ \\theta &amp; = \\frac{opposite}{adjacent} \\end{align*}\\] ## quartz_off_screen ## 2 Figure 15.3: Sine And Cosine Curves Unit Circle A unit circle serves as quick reference for conversion between degrees to radians, or between radians to fractions without pi \\(\\pi\\). Figure 15.4: Unit Circle Using the unit circle as reference, we can quickly derive the answers for the questions below: To get the cosine, sine, and tangent of a right triangle at 30° angle: \\[ cos\\ \\alpha = \\frac{\\sqrt{3}}{2}, \\ \\ \\ \\sin\\ \\beta = \\frac{1}{2}, \\ \\ \\ \\tan \\theta = \\frac{1}{\\sqrt{3}} \\] To get the cosine, sine, and tangent of a right triangle at 60° angle: \\[ cos\\ \\alpha = \\frac{1}{2}, \\ \\ \\ \\sin\\ \\beta = \\frac{\\sqrt{3}}{2}, \\ \\ \\ \\tan \\theta = \\sqrt{3} \\] To get the cosine, sine, and tangent at 75° angle of a right triangle: \\[ cos\\ \\alpha = \\frac{\\sqrt{3} + 1}{\\sqrt{2}}, \\ \\ \\ \\sin\\ \\beta = \\frac{\\sqrt{3} - 1}{\\sqrt{2}}, \\ \\ \\ \\tan \\theta = \\frac{\\sqrt{3} - 1}{\\sqrt{3} + 1} \\] Common Trigonometric Formulas Table 15.1: Trigonometric Formulas Properties Formula Power Properties \\(sin^2x = 1 - cos^2x\\) \\(cos^2x = 1 - sin^2x\\) \\(sec^2x = 1 + tan^2x\\) \\(cos^2x + sin^2x = 1\\) \\(sec^2x - tan^2 = 1\\) Product Properties \\(sin\\ x\\cdot sin\\ y = \\frac{cos(x+y)+cos(x-y)}{2}\\) \\(cos\\ x\\cdot cos\\ y = \\frac{cos(x+y)-cos(x-y)}{2}\\) \\(sin\\ x\\cdot cos\\ y = \\frac{sin(x+y)+sin(x-y)}{2}\\) Sum/Minus Properties \\(sin\\ x + sin\\ y = 2 sin \\frac{x+y}{2} cos \\frac{x-y}{2}\\) \\(sin\\ x - sin\\ y = 2 cos \\frac{x+y}{2} sin \\frac{x-y}{2}\\) \\(cos\\ x + cos\\ y = 2 cos \\frac{x+y}{2} cos \\frac{x-y}{2}\\) \\(cos\\ x - cos\\ y = 2 sin \\frac{x+y}{2} sin \\frac{x-y}{2}\\) Reciprocal Properties \\(sin\\ x = \\frac{1}{csc\\ x}\\) \\(cos\\ x = \\frac{1}{sec\\ x}\\) \\(tan\\ x = \\frac{1}{cot\\ x}\\) \\(cot\\ x = \\frac{1}{tan\\ x}\\) \\(sec\\ x = \\frac{1}{cos\\ x}\\) \\(csc\\ x = \\frac{1}{sin\\ x}\\) Double Angle \\(sin(2x) = 2sin\\ x\\ cos\\ x\\) \\(cos(2x) = cos^2x\\ -\\ sin^2x\\) \\(cos(2x) = 2cos^2x\\ - 1\\) \\(cos(2x) = 1\\ -\\ 2sin^2x\\) \\(tan(2x) = \\frac{2\\ tan(x)}{1-tan^2x}\\) Half Angle \\(sin(\\frac{x}{2}) = \\pm\\sqrt{\\frac{1\\ -\\ cos\\ x}{2}}\\) \\(cos(\\frac{x}{2}) = \\pm\\sqrt{\\frac{1\\ + \\ cos\\ x}{2}}\\) \\(tan(\\frac{x}{2}) = \\sqrt{\\frac{1\\ -\\ cos\\ x}{1\\ +\\ cos\\ x}}, also\\) \\(tan(\\frac{x}{2}) = \\frac{1\\ -\\ cos\\ x}{sin\\ x}\\) Sum/Minus of Angles \\(sin(x \\pm y) = sin\\ x\\ cos\\ y\\ \\pm cos\\ x\\ sin\\ y\\) \\(cos(x \\pm y) = cos\\ x\\ cos\\ y\\ \\pm sin\\ x\\ sin\\ y\\) \\(tan(x + y) = \\frac{tan(x) + tan(y)}{1 - tan(x)tan(y)}\\) \\(tan(x - y) = \\frac{tan(x) - tan(y)}{1 + tan(x)tan(y)}\\) Other Formulas \\(a^2 = b^2\\ +\\ c^2 - 2bc\\ cos\\ A\\) \\(b^2 = a^2\\ +\\ c^2 - 2ac\\ cos\\ B\\) \\(c^2 = a^2\\ +\\ b^2 - 2ab\\ cos\\ C\\) \\(\\frac{a}{sin\\ A} = \\frac{b}{cos\\ B} = \\frac{c}{sin\\ C}\\) \\(sin\\ y = x \\to y = sin^{-1}x\\) 15.1.2 Logarithms Below we list a few logarithmic and Euler’s formulas Table 15.2: Logarithmic and Exponents Formulas Properties Formula Logarithms \\(log_{a}(x\\cdot y) = log_{a}x + log_{a}y\\) \\(log_{a}(\\frac{x}{y}) = log_{a}x - log_{a}y\\) \\(log_{a}(x^n) = n\\cdot\\ log_{a}x\\) Natural Logarithms \\(ln(x) = log_{e}x\\) \\(ln(x\\cdot y) = ln(x) + ln(y)\\) \\(ln(\\frac{x}{y}) = ln(x) - ln(y)\\) \\(ln(x^n) = n\\cdot ln(x)\\) \\(ln(\\frac{1}{x}) = -ln(x)\\) \\(ln(1) = 0\\) Euler’s \\(ln(e^x) = x\\) \\(ln(e) = 1\\) \\(e^{ln(x)} = x\\) \\(e^{(ln(x)\\cdot y)} = e^{ln (x) +ln (y)} = e^{ln (x)}\\cdot e^{ln(y)} = x\\cdot y\\) \\(\\frac{e^a}{e^b} = e^{a-b}\\) \\((e^a)^b = e^{ab}\\) \\(e^{ln(x^y)} = \\frac{x}{y} = \\frac{e^{ln(x)}}{e^{ln(y)}} = e^{ln(x) - ln(y)}\\) Other Formulas \\((a^0) = 1\\) \\((a^n)^m = a^{n\\ \\cdot\\ m}\\) \\(a^n\\cdot\\ a^m = a^{n\\ +\\ m}\\) \\(\\frac{a^n}{a^m} = a^{n\\ -\\ m}\\) \\(a^{-n} = \\frac{1}{a^n}\\) 15.1.3 Category Theory Let us cover a few concepts around Category Theory and Abstract Algebra. Surjective vs Injective: Consider the following notation: \\[ F: X \\rightarrow Y \\] Given a set of X and a set of Y, a function, denoted as F, maps elements of X to elements of Y. Figure 15.5 illustrates a function characterized by the mapping between X elements and Y elements. The set of X is what we call X domain, and the set of Y to which elements are mapped is called Y co-domain. Figure 15.5: Surjective and Injective A function maps elements between the domains in two ways: Surjective: Every element in codomain Y is mapped to at least one element in domain X Injective: Every element in codomain Y is mapped to at most one element in domain X Surjective is also called “onto” mapping. A function with “onto” mapping is a Surjective function. Injective is also called “one-to-one” mapping. A function with “one-to-one” mapping is an Injective function. Range - or image - is the set of elements in the codomain Y that are mapped to. If an element is not mapped to in codomain Y, then that element does not belong to the Range. If the range has the same set of elements as the codomain Y itself, then the mapping is surjective. Based on that, a function has four combinations of those types of mapping. Surjective and Injective (Bijective) Surjective and Non-Injective Non-Surjective and Injective Non-Surjective and Non-Injective For example, in Figure 15.6, we have three graphs in a cartesian plane: a line, a quadratic curve, and a cubic curve. A data point in the line or curve represents a coordinate (x, y). Here, coordinate (x1,y1) is associated with a function, e.g. \\(f(x1) = y1\\). Figure 15.6: Properties of Function Suppose that the first graph with a straight line has the following domains: domain(X) = {x1, x2} codomain(Y) = {y1, y2} where: \\[\\begin{align*} f: x1 \\rightarrow y1\\\\ f: x2 \\rightarrow y2 \\end{align*}\\] In that supposition, we conclude that the function is bijective function - it is both surjective and injective. Suppose, on the other hand, that the same first graph with a straight line has the following domains instead: domain(X) = {x1} codomain(Y) = {y1, y2} where: \\[ f: x1 \\rightarrow y1 \\] In that supposition, we conclude that the function is a non-surjective injective function. It is non-surjective because {y2} in the codomain(Y) does not map to any existing element in domain(X) though there is a one-to-one mapping between x1 and y1. Suppose now that the second graph with the quadratic curve has the following domains: domain(X) = {x1, x2, x3} codomain(Y) = {y1, y2} where: \\[\\begin{align*} f: x1 \\rightarrow y1\\\\ f: x2 \\rightarrow y2\\\\ f: x3 \\rightarrow y1 \\end{align*}\\] In that supposition, we conclude that the function is surjective non-injective function. It is non-injective because multiple X elements, {x1, x3}, maps to {y1}. Suppose lastly that the third graph with the cubic curve has the following domains: domain(X) = {x1, x3, x4} codomain(Y) = {y1, y2} where: \\[\\begin{align*} f: x1 \\rightarrow y1\\\\ f: x3 \\rightarrow y1\\\\ f: x4 \\rightarrow y1 \\end{align*}\\] In that supposition, we conclude that the function is non-surjective non-injective function. It is non-injective because {x1, x3, x4} maps to {y1}. It is non-surjective because {y2} in codomain(Y) does not map to any existing element in the domain(X). Morphism: Morphism is a topic in Category Theory. Here, of the different kinds of categories, we will discuss Morphism in the context of the algebraic (concrete) category. Moreover, in that context, Morphism is a generalization of functions that are either surjective, injective, both, or neither. Let us recall the following notation one more time: \\[ F: X \\rightarrow Y \\] We interpret the notation as: Given a function, we map elements of X to elements of Y. Basically, that operation or function of matching or pairing of elements with the arrow is what we call morphism. In Category Theory, instead of describing the map in terms of functions and elements of domain and codomain, we characterize the map in terms of morphism, the source objects, and the target objects. In a morphism, we do not talk about the domain of X and codomain of Y. We do not talk about elements in a domain or codomain. Furthermore, most of all, we do not talk about elements in a domain or codomain. We only talk about sources and targets. We have a source element (e.g., X) and a target element (e.g., Y). Depending on the groups of elements used in mapping, we can define different categories of morphisms, starting with Homomorphism: Homomorphism - is a morphism that preserves the structure. For example, the linear transformation of a vector, \\(T:\\mathbb{R}^m \\rightarrow \\mathbb{R}^n\\), is Homomorphism since it preserves its vector space structure. Endomorphism - is a homomorphism in which the source X is mapped to the same X as the target. This is also called Identity morphism. \\[ f: X \\rightarrow X \\] Figure 15.7: Identity Morphism Monomorphism - is a homomorphism that generalizes injective functions in that the following notation applies: \\[ f \\circ g1 = f \\circ g2 \\implies g1 = g2 \\] Figure 15.8: Monomorphism Epimorphism - is a homomorphism that generalizes surjective functions in that the following notation applies: \\[ g1 \\circ f = g2 \\circ f \\implies g1 = g2 \\] Figure 15.9: Epimorphism A function that converts a vector into scalar, \\(R:\\mathbb{R}^2 \\rightarrow \\mathbb{R}\\), is an epimorphism. For example, vector \\(\\vec{v} = (v_1, v_2) \\rightarrow v_1 + v_2 = c\\). A determinant operator is epimorphism. We will not be covering the different uses of Epimorphism in this book; but if interest calls for it, it helps to use outside references and be familiar with normal Epimorphism, split epimorphism, strong Epimorphism, and on. Isomorphism - is a homomorphism that generalizes bijective functions. Automorphism - is a bijective endomorphism that generalizes bijective functions. It is an Isomorphism from an object onto itself. 15.2 Appendix B This appendix contains short probability concepts, permutations and combinations. 15.2.1 On Random chances Let us use three of the widely used example of probability: Tossing a fair coin. Rolling a fair 6-sided dice. Drawing a fair card from a deck of 52 cards. Tossing a coin, what is the odd of getting a tail? Total number of outcomes: 2 faces Answer: 1 / 2 = 50% population &lt;- 1:5 sample(population, 5) ## [1] 5 3 4 2 1 population &lt;-c(10,20,30,40) sample(population,2) ## [1] 40 10 population &lt;-c(&quot;Dog&quot;, &quot;Cat&quot;, &quot;Mice&quot;) sample(population,2) ## [1] &quot;Mice&quot; &quot;Dog&quot; sample(population, 9, replace=TRUE) ## [1] &quot;Cat&quot; &quot;Mice&quot; &quot;Mice&quot; &quot;Mice&quot; &quot;Cat&quot; &quot;Cat&quot; &quot;Cat&quot; &quot;Dog&quot; &quot;Cat&quot; Rolling a dice, what is the odd of getting number 6? Total number of outcomes: 6 sides Answer: 1 / 6 = 16.7% Rolling a dice, what is the odd of getting number 3? Answer: 1 / 6 = 16.7% Drawing a deck of cards, what is the odd of getting Queen of Hearts? Total number of outcomes: 52 cards Answer: 1 / 52 = 1.9% Drawing a deck of cards, what is the odd of getting a Queen? Total number of outcomes: 52 cards In a deck: there are 4 queens Answer: 4 / 52 = 7.7% In a deck of cards full of queens, what is the odd of getting a Queen? Answer: 52 / 52 = 100% ( you will always get a queen every single time ) Rolling two dice, what is the odd of getting a number 3 from first dice and number 6 from second dice? First dice: 1 / 6 = 16.7% Second dice: 1 / 6 = 16.7% Answer: 1 / 6 + 1 / 6 = 2 / 6 = 1 / 3 = 33.3% 15.2.2 On Replacements Given a deck of cards, what is the odd of getting a Queen the first time, then another Queen the second time. Consider no replacement. It means, you don’t put back the card after drawing it. Total number of outcomes for the first time: 52 cards First time: 4 / 52 = 7.69% Total number of outcomes for second time: 51 cards (since there is no replacement) Second time: 3 / 51 = 5.88% Answer: 4/52 * 3/51 = 0.45% Given a deck of cards, what is the odd of getting a Queen the first time, then another Queen the second time. Consider replacement. It means, you have to put back the card after drawing it. Total number of outcomes for the first time: 52 cards First time: 4 / 52 = 7.69% Total number of outcomes for second time: 52 cards (since there is replacement) Second time: 4 / 52 = 7.69% Answer: 4/52 * 4/52 = 0.59% Why do we use multiplication? Imagine two unbiased coins. Here are all the possibilities to get the outcome of tossing one coin the first time, then the same coin the second time. Table 15.3: Toss a Fair Coin First Time Second Time T T T H H T H H There are two possibilities from first toss and each of those possibilities may have two other possibilities from the second toss – that makes it 4 combined possibilities ( 2 x 2 ). 15.2.3 On Permutations and Combinations How many ways can we vote for a president and a vice president out of 3 candidates? Let us assume candidates: A B C D E First, we have 5 candidates ( five choices ) to pick a president. Second, once we have chosen a president, there are only 4 candidates ( 4 choices) left for vice president. Answer: 5 * 4 = 20 ways to choose a president and a vice president out of 5 candidates. Looks Familiar? Yes, this is similar to probability without replacement. In fact, it is expanded like this: Total outcome (n): 5! = 5 \\(\\times\\) 4 \\(\\times\\) 3 \\(\\times\\) 2 \\(\\times\\) 1 = 120 Total candidates needed for a president and a vice president (k): 2 candidates. Let us use this permutation formula (without repitition): \\[ P(n,k) = \\frac{n!}{(n-k)!} = {n}P{k} \\] \\[\\begin{align*} \\frac{5!}{ ( 5 - 2 )!} {}&amp;= \\frac{5!} {3!} \\\\ &amp;= \\frac{(5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1 )}{( 3 \\cdot 2 \\cdot 1)} \\\\ &amp;= 5 \\cdot 4 \\\\ &amp;= 20 \\end{align*}\\] Another permutation is on digital lock. Given 4 digit lock, how many permutation can we use to unlock the lock if each digit can range from 0-9? That’s a homework to work on … How many ways can we form a 2 member team out of 5 candidates? Let’s assume candidates: A B C D E Hare are all the possible combination: AB, AC, AD, AE, BC, BD, BE, CD, CE, DE There are 10 combinations. Notice that AB and BA are the same, AC and CA are the same, and so on. Order does not matter. This is similar to probability without replacement. In fact, it is expanded like this: Total outcome (n): 5! = 5 \\(\\times\\) 4 \\(\\times\\) 3 \\(\\times\\) 2 \\(\\times\\) 1 = 120 Total candidates needed for a 2 member team (k): 2 candidates. Let us use this combination formula (without repitition): \\[ C(n,k) = \\frac{n!}{(n-k)!k!} = {n}C{k} \\] \\[\\begin{align*} \\frac{5!} { ( 5-2)! 2!} {}&amp;= \\frac{5!} { 3!2!}\\\\ &amp;= \\frac{(5 \\cdot 4 \\cdot 3 \\cdot 2 \\cdot 1)} {( 3 \\cdot 2 \\cdot 1 ) (2 \\cdot 1)} \\\\ &amp;= \\frac{(5 \\cdot 4)}{ 2} \\\\ &amp; = 10 \\end{align*}\\] what is the odd of getting a Queen if drawing three cards from a deck of cards? Let’s use combination First, There are 3 cards to choose in a deck of card. The combination to get 3 cards has this formula: nCk = 52C3 Second, There is only 1 queen out of 4 queens in a deck. That makes it: nCk = 4C1 Lastly, There are 48 cards in a deck that are non-Queen cards to choose the other 2 cards. That makes it: nCk = 48C2 Probability formula therefore: \\[ P(Queen) = \\frac{(48C2 \\cdot 4C1)}{52C3} \\] where: \\[\\begin{align*} 4C1 &amp;= 4\\\\ 48C2 {}&amp;= \\frac{( 48 \\cdot 47 )}{2} = 1128\\\\ 52C3 &amp;= \\frac{( 52 \\cdot 51 \\cdot 50 )}{( 3 \\cdot 2 \\cdot 1)} = 22100 \\end{align*}\\] thus: \\[ P(Queen) = \\frac{(1128 \\cdot 4)}{22100} = 20.4 \\% \\] 15.2.4 On Conditional Probabilities Given an Event (A): What is the probability that event A will occur? \\[ P(A) \\] What is the probability that event A will not occur? \\[ P(A&#39;) \\] Therefore, the probability that event A will occur also can be shown this way? \\[ P(A) = 1\\ - P(A&#39;) \\] Therefore, the probability that event A will not occur also can be shown this way? \\[ P(A&#39;) = 1\\ - P(A) \\] Given two Events (A, B): What is the probability that both event A and event B will occur? This involves intersection of two events \\[ P(A \\cap B)\\ or\\ P(A\\ and\\ B) \\] What is the probability that event A or event B will occur? This involves union of two events \\[ P(A \\cup B) \\] What is the probability that event A will occur Given B has occurred? This involves conditional probability \\[ P(A|B) \\] 15.2.5 The Arithmetic of Probabilities Given two Events (A, B) Multiplication Rule: What is the probability that event A will occur times the probability that event B will occur, given that event A has occurred? \\[ P(A \\cap B) = P(A) \\cdot P(B|A) \\] Addition Rule: What is the probability that event A will occur plus the probability that event B will occur minus the probability that both event A and event B will occur? \\[ P(A \\cup B) = P(A) + P(B)\\ - P(A \\cap B) \\] 15.2.6 On Dependent and Independent Events If the probability of event A – P(A) – does not change the probability of event B – P(B), then event A and event B are both independent. \\[ P(B|A) = P(B) \\] Note: The conditional probability of event B occurring given event A has occurred, and event A does not affect B. If the probability of event A – P(A) – changes the probability of event B – P(B), then event A and event B are both dependent. \\[ P(A|B) = P(A|B) \\] 15.2.7 On Mutual Exclusivity Mutually exclusive: Both event A and event B are mutually exclusive if they do not intersect. Denote it this way: P(A \\(\\cap\\) B) = 0 Conditional probably can also be stated this way: the probability of both event A and event B occurring divided by the probability of A occurring. That is the same as saying: the probability of event A occurring, given B, has occurred. \\[\\begin{align*} P(A|B) {}&amp;= \\frac{P(A \\cap B)}{P(A)}\\\\ P(A|B) &amp;= \\frac{P(A\\ and\\ B)}{ P(A)} \\end{align*}\\] Let us sample the formulas: Given a deck of cards, what is the probability of drawing a heart the second time if the first time, we can draw a card of the heart (consider no replacement)? Note: Since we have already drawn a card of the heart the first time, there are only 12 hearts left out of 51 cards left. Event A: Drawing a heart the first time (already occurred). No probabilities to consider – actually 100% Event B: Drawing a heart the second time Formula to use: P(B|A) = P(Drawing a heart the second time | Drawing a heart the first) \\[ P(B|A) = \\frac{12}{51} = 23.53 \\ \\% \\] Given a basket of 12 fruits (four apples, four oranges, four pears ), what is the probability of picking up an apple the first time and orange the second time? Event A: Picking up an apple the first time. No occurrence yet. \\[ P(A) = \\frac{(\\ 4\\text{ apple} )}{(\\ 12\\text{ fruits} )} \\] Event B: Picking up an orange the second time. Not occurred yet. \\[ P(B|A) = \\frac{(\\ 4\\text{ oranges} )}{(\\ 11\\text{ fruits left} )} \\] Formula to use: \\[ P(A \\cap B) = P(A)\\cdot P(B|A) \\] Therefore: \\[\\begin{align*} P(A \\cap B) {}&amp;= \\frac{4}{12} \\cdot \\frac{4}{11}\\\\ &amp;= \\frac{4}{33}\\\\ &amp;= 12.12\\% \\end{align*}\\] 15.3 Appendix C This appendix contains statistics tables for reference. Table 15.4: Student’s T Table One-Tail 0.25 0.20 0.15 0.10 0.05 0.025 0.01 0.005 Two-Tail 0.50 0.40 0.30 0.20 0.10 0.05 0.02 0.001 df=n-1 1 1.000 1.376 1.963 3.078 6.314 12.706 31.821 63.657 2 0.816 1.061 1.386 1.886 2.920 4.303 6.965 9.925 3 0.765 0.978 1.250 1.638 2.353 3.182 4.541 5.841 4 0.741 0.941 1.190 1.533 2.132 2.776 3.747 4.604 5 0.727 0.920 1.156 1.476 2.015 2.571 3.365 4.032 6 0.718 0.906 1.134 1.440 1.943 2.447 3.143 3.707 7 0.711 0.896 1.119 1.415 1.895 2.365 2.998 3.499 8 0.706 0.889 1.108 1.397 1.860 2.306 2.896 3.355 9 0.703 0.883 1.100 1.383 1.833 2.262 2.821 3.250 10 0.700 0.879 1.093 1.372 1.812 2.228 2.764 3.169 11 0.697 0.876 1.088 1.363 1.796 2.201 2.718 3.106 12 0.695 0.873 1.083 1.356 1.782 2.179 2.681 3.055 13 0.694 0.870 1.079 1.350 1.771 2.160 2.650 3.012 14 0.692 0.868 1.076 1.345 1.761 2.145 2.624 2.977 15 0.691 0.866 1.074 1.341 1.753 2.131 2.602 2.947 16 0.690 0.865 1.071 1.337 1.746 2.120 2.583 2.921 17 0.689 0.863 1.069 1.333 1.740 2.110 2.567 2.898 18 0.688 0.862 1.067 1.330 1.734 2.101 2.552 2.878 19 0.688 0.861 1.066 1.328 1.729 2.093 2.539 2.861 20 0.687 0.860 1.064 1.325 1.725 2.086 2.528 2.845 Table 15.5: Z Table (-2.9 to 0) z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 -2.9 0.0019 0.0018 0.0018 0.0017 0.0016 0.0016 0.0015 0.0015 -2.8 0.0026 0.0025 0.0024 0.0023 0.0023 0.0022 0.0021 0.0021 -2.7 0.0035 0.0034 0.0033 0.0032 0.0031 0.0030 0.0029 0.0028 -2.6 0.0047 0.0045 0.0044 0.0043 0.0041 0.0040 0.0039 0.0038 -2.5 0.0062 0.0060 0.0059 0.0057 0.0055 0.0054 0.0052 0.0051 -2.4 0.0082 0.0080 0.0078 0.0075 0.0073 0.0071 0.0069 0.0068 -2.3 0.0107 0.0104 0.0102 0.0099 0.0096 0.0094 0.0091 0.0089 -2.2 0.0139 0.0136 0.0132 0.0129 0.0125 0.0122 0.0119 0.0116 -2.1 0.0179 0.0174 0.0170 0.0166 0.0162 0.0158 0.0154 0.0150 -2.0 0.0228 0.0222 0.0217 0.0212 0.0207 0.0202 0.0197 0.0192 -1.9 0.0287 0.0281 0.0274 0.0268 0.0262 0.0256 0.0250 0.0244 -1.8 0.0359 0.0351 0.0344 0.0336 0.0329 0.0322 0.0314 0.0307 -1.7 0.0446 0.0436 0.0427 0.0418 0.0409 0.0401 0.0392 0.0384 -1.6 0.0548 0.0537 0.0526 0.0516 0.0505 0.0495 0.0485 0.0475 -1.5 0.0668 0.0655 0.0643 0.0630 0.0618 0.0606 0.0594 0.0582 -1.4 0.0808 0.0793 0.0778 0.0764 0.0749 0.0735 0.0721 0.0708 -1.3 0.0968 0.0951 0.0934 0.0918 0.0901 0.0885 0.0869 0.0853 -1.2 0.1151 0.1131 0.1112 0.1093 0.1075 0.1056 0.1038 0.1020 -1.1 0.1357 0.1335 0.1314 0.1292 0.1271 0.1251 0.1230 0.1210 -1.0 0.1587 0.1562 0.1539 0.1515 0.1492 0.1469 0.1446 0.1423 -0.9 0.1841 0.1814 0.1788 0.1762 0.1736 0.1711 0.1685 0.1660 -0.8 0.2119 0.2090 0.2061 0.2033 0.2005 0.1977 0.1949 0.1922 -0.7 0.2420 0.2389 0.2358 0.2327 0.2296 0.2266 0.2236 0.2206 -0.6 0.2743 0.2709 0.2676 0.2643 0.2611 0.2578 0.2546 0.2514 -0.5 0.3085 0.3050 0.3015 0.2981 0.2946 0.2912 0.2877 0.2843 -0.4 0.3446 0.3409 0.3372 0.3336 0.3300 0.3264 0.3228 0.3192 -0.3 0.3821 0.3783 0.3745 0.3707 0.3669 0.3632 0.3594 0.3557 -0.2 0.4207 0.4168 0.4129 0.4090 0.4052 0.4013 0.3974 0.3936 -0.1 0.4602 0.4562 0.4522 0.4483 0.4443 0.4404 0.4364 0.4325 0.0 0.5000 0.4960 0.4920 0.4880 0.4840 0.4801 0.4761 0.4721 Table 15.6: Z Table (0 - 2.9) z 0.00 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.0 0.5000 0.5040 0.5080 0.5120 0.5160 0.5199 0.5239 0.5279 0.1 0.5398 0.5438 0.5478 0.5517 0.5557 0.5596 0.5636 0.5675 0.2 0.5793 0.5832 0.5871 0.5910 0.5948 0.5987 0.6026 0.6064 0.3 0.6179 0.6217 0.6255 0.6293 0.6331 0.6368 0.6406 0.6443 0.4 0.6554 0.6591 0.6628 0.6664 0.6700 0.6736 0.6772 0.6808 0.5 0.6915 0.6950 0.6985 0.7019 0.7054 0.7088 0.7123 0.7157 0.6 0.7257 0.7291 0.7324 0.7357 0.7389 0.7422 0.7454 0.7486 0.7 0.7580 0.7611 0.7642 0.7673 0.7704 0.7734 0.7764 0.7794 0.8 0.7881 0.7910 0.7939 0.7967 0.7995 0.8023 0.8051 0.8078 0.9 0.8159 0.8186 0.8212 0.8238 0.8264 0.8289 0.8315 0.8340 1.0 0.8413 0.8438 0.8461 0.8485 0.8508 0.8531 0.8554 0.8577 1.1 0.8643 0.8665 0.8686 0.8708 0.8729 0.8749 0.8770 0.8790 1.2 0.8849 0.8869 0.8888 0.8907 0.8925 0.8944 0.8962 0.8980 1.3 0.9032 0.9049 0.9066 0.9082 0.9099 0.9115 0.9131 0.9147 1.4 0.9192 0.9207 0.9222 0.9236 0.9251 0.9265 0.9279 0.9292 1.5 0.9332 0.9345 0.9357 0.9370 0.9382 0.9394 0.9406 0.9418 1.6 0.9452 0.9463 0.9474 0.9484 0.9495 0.9505 0.9515 0.9525 1.7 0.9554 0.9564 0.9573 0.9582 0.9591 0.9599 0.9608 0.9616 1.8 0.9641 0.9649 0.9656 0.9664 0.9671 0.9678 0.9686 0.9693 1.9 0.9713 0.9719 0.9726 0.9732 0.9738 0.9744 0.9750 0.9756 2.0 0.9772 0.9778 0.9783 0.9788 0.9793 0.9798 0.9803 0.9808 2.1 0.9821 0.9826 0.9830 0.9834 0.9838 0.9842 0.9846 0.9850 2.2 0.9861 0.9864 0.9868 0.9871 0.9875 0.9878 0.9881 0.9884 2.3 0.9893 0.9896 0.9898 0.9901 0.9904 0.9906 0.9909 0.9911 2.4 0.9918 0.9920 0.9922 0.9925 0.9927 0.9929 0.9931 0.9932 2.5 0.9938 0.9940 0.9941 0.9943 0.9945 0.9946 0.9948 0.9949 2.6 0.9953 0.9955 0.9956 0.9957 0.9959 0.9960 0.9961 0.9962 2.7 0.9965 0.9966 0.9967 0.9968 0.9969 0.9970 0.9971 0.9972 2.8 0.9974 0.9975 0.9976 0.9977 0.9977 0.9978 0.9979 0.9979 2.9 0.9981 0.9982 0.9982 0.9983 0.9984 0.9984 0.9985 0.9985 Table 15.7: F Table (alpha=0.05, right-tail): Note that the column-wise df1 is the degrees of freedom of the numerator. And the row-wise df2 is the degrees of freedom of the denominator. See F-Test section in the Statistics chapter. df1 1 2 3 5 10 15 25 df2 1 161.448 199.500 215.707 230.162 241.882 245.950 249.260 2 18.513 19.000 19.164 19.296 19.396 19.429 19.456 3 10.128 9.552 9.277 9.013 8.786 8.703 8.634 4 7.709 6.944 6.591 6.256 5.964 5.858 5.769 5 6.608 5.786 5.409 5.050 4.735 4.619 4.521 6 5.987 5.143 4.757 4.387 4.060 3.938 3.835 7 5.591 4.737 4.347 3.972 3.637 3.511 3.404 8 5.318 4.459 4.066 3.687 3.347 3.218 3.108 9 5.117 4.256 3.863 3.482 3.137 3.006 2.893 10 4.965 4.103 3.708 3.326 2.978 2.845 2.730 11 4.844 3.982 3.587 3.204 2.854 2.719 2.601 12 4.747 3.885 3.490 3.106 2.753 2.617 2.498 13 4.667 3.806 3.411 3.025 2.671 2.533 2.412 14 4.600 3.739 3.344 2.958 2.602 2.463 2.341 15 4.543 3.682 3.287 2.901 2.544 2.403 2.280 16 4.494 3.634 3.239 2.852 2.494 2.352 2.227 17 4.451 3.592 3.197 2.810 2.450 2.308 2.181 18 4.414 3.555 3.160 2.773 2.412 2.269 2.141 19 4.381 3.522 3.127 2.740 2.378 2.234 2.106 20 4.351 3.493 3.098 2.711 2.348 2.203 2.074 21 4.325 3.467 3.072 2.685 2.321 2.176 2.045 22 4.301 3.443 3.049 2.661 2.297 2.151 2.020 23 4.279 3.422 3.028 2.640 2.275 2.128 1.996 24 4.260 3.403 3.009 2.621 2.255 2.108 1.975 25 4.242 3.385 2.991 2.603 2.236 2.089 1.955 26 4.225 3.369 2.975 2.587 2.220 2.072 1.938 27 4.210 3.354 2.960 2.572 2.204 2.056 1.921 28 4.196 3.340 2.947 2.558 2.190 2.041 1.906 29 4.183 3.328 2.934 2.545 2.177 2.027 1.891 30 4.171 3.316 2.922 2.534 2.165 2.015 1.878 Table 15.8: Chi Square (Wishart) Table alpha 0.100 0.050 0.025 0.01 0.001 0.005 df 1 2.7055 3.8415 5.0239 6.6349 10.8276 7.8794 2 4.6052 5.9915 7.3778 9.2103 13.8155 10.5966 3 6.2514 7.8147 9.3484 11.3449 16.2662 12.8382 4 7.7794 9.4877 11.1433 13.2767 18.4668 14.8603 5 9.2364 11.0705 12.8325 15.0863 20.5150 16.7496 6 10.6446 12.5916 14.4494 16.8119 22.4577 18.5476 7 12.0170 14.0671 16.0128 18.4753 24.3219 20.2777 8 13.3616 15.5073 17.5345 20.0902 26.1245 21.9550 9 14.6837 16.9190 19.0228 21.6660 27.8772 23.5894 10 15.9872 18.3070 20.4832 23.2093 29.5883 25.1882 11 17.2750 19.6751 21.9200 24.7250 31.2641 26.7568 12 18.5493 21.0261 23.3367 26.2170 32.9095 28.2995 13 19.8119 22.3620 24.7356 27.6882 34.5282 29.8195 14 21.0641 23.6848 26.1189 29.1412 36.1233 31.3193 15 22.3071 24.9958 27.4884 30.5779 37.6973 32.8013 16 23.5418 26.2962 28.8454 31.9999 39.2524 34.2672 17 24.7690 27.5871 30.1910 33.4087 40.7902 35.7185 18 25.9894 28.8693 31.5264 34.8053 42.3124 37.1565 19 27.2036 30.1435 32.8523 36.1909 43.8202 38.5823 20 28.4120 31.4104 34.1696 37.5662 45.3147 39.9968 21 29.6151 32.6706 35.4789 38.9322 46.7970 41.4011 22 30.8133 33.9244 36.7807 40.2894 48.2679 42.7957 23 32.0069 35.1725 38.0756 41.6384 49.7282 44.1813 24 33.1962 36.4150 39.3641 42.9798 51.1786 45.5585 25 34.3816 37.6525 40.6465 44.3141 52.6197 46.9279 26 35.5632 38.8851 41.9232 45.6417 54.0520 48.2899 27 36.7412 40.1133 43.1945 46.9629 55.4760 49.6449 28 37.9159 41.3371 44.4608 48.2782 56.8923 50.9934 29 39.0875 42.5570 45.7223 49.5879 58.3012 52.3356 30 40.2560 43.7730 46.9792 50.8922 59.7031 53.6720 Table 15.9: Tukey’s Table (alpha=0.05): Note that the column-wise p is the number of groups (or rank). And the row-wise dfW is the degrees of freedom of (dfW). See Tukey’s Test section in the Statistics chapter. p 2 3 4 5 6 7 8 9 dfW 2 6.080 8.331 9.799 10.881 11.734 12.435 13.028 13.542 3 4.501 5.910 6.825 7.502 8.037 8.478 8.852 9.177 4 3.927 5.040 5.757 6.287 6.706 7.053 7.347 7.602 5 3.635 4.602 5.218 5.673 6.033 6.330 6.582 6.801 6 3.460 4.339 4.896 5.305 5.628 5.895 6.122 6.319 7 3.344 4.165 4.681 5.060 5.359 5.606 5.815 5.997 8 3.261 4.041 4.529 4.886 5.167 5.399 5.596 5.767 9 3.199 3.948 4.415 4.755 5.024 5.244 5.432 5.595 10 3.151 3.877 4.327 4.654 4.912 5.124 5.304 5.460 11 3.113 3.820 4.256 4.574 4.823 5.028 5.202 5.353 12 3.081 3.773 4.199 4.508 4.750 4.950 5.119 5.265 13 3.055 3.734 4.151 4.453 4.690 4.884 5.049 5.192 14 3.033 3.701 4.111 4.407 4.639 4.829 4.990 5.130 15 3.014 3.673 4.076 4.367 4.595 4.782 4.940 5.077 16 2.998 3.649 4.046 4.333 4.557 4.741 4.896 5.031 17 2.984 3.628 4.020 4.303 4.524 4.705 4.858 4.991 18 2.971 3.609 3.997 4.276 4.494 4.673 4.824 4.955 19 2.960 3.593 3.977 4.253 4.468 4.645 4.794 4.924 20 2.950 3.578 3.958 4.232 4.445 4.620 4.768 4.895 21 2.941 3.565 3.942 4.213 4.424 4.597 4.743 4.870 22 2.933 3.553 3.927 4.196 4.405 4.577 4.722 4.847 23 2.926 3.542 3.914 4.180 4.388 4.558 4.702 4.826 24 2.919 3.532 3.901 4.166 4.373 4.541 4.684 4.807 25 2.913 3.523 3.890 4.153 4.358 4.526 4.667 4.789 26 2.907 3.514 3.880 4.141 4.345 4.511 4.652 4.773 27 2.902 3.506 3.870 4.130 4.333 4.498 4.638 4.758 28 2.897 3.499 3.861 4.120 4.322 4.486 4.625 4.745 29 2.892 3.493 3.853 4.111 4.311 4.475 4.613 4.732 30 2.888 3.486 3.845 4.102 4.301 4.464 4.601 4.720 15.4 Appendix D 15.4.1 Lubridate Library Here are a few useful functions derived from the lubridate library that allow us to format date and time with ease. They are useful functions for Time-Series modeling. Given different date formats: options(width=70) d1 = c(&quot;05-02-2010&quot;, &quot;11.03.2011&quot;, &quot;06032011&quot;, &quot;06/04/2011&quot;) d2 = c(&quot;02-05-2010&quot;, &quot;03-11-2011&quot;, &quot;03062011&quot;, &quot;04/06/2011&quot;) d3 = c(&quot;2010-02-05&quot;, &quot;2011-03-11&quot;, &quot;20110603&quot;, &quot;2011/06/04&quot;) We can translate them into the format of our choosing: options(width=70) library(lubridate) # Day-Month-Year Formats dmy(d1) ## [1] &quot;2010-02-05&quot; &quot;2011-03-11&quot; &quot;2011-03-06&quot; &quot;2011-04-06&quot; # Month-Day-Year Formats mdy(d2) ## [1] &quot;2010-02-05&quot; &quot;2011-03-11&quot; &quot;2011-03-06&quot; &quot;2011-04-06&quot; # Year-Month-Day Formats ymd(d3) ## [1] &quot;2010-02-05&quot; &quot;2011-03-11&quot; &quot;2011-06-03&quot; &quot;2011-06-04&quot; # Year-Month-Day HH:MM:SS ymd_hms(&quot;2011-07-01 09:00:00&quot;, tz = &quot;America/Los_Angeles&quot;) ## [1] &quot;2011-07-01 09:00:00 PDT&quot; Other functions are as follows: options(width=70) # Use only year and month and floor day to 1 floor_date(ymd(d3), &quot;month&quot;) ## [1] &quot;2010-02-01&quot; &quot;2011-03-01&quot; &quot;2011-06-01&quot; &quot;2011-06-01&quot; # Use only year and floor month and day to 1 floor_date(ymd(d3), &quot;year&quot;) ## [1] &quot;2010-01-01&quot; &quot;2011-01-01&quot; &quot;2011-01-01&quot; &quot;2011-01-01&quot; # Add 5 months to date. ymd(d3) + months(5) ## [1] &quot;2010-07-05&quot; &quot;2011-08-11&quot; &quot;2011-11-03&quot; &quot;2011-11-04&quot; # Subtract 5 months from date. ymd(d3) - months(5) ## [1] &quot;2009-09-05&quot; &quot;2010-10-11&quot; &quot;2011-01-03&quot; &quot;2011-01-04&quot; # Add 5 days to date. ymd(d3) + days(5) ## [1] &quot;2010-02-10&quot; &quot;2011-03-16&quot; &quot;2011-06-08&quot; &quot;2011-06-09&quot; # Subtract 5 days from date. ymd(d3) - days(5) ## [1] &quot;2010-01-31&quot; &quot;2011-03-06&quot; &quot;2011-05-29&quot; &quot;2011-05-30&quot; # Add 1 week to date. ymd(d3) + weeks(1) ## [1] &quot;2010-02-12&quot; &quot;2011-03-18&quot; &quot;2011-06-10&quot; &quot;2011-06-11&quot; # Subtract 1 week from date. ymd(d3) - weeks(1) ## [1] &quot;2010-01-29&quot; &quot;2011-03-04&quot; &quot;2011-05-27&quot; &quot;2011-05-28&quot; # Capture interval between two dates #(Between Jan 01 2010 and Dec 31 2010) one_year = interval( ymd(&quot;2010-01-01&quot;), ymd(&quot;2010-12-31&quot;)) one_year ## [1] 2010-01-01 UTC--2010-12-31 UTC # Check if our schedule has overlap with our #travel plans (&quot;2010-09-12&quot;, &quot;2010-06-28&quot;)) d4 = interval( ymd(&quot;2010-09-12&quot;), ymd(&quot;2011-06-28&quot;) ) int_overlaps(one_year, d4) ## [1] TRUE # See which one is available for travel. setdiff(d4, one_year) ## [1] 2010-12-31 UTC--2011-06-28 UTC "],["bibliography.html", "Bibliography", " Bibliography Abebe, T. H. 2020. “The Derivation and Choice of Approppriate Test Statistic (Z, T, F and Chi-Square Test) in Research Methodology.” European Journal of Statistics and Probability, Vol. 8, No.1, Pp. 60-73, April 2020. Department of Economics, Ambo University, Ethiopia: ECRTD-UK. Adams, R. A. 1995. “Calculus of Several Variables.” 3rd Edition. Agresti, A., Franklin, C., Klingenberg, B., &amp; Posner, M. 2017. Statistics: The Art and Science of Learning from Data. 4th Edition. Pearson. Alfaki, M. 2008. “Improving Efficiency in Parameter Estimation Using the Hamiltonian Monte Carlo Algorithm.” A thesis (MSc): University of Bergen, Department of Informatics. http://www.ii.uib.no/~mohammeda/publications/alfakithesis.pdf. An, D. 2009. “Understanding Krylov Subspace Methods.” [Video]. https://www.youtube.com/watch?v=UgyLaAXqlQ4. Ang, A. 2014. “Gauss-Markov Theorem for Ols Is the Best Linear Unbiased Estimator.” https://angms.science/doc/Regression/Regression_3_GaussMarkov.pdf. Anggraeni, D., Sanjaya, W. S. M., Nurasyidiek, M. Y. S., &amp; Munawwaroh, M. 2018. “The Implementation of Speech Recognition Using Mel-Frequency Cepstrum Coefficients (Mfcc) and Support Vector Machine (Svm) Method Based on Python Ton Control Robot Arm.” IOP Conf. Series: Materials Science and Engineering 288 (2018) 012042 Doi:10.1088/1757-899X/288/1/012042. https://iopscience.iop.org/article/10.1088/1757-899X/288/1/012042/pdf. Anley, E. F. 2016. “The Qr Method for Determining All Eigenvalues of Real Square Matrices.” Pure and Applied Mathematics Journal. Volume 5, Issue 4, August 2016 , Pp. 113-119. Doi: 10.11648/J.pamj.2016050. Ardelean, F. A. 2017. “Case Study Using Analysis of Variance to Determine Group’s Variations.” MATEC Web of Converence 126, 04008 (2017), Annual Session of Scientific Papers IMT ORADEA 2017. DOI: 10.1051/matecconf/201712604008. Armstrong, R. A. 2014. “When to Use the Bonferroni Correction.” The Journal of the College of Optometrists. School of Life; Health Sciences, Aston University, Birmham, UK. https://doi.org/10.1111/opo.12131. Arreola, J. 2018. “Variational Gaussian Mixtures for Face Detection.” https://jean9208.github.io/vgmm_fd/. Atkinson, K. E. 1989. An Introduction to Numerical Analysis. John Wiley &amp; Sons. Baarsch, J., &amp; Celebi, M. E. 2012. “Investigation of Internal Validty Measures for K-Means Clustering.” Proceedings of the International MultiConference of Engineers and Computer Scientists 2012, Vol I, IMECS 2012, Mar 14-16, 2012, Hong Kong. http://www.iaeng.org/publication/IMECS2012/IMECS2012_pp471-476.pdf. Bahdanau, D., Cho, K., &amp; Bengio, Y. 2015. “Neural Machine Translation by Jointly Learning to Align and Translate.” Conference Paper at ICLR 2015. https://arxiv.org/pdf/1409.0473.pdf. Bai, Z., Demmel, J., Dongarra, J., Ruhe, A., &amp; van der Vorst, H. 2000. “Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide.” SIAM, Philadelphia, 2000. https://doi.org/10.1137/1.9780898719581. Bailey, D. H. 2021. “A catalogue of mathematical formulas involving \\(\\pi\\) with analysis.” https://www.davidhbailey.com/dhbpapers/pi-formulas.pdf. Baker, K. 2013. “Singular Value Decomposition Tutorial.” March 29 2005 (Revised January 14, 2013). https://datajobs.com/data-science-repo/SVD-Tutorial-[Kirk-Baker].pdf. Barthelme, S. 2020. Imager: Image Processing Library Based on ’Cimg’. https://CRAN.R-project.org/package=imager. Bates, D., &amp; Maechler, M. 2021. Matrix: Sparse and Dense Matrix Classes and Methods. https://CRAN.R-project.org/package=Matrix. Bengio, Y., Ducharme, R., Vincent, P., &amp; Jauvin, C. 2003. “A Neural Probabilistic Language Model.” Machine Learning Research 3 (2003), 1137-1155. https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf. Benjamini, Y., &amp; Hochberg, Y. 1994. “Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing.” Journal of the Royal Statistical Society. B (1995), 57, No.1, Pp. 289-300. Tel Aviv University, Israel. https://doi.org/10.1111/opo.12131. Berhane, F. n.d. “Building Your Recurrent Neural Network - Step by Step.” https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html. Bishop, C. M. 2006. Pattern Recognition and Machine Learning. Corrected Printing 2009. Springer. Blei, D. M. 2012. “Probabilistic Topic Models.” In Surveying a Suite of Algorithms That Offer a Solution to Managing Large Document Archives. http://www.cs.columbia.edu/~blei/papers/Blei2012.pdf. ———. n.d. “Variational Inference.” Princeton lecture. https://www.cs.princeton.edu/courses/archive/fall11/cos597C/lectures/variational-inference-i.pdf. Blei, D. M., Kucukelbir A., &amp; McAuliffe, J. D. 2017. “Variational Inference: A Review for Statisticians.” American Statistical Association, 112:518, 859-877. http://dx.doi.org/10.1080/01621459.2017.1285773. Blei, D. M., Ng, A. Y., &amp; Jordan, M. I. 2003. “Latent Dirichlet Allocation.” Machine Learning Research 3 (2003) 993-1022. http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf. Bolotov, D., Jena, T., Williams, G., Lin, W-C., Michael, H., Hahsler, Ishwaran, H., Kogalur, B. U., &amp; Guha, R. 2021. Pmml: Generate Pmml for Various Models. https://CRAN.R-project.org/package=pmml. Borchers, B. 2001. “The Partial Autocorrelation Function.” http://www.ees.nmt.edu/outside/courses/GEOP505/Docs/pac.pdf. Bouchet-Valat, M. 2020. SnowballC: Snowball Stemmers Based on the c ’Libstemmer’ Utf-8 Library. https://CRAN.R-project.org/package=SnowballC. Breiman, L. 1994. “Bagging Predictors.” https://www.stat.berkeley.edu/~breiman/bagging.pdf. ———. 2001. “Random Forests.” Machine Learning 45, 5–32. https://doi.org/10.1023/A:1010933404324. Bruyninckx, H. 2002. “Bayesian Probability.” Department of Mechanical Engineering, K.U. Leuven Belgium. https://people.cs.kuleuven.be/~danny.deschreye/urks2to4_text.pdf. Buchta, C., Hahsler, M., &amp; Diaz, D. 2020. ArulesSequences: Mining Frequent Sequences. https://CRAN.R-project.org/package=arulesSequences. Bugallo, M. F., Martino, L., Elvira, V., &amp; Luengo, D. 2017. “Adaptive Importance Sampling: The Past, the Present, and the Future.” IEEE Signal Processing Magazine, Vol. 34, No. 4, Pp. 60-79, July 2017, Doi: 10.1109/MSP.2017.2699226. https://ieeexplore.ieee.org/document/7974876. Burden, R. L., Faires, D. J., &amp; Burden, A. M. 2005. Numerical Analysis. (10th Edition, print 2016). CENGAGE Learning. Burke, N. 2018. “Metropolis, Metropolis-Hastings and Gibbs Sampling Algorithms.” Lakehead University Thunder Bay, Ontario. https://www.lakeheadu.ca/sites/default/files/uploads/77/Burke.pdf. Cameron, A. C., &amp; Windmeijer, F. A.G.. 1995. “R-Squared Measures for Count Data Regression Models with Applications to Health Care Utilization.” Journal of Business and Economic Statistics (forthcoming). http://cameron.econ.ucdavis.edu/research/jbes96preprint.pdf. CE 108, University of Southern California. n.d. “Numerical Analysis, Abscissas and Weight Factors for Gaussian Integration.” http://www-classes.usc.edu/engr/ce/108/gauss_weights.pdf. Chacón, J. E., &amp; Duong, T. 2018. “Multivariate Kernel Smoothing and Its Applications.” Taylor &amp; Francis Group, LLC, CRS Press. Chan, Y., &amp; Walmsley, R. P. 1997. “Learning and Understanding the Kruskal-Wallis One-Way Analysis-of-Variance-by-Ranks Test for Differences Among Three or More Independent Groups.” Physical Therapy, Volume 77:1755-1762. https://doi.org/10.1093/ptj/77.12.1755. Chao, W-L., Solomon, J., Michels, D. L., &amp; Sha, F. 2015. “Exponential Integration for Hamiltonian Monte Carlo.” Department of Computer Science, University of South California, Los Angeles, CA 90089. https://people.csail.mit.edu/jsolomon/assets/exponential_hmc.pdf. Chen, L., Yuan, F., Jose, J. M., &amp; Zhang, W. 2018. “Improving Negative Sampling for Word Representation using Self-embedded Features.” The 11th International Conference on Web Searching and Data Mining (WSDM 2018), Los Angeles, CA, USA, 05-09 Feb 2018, pp. 99-107. ISBN 9781450355810. https://dl.acm.org/doi/10.1145/3159652.3159695. Chen, S-Y., Feng, Z., &amp; Yi, X. 2017. “A General Introduction to Adjustment for Multiple Comparisons.” J Thorac Dis. 2017 Jun; 9(6): 1725–1729. https://dx.doi.org/10.21037%2Fjtd.2017.05.34. Chen, T., Fox, E. B., &amp; Guestrin, C. 2014. “Stochastic Gradient Hamiltonian Monte Carlo.” MODE Lab, University of Washington, Seattle, WA. https://arxiv.org/pdf/1402.4102v2.pdf. Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., Li, M., Xie, J., Lin, M., Geng, Y., &amp; Li, Y. 2020. Xgboost: Extreme Gradient Boosting. https://CRAN.R-project.org/package=xgboost. Cheruiyot, L. R., Orwa, G. O., &amp; Otieno, O. R. 2020. “Kernel Function and Nonparametric Regression Estimation: Which Function Is Appropriate?” African Journal of Mathematics and Statistics Studies, ISSN: 2689-5323, Volume 3, Issue 3, 2020 (Pp.51-59). Cho, K., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. 2014. “Learning Phase Representations using RNN Encoder-Decoder for Statistical Machine Translation.” https://arxiv.org/pdf/1406.1078v3.pdf. Chung, J., Gulcehre, C., Cho, K., &amp; Bengio, Y. 2014. “Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.” https://arxiv.org/pdf/1412.3555.pdf. Cleveland, W. S., &amp; Devlin, S. J. 1988. “Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting.” Journal of the American Statistical Association, 83(403), 596–610. Https://Doi.org/10.2307/2289282. Collings, I. 2021. “How Are the Fourier Series, Fourier Transform, Dtft, Dft, Fft, Lt, and Zt Related.” [Video]. https://www.youtube.com/watch?v=hF72sY70_IQ. Cover, T. M., &amp; Thomas, J. A. 2006. “Elements of Information Theory.” John Wiley &amp; Sons, Inc., Hoboken, New Jersey. Cox, D. R. 1972. “Regression Models and Life-Tables.” Journal of the Royal Statistical Society. Series B (Methodological), Vol. 34, No. 2 (1972), 187-220. https://www.jstor.org/stable/2985181. Cox, M., van de Laar, T., &amp; de Vries, B. 2018. “A Factor Graph Approach to Automated Design of Bayesian Signal Processing Algorithms.” Department of Electrical Engineering. Eindhoven University of Technology, PO Box 513, 6500 MB, Eindhoven, the Netherlands. https://arxiv.org/pdf/1811.03407.pdf. Dablander, F. 2018. “A Brief Primer on Variational Inference.” [Web Article] 3rd step Variational Bayes. https://fabiandablander.com/r/Variational-Inference.html. Das, B., &amp; Chakrabarty, D. 2016. “Lagrange’s Interpolation Formula: Representation of Numerical Data by a Polynomial Curve.” International Journal of Mathematics Trends and Technology 34(2):64-72, DOI: 10.14445/22315373/IJMTT-V34P514. Davis, P., &amp; Rabinowitz, P. 1956. “Abscissas and Weights for Gaussian Quadratures of High Order.” Journal of Research of the National Bureau of Standards Vol. 56, 35-37, 1956 RP2645. https://nvlpubs.nist.gov/nistpubs/jres/56/jresv56n1p35_A1b.pdf. Davis, S., &amp; Mermelstein, P. 1980. “Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences.” IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 28, no. 4, pp. 357-366, August 1980, doi: 10.1109/TASSP.1980.1163420. https://doi.org/10.1109/TASSP.1980.1163420. Dawkins, P. 2007. “Linear Algebra.” http://www.math.utoledo.edu/~melbial2/classes/Linear%20Algebra-2890/LinAlg_Dawkins.pdf. De Boor, C. 2002. “On Calculating with B-Splines.” Journal of Approximation Theory 6, 50-62 (1972). Division of Mathematical Sciences, Purdue University, Lafayette, Indiana, 47907: Academmic Press, Inc. https://web.stanford.edu/class/cme324/classics/deboor.pdf. Deetoher. 2013. “Conjugate Prior Poisson.” [Video]. https://www.youtube.com/watch?v=2mjbmvHPAww. de Groot, A. D. 2006. “The Meaning of Significance for Different Types of Research.” Translated and Annotated by Eric-Jan Wagenmakers, et Al. Dellaert, F. 2021. “Factor Graphs: Exploiting Structure in Robotics.” School of Interactive Computing, Georgia Institute of Technology, Atlanta, Georgia 30332; Google AI, Mountain View, California 94043, USA. https://www.annualreviews.org/doi/pdf/10.1146/annurev-control-061520-010504. Dellaert, F., &amp; Kaess, M. 2017. “Factor Graphs for Robot Perception.” Georgia Institute of Technology; Carnegie Mellon niversity. https://www.cs.cmu.edu/~kaess/pub/Dellaert17fnt.pdf. de Mendiburu, F. 2020. Agricolae: Statistical Procedures for Agricultural Research. https://CRAN.R-project.org/package=agricolae. Dey, A. n.d. “Bipartite Graph.” {https://www.thealgorists.com/Algo/Bipartite}. Dowle, M. &amp; Srinivasan, A. 2020. Data.table: Extension of ‘Data.frame‘. https://CRAN.R-project.org/package=data.table. Dozat, T. 2016. “Incorporating Nesterov Momentum into Adam.” Workshop Track - ICLR 2016. https://openreview.net/pdf/OM0jvwB8jIp57ZJjtNEZ.pdf. Driscoll, T. 2012. “Krylov Subspaces.” [Video]. https://www.youtube.com/watch?v=ji__O4deIZo. Driscoll, T. A., &amp; Braun, R. J. 2020. “Fundamentals of Numerical Computation.” Society of Applied and Industrial Mathematics, 2017. https://fncbook.github.io/fnc/frontmatter.html. Dr. Lin Himmelmann et al. 2010. HMM: HMM - Hidden Markov Models. https://CRAN.R-project.org/package=HMM. Drucker, H. 1997. “Improving Regressors Using Boosting Techniques.” Proceedings of the 14th International Conference on Machine Learning. http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.31.314&amp;rep=rep1&amp;type=pdf. Duchi, J., Hazan, E., &amp; Singer, Y. 2011. “Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.” JMLR, 12:2121-2159. https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf. Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, S., &amp; Harshman, R. 1988. “Using Latent Semantic Analysis to Improve Information Retrieval.” In CHI ’88: Proceedings of the SIGCHI Conference on Human Factors in Computing SystemsMay 1988 Pages 281–285. https://doi.org/10.1145/57167.57214. Dumoulin, V., &amp; Visin, F. 2018. “A guide to convolution arithmetic for deep learning.” https://arxiv.org/pdf/1603.07285.pdf. Duong, T. 2020. Ks: Kernel Smoothing. https://CRAN.R-project.org/package=ks. Ebrahimi, N., Soofi, E. S., &amp; Soyer, R. 2010. “Information Measures in Perspective.” Journal Compilation (2010) International Statistical Institute. Blackwell Publishing Ltd. https://business.gwu.edu/sites/g/files/zaxdzs1611/f/downloads/Department_decision_sciences_Publicaiton_Refik-soyer_Information.pdf. Edwards, C. H., Penney, D. E., &amp; Calvis, D. T. 2018. Differential Equations and Linear Algebra. (4th Edition). Pearson. Ehiwario, J. C. 2014. “Comparative Study of Bisection, Newton-Raphson and Secant Methods of Root- Finding Problems.” IOSR Journal of Engineering. 4. 01-07. 10.9790/3021-04410107 4. http://www.iosrjen.org/Papers/vol4_issue4%20(part-1)/A04410107.pdf. Ekeocha, R. J. O., Uzor, C. &amp; Anetor, C. 2018. “The Use of the Duality Principle to Solve Optimization Problems.” Int. J. Recent Contributions Eng. Sci. IT 6: 33–42. Elgabry, O. 2019. “The Ultimate Guide to Data Cleaning.” [Article]. https://towardsdatascience.com/the-ultimate-guide-to-data-cleaning-3969843991d4. Elharati, H. 2019. “Performance Evaluation of Speech Recognition System Using Conventional and Hybrid Features and Hidden Markov Model Classifier.” [PhD Dissertation]. College of Engineering; Science of Florida Institute of Technology. https://repository.lib.fit.edu/bitstream/handle/11141/3024/ELHARATI-DISSERTATION-2019.pdf. Elvira, V.*, &amp; Martino, L.†. 2021. “Advances in Importance Sampling.” *School of Mathematics, University of Edinburgh (United Kingdom), †Universidad Rey Juan Carlos de Madrid (Spain). https://arxiv.org/pdf/2102.05407.pdf. Engle, R. F. 1984. “Wald, Likelihood Ratio, and Lagrange Multiplier Tests in Econometrics.” Handbook of Econometrics, Volume II, Edited by Z. Griliches and M.D. Intriligator. Elsevier Science Publishers BV, 1984. http://hedibert.org/wp-content/uploads/2014/04/W-LR-LM-Tests-in-Econometrics-Engle1984.pdf. Erraqabi, A., Valko, M., Carpentier, A., &amp; Maillard, O-A. 2016. “Pliable Rejection Sampling.” Proceedings of the 33rd International Conference on MachineLearning, New York, NY, USA, 2016. JMLR: W&amp;CP Volume 48. http://proceedings.mlr.press/v48/erraqabi16.pdf. Ester, M., Kriegel, H-P., Sander, J., &amp; Xu, X. 1996. A Density-Based Algorithm for Discovering Clusters. KDD-96 Proceedings. https://www.aaai.org/Papers/KDD/1996/KDD96-037.pdf. Fellows, I. 2018. Wordcloud: Word Clouds. https://CRAN.R-project.org/package=wordcloud. Fill, J. A., &amp; Fishkind, D. E. 1998. “The Moore-Penrose Generalized Inverse for Sums of Matrices.” AMS 1991 Subject Classifications. Primary 15A09; Secondary 15A18. The Johns Hopkins University, Baltimore, Maryland; Department of MathematicsandStatistics,University of SouthernMaine,Portland,Maine. https://www.ams.jhu.edu/~fill/papers/MoorePenrose.pdf. Fink, D. 1997. “A Compendium of Conjugate Priors.” Montana State University, Bozeman, MT 59717. https://www.johndcook.com/CompendiumOfConjugatePriors.pdf. Forsyth, D. 2018. Probability and Statistics for Computer Science. Springer. Fox, C.W., &amp; Roberts, S.J. 2012. “A Tutorial on Variational Bayesian Inference.” Artif Intell Rev 38, 85–95 (2012). https://doi.org/10.1007/s10462-011-9236-8. Fox, D., Hightower, J., Liao, L., Schulz, D., &amp; Borriello, G. 2003. “Bayesian Filtering for Location Estimation.” University of Washington. https://rse-lab.cs.washington.edu/postscripts/bayes-filter-pervasive-03.pdf. Freund, Y., &amp; Schapire, R. E. 1996. “Experiments with a New Boosting Algorithm.” Machine Learning: Proceedings of the Thirteenth International Conference, 1996. https://cseweb.ucsd.edu/~yfreund/papers/boostingexperiments.pdf. ———. 1999. “A Short Introduction to Boosting.” Journal of Japanese Society for Artificial Intelligence,14(5):771-780,September,1999. http://rob.schapire.net/papers/Schapire99c.pdf. Friedman, J. H. 1999a. “Greedy Function Approximation: A Gradient Boosting Machine.” IMS 1999 Reitz Lecture, February 24, 1999, (modified March 15, 2000, April 19, 2001). https://statweb.stanford.edu/~jhf/ftp/trebst.pdf. ———. 1999b. “Stochastic Gradient Boosting.” https://jerryfriedman.su.domains/ftp/stobst.pdf. ———. 2001. “Greedy Function Approximation: A Gradient Boosting Machine.” The Annals of Statistics, Vol. 29, No. 5 (Oct., 2001), pp. 1189-1232, Published by: Institute of Mathematical Statistics. https://www.jstor.org/stable/2699986?origin=JSTOR-pdf. ———. 2002. “Stochastic Gradient Boosting.” Computational Statistics &amp; Data Analysis. 38. 367-378. 10.1016/S0167-9473(01)00065-2. https://www.researchgate.net/profile/Jerome-Friedman/publication/222573328_Stochastic_Gradient_Boosting. Ganchev, T., Fakotakis, N., &amp; George, K. 2005. “Comparative evaluation of various MFCC implementations on the speaker verification task.” Proceedings of the SPECOM. 1. https://www.researchgate.net/profile/Todor-Ganchev/publication/228756314_Comparative_evaluation_of_various_MFCC_implementations_on_the_speaker_verification_task. Gander, W. 1980. “Algorithms for the Qr-Decomposition.” Research Report No. 80-02, SEMINAR FUER ANGEWANDTE MATHEMATIK, CH-8092 Zuerich. https://people.inf.ethz.ch/gander/papers/qrneu.pdf. Gautschi, W. 1983. “On the Convergence Behavior of Continued Fractions with Real Elements.” Mathematics of Computation, Volume 40, Number 161, Pp. 337-342. https://www.ams.org/journals/mcom/1983-40-161/S0025-5718-1983-0679450-2/S0025-5718-1983-0679450-2.pdf. Ge, R., Kakade, S. M., Kidambi, R., &amp; Netrapalli, P. 2019. “The Step Decay Schedule: A Near Optimal, Geometrically Decaying Learning Rate Procedure for Least Squares.” https://arxiv.org/pdf/1904.12838.pdf. Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., &amp; Rubin, D. B. 2013. Bayesian Data Analysis: Texts in Statistical Science. 3rd Edition. CRS Press. Gelman, A., Vehtari, A., Sivula, T., Jylänki, P., Tran, D., Sahai, S., Blomstedt, P., Cunningham, J. P., Schiminovich, D., &amp; Robert, C. 2017. “Expectation Propagation as a Way of Life: A Framework for Bayesian Inference on Partitioned Data.” http://www.stat.columbia.edu/~gelman/research/unpublished/ep_arxiv.pdf. Gharbieh, W. 2018. “Connectionist Temporal Classification, Labelling Unsegmented Sequence Data with Rnn.” [Video]. https://www.youtube.com/watch?v=UMxvZ9qHwJs. Ghosh, A. 2019. “Kalman Filter to Stabilize Sensor Readings.” https://thecustomizewindows.com/2019/03/kalman-filter-to-stabilize-sensor-readings/. Gilks, W. R., &amp; Wild, P. 1992. “Adaptive Rejection Sampling for Gibbs Sampling.” Journal of the Royal Statistical Society. Series C (Applied Statistics), 41(2), 337–348. https://doi.org/10.2307/2347565. Glorot, X., &amp; Bengio Y. 2010. “Understanding the Difficulty of Training Deep Feedforward Neural Networks.” http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf. Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., &amp; Bengio, Y. 2014. “Generative Adversarial Networks.” https://arxiv.org/pdf/1406.2661.pdf. Gordon, M. &amp; Lumley, T. 2019. Forestplot: Advanced Forest Plot Using ’Grid’ Graphics. https://CRAN.R-project.org/package=forestplot. Graves, A., Fernández, S., Gomez, F., &amp; Schmmidhuber, J. 2006. “Connectionist Temporal Classifications: Labelling Unsegmented Sequence Data with Recurrent Neural Networks.” ICML ’06: Proceedings of the 23rd international conference on Machine learningJune 2006 Pages 369–376. https://dl.acm.org/doi/10.1145/1143844.1143891. Graves, A., Mohamed, A-r., &amp; Hinton, G. 2013. “Speech Recognition with Deep Recurrent Neural Networks.” Department of Computer Science, University of Toronto. https://arxiv.org/pdf/1303.5778.pdf. Gross, J., &amp; Ligges, U. 2015. Nortest: Tests for Normality. https://CRAN.R-project.org/package=nortest. Grün, B., &amp; Hornik, K. 2011. “topicmodels: An R Package for Fitting Topic Models.” Journal of Statistical Software 40 (13): 1–30. https://doi.org/10.18637/jss.v040.i13. Gupta, A. 2021. “The 6 Dimensionf of Data Quality.” [Article]. https://www.collibra.com/us/en/blog/the-6-dimensions-of-data-quality. Hager, W. W., &amp; Zhang, H. 2005. “A Survey of Nonlinear Conjugate Gradient Methods.” The National Science Foundation Under Grant No. 0203270. https://www.caam.rice.edu/~zhang/caam454/pdf/cgsurvey.pdf. Halliwell, L. J. 2015. “The Gauss-Markov Theorem: Beyond the Blue.” Casualty Actuarial Society E-Forum, Fall 2015. https://www.casact.org/sites/default/files/database/forum_15fforum_halliwell_gm.pdf. Han, J., Kamber, M., &amp; Pei, J. 2002. Data Mining Concepts and Techniques. Third Edition. Morgan Kaufmann. Han, J., Pei, J., Mortazavi-Asl, B., Chen, Q., Dayal, U., &amp; Hsu, M-C. 2000. “FreeSpan: Frequent Pattern-Projected Sequential Pattern Mining.” Intelligent Database Systems Research Lab., Burnaby, B.C., Canada V5A 1S6 &amp; Hewlett-Packard Labs, Palo Alto, California 94303-0969. https://www.cs.sfu.ca/~jpei/publications/freespan.pdf. Han, J., Pei, J., Yin, Y., &amp; Mao, R. 2000. “Mining Frequent Patterns Without Candidate Generation: A Frequent-Pattern Tree Approach.” Data Mining and Knowledge Discovery, 8, 53-87, 2004. Simon Fraser University, Canada: Kluwer Academic Publishers. http://www.philippe-fournier-viger.com/spmf/fpgrowth_04.pdf. Harris, T., &amp; Hardin, J. W. 2013. “Exact Wilcoxon Signed-Rank and Wilcoxon Mann-Whitney Ranksum Tests.” The Statat Journal (2013), 13, Number 2, pp. 337-343. https://journals.sagepub.com/doi/pdf/10.1177/1536867X1301300208. Hasenclever, L., Webb, S., Lienart, T., Vollmer, S., Lakshminarayanan, B., Blundell, C., &amp; Teh, Y. W. 2017. “Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server.” Machine Learning Research 18 (2017) 1-37. https://dl.acm.org/doi/pdf/10.5555/3122009.3176850?download=true. Hastie, T., Tibshirani, R., &amp; Friedman, J. 2016. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. 2nd Edition, 11th printing 2016. Springer. He, K., Zhang, X., Ren, S., &amp; Sun, J. 2015. “Delving Deep into Rectifiers: Surpassing Human-Level Performance on Imagenet Classification.” https://arxiv.org/pdf/1502.01852.pdf. Heath, M. T. 2002. Scientific Computing: An Introductory Survey. (Revised 2nd Edition, 2018). Society for Industrial; Applied Mathematics. Hefferon, J. 2020. “Linear Algebra.” https://joshua.smcvt.edu/linearalgebra. Hermansky, H. 1990. “Perceptual Linear Predictive (PLP) analysis of speech.” The Journal of the Acoustical Society of America 87, 1738 (1990). https://doi.org/10.1121/1.399423. Hochreiter, S., &amp; Schmidhuber, J. 1997. “Long Short-Term Memory.” Neural ComputationVolume 9, Issue 8, November 15, 1997 Pp 1735–1780. https://dl.acm.org/doi/10.1162/neco.1997.9.8.1735. Hofmann, T. 1999. “Probabilistic Latent Semantic Analysis.” EECS Department, Computer Science Division, University of California, Berkeley &amp; International Computer Science Institute, Berkeley, CA. https://arxiv.org/pdf/1301.6705.pdf. Holoborodko, P. 2012. “Abscissas and Weights of Classical Gaussian Quadrature Rules.” https://www.advanpix.com/2012/05/30/abscissas-and-weights-classical-gaussian-quadrature-rules/. Hong, L. 2012. “Probabilistic Latent Semantic Analysis.” https://arxiv.org/pdf/1212.3900.pdf. Hoppe, T. n.d. “An Application of the Lanczos Method.” Drexel Unniversity. http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_02.pdf. Hoppe T. n.d. “Lanczos Vector Procedures.” Drexel Unniversity. http://www.physics.drexel.edu/~bob/Term_Reports/Hoppe_01.pdf. Hornik, K. 2015. openNLPmodels.en: Apache OpenNLP Models for English. ———. 2019. OpenNLP: Apache Opennlp Tools Interface. https://CRAN.R-project.org/package=openNLP. ———. 2020. NLP: Natural Language Processing Infrastructure. https://CRAN.R-project.org/package=NLP. Horny, M. 2014. “Bayesian Networks.” Department of Health Policy; Management, Boston University School of Public Health. https://www.bu.edu/sph/files/2014/05/bayesian-networks-final.pdf. Hsieh, C-J., Chang, K-W., Lin, C-J., Keerthi, S. S., &amp; Sundararajan, S. 2008. “A Dual Coordiate Descent Method for Large-scale Linear SVM.” https://icml.cc/Conferences/2008/papers/166.pdf. Hssina, B., Merbrouha, A., Hanane, E., &amp; Mohammed, E. 2014. “A Comparative Study of Decision Tree Id3 and C4.5.” (IJACSA) International Journal of Advanced Computer Science and Applications. Special Issue on Advances in Vehicular Ad Hoc Networking and Applications. 10.14569/SpecialIssue.2014.040203. University of Sydney, Sydney Australia 2006. https://www.researchgate.net/publication/265162251_A_comparative_study_of_decision_tree_ID3_and_C45. Huang, Z., Liang, D., Xu, P., &amp; Xiang, B. 2020. Improve Transformer Models with Better Relative Position Embeddings. https://arxiv.org/pdf/2009.13658.pdf. Hyndman, R. J. 2013. Fpp: Data for &quot;Forecasting: Principles and Practice&quot;. https://CRAN.R-project.org/package=fpp. Ihaka, R. n.d. “A Statistics 726 Course.” Levinson-Durbin Code. https://www.stat.auckland.ac.nz/~ihaka/courses/726/sol02.pdf. Illowsky, B., Dean, S., et al. 2018. “Introductory Statistics.” Rice University. https://openstax.org/details/books/introductory-statistics. Ivrii, V. 2021. “Partial Differential Equations.” Department of Mathematics, University of Toronto. http://www.math.toronto.edu/ivrii/PDE-textbook/PDE-textbook.pdf. Jakobsen, P. K. 2019. “An Introduction to Partial Differential Equations.” arXiv: 1901.03022v1. Department of Mathematics and Statistics, the Artctic University of Norway, 9019 Tromsø, Norway. https://arxiv.org/pdf/1901.03022.pdf. Jarlebring, E. 2018. “Broyden’s Method for Nonlinear Eigenproblems.” arXiv:1802.07322v1. https://arxiv.org/pdf/1802.07322.pdf. Joshy, J. &amp; Sambyo, K. 2016. “A Comparison and Contrast of the Various Feature Extraction Techniques in Speaker Recognition.” International Journal of Signal Processing, Image Processing and Pattern Recognition Vol.9, No.11, (2016), pp.99-108. http://dx.doi.org/10.14257/ijsip.2016.9.11.10. Kalman, D. 1996. “A Singularly Valuable Decomposition: The Svd of a Matrix.” THE COLLEGE MATHEMATICS JOURNAL; VOL. 27, NO. 1, JANUARY 1996. http://dankalman.net/AUhome/pdffiles/svd.pdf. Karpathy, A. 2015. “The Unreasonable Effectiveness of Recurrent Neural Network.” [Article]. http://karpathy.github.io/2015/05/21/rnn-effectiveness/. Karunanithi, S., Gajalakshmi, N., Malarvizhi, M., &amp; Saileshwari, M. 2018. “A Study of the Comparison of Jacobi, Gauss-Seidel and Sor Methods for Th eSolution in System of Linear Equations.” International Journal of Mathematics Trends and Technology (IJMTT) - Volume 56 Issue 4 - April 2018. Keng, B. 2018. “Variational Bayes and the Mean-Field Approximation.” [Web Article] 3rd step Variational Bayes. http://bjlkeng.github.io/posts/variational-bayes-and-the-mean-field-approximation/. Kingma, D. P., &amp; Ba, J. L. 2015. “ADAM: A Method for Stochastic Optimization.” Conference Paper at ICLR 2015. https://arxiv.org/pdf/1412.6980.pdf. Kohler, D. F. 1982. “The Relation Among the Likelihood Ratio-, Wald-, and Lagrange Multiplier Tests and Their Applicability to Small Samples.” The Rand Corpoation, Santa Monica, California 90406. https://www.rand.org/content/dam/rand/pubs/papers/2008/P6756.pdf. Korzinek, D. 2021. “How Oes Htk Compute Features?” [Article]. https://notebook.community/danijel3/PyHTK/python-notebooks/HTKFeaturesExplained. Krizhevsky, A. 2009. “Learning Multiple Layers of Features from Tiny Images.” https://www.cs.toronto.edu/~kriz/cifar.html. Kruschke, J. K. 2015. Doing Bayesian Data Analsysi: A Tutorial with R, Jags, and Stan. 2nd Edition. Academic Press: an imprint of Elsevier. Kuhn, M. 2020. Caret: Classification and Regression Training. https://CRAN.R-project.org/package=caret. Kumar, A. 2021. “Deep Learning 71: Back-propagation in Gated Recurrent Unit (GRU) Architecture.” [Video]. https://www.youtube.com/watch?v=P0W3iHKYOHc. Kwak, S. G., &amp; Kim, J. H. 2016. “Central Limit Theorem: The Cornerstone of Modern Statistics.” Departments of Medical Statistics, Anesthesiology and Pain Medicine, School of Medicine, Catholic University of Daegu, Korea. https://ekja.org/upload/pdf/kjae-70-144.pdf. Lambart, B. 2014. “Proof: Gamma Prior Is Conjugate to Poisson Likelihood.” [Video, Ox educ channel]. https://www.youtube.com/watch?v=CBFpqjNZXV0. Lambert, B. 2018. A Student’s Guide to Bayesian Statistics. SAGE. Lanczos, C. 1950. An Iteration Method for the Solution of the Eigenvalue Problem of Linear Differential and Integral Operators. Research of the National Bureau of Standards, Vol. 45, No. 4, October 1950, Research Paper 2133. https://nvlpubs.nist.gov/nistpubs/jres/045/jresv45n4p255_a1b.pdf. Larson, M. G. 2008. “Analysis of Variance: Statistical Primer for Cardiovascular Research.” Department of Mathematics and Statistics, Boston University, Boston: American Heart Association, Inc. https://www.ahajournals.org/doi/pdf/10.1161/CIRCULATIONAHA.107.654335. Larson, R., Edwards, B. H., Falvo, D. C. 2006. Calculus: An Applied Approach. (4th Edition). Houghton Mifflin Company. Lee, S., &amp; Lee, D. K. 2018. “What Is the Proper Way to Apply the Multiple Comparison Test.” Korean J Anesthesiol. 2018 Oct; 71(5): 353–360. https://dx.doi.org/10.4097%2Fkja.d.18.00242. Lele, S. R., Keim, J. K., &amp; Solymos, P. 2019. ResourceSelection: Resource Selection (Probability) Functions for Use-Availability Data. https://CRAN.R-project.org/package=ResourceSelection. Lempert, R. O. 2008. “The Significance of Statisistical Significance: Two Authors Restate an Inconrovertible Caution. Why a Book?” University of Michigan Law School. LeVeque, R. J. 2007. “Finite Difference Methods for Ordinary and Partial Differential Equations.” Society for Industrial; Applied Mathematics. Lewis, A. D. 2017. “Introduction to Differential Equations (for Smart Kids).” https://mast.queensu.ca/~andrew/teaching/pdf/237-notes.pdf. Li, P. 2012. “Robust Logitboost and Adaptive Base Class (Abc) Logitboost.” https://arxiv.org/pdf/1203.3491.pdf. Ligges, U., Krey, S., Mersmann, O., &amp; Schnackenberg, S. 2018. tuneR: Analysis of Music and Speech. https://CRAN.R-project.org/package=tuneR. Loffe, S., &amp; Szegedy, C. 2019. “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift.” Proceedings of the 32nd International Conference on Machine Learning, Lille, France, 2015. JMLR: W&amp;CP Volume 37. http://proceedings.mlr.press/v37/ioffe15.pdf. Luong, M-T., Pham, H., &amp; Manning, C. D. 2015. “Effective Approaches to Attention-Based Neural Machine Translation.” Stanford University, Stanford, cA 94305. https://arxiv.org/pdf/1508.04025.pdf. Lyon, D. 2009. “The Discrete Fourier Transform, Part 4: Spectral Leakage.” Journal of Object Technology. Vol. 8, No. 7, November-December 2009. http://www.jot.fm/issues/issue_2009_11/column2.pdf. Lyons, R. 2013. “A Quadrature Signals Tutorial: Complex, but Not Complicated.” https://www.dsprelated.com/showarticle/192.php. Mahto, A. 2019. Splitstackshape: Stack and Reshape Datasets After Splitting Concatenated Values. https://CRAN.R-project.org/package=splitstackshape. Mathai, A. M. 1993. “A Handbook of Generalized Special Functions for Statistical and Physical Sciences.” Department of Mathematics; Statistics, McGill University: Oxford University Press. Mathew, T. 1997. “Wishart and Chi-Square Distributions Associated with Matrix Quadratic Forms.” Journal of Multivariate Analysis 61, 129-143 (1997). Article No. MV971665. Máté, A. 2014. “The Jordan Canonical Form.” Brooklyn College of the City University of New York. http://www.sci.brooklyn.cuny.edu/~mate/misc/jordan_canonical.pdf. McCullagh, P., &amp; Nelder, J. A. 1983. “Generalized Linear Models.” https://doi.org/10.1201/9780203753736. McHugh, M. L. 2013. “The Chi-Square Test of Independence.” Lessons in Biostatistics. https://www.biochemia-medica.com/en/journal/23/2/10.11613/BM.2013.018. McLaughlin, M. P. 2016. “Compendium of Common Probability Distributions.” https://www.causascientia.org/math_stat/Dists/Compendium.pdf. Merchant, F., Vatwani, T., Chattopadhyay, A., Raha, S., Nandy, S. K., &amp; Narayan, R. 2018. “Achieving Efficient Realization of Kalman Filter on Cgra Through Algorithm-Architecture Co-Design.” https://arxiv.org/pdf/1802.03650.pdf. Merity, S. 2015. “Explaining and Illustrating Orthogonal Initialization for Recurrent Neural Networks.” [Article]. https://smerity.com/articles/2016/orthogonal_init.html. Michalke, M. 2020. koRpus.lang.en: Language Support for ’koRpus’ Package: English. https://reaktanz.de/?c=hacking&amp;s=koRpus. ———. 2021. koRpus: Text Analysis with Emphasis on POS Tagging, Readability, and Lexical Diversity. https://reaktanz.de/?c=hacking&amp;s=koRpus. Mikolov, T., Corrado, G., Chen, K., &amp; Dean, J. 2013. “Efficient Estimation of Word Representations in Vector Space.” Google Inc., Mountain View, CA. https://arxiv.org/pdf/1301.3781.pdf. Milborrow, S. 2019. Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’Plot.rpart’. https://CRAN.R-project.org/package=rpart.plot. Minka, T. P. 2001a. “A Family of Algorithms for Approximate Bayesian Inference.” https://tminka.github.io/papers/ep/minka-thesis.pdf. ———. 2001b. “Expectation Propagation for Approximate Bayesian Inference.” Carnegie Mellon University. https://arxiv.org/pdf/1301.2294.pdf. Misra, D. 2019. “Mish: A Self Regularized Non-Monotonic Neural Activation Function.” https://arxiv.org/vc/arxiv/papers/1908/1908.08681v2.pdf. Mohammed, A. S., &amp; Al-jawary, M. A. 2021. “Three Weighted Residuals Method for Solving the Nonlinear Thin Film Flow Problem.” Ibn Al-Haitham International Conference for Pure and Applied Sciences (IHICPS), Journal of Physics: Conference Series, 1879 (2021) 022096, Doi:10.1088/1742-6596/1879/2/022096. Department of Mathematics, College of Education for Pure Sciences, Universy of Baghdad, Iraq. Morris, T. P., White, I. R., &amp; Royston, P. 2014. “Tuning Multiple Imputation by Predictive Mean Matching and Local Residual Draws.” BMC Med Res Methodol 14, 75. https://doi.org/10.1186/1471-2288-14-75. Mortimer, S., &amp; Bennett, C. 2017. Aurelius: Generates Pfa Documents from R Code and Optionally Runs Them. https://CRAN.R-project.org/package=aurelius. Muda, L., Begam, M., &amp; Elamvazuthi, I. 2010. “Voice Recognition Algorithms Using Mel Frequency Cepstral Coefficient (Mfcc) and Dynamic Time Warping (Dtw) Techniques.” Computing, Volume 2, ISSUE 3, March 2010, ISSN 2151-9617. https://arxiv.org/pdf/1003.4083.pdf. Murphy, K. P. 2012. Machine Learning: A Probabilistic Perspective. Massachusetts Institute of Technology. Musto, C., Semeraro, G., &amp; Polignano, M. n.d. “A Comparison of Lexicon-Based Approaches for Sentiment Analysis for Microblog Posts.” Department of Computer Science, University of Bari Aldo Moro, Italy. http://ceur-ws.org/Vol-1314/paper-06.pdf. Muthumalai, R. K. 2012. “Note on Newton Interpolation Formula.” International Journal of Mathematics Analysis, Vol. 6, 2012, No. 50, 2459-2465. http://www.m-hikari.com/ijma/ijma-2012/ijma-49-52-2012/muthumalaiIJMA49-52-2012.pdf. Natoli, C. 2017. “Understanding Analysis of Variance: Best Practice.” STAT Center of Excellence, STAT COE-Report-29-2017. 2950 Hobson Way-Wright-Patterson AFB, OH 45433. https://www.afit.edu/stat/statcoe_files/ANOVA%20Final.pdf. Neuwirth, E. 2014. RColorBrewer: ColorBrewer Palettes. https://CRAN.R-project.org/package=RColorBrewer. Nguyen, L. 2013. “Overview of Bayesian Network.” University of Technology, Ho Chi Minh city, Vietnam. https://www.researchgate.net/profile/Loc-Nguyen-101/publication/282685628_Overview_of_Bayesian_Network/links/5f55ff10299bf13a31a7d529/Overview-of-Bayesian-Network.pdf. Noughreche, A., Boulouma, S., &amp; Benbaghdad, M. 2021. “Design and Implementation of an Automatic Speech Recognition Based on Voice Control System.” https://easychair.org/publications/preprint_download/wzRf. Olsen-Kettle, L. 2011. “Numerical Solution of Partial Differential Equations.” The University of Queensland, School of Eartch Sciences, Centre for Geoscience Computing. https://espace.library.uq.edu.au/view/UQ:239427/Lectures_Book.pdf. Ooms, J. 2014. “The Jsonlite Package: A Practical and Consistent Mapping Between Json Data and R Objects.” arXiv:1403.2805 [stat.CO]. https://arxiv.org/abs/1403.2805. Osborne, J. W. 2013. “Best Practises in Data Cleaning: A Complete Guide to Everything You Need to Do Before and After Collecting Your Data.” http://pzs.dstu.dp.ua/DataMining/preprocessing/bibl/cleaning.pdf. Paige, C. C. 1975. “Error Analysis of the Lanczos Algorithm for Tridiagonalizing a Sysmmetric Matrix.” J. Inst. Maths Applies (1976, 18, 341-349). School of Computer Science, McGill University, Montreal, Quebec, Canada. https://www.cs.mcgill.ca/~chris/pubClassic/76JIMA001.pdf. Papadopoulos, P. 2015. “Introduction to the Finite Element Method.” Department of Mechanical Engineering, University of California, Berkeley. Papineni, K., &amp; Roukos, S., Ward, T., &amp; Zhu, W-J. 2002. “BLEU: A Method for Automatic Evaluation of Machine Translation.” Proceedings of the 40th Annual Meeting on Association for Computational Linguistics, July 2002. https://dl.acm.org/doi/10.3115/1073083.1073135. Parlett, B. N. 1974. “The Rayleigh Quotient Iteration and Some Generalizations for Nonnormal Matrices.” Mathematics of Computation, Volume 28, Number 127, July 1974, Pages 679-693. https://www.ams.org/journals/mcom/1974-28-127/S0025-5718-1974-0405823-3/S0025-5718-1974-0405823-3.pdf. ———. 1994. “Do We Fully Understand the Symmetric Lanczos Algorithm yet?” Supported by ONR, Contract N000014-90-J-1372. Department of Mathematics, University of California, Berkeley, CA 94720, USA. https://apps.dtic.mil/sti/pdfs/ADA289614.pdf. Pedersen, T. L. 2020. Patchwork: The Composer of Plots. https://CRAN.R-project.org/package=patchwork. Pei, J., Han, J., Mortazavi-Asl, B., Wang, J., Pinto, H., Chen, Q., Dayal, U., &amp; Hsu, M-C. 2004. “Mining Sequential Patterns by Pattern-Growth: The Prefixspan Approach.” IEEE Transactions on Knowledge and Data Engineering, VOL. 16, NO.10. Oct 2004. IEEE Computer Society. http://hanj.cs.illinois.edu/pdf/tkde04_spgjn.pdf. Pernkopf, F., Peharz, R., &amp; Tschiatschek, S. 2014. “Introduction to Probabilistic Graphical Models.” In Academic Press Library in Signal Processing: Signal Processing Theory and Machine Learning (Pp. 989-1064). Academic Press. Pivarski, J., Bennett, C., &amp; Grossman, R. L. 2016. “Deploying Analytics with the Portable Format for Analytics (Pfa).” Department of Computer Science, University of Toronto. https://www.kdd.org/kdd2016/papers/files/adp0884-pivarskiAcb.pdf. Platt, J. C. 1998. “Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines.” https://www.microsoft.com/en-us/research/publication/sequential-minimal-optimization-a-fast-algorithm-for-training-support-vector-machines/. Plummer, M. 2003. Doing Bayesian Data Analsysi: A Tutorial with R, Jags, and Stan. Proceedings of the 3rd International Workshop on Distributed Statistical Computing (DSC 2003). https://www.r-project.org/conferences/DSC-2003/Proceedings/Plummer.pdf. ———. 2019. Rjags: Bayesian Graphical Models Using Mcmc. https://CRAN.R-project.org/package=rjags. Prautzsch, H., Boehm, W., &amp; Paluszny, M. 2002. “Bézier- and B-Spline Techniques.” https://geom.ivd.kit.edu/downloads/pubs/pub-boehm-prautzsch_2002_preview.pdf. Press, W. H., Teukolsky, S. A., Vetterling, W. T., &amp; Flannery, B. P. 2007. Numerical Recipes: The Art of Scientific Computing. (3rd Edition, printed 2007). Cambridge University Press. Quinlan, J. R. 1986. “Induction of Decision Trees. Mach Learn 1.” 81–106 (1986). https://doi.org/10.1007/BF00116251. ———. 1996. “Improving Regressors Using Boosting Techniques.” Journal of Artificial Intelligence Research 4 (1996) 77-90. University of Sydney, Sydney Australia 2006. https://arxiv.org/pdf/cs/9603103.pdf. Rabiner, L. R. 1988. “A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition.” Proceedings of the IEEE, Vol. 77, No. 2, 1989. https://www.cs.cmu.edu/~cga/behavior/rabiner1.pdf. Raissi, M. 2021. “Mel-Spectrogram and MFCCs.” [Video]. https://www.youtube.com/watch?v=hF72sY70_IQ. Ramachandran, P., Zoph, B., &amp; Le, Q. V. 2017. “SWISH: A Self-Gated Activation Function.” https://arxiv.org/pdf/1710.05941v1.pdf. Ramachandran, T., D. Udayakumar, D., &amp; Parimala, R. 2017. “Contra-Harmonic Mean Derivative - Based Closed Newton Cotes Quadrature.” Global Journal of Pure and Applied Mathematics. ISSN 0973-1768 Volume 13, Number 5(2017), Pp. 1319-1330. Research India Publications. https://www.ripublication.com/gjpam17/gjpamv13n5_01.pdf. R Core Team. 2019. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Reddi, S. J., Kale, S., &amp; Kumar, S. 2018. “On the Convergence of Adam and Beyond.” Conference Paper at ICLR 2018. https://arxiv.org/pdf/1904.09237.pdf. Rinker, T. W. 2019. sentimentr: Calculate Text Polarity Sentiment. Buffalo, New York. http://github.com/trinker/sentimentr. Robert, C., &amp; Casella, G. 2008. “A History of Markov Chain Monte Carlo - Subjective Recollections from Incomplete Data -.” hal-00311793. https://hal.archives-ouvertes.fr/hal-00311793/document. Robert, C. P. 2016. “The Metropolis-Hastings Algorithm.” https://arxiv.org/pdf/1504.01896.pdf. Robertson, S. E., Walker, S., Jones, S., Hancock-Beaulieu, M. M., &amp; Gatford, M. 1994. “Okapi at TREC-3.” Centre for Interactive Systems Research. Department of Information Science, City University, Northampton Square, London ECIV 0HB, UK. https://www.computing.dcu.ie/~gjones/Teaching/CA437/city.pdf. Rodger, R. S., &amp; Roberts, M. 2013. “Comparison of Power for Multiple Comparison Procedures.” Journal of Methods and Measurement in the Social Sciences, Vol.4, No1,20-47, 2013. https://journals.librarypublishing.arizona.edu/jmmss/article/799/galley/794/view/. Ross, S. 2010. “A First Course in Probability.” University of Southern California: Pearson. Saad, Y., &amp; Schultz, M. H. 1986. GMRES: A Generalized Minimal Residual Algorithm for Solving Nonsymmetric Linear Systems. SIAM Journal on Scientific and Statistical Computing. Sabri, F., &amp; Gyateng, T. 2015. “Understanding Statistical Significance: A Short Guide.” NPC’s Data Labs Project, Funded by the Oak Foundation. Saha, M., &amp; Chakrabarty, J. 2018. “On Generalized Jacobi, Gauss-Seidel and Sor Method.” https://arxiv.org/pdf/1806.07682.pdf. Salih, A. A. n.d. “Finite Element Method.” Department of Aerospace Engineering, Indian Institute of Space Science; Technology, Thiruvananthapuram, India. https://www.iist.ac.in/sites/default/files/people/IN08026/FEM.pdf. Sankar, A. 2019. “Sequence to Sequence Learning with Encoder-Decoder Neural Network Models.” [Video]. https://www.youtube.com/watch?v=bBBYPuVUnug. Santunkar, S., Tsipras, D., Ilyas, A., &amp; Madry, A. 2018. “How Does Batch Normalization Help Optimization.” 32nd International Conference on Neural Information Processing Systems (NIPS 2018), Montréal, Canada. http://proceedings.mlr.press/v37/ioffe15.pdf. Savov, I. 2017. “No Bullshit Guide to Linear Algebra.” https://minireference.com/static/excerpts/noBSLA_v2_preview.pdf. Schapire, R. E. 1999. “A Brief Introduction to Boosting.” Proceedings of the Sixteenth International Joint Conference on Artificial Intelligence. http://rob.schapire.net/papers/Schapire99c.pdf. Schuster, M., &amp; Paliwal, K. K. 1997. “Bidirectional Recurrent Neural Network.” Signal Processing, IEEE Transactions on. 45. 2673 - 2681. 10.1109/78.650093. https://www.researchgate.net/profile/Mike-Schuster-2/publication/3316656_Bidirectional_recurrent_neural_networks. Shalev-Shwartz, S., Singer, Y., &amp; Srebro, N. 2007. “Pegasos: Primal Estimated Sub-Gradient Solver for Svm.” ACM International Conference Proceeding Series. 227. 807-814. 10.1145/1273496.1273598. https://doi.org/10.1007/s10107-010-0420-4. Shalev-Shwatz, S., &amp; Zhang, T. 2013. “Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization.” https://arxiv.org/pdf/1209.1873.pdf. Shao, X., &amp; Johnson, S. G. 2009. “Typte-Ii/Iii Dct/Dst Algorithms with Reduced Number of Arithmetic Operations.” Department of Mathematics, Massachusetts Institute of Technology,Cambridge MA 02139. https://arxiv.org/pdf/cs/0703150.pdf. Shaw, P., Uszkoreit, J., &amp; Vaswani, A. 2018. Self-Attention with Relative Position Representations. https://arxiv.org/pdf/1803.02155.pdf. Silge, J., &amp; Robinson, D. 2016. “Tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” JOSS 1 (3). The Open Journal. https://doi.org/10.21105/joss.00037. Sleijpen, G. 2014. “Krylov Subspace Methods.” Program Lecture 3, Upssala, April 2014. Department of Mathematics, Universiteit Utrecht. http://www.math.uu.nl/~sleij101/Uppsala/Lecture-handouts3.pdf. Smith, L. N. 2017. “Cyclical Learning Rate for Training Neural Networks.” https://arxiv.org/pdf/1506.01186.pdf. Soetaert, K. 2019. Plot3D: Plotting Multi-Dimensional Data. https://CRAN.R-project.org/package=plot3D. Solomatine, D., &amp; Shrestha, D. 2004. “AdaBoost.RT: A Boosting Algorithm for Regression Problems.” IEEE International Conference on Neural Networks - Conference Proceedings. 2. 1163 - 1168 vol.2. 10.1109/IJCNN.2004.1380102. https://www.researchgate.net/publication/4116773_AdaBoostRT_A_boosting_algorithm_for_regression_problems. Speagle, J. S. 2020. “A Conceptual Introduction to Markov Chain Monte Carlo Methods.” Center for Astrophysics, Harvard &amp; Smithsonian, 60 Garden St. Cambridge, MA 02138, USA. https://arxiv.org/pdf/1909.12313.pdf. Srikant, R., &amp; Agrawal, R. 1996. “Mining Sequential Patterns: Generalizations and Performance Improvements.” In: Apers P., Bouzeghoub M., Gardarin G. (Eds) Advances in Database Technology — EDBT ’96. EDBT 1996. Lecture Notes in Computer Science, Vol 1057. Berlin, Heidelberg: Springer. https://doi.org/10.1007/BFb0014140. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. 2014. “Dropout: A Simple Way to Prevent Neural Networks from Overfitting.” Machine Learning Research 15 (2014), 1929-1958. https://dl.acm.org/doi/pdf/10.5555/2627435.2670313. Stachniss, C. 2013. “SLAM Course - 06 - Unscented Kalman Filter.” https://www.youtube.com/watch?v=DWDzmweTKsQ&amp;t=233s. Stanford. 2009. “The Simplified SMO Algorithm.” CS 229, Automn 2009. http://cs229.stanford.edu/materials/smo.pdf. Stanimirovíc, P. S., &amp; Tasić, M. B. 2011. “Computing Generalized Inverses Using Lu Factorization of Matrix Product.” University of Niš, Department of Mathematics, Faculty of Science, Višegradska, 33, 1800 Niš, Serbia. https://arxiv.org/pdf/1104.1697.pdf. Sternstein, M. 1996. “Statistics.” Barron’s Educational Series, Inc. Stewart, W. J. 2009. Probability, Markov Chains, Queues, and Simulation: The Mathematical Basis of Performance Modeling. Princeton University Press. Stobierski, T. 2019. What Is Statistical Modeling for Data Analysis. What Is Statistical Modeleling for Data Analysis. https://www.northeastern.edu/graduate/blog/statistical-modeling-for-data-analysis/. Strang, G. 1999. “The Discrete Cosine Transform.” SIAM Review, Vol. 41, No. I, Pp. 135-147. https://www.unioviedo.es/compnum/transversal_eng/DCT5.pdf. ———. 2005. “Linear Algebra.” [Video series] Lec 1: MIT 18.06 Linear Algebra, Spring 2005. Massachusetts Institute of Technology: MIT OpenCourseware. https://www.youtube.com/watch?v=QVKj3LADCnA. Strauss, W. A. 2008. “Partial Differential Equations: An Introduction.” John Wiley &amp; Sons, Inc. Sueur, J. 2018. “Sound Analysis and Synthesis with R.” [1st Edition], p.390. https://link.springer.com/book/10.1007/978-3-319-77647-7. Sun, S., &amp; Iyyer, M. 2021. “Revisiting Simple Neural Probabilistic Language Model.” College of Information and Computer Sciences, University of Massachusetts Amherst. https://arxiv.org/pdf/2104.03474.pdf. Tan, P-N., Kumar, V., &amp; Srivastava, J. 2002. “Selecting the Right Interestingness Measure for Association Patterns.” SIGKDD ’02 Edmonton, Alberta, Canada. http://www.cse.msu.edu/~ptan/publication/kdd2002.pdf. Tang Y., &amp; ONNX Authors. 2021. Onnx: R Interface to ’Onnx’. https://CRAN.R-project.org/package=onnx. Therneau, T., &amp; Atkinson, B. 2019. Rpart: Recursive Partitioning and Regression Trees. https://CRAN.R-project.org/package=rpart. Therneau, T. M., Atkinson, E. J., &amp; Mayo Foundation. 1997. “An Introduction to Recursive Partitioning Using the Rpart Routines.” https://www.stat.auckland.ac.nz/~yee/784/files/techrep.pdf. ———. 2019. “An Introduction to Recursive Partitioning Using the Rpart Routines.” https://cran.r-project.org/web/packages/rpart/vignettes/longintro.pdf. Thompson, L., &amp; Mimno, D. 1997. “Topic Modeling with Contextualized Word Representation Clusters.” https://arxiv.org/pdf/2010.12626.pdf. Tieleman, T., &amp; Hinton, G. 2012. “Lecture 6.5-Rmsprop: Divide the Gradient by a Running Average of Its Recent Magnitude.” COURSERA: Neural Networks for Machine Learning, 4(2), 26-31. Tolver, A. 2016. “An Introduction to Markov Chains.” Department of Mathematical Sciences, University of Copenhagen, Universitetsparken 5, DK-2100 CopenHagen Ø, Denmark. http://web.math.ku.dk/noter/filer/stoknoter.pdf. Trapletti, A., &amp; Hornik, K. 2019. Tseries: Time Series Analysis and Computational Finance. https://CRAN.R-project.org/package=tseries. Ugoni A., &amp; Walker, B. F. 1995. “The T Test: An Introduction.” COMSIG review, 4(2), 37–40. https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2050377/. Umesh, S., Cohen, L., &amp; Nelson, D. J. 1999. “Fitting the Mel Scale.” In Fitting the Mel scale. 1. 217 - 220 vol.1. 10.1109/ICASSP.1999.758101. https://www.researchgate.net/publication/3793925_Fitting_the_Mel_scale. Urbanek, S. 2019. Jpeg: Read and Write Jpeg Images. https://CRAN.R-project.org/package=jpeg. Urrea, C. &amp; Agramonte, R. 2021. “Kalman Filter: Historical Overview and Review of Its Use in Robotics 60 Years After Its Creation.” Hindawi, Journal of SensorsVolume 2021, Article ID 9674015, 21 Pages. https://doi.org/10.1155/2021/9674015. van Biezen, M. 2015. “The Kalam Filter: A Video Series.” [Video]. https://www.youtube.com/watch?v=CaCcOwJPytQ. van der Merwe, R. &amp; Wan, E. 2013. “Sigma-Point Kalman Filters for Probabilistic Inference in Dynamic State-Space Models.” OGI School of Science &amp; Engineering. https://www.gatsby.ucl.ac.uk/~byron/nlds/merwe2003a.pdf. Vapnik, V. N. 2000. “The Nature of Statistical Learning Theory: Statistics for Engineering and Information Science.” Second Edition 1999-2000, First Edition 1995-1996. Springer-Verlag New York, Inc. Vaswani, A., Shadeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. 2017. “Attention Is All You Need.” 31st Conference on Neural Information Processing Systems (NIPS 2017). Long Beach, CA, USA. https://arxiv.org/pdf/1706.03762.pdf. Vorontsov, E. 2017. “On Orthogonality and Learning Recurrent Networks with Long Term Dependencies.” https://arxiv.org/pdf/1702.00071.pdf. ———. 2020. “Provable Benefit of Orthogonal Initialization in Optimizing Deep Linear Networks.” Conference Paper at ICLR 2020. https://arxiv.org/pdf/2001.05992.pdf. Walck, C. 2007. “Hand-Book on Statistical Distributions for Experimentalists.” Internal Report SUF-PFY/96-01. University of Stockholm. http://www.stat.rice.edu/~dobelman/textfiles/DistributionsHandbook.pdf. Wan, E. A., &amp; van der Merwe, R. 2000. “The Unscented Kalman Filter for Nonlinear Estimation.” Proceedings of the IEEE 2000 Adaptive Systems for Signal Processing, Communications, and Control Symposium (Cat. No.00EX373). https://ieeexplore.ieee.org/document/882463. Wand, M. 2019. KernSmooth: Functions for Kernel Smoothing Supporting Wand &amp; Jones (1995). https://CRAN.R-project.org/package=KernSmooth. Wei, T. &amp; Simko, V. 2017. R Package &quot;Corrplot&quot;: Visualization of a Correlation Matrix. https://github.com/taiyun/corrplot. White, H. 1982. “Econometrica: Maximum Likelihood Estimation of Misspecified Models.” Econometrica, Vol. 50, No. 1. (Jan., 1982), Pp. 1-25. The Econometric Society. https://www.jstor.org/stable/1912526. Wickham, H., François, R., Henry, L., &amp; Müller, K. 2021. Dplyr: A Grammar of Data Manipulation. https://CRAN.R-project.org/package=dplyr. Wickham, H., &amp; Seidel, D. 2019. Scales: Scale Functions for Visualization. https://CRAN.R-project.org/package=scales. Wild, F. 2020. Lsa: Latent Semantic Analysis. https://CRAN.R-project.org/package=lsa. Xia, F. 2013. DunnettTests: Software Implementation of Step-down and Step-up Dunnett Test Procedures. https://CRAN.R-project.org/package=DunnettTests. Xie, Y. 2019. Knitr: A General-Purpose Package for Dynamic Report Generation in R. Yang, Q. 2019. “The Preconditioned Lanczos Method for Solving Fractional Diffusion-Reaction Equations.” AMS Winter School 2019. https://cai.centre.uq.edu.au/files/10578/Lec3_Preconditioned_Lanczos_for_FDRE.pdf. Yang, X. 2017. “Understanding the Variational Lower Bound.” http://www.xyang35.umiacs.io/files/understanding-variational-lower.pdf. Yao, Y., Vehtari, A., Simpson, D., &amp; Gelman, A. 2018. “Yes, but Did It Work?: Evaluating Variational Inference.” 3rd step Variational Bayes. https://arxiv.org/pdf/1802.02538.pdf. YPMA, T. J. 1995. “Historical Development of the Newton-Raphson Method.” SIAM Review, 37(4), 531–551. http://www.jstor.org/stable/2132904. Yuan, Q., Gu, M., &amp; Li, B. 2018. “A Tour of the Lanczos Algorithm and Its Convergence Guarantees Through the Decades.” [PowerPoint Slide]. Department of Mathematics, UC Berkeley. https://math.berkeley.edu/~mgu/MA128BSpring2018/Lanczos.pdf. Zafar, F., Saleem, S., O.E.Burg, C., &amp; Minhós, F. 2014. “New Derivative Based Open Newton-Cotes Quadrature Rules.” Abstract and Applied Analysis, Volume 2014,Article ID 109138. Hindawi Publishing Corporation. {https://doi.org/10.1155/2014/109138}. Zaki, M. J. 2001. “SPADE: An Efficient Algorithm for Mining Frequent Sequences.” Machine Learning, 42, 31-60, 2001. Computer Science Department, Rensselaer Polytechnic Institute, Troy NY 12180-3590: Kluwer Academic Publishers. http://www.philippe-fournier-viger.com/spmf/SPADE.pdf. Zeiler, M. D. 2012. “ADADELTA: An Adaptive Learning Rate Method.” https://arxiv.org/pdf/1212.5701.pdf. Zhao, W., &amp; Li, H. 2013. “Midpoint Derivative-Based Closed Newton-Cotes Quadrature.” Hindawi Publishing Corporation, Abstract and Applied Analysis, Volume 2013, Article ID 492507, 10 pages. Research India Publications. http://dx.doi.org/10.1155/2013/492507. Zhirui, Y., Zhang, Y., &amp; Lord, D. 2012. “Goodness-of-Fit Testing for Accidental Models with Low Means.” Accident analysis and prevention. 61. 10.1016/j.aap.2012.11.007. https://www.researchgate.net/publication/233877185_Goodness-of-fit_testing_for_accident_models_with_low_means. Zhu, J., Rosset, S., Zou, H., &amp; Hastie, T. 2006. “Multi-Class Adaboost.” https://web.stanford.edu/~hastie/Papers/samme.pdf. Zucchini, W. 2003. A Density-Based Algorithm for Discovering Clusters. KDD-96 Proceedings, (c) 1996, AAAI (Www.aaai.org). http://staff.ustc.edu.cn/~zwp/teach/Math-Stat/kernel.pdf. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
