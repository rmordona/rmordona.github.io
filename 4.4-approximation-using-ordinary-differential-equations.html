<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.4 Approximation using Ordinary Differential Equations  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="4.4 Approximation using Ordinary Differential Equations  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.4 Approximation using Ordinary Differential Equations  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="4.3-approximation-by-numerical-differentiation.html"/>
<link rel="next" href="4.5-approximation-using-functional-differential-equations.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to System of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="approximation-using-ordinary-differential-equations" class="section level2 hasAnchor">
<h2><span class="header-section-number">4.4</span> Approximation using Ordinary Differential Equations  <a href="4.4-approximation-using-ordinary-differential-equations.html#approximation-using-ordinary-differential-equations" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Change</strong> happens from moment to moment and place to place. It happens from the movement of celestial bodies down to the motion of a pendulum. There is <strong>Change</strong> where there is growing, aging, decaying, cooling, and heating. There is also <strong>Change</strong> as water flows downstream or as the wind blows in all directions.</p>
<p>In this section, we deal with the rate of <strong>Change</strong> with respect to time - we introduce <strong>Differential Equations</strong>. </p>
<p><strong>Differential Equations</strong>, in one of its most basic examples, is expressed in one of the following general forms:</p>
<p><span class="math display" id="eq:equate1060109">\[\begin{align}
y&#39; = y\ \ \ \ \ \ or\ \ \ \ \ \ \ y&#39;&#39; = y\ \ \ \ \ \ or\ \ \ \ \ \ \ y&#39;&#39; + y&#39; = y\ \ \ \ \  or\ \ \ \ \ \ \ y&#39;&#39;&#39; + y&#39;&#39; + y&#39; = y \tag{4.111} 
\end{align}\]</span></p>
<p>It simply means that the equation comes with a function <strong>y</strong> and its corresponding one or more derivatives <strong>yâ</strong>, <strong>yââ</strong>, etc. Note that what we refer to as function <strong>y</strong> can also be expressed in this form:</p>
<p><span class="math display" id="eq:equate1060110">\[\begin{align}
y&#39; = y\ \ \ \rightarrow\ \ \  f&#39;(x) = f(x)\ \ \ \ \ \ where\ y = f(x)\ \ and\ \ y&#39; = f&#39;(x) \tag{4.112} 
\end{align}\]</span></p>
<p>For most of the <strong>methods of differential equations</strong> covered in this chapter, we use the simplest form: <span class="math inline">\(y&#39; = y\ or\ y&#39;&#39;=y\)</span> to explain a point.</p>
<p>The idea of <strong>differential equations</strong> is to <strong>solve for function</strong> <strong>y</strong>. For that, we use a method called <strong>separation of variables</strong>. As long as a <strong>differential equation</strong> can be separable, then a <strong>separable differential equation</strong> can yield us a solution to a function. For example, we know that:</p>
<p><span class="math display" id="eq:equate1060111">\[\begin{align}
\frac{dy}{dx} = y\ \ \ \ \leftarrow\ \ \ \ \ y&#39; = y  \tag{4.113} 
\end{align}\]</span></p>
<p>Here we separate <strong>x</strong> and <strong>y</strong> like so, then we integrate:</p>
<p><span class="math display">\[
\frac{dy}{dx} = y\ \ \ \ \rightarrow\ \ \ \frac{dy}{y} = dx\ \ \rightarrow\ \ \ \int\frac{dy}{y} = \int dx\ \ \ \rightarrow\ \ \ \ln|y|+c = x + c\ \ \ \ \rightarrow\ \ \ y = e^{x}
\]</span></p>
<p>Here, we illustrate how to solve a function which can be expressed this way:</p>
<p><span class="math display">\[
y = e^{x}\ \  \ \ \rightarrow\ \ \ \ \ f(x) = e^{x}
\]</span></p>
<p>Plotting <span class="math inline">\(\mathbf{f(x) = e^{x}}\)</span> gives us the following Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:differential1">4.13</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:differential1"></span>
<img src="embed0032.png" alt="Separable Differential Equations" width="70%" />
<p class="caption">
Figure 4.13: Separable Differential Equations
</p>
</div>
<p>One pointer to note is that not all <strong>Differential Equations</strong> are <strong>separable</strong>; also, solving the function <strong>y</strong> may pose an extreme challenge. For that, we need <strong>to approximate</strong>. We investigate a few <strong>approximation</strong> methods to solve function <strong>y</strong> in <strong>ODE</strong> following the rest of the sections. Note that other literature denotes the approximate solution <strong>y</strong> as <strong>u</strong> to distinguish exact solution vs.Â approximate solution, e.g.:</p>
<p><span class="math display">\[
\mathbf{y_{(exact)}\ vs\ y_{(approx)}}\ \ \ \rightarrow
\ \ \ \ \ \ \ \ y=e^x\ \ \ \ \ \ vs\ \ \ \ u\approx c_0 + c_1x + c_2 x^2
\]</span></p>
<p>In most of our discussions, we use the notation <strong>y</strong> to denote the approximate solution. And we express the exact solution more explicitly.</p>
<p>First, we introduce two types of <strong>Differential equations</strong>: the first type is the <strong>Ordinary Differential Equations (ODE)</strong> with one input. The second type is the <strong>Partial Differential Equations (PDE)</strong> with multiple inputs.</p>
<p>With <strong>ODE</strong>, there are two problems we try to solve: the <strong>initial value problem (IVP)</strong> and the <strong>boundary value problem (BVP)</strong>.</p>
<p>In many cases, one of the challenges we deal with when it comes to <strong>Change</strong> is determining the initial state of the problem before the first <strong>Change</strong> - what was the <strong>initial value</strong> of the problem? If the <strong>initial value</strong> is unknown, we face multiple solutions. But suppose a problem has a given initial value, e.g., <span class="math inline">\(y(0) = 1\)</span>, and has a derivative of an unknown function. In that case, this is an <strong>initial value problem (IVP)</strong>, so then we can approximate a target value.</p>
<p>We introduce a few methods that deal with solving <strong>IVPs</strong> in the following sections, using an <strong>initial value</strong> as a starting point for an <strong>iterative method</strong>. We start with one of the classic methods called <strong>Eulerâs method</strong>.</p>
<div id="eulers-method-explicit" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.1</span> Eulerâs Method (Explicit) <a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>All Change starts from an <strong>initial value</strong>, then <strong>Change</strong> happens one step at a time over the course of a period. If we are to express the statement into an <strong>Euler</strong> equation, we get:</p>
<p><span class="math display" id="eq:equate1060112">\[\begin{align}
y_{1} = y_0 + \Delta t y&#39;_0\ \ \ \leftarrow y_1 = y_0 + \Delta t\ f(t_0, y_0) \ \ \ \ \ \leftarrow \text{recall}\ y = mx + b  \tag{4.114} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[
\begin{array}{lll}
y_0 &amp;\leftarrow &amp; \text{initial value} \\
y_1 &amp;\leftarrow &amp;\text{next value} \\
t &amp;\leftarrow &amp; \text{time}\ \ \ \text{t is the abscissa x in x-axis }\\
\Delta t  &amp;\leftarrow &amp;\text{one step at a time (stepsize of time)}  \\
y_0&#39; &amp;\leftarrow &amp;\text{course of time (change with respect to time)}\ \ \leftarrow \frac{dy_0}{dt_0}
\end{array}
\]</span></p>
<p>Here, we are after approximating an <strong>unknown target point</strong> given an <strong>initial point</strong> and a <strong>first-order derivative</strong> of a function. In general, the formula is as follows:</p>
<p><span class="math display" id="eq:equate1060113">\[\begin{align}
y_{k+1} = y_{k} + \Delta t y&#39;_k \ \ \ \leftarrow y_{k+1} = y_k + \Delta t\ f(t_k, y_k) \tag{4.115} 
\end{align}\]</span></p>
<p>and where:</p>
<p><span class="math display" id="eq:equate1060114">\[\begin{align}
y_k&#39; = f(t_k, y_k) \tag{4.116} 
\end{align}\]</span></p>
<p>To illustrate, we use basic <strong>iterative method</strong> called <strong>Eulerâs method</strong>.</p>
<p>For example, use <strong>Eulerâs method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display" id="eq:equate1060115">\[\begin{align}
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y  \tag{4.117} 
\end{align}\]</span></p>
<p>Here, we have:</p>
<p><span class="math display" id="eq:equate1060116">\[\begin{align}
f(t_k, y_k) = y_k&#39; = y\ \ \ \ \ \ \ \ \text{where our exact solution is }\ \mathbf{y = e^t} \tag{4.118} 
\end{align}\]</span></p>
<p>See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:eulertable1">4.7</a>.</p>

<table>
<caption><span id="tab:eulertable1">Table 4.7: </span>Euler Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(y_k + \Delta t y&#39;_k\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right"><span class="math inline">\(1.000 + (0.2)(1.000)\)</span></td>
<td align="right">1.200</td>
<td align="right">1.221</td>
<td align="right">0.021</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.200</td>
<td align="right"><span class="math inline">\(1.200 + (0.2)(1.200)\)</span></td>
<td align="right">1.440</td>
<td align="right">1.492</td>
<td align="right">0.052</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.440</td>
<td align="right"><span class="math inline">\(1.440 + (0.2)(1.440)\)</span></td>
<td align="right">1.728</td>
<td align="right">1.822</td>
<td align="right">0.094</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.728</td>
<td align="right"><span class="math inline">\(1.728 + (0.2)(1.728)\)</span></td>
<td align="right">2.074</td>
<td align="right">2.226</td>
<td align="right">0.152</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.074</td>
<td align="right"><span class="math inline">\(2.074 + (0.2)(2.074)\)</span></td>
<td align="right">2.488</td>
<td align="right">2.718</td>
<td align="right">0.230</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.488}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.230}\)</span></td>
</tr>
</tbody>
</table>

<p>Here is a naive implementation of <strong>Eulerâs method</strong> in R code using a more general <strong>IVP method function</strong> (see Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:ivp">4.14</a>):</p>

<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb164-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>( x) }</a>
<a class="sourceLine" id="cb164-2" data-line-number="2">f_forward &lt;-<span class="st"> </span><span class="cf">function</span>(tk, yk) { yk } </a>
<a class="sourceLine" id="cb164-3" data-line-number="3">f_backward &lt;-<span class="st"> </span><span class="cf">function</span>(delta_tk, yk) { yk <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>delta_tk )} </a>
<a class="sourceLine" id="cb164-4" data-line-number="4">ivp_method &lt;-<span class="st"> </span><span class="cf">function</span>(init_val, fun, iteration, target, </a>
<a class="sourceLine" id="cb164-5" data-line-number="5">                       <span class="dt">type=</span><span class="st">&quot;rungekutta&quot;</span>) {</a>
<a class="sourceLine" id="cb164-6" data-line-number="6">   x0 =<span class="st"> </span>init_val[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb164-7" data-line-number="7">   y0 =<span class="st"> </span>yk =<span class="st"> </span>init_val[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb164-8" data-line-number="8">   k =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, target, <span class="dt">length.out=</span>iteration )</a>
<a class="sourceLine" id="cb164-9" data-line-number="9">   stepsize =<span class="st"> </span>k[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>k[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb164-10" data-line-number="10">   sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb164-11" data-line-number="11">   cnt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb164-12" data-line-number="12">   <span class="cf">for</span> (tk <span class="cf">in</span> k) {</a>
<a class="sourceLine" id="cb164-13" data-line-number="13">       <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;explicit&quot;</span>) {  <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb164-14" data-line-number="14">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">f_forward</span>(tk, yk)</a>
<a class="sourceLine" id="cb164-15" data-line-number="15">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;implicit&quot;</span>) { <span class="co"># euler backward</span></a>
<a class="sourceLine" id="cb164-16" data-line-number="16">           yk =<span class="st">  </span><span class="kw">f_backward</span>( stepsize, yk)</a>
<a class="sourceLine" id="cb164-17" data-line-number="17">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;heun&quot;</span>) {</a>
<a class="sourceLine" id="cb164-18" data-line-number="18">           k1 =<span class="st">  </span><span class="kw">f_forward</span>(tk, yk) </a>
<a class="sourceLine" id="cb164-19" data-line-number="19">           k2 =<span class="st">  </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k1 )</a>
<a class="sourceLine" id="cb164-20" data-line-number="20">           yk =<span class="st"> </span>heun =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(k1 <span class="op">+</span><span class="st"> </span>k2)</a>
<a class="sourceLine" id="cb164-21" data-line-number="21">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;rungekutta&quot;</span>) {</a>
<a class="sourceLine" id="cb164-22" data-line-number="22">           k1 =<span class="st"> </span><span class="kw">f_forward</span>(tk, yk)  </a>
<a class="sourceLine" id="cb164-23" data-line-number="23">           k2 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1 )  </a>
<a class="sourceLine" id="cb164-24" data-line-number="24">           k3 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2 )  </a>
<a class="sourceLine" id="cb164-25" data-line-number="25">           k4 =<span class="st"> </span><span class="kw">f_forward</span>(tk <span class="op">+</span><span class="st"> </span>stepsize, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3)  </a>
<a class="sourceLine" id="cb164-26" data-line-number="26">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>( k1 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k3 <span class="op">+</span><span class="st"> </span>k4)</a>
<a class="sourceLine" id="cb164-27" data-line-number="27">       }</a>
<a class="sourceLine" id="cb164-28" data-line-number="28">       true_y =<span class="st"> </span><span class="kw">exp</span>(tk )  </a>
<a class="sourceLine" id="cb164-29" data-line-number="29">       err =<span class="st"> </span><span class="kw">abs</span>( true_y <span class="op">-</span><span class="st"> </span>y0 )</a>
<a class="sourceLine" id="cb164-30" data-line-number="30">       cnt =<span class="st"> </span>cnt <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb164-31" data-line-number="31">       sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(cnt, stepsize, tk,  y0, </a>
<a class="sourceLine" id="cb164-32" data-line-number="32">                                    true_y, err))</a>
<a class="sourceLine" id="cb164-33" data-line-number="33">       y0 =<span class="st"> </span>yk</a>
<a class="sourceLine" id="cb164-34" data-line-number="34">   }</a>
<a class="sourceLine" id="cb164-35" data-line-number="35">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;iteration&quot;</span>, <span class="st">&quot;stepsize&quot;</span>, <span class="st">&quot;tk&quot;</span>,</a>
<a class="sourceLine" id="cb164-36" data-line-number="36">                          <span class="st">&quot;y-approx&quot;</span>, <span class="st">&quot;y-exact&quot;</span>, <span class="st">&quot;absolute error&quot;</span>)</a>
<a class="sourceLine" id="cb164-37" data-line-number="37">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=<span class="st"> </span>sequence, <span class="st">&quot;count&quot;</span>=<span class="kw">nrow</span>(sequence), <span class="st">&quot;type&quot;</span>=type )</a>
<a class="sourceLine" id="cb164-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb164-39" data-line-number="39">ivp_plot &lt;-<span class="st"> </span><span class="cf">function</span>(ivp, base_delta, target, <span class="dt">label=</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb164-40" data-line-number="40">  iteration =<span class="st"> </span>ivp<span class="op">$</span>Iteration</a>
<a class="sourceLine" id="cb164-41" data-line-number="41">  iteration =<span class="st"> </span>iteration[ <span class="kw">which</span>( <span class="kw">round</span>(iteration[,<span class="dv">3</span>],<span class="dv">3</span>) <span class="op">%in%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb164-42" data-line-number="42"><span class="st">                         </span><span class="kw">round</span>(<span class="kw">seq</span>(<span class="dv">0</span>, target, base_delta),<span class="dv">3</span>) ), ]</a>
<a class="sourceLine" id="cb164-43" data-line-number="43">  n =<span class="st"> </span><span class="kw">nrow</span>(iteration)</a>
<a class="sourceLine" id="cb164-44" data-line-number="44">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb164-45" data-line-number="45">     h =<span class="st"> </span>iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">3</span>] <span class="op">-</span><span class="st"> </span>iteration[k,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb164-46" data-line-number="46">     x1 =<span class="st"> </span>iteration[k,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb164-47" data-line-number="47">     x2 =<span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>h</a>
<a class="sourceLine" id="cb164-48" data-line-number="48">     y1_true =<span class="st"> </span><span class="kw">f_true</span>(x1)</a>
<a class="sourceLine" id="cb164-49" data-line-number="49">     y2_true =<span class="st"> </span><span class="kw">f_true</span>(x2)</a>
<a class="sourceLine" id="cb164-50" data-line-number="50">     y1 =<span class="st"> </span>iteration[k,<span class="dv">4</span>]</a>
<a class="sourceLine" id="cb164-51" data-line-number="51">     y2 =<span class="st"> </span>iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">4</span>] </a>
<a class="sourceLine" id="cb164-52" data-line-number="52">     <span class="cf">if</span> (k <span class="op">==</span><span class="st"> </span>n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb164-53" data-line-number="53">       a =<span class="st"> </span><span class="kw">round</span>( iteration[k<span class="op">+</span><span class="dv">1</span>,<span class="dv">4</span>],<span class="dv">5</span>)</a>
<a class="sourceLine" id="cb164-54" data-line-number="54">       stepsize =<span class="st"> </span>iteration[k, <span class="dv">2</span>]</a>
<a class="sourceLine" id="cb164-55" data-line-number="55">       <span class="kw">text</span>(<span class="fl">1.2</span>, y2, <span class="dt">cex=</span><span class="fl">0.7</span>, <span class="dt">label=</span><span class="kw">substitute</span>(</a>
<a class="sourceLine" id="cb164-56" data-line-number="56">           <span class="kw">paste</span>(Delta,<span class="st">&quot;t=&quot;</span>, stepsize, <span class="st">&quot;, y(1)=&quot;</span>, a, <span class="dt">sep=</span><span class="st">&quot;&quot;</span>))) </a>
<a class="sourceLine" id="cb164-57" data-line-number="57">     }</a>
<a class="sourceLine" id="cb164-58" data-line-number="58">     <span class="kw">lines</span>(<span class="kw">c</span>(x1,x2), <span class="kw">c</span>(y1,y2), <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb164-59" data-line-number="59">     <span class="kw">points</span>(<span class="kw">c</span>(x1, x2), <span class="kw">c</span>(y1, y2), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb164-60" data-line-number="60">     <span class="kw">points</span>(<span class="kw">c</span>(x1, x2), <span class="kw">c</span>(y1_true, y2_true), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb164-61" data-line-number="61">  }</a>
<a class="sourceLine" id="cb164-62" data-line-number="62">  <span class="cf">if</span> (ivp<span class="op">$</span>type <span class="op">==</span><span class="st"> &quot;explicit&quot;</span> <span class="op">&amp;&amp;</span><span class="st"> </span>label<span class="op">==</span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb164-63" data-line-number="63">    <span class="kw">text</span>(<span class="dv">1</span>,<span class="fl">2.3</span>, <span class="dt">cex=</span><span class="fl">0.7</span>, <span class="dt">label=</span><span class="st">&quot;tangent&quot;</span>)</a>
<a class="sourceLine" id="cb164-64" data-line-number="64">  }</a>
<a class="sourceLine" id="cb164-65" data-line-number="65">  <span class="cf">if</span> (label<span class="op">==</span><span class="ot">TRUE</span>) { iteration }</a>
<a class="sourceLine" id="cb164-66" data-line-number="66">}</a>
<a class="sourceLine" id="cb164-67" data-line-number="67">n =<span class="st"> </span><span class="dv">15</span></a>
<a class="sourceLine" id="cb164-68" data-line-number="68">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a></code></pre></div>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1      0.2 0.0  1.00000 1.000000     0.00000000
## [2,]         2      0.2 0.2  1.20000 1.221403     0.02140276
## [3,]         3      0.2 0.4  1.44000 1.491825     0.05182470
## [4,]         4      0.2 0.6  1.72800 1.822119     0.09411880
## [5,]         5      0.2 0.8  2.07360 2.225541     0.15194093
## [6,]         6      0.2 1.0  2.48832 2.718282     0.22996183</code></pre>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1     0.05 0.0 1.000000 1.000000    0.000000000
## [2,]         5     0.05 0.2 1.215506 1.221403    0.005896508
## [3,]         9     0.05 0.4 1.477455 1.491825    0.014369254
## [4,]        13     0.05 0.6 1.795856 1.822119    0.026262474
## [5,]        17     0.05 0.8 2.182875 2.225541    0.042666340
## [6,]        21     0.05 1.0 2.653298 2.718282    0.064984123</code></pre>
<pre><code>##      iteration stepsize  tk y-approx  y-exact absolute error
## [1,]         1    0.002 0.0 1.000000 1.000000   0.0000000000
## [2,]       101    0.002 0.2 1.221159 1.221403   0.0002439310
## [3,]       201    0.002 0.4 1.491229 1.491825   0.0005958164
## [4,]       301    0.002 0.6 1.821027 1.822119   0.0010914887
## [5,]       401    0.002 0.8 2.223764 2.225541   0.0017773523
## [6,]       501    0.002 1.0 2.715569 2.718282   0.0027133078</code></pre>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ivp"></span>
<img src="DS_files/figure-html/ivp-1.png" alt="IVP Methods" width="70%" />
<p class="caption">
Figure 4.14: IVP Methods
</p>
</div>

<p>Note that the dotted lines in the figure represent <strong>tangent lines</strong> for which the corresponding slope is based on the approximated point (<span class="math inline">\(\mathbf{t_{k+1}, y_{k+1}}\)</span>). We sample three iterations in the code towards an approximate value. The one with the smallest step size (0.002) results in better accuracy. It yields an approximate value of <span class="math inline">\(\mathbf{2.71557}\)</span> with an absolute error of <span class="math inline">\(\mathbf{0.0027}\)</span> compared to the other two. As the plot shows in Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:ivp">4.14</a>, we can observe that the smaller the step size becomes, the tangent line gets more aligned to the actual function curve. Also, we excluded all other nodes (or tangent lines) in the plot to show a fixed set of select but fixed nodes.</p>
<p>Also, recall that if we do not specify an <strong>initial value</strong>, we end up with an infinite number of solutions. Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:euler2">4.15</a> shows an example of multiple solutions. In the figure, we show three possible initial points, <span class="math inline">\(y(0) \in (0.5,1,5)\)</span>, arriving at three separate unique solutions.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:euler2"></span>
<img src="DS_files/figure-html/euler2-1.png" alt="Initial Value Problem - Euler Method" width="70%" />
<p class="caption">
Figure 4.15: Initial Value Problem - Euler Method
</p>
</div>

<p>Indeed, our goal in solving <strong>IVPs</strong> is to set a unique solution by having an <strong>initial value</strong> and a corresponding derivative of a function.</p>
</div>
<div id="eulers-method-implicit" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.2</span> Eulerâs Method (Implicit)<a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The previous <strong>Eulerâs method</strong> is an <strong>explicit method</strong> which uses conditions under the current state to advance to the next state. This method is called <strong>Forward Eulerâs method</strong> with the following general formula:</p>
<p><span class="math display" id="eq:equate1060117">\[\begin{align}
y_{k+1} = y_{k} + \Delta t\ y&#39;_{k} \ \ \ \leftarrow y_{k+1} = y_k + \Delta t\ f(t_{k}, y_{k})
\ \ \leftarrow\ \ \ \frac{y_{k+1} - y_{k-1}}{\Delta t} = f(t_{k}, y_{k}) \tag{4.119} 
\end{align}\]</span></p>
<p>which can also be derived from a truncated <strong>Taylor series</strong> expansion:</p>
<p><span class="math display" id="eq:equate1060118">\[\begin{align}
y_{k+1} \approx f(t_k + \Delta t) = f(t_k) + \frac{f&#39;(t_k)\Delta t^1}{1!} + O(\Delta t^2) \tag{4.120} 
\end{align}\]</span></p>
<p>Here, we explore <strong>Eulerâs method</strong> further by illustrating <strong>implicit method</strong> called <strong>Backward Eulerâs method</strong>. The method uses conditions of the next state, which may appear counter-intuitive, but the method becomes more apparent using the following general formula and an illustration.</p>
<p><span class="math display" id="eq:equate1060120" id="eq:equate1060119">\[\begin{align}
y_{k+1} &amp;= y_{k} + \Delta t y&#39;_{k+1}  \tag{4.121} \\
&amp;= y_k + \Delta t\ f(t_{k+1}, y_{k+1})
\ \ \leftarrow\ \ \ \frac{y_{k+1} - y_{k}}{\Delta t} = f(t_{k+1}, y_{k+1}) \tag{4.122} 
\end{align}\]</span></p>
<p>which can also be derived from a truncated <strong>Taylor series</strong> expansion:</p>
<p><span class="math display" id="eq:equate1060121">\[\begin{align}
y_{k} \approx f(t_{k+1} - \Delta t) = f(t_{k+1}) - \frac{f&#39;(t_{k+1})\Delta t^1}{1!} + O(\Delta t^2) \tag{4.123} 
\end{align}\]</span></p>
<p>The <strong>implicit method</strong> improves upon the stability that otherwise, in certain cases, is unattainable using the <strong>explicit method</strong>.</p>
<p>To illustrate, we use <strong>implicit method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Let us re-arrange our <strong>implicit method</strong> equation to adapt to our specific problem:</p>
<p><span class="math display" id="eq:equate1060126" id="eq:equate1060125" id="eq:equate1060124" id="eq:equate1060123" id="eq:equate1060122">\[\begin{align}
y_{k+1} {}&amp;= y_k + \Delta t\ f(t_{k+1}, y_{k+1})  \tag{4.124} \\
y_{k+1} &amp;= y_k + \Delta t\ y_{k+1}\ \ \ \ \leftarrow\ \ \ \ \ \ f(t_{k+1}, y_{k+1}) = y&#39;_{k+1} = y_{k+1}  \tag{4.125} \\
y_{k+1} - \Delta t\ y_{k+1} &amp;= y_k  \tag{4.126} \\
y_{k+1}( 1 - \Delta t) &amp;= y_k  \tag{4.127} \\
y_{k+1} &amp;= \frac{y_k}{(1 - \Delta t)} \tag{4.128} 
\end{align}\]</span></p>
<p>See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:eulertable2">4.8</a>.</p>
<table>
<caption><span id="tab:eulertable2">Table 4.8: </span>Backward Euler Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(y_k / (1 - \Delta t )\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right"><span class="math inline">\(1.000 / (0.8)\)</span></td>
<td align="right">1.250</td>
<td align="right">1.221</td>
<td align="right">0.029</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.250</td>
<td align="right"><span class="math inline">\(1.250 / (0.8)\)</span></td>
<td align="right">1.562</td>
<td align="right">1.492</td>
<td align="right">0.070</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.562</td>
<td align="right"><span class="math inline">\(1.562 / (0.8)\)</span></td>
<td align="right">1.953</td>
<td align="right">1.822</td>
<td align="right">0.131</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.953</td>
<td align="right"><span class="math inline">\(1.953 / (0.8)\)</span></td>
<td align="right">2.441</td>
<td align="right">2.226</td>
<td align="right">0.215</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.441</td>
<td align="right"><span class="math inline">\(2.441 / (0.8)\)</span></td>
<td align="right">3.052</td>
<td align="right">2.718</td>
<td align="right">0.334</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{3.052}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.334}\)</span></td>
</tr>
</tbody>
</table>
<p>As we can see, <strong>Backward Eulerâs method</strong> requires extra algebraic preparation to arrive at a new equation such as: <span class="math inline">\(y_{k+1} = y_k / (1 - \Delta t)\)</span>. Such preparation is only possible because we use a simple <strong>differential equation</strong>: <span class="math inline">\(y&#39; = y\)</span>. It could have been a lot more complicated. But even after simplifying the equation, <strong>Backward Eulerâs method</strong> does not always yield better accuracy than <strong>the explicit method</strong> though perhaps more stable.</p>
<p>As for applying a naive implementation of the <strong>implicit method</strong> in R code, just set the type to <strong>implicit</strong>.</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb168-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb168-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb168-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb168-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, <span class="dt">type=</span><span class="st">&quot;implicit&quot;</span>)</a>
<a class="sourceLine" id="cb168-5" data-line-number="5"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
</div>
<div id="heuns-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.3</span> Heunâs Method <a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Heunâs method</strong> is called an <strong>improved Eulerâs method</strong>. It is also a second-order <strong>implicit method</strong> that combines both the <strong>Eulerâs explicit and implicit</strong> equations to arrive at the following:</p>
<p><span class="math display" id="eq:equate1060127">\[\begin{align}
y_{k+1} = y_k + \frac{\Delta t}{2}\left( \mathbf{k_1} + \mathbf{k_2} \right) \tag{4.129} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{k_1} {}&amp;= f(t_{k}, y_{k}) \\
\mathbf{k_2} &amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \Delta t, y_k + \Delta t k_1)
\end{align*}\]</span></p>
<p>To illustrate, we use <strong>Heun method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Note that:</p>
<p><span class="math display" id="eq:equate1060128">\[\begin{align}
y_k&#39; = f(t_k, y_k) \tag{4.130} 
\end{align}\]</span></p>
<p><strong>First</strong>, we solve for <span class="math inline">\(\mathbf{k_1}\)</span>.</p>
<p><span class="math display" id="eq:equate1060129">\[\begin{align}
\mathbf{k_1} = f(t_{k}, y_{k}) = y&#39;_k = y_k  = 1  \tag{4.131} 
\end{align}\]</span></p>
<p><strong>Second</strong>, we solve for <span class="math inline">\(\mathbf{k_2}\)</span>.</p>
<p><span class="math display" id="eq:equate1060130">\[\begin{align}
\mathbf{k_2} {}&amp;= f(t_{k+1}, y_{k+1}) = f(t_k + \Delta t, y_k + \Delta t k_1)  \tag{4.132} \\
&amp;= f( 0.0 + 0.2, 1 + 0.2(1)) = f(0.2, 1.2) \nonumber \\
&amp;= 1.2\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k   \nonumber
\end{align}\]</span></p>
<p><strong>Third</strong>, we solve for <span class="math inline">\(\mathbf{y_{k+1}}\)</span>:</p>
<p><span class="math display" id="eq:equate1060131">\[\begin{align}
y_{k+1} {}&amp;= y_k + \frac{\Delta t}{2}\left( \mathbf{k_1} + \mathbf{k_2} \right)  \tag{4.133} \\
&amp;= 1 + \frac{0.2}{2}(1 + 1.2) \nonumber \\
&amp;= 1 + 0.1 (2.2) = 1.22 \nonumber
\end{align}\]</span></p>
<p><strong>Finally</strong>, we repeat the process until we hit the target, substituting <span class="math inline">\(\mathbf{y_{k}}\)</span> with <span class="math inline">\(\mathbf{y_{k+1}}\)</span>.</p>
<p>See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:heuntable">4.9</a>.</p>
<table>
<caption><span id="tab:heuntable">Table 4.9: </span>Heun Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(f(t_k, y_k)\)</span></th>
<th align="right"><span class="math inline">\(f(t_{k+1}, y_{k+1})\)</span></th>
<th align="right">approx:<span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.000</td>
<td align="right">1.220</td>
<td align="right">1.220</td>
<td align="right">1.221</td>
<td align="right">0.001</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.220</td>
<td align="right">1.220</td>
<td align="right">1.464</td>
<td align="right">1.488</td>
<td align="right">1.492</td>
<td align="right">0.003</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.488</td>
<td align="right">1.488</td>
<td align="right">1.786</td>
<td align="right">1.816</td>
<td align="right">1.822</td>
<td align="right">0.006</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.816</td>
<td align="right">1.816</td>
<td align="right">2.179</td>
<td align="right">2.215</td>
<td align="right">2.223</td>
<td align="right">0.010</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.215</td>
<td align="right">2.215</td>
<td align="right">2.658</td>
<td align="right">2.702</td>
<td align="right">2.718</td>
<td align="right">0.015</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.702}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{0.015}\)</span></td>
</tr>
</tbody>
</table>
<p>As for applying a naive implementation of the <strong>Heunâs</strong> method in R code, just set type to <strong>heun</strong>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb169-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb169-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb169-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb169-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, <span class="dt">type=</span><span class="st">&quot;heun&quot;</span>)</a>
<a class="sourceLine" id="cb169-5" data-line-number="5"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
</div>
<div id="runge-kutta-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.4</span> Runge-Kutta Method <a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The <strong>Runge-Kutta method</strong> is an <strong>implicit method</strong> and comes with different formulas of N-order <span class="citation">(Heath M.T. <a href="bibliography.html#ref-ref187m">2002</a>; Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>. Below are two examples of the <strong>Runge-Kutta</strong> method of the 3rd-order and 4th-order.</p>
<p><span class="math display">\[
\begin{array}{llll}
\begin{array}{l}
3rd-Order \\
========\\
\mathbf{k_1} =  f(t_k, y_k)   \\
\mathbf{k_2} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_1} \right)  \\ 
\mathbf{k_3} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + 2 \mathbf{k_2} - \mathbf{k_1} \right) \\
\\
\\
\mathbf{y_{k+1}} = y_k + \frac{\Delta t}{6}\ ( \mathbf{k_1} + 4\mathbf{k_2} + \mathbf{k_3} )
\end{array} &amp; &amp; &amp;
\begin{array}{l}
4th-Order \\
========\\
\mathbf{k_1} =  f(t_k, y_k)   \\
\mathbf{k_2} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_1} \right)  \\ 
\mathbf{k_3} =  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t}{2} \mathbf{k_2} \right) \\
\mathbf{k_4} =  f(t_k + \Delta t,\ y_k + \Delta t  \mathbf{k_3})\\
\\
\mathbf{y_{k+1}} = y_k + \frac{\Delta t}{6}\ ( \mathbf{k_1} + 2\mathbf{k_2} + 2\mathbf{k_3} + \mathbf{k_4})
\end{array}
\end{array}
\]</span></p>
<p>In fact, <strong>Heunâs</strong> method is a 2nd-order <strong>Runge-Kutta</strong> method:</p>
<p><span class="math display" id="eq:equate1060132">\[\begin{align}
y_{k+1} = y_k + \frac{\Delta t}{2}(\mathbf{k_1} + \mathbf{k_2}) \tag{4.134} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display" id="eq:equate1060134" id="eq:equate1060133">\[\begin{align}
\mathbf{k_1} {}&amp;= f(t_{k}, y_{k})   \tag{4.135} \\
\mathbf{k_2} &amp;=  f\left(t_k + \Delta t,\ y_k + \Delta t \mathbf{k_1} \right) \tag{4.136} 
\end{align}\]</span></p>
<p>We leave the readers to investigate other orders such as 5th-order and 6th-order.</p>
<p>Here, we illustrate <strong>Runge-Kutta</strong> method using the 4th-order. Note that, with the 4th order, the parameters <span class="math inline">\(\mathbf{k_1, k_2, k_3, k_4}\)</span> are all <strong>slopes</strong> of tangent line to an estimated curve function. Those four slopes are averaged to arrive at an optimal slope, e.g. <span class="math inline">\(y_{k+1} = y&#39;_1\)</span>. See Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:rungekutta">4.16</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rungekutta"></span>
<img src="rungekutta.png" alt="Runge-Kutta Method" width="70%" />
<p class="caption">
Figure 4.16: Runge-Kutta Method
</p>
</div>
<p>To illustrate, we use <strong>Runge-Kutta method</strong> to approximate a target point (<span class="math inline">\(\mathbf{x_1, y_1}\)</span>) given the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_1(1) = &lt;unknown\ target&gt;\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39; = y, 
\]</span>
and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p><strong>First</strong>, solve for <span class="math inline">\(\mathbf{k_1}\)</span>:</p>
<p><span class="math display" id="eq:equate1060135">\[\begin{align}
k_1 =  f(t_k, y_k) =  y_k&#39; = y_k = 1 \tag{4.137} 
\end{align}\]</span></p>
<p>Notice <span class="math inline">\(\mathbf{k_1}\)</span> is similar to <strong>forward Eulerâs method</strong></p>
<p><strong>Second</strong>, solve for <span class="math inline">\(\mathbf{k_2}\)</span>:</p>
<p><span class="math display" id="eq:equate1060136">\[\begin{align}
k_2 {}&amp;=  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t k_1}{2} \right)   \tag{4.138} \\
&amp;= f \left(0.0 + \frac{0.2}{2},\ 1.0 + \frac{0.2 (1)}{2} \right) \nonumber \\
&amp;= f \left(0.1,\ 1.1 \right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k \nonumber \\
&amp;=1.1 \nonumber
\end{align}\]</span></p>
<p><strong>Third</strong>, solve for <span class="math inline">\(\mathbf{k_3}\)</span>:</p>
<p><span class="math display" id="eq:equate1060137">\[\begin{align}
k_3 {}&amp;=  f\left(t_k + \frac{\Delta t}{2},\ y_k + \frac{\Delta t k_2}{2}\right)  \tag{4.139} \\
&amp;=  f\left(0.0 + 0.1,\ 1.0 + 0.1(1.1)\right) \nonumber \\
&amp;= f\left(0.1, 1.11\right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k  \nonumber \\
&amp;=1.11 \nonumber
\end{align}\]</span></p>
<p><strong>Fourth</strong>, solve for <span class="math inline">\(\mathbf{k_4}\)</span>:</p>
<p><span class="math display" id="eq:equate1060138">\[\begin{align}
k_4 {}&amp;=  f\left(t_k + \Delta t,\ y_k + \Delta t  k_3\right)  \tag{4.140} \\
&amp;=  f\left(0.0 + 0.2,\ 1.0 + 0.2 (1.11)\right) \nonumber \\
&amp;= f\left(0.2, 1.222\right)\ \ \ \leftarrow\ \ \ \ f(t_k, y_k) = y_k  \nonumber \\
&amp;= 1.222 \nonumber
\end{align}\]</span></p>
<p><strong>Fifth</strong>, solve for <span class="math inline">\(\mathbf{y_{k+1}}\)</span>:</p>
<p><span class="math display" id="eq:equate1060139">\[\begin{align}
y_{k+1} {}&amp;= y_k + \frac{\Delta t}{6}\ ( k_1 + 2k_2 + 2k_3 + k_4)  \tag{4.141} \\
&amp;= 1.0 + \frac{0.2}{6}\ ( 1.0 + 2(1.1) + 2(1.11) + 1.222) \nonumber \\ 
&amp;= 1.0 + \frac{0.2}{6}\ (6.642) \nonumber \\
&amp;= 1.2214 \nonumber
\end{align}\]</span></p>
<p><strong>Finally</strong>, repeat the process until we hit the target, substituting <span class="math inline">\(\mathbf{y_{k}}\)</span> with <span class="math inline">\(\mathbf{y_{k+1}}\)</span>. See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:rungetable">4.10</a>. In the table, <span class="math inline">\(\mathbf{y_{k+1}}\)</span> represents the approximate solution and <span class="math inline">\(\mathbf{e^{t+1}}\)</span> represents the exact target.</p>
<table>
<caption><span id="tab:rungetable">Table 4.10: </span>Runge-Kutta Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(k_1\)</span></th>
<th align="right"><span class="math inline">\(k_2\)</span></th>
<th align="right"><span class="math inline">\(k_=3\)</span></th>
<th align="right"><span class="math inline">\(k_4\)</span></th>
<th align="right"><span class="math inline">\(\ y_{k+1}\)</span></th>
<th align="right"><span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.000</td>
<td align="right">1.100</td>
<td align="right">1.110</td>
<td align="right">1.222</td>
<td align="right">1.221</td>
<td align="right">1.221</td>
<td align="right">2.8e-6</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.221</td>
<td align="right">1.221</td>
<td align="right">1.344</td>
<td align="right">1.356</td>
<td align="right">1.493</td>
<td align="right">1.492</td>
<td align="right">1.492</td>
<td align="right">6.7e-6</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.492</td>
<td align="right">1.492</td>
<td align="right">1.641</td>
<td align="right">1.660</td>
<td align="right">1.823</td>
<td align="right">1.822</td>
<td align="right">1.822</td>
<td align="right">1.2e-5</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.822</td>
<td align="right">1.822</td>
<td align="right">2.004</td>
<td align="right">2.023</td>
<td align="right">2.227</td>
<td align="right">2.226</td>
<td align="right">2.226</td>
<td align="right">2.0e-5</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.226</td>
<td align="right">2.226</td>
<td align="right">2.448</td>
<td align="right">2.470</td>
<td align="right">2.720</td>
<td align="right">2.718</td>
<td align="right">2.718</td>
<td align="right">3.1e-5</td>
</tr>
<tr class="even">
<td align="right"><span class="math inline">\(\mathbf{1.0}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right">â</td>
<td align="right"><span class="math inline">\(\mathbf{2.718}\)</span></td>
<td align="right"><span class="math inline">\(\mathbf{3.1e-5}\)</span></td>
</tr>
</tbody>
</table>
<p>As for applying a naive implementation of the <strong>Runge-Kutta method</strong> in R code, just set the type to <strong>rungekutta</strong>.</p>
<div class="sourceCode" id="cb170"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb170-1" data-line-number="1"><span class="co"># invoke ivp_methods</span></a>
<a class="sourceLine" id="cb170-2" data-line-number="2">target =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb170-3" data-line-number="3">base_delta =<span class="st"> </span><span class="fl">0.2</span></a>
<a class="sourceLine" id="cb170-4" data-line-number="4">ivp =<span class="st"> </span><span class="kw">ivp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb170-5" data-line-number="5">                 <span class="dt">type=</span><span class="st">&quot;rungekutta&quot;</span>)</a>
<a class="sourceLine" id="cb170-6" data-line-number="6"><span class="kw">ivp_plot</span>(ivp, base_delta, target,  <span class="dt">label=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<p>Before we move on to <strong>Boundary Value Problems</strong>, we leave readers to investigate two enhancements to Runge-Kutta methods that use <strong>Adaptive Stepsize</strong>, which can yield errors in the order of <span class="math inline">\(O(h^9)\)</span> or even <span class="math inline">\(O(h^{10})\)</span>.</p>
<ul>
<li>Dormand-Prince Method (1980)</li>
<li>Bogacki-Shampine Method (1989)</li>
</ul>
<p>We also leave readers to investigate <strong>Multi-Step Methods</strong> such as:</p>
<ul>
<li>Adams-Bashforth Method</li>
<li>Adams-Moulton Method</li>
</ul>
</div>
<div id="shooting-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.5</span> Shooting Method <a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We have methods that deal with <strong>Initial Value Problems (IVP)</strong> in the previous sections. This section covers the discussion around <strong>Boundary Value Problems (BVP)</strong>. Here, we shall see that we can reduce <strong>BVP</strong> into <strong>IVP</strong> to simplify the process. As a start, we use the <strong>Shooting</strong> method. To explain the method, we use Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:shootingmethod">4.17</a>.     </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:shootingmethod"></span>
<img src="shootingmethod.png" alt="Shooting Method" width="70%" />
<p class="caption">
Figure 4.17: Shooting Method
</p>
</div>
<p>As observed in the figure, our target is represented as a slope given by <span class="math inline">\(y&#39;(b) = \beta\)</span> at a horizontal distance <span class="math inline">\(b\)</span>. We first try to aim at the target by adjusting our angle in the form of a slope given by <span class="math inline">\(y&#39;(a) = \alpha_1\)</span>. This first attempt to <strong>shoot an arrow</strong> forms a trajectory that eventually lands at <span class="math inline">\(b\)</span> with an angle represented as a slope given by <span class="math inline">\(y&#39;(b) = \sigma_1\)</span>. Similarly, we make a second attempt, <strong>shooting</strong> at an angle, <span class="math inline">\(y&#39;(a) = \alpha_2\)</span>, and hits a slope, <span class="math inline">\(y&#39;(b) = \sigma_2\)</span>. It does show that we miss the target twice.</p>
<p>Ideally, if we shoot multiple attempts, we can use <strong>interpolation</strong> using the approximate targets to find one unique solution closer to the actual target. We explain that later in this section.</p>
<p>Now, when it comes to boundaries, notice that there seems to be a boundary condition in place for both the starting point and ending point ( a Dirichlet type of boundary condition):</p>
<p><span class="math display">\[
a &lt; x &lt; b 
\]</span></p>
<p>There are a few common boundary conditions we can use depending on the situation. See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:bvpboundary">4.11</a>.</p>

<table>
<caption><span id="tab:bvpboundary">Table 4.11: </span>Shooting Method Boundaries</caption>
<thead>
<tr class="header">
<th align="left">Boundary Conditions</th>
<th align="left">Starting Point (a)</th>
<th align="left">Ending Point (b)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Neumann</td>
<td align="left"><span class="math inline">\(y&#39;(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(b) = \beta\)</span></td>
</tr>
<tr class="even">
<td align="left">Dirichlet</td>
<td align="left"><span class="math inline">\(y(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y(b) = \beta\)</span></td>
</tr>
<tr class="odd">
<td align="left">Cauchy</td>
<td align="left"><span class="math inline">\(y(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(a) = \beta\)</span></td>
</tr>
<tr class="even">
<td align="left">Robin</td>
<td align="left"><span class="math inline">\(c_1 y(a) + c_2 y&#39;(a) = \alpha\)</span></td>
<td align="left"><span class="math inline">\(c_1 y(b) + c_2 y&#39;(b) = \beta\)</span></td>
</tr>
<tr class="odd">
<td align="left">Periodic</td>
<td align="left"><span class="math inline">\(y(a) = y(b)\)</span></td>
<td align="left"><span class="math inline">\(y&#39;(b) = y&#39;(b)\)</span></td>
</tr>
</tbody>
</table>

<p>For our illustration, we use the <strong>Dirichlet</strong> boundary condition against a nonlinear second-order <strong>BVP</strong> equation:</p>
<p><span class="math display" id="eq:equate1060140">\[\begin{align}
y&#39;&#39; = f(t, y, y&#39;)\ \ \ \ \ \ \ \ \ where\ \ \ \ a \leq t \leq b \tag{4.142} 
\end{align}\]</span></p>
<p>with boundary conditions,</p>
<p><span class="math display" id="eq:equate1060141">\[\begin{align}
y(a) = \alpha,\ \ \ \ \ \ \ \ \ y(b) = \beta  \tag{4.143} 
\end{align}\]</span></p>
<p>One way to solve for two-point <strong>BVP</strong> is to resort back to any <strong>IVP</strong> methods; in particular, we arbitrarily choose the simple first-order <strong>forward Euler method</strong>. We know that <strong>IVP</strong> is satisfied with the following conditions (which is a <strong>Cauchy</strong> type of boundary condition):</p>
<p><span class="math display">\[
y(a) = \alpha,\ \ \ \ \ \ y&#39;(a) = \sigma,\ \ \ \ where\ \sigma =\ &lt;guess&gt;
\]</span></p>
<p>To use <strong>IVP</strong> method, we perform some mathematical manipulation against the following equation:</p>
<p><span class="math display" id="eq:equate1060142">\[\begin{align}
 y&#39;&#39;(t) = \frac{d^2y}{dt^2}\ \ \ \rightarrow (y&#39;(t))&#39; = \frac{d}{dt}\left(\frac{dy}{dt}\right) \tag{4.144} 
\end{align}\]</span></p>
<p>by extracting such first-order derivative from the second-order derivative of the below <strong>Leibnitz</strong> notation:</p>
<p><span class="math display" id="eq:equate1060143">\[\begin{align}
\frac{d^2y}{dt^2} = \frac{d}{dt}\left(\frac{dy}{dt}\right) \ \ \ \rightarrow\ \ \ \ (1)\ \ \ \
\frac{dy}{dt} = x = f(x,y,t),\ \ \ \ (2)\ \ \ \ \frac{dx}{dt} = g(x,y,t) \tag{4.145} 
\end{align}\]</span></p>
<p>and then use <strong>Eulerâs forward method</strong> formula (e.g. <span class="math inline">\(y_{k+1} = y_k + \Delta t\ f\)</span>):</p>
<p><span class="math display" id="eq:equate1060145" id="eq:equate1060144">\[\begin{align}
(1)\ \ \ \
\frac{dy}{dt} &amp;= f(x,y,t)\ \ \rightarrow y_{k+1} = y_k + f(x_k, y_k, t_k) \Delta t  \tag{4.146} \\
(2)\ \ \ \ 
\frac{dx}{dt} &amp;= g(x,y,t)\ \ \rightarrow x_{k+1} = x_k + g(x_k, y_k, t_k) \Delta t   \tag{4.147} 
\end{align}\]</span></p>
<p>So in general, solving <strong>BVP</strong> using <strong>shooting method</strong> given:</p>
<p><span class="math display" id="eq:equate1060146">\[\begin{align}
\frac{d^2y}{dt^2} = \frac{dx}{dt} =g(x, y, t) \ \ \ \ \rightarrow\ \ \ \ where\ \ \ y(a) = \alpha,\ \ \ y(b) = \beta \tag{4.148} 
\end{align}\]</span></p>
<p>can be solved using <strong>IVP</strong> following the below equation and condition:</p>
<p><span class="math display" id="eq:equate1060147">\[\begin{align}
\frac{dy}{dt} = f(x,y,t) \ \ \ \ \rightarrow\ \ \ \ where\ \ y(a) = \alpha,\ \ \ y&#39;(a) = \sigma \tag{4.149} 
\end{align}\]</span></p>
<p>Our goal then is to guess for:</p>
<p><span class="math display" id="eq:equate1060148">\[\begin{align}
\frac{dy}{dt}(0) = y&#39;(0) = x(0) = &lt;unknown&gt; \tag{4.150} 
\end{align}\]</span></p>
<p>To illustrate, suppose we have the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span></p>
<p>and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p><strong>First</strong>, let us aim for our <strong>target</strong>. We know that <strong>aiming</strong> for the <strong>target</strong> can be a <strong>miss</strong> or a <strong>hit</strong>. We assume that our first attempt can be a <strong>miss</strong> (a close one but still a miss).</p>
<p>Our approximate target (guess) is:</p>
<p><span class="math display">\[
y_0&#39;(0) = x_0 = 1.10
\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{y_1}\)</span>:</p>
<p><span class="math display" id="eq:equate1060150" id="eq:equate1060149">\[\begin{align}
y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \Delta t  \tag{4.151} \\
y_1 &amp;= y_0 + f(x_0, y_0, t_0) \Delta t  \tag{4.152} \\
&amp;= 1 + f(1.100, 1.00, 0.00) 0.2 \nonumber \\
&amp;= 1 + 1.100 * 0.2  \ \ \leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \nonumber  \\
&amp;= 1.220 \nonumber 
\end{align}\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{x_1}\)</span>:</p>
<p><span class="math display" id="eq:equate1060152" id="eq:equate1060151">\[\begin{align}
x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \Delta t  \tag{4.153} \\
x_1 &amp;= x_0 + g(x_0, y_0, t_0) \Delta t  \tag{4.154} \\
&amp;= 1.100 + g(1.100, 1.00, 0.00) 0.2 \nonumber \\
&amp;= 1.100 + 1.00 * 0.2  \ \ \leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \nonumber \\
&amp;= 1.300 \nonumber 
\end{align}\]</span></p>
<p><strong>Second</strong>, let us iterate one more time:</p>
<p>Solve for <span class="math inline">\(\mathbf{y_2}\)</span>:</p>
<p><span class="math display" id="eq:equate1060154" id="eq:equate1060153">\[\begin{align}
y_{k+1} {}&amp;= y_k + f(x_k, y_k, t_k) \Delta t  \tag{4.155} \\
y_2 &amp;= y_1 + f(x_1, y_1, t_1) \Delta t  \tag{4.156} \\
&amp;= 1.220 + f(1.300, 1.220, 0.20) 0.2 \nonumber \\
&amp;= 1.220 + 1.300 * 0.2  \ \ \leftarrow f(x_k, y_k, t_k) = y_k&#39; = x_k \nonumber  \\
&amp;= 1.480 \nonumber 
\end{align}\]</span></p>
<p>Solve for <span class="math inline">\(\mathbf{x_2}\)</span>:</p>
<p><span class="math display" id="eq:equate1060156" id="eq:equate1060155">\[\begin{align}
x_{k+1} {}&amp;= x_k + g(x_k, y_k, t_k) \Delta t  \tag{4.157} \\
x_2 &amp;= x_1 + g(x_1, y_1, t_1) \Delta t  \tag{4.158} \\
&amp;= 1.300 + g(1.300, 1.220, 0.20) 0.2 \nonumber \\
&amp;= 1.300 + 1.220 * 0.2  \ \ \leftarrow g(x_k, y_k, t_k) = y_k&#39;&#39; = y_k \nonumber \\
&amp;= 1.544 \nonumber 
\end{align}\]</span></p>
<p>We repeat a few times until we reach <span class="math inline">\(y_k(1)\)</span>.</p>
<p>See Table <a href="4.4-approximation-using-ordinary-differential-equations.html#tab:shooting">4.12</a>.</p>
<table>
<caption><span id="tab:shooting">Table 4.12: </span>Shooting Method Table</caption>
<thead>
<tr class="header">
<th align="right">t</th>
<th align="right"><span class="math inline">\(y_k\)</span></th>
<th align="right"><span class="math inline">\(x_1\)</span></th>
<th align="right">exact:<span class="math inline">\(\ e^{t+1}\)</span></th>
<th align="right">Err</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0</td>
<td align="right">1.000</td>
<td align="right">1.100</td>
<td align="right">1.100</td>
<td align="right">0.000</td>
</tr>
<tr class="even">
<td align="right">0.2</td>
<td align="right">1.220</td>
<td align="right">1.300</td>
<td align="right">1.221</td>
<td align="right">0.001</td>
</tr>
<tr class="odd">
<td align="right">0.4</td>
<td align="right">1.480</td>
<td align="right">1.544</td>
<td align="right">1.492</td>
<td align="right">0.012</td>
</tr>
<tr class="even">
<td align="right">0.6</td>
<td align="right">1.789</td>
<td align="right">1.840</td>
<td align="right">1.822</td>
<td align="right">0.033</td>
</tr>
<tr class="odd">
<td align="right">0.8</td>
<td align="right">2.157</td>
<td align="right">2.198</td>
<td align="right">2.226</td>
<td align="right">0.069</td>
</tr>
<tr class="even">
<td align="right">1.0</td>
<td align="right">2.596</td>
<td align="right">2.630</td>
<td align="right">2.718</td>
<td align="right">0.122</td>
</tr>
</tbody>
</table>
<p>Here is a naive implementation of <strong>Shooting method</strong> in R code using a more general <strong>BVP method function</strong> and also, this time, using the <strong>Runge-Kutta</strong> method for <strong>IVP</strong>:</p>

<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb171-1" data-line-number="1">f_true &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>( x) }</a>
<a class="sourceLine" id="cb171-2" data-line-number="2">f_forward &lt;-<span class="st"> </span><span class="cf">function</span>(xk, yk, tk) { xk }</a>
<a class="sourceLine" id="cb171-3" data-line-number="3">g_forward &lt;-<span class="st"> </span><span class="cf">function</span>(xk, yk, tk) { yk }</a>
<a class="sourceLine" id="cb171-4" data-line-number="4">f_rungekutta &lt;-<span class="st"> </span><span class="cf">function</span>(f, xk, yk, tk, stepsize) {</a>
<a class="sourceLine" id="cb171-5" data-line-number="5">    k1 =<span class="st"> </span><span class="kw">f</span>(xk, yk, tk)  </a>
<a class="sourceLine" id="cb171-6" data-line-number="6">    k2 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k1, </a>
<a class="sourceLine" id="cb171-7" data-line-number="7">           tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> )  </a>
<a class="sourceLine" id="cb171-8" data-line-number="8">    k3 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k2, </a>
<a class="sourceLine" id="cb171-9" data-line-number="9">           tk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb171-10" data-line-number="10">    k4 =<span class="st"> </span><span class="kw">f</span>(xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3, yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span>k3, tk <span class="op">+</span><span class="st"> </span>stepsize)  </a>
<a class="sourceLine" id="cb171-11" data-line-number="11">    yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize<span class="op">/</span><span class="dv">6</span> <span class="op">*</span><span class="st"> </span>( k1 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k2 <span class="op">+</span><span class="st"> </span><span class="dv">2</span><span class="op">*</span>k3 <span class="op">+</span><span class="st"> </span>k4)</a>
<a class="sourceLine" id="cb171-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb171-13" data-line-number="13">bvp_method &lt;-<span class="st"> </span><span class="cf">function</span>(init_val, fun, iteration, target, </a>
<a class="sourceLine" id="cb171-14" data-line-number="14">                       <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>) {</a>
<a class="sourceLine" id="cb171-15" data-line-number="15">   t0 =<span class="st"> </span>init_val[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb171-16" data-line-number="16">   y0 =<span class="st"> </span>yk =<span class="st"> </span>init_val[<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb171-17" data-line-number="17">   x0 =<span class="st"> </span>xk =<span class="st"> </span>guess =<span class="st"> </span>init_val[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb171-18" data-line-number="18">   k =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, target, <span class="dt">length.out=</span>iteration )</a>
<a class="sourceLine" id="cb171-19" data-line-number="19">   stepsize =<span class="st"> </span>k[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span>k[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb171-20" data-line-number="20">   sequence =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">0</span>, <span class="dv">7</span>)</a>
<a class="sourceLine" id="cb171-21" data-line-number="21">   cnt =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb171-22" data-line-number="22">   <span class="cf">for</span> (tk <span class="cf">in</span> k) {</a>
<a class="sourceLine" id="cb171-23" data-line-number="23">       <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;shooting:euler&quot;</span>) {</a>
<a class="sourceLine" id="cb171-24" data-line-number="24">           yk =<span class="st"> </span>yk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">f_forward</span>(x0, y0, tk) <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb171-25" data-line-number="25">           xk =<span class="st"> </span>xk <span class="op">+</span><span class="st"> </span>stepsize <span class="op">*</span><span class="st"> </span><span class="kw">g_forward</span>(x0, y0, tk) <span class="co"># euler forward</span></a>
<a class="sourceLine" id="cb171-26" data-line-number="26">       } <span class="cf">else</span> <span class="cf">if</span> (type <span class="op">==</span><span class="st"> &quot;shooting:rkutta&quot;</span>) {</a>
<a class="sourceLine" id="cb171-27" data-line-number="27">           yk =<span class="st"> </span><span class="kw">f_rungekutta</span>(f_forward, x0, y0, tk, stepsize)  </a>
<a class="sourceLine" id="cb171-28" data-line-number="28">           xk =<span class="st"> </span><span class="kw">f_rungekutta</span>(g_forward, x0, y0, tk, stepsize)</a>
<a class="sourceLine" id="cb171-29" data-line-number="29">       }  </a>
<a class="sourceLine" id="cb171-30" data-line-number="30">       true_y =<span class="st"> </span><span class="kw">exp</span>(tk )  </a>
<a class="sourceLine" id="cb171-31" data-line-number="31">       err =<span class="st"> </span><span class="kw">abs</span>( true_y <span class="op">-</span><span class="st"> </span>y0 )</a>
<a class="sourceLine" id="cb171-32" data-line-number="32">       cnt =<span class="st"> </span>cnt <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb171-33" data-line-number="33">       sequence =<span class="st"> </span><span class="kw">rbind</span>(sequence, <span class="kw">c</span>(cnt, stepsize, tk,  y0, x0, </a>
<a class="sourceLine" id="cb171-34" data-line-number="34">                                    true_y, err))</a>
<a class="sourceLine" id="cb171-35" data-line-number="35">       y0 =<span class="st"> </span>yk</a>
<a class="sourceLine" id="cb171-36" data-line-number="36">       x0 =<span class="st"> </span>xk</a>
<a class="sourceLine" id="cb171-37" data-line-number="37">   }</a>
<a class="sourceLine" id="cb171-38" data-line-number="38">   <span class="kw">colnames</span>(sequence) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;iteration&quot;</span>, <span class="st">&quot;stepsize&quot;</span>, <span class="st">&quot;tk&quot;</span>,</a>
<a class="sourceLine" id="cb171-39" data-line-number="39">                          <span class="st">&quot;yk&quot;</span>, <span class="st">&quot;xk&quot;</span>, <span class="st">&quot;y-exact&quot;</span>, <span class="st">&quot;absolute error&quot;</span>)</a>
<a class="sourceLine" id="cb171-40" data-line-number="40">   <span class="kw">list</span>(<span class="st">&quot;Iteration&quot;</span>=<span class="st"> </span>sequence, <span class="st">&quot;count&quot;</span>=<span class="kw">nrow</span>(sequence), </a>
<a class="sourceLine" id="cb171-41" data-line-number="41">        <span class="st">&quot;type&quot;</span>=type, <span class="st">&quot;guess&quot;</span>=guess )</a>
<a class="sourceLine" id="cb171-42" data-line-number="42">}</a>
<a class="sourceLine" id="cb171-43" data-line-number="43">bvp =<span class="st"> </span><span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.1</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb171-44" data-line-number="44">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>)</a>
<a class="sourceLine" id="cb171-45" data-line-number="45">bvp</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.100000 1.000000     0.00000000
## [2,]         2      0.2 0.2 1.243540 1.221400 1.221403     0.02213724
## [3,]         3      0.2 0.4 1.513958 1.518860 1.491825     0.02213326
## [4,]         4      0.2 0.6 1.850234 1.849148 1.822119     0.02811471
## [5,]         5      0.2 0.8 2.259635 2.259875 2.225541     0.03409400
## [6,]         6      0.2 1.0 2.759971 2.759918 2.718282     0.04168948
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1.1</code></pre>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb173-1" data-line-number="1">bvp =<span class="st"> </span><span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.3</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb173-2" data-line-number="2">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>)</a>
<a class="sourceLine" id="cb173-3" data-line-number="3">bvp</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.300000 1.000000     0.00000000
## [2,]         2      0.2 0.2 1.287820 1.221400 1.221403     0.06641724
## [3,]         3      0.2 0.4 1.558238 1.572943 1.491825     0.06641326
## [4,]         4      0.2 0.6 1.906488 1.903232 1.822119     0.08436882
## [5,]         5      0.2 0.8 2.327863 2.328584 2.225541     0.10232222
## [6,]         6      0.2 1.0 2.843412 2.843252 2.718282     0.12512981
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1.3</code></pre>

<p>Instead of going for a third shot, let us perform <strong>linear interpolation</strong> using the results of the two attempts of <strong>shooting</strong> for the target (where superscripts indicate 1st and 2nd attempts) :</p>
<p><span class="math display" id="eq:equate1060157">\[\begin{align}
y_0&#39;(0) &amp;= x_0^{(1)} +  \frac { x_0^{(2)} - x_0^{(1)}}{y_0^{(2)} - y_0^{(1)}}( y - y_0^{(1)})  \tag{4.159} \\
y_0&#39;(0) &amp;= 1.10 + \frac{1.30 - 1.10}{2.843412 - 2.759971} (2.718282 - 2.759971) \nonumber \\
y_0&#39;(0) &amp;= 1.00 \nonumber
\end{align}\]</span></p>
<p>where <span class="math inline">\(y = f(1) = 2.718282\)</span> which is our exact solution.</p>
<p>We now go back and attempt shooting another arrow given a guess of <span class="math inline">\(y&#39;(0) = 1.00\)</span>. We use the interpolation result to become our next 3rd attempt to guess the target.</p>
<p><span class="math display">\[
y_0&#39;(0) = x_0^{(3)} = 1.00
\]</span></p>

<div class="sourceCode" id="cb175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb175-1" data-line-number="1">(<span class="dt">bvp =</span> <span class="kw">bvp_method</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="fl">1.0</span>), <span class="dt">iteration=</span><span class="dv">6</span>, <span class="dt">target =</span> target, </a>
<a class="sourceLine" id="cb175-2" data-line-number="2">                 <span class="dt">type=</span><span class="st">&quot;shooting:rkutta&quot;</span>))</a></code></pre></div>
<pre><code>## $Iteration
##      iteration stepsize  tk       yk       xk  y-exact absolute error
## [1,]         1      0.2 0.0 1.000000 1.000000 1.000000   0.000000e+00
## [2,]         2      0.2 0.2 1.221400 1.221400 1.221403   2.758160e-06
## [3,]         3      0.2 0.4 1.491818 1.491818 1.491825   6.737641e-06
## [4,]         4      0.2 0.6 1.822106 1.822106 1.822119   1.234405e-05
## [5,]         5      0.2 0.8 2.225521 2.225521 2.225541   2.010271e-05
## [6,]         6      0.2 1.0 2.718251 2.718251 2.718282   3.069185e-05
## 
## $count
## [1] 6
## 
## $type
## [1] &quot;shooting:rkutta&quot;
## 
## $guess
## [1] 1</code></pre>

<p>The solution we get out of our <strong>interpolated</strong> guess, <span class="math inline">\(x_0^{(3)} = 1.0\)</span>, ends up with the following:</p>
<p><span class="math display">\[
y_0 = 1.000,\ \ \ \
y_1 = 1.221,\ \ \ \
y_2 = 1.492,\ \ \ \
y_3 = 1.822,\ \ \ \
y_4 = 2.226,\ \ \ \
y_5 = 2.718
\]</span></p>
<p>If we graph the trajectory, we get the Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:bvptrajectory">4.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bvptrajectory"></span>
<img src="embed0035.png" alt="BVP Trajectory For ODE" width="70%" />
<p class="caption">
Figure 4.18: BVP Trajectory For ODE
</p>
</div>
<p>Note that the unique solution we seek is a set of <span class="math inline">\(\mathbf{y_k} = f(x_k)\)</span> with their corresponding <strong>slopes</strong>, <span class="math inline">\(\mathbf{x_k}\)</span>, forming a trajectory that lands close to the target, <span class="math inline">\(y(1) = 2.718\)</span>. The next method we cover, <strong>Finite Difference</strong>, illustrates how we solve for Systems of Linear Equations to derive the solution for our BVP.</p>
</div>
<div id="finite-difference-method" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.6</span> Finite Difference Method  <a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we solve <strong>BVPs</strong> using <strong>Finite Difference</strong>. Recall <strong>second-order Centered Finite Difference</strong> formula:</p>
<p><span class="math display" id="eq:equate1060158">\[\begin{align}
f&#39;&#39;(x)\ \approx \frac{f(x + h) - 2f(x) + f(x -h) }{h^2}\ \ \ \rightarrow\ \ \ \
y&#39;&#39; \approx \frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2} \tag{4.160} 
\end{align}\]</span></p>
<p>Recall also <strong>first-order Forward Finite Difference</strong> formula:</p>
<p><span class="math display" id="eq:equate1060159">\[\begin{align}
f&#39;(x) \approx \frac{f(x+h) - f(x)}{h} \ \ \ \rightarrow\ \ \ \
y&#39; \approx \frac{ y_{k+1}  -y_k}{\Delta t} \tag{4.161} 
\end{align}\]</span></p>
<p>To illustrate the use of those formulas, suppose we have the following:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_0(1) = 2.718, \ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span>
and where our exact solution is <span class="math inline">\(\mathbf{y=e^t}\)</span>.</p>
<p>Here, because <span class="math inline">\(y&#39;&#39; = f&#39;&#39;(x)\)</span>, we simply substitute:</p>
<p><span class="math display" id="eq:equate1060160">\[\begin{align}
y&#39;&#39; = y\ \ \ \rightarrow\ \ \ \ \ \frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2} = y_k \tag{4.162} 
\end{align}\]</span></p>
<p>Note that we are not substituting <span class="math inline">\(y&#39;\)</span> for anything because our problem statement does not include the first derivative (e.g., <span class="math inline">\(y&#39;&#39; = y\)</span> ).</p>
<p>Now, let us simplify:</p>

<p><span class="math display" id="eq:equate1060165" id="eq:equate1060164" id="eq:equate1060163" id="eq:equate1060162" id="eq:equate1060161">\[\begin{align}
\frac{ y_{k+1} -2y_k + y_{k-1}}{(\Delta t)^2} {}&amp;= y_k \tag{4.163} \\
\nonumber \\
y_{k+1} - 2y_k + y_{k-1} &amp;= y_k(\Delta t)^2  \tag{4.164} \\
y_{k+1} - 2y_k - y_k(\Delta t)^2  + y_{k-1} &amp;= 0 \tag{4.165} \\
y_{k+1} - (2 + (\Delta t)^2)y_k  + y_{k-1} &amp;= 0 \tag{4.166} \\
y_{k-1} - (2 + (\Delta t)^2)y_k  + y_{k+1} &amp;= 0 \tag{4.167} 
\end{align}\]</span>
</p>
<p>We now have an equation we can use to solve our <strong>BVP</strong> (note that this is a discretization of the <strong>domain</strong> of our general function, <span class="math inline">\(\mathbf{f(x)}\)</span>):</p>
<p><span class="math display" id="eq:equate1060166">\[\begin{align}
y_{k-1} - (2 + (\Delta t)^2)y_k  + y_{k+1} = 0 \tag{4.168} 
\end{align}\]</span></p>
<p>To be able to use the equation, let us review Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:finitediffode">4.19</a> which illustrates our problem statement; where our <strong>Dirichlet</strong> boundary is <span class="math inline">\(t \in [a,b]\)</span>. Notice that our general function, <span class="math inline">\(f(x)\)</span>, is discretized into nodes of individual functions, <span class="math inline">\(f(x) \in \{f(0), f(0.2), ...,f(0.8), f(1)\}\)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:finitediffode"></span>
<img src="embed0036.png" alt="Discretized Nodes For ODE" width="70%" />
<p class="caption">
Figure 4.19: Discretized Nodes For ODE
</p>
</div>
<p>In the figure, we see six nodes of functions that we now can use to form our system of equations:</p>

<p><span class="math display" id="eq:equate1060172" id="eq:equate1060171" id="eq:equate1060170" id="eq:equate1060169" id="eq:equate1060168" id="eq:equate1060167">\[\begin{align}
f(0.0)\ \ \ &amp;\rightarrow\ \ \ \ y_0 = 1.0\ \ \leftarrow boundary\ a \tag{4.169} \\ 
f(0.2)\ \ \ &amp;\rightarrow\ \ \ \ y_0 - (2 + (\Delta t)^2)y_1  + y_2 = 0  \tag{4.170} \\ 
f(0.4)\ \ \ &amp;\rightarrow\ \ \ \ y_1 - (2 + (\Delta t)^2)y_2  + y_3 = 0  \tag{4.171} \\ 
f(0.6)\ \ \ &amp;\rightarrow\ \ \ \ y_2 - (2 + (\Delta t)^2)y_3  + y_4 = 0  \tag{4.172} \\ 
f(0.8)\ \ \ &amp;\rightarrow\ \ \ \ y_3 - (2 + (\Delta t)^2)y_4  + y_5 = 0  \tag{4.173} \\ 
f(1.0)\ \ \ &amp;\rightarrow\ \ \ \ y_5 = 2.718\ \ \leftarrow boundary\ b  \tag{4.174} 
\end{align}\]</span>
</p>
<p>and then to translate into <strong>tridiagonal</strong> matrix form where:</p>
<p><span class="math display">\[
-(2 + (\Delta t)^2) = -2.04.
\]</span>
We have:</p>

<p><span class="math display">\[
\left[
\begin{array}{rrrrrr}
1 &amp; . &amp; . &amp; . &amp; . &amp; . \\
1 &amp; -2.04 &amp; 1 &amp; . &amp; . &amp; . \\
. &amp; 1 &amp; -2.04  &amp; 1 &amp; . &amp; . \\
. &amp; . &amp; 1 &amp; -2.04  &amp; 1 &amp; . \\
. &amp; . &amp; . &amp; 1 &amp; -2.04  &amp; 1  \\
. &amp; . &amp; . &amp; . &amp; . &amp; 1 \\
\end{array}
\right]
\left[\begin{array}{r}y_0 \\ y_1 \\ y_2 \\ y_3 \\ y_4 \\ y_5  \end{array}\right] =
\left[\begin{array}{r}1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 2.718  \end{array}\right]
\]</span>
</p>
<p>We can then use any numerical methods (e.g., Newton, Broyden, etc.) discussed in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) to solve for Systems of Linear Equations, forming a domain with the following set of <strong>y</strong> values.</p>
<p><span class="math display">\[
y_0 = 1.000,\ \ \ \
y_1 = 1.222,\ \ \ \
y_2 = 1.492,\ \ \ \
y_3 = 1.823,\ \ \ \
y_4 = 2.226,\ \ \ \
y_5 = 2.718
\]</span>
And if we graph the trajectory of the approximate solution, it will show a geometric match for the <strong>Shooting method</strong> shown in Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:bvptrajectory">4.18</a>. Note that the figure shows the trajectory of our exact solution itself, e.g. <span class="math inline">\(\mathbf{y = e^x}\)</span>.</p>
</div>
<div id="finite-element-method-based-on-wrm-and-vm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.7</span> Finite Element Method (based on WRM and VM) <a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Finite Element</strong> method has been used across many fields, from mechanical and aerospace engineering to material science. It is also used in structural (solid) mechanics, fluid (liquid and gas) mechanics, quantum mechanics, and other fields of physics. Computer simulations assist in <strong>Finite Element analysis</strong> to understand such dynamic systems <span class="citation">(Burden R.L. et al. <a href="bibliography.html#ref-ref196r">2005</a>)</span>. We leave readers to investigate a few simulation software starting with Ansys, OpenFOAM, and SimScale.</p>
<p>In structural engineering, construction materials are gauged based on strain when stress is applied or displacement when force is applied. Similar to <strong>piecewise</strong> polynomial interpolation, a well-known approach to determine the overall state of the entire material is to first partition the material into pieces (or elements) and compute the local state of each element; hence, finite element. And then aggregate the individual (local) states to form the overall (global) state. There are two common ways to partition the material: rectangular or triangular. See Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:finiteelement">4.20</a>. Note that the figure shows the most rudimentary way of generating the mesh. <strong>Mesh generation</strong> can be done precisely by computers today using graphics software following <strong>Delaunay Refinement algorithm</strong> for <strong>Triangular Mesh</strong> and adjusting vertices following <strong>Ruppertâs algorithm</strong>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:finiteelement"></span>
<img src="finiteelement.png" alt="Finite Element Mesh" width="70%" />
<p class="caption">
Figure 4.20: Finite Element Mesh
</p>
</div>
<p>In this section, instead of discussing complex structures involving constructing a complex mesh of elements, let us first get the intuition of FEM using a simple one-dimension two-point <strong>BVP</strong>. Then, we use a <strong>stencil</strong> to show that. Discussion about complex structures is left to readers pursuing structural or mechanical engineering or material science.</p>
<p>Moreover, in other literature, <strong>FEM</strong> offers two approaches, namely <strong>Weighted Residual Method (WRM)</strong> and <strong>Variational Method (VM)</strong>. In this section, we only focus on <strong>WRM</strong>.</p>
<p>Similar to <strong>Finite Difference Methods (FDM)</strong>, to solve <strong>BVPs</strong> using <strong>Finite Element methods (FEM)</strong>, we discretize the <strong>domain</strong> of the function into equally-spaced intervals forming a set of discretized nodes - each node representing a function. The difference, however, is that in <strong>FEM</strong>, we build a linear combination (as an approximate solution) constructed from a standard series or a polynomial of choice associated with a set of discretized nodes. These nodes are considered <strong>basis functions</strong>. These <strong>basis functions</strong> denoted as <span class="math inline">\(\phi(t)\)</span> form an approximation to the solution. The <strong>approximate solution</strong> is a <strong>weighted solution</strong> using the following choice of polynomials or series for its <strong>basis functions</strong>: </p>
<ul>
<li>Polynomial Interpolation (e.g.Â Lagrange, Newton, B-spline, Legendre, Chebyshev, â¦)</li>
<li>Fourier Series (e.g.Â Sinusoidal, â¦)</li>
<li>Wavelets (e.g.Â Harmonic, â¦)</li>
</ul>
<p>In here, we use the generalized <strong>Taylor series</strong> for our trial function, <span class="math inline">\(y(x)\)</span>. </p>
<p><span class="math display" id="eq:equate1060173">\[\begin{align}
y(x) = \sum_{n=0}^{\infty} c_n x^n =  c_0 + c_1x + c_2 x^2 + c_3 x^3 +\ . . . \ \ \ \ where\ c_n = \frac{f(n)(a)}{n!} \tag{4.175} 
\end{align}\]</span></p>
<p>One may prefer to use <strong>Sinusoidal series</strong> instead (depending on study or use):</p>
<p><span class="math display" id="eq:equate1060174">\[\begin{align}
y(x) = \sum_{n=1}^{\infty} c_n sin\frac{n\pi x}{L} = c_1 sin \frac{\pi x}{L} + c_2 sin \frac{2\pi x}{L} + c_3 sin \frac{3\pi x}{L} +\ . . . \tag{4.176} 
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(c_i\)</span> are the <strong>unknown coefficients</strong> that we need to compute. From physical connotation, the number of unknown coefficients is based on the characteristic of the material. In other words, the number of properties of each element determines the order of the polynomial we use. For example, we may account for stress, strain, and displacement in solid dynamics. In this case, we may need a <strong>cubic polynomial</strong> for each element of the elastic material. And in fluid dynamics, we may account for viscosity, temperature, density, pressure, and velocity.</p>
<p>As for simple illustrations, we narrow down our case to a one-dimension two-point <strong>BVP</strong> covering a few <strong>Weighted Residual</strong> approaches under <strong>Finite Element</strong> methods and using the usual problem statement below:</p>
<p><span class="math display">\[
y_0(0) = 1,\ \ \ \ y_0(1) = 2.718, \ \ \ \ where \ \ \ \ \ \Delta t = 0.2,\ \ \ \ y&#39;&#39; =  y,
\]</span></p>
<p>and just as the same, we use the below exact solution as a baseline to validate our choice of trial function - this is our <strong>governing equation</strong>:</p>
<p><span class="math display">\[
y = e^t\ \ \ \ \ \leftarrow \text{exact function as our baseline}
\]</span>
Note that <strong>governing equations</strong> are used for the more practical cases for <strong>Finite Element analysis</strong>. One <strong>governing equation</strong> is the <strong>Navier-Stokes equation</strong> for <strong>Fluid Dynamics</strong>. We base this equation on the law of conservation (e.g., mass, momentum, energy) around the physical properties of fluids such as viscosity, density, pressure, temperature, and velocity. </p>
<p>In our case, we use the most simple <strong>governing equation</strong> to illustrate the fundamental idea of the <strong>Finite Element method</strong> and may not necessarily reflect any physical phenomenon other than to show our approximate geometric representation of the actual equation:</p>
<p><span class="math display">\[
y = e^t
\]</span></p>
<p>Our first step is to generate a set of <strong>trial functions</strong> using the Taylor series as our choice. Given the <strong>Dirichlet</strong> boundary condition above, it shows that we have six nodes based on <span class="math inline">\(\Delta t = 0.2\)</span>, which is just our way of illustrating the partition of a <strong>global</strong> domain (the geometric curve) into five sub-domains (or elements). Here is the <strong>stencil</strong> for that which is shown in Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:trialfunction">4.21</a>: </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:trialfunction"></span>
<img src="trialfunction.png" alt="Stencil for Trial Function" width="70%" />
<p class="caption">
Figure 4.21: Stencil for Trial Function
</p>
</div>
<p>For our <strong>trial function</strong>, we use a simple <strong>cubic polynomial</strong> - a truncated taylor series - for each element:</p>
<p><span class="math display" id="eq:equate1060175">\[\begin{align}
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3 \tag{4.177} 
\end{align}\]</span></p>
<p>One may choose other <strong>polynomials</strong> as <strong>basis (trial) function</strong> as long as it satisfies three conditions:</p>
<ul>
<li>it satisfies the essential boundary conditions</li>
<li>it is continuous</li>
<li>and as such, if it is continuous, it is differentiable in which the square of its derivative is integrable and bounded.</li>
</ul>
<p>One point to make is that the <strong>basis function</strong> is regarded as a <strong>weighted function</strong>. In variational formulation, where we require to solve for the <strong>roots</strong> of the equation (the properties of the elements), we relax the third condition above, which means that we do not require a second-order derivation of continuity - which makes it, therefore, a <strong>weak formulation</strong>. Otherwise, the <strong>weighted function</strong> requires it if we do not reduce (meaning, we weaken the requirement of) the second-order of continuity (of the approximate solution) using integration (but not by parts) and thus is considered a <strong>strong formulation</strong>. </p>
<p>In the next few sections, we focus on <strong>Weighted Residual methods (WRM)</strong> to solve for <strong>Bounded Value Problems</strong>. We start with the <strong>Least-Square Method</strong>.</p>
</div>
<div id="least-square-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.8</span> Least-Square Method (using WRM)<a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us introduce a <strong>WRM</strong> under <strong>FEM</strong> called the <strong>Least-Square</strong> method. Foremost, note that <strong>WRM</strong> is an integral approach that integrates <strong>weighted residuals</strong> of <strong>trial functions</strong> to form an approximation function which is a weaker formulation of the actual function <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>.</p>
<p>To illustrate, let us form a system of equations starting with the <strong>basis function</strong>, which we derived from <strong>Taylor Series</strong> as pointed out previously:</p>
<p><span class="math display" id="eq:equate1060176">\[\begin{align}
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3 \tag{4.178} 
\end{align}\]</span></p>
<p>Our first goal is to find the values of the <strong>unknown coefficients</strong> that minimize <span class="math inline">\(y(x)\)</span>, which we require to fulfill our ultimate goal of formulating the approximation function that we can use to determine the values of each element in the <strong>domain</strong> - the curve.</p>
<p><strong>First</strong>, for the first boundary, <strong>a</strong>, where <span class="math inline">\(\mathbf{y(0) = 1}\)</span>, we compute for <span class="math inline">\(\mathbf{c_0}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2 + c_3 x^3 \\
y(0) &amp;= c_0 + c_1 (0) + c_2 (0)^2 + c_3 (0)^3 \\
y(0) &amp;= c_0  \\
\therefore c_0 &amp;= 1
\end{align*}\]</span></p>
<p><strong>Second</strong>, for the second boundary, <strong>b</strong>, where <span class="math inline">\(\mathbf{y(1) = 2.718}\)</span>, we compute for <span class="math inline">\(\mathbf{c_1}\)</span> (note that we also substitute <span class="math inline">\(c_0\)</span> in the equation):</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2  + c_3 x^3\\
y(1) &amp;= 1 + c_1 (1) + c_2 (1)^2 + c_3 (1)^3\\
y(1) &amp;= 1 + c_1  + c_2  + c_3 \\
c_1 &amp;= y(1) - 1 - c_2 -  c_3  \\
c_1 &amp;= 2.718 - 1 - c_2 - c_3   \\
\therefore c_1 &amp;= 1.718 - c_2 - c_3
\end{align*}\]</span></p>
<p><strong>Third</strong>, let us simplify our <strong>trial function</strong>:</p>
<p><span class="math display">\[\begin{align*}
y(x) {}&amp;=  c_0 + c_1x + c_2 x^2 + c_3 x^3 \\
y(x) &amp;= 1 + (1.718 - c_2 - c_3)x + c_2 x^2 + c_3 x^3  \\
y(x) &amp;= 1 + 1.718x - c_2x - c_3x + c_2 x^2 + c_3 x^3 \\
y(x) &amp;= 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) \\
\end{align*}\]</span></p>
<p><strong>Fourth</strong>, let us get the first-order and-second order of the simplified <strong>trial function</strong>:</p>
<p><span class="math display">\[\begin{align*}
y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\
y&#39;&#39;(x) &amp;= 2c_2 + 6c_3x  \\
\end{align*}\]</span></p>
<p><strong>Fifth</strong>, let us now plug our derivatives and trial functions into our problem statement to get the <strong>Residual</strong> equation:</p>
<p><span class="math display">\[\begin{align*}
y&#39;&#39; = y\ \ \ \rightarrow\ \ \ \ \ \ \ R = y&#39;&#39; - y {}&amp;= 0\\
\\
2c_2 + 6c_3x  - ( 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x)) &amp;= 0 \\
2c_2 + 6c_3x   - 1 - 1.718x - c_2(x^2 - x) - c_3(x^3 - x)  &amp;= 0 \\
2c_2 + 6c_3x   - 1 -  1.718x - c_2 x^2 + c_2 x  - c_3x^3 + c_3 x &amp;=  0 \\
c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1 &amp; = 0 \\
\\
\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1  &amp;= 0
\end{align*}\]</span></p>
<p><strong>Sixth</strong>, given the following integral form of our weighted residual, let us compute for other equations (this is the core of <strong>WRM</strong>):</p>
<p><span class="math display" id="eq:eqnnumber12">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = 2 + x - x^2 \\
W_2 = 6x - x^3 + x
\end{cases} \tag{4.179}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (2 + x - x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
4.7c_2+7.05c_3-4.0278 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (6x - x^3 + x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
7.050c_2+13.676 c_3-6.9150 = 0
\]</span></p>
<p>Based on the four equations we generated, we form our <strong>matrix</strong> of system of equations:</p>

<p><span class="math display">\[
\begin{array}{r}
c_0 = 1.0000\\
4.7c_2+7.05c_3 = 4.0278 \\
7.05c_2+13.676 c_3 = 6.9150 \\
c_1 + c_2 + c_3 = 1.7182
\end{array}\ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; . &amp; 4.700 &amp; 7.050 \\
. &amp; . &amp; 7.050 &amp; 13.676 \\
. &amp; 1 &amp; 1 &amp; 1 
\end{array}
\right]
\left[\begin{array}{r} c_0 \\ c_1 \\ c_2 \\ c_3 \end{array}\right] = 
\left[\begin{array}{r} 1.0000 \\ 4.0278 \\ 6.9150 \\ 1.7182 \end{array}\right]
\]</span>
</p>
<p>Therefore, our unknown coefficients are:</p>
<p><span class="math display">\[
c_0 = 1\ \ \ \ \ c_1 = 1.002031 \ \ \ \ \ c_2 = 0.4345505  \ \ \ \ \ c_3 = 0.2816188 
\]</span></p>
<p>Giving us the <strong>weak formulation</strong> of our <strong>approximate solution</strong> ( note that we chose <strong>cubic polynomial</strong>):</p>
<p><span class="math display">\[
y(x) = 1 + 1.002031 x + 0.4345505  x^2 + 0.2816188 x^3
\]</span></p>
<p>With that, we get a geometric match of the exact solution, <span class="math inline">\(y=e^x\)</span>, in Figure <a href="4.4-approximation-using-ordinary-differential-equations.html#fig:fem0">4.22</a>:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fem0"></span>
<img src="embed0037.png" alt="Finite Element Method (WRM)" width="70%" />
<p class="caption">
Figure 4.22: Finite Element Method (WRM)
</p>
</div>
<p>Also, in terms of error, we get the following reasonable result:</p>

<div class="sourceCode" id="cb177"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb177-1" data-line-number="1">f_exact &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>(x) }</a>
<a class="sourceLine" id="cb177-2" data-line-number="2">f_approx &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.002031</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.4345505</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb177-3" data-line-number="3"><span class="st">                           </span><span class="fl">0.2816188</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> }</a>
<a class="sourceLine" id="cb177-4" data-line-number="4">n =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb177-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb177-6" data-line-number="6">y_exact =<span class="st"> </span><span class="kw">f_exact</span>(x)</a>
<a class="sourceLine" id="cb177-7" data-line-number="7">y_approx =<span class="st"> </span><span class="kw">f_approx</span>(x)</a>
<a class="sourceLine" id="cb177-8" data-line-number="8">err =<span class="st"> </span>y_exact <span class="op">-</span><span class="st"> </span>y_approx</a>
<a class="sourceLine" id="cb177-9" data-line-number="9">m =<span class="st"> </span><span class="kw">cbind</span>( y_exact, <span class="kw">cbind</span>( y_approx, err))</a>
<a class="sourceLine" id="cb177-10" data-line-number="10"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Exact&quot;</span>, <span class="st">&quot;Approximate&quot;</span>, <span class="st">&quot;Error&quot;</span> )</a>
<a class="sourceLine" id="cb177-11" data-line-number="11">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Least-Square Method&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb177-12" data-line-number="12">              <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:femleastsqr">Table 4.13: </span>Least-Square Method</caption>
<thead>
<tr class="header">
<th align="right">Exact</th>
<th align="right">Approximate</th>
<th align="right">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.000000</td>
<td align="right">1.000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="right">1.221403</td>
<td align="right">1.220041</td>
<td align="right">0.0013616</td>
</tr>
<tr class="odd">
<td align="right">1.491825</td>
<td align="right">1.488364</td>
<td align="right">0.0034606</td>
</tr>
<tr class="even">
<td align="right">1.822119</td>
<td align="right">1.818486</td>
<td align="right">0.0036324</td>
</tr>
<tr class="odd">
<td align="right">2.225541</td>
<td align="right">2.223926</td>
<td align="right">0.0016150</td>
</tr>
<tr class="even">
<td align="right">2.718282</td>
<td align="right">2.718200</td>
<td align="right">0.0000815</td>
</tr>
</tbody>
</table>

</div>
<div id="galerkin-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.9</span> Galerkin Method (using WRM)<a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us introduce another <strong>WRM</strong> under <strong>FEM</strong> called the <strong>Galerkin</strong> method <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>. Similar to the <strong>Least Square</strong> method, we use the same <strong>trial function</strong> as below (note that we can use other polynomials instead):</p>
<p><span class="math display">\[
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\]</span></p>
<p>with its <strong>simplified form</strong> as illustrated in the <strong>Least-Square</strong> method:</p>
<p><span class="math display">\[
y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) 
\]</span></p>
<p>Now, the <strong>Weighing function</strong> is different in the <strong>Sixth</strong> step in the <strong>Least Square</strong> method for the <strong>Galerkin</strong> method. The difference this time is that we use the <strong>simplified form</strong> to extract our <strong>weighing functions</strong>:</p>
<p>So given the following integral form of our weighted residual, let us compute for other equations:</p>
<p><span class="math display">\[
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x^2 - x \\
W_2 = x^3 - x
\end{cases}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{R}\)</span> remains to be (the same as Least Squares method):</p>
<p><span class="math display" id="eq:eqnnumber13">\[\begin{align}
\therefore R = c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1  \tag{4.180}
\end{align}\]</span></p>
<p>So that, for <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^2 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.3098-0.3667c_2-0.5500c_3 = 0
\]</span></p>
<p>And for <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^3 - x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.4791-0.5500c_2-0.8762c_3 = 0
\]</span></p>
<p>Based on the four equations we generated, we form our <strong>matrix</strong> of system of equations:</p>

<p><span class="math display">\[
\begin{array}{r}
c_0 = 1.0000\\
0.3667c_2+0.5500c_3 = 0.3098 \\
0.5500c_2+0.8762 c_3 = 0.4791 \\
c_1 + c_2 + c_3 = 1.7182
\end{array}\ \ \ \rightarrow\ \ \ \
\left[
\begin{array}{rrrr}
1 &amp; . &amp; . &amp; . \\
. &amp; . &amp; 0.3667 &amp; 0.5500 \\
. &amp; . &amp; 0.5500 &amp; 0.8762 \\
. &amp; 1 &amp; 1 &amp; 1 
\end{array}
\right]
\left[\begin{array}{r} c_0 \\ c_1 \\ c_2 \\ c_3 \end{array}\right] = 
\left[\begin{array}{r} 1.0000 \\ 0.3098 \\ 0.4791 \\ 1.7182 \end{array}\right]
\]</span>
</p>
<p>Therefore, our unknown coefficients are:</p>
<p><span class="math display">\[
c_0 = 1\ \ \ \ \ c_1 = 1.014161 \ \ \ \ \ c_2 = 0.422377 \ \ \ \ \ c_3 = 0.2816625 
\]</span></p>
<p>and our complete approximate solution becomes:</p>
<p><span class="math display">\[
y(x) = 1 + 1.014161x + 0.422377 x^2 + 0.2816625 x^3
\]</span></p>
<p>Also, in terms of error, we get the following reasonable result:</p>
<div class="sourceCode" id="cb178"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb178-1" data-line-number="1">f_exact &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">exp</span>(x) }</a>
<a class="sourceLine" id="cb178-2" data-line-number="2">f_approx &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="fl">1.014161</span> <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span><span class="fl">0.422377</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span></a>
<a class="sourceLine" id="cb178-3" data-line-number="3"><span class="st">                           </span><span class="fl">0.2816625</span> <span class="op">*</span><span class="st"> </span>x<span class="op">^</span><span class="dv">3</span> }</a>
<a class="sourceLine" id="cb178-4" data-line-number="4">n =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb178-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb178-6" data-line-number="6">y_exact =<span class="st"> </span><span class="kw">f_exact</span>(x)</a>
<a class="sourceLine" id="cb178-7" data-line-number="7">y_approx =<span class="st"> </span><span class="kw">f_approx</span>(x)</a>
<a class="sourceLine" id="cb178-8" data-line-number="8">err =<span class="st"> </span>y_exact <span class="op">-</span><span class="st"> </span>y_approx</a>
<a class="sourceLine" id="cb178-9" data-line-number="9">m =<span class="st"> </span><span class="kw">cbind</span>( y_exact, <span class="kw">cbind</span>( y_approx, err))</a>
<a class="sourceLine" id="cb178-10" data-line-number="10"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Exact&quot;</span>, <span class="st">&quot;Approximate&quot;</span>, <span class="st">&quot;Error&quot;</span> )</a>
<a class="sourceLine" id="cb178-11" data-line-number="11">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Galerkin Method&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb178-12" data-line-number="12">              <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:femgalerkin">Table 4.14: </span>Galerkin Method</caption>
<thead>
<tr class="header">
<th align="right">Exact</th>
<th align="right">Approximate</th>
<th align="right">Error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1.000000</td>
<td align="right">1.000000</td>
<td align="right">0.0000000</td>
</tr>
<tr class="even">
<td align="right">1.221403</td>
<td align="right">1.221981</td>
<td align="right">-0.0005778</td>
</tr>
<tr class="odd">
<td align="right">1.491825</td>
<td align="right">1.491271</td>
<td align="right">0.0005536</td>
</tr>
<tr class="even">
<td align="right">1.822119</td>
<td align="right">1.821391</td>
<td align="right">0.0007274</td>
</tr>
<tr class="odd">
<td align="right">2.225541</td>
<td align="right">2.225861</td>
<td align="right">-0.0003204</td>
</tr>
<tr class="even">
<td align="right">2.718282</td>
<td align="right">2.718201</td>
<td align="right">0.0000813</td>
</tr>
</tbody>
</table>
</div>
<div id="petrov-galerkin-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.10</span> Petrov-Galerkin Method (using WRM)<a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In the <strong>Petrov-Galerkin</strong> method, our <strong>Weighing functions</strong> are such that given the following integral form of our weighted residual, we just compute for the equations using <strong>x</strong> and raise the order until we meet the system of equations we require:</p>
<p><span class="math display" id="eq:eqnnumber14">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x \\
W_2 = x^2
\end{cases}\ or
\begin{cases}
W_1 = x^2 \\
W_2 = x^3
\end{cases} \tag{4.181}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
3.25c_2+6.4c_3-3.218 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{W_2}\)</span>:</p>
<p><span class="math display">\[
\int_0^1 (x^2) ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.15c_2+4.75c_3-2.2885 = 0
\]</span></p>
<p>Our complete approximate solution becomes:</p>
<p><span class="math display">\[
y(x) = 1 + 1.027872 x + 0.3093443 x^2 + 0.3809836 x^3
\]</span></p>
</div>
<div id="rayleigh-ritz-method-using-wrm" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.11</span> Rayleigh-Ritz Method (using WRM)<a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Finally, we introduce the <strong>Rayleigh-Ritz</strong> method using <strong>WRM under FEM</strong> <span class="citation">(Papadopoulos P. <a href="bibliography.html#ref-ref491p">2015</a>)</span>. Using the same approach as <strong>Galerkin</strong>, we use the same <strong>trial function</strong> as before:</p>
<p><span class="math display">\[
y(x) =  c_0 + c_1x + c_2 x^2 + c_3 x^3
\]</span></p>
<p>with its <strong>simplified form</strong> as illustrated in the <strong>Least Square</strong>:</p>
<p><span class="math display">\[
y(x) = 1 + 1.718x + c_2(x^2 - x) + c_3(x^3 - x) 
\]</span></p>
<p>and with its first-order derivative:</p>
<p><span class="math display">\[\begin{align*}
y&#39;(x) {}&amp;= 1.718 + c_2(2x - 1)+ c_3(3x^2 - 1) \\
\end{align*}\]</span></p>
<p>Now, similar to <strong>Galerkin</strong> method, we use the <strong>simplified form</strong> to extract our <strong>weighing functions</strong>:</p>

<p><span class="math display" id="eq:eqnnumber8">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
W_1 = x^2 - x,\ \ \  W&#39;_1 = 2x - 1 \\
W_2 = x^3 - x,\ \ \  W&#39;_2 = 3x^2 - 1 \\
\end{cases} \tag{4.182}
\end{align}\]</span>
</p>
<p>where this time, we use integration by parts starting with <span class="math inline">\(\mathbf{R}\)</span>:</p>

<p><span class="math display" id="eq:equate1060177">\[\begin{align}
R = y&#39;&#39; - y = 0\ \ \ \ \rightarrow\ \ \ \ \ R = \frac{d^2y}{dx^2} -  \frac{dy}{dx} = 0 \tag{4.183} 
\end{align}\]</span>
</p>
<p>So then, that brings our <strong>integral</strong> equation into the following form:</p>

<p><span class="math display" id="eq:equate1060178">\[\begin{align}
\int_{i=0}^{n} W_i R\ dx = 0
\ \ \ \ \ \rightarrow \ \ \ \ \ \ 
\int_{i=0}^{n} W_i \left(\frac{d^2y}{dx^2} -  \frac{dy}{dx}\right)\ dx = 0 \tag{4.184} 
\end{align}\]</span>
</p>
<p>And by integration by parts:</p>

<p><span class="math display" id="eq:equate1060179">\[\begin{align}
\int_{i=0}^{n} W_i \left(\frac{d^2y}{dx^2}\right) dx -  \int_{i=0}^{n} W_i \left(\frac{dy}{dx}\right)\ dx = 0 \tag{4.185} 
\end{align}\]</span>
</p>
<p>We continue solving for each part:</p>

<p><span class="math display" id="eq:equate1060180">\[\begin{align}
\left[w\left(\frac{dy}{dx}\right)\right]_0^1  - \int_{0}^{1}  \left(\frac{dw}{dx}\right)\left(\frac{dy}{dx}\right) dx -  \int_{0}^{1} W_i \left(\frac{dy}{dx}\right)\ dx = 0 \tag{4.186} 
\end{align}\]</span>
</p>
<p>giving us the final integral form:</p>

<p><span class="math display">\[
- \int_{0}^{1}  \left(\frac{dw}{dx}\right)\left(\frac{dy}{dx}\right) dx -  \int_{0}^{1} W_i \left(\frac{dy}{dx}\right)\ dx = 0, \ \ \ \ \ \ \ \ where\ \ \ \left[w\left(\frac{dy}{dx}\right)\right]_0^1 is\ cancelled
\]</span>
</p>
<p>So that, for <span class="math inline">\(\mathbf{W_1}\)</span> and <span class="math inline">\(\mathbf{W_1&#39;}\)</span>:</p>

<p><span class="math display">\[\begin{align*}
{}&amp;- \int_{0}^{1} (2x-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\
&amp;\ \ \ - \int_{0}^{1} (x^2-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0
\end{align*}\]</span>
</p>
<p>we get the following equation:</p>
<p><span class="math display">\[
-0.3333 c_2-0.5167 c_3+0.2863 = 0
\]</span></p>
<p>And for <span class="math inline">\(\mathbf{W_2}\)</span> and <span class="math inline">\(\mathbf{W_2&#39;}\)</span>:</p>
<p><span class="math display">\[\begin{align*}
{}&amp;- \int_{0}^{1} (3x^2-1)(1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx \\
&amp;\ \ \ - \int_{0}^{1} (x^3-x) (1.718 + c_2(2x - 1)+ c_3(3x^2 - 1)) dx = 0
\end{align*}\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
0.4295-0.48333 c_2-0.8c_3 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 0.835589 x + 0.8733474 x^2 + 0.0092640 x^3
\]</span></p>
</div>
<div id="subdomain-method-using-subdomains" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.12</span> Subdomain Method (using subdomains)<a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Subdomain Method</strong> does not require the <strong>Weighing function</strong>; however, it uses integration to subdivide the <strong>Residual</strong> equation (See Equation <a href="4.4-approximation-using-ordinary-differential-equations.html#eq:eqnnumber13">(4.180)</a>) into subdomains, generating the required equations <span class="citation">(Papadopoulos P. <a href="bibliography.html#ref-ref491p">2015</a>; Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>)</span>. One may use as many subdomains as there are to fit a domain; however, because we only need a number of equations sufficient enough to form a system of equations, we only need two more equations in our case.</p>
<p><span class="math display" id="eq:eqnnumber9">\[\begin{align}
\int_a^b  R\ dx = 0\ \ \ \ \ \ \ \ \ where\ \ \ \
\begin{cases}
S_1 \rightarrow a = 0.0, b = 0.5 \\
S_2 \rightarrow a = 0.5, b = 1.0
\end{cases} \tag{4.187}
\end{align}\]</span></p>
<p>For <span class="math inline">\(\mathbf{S_1}\)</span>:</p>
<p><span class="math display">\[
\int_0^{0.5}  ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
1.08334c_2+0.859375c_3-0.71475 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{S_2}\)</span>:</p>
<p><span class="math display">\[
\int_{0.5}^{1}  ( c_2 ( 2 - x^2 + x) + c_3 ( 6x - x^3 + x) - 1.718x - 1) dx = 0
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
1.08333c_2+2.390625c_3-1.14425 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 1.000445 x + 0.4372653 x^2 + 0.2804898 x^3
\]</span></p>
</div>
<div id="collocation-method-using-direct-location-points" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.13</span> Collocation Method (using direct location points) <a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Collocation Method</strong> does not require the <strong>Weighing function</strong> and the integration <span class="citation">(Salih A., <a href="bibliography.html#ref-ref483a">n.d.</a>; Mohammed A. S. et al. <a href="bibliography.html#ref-ref499a">2021</a>)</span>. Instead, we plug the location directly into the <strong>Residual equation</strong>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - x_i^2 + x_i) + c_3 ( 6x_i - x_i^3 + x_i) - 1.718x_i - 1  = 0\ \ \ \ \ where\ \ \
x \in \{ 0.2, 0.4, 0.6, 0.8 \}
\]</span></p>
<p>Note that we need only two equations to complete the <strong>system of equations</strong> for the matrix and so we can just randomly choose two from the set of x. Here, we choose <span class="math inline">\(x \in \{0.2, .06\}\)</span>:</p>
<p>For <span class="math inline">\(\mathbf{x_i = 0.2}\)</span>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - (0.2)^2 + (0.2)) + c_3 ( 6(0.2) - (0.2)^3 + (0.2)) - 1.718(0.2) - 1 = 0 
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.16c_2+1.392c_3-1.3436 = 0
\]</span></p>
<p>For <span class="math inline">\(\mathbf{x_i = 0.6}\)</span>:</p>
<p><span class="math display">\[
R = c_2 ( 2 - (0.6)^2 + (0.6)) + c_3 ( 6(0.6) - (0.6)^3 + (0.6)) - 1.718(0.6) - 1 = 0 
\]</span></p>
<p>we get the following equation:</p>
<p><span class="math display">\[
2.24c_2+3.984c_3-2.0308 = 0
\]</span></p>
<p>That gives us the following approximate solution:</p>
<p><span class="math display">\[
y(x) = 1 + 1.006949 x + 0.4603359 x^2 + 0.2509156 x^3
\]</span></p>
</div>
<div id="weighted-residual-summary" class="section level3 hasAnchor">
<h3><span class="header-section-number">4.4.14</span> Weighted Residual Summary <a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In summary, comparing the different <strong>Weighted Residual methods</strong> vs. <strong>Subdomain</strong> and <strong>Collocation</strong>, we get the following:</p>
<table>
<caption><span id="tab:femtable">Table 4.15: </span>Weighted Residual</caption>
<thead>
<tr class="header">
<th align="left">Method</th>
<th align="left"><span class="math inline">\(c_0\)</span></th>
<th align="left"><span class="math inline">\(c_1\)</span></th>
<th align="left"><span class="math inline">\(c_2\)</span></th>
<th align="left"><span class="math inline">\(c_3\)</span></th>
<th align="left">y</th>
<th align="left">abs error</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Least Square</td>
<td align="left">1</td>
<td align="left">1.002031</td>
<td align="left">0.4345505</td>
<td align="left">0.2816188</td>
<td align="left">2.7182</td>
<td align="left">3e-07</td>
</tr>
<tr class="even">
<td align="left">Galerkin</td>
<td align="left">1</td>
<td align="left">1.014161</td>
<td align="left">0.4223770</td>
<td align="left">0.2816625</td>
<td align="left">2.7182</td>
<td align="left">5e-07</td>
</tr>
<tr class="odd">
<td align="left">Petrov-Galerkin</td>
<td align="left">1</td>
<td align="left">1.027872</td>
<td align="left">0.3093443</td>
<td align="left">0.3809836</td>
<td align="left">2.7182</td>
<td align="left">1e-07</td>
</tr>
<tr class="even">
<td align="left">Rayleigh-Ritz</td>
<td align="left">1</td>
<td align="left">0.835589</td>
<td align="left">0.8733474</td>
<td align="left">0.0092640</td>
<td align="left">2.7182</td>
<td align="left">4e-07</td>
</tr>
<tr class="odd">
<td align="left">Subdomain</td>
<td align="left">1</td>
<td align="left">1.000445</td>
<td align="left">0.4372653</td>
<td align="left">0.2804898</td>
<td align="left">2.7182</td>
<td align="left">1e-07</td>
</tr>
<tr class="even">
<td align="left">Collocation</td>
<td align="left">1</td>
<td align="left">1.006949</td>
<td align="left">0.4603359</td>
<td align="left">0.2509156</td>
<td align="left">2.7182</td>
<td align="left">5e-07</td>
</tr>
</tbody>
</table>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="4.3-approximation-by-numerical-differentiation.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="4.5-approximation-using-functional-differential-equations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
