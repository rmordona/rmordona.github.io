<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>13.5 Transformer Neural Network (TNN)  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="13.5 Transformer Neural Network (TNN)  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="13.5 Transformer Neural Network (TNN)  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is essential to have some introductory perspective of the mathematical foundations in which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-03-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="13.4-deep-stacked-bidirectional-rnn.html"/>
<link rel="next" href="13.6-applications-using-tnn-and-rnn.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat-and-disclaimer.html"><a href="caveat-and-disclaimer.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqns-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-poisson-equation"><i class="fa fa-check"></i><b>4.6.1</b> The Poisson Equation </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.4</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.6</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-learning-deep-rl.html"><a href="13.8-deep-reinforcement-learning-deep-rl.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Learning (Deep RL)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="transformer-neural-network-tnn" class="section level2 hasAnchor">
<h2><span class="header-section-number">13.5</span> Transformer Neural Network (TNN)  <a href="13.5-transformer-neural-network-tnn.html#transformer-neural-network-tnn" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The <strong>Attention</strong> mechanism was first introduced in the context of <strong>machine translation</strong> in a paper published by Dzmitry Bahdanau et al. <span class="citation">(<a href="bibliography.html#ref-ref1317d">2015</a>)</span>, followed by another paper published by Minh-Thang Luong et al. <span class="citation">(<a href="bibliography.html#ref-ref1328m">2015</a>)</span> for a variant of the mechanism. Two years later, presented in a paper called <strong>Attention is all you need</strong> <span class="citation">(Vaswani A. et al. <a href="bibliography.html#ref-ref1339a">2017</a>)</span>, <strong>Attention</strong> kick-started the emergence of <strong>Transformer</strong> architectures - also called <strong>Transformer Neural Network</strong> architectures.</p>
<div id="attention" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.1</span> Attention <a href="13.5-transformer-neural-network-tnn.html#attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To start this topic, let us first build our fundamental knowledge around <strong>Encoder-Decoder</strong> architecture, using formulas and referencing diagrams from Bahdanau and Luong (with some modifications for presentation purposes only). As we pointed out in the <strong>RNN</strong> sections, an <strong>Encoder-Decoder</strong> model constitutes two separate components: an <strong>Encoder</strong> and a <strong>Decoder</strong>. They correspond to the <strong>Many-to-One</strong> and <strong>One-to-Many</strong> models. The <strong>Encoder</strong> component can use an RNN architecture (using <strong>LSTM</strong> or <strong>GRU</strong> as an alternative and in a bidirectional fashion) with a series of <strong>X</strong> as input and a single <strong>Encoder Vector</strong> as output. The <strong>Decoder</strong> component takes the <strong>Encoder Vector</strong> for its initial state and produces a series of <strong>Y</strong> as output. See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:encoderdecoder">13.17</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:encoderdecoder"></span>
<img src="encoderdecoder.png" alt="Encoder-Decoder" width="100%" />
<p class="caption">
Figure 13.17: Encoder-Decoder
</p>
</div>
<p>Mathematically, our interest is to produce the final <strong>hidden state</strong>, e.g. <span class="math inline">\(\mathbf{h_t}\)</span>, from the encoder using the following function:</p>
<p><span class="math display" id="eq:equate1150078">\[\begin{align}
h_t = f(h_{t-1}, x_t)
\ \ \ \ \ \ \ \rightarrow\ \ \ \ \text{(recall RNN function)} \tag{13.80} 
\end{align}\]</span></p>
<p>The final <strong>hidden state</strong>, e.g., <span class="math inline">\(\mathbf{h_t}\)</span>, is then handed over to serve as the initial state <span class="math inline">\(\mathbf{s_0}\)</span> for the decoder. We use the following function for each decoded output:</p>
<p><span class="math display" id="eq:equate1150079">\[\begin{align}
y_t = g(s_{t}, y_{t-1}, c)\ \ \ \ \ \ \ \rightarrow\ \ \ \ \text{(also recall RNN function)} \tag{13.81} 
\end{align}\]</span></p>
<p>where <strong>c</strong> is the <strong>encoder vector</strong>, also called <strong>context vector</strong>. The symbol <span class="math inline">\(\mathbf{c}\)</span> should not be confused with the <strong>c</strong> state of <strong>LSTM</strong> in case we decide to use that variant of <strong>RNN</strong>.</p>
<p>One known drawback of such a traditional model lies in the fact that the <strong>Encoder</strong> produces only one single vector representing the entire sentence - sort of a single dot in <span class="math inline">\(\mathbf{d_k}\)</span> dimensional vector space <span class="citation">(Sankar A. <a href="bibliography.html#ref-ref1487a">2019</a>)</span>. One vector representing a context (or multiple contexts) of the entire sentence may not seem fitting, especially with a very long sentence. So we need a different approach. Take the following simple sentence as an example:</p>
<p><span class="math display">\[\begin{align*}
\mathbf{\text{I travel the world in search for the fountain of youth.}}
\end{align*}\]</span></p>
<p>So instead of feeding the decoder with a single vector representing the entire sentence, a proposed idea is to focus on individual target words, e.g., <strong>travel</strong>, and figure out which surrounding words are more relevant to the target word <strong>travel</strong>. For example, the words <strong>I</strong>, <strong>the</strong>, and <strong>world</strong> seem more strongly relevant than the rest. We can then form a vector that may signify some representation about <strong>I travel the world</strong> for the target word <strong>travel</strong>. Similarly, we can also form a vector representation of the target word <strong>fountain</strong> by looking for the closest relevant words such as <strong>in</strong>, <strong>search</strong>, <strong>for</strong>, <strong>the</strong>, and <strong>youth</strong>, then feed the decoder with another vector that signifies another kind of representation, this time, about <strong>search for the fountain of youth</strong>. Thus, herein lies the concept of the <strong>Attention</strong> mechanism, which is used to draw surrounding words that can <strong>pay more attention</strong> to <strong>target words</strong>. We call these surrounding words <strong>contextual words</strong>, which form <strong>contexts</strong> of <strong>target words</strong>.</p>
<p>To illustrate the use of the <strong>Attention</strong> mechanism, we have to modify our diagram in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:encoderdecoder">13.17</a>. The modification is shown in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:attentionencoder">13.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attentionencoder"></span>
<img src="attentionencoder.png" alt="Encoder-Decoder with Attention" width="100%" />
<p class="caption">
Figure 13.18: Encoder-Decoder with Attention
</p>
</div>
<p>Notice that an <strong>Attention unit</strong> is added to the diagram. The unit forms the core function of the <strong>Attention mechanism</strong>. Here, all the hidden states {<span class="math inline">\(\mathbf{h_1}, \mathbf{h_2}, ..., \mathbf{h_t}\)</span>} are captured by the <strong>Attention unit</strong> which are then used to eventually generate distinct <strong>contextualized</strong> vectors, each denoted by <span class="math inline">\(\mathbf{c_i}\)</span> representing the context of a selected target word (a timestep in the <strong>Encoder</strong>). The individual vectors are then fed into their corresponding <strong>RNN</strong> timesteps in the <strong>Decoder</strong>.</p>
<p>Two popular variants of <strong>Attention</strong> mechanisms were developed. Bahdanau D. et al. <span class="citation">(<a href="bibliography.html#ref-ref1317d">2015</a>)</span> demonstrated the use of the first <strong>Attention</strong> mechanism; hereafter, we call this the <strong>Bahdanau Attention</strong>, regarded as <strong>Additive Attention</strong>. Luong M. et al. <span class="citation">(<a href="bibliography.html#ref-ref1328m">2015</a>)</span> demonstrated the second variant; hereafter, we call this the <strong>Luong Attention</strong>, regarded as <strong>Multiplicative Attention</strong>. We can see the difference between the two mechanisms in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:attentionencoder">13.18</a>. In the <strong>Luong Attention</strong>, the individual <strong>context vector</strong> <span class="math inline">\(\mathbf{c_t}\)</span> is also added with the output. We show this in the diagram as dashed arrows which are not found in <strong>Bahdanau Attention</strong>. Additionally, the reason becomes clear why we call the former <strong>Additive Attention</strong> and the latter as <strong>Multiplicative Attention</strong>. Notice that the <strong>Attention</strong> unit takes two inputs, namely, the hidden state <span class="math inline">\(\mathbf{h_t}\)</span> from the encoder and the hidden state <span class="math inline">\(\mathbf{s_t}\)</span> from the decoder. The alignment model uses the two states to calculate the <strong>attention scores</strong>. How we calculate the <strong>attention scores</strong> depends on the two variants.</p>
<p>Let us use Figure <a href="13.5-transformer-neural-network-tnn.html#fig:attention">13.19</a> to illustrate the first variant, <strong>Bahdanau attention</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attention"></span>
<img src="attention.png" alt="Attention Unit" width="100%" />
<p class="caption">
Figure 13.19: Attention Unit
</p>
</div>
<p>Mathematically, the use of a single context vector <strong>c</strong> is replaced by individual distinct context vectors <span class="math inline">\(\mathbf{c_t}\)</span> like so:</p>
<p><span class="math display" id="eq:equate1150080">\[\begin{align}
from: y_t = g(s_{t}, y_{t-1}, c)
\ \ \ \ \ \ \
to: y_t = g(s_{t}, y_{t-1}, c_t) \tag{13.82} 
\end{align}\]</span></p>
<p>It is worth noting that the <strong>g(.)</strong> function yields the probability of every output word conditioned on both previous outputs and the corresponding context vector as written below:</p>
<p><span class="math display" id="eq:equate1150081">\[\begin{align}
g(s_{t}, y_{t-1}, c_t) = P(y_t |y_1, .... y_{t-1}, c_t) \tag{13.83} 
\end{align}\]</span></p>
<p>whereas originally, we are conditioned on a single context vector:</p>
<p><span class="math display" id="eq:equate1150082">\[\begin{align}
g(s_{t}, y_{t-1}, c) = P(y_t |y_1, .... y_{t-1}, c) \tag{13.84} 
\end{align}\]</span></p>
<p>We know that the hidden states <span class="math inline">\(\mathbf{s_{t}}\)</span> are generated by <strong>RNN</strong> in the <strong>Decoder</strong> so that each <strong>RNN</strong> unit feeds from previous hidden states. However, on the other hand, it also feeds from corresponding context vectors <span class="math inline">\(\mathbf{c_i}\)</span> generated by the <strong>Attention unit</strong> using the following formula:</p>
<p><span class="math display" id="eq:equate1150083">\[\begin{align}
c_i = \sum_{j=1}^t \alpha_{ij} h_j \tag{13.85} 
\end{align}\]</span></p>
<p>Here, the symbol <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) represents the <strong>attention weight</strong> which is calculated using <strong>softmax</strong> operation:</p>
<p><span class="math display" id="eq:equate1150084">\[\begin{align}
\alpha_{ij} = \frac{\mathbf{\text{exp}}(e_{ij})}{\sum_k^t \mathbf{\text{exp}}(e_{ik})}\ \ \ \ \ \ \
\rightarrow\ \ \ \ \text{(softmax)}
\ \ \ \ \ \ \text{where:}\ \ \ \ \ \ \
\sum_{j=1} \alpha_{j} = 1 \tag{13.86} 
\end{align}\]</span></p>
<p>Now, in <strong>Bahdanau Attention</strong>, the <strong>attention scores</strong> denoted by <span class="math inline">\(\mathbf{e_{ij}}\)</span> are calculated by an <strong>alignment function</strong>. It is called <strong>Additive Attention</strong> because both states are parameterized for training, then added together, then <strong>tanh</strong> is applied before being multiplied by another parameter.</p>
<p><span class="math display" id="eq:equate1150085">\[\begin{align}
e_{ij} = a(s_{i-1}, h_j) = v_a^\text{T}\ \mathbf{\text{tanh}}\left(W_a\ s_{i-1} + U_a\ h_j\right) \tag{13.87} 
\end{align}\]</span></p>
<p>In <strong>Luong Attention</strong>, we are given three choices of <strong>alignment function</strong> to calculate the <strong>attention scores</strong>:</p>
<p><span class="math display" id="eq:eqnnumber803">\[\begin{align}
e_{ij} = score(h_t, \bar{h}_s) = 
\begin{cases}
h_t^T \bar{h}_s &amp; \text{dot}\\
h_t^T W_a \bar{h}_s &amp; \text{general}\\
v_a^T\ \mathbf{\text{tanh}}\left(W_a [ h_t; \bar{h}_s]\right) &amp; \text{concat}
\end{cases} \tag{13.88}
\end{align}\]</span></p>
<p>Additionally, <strong>Luong Attention</strong> also proposes <strong>Local Attention</strong> in which only a subset of source input is considered for alignment rather than the entire source input. A good example of such implementation is in <strong>Skip-gram</strong>, which we cover as part of our <strong>Word Embedding</strong> discussion in a later section in which we discuss the idea of using a window that is controlled by window size and that slides across a series of input throughout the iteration. There are good ideas presented in <strong>Luong Attention</strong>, such as using <strong>monotonic alignment</strong>, which considers a few timesteps before and after the target word to construct the window. Alternatively, using <strong>predictive alignment</strong> based on <strong>Gaussian distribution</strong>. The idea of <strong>predictive alignment</strong> is to consider <strong>coverage</strong> of <strong>attentiveness</strong> so that we favor neighboring words in proximity and words that are strongly attentive (closer to the mean) though they may be remote. Consider the following sentence:</p>
<p><span class="math display">\[
\mathbf{\text{I gave browny, my cute little dog, treats.}}
\]</span></p>
<p>Notice that if our target word is <strong>gave</strong>, we can form <strong>contextual words</strong> using <strong>I</strong> and <strong>treats</strong>, both of which happen to be not in proximity.</p>
<p><span class="math display">\[
\mathbf{\text{I gave treats.}}
\]</span></p>
<p>We leave readers to investigate the below <strong>predictive alignment function</strong> with <strong>gaussian</strong> formula:</p>
<p><span class="math display" id="eq:equate1150086">\[\begin{align}
a_t(s)= \mathbf{align}(h_t, \bar{h}_s)  \mathbf{exp}\left(-\frac{(s - p_t)^2}{2 \sigma^2}\right)  \tag{13.89} 
\end{align}\]</span></p>
<p>We also leave readers to investigate <strong>soft</strong> and <strong>hard</strong> Attention mentioned in the Luong M. paper referring to using <strong>soft</strong> Attention for all patches in an image (see <strong>CNN</strong> for receptive fields); otherwise, using <strong>hard</strong> Attention one patch a time.</p>
<p>In the next section, we shall use <strong>dot product</strong> for our alignment function using <strong>Self-Attention</strong> to demonstrate <strong>Global Attention</strong>.</p>
</div>
<div id="self-attention-and-trainability" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.2</span> Self-Attention and Trainability <a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We saw a small glimpse of how the <strong>Attention</strong> mechanism works, though that is not the complete picture. For example, we are yet to cover how the mechanism can be trained by introducing learnable parameters - an essential property of <strong>Attention</strong>.</p>
<p>In this section, let us briefly take a closer look at <strong>Attention unit</strong> under the hood. Afterwhich, we discuss the trainability of <strong>Attention units</strong>. Let us use Figure <a href="13.5-transformer-neural-network-tnn.html#fig:attention">13.19</a> to illustrate:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:selfattention"></span>
<img src="selfattention.png" alt="Attention Unit" width="90%" />
<p class="caption">
Figure 13.20: Attention Unit
</p>
</div>
<p>In the figure, we see a layered RNN stack producing a <strong>sequence</strong> of vectors denoted by <span class="math inline">\(\mathbf{\vec{h}^{(2)}}\)</span> from the second RNN layer. This sequence of vectors is fed through an <strong>Attention unit</strong>. It helps to point out that the diagram reflects a <strong>Self-attention</strong> unit. By <strong>self</strong>, each token in the series is paired with the other. The idea is to derive some contextual relationship amongst each other - although being strongly attentive to oneself becomes implicitly part of the computation.</p>
<p>Now, in a paper titled <strong>Attention is all we need</strong>, published by Ashish Vaswani et al. <span class="citation">(<a href="bibliography.html#ref-ref1339a">2017</a>)</span>, the trainability of an <strong>Attention unit</strong> is made possible with the introduction of attention coefficients - learnable parameters (weights) - constructed in the form of three matrices which the paper refers to as <strong>Q matrix</strong>, <strong>K matrix</strong>, and <strong>V matrix</strong>. To illustrate, let us use Figure <a href="13.5-transformer-neural-network-tnn.html#fig:selfattention1">13.21</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:selfattention1"></span>
<img src="selfattention1.png" alt="Attention with Weights" width="100%" />
<p class="caption">
Figure 13.21: Attention with Weights
</p>
</div>
<p>The <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are instances manifested from the same embedding source that are given roles to play, namely <strong>query</strong>, <strong>keys</strong>, <strong>values</strong>. To give a better picture, let us continue to use our previous example sentence:</p>
<p><span class="math display">\[
\underbrace{\mathbf{\text{I }}}_{\mathbf{h1}}
\underbrace{\mathbf{\text{ travel }}}_{\mathbf{h2}}
\underbrace{\mathbf{\text{ the }}}_{\mathbf{h3}}
\underbrace{\mathbf{\text{ world }}}_{\mathbf{h4}}
\underbrace{\mathbf{\text{ in }}}_{\mathbf{h5}}
\underbrace{\mathbf{\text{ search }}}_{\mathbf{h6}}
\underbrace{\mathbf{\text{ for }}}_{\mathbf{h7}} 
\underbrace{\mathbf{\text{ the }}}_{\mathbf{h8}}
\underbrace{\mathbf{\text{ fountain }}}_{\mathbf{h9}}
\underbrace{\mathbf{\text{ of }}}_{\mathbf{h10}}
\underbrace{\mathbf{\text{ youth }}}_{\mathbf{h11}}
\]</span></p>
<p>Based on <strong>Retrieval Information</strong> theory, particularly in <strong>search engine</strong> systems, we supply a query keyword to the engine and expect the engine to respond with a list of relevant documents. In the context of <strong>Self-Attention</strong>, we supply a query keyword in search for <strong>contextual words</strong> in a list of words called <strong>keys</strong>. The idea is to know which of these keys will pay more <strong>attention</strong> to the query. For a better intuition, let us step through the operations.</p>
<p><strong>First</strong>, for illustration, let us cut the sentence above to only the subject line, <strong>I travel the world</strong>, giving us only about four tokens (or word embeddings). Assume for a moment that our embeddings have five dimensions (dk=5). For now, let us randomly assign some continuous numbers.</p>

<div class="sourceCode" id="cb2141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2141-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">digits=</span><span class="dv">8</span>)</a>
<a class="sourceLine" id="cb2141-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">12</span>)</a>
<a class="sourceLine" id="cb2141-3" data-line-number="3">t    =<span class="st"> </span><span class="dv">4</span>  <span class="co"># number of tokens (embedding)</span></a>
<a class="sourceLine" id="cb2141-4" data-line-number="4">dk   =<span class="st"> </span><span class="dv">5</span>  <span class="co"># dimension per token</span></a>
<a class="sourceLine" id="cb2141-5" data-line-number="5">random &lt;-<span class="st"> </span><span class="cf">function</span>(dk) { <span class="kw">round</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk, <span class="dt">min=</span><span class="fl">0.100</span>, <span class="dt">max=</span><span class="fl">0.900</span>), <span class="dv">3</span>)   }</a>
<a class="sourceLine" id="cb2141-6" data-line-number="6"><span class="co"># assume hidden states produced by RNN for each word</span></a>
<a class="sourceLine" id="cb2141-7" data-line-number="7">h1   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># I       </span></a>
<a class="sourceLine" id="cb2141-8" data-line-number="8">h2   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># travel   </span></a>
<a class="sourceLine" id="cb2141-9" data-line-number="9">h3   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># the</span></a>
<a class="sourceLine" id="cb2141-10" data-line-number="10">h4   =<span class="st"> </span><span class="kw">random</span>(dk)  <span class="co"># world </span></a>
<a class="sourceLine" id="cb2141-11" data-line-number="11">(<span class="dt">h    =</span> <span class="kw">rbind</span>(h1, h2, h3, h4))</a></code></pre></div>
<pre><code>##     [,1]  [,2]  [,3]  [,4]  [,5]
## h1 0.155 0.754 0.854 0.316 0.235
## h2 0.127 0.243 0.613 0.118 0.107
## h3 0.414 0.751 0.401 0.405 0.312
## h4 0.451 0.466 0.533 0.633 0.190</code></pre>

<p><strong>Second</strong>, let us query the keys to obtain scores. Mathematically, we have the following expression, including the normalizer:</p>
<p><span class="math display" id="eq:equate1150087">\[\begin{align}
\text{scaled scores} = \frac{(\text{query})\ (\text{keys}^\text{T})} {\sqrt{d_k}}  \tag{13.90} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2143-1" data-line-number="1">query  =<span class="st"> </span>keys =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2143-2" data-line-number="2">scores =<span class="st"> </span>(query) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(keys)</a>
<a class="sourceLine" id="cb2143-3" data-line-number="3">scaled.scores =<span class="st"> </span><span class="kw">round</span>(scores <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(dk), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2143-4" data-line-number="4"><span class="kw">colnames</span>(scaled.scores) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;q&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2143-5" data-line-number="5"><span class="kw">rownames</span>(scaled.scores) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;s&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2143-6" data-line-number="6">scaled.scores</a></code></pre></div>
<pre><code>##       q1    q2    q3    q4
## s1 0.661 0.353 0.525 0.501
## s2 0.353 0.213 0.251 0.265
## s3 0.525 0.251 0.518 0.477
## s4 0.501 0.265 0.477 0.510</code></pre>

<p><strong>Third</strong>, we perform softmax for each query like so:</p>
<p><span class="math display" id="eq:equate1150088">\[\begin{align}
\alpha_i = \frac{exp(s_i)} {\sum_j^t(exp(s_{i,j}))} \tag{13.91} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2145-1" data-line-number="1">attention.weights =<span class="st"> </span><span class="kw">softmax</span>(scaled.scores)</a>
<a class="sourceLine" id="cb2145-2" data-line-number="2"><span class="kw">colnames</span>(attention.weights) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;q&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2145-3" data-line-number="3"><span class="kw">rownames</span>(attention.weights) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;w&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2145-4" data-line-number="4">attention.weights</a></code></pre></div>
<pre><code>##       q1    q2    q3    q4
## w1 0.289 0.212 0.252 0.246
## w2 0.271 0.236 0.245 0.248
## w3 0.270 0.205 0.268 0.257
## w4 0.265 0.209 0.259 0.267</code></pre>

<p>So that if we add the softmax scores, we should get a 1 for all attention weights.</p>
<p><span class="math display" id="eq:equate1150089">\[\begin{align}
\sum_i^t \left( \alpha_i\right) = 1 \tag{13.92} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2147-1" data-line-number="1"><span class="kw">round</span>(<span class="kw">apply</span>(attention.weights, <span class="dv">1</span>, sum), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## w1 w2 w3 w4 
##  1  1  1  1</code></pre>

<p><strong>Finally</strong>, we obtain the contextual embeddings using the below formula per embedding. Alternatively, we can perform matrix multiplication instead.</p>
<p><span class="math display" id="eq:equate1150090">\[\begin{align}
c_i = \sum \alpha_i \cdot v_i  \tag{13.93} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2149-1" data-line-number="1">values =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2149-2" data-line-number="2">context =<span class="st"> </span><span class="kw">round</span>(attention.weights <span class="op">%*%</span><span class="st"> </span>(values), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2149-3" data-line-number="3"><span class="kw">colnames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;f&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,dk))</a>
<a class="sourceLine" id="cb2149-4" data-line-number="4"><span class="kw">rownames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;c&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2149-5" data-line-number="5"><span class="kw">t</span>(context)</a></code></pre></div>
<pre><code>##       c1    c2    c3    c4
## f1 0.287 0.285 0.295 0.295
## f2 0.573 0.561 0.574 0.570
## f3 0.609 0.607 0.601 0.601
## f4 0.374 0.370 0.381 0.382
## f5 0.216 0.212 0.218 0.216</code></pre>

<p>Using <strong>learnable weights</strong>, we first initialize three matrices (<strong>Q</strong>, <strong>K</strong>, <strong>V</strong>) using a random uniform distribution.</p>

<div class="sourceCode" id="cb2151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2151-1" data-line-number="1">Q =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a>
<a class="sourceLine" id="cb2151-2" data-line-number="2">K =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a>
<a class="sourceLine" id="cb2151-3" data-line-number="3">V =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="dt">nrow=</span>dk)</a></code></pre></div>

<p>We then multiply <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> embeddings with the corresponding matrix like so (noting that <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are sourced from <strong>e</strong>):</p>

<div class="sourceCode" id="cb2152"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2152-1" data-line-number="1">q =<span class="st"> </span>k =<span class="st"> </span>v =<span class="st"> </span>h</a>
<a class="sourceLine" id="cb2152-2" data-line-number="2">query  =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>Q</a>
<a class="sourceLine" id="cb2152-3" data-line-number="3">keys   =<span class="st"> </span>k <span class="op">%*%</span><span class="st"> </span>K</a>
<a class="sourceLine" id="cb2152-4" data-line-number="4">values =<span class="st"> </span>v <span class="op">%*%</span><span class="st"> </span>V </a></code></pre></div>

<p>Note that we created <strong>q</strong>, <strong>k</strong>, <strong>v</strong> as three separate instances for demonstration. In actual implementation, we may use <strong>e</strong> for the three computations instead to save memory.</p>
<p>We go through the same operations as above to get our context using the <strong>query</strong>, <strong>keys</strong>, and <strong>values</strong>. Here, we try to implement our example of an attention function:</p>

<div class="sourceCode" id="cb2153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2153-1" data-line-number="1">attention.unit &lt;-<span class="st"> </span><span class="cf">function</span>(query, keys, values) {</a>
<a class="sourceLine" id="cb2153-2" data-line-number="2">   t                 =<span class="st"> </span><span class="kw">ncol</span>(query) <span class="co"># t-tokens</span></a>
<a class="sourceLine" id="cb2153-3" data-line-number="3">   dk                =<span class="st"> </span><span class="kw">nrow</span>(query) <span class="co"># k-dimension </span></a>
<a class="sourceLine" id="cb2153-4" data-line-number="4">   scores            =<span class="st"> </span><span class="kw">t</span>(query) <span class="op">%*%</span><span class="st"> </span>keys  </a>
<a class="sourceLine" id="cb2153-5" data-line-number="5">   scaled.scores     =<span class="st"> </span>scores <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(dk)       <span class="co"># scaled</span></a>
<a class="sourceLine" id="cb2153-6" data-line-number="6">   attention.weights =<span class="st"> </span><span class="kw">softmax</span>(scaled.scores) <span class="co"># column-wise </span></a>
<a class="sourceLine" id="cb2153-7" data-line-number="7">   context           =<span class="st"> </span><span class="kw">round</span>(attention.weights <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(values), <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb2153-8" data-line-number="8">   <span class="kw">colnames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;f&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,dk))</a>
<a class="sourceLine" id="cb2153-9" data-line-number="9">   <span class="kw">rownames</span>(context) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;c&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>,t))</a>
<a class="sourceLine" id="cb2153-10" data-line-number="10">   <span class="kw">t</span>(context) <span class="co"># transpose</span></a>
<a class="sourceLine" id="cb2153-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb2153-12" data-line-number="12"><span class="kw">attention.unit</span>(query, keys, values)</a></code></pre></div>
<pre><code>##        c1     c2     c3     c4     c5
## f1 -0.032 -0.026 -0.075 -0.067 -0.063
## f2 -0.010 -0.006 -0.031 -0.027 -0.024
## f3 -0.035 -0.029 -0.079 -0.069 -0.066
## f4 -0.086 -0.081 -0.123 -0.115 -0.111</code></pre>

<p>We should note that the context produced by our attention function is still raw because we relied on the initial values of the <strong>learnable weights</strong>. Similar to our discussion on <strong>MLP</strong> and other types of networks, we need to plug the attention unit into a neural network to allow gradients to flow through the attention layer through <strong>backpropagation</strong>. We will discuss this further once we get to the <strong>Transformer</strong> architecture.</p>
</div>
<div id="multi-head-attention" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.3</span> Multi-Head Attention <a href="13.5-transformer-neural-network-tnn.html#multi-head-attention" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Multi-Head Attention</strong> emphasizes having multiple <strong>heads</strong> of <strong>attention units</strong>, each <strong>head</strong> is trained using a set of <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> matrices containing the <strong>trainable parameters</strong>. See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:multiheadattention">13.22</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multiheadattention"></span>
<img src="multiheadattention.png" alt="Multi-Head Attention" width="100%" />
<p class="caption">
Figure 13.22: Multi-Head Attention
</p>
</div>
<p>Having multi-head attention comes from the fact that a single sentence (an input sequence) may contain multiple aspects or perspectives. In analogy, each attention head is similar to a convolution filter in <strong>CNN</strong> designed to capture different features of an image. Similarly, each attention head can focus on different aspects or perspectives of an input sequence.  So, for example, we can break our  example sentence in the previous section into three possible aspects like so:</p>
<p><span class="math display">\[
\begin{array}{lll}
\mathbf{\text{aspect 1}} &amp;\rightarrow\ \text{I travel the world} &amp; \ \ \ \ (\text{traveling aspect})\\
\mathbf{\text{aspect 2}} &amp;\rightarrow\ \text{search for the fountain} &amp; \ \ \ \ (\text{searching aspect})\\
\mathbf{\text{aspect 3}} &amp;\rightarrow\ \text{fountain of youth} &amp; \ \ \ \ (\text{magical aspect})
\end{array}
\]</span></p>
<p>We may allow each of three <strong>attention</strong> heads to focus on a different aspect of the input sequence. We can choose to use even more heads if we <strong>feel</strong> that there are more hidden aspects. Each head uses a set of <strong>Q</strong>, <strong>K</strong>, <strong>V</strong> matrices containing learned parameters to attend to a different aspect.</p>
<p>In terms of computation, notice in the figure that each of the three instances of the embedding, namely <strong>q</strong>, <strong>k</strong>, and <strong>v</strong>, runs through matrix multiplication with each corresponding matrix. Because we are dealing with three attention heads for the three contexts in our example above, it means that <strong>q</strong> gets multiplied with <strong>Q1</strong>, <strong>Q2</strong>, and <strong>Q3</strong> matrices, resulting in three <strong>score matrices</strong>. Similarly, <strong>k</strong> gets multiplied with <strong>K1</strong>, <strong>K2</strong>, and <strong>K3</strong> matrices, resulting in three <strong>score matrices</strong>. The same applies to <strong>v</strong>. Next, the scores are scaled using a normalizer, e.g., <span class="math inline">\(\sqrt{d_k}\)</span>, followed by <strong>softmax</strong> to derive the attention weights - or attention probabilities. The final context is obtained by performing another matrix multiplication between the output of the <strong>softmax</strong> and the output of the score matrices from <strong>v</strong> and the three <strong>V</strong> matrices. For example, in the multi-head attention, the softmax accepts the following input:</p>
<p><span class="math display" id="eq:equate1150091">\[\begin{align}
\text{attention}(Q,K,V) = \text{softmax}\left(\frac{QK^\text{T}}{\sqrt{d_k}}\right)\cdot V  \tag{13.94} 
\end{align}\]</span></p>
<p>To give an example of how we use <strong>Multi-head Attention</strong>, suppose we have the number of heads equal to 5. Let us randomly initialize the <strong>Q</strong>, <strong>K</strong>, and <strong>V</strong> tensors, noting that the tensors have dimension <span class="math inline">\((k \times k \times h)\)</span>:</p>

<div class="sourceCode" id="cb2155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2155-1" data-line-number="1">h =<span class="st"> </span><span class="dv">5</span>;  t =<span class="st"> </span><span class="dv">4</span> <span class="co"># here, h equals no. of heads, t equals no. of tokens</span></a>
<a class="sourceLine" id="cb2155-2" data-line-number="2">Q =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-3" data-line-number="3">K =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-4" data-line-number="4">V =<span class="st"> </span><span class="kw">array</span>(<span class="kw">runif</span>(<span class="dt">n=</span>dk <span class="op">*</span><span class="st"> </span>dk, <span class="dt">min=</span><span class="op">-</span><span class="fl">0.5</span>, <span class="dt">max=</span><span class="fl">0.5</span>), <span class="kw">c</span>(dk, dk, h))</a>
<a class="sourceLine" id="cb2155-5" data-line-number="5">K[,,<span class="dv">1</span>]  <span class="co"># display an initialized K matrix for the first head.</span></a></code></pre></div>
<pre><code>##            [,1]        [,2]        [,3]        [,4]
## [1,] 0.15782952 0.158294656  0.30144395 0.389498414
## [2,] 0.38407710 0.078136669 -0.28551430 0.334255804
## [3,] 0.33999093 0.155190383  0.47465562 0.140585024
## [4,] 0.44216319 0.297951073 -0.32631462 0.373908170
## [5,] 0.30393706 0.069033058 -0.48419310 0.094678544
##             [,5]
## [1,] -0.20034764
## [2,]  0.34011628
## [3,]  0.09883009
## [4,]  0.21031208
## [5,] -0.16618633</code></pre>

<p>We then multiply <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> embeddings with the corresponding matrix like so (noting that <strong>q</strong>, <strong>k</strong>, and <strong>v</strong> are sourced from <strong>e</strong>:</p>

<div class="sourceCode" id="cb2157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2157-1" data-line-number="1">query =<span class="st"> </span>keys =<span class="st"> </span>values =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dk, t, h)) <span class="co"># create the q,k,v structures</span></a>
<a class="sourceLine" id="cb2157-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>h) {</a>
<a class="sourceLine" id="cb2157-3" data-line-number="3">  query[,,i]  =<span class="st"> </span>q <span class="op">%*%</span><span class="st"> </span>Q[,,i]</a>
<a class="sourceLine" id="cb2157-4" data-line-number="4">  keys[,,i]   =<span class="st"> </span>k <span class="op">%*%</span><span class="st"> </span>K[,,i]</a>
<a class="sourceLine" id="cb2157-5" data-line-number="5">  values[,,i] =<span class="st"> </span>v <span class="op">%*%</span><span class="st"> </span>V[,,i] </a>
<a class="sourceLine" id="cb2157-6" data-line-number="6">}</a></code></pre></div>

<p>Let us now invoke our <strong>attention.unit</strong> function.</p>

<div class="sourceCode" id="cb2158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2158-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb2158-2" data-line-number="2">contextualized =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(dk, t, h)) </a>
<a class="sourceLine" id="cb2158-3" data-line-number="3"><span class="cf">for</span>(i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>h) {</a>
<a class="sourceLine" id="cb2158-4" data-line-number="4">   contextualized[,,i] =<span class="st"> </span><span class="kw">attention.unit</span>(query[,,i], keys[,,i], values[,,i])</a>
<a class="sourceLine" id="cb2158-5" data-line-number="5">}</a></code></pre></div>
<p>The dimension is [5, 4, 5].</p>

<p>Note that the output of the multi-head attention is a list of <strong>contextualized</strong> matrices. The list has a size equal to the number of heads. To obtain only one <strong>contextualized matrix</strong> with the same dimension as the original embedding, we need to merge the list. The proposed <strong>merge</strong> approach is to concatenate and then run through a dense FC layer, reshaping the output in the output layer, then using softmax past the output layer.</p>
<p><strong>Multihead Attention</strong> is seen in the paper by Ashish Vaswani et al. <span class="citation">(<a href="bibliography.html#ref-ref1339a">2017</a>)</span>. It is used as one of the core components of a novel <strong>Attention-based</strong> architecture called <strong>Transformer</strong>.</p>
<p>Before we jump to <strong>Transformers</strong>, let us discuss a few essential topics, focusing on <strong>sequence-to-sequence</strong> applications. Among many others, we need to build our intuition around the following topics:</p>
<ul>
<li><strong>Word Embedding</strong> - numerical representation of individual sequence elements (e.g., words),</li>
<li><strong>Positional Embedding</strong> - numerical representation of the position of sequence elements, and</li>
<li><strong>Sequence Alignment</strong> - alignment between sequences if we are to translate one sequence to another.</li>
</ul>
</div>
<div id="word-embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.4</span> Word Embedding <a href="13.5-transformer-neural-network-tnn.html#word-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A few concepts are introduced in Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), particularly in the context of Natural Language Processing (<strong>NLP</strong>). One of the applications of <strong>NLP</strong> is around <strong>search engines</strong> in which we deal with a given query and a list of documents available to search. Here, we start with <strong>pre-processing</strong> of texts to extract <strong>bag of words (BoW)</strong> using <strong>tokenization</strong>, <strong>case-sensitivity</strong>, <strong>stopwords</strong>, <strong>N-grams</strong>, <strong>stemming</strong>, <strong>lemmatization</strong>, and so on. While such a <strong>bag of words</strong> is merely an extracted list of terms, it does not necessarily carry any numerical representation. Instead, we measure the relevance of each term (or relevance of a query made of terms) by way of ranking techniques. <strong>Okapi BM25</strong> technique is introduced in the Chapter mentioned, which is a rather advanced variant of the generic <strong>TF-IDF</strong> technique covered in the field of <strong>Information Retrieval</strong>. The idea is to be able to rank terms found both in a given query and also in a list of documents. Fundamentally, this is how we score a term, or in other words, how we cast numerical values of terms. Mathematically, we cast words into their numerical representations in the form of <strong>Vector Space Model (SVM)</strong>. From an engineering perspective, we can use <strong>Document Term Matrix (DTM)</strong> or <strong>Term Document Matrix (TDM)</strong> interchangeably to compare documents. Both matrices may contain simple counts of terms or <strong>TF-IDF</strong> results. From there, we use <strong>cosine similarity</strong> to identify the relationship of documents. See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:vsmtfidf">13.23</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:vsmtfidf"></span>
<img src="vsm_tfidf.png" alt="Vector Space Model (VSM) with TF-IDF scores" width="90%" />
<p class="caption">
Figure 13.23: Vector Space Model (VSM) with TF-IDF scores
</p>
</div>
<p>From the idea of <strong>Vector Space Model (VSM)</strong>, we carry the same intuition for <strong>Word Embedding</strong>. Perhaps, the fundamental definition of <strong>Word Embedding</strong> exemplifies the idea that words can be translated to their numeric representation. To further build the intuition around this, imagine a <strong>matching site</strong> that allows users to enter a list of personality traits and allows its matching engine to pair individual users based on traits. Here, we may compile traits such as <strong>athletic</strong>, <strong>outgoing</strong>, <strong>adventurous</strong>, <strong>likes movies</strong>, and others. See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:context">13.24</a>. From this perspective, we can readily say that a person is characterized or represented by a vector of <strong>trait features</strong>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:context"></span>
<img src="context.png" alt="Word Embedding)" width="90%" />
<p class="caption">
Figure 13.24: Word Embedding)
</p>
</div>
<p>Suppose that, by some algorithm, the matching engine can assign numbers to each trait so that a particular user labeled as <strong>You</strong> is 0.85 athletic, 0.61 outgoing, and on. Similarly, a particular unknown individual labeled as <strong>Person</strong> resembles almost the same weights as <strong>You</strong> in terms of traits. Ideally, using a scoring formula such as <strong>cosine similarity</strong>, we can suggest that <strong>You</strong> and <strong>Person</strong> match by proximity. Moreover, imagine by the same token that if there is another user labeled as <strong>Elf</strong> whose traits closely match the combined traits of <strong>You</strong> and <strong>Person</strong>, we now begin to see a cluster of users with similar characteristics (visualizing by reducing dimensionality to two using <strong>PCA</strong>).</p>
<p>In the context of <strong>semantic analysis</strong>, the meaning of a word (its <strong>context</strong>) does not have to be derived only by how the word can be described (based on traits). Instead, it can also be derived from how the word is used in a <strong>sequence of words</strong> based on its surrounding words. This idea comes from the famous quote below used by other literature:</p>
<p><span class="math display">\[
   \text{&quot;You shall know a word by the company it keeps&quot;, }
   \ \ \ \ \ \ \ \mathbf{\text{John Rupert Firth.}}
\]</span></p>
<p>In Chapter <strong>11</strong> (<strong>Computational Learning III</strong>), we introduce two techniques that allow us to capture the semantic essence of texts, namely <strong>Probabilistic Latent Semantic Analysis (pLSA)</strong> and <strong>Latent Dirichlet Allocation (LDA)</strong>. Mostly, the techniques are used for <strong>Topic Modeling</strong> relying on probabilistic language modeling to create a <strong>contextualized distributed representation</strong> of words (in particular using <strong>Dirichlet Distributions</strong> for <strong>LDA</strong>) to highlight probabilities of <strong>topic words</strong>. Both techniques introduce the use of <strong>latent</strong> parameters that follow certain distributions. We can say that <strong>Topic Modeling</strong> creates clustered <strong>contextualized word representation</strong> <span class="citation">(Laure Thompson et al. <a href="bibliography.html#ref-ref1236l">1997</a>)</span>.    </p>
<p>One important criterion that is lacking when dealing with traditional techniques such as <strong>pLSA</strong> and <strong>LDA</strong> is their learnability or trainability. When it comes to that, <strong>Neural Probabilistic Language Modeling (NPLM)</strong> comes into the picture, introduced by Yoshua Bengio et al. <span class="citation">(<a href="bibliography.html#ref-ref1227y">2003</a>)</span>. This model uses <strong>Neural Network</strong> to learn <strong>Word Embeddings</strong>. Figure <a href="13.5-transformer-neural-network-tnn.html#fig:neuralprobmodel">13.25</a> shows a neural architecture depicting the original <strong>NPLM</strong> neural network architecture from <strong>Bengio’s</strong> paper.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:neuralprobmodel"></span>
<img src="neuralprobmodel.png" alt="Neural Probabilistic Language Model" width="80%" />
<p class="caption">
Figure 13.25: Neural Probabilistic Language Model
</p>
</div>
<p>The model expects a series of word indexes for which we perform a table lookup against a matrix <strong>C</strong> which can also serve as a <strong>Word Embedding</strong> lookup. We expect the lookup to return a set of feature vectors, each vector corresponding to each word index. We then concatenate the set of feature vectors. This embedding gets forwarded to the hidden layer for matrix multiplication with a matrix of learnable <strong>weight</strong> parameters denoted by (<span class="math inline">\(\omega_h\)</span>). Then a non-linearity (<strong>tanh</strong>) is applied, followed by <strong>softmax</strong>. This architectural process demonstrates a shallow <strong>MLP</strong> with characteristics of a one hidden-layer fully-connected neural network architecture that performs the usual <strong>forward feed</strong> and <strong>backpropagation</strong>.</p>
<p>Many other architectural variants are motivated by the <strong>NPLM</strong> design, laid by Bengio’s paper. Revisiting our <strong>CNN</strong>, variant designs of <strong>NPLM</strong> may utilize different combinations of non-linearity, dropouts, layer normalization, optimization, residual nets, and on <span class="citation">(Sun S., Iyyer M. <a href="bibliography.html#ref-ref1582s">2021</a>)</span>.</p>
<p>One of the more advanced techniques motivated by <strong>NLPM</strong> architecture is called <strong>Word2vec</strong>. It was introduced by Tomas Mikolov et al. <span class="citation">(<a href="bibliography.html#ref-ref1215m">2013</a>)</span>. The grand idea follows <strong>NLPM</strong> in that it associates a word with surrounding words that serve as clues to predict <strong>context</strong>. Now, the type of <strong>Word Embedding</strong> created by <strong>Word2vec</strong> is a set of feature vectors that are learned or trained. The neural architecture of <strong>Word2vec</strong> is designed to utilize two <strong>Word Embedding</strong> approaches, namely <strong>Continuous Bag of Words (CBOW)</strong> and <strong>Skip-Gram</strong>. A <strong>CBOW</strong> model is trained to predict the probability of a target word to occur based on a given <strong>context</strong>. For example, given the context, <strong>I travel the world in search for the fountain of [____]</strong>, the most probable target word is <strong>youth</strong> from a list of word distributions, e.g., <strong>apples, sky, youth</strong>. The words <strong>apples</strong> and <strong>sky</strong> will receive the lowest probability scores. </p>
<p>On the other hand, the <strong>Skip-Gram</strong> model does the reverse in that it is trained to predict the <strong>context</strong> (list of contextual words) from a given target word. In other words, in <strong>Skip-Gram</strong>, it might make sense to predict surrounding words such as <strong>I travel the world in search for the fountain of [____]</strong>, given the target word <strong>youth</strong>.</p>
<p>To illustrate <strong>Skip-Gram</strong>, let us use Figure <a href="13.5-transformer-neural-network-tnn.html#fig:skipgram">13.26</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:skipgram"></span>
<img src="skipgram.png" alt="Skip-Gram Architecture" width="100%" />
<p class="caption">
Figure 13.26: Skip-Gram Architecture
</p>
</div>
<p>The neural architecture of <strong>Skip-Gram</strong> follows a shallow fully-connected <strong>MLP</strong> in which we have <strong>V-dimension</strong> inputs (as one-hot encoding) in the input layer, <strong>H</strong> neurons in the hidden layer, and <strong>O</strong> units for the output. Note that a <strong>window</strong> is used to define the scope of the surrounding words, the target word being at the center. The window is <strong>monotonic</strong> (see <strong>Luong Attention</strong>), and a hyperparameter <strong>window size</strong> can be used to adjust the number of surrounding words. This same number becomes the size of the <strong>O</strong> units which contains the number of predicted <strong>contextual words</strong> (the surrounding words). The window slides iteratively across the series of words until the entire series is accommodated. The size of the <strong>Hidden layer</strong> is defined based on the number of neurons to use for the training. The <strong>output</strong> of the forward feed at every iteration is a matrix that consists of a predicted set of probability vectors. Each vector represents a predicted <strong>contextual word</strong> that is then evaluated against the <strong>context clues</strong>, optimizing the Loss function. The final <strong>output</strong> of the <strong>Skip-Gram</strong> itself is a trained model consisting of the optimized weighted matrices (embedding matrix and context matrix). The matrices contain learnable parameters.  </p>
<p>Additionally, the output layer uses <strong>softmax</strong> to calculate word probabilities. It helps to note, however, that <strong>softmax</strong> is known to be computationally expensive because it involves calculating a matrix that carries the vocabulary (<strong>V</strong>) size. Furthermore, it can get very large. For that reason, <strong>Word2vec Skip-gram</strong> also demonstrates using <strong>negative sampling</strong> with <strong>sigmoid function</strong> as an alternative, switching the classification to binary (second paper published by <span class="citation">(Mikolov T. <a href="bibliography.html#ref-ref1215m">2013</a>)</span>. Other literature emphasizes the same alternative, evaluating the benefits <span class="citation">(Long Chen et al. <a href="bibliography.html#ref-ref1242l">2018</a>)</span>.</p>
<p>We leave readers to investigate other tools such as <strong>Inpatient2vec</strong>, <strong>doc2vec</strong>, and <strong>med2vec</strong> intended for specific <strong>Word Embeddings</strong> while providing a multi-layer medical representation learning for patients, diagnostics, and medical codes, respectively. Such representations provide context to targetted medical terms. Also, it helps to investigate <strong>Global Vectors for Word Representation (GloVe)</strong>.</p>
</div>
<div id="positional-embedding" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.5</span> Positional Embedding <a href="13.5-transformer-neural-network-tnn.html#positional-embedding" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the emergence of <strong>Attention</strong>, the sequential approach, seen in <strong>RNN</strong> (e.g., <strong>LSTM</strong> and <strong>GRU</strong>), is replaced by an architecture that handles a series of words in parallel. This architecture, however, loses the <strong>order</strong> information. Therefore, that is where <strong>Positional Embedding</strong> is required.</p>
<p><strong>Positional Embedding</strong> is a <strong>learned</strong> numerical representation of the position and order of words in a given <strong>sequence of words</strong>. This information is not captured in vanilla <strong>Word Embeddings</strong>. Thus, we need to add the information to the <strong>Word Embedding</strong>.</p>
<p><span class="math display">\[
\mathbf{\text{Word Embedding}} + \mathbf{\text{Positional Embedding}} =
\mathbf{\text{Position-Aware Word Embedding}}
\]</span></p>
<p>In terms of preserving <strong>position</strong> information, the first thoughts were using <strong>word indices</strong> or reducing the <strong>word indices</strong> to the range 0 and 1 using fractions. However, for the former, a large index at the end of a sequence tends to dominate the value of the <strong>Word Embedding</strong> when added. For the latter, even though the decimal value is the same, e.g., 0.25, a position in the form 1/4 (position one from a sequence of 4 words) does not equal a position in the form 2/8 (position two from a sequence of 8 words). Therefore, a novel approach introduced is to use <strong>Sinusoidal Position Encoding</strong> <span class="citation">(Vaswani A. et al. <a href="bibliography.html#ref-ref1339a">2017</a>)</span>. The big idea is to use frequency produced by <strong>sine</strong> and <strong>cosine</strong> to yield a position representation. Below is the expression used (derivation not included):</p>
<p><span class="math display" id="eq:equate1150092">\[\begin{align}
PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/dmodel}}\right) 
\ \ \ \ \ \ \ \ \ 
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/dmodel}}\right) \tag{13.95} 
\end{align}\]</span></p>
<p>An example implementation is written below.</p>

<div class="sourceCode" id="cb2159"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2159-1" data-line-number="1">sinusoid.encoding &lt;-<span class="st"> </span><span class="cf">function</span>(npos, emb.dim, pos.range) {</a>
<a class="sourceLine" id="cb2159-2" data-line-number="2">    angle &lt;-<span class="st"> </span><span class="cf">function</span>(pos, i) { pos <span class="op">/</span><span class="st"> </span><span class="dv">10000</span><span class="op">^</span>((<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>i) <span class="op">/</span><span class="st">  </span>emb.dim)}</a>
<a class="sourceLine" id="cb2159-3" data-line-number="3">    pos.seq       =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, npos<span class="dv">-1</span>, <span class="dt">length.out=</span>pos.range)</a>
<a class="sourceLine" id="cb2159-4" data-line-number="4">    emb.seq       =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, emb.dim<span class="dv">-1</span>, <span class="dt">length.out=</span>emb.dim)</a>
<a class="sourceLine" id="cb2159-5" data-line-number="5">    pos_embedding =<span class="st"> </span><span class="kw">array</span>(<span class="dv">0</span>, <span class="kw">c</span>(pos.range , emb.dim))</a>
<a class="sourceLine" id="cb2159-6" data-line-number="6">    <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>pos.range) {</a>
<a class="sourceLine" id="cb2159-7" data-line-number="7">        pos_embedding[p,] =<span class="st"> </span><span class="kw">angle</span>(pos.seq[p], emb.seq)</a>
<a class="sourceLine" id="cb2159-8" data-line-number="8">    }</a>
<a class="sourceLine" id="cb2159-9" data-line-number="9">    sini =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">0</span>, <span class="dt">to=</span>emb.dim<span class="dv">-1</span>, <span class="dt">by=</span><span class="dv">2</span>) <span class="co"># 2i</span></a>
<a class="sourceLine" id="cb2159-10" data-line-number="10">    cosi =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="dv">1</span>, <span class="dt">to=</span>emb.dim<span class="dv">-1</span>, <span class="dt">by=</span><span class="dv">2</span>) <span class="co"># 2i + 1</span></a>
<a class="sourceLine" id="cb2159-11" data-line-number="11">    pos_embedding[,sini] =<span class="st"> </span><span class="kw">sin</span>(pos_embedding[,sini])</a>
<a class="sourceLine" id="cb2159-12" data-line-number="12">    pos_embedding[,cosi] =<span class="st"> </span><span class="kw">cos</span>(pos_embedding[,cosi])</a>
<a class="sourceLine" id="cb2159-13" data-line-number="13">    pos_embedding</a>
<a class="sourceLine" id="cb2159-14" data-line-number="14">}</a></code></pre></div>

<p>Assume the number of positions is 20 with an embedding size of 40. We should be able to produce a rotary matrix to be then added to a word embedding of the same dimension.</p>
<div class="sourceCode" id="cb2160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2160-1" data-line-number="1">npos     =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb2160-2" data-line-number="2">pos.range =<span class="st"> </span><span class="dv">200</span> <span class="co"># only needed to plot a smoother curve</span></a>
<a class="sourceLine" id="cb2160-3" data-line-number="3">v =<span class="st"> </span><span class="kw">sinusoid.encoding</span>(<span class="dt">npos =</span> npos, <span class="dt">emb.dim=</span><span class="dv">40</span>, <span class="dt">pos.range=</span>pos.range)</a>
<a class="sourceLine" id="cb2160-4" data-line-number="4"><span class="kw">data.frame</span>(v[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>,<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>]) <span class="co"># display only 10x3 portion of the matrix</span></a></code></pre></div>
<pre><code>##            X1          X2         X3
## 1  1.00000000 0.000000000 1.00000000
## 2  0.99544550 0.060205727 0.99927770
## 3  0.98182347 0.120193027 0.99711184
## 4  0.95925801 0.179744265 0.99350554
## 5  0.92795465 0.238643386 0.98846403
## 6  0.88819855 0.296676704 0.98199457
## 7  0.84035184 0.353633673 0.97410652
## 8  0.78485036 0.409307652 0.96481128
## 9  0.72219967 0.463496655 0.95412226
## 10 0.65297046 0.516004082 0.94205492</code></pre>
<p>A plot of the matrix is demonstrated in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:sinusoidal">13.27</a>. Here, we choose the first four elements (four dimensions) of an embedding.</p>

<div class="sourceCode" id="cb2162"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2162-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, npos<span class="dv">-1</span>, <span class="dt">length.out=</span>pos.range)</a>
<a class="sourceLine" id="cb2162-2" data-line-number="2">y =<span class="st"> </span>v</a>
<a class="sourceLine" id="cb2162-3" data-line-number="3"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(y),   </a>
<a class="sourceLine" id="cb2162-4" data-line-number="4">      <span class="dt">ylab=</span><span class="st">&quot;Embedding Dimension (i)&quot;</span>,  <span class="dt">xlab=</span><span class="st">&quot;Position (pos)&quot;</span>,   </a>
<a class="sourceLine" id="cb2162-5" data-line-number="5">      <span class="dt">main=</span><span class="st">&quot;Sinusoidal Position Encoding&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb2162-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb2162-7" data-line-number="7"><span class="kw">lines</span>(x, v[,<span class="dv">1</span>], <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>,  <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)   <span class="co"># 2i   (sine)</span></a>
<a class="sourceLine" id="cb2162-8" data-line-number="8"><span class="kw">lines</span>(x, v[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i+1 (cosine)</span></a>
<a class="sourceLine" id="cb2162-9" data-line-number="9"><span class="kw">lines</span>(x, v[,<span class="dv">3</span>], <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>,        <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i   (sine)</span></a>
<a class="sourceLine" id="cb2162-10" data-line-number="10"><span class="kw">lines</span>(x, v[,<span class="dv">4</span>], <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>,       <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">3</span>)   <span class="co"># 2i+1 (cosine)</span></a>
<a class="sourceLine" id="cb2162-11" data-line-number="11"><span class="kw">abline</span>(<span class="dt">v=</span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">15</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb2162-12" data-line-number="12"><span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>), <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">pch=</span><span class="dv">16</span>,   <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb2162-13" data-line-number="13"><span class="kw">points</span>(<span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>,<span class="dv">15</span>), <span class="kw">c</span>(<span class="op">-</span><span class="fl">0.75</span>, <span class="dv">0</span>, <span class="fl">0.95</span>, <span class="fl">-0.58</span>), <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb2162-14" data-line-number="14"><span class="kw">text</span>(<span class="dv">14</span>, <span class="fl">-0.75</span>, <span class="st">&quot;i=1&quot;</span>);  <span class="kw">text</span>(<span class="dv">16</span>,  <span class="fl">0.00</span>, <span class="st">&quot;i=2&quot;</span>)</a>
<a class="sourceLine" id="cb2162-15" data-line-number="15"><span class="kw">text</span>(<span class="dv">16</span>,  <span class="fl">0.92</span>, <span class="st">&quot;i=3&quot;</span>);  <span class="kw">text</span>(<span class="dv">14</span>, <span class="fl">-0.58</span>, <span class="st">&quot;i=4&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sinusoidal"></span>
<img src="DS_files/figure-html/sinusoidal-1.png" alt="Sinusoidal Position Encoding" width="70%" />
<p class="caption">
Figure 13.27: Sinusoidal Position Encoding
</p>
</div>

<p>Notice that the absolute position of a word is reduced to a sinusoidal value that ranges between -1 and 1. For example, in the plot for the first dimension (i=1), the position of the first word at zero renders a frequency value of 1, but the same word at position 15 has a frequency value of -0.75.</p>
<p>From here, we can add the derived positional embeddings to the word embeddings, so that assumes we have a word embedding of vector size 40 (40-dimensional features),</p>
<p><span class="math display">\[
\begin{array}{ll}
\text{W1} &amp;= \text{[0.734, 0.123, 0.512, ..., 0.111]}\ \ \ \ (1 \times 40 )\\
\text{W2} &amp;= \text{[0.813, 0.022, 0.762, ..., 0.212]}\ \ \ \ (1 \times 40 ) \\
&amp;... \\
\text{W15} &amp;= \text{[0.544, 0.653, 0.912, ..., 0.433]}\ \ \ \ (1 \times 40 )
\end{array}
\]</span>
we can then, as an example, add the vector of the first dimension like so:</p>
<p><span class="math display">\[\begin{align*}
\underbrace{\text{[0.734, 0.123, 0.512, ..., 0.111]}}_{\text{Word Embedding}} + 
\underbrace{\text{[1.000, 0.653, -0.241, ..., 0.988]}}_{\text{Positional Embedding}} = \\
\underbrace{[0.734, 0.080, ..., 0.110]}_{\text{Position-aware Word Embedding}}
\end{align*}\]</span></p>
<p>Alternatively, instead of dealing with absolute positions such as represented by <strong>sinusoidal embeddings</strong>, we can deal with <strong>relative position embedding</strong>. The idea is to represent the position in calculating the location of a word relevant to the location of other neighboring words. In other words, we measure the distance relationship between words. Below is a sample implementation of deriving distances based on the difference between absolute positions. The resulting distances are shown in the <strong>Toeplitz</strong> matrix form below:</p>

<div class="sourceCode" id="cb2163"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2163-1" data-line-number="1">distance &lt;-<span class="st"> </span><span class="cf">function</span>(w1, w2) {  <span class="kw">abs</span>(w2) <span class="op">-</span><span class="st"> </span><span class="kw">abs</span>(w1) }</a>
<a class="sourceLine" id="cb2163-2" data-line-number="2">w =<span class="st"> </span>word.indices =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>,<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">6</span>)</a>
<a class="sourceLine" id="cb2163-3" data-line-number="3">n =<span class="st"> </span><span class="kw">length</span>(word.indices)</a>
<a class="sourceLine" id="cb2163-4" data-line-number="4">m =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, n <span class="op">*</span><span class="st"> </span>n), <span class="dt">nrow=</span>n)</a>
<a class="sourceLine" id="cb2163-5" data-line-number="5"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {  m[i, ] =<span class="st"> </span><span class="kw">distance</span>(w[i], w) }</a>
<a class="sourceLine" id="cb2163-6" data-line-number="6">m</a></code></pre></div>
<pre><code>##      [,1] [,2] [,3] [,4] [,5] [,6] [,7]
## [1,]    0    1    2    3    4    5    6
## [2,]   -1    0    1    2    3    4    5
## [3,]   -2   -1    0    1    2    3    4
## [4,]   -3   -2   -1    0    1    2    3
## [5,]   -4   -3   -2   -1    0    1    2
## [6,]   -5   -4   -3   -2   -1    0    1
## [7,]   -6   -5   -4   -3   -2   -1    0</code></pre>

<p>The matrix above contains scalar distances. In terms of <strong>embeddings</strong> or vectors, the resulting distance is illustrated in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:relativepos">13.28</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:relativepos"></span>
<img src="relativepos.png" alt="Relative Position Embedding" width="70%" />
<p class="caption">
Figure 13.28: Relative Position Embedding
</p>
</div>
<p>Each word embedding is paired to itself and every other word in the sequence in a pair-wise manner for which a relative distance representation is formed. See the table in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:relativepair">13.29</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:relativepair"></span>
<img src="relativepair.png" alt="Pairwise Relative Relationship" width="100%" />
<p class="caption">
Figure 13.29: Pairwise Relative Relationship
</p>
</div>
<p>Notice that if we pair two embeddings (two vectors), the result is a vector, e.g. (<span class="math inline">\(\vec{\alpha_{1,1}}\)</span>), assuming we pair by element-wise multiplication. Hence, each element in the table in Figure <a href="13.5-transformer-neural-network-tnn.html#fig:relativepair">13.29</a> represents a vector. Furthermore, if we are to derive the relative position embedding for <span class="math inline">\(\mathbf{\vec{e_1}}\)</span>, we have to stack the vectors <span class="math inline">\(\vec{\alpha_{1,1}}, \vec{\alpha_{2,1}}, ..., \vec{\alpha_{t,1}}\)</span> and add the vectors element-wise to obtain the vector <span class="math inline">\(\vec{\alpha_1}\)</span> which we then add to the first word embedding, e.g. <span class="math inline">\(\vec{e_1}\)</span>, to get a <strong>position-aware</strong> embedding. Here, we choose to use addition for the stacked vectors to demonstrate how to get the final relative position embedding. However, it may not be a good idea to stack multiple vectors and add them to the vanilla embedding - this may dilute the influence of the <strong>Word Embedding</strong> over the <strong>Position Embedding</strong>.</p>
<p>An alternative approach is to use the <strong>Attention</strong> mechanism to obtain <strong>Relative Position Embedding</strong>. This idea is popularized by Peter Shaw et al. <span class="citation">(<a href="bibliography.html#ref-ref181p">2018</a>)</span> and by Zhiheng Huang et al. <span class="citation">(<a href="bibliography.html#ref-ref191z">2020</a>)</span>. Here, we perform a dot-product between a matrix and its transpose. The matrix, denoted by (<span class="math inline">\(\mathbf{e}\)</span>), contains the embeddings (in column-wise fashion). See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:attentionpair">13.30</a></p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:attentionpair"></span>
<img src="attentionpair.png" alt="Relative Position using Attention Mechanism" width="100%" />
<p class="caption">
Figure 13.30: Relative Position using Attention Mechanism
</p>
</div>
<p>After performing the dot product, the resulting matrix containing the <strong>re-weighted values</strong> or <strong>score values</strong> denoted by the symbol <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) is then multiplied by the original matrix, e.g. (<span class="math inline">\(\mathbf{e}\)</span>). The operation is also a dot product calculation. Note that our vectors are arranged in a column-wise fashion; thus, the expression below is written as:</p>
<p><span class="math display" id="eq:equate1150093">\[\begin{align}
\alpha = e^\text{T}e\ \ \ \ \rightarrow \ \ \ \ \  c = \alpha \times e^\text{T} \tag{13.96} 
\end{align}\]</span></p>
<p>To illustrate, let us create an <strong>input</strong> matrix with 3 word embeddings that are arranged in column-wise fashion and in sequence, e.g. { <span class="math inline">\(\mathbf{\vec{e_1}}\)</span>, <span class="math inline">\(\mathbf{\vec{e_2}}\)</span>, <span class="math inline">\(\mathbf{\vec{e_3}}\)</span> }, so that, as an example, the first embedding, e.g. (<span class="math inline">\(\mathbf{\vec{e_1}}\)</span>), contains three contextual features (3 dimensions):</p>

<div class="sourceCode" id="cb2165"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2165-1" data-line-number="1">e =<span class="st">  </span>(<span class="kw">array</span>(<span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">9</span>), <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb2165-2" data-line-number="2"><span class="kw">colnames</span>(e) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;e1&quot;</span>, <span class="st">&quot;e2&quot;</span>, <span class="st">&quot;e3&quot;</span>)</a>
<a class="sourceLine" id="cb2165-3" data-line-number="3"><span class="kw">rownames</span>(e) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>)</a>
<a class="sourceLine" id="cb2165-4" data-line-number="4">e</a></code></pre></div>
<pre><code>##          e1 e2 e3
## feature1  1  4  7
## feature2  2  5  8
## feature3  3  6  9</code></pre>

<p>We now perform dot product to get some kind of <strong>score</strong> values like so.</p>
<div class="sourceCode" id="cb2167"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2167-1" data-line-number="1">alpha  =<span class="st"> </span><span class="kw">t</span>(e) <span class="op">%*%</span><span class="st"> </span>(e) <span class="co"># dot product</span></a>
<a class="sourceLine" id="cb2167-2" data-line-number="2"><span class="kw">colnames</span>(alpha) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;a1&quot;</span>, <span class="st">&quot;a2&quot;</span>, <span class="st">&quot;a3&quot;</span>) <span class="co"># scores</span></a>
<a class="sourceLine" id="cb2167-3" data-line-number="3">alpha</a></code></pre></div>
<pre><code>##    a1  a2  a3
## e1 14  32  50
## e2 32  77 122
## e3 50 122 194</code></pre>
<p>Afterwhich, we then obtain a vector output, e.g., <span class="math inline">\(\mathbf{\vec{c_1}}\)</span>, by multiplying each word embedding by each scalar score using the below expression:</p>
<p><span class="math display" id="eq:equate1150094">\[\begin{align}
c_i = \sum_j \left(a_{i, j} \times \vec{e_j}\right) \tag{13.97} 
\end{align}\]</span></p>
<p>We can expand this to show the following solution, along with a simple implementation:</p>
<p><span class="math display" id="eq:eqnnumber804">\[\begin{align}
\vec{c_1} &amp;= \left(a_{1,1} \times \vec{e_1}\right) + \left(a_{1,2} \times \vec{e_2}\right) + \left(a_{1,3} \times \vec{e_3}\right) \tag{13.98}\\
&amp;= 14 \times \left[\begin{array}{l}1 \\ 2 \\ 3\end{array}\right] + 
  32 \times \left[\begin{array}{l}4 \\ 5 \\ 6\end{array}\right]  + 
  50 \times \left[\begin{array}{l}7 \\ 8 \\ 9\end{array}\right] \nonumber \\
&amp;= 
  \left[\begin{array}{l}492 \\ 588 \\ 684\end{array}\right] \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb2169"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2169-1" data-line-number="1">a1 =<span class="st"> </span>alpha[,<span class="dv">1</span>] <span class="co"># scores</span></a>
<a class="sourceLine" id="cb2169-2" data-line-number="2">(<span class="dt">c1 =</span> a1[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a1[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a1[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>])</a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##      492      588      684</code></pre>

<p>We perform the same operations for <span class="math inline">\(\mathbf{\vec{c_2}}\)</span>,</p>
<p><span class="math display" id="eq:equate1150095">\[\begin{align}
\vec{c_2} = \left(a_{2,1} \times \vec{e_1}\right) + \left(a_{2,2} \times \vec{e_2}\right) + \left(a_{2,3} \times \vec{e_3}\right)  \tag{13.99} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2171"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2171-1" data-line-number="1">a2 =<span class="st"> </span>alpha[,<span class="dv">2</span>]</a>
<a class="sourceLine" id="cb2171-2" data-line-number="2">(<span class="dt">c2 =</span> a2[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a2[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a2[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>]) </a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##     1194     1425     1656</code></pre>

<p>and the same operations for <span class="math inline">\(\mathbf{\vec{c_3}}\)</span>.</p>
<p><span class="math display" id="eq:equate1150096">\[\begin{align}
\vec{c_3} = \left(a_{3,1} \times \vec{e_1}\right) + \left(a_{3,2} \times \vec{e_2}\right) + \left(a_{3,3} \times \vec{e_3}\right)  \tag{13.100} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb2173"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2173-1" data-line-number="1">a3 =<span class="st"> </span>alpha[,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb2173-2" data-line-number="2">(<span class="dt">c3 =</span> a3[<span class="dv">1</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">1</span>]  <span class="op">+</span><span class="st"> </span>a3[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>a3[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>e[,<span class="dv">3</span>])</a></code></pre></div>
<pre><code>## feature1 feature2 feature3 
##     1896     2262     2628</code></pre>

<p>Alternatively, we can instead use a dot product between the two matrices like so:</p>

<div class="sourceCode" id="cb2175"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2175-1" data-line-number="1">c =<span class="st"> </span>(alpha) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(e) </a>
<a class="sourceLine" id="cb2175-2" data-line-number="2"><span class="kw">rownames</span>(c) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;c1&quot;</span>, <span class="st">&quot;c2&quot;</span>, <span class="st">&quot;c3&quot;</span>)</a>
<a class="sourceLine" id="cb2175-3" data-line-number="3"><span class="kw">colnames</span>(c) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;feature1&quot;</span>, <span class="st">&quot;feature2&quot;</span>, <span class="st">&quot;feature3&quot;</span>)</a>
<a class="sourceLine" id="cb2175-4" data-line-number="4"><span class="kw">t</span>(c)</a></code></pre></div>
<pre><code>##           c1   c2   c3
## feature1 492 1194 1896
## feature2 588 1425 2262
## feature3 684 1656 2628</code></pre>

<p>The vectors {<span class="math inline">\(\mathbf{\vec{c_1}}\)</span>, <span class="math inline">\(\mathbf{\vec{c_2}}\)</span>, <span class="math inline">\(\mathbf{\vec{c_3}}\)</span>} represent the final <strong>Location-aware word Embeddings</strong>.</p>
<p>In the examples above, we focused on the operations only, but we can add other improvements such as normalization for numerical stability.</p>
<p><span class="math display" id="eq:equate1150097">\[\begin{align}
\alpha = \text{normalize}(e^\text{T}e)\ \ \ \ \rightarrow \ \ \ \ \  c = \alpha \times e^\text{T} \tag{13.101} 
\end{align}\]</span></p>
</div>
<div id="sequence-alignment" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.6</span> Sequence Alignment<a href="13.5-transformer-neural-network-tnn.html#sequence-alignment" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Alignment</strong> is a consideration that applies mostly to <strong>machine translation</strong>. For example, take the following sequence-to-sequence translation:</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{\text{encoder sequence}} &amp;= 
\underbrace{\text{word}_{(1)}, \text{word}_{(2)}}_{\text{context 1}}, 
\underbrace{\text{word}_{(3)}}_{\text{context 2}},
\underbrace{\text{word}_{(4)}, \text{word}_{(5)}, \text{word}_{(6)}}_{\text{context 3}},\underbrace{\text{word}_{(7)}}_{\text{context 4}}\\
\\ 
&amp;\mathbf{\text{encode-decode (translation)}}\\
\\ 
\mathbf{\text{decoder sequence}} &amp;= 
\underbrace{\text{word}_{(1)}}_{\text{context 1}},  
\underbrace{\text{word}_{(2)}, \text{word}_{(3)}}_{\text{context 2}},
\underbrace{\text{word}_{(4)}, \text{word}_{(5)}}_{\text{context 3}},
\underbrace{\text{word}_{(6)}}_{\text{context 4}},
\underbrace{\text{word}_{(7)}}_{\text{context 5}}\\
\end{array}
\]</span></p>
<p>Note in the example above that words are grouped based on <strong>context</strong>. In previous discussions, we used <strong>alignment scores</strong> to identify words that are more aligned to a target word in terms of relevance or attentiveness. Such alignment scores are differently calculated as presented in <strong>Luong</strong> paper.</p>
<p>So far, our discussion around <strong>alignment</strong> only emphasizes the <strong>encoder</strong> side. However, we also have to consider <strong>alignment</strong> on the side of the <strong>decoder</strong>. We have to be able to map both alignments to help with the translation. It, therefore, makes sense to use the <strong>Attention</strong> mechanism on both sides. Thus, one that enforces the idea is shown in the <strong>Transformer</strong> architecture.</p>
</div>
<div id="transformer-architectures" class="section level3 hasAnchor">
<h3><span class="header-section-number">13.5.7</span> Transformer Architectures <a href="13.5-transformer-neural-network-tnn.html#transformer-architectures" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like <strong>CNN</strong> architectures, a few <strong>TNN</strong> architectures made a name for themselves, namely <strong>Bert</strong>, <strong>RoBERTa</strong>, and <strong>GPT</strong>. However, the one architecture that pioneered it is the <strong>Transformer</strong>, introduced in the paper titled <strong>Attention is all you need</strong>. See Figure <a href="13.5-transformer-neural-network-tnn.html#fig:transformer">13.31</a> <span class="citation">(Vaswani A. et al <a href="bibliography.html#ref-ref1339a">2017</a>)</span>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:transformer"></span>
<img src="transformer.png" alt="Transformer Architecture" width="50%" />
<p class="caption">
Figure 13.31: Transformer Architecture
</p>
</div>

<p>The diagram in the figure illustrates an <strong>Encoder-Decoder</strong> model architecture. The <strong>Encoder</strong> and <strong>Decoder</strong> components are enclosed in dashed boxes and are stacked in N identical layers. Each component is made up of a <strong>Multi-head Attention</strong> sublayer with <strong>Add & Norm</strong> unit and a <strong>Feedforward</strong> sublayer with <strong>Add & Norm</strong> unit. However, the <strong>encoder</strong> has one additional <strong>Multi-head Attention</strong> sublayer, as shown in the diagram.  Note and recall the concept of <strong>Residual</strong> discussed in previous sections being added to the network. That is the <strong>Add</strong> unit included in the component design.
</p>

<p>The design shows that the encoder takes an input embedding, and the decoder takes a right-shifted output embedding. The embeddings combine with their corresponding sinusoidal encoding forming the positional embeddings. Each layer in the N identical layers captures and learns different representations of each sequence relayed from the previous layer. At the same time, each head in the multi-head attention sublayer captures and learns different parts of the input sequence.  The second multi-head attention sublayer in the decoder is also called the encoder-decoder attention sublayer since it takes the encoder's output along with the output of its first self-attention sublayer, masked in the case of being in an autoregressive mode. The output of the encoder-decoder transformer produces an outcome of probabilities via softmax.</p>

<p>Note that there are <strong>Large Language Models (LLM)</strong> that are built based on this architecture. <strong>Bidrectional Encoder Representations from Transformers (BERT)</strong> and <strong>Generative Pre-Trained Transformer (GPT)</strong> are two famous architectures built based on variants of the <strong>Transformer</strong> architecture. Many <strong>LLMs</strong> leverage these base models by fine-tuning them to create new models for downstream tasks such as <strong>Text Generation</strong>, <strong>Summarization</strong>, <strong>Classification</strong>, <strong>Answer-Questioning</strong>, and many other tasks.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="13.4-deep-stacked-bidirectional-rnn.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="13.6-applications-using-tnn-and-rnn.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
