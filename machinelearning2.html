<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 10 Computational Learning II | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 10 Computational Learning II | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Computational Learning II | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machinelearning1.html"/>
<link rel="next" href="machinelearning3.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="machinelearning2" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 10</span> Computational Learning II<a href="machinelearning2.html#machinelearning2" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '10.'+n}
      } 
  }
});
</script>
<p>In this chapter, we continue to discuss <strong>Computational Learning</strong>, emphasizing <strong>Regression</strong> and <strong>Classification</strong>. Note that in <strong>Learning</strong>, we emphasize the <strong>Generalization Ability</strong> of our models <span class="citation">(Vapnik V. <a href="bibliography.html#ref-ref572v">2000</a>)</span>. </p>
<div id="regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">10.1</span> Regression (Supervised)<a href="machinelearning2.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Our simple definition of <strong>Regression</strong> is finding an estimated value closest to the actual value. With that definition, the <strong>Gauss-Markov</strong> theorem is worth mentioning, which stresses the idea of finding the most minimum variance to produce the best-unbiased estimation <span class="citation">(Halliwell L. J. <a href="bibliography.html#ref-ref2166l">2015</a>; Ang A. <a href="bibliography.html#ref-ref2159a">2014</a>)</span>. It may be fair to say that <strong>Regression</strong> intends to achieve the same basic goal - the so-called <strong>bias-variance trade-off</strong>. </p>
<p>In previous chapters, we discuss the use of <strong>Ordinary Least Square</strong> as a classic <strong>linear regression</strong> solution. We also discuss using <strong>Polynomial Regression</strong> as a solution for low and high-degree polynomials. Then we cover <strong>Kernel functions</strong> and <strong>Splines</strong> as additional methods to solve irregular non-linear cases. All that illustrates <strong>Regression</strong> in detail for linear, regular non-linear, and irregular non-linear cases.</p>
<p>In the following few sections, we delve into the inner workings of a few popular regression algorithms. The emphasis is on the strategies used by the algorithms to balance <strong>bias</strong> and <strong>variance</strong> such that regression models are generalized to a certain degree.</p>
<div id="regression-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.1</span> Regression Trees <a href="machinelearning2.html#regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>It becomes a challenge to use <strong>simple</strong> or <strong>multiple linear regression</strong> to predict an outcome for our <strong>response variable</strong>, especially when we begin to form and encounter highly complex interactions of a large number of features. In this circumstance, a <strong>Regression Tree</strong>, also called <strong>Decision Tree</strong>, is one of many other better <strong>ML algorithmic</strong> alternatives to use. A <strong>Regression Tree</strong> allows for handling irregular non-linear complex datasets and allows better interpretability.</p>
<p>In this section, we discuss one classic tree algorithm used to build a regression, namely <strong>Classification and Regression Tree (CART)</strong>. While <strong>CART</strong> can handle both classification and regression, let us discuss <strong>Decision Trees</strong> for regression only. For classification, we mention three other classic tree algorithms, namely, <strong>ID3</strong>, <strong>C4.5</strong>, and <strong>C5.0</strong>. We cover them in a later section ahead under <strong>Classification Trees</strong>.      </p>
<p>Figure <a href="machinelearning2.html#fig:regressiontree1">10.1</a> illustrates a <strong>Regression Tree</strong> modeled using dataset <strong>mtcars</strong> to perform prediction. Here, we use a function called <strong>rpart(.)</strong> to model the tree and another function called <strong>rpart.plot</strong> to plot the tree <span class="citation">(Therneau T. et al. <a href="bibliography.html#ref-ref480">2019</a>)</span>.</p>

<div class="sourceCode" id="cb1227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1227-1" data-line-number="1"><span class="kw">library</span>(rpart); <span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb1227-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1227-3" data-line-number="3">mtcars2 =<span class="st"> </span>mtcars[, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;cyl&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;drat&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;qsec&quot;</span>)]</a>
<a class="sourceLine" id="cb1227-4" data-line-number="4">datacars =<span class="st"> </span>mtcars2 =<span class="st"> </span><span class="kw">within</span>(mtcars2, {  cyl  &lt;-<span class="st"> </span><span class="kw">as.factor</span>(cyl) })</a>
<a class="sourceLine" id="cb1227-5" data-line-number="5">rpart.control =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>, </a>
<a class="sourceLine" id="cb1227-6" data-line-number="6">                              <span class="dt">minbucket =</span> <span class="dv">2</span>, <span class="dt">cp=</span><span class="dv">0</span>, <span class="dt">xval=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1227-7" data-line-number="7">tree.model    =<span class="st"> </span><span class="kw">rpart</span>(mpg <span class="op">~</span><span class="st"> </span>., </a>
<a class="sourceLine" id="cb1227-8" data-line-number="8">                <span class="dt">data =</span> datacars, <span class="dt">control =</span> rpart.control)</a>
<a class="sourceLine" id="cb1227-9" data-line-number="9"><span class="kw">rpart.plot</span>(tree.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressiontree1"></span>
<img src="DS_files/figure-html/regressiontree1-1.png" alt="Regression Tree" width="70%" />
<p class="caption">
Figure 10.1: Regression Tree
</p>
</div>

<p>A tree consists of two types of nodes, namely <strong>decision nodes</strong> and <strong>leaf or terminal nodes</strong>. In our example, the tree is a <strong>CART</strong> tree - a binary tree that starts with a <strong>root node</strong> - the first decision node that splits into two child nodes. A child node can be another decision node or a terminal node that does not branch further.</p>
<p>Note that <strong>CART</strong> is a greedy algorithm using a <strong>recursive binary splitting</strong> technique. Here, we discuss the algorithm of <strong>splitting a tree</strong> and <strong>pruning a tree</strong>.</p>
<p>Note that we utilize a few helper functions for constructing a regression tree such as <strong>split.input(.)</strong>, <strong>cost.rule(.)</strong>, <strong>split.goodness(.)</strong>, <strong>rank.importance(.)</strong>, and <strong>my.tree(.)</strong>. However, we naively implement them to focus more on certain main points of the algorithm. Such helper functions do not account for optimization, and thus they are limited in terms of recognizing only the following <strong>model parameters</strong>:</p>
<ul>
<li><strong>minbucket</strong> - minimum observation of a node.</li>
<li><strong>maxdepth</strong> - maximum depth of a tree.</li>
</ul>
<p>The <strong>rpart(.)</strong> uses a separate parameter called <strong>minsplit</strong> that controls the minimum number of observations allowed in a decision node. Our implementation uses only <strong>minbucket</strong> for both decision and terminal nodes.</p>
<p><strong>Splitting a Tree Node and Building a Tree</strong></p>
<p><strong>First</strong>, suppose we use any arbitrary single input variable, namely <strong>wt</strong> in this case, and also choose any arbitrary location to split the data. For example, suppose we split the input data evenly so that the split is located between a chosen pair of input points, namely the sixth and seventh input points. Note that duplicated data points are skipped along the way.</p>

<div class="sourceCode" id="cb1228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1228-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">60</span>)</a>
<a class="sourceLine" id="cb1228-2" data-line-number="2">wt =<span class="st"> </span>datacars[[<span class="st">&quot;wt&quot;</span>]]</a>
<a class="sourceLine" id="cb1228-3" data-line-number="3">(<span class="dt">sorted.input =</span> <span class="kw">sort</span>(wt, <span class="dt">index.return =</span> <span class="ot">TRUE</span>))<span class="op">$</span>x</a></code></pre></div>
<pre><code>##  [1] 1.513 1.615 1.835 1.935 2.140 2.200 2.320 2.465 2.620
## [10] 2.770 2.780 2.875 3.150 3.170 3.190 3.215 3.435 3.440
## [19] 3.440 3.440 3.460 3.520 3.570 3.570 3.730 3.780 3.840
## [28] 3.845 4.070 5.250 5.345 5.424</code></pre>
<div class="sourceCode" id="cb1230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1230-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">60</span>)</a>
<a class="sourceLine" id="cb1230-2" data-line-number="2">split.index =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb1230-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;point.1&quot;</span>   =<span class="st"> </span>sorted.input<span class="op">$</span>x[split.index],  </a>
<a class="sourceLine" id="cb1230-4" data-line-number="4">  <span class="st">&quot;point.2&quot;</span>   =<span class="st"> </span>sorted.input<span class="op">$</span>x[split.index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>],</a>
<a class="sourceLine" id="cb1230-5" data-line-number="5">  <span class="st">&quot;left.obs&quot;</span>  =<span class="st"> </span><span class="kw">length</span>(sorted.input<span class="op">$</span>x[<span class="dv">1</span><span class="op">:</span>split.index]),</a>
<a class="sourceLine" id="cb1230-6" data-line-number="6">  <span class="st">&quot;right.obs&quot;</span> =<span class="st"> </span><span class="kw">length</span>(sorted.input<span class="op">$</span>x</a>
<a class="sourceLine" id="cb1230-7" data-line-number="7">                       [(split.index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span><span class="kw">length</span>(sorted.input<span class="op">$</span>x)]),</a>
<a class="sourceLine" id="cb1230-8" data-line-number="8">  <span class="st">&quot;split.avg&quot;</span> =<span class="st"> </span>(sorted.input<span class="op">$</span>x[split.index] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1230-9" data-line-number="9"><span class="st">                   </span>sorted.input<span class="op">$</span>x[split.index <span class="op">+</span><span class="st"> </span><span class="dv">1</span>])<span class="op">/</span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>##   point.1   point.2  left.obs right.obs split.avg 
##      2.20      2.32      6.00     26.00      2.26</code></pre>

<p>The average of the two input points is 2.26. We then use this average as a <strong>decision criterion</strong> to decide which group to assign for a data point. Any input less than the average goes to the left group, and the rest to the right group. Below is our implementation of the splitting criterion. What we get are the indices of the left and right groups. </p>

<div class="sourceCode" id="cb1232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1232-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">60</span>)</a>
<a class="sourceLine" id="cb1232-2" data-line-number="2">split.input &lt;-<span class="st"> </span><span class="cf">function</span>(sorted.input, idx, <span class="dt">is.factor =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1232-3" data-line-number="3">  n =<span class="st"> </span><span class="kw">length</span>(sorted.input<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb1232-4" data-line-number="4">  left.indices  =<span class="st">  </span>sorted.input<span class="op">$</span>ix[<span class="dv">1</span><span class="op">:</span>idx]</a>
<a class="sourceLine" id="cb1232-5" data-line-number="5">  right.indices =<span class="st">  </span>sorted.input<span class="op">$</span>ix[(idx<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n]</a>
<a class="sourceLine" id="cb1232-6" data-line-number="6">  <span class="kw">list</span>(<span class="st">&quot;left&quot;</span> =<span class="st"> </span>left.indices, <span class="st">&quot;right&quot;</span> =<span class="st"> </span>right.indices )</a>
<a class="sourceLine" id="cb1232-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1232-8" data-line-number="8">(<span class="dt">split =</span> <span class="kw">split.input</span>(sorted.input, split.index ))</a></code></pre></div>
<pre><code>## $left
## [1] 28 19 20 26 27 18
## 
## $right
##  [1]  3 21  1 30 32  2  9 29  8  4 23  5 10 11  6 22  7 31
## [19] 13 14 24 25 12 15 17 16</code></pre>

<p>We can use the indices to split our <strong>target (or response)</strong> variable like so:</p>

<div class="sourceCode" id="cb1234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1234-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">60</span>)</a>
<a class="sourceLine" id="cb1234-2" data-line-number="2"><span class="co"># all target values belonging to the left side</span></a>
<a class="sourceLine" id="cb1234-3" data-line-number="3">(<span class="dt">left.data =</span> datacars[split<span class="op">$</span>left, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)])</a></code></pre></div>
<pre><code>## [1] 30.4 30.4 33.9 27.3 26.0 32.4</code></pre>
<div class="sourceCode" id="cb1236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1236-1" data-line-number="1"><span class="co"># all target values belonging to the right side</span></a>
<a class="sourceLine" id="cb1236-2" data-line-number="2">(<span class="dt">right.data =</span> datacars[split<span class="op">$</span>right, <span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)])</a></code></pre></div>
<pre><code>##  [1] 22.8 21.5 21.0 19.7 21.4 21.0 22.8 15.8 24.4 21.4 15.2
## [12] 18.7 19.2 17.8 18.1 15.5 14.3 15.0 17.3 15.2 13.3 19.2
## [23] 16.4 10.4 14.7 10.4</code></pre>
<p></p>
<p><strong>Second</strong>, we use Figure <a href="machinelearning2.html#fig:regressiontree">10.2</a> as reference to measure the goodness of a split.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressiontree"></span>
<img src="regressiontree.png" alt="Cost Computation" width="70%" />
<p class="caption">
Figure 10.2: Cost Computation
</p>
</div>
<p>We explain <strong>classification tree</strong> in Chapter , but here, we focus on <strong>Regression tree</strong>. In the case of <strong>regression</strong>, we use the <strong>sum squared error</strong> (SSE), also called <strong>sum squared residual</strong> (SSR) to measure the goodness of fit - this is our <strong>splitting criterion</strong>.     </p>
<p><span class="math display">\[\begin{align}
SSE_{(split)}= SSE_{(left)} + SSE_{(right)} 
= \sum_{i:L.split}^n\left( y_i - \bar{y}_L\right)^2 +  \sum_{i:R.split}^m\left( y_i - \bar{y}_R\right)^2
\end{align}\]</span></p>
<p>where <span class="math inline">\(L.split = x \in L_{(split)}\)</span> and <span class="math inline">\(R.split = x \in R_{(split)}\)</span>.</p>
<p>We have the following implementation of measuring <strong>SSE</strong> for our simple split: </p>

<div class="sourceCode" id="cb1238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1238-1" data-line-number="1">left.sse =<span class="st"> </span><span class="kw">sum</span>((left.data <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(left.data))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1238-2" data-line-number="2">right.sse =<span class="st"> </span><span class="kw">sum</span>((right.data <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(right.data))<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1238-3" data-line-number="3">(<span class="dt">split.sse =</span> left.sse <span class="op">+</span><span class="st"> </span>right.sse)</a></code></pre></div>
<pre><code>## [1] 391.1</code></pre>

<p>Ultimately, our goal is to minimize <strong>SSE</strong> such that the optimal split renders the least <strong>SSE</strong>. Here, we use an example implementation of our loss function for the split. The object of interest is the computation of <strong>SSE</strong> and <strong>improvement of deviance</strong> - to be discussed further. </p>

<div class="sourceCode" id="cb1240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1240-1" data-line-number="1">sample =<span class="st"> </span><span class="kw">nrow</span>(datacars)</a>
<a class="sourceLine" id="cb1240-2" data-line-number="2">split.loss &lt;-<span class="st"> </span><span class="cf">function</span>(loss, sort.input, output, left, right, </a>
<a class="sourceLine" id="cb1240-3" data-line-number="3">                       avg, minbucket ) {</a>
<a class="sourceLine" id="cb1240-4" data-line-number="4">  cs =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1240-5" data-line-number="5">  n =<span class="st"> </span><span class="kw">length</span>(output)</a>
<a class="sourceLine" id="cb1240-6" data-line-number="6">  parent.mean =<span class="st"> </span><span class="kw">mean</span>(output)</a>
<a class="sourceLine" id="cb1240-7" data-line-number="7">  parent.sse  =<span class="st"> </span><span class="kw">sum</span>((output <span class="op">-</span><span class="st"> </span>parent.mean)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1240-8" data-line-number="8">  <span class="cf">if</span> (<span class="kw">length</span>(left) <span class="op">&gt;=</span><span class="st"> </span>minbucket <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(right) <span class="op">&gt;=</span><span class="st"> </span>minbucket ) {</a>
<a class="sourceLine" id="cb1240-9" data-line-number="9">      left.idx  =<span class="st"> </span>sort.input<span class="op">$</span>ix[left]</a>
<a class="sourceLine" id="cb1240-10" data-line-number="10">      right.idx =<span class="st"> </span>sort.input<span class="op">$</span>ix[<span class="op">-</span>left]</a>
<a class="sourceLine" id="cb1240-11" data-line-number="11">      o1 =<span class="st"> </span>output[left.idx]; o2 =<span class="st"> </span>output[right.idx]</a>
<a class="sourceLine" id="cb1240-12" data-line-number="12">      m1 =<span class="st"> </span><span class="kw">mean</span>(o1);         m2 =<span class="st"> </span><span class="kw">mean</span>(o2)</a>
<a class="sourceLine" id="cb1240-13" data-line-number="13">      <span class="cf">if</span> (parent.mean <span class="op">!=</span><span class="st"> </span>m1 <span class="op">||</span><span class="st"> </span>parent.mean <span class="op">!=</span><span class="st"> </span>m2) {</a>
<a class="sourceLine" id="cb1240-14" data-line-number="14">          child.sse     =<span class="st"> </span><span class="kw">sum</span>((o1 <span class="op">-</span><span class="st"> </span>m1)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>((o2 <span class="op">-</span><span class="st"> </span>m2)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1240-15" data-line-number="15">          child.improve =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(child.sse <span class="op">/</span><span class="st"> </span>parent.sse)</a>
<a class="sourceLine" id="cb1240-16" data-line-number="16">          o.len =<span class="st"> </span>n; l.len =<span class="st"> </span><span class="kw">length</span>(o1); r.len =<span class="st"> </span><span class="kw">length</span>(o2)</a>
<a class="sourceLine" id="cb1240-17" data-line-number="17">          cs<span class="op">$</span>split   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>split, avg)</a>
<a class="sourceLine" id="cb1240-18" data-line-number="18">          cs<span class="op">$</span>sse     =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>sse, <span class="kw">round</span>(child.sse,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1240-19" data-line-number="19">          cs<span class="op">$</span>improve =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>improve, <span class="kw">round</span>(child.improve,<span class="dv">6</span>))</a>
<a class="sourceLine" id="cb1240-20" data-line-number="20">          cs<span class="op">$</span>obs     =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>obs, o.len);  </a>
<a class="sourceLine" id="cb1240-21" data-line-number="21">          cs<span class="op">$</span>l.ymean =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>l.ymean, m1)</a>
<a class="sourceLine" id="cb1240-22" data-line-number="22">          cs<span class="op">$</span>r.ymean =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>r.ymean, m2)</a>
<a class="sourceLine" id="cb1240-23" data-line-number="23">          cs<span class="op">$</span>l.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>l.son, l.len)</a>
<a class="sourceLine" id="cb1240-24" data-line-number="24">          cs<span class="op">$</span>r.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>r.son, r.len)</a>
<a class="sourceLine" id="cb1240-25" data-line-number="25">          cs<span class="op">$</span>left.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>left.idx</a>
<a class="sourceLine" id="cb1240-26" data-line-number="26">          cs<span class="op">$</span>right.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>right.idx</a>
<a class="sourceLine" id="cb1240-27" data-line-number="27">      }</a>
<a class="sourceLine" id="cb1240-28" data-line-number="28">  }</a>
<a class="sourceLine" id="cb1240-29" data-line-number="29">  cs</a>
<a class="sourceLine" id="cb1240-30" data-line-number="30">}</a></code></pre></div>

<p>We use the following rules to implement minimization. Additionally, we use the averages of each split to determine the direction (whether towards the left or right). Subsequently, we swap the direction to ensure the split with a lesser average goes towards the left. Here, we optimize by minimizing loss:</p>

<div class="sourceCode" id="cb1241"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1241-1" data-line-number="1">minimum.loss &lt;-<span class="st"> </span><span class="cf">function</span>(feature, loss, output, data, factor) {</a>
<a class="sourceLine" id="cb1241-2" data-line-number="2">    cs           =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1241-3" data-line-number="3">    indices      =<span class="st"> </span>data<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1241-4" data-line-number="4">    data         =<span class="st"> </span>data<span class="op">$</span>dataset[indices,]</a>
<a class="sourceLine" id="cb1241-5" data-line-number="5">    parent.mean  =<span class="st"> </span><span class="kw">mean</span>(output)</a>
<a class="sourceLine" id="cb1241-6" data-line-number="6">    parent.sse   =<span class="st"> </span><span class="kw">sum</span>((output <span class="op">-</span><span class="st"> </span>parent.mean)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1241-7" data-line-number="7">    parent.mse   =<span class="st"> </span><span class="kw">mean</span>((output <span class="op">-</span><span class="st"> </span>parent.mean)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1241-8" data-line-number="8">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(cs<span class="op">$</span>sse) <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(output) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1241-9" data-line-number="9">        min.idx  =<span class="st"> </span><span class="kw">which.min</span>(cs<span class="op">$</span>sse)  </a>
<a class="sourceLine" id="cb1241-10" data-line-number="10">        avg      =<span class="st"> </span><span class="kw">as.character</span>(cs<span class="op">$</span>split[min.idx])</a>
<a class="sourceLine" id="cb1241-11" data-line-number="11">        perc     =<span class="st"> </span><span class="kw">round</span>(cs<span class="op">$</span>obs[min.idx] <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1241-12" data-line-number="12">        lson     =<span class="st"> </span>cs<span class="op">$</span>l.son[min.idx]</a>
<a class="sourceLine" id="cb1241-13" data-line-number="13">        rson     =<span class="st"> </span>cs<span class="op">$</span>r.son[min.idx]</a>
<a class="sourceLine" id="cb1241-14" data-line-number="14">        lindices =<span class="st"> </span>cs<span class="op">$</span>left.indices[[avg]]</a>
<a class="sourceLine" id="cb1241-15" data-line-number="15">        lindices  =<span class="st"> </span>indices[lindices]</a>
<a class="sourceLine" id="cb1241-16" data-line-number="16">        rindices =<span class="st"> </span>cs<span class="op">$</span>right.indices[[avg]]</a>
<a class="sourceLine" id="cb1241-17" data-line-number="17">        rindices  =<span class="st"> </span>indices[rindices]</a>
<a class="sourceLine" id="cb1241-18" data-line-number="18">        node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span> =<span class="st"> </span>feature,         <span class="st">&quot;split&quot;</span> =<span class="st"> </span>avg,  </a>
<a class="sourceLine" id="cb1241-19" data-line-number="19">         <span class="st">&quot;obs&quot;</span>           =<span class="st"> </span>cs<span class="op">$</span>obs[min.idx],       <span class="st">&quot;left&quot;</span> =<span class="st"> </span>lson, </a>
<a class="sourceLine" id="cb1241-20" data-line-number="20">         <span class="st">&quot;right&quot;</span>         =<span class="st"> </span>rson,                   </a>
<a class="sourceLine" id="cb1241-21" data-line-number="21">         <span class="st">&quot;sse&quot;</span>           =<span class="st"> </span>cs<span class="op">$</span>sse[min.idx],  </a>
<a class="sourceLine" id="cb1241-22" data-line-number="22">         <span class="st">&quot;ymean&quot;</span>         =<span class="st"> </span><span class="kw">round</span>(parent.mean,<span class="dv">2</span>),   </a>
<a class="sourceLine" id="cb1241-23" data-line-number="23">         <span class="st">&quot;mse&quot;</span>           =<span class="st"> </span><span class="kw">round</span>(parent.mse,<span class="dv">6</span>),</a>
<a class="sourceLine" id="cb1241-24" data-line-number="24">         <span class="st">&quot;improve&quot;</span>       =<span class="st"> </span>cs<span class="op">$</span>improve[min.idx],   <span class="st">&quot;perc&quot;</span> =<span class="st"> </span>perc,</a>
<a class="sourceLine" id="cb1241-25" data-line-number="25">         <span class="st">&quot;left.indices&quot;</span>  =<span class="st"> </span>lindices,     <span class="st">&quot;right.indices&quot;</span> =<span class="st"> </span>rindices,</a>
<a class="sourceLine" id="cb1241-26" data-line-number="26">         <span class="st">&quot;indices&quot;</span>       =<span class="st"> </span>indices,           <span class="st">&quot;response&quot;</span> =<span class="st"> </span>output,</a>
<a class="sourceLine" id="cb1241-27" data-line-number="27">         <span class="st">&quot;ntype&quot;</span> =<span class="st"> &quot;node&quot;</span>)</a>
<a class="sourceLine" id="cb1241-28" data-line-number="28">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1241-29" data-line-number="29">         node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span>  =<span class="st"> </span>feature, <span class="st">&quot;split&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1241-30" data-line-number="30">            <span class="st">&quot;obs&quot;</span>       =<span class="st"> </span><span class="kw">length</span>(output),   <span class="st">&quot;left&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1241-31" data-line-number="31">            <span class="st">&quot;right&quot;</span>     =<span class="st"> &quot;.&quot;</span>,       </a>
<a class="sourceLine" id="cb1241-32" data-line-number="32">            <span class="st">&quot;sse&quot;</span>       =<span class="st"> </span><span class="kw">round</span>(parent.sse,<span class="dv">4</span>),</a>
<a class="sourceLine" id="cb1241-33" data-line-number="33">            <span class="st">&quot;ymean&quot;</span>     =<span class="st"> </span><span class="kw">round</span>(parent.mean,<span class="dv">2</span>),  </a>
<a class="sourceLine" id="cb1241-34" data-line-number="34">            <span class="st">&quot;mse&quot;</span>       =<span class="st"> </span><span class="kw">round</span>(parent.mse, <span class="dv">6</span>),</a>
<a class="sourceLine" id="cb1241-35" data-line-number="35">            <span class="st">&quot;improve&quot;</span>   =<span class="st"> &quot;.&quot;</span>, </a>
<a class="sourceLine" id="cb1241-36" data-line-number="36">            <span class="st">&quot;perc&quot;</span>      =<span class="st"> </span><span class="kw">round</span>( <span class="kw">length</span>(output) <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1241-37" data-line-number="37">            <span class="st">&quot;indices&quot;</span>   =<span class="st"> </span>indices, <span class="st">&quot;response&quot;</span>  =<span class="st"> </span>output,</a>
<a class="sourceLine" id="cb1241-38" data-line-number="38">            <span class="st">&quot;ntype&quot;</span>     =<span class="st"> &quot;leaf&quot;</span>)</a>
<a class="sourceLine" id="cb1241-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb1241-40" data-line-number="40">    node</a>
<a class="sourceLine" id="cb1241-41" data-line-number="41">}</a>
<a class="sourceLine" id="cb1241-42" data-line-number="42">optimizer &lt;-<span class="st"> </span>minimum.loss</a></code></pre></div>

<p>Let us use the cost function to get details of the split at index 6:</p>

<div class="sourceCode" id="cb1242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1242-1" data-line-number="1">feature =<span class="st"> &quot;wt&quot;</span>; response =<span class="st"> &quot;mpg&quot;</span></a>
<a class="sourceLine" id="cb1242-2" data-line-number="2">split.index =<span class="st"> </span>i =<span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb1242-3" data-line-number="3">input =<span class="st"> </span>datacars[,<span class="kw">c</span>(feature)]</a>
<a class="sourceLine" id="cb1242-4" data-line-number="4">output =<span class="st"> </span>datacars[,<span class="kw">c</span>(response)]</a>
<a class="sourceLine" id="cb1242-5" data-line-number="5">sort.input  =<span class="st"> </span><span class="kw">sort</span>(input, <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1242-6" data-line-number="6">avg   =<span class="st"> </span>(sort.input<span class="op">$</span>x[i] <span class="op">+</span><span class="st"> </span>sort.input<span class="op">$</span>x[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1242-7" data-line-number="7">left  =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&lt;</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1242-8" data-line-number="8">right =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&gt;=</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1242-9" data-line-number="9">loss  =<span class="st"> </span><span class="kw">split.loss</span>( <span class="kw">list</span>(), sort.input, output, left, right, avg, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1242-10" data-line-number="10">loss<span class="op">$</span>left.indices =<span class="st"> </span><span class="ot">NULL</span>; loss<span class="op">$</span>right.indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1242-11" data-line-number="11"><span class="kw">t</span>(<span class="kw">as.matrix</span>(loss))</a></code></pre></div>
<pre><code>##      split sse   improve obs l.ymean r.ymean l.son r.son
## [1,] 2.26  391.1 0.6527  32  30.07   17.79   6     26</code></pre>

<p>Here, <strong>SSE</strong> is the <strong>criterion</strong> for the goodness of split, which gives us: 391.1199.</p>
<p>However, we may not know if the obtained SSE concludes an optimal split. Ideally, we need to know all the possible splits and thus compare all the corresponding SSEs. The idea is to find the minimum SSE, which becomes the choice of split to use. To do that, let us use two cost functions, namely <strong>split.continuous(.)</strong> to handle continuous input variables and <strong>split.categorical(.)</strong> to handle categorical input variables.</p>
<p>For <strong>continuous input</strong>, we step through each pair of sorted input values, computing for the combined SSE of the binary splits.</p>

<div class="sourceCode" id="cb1244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1244-1" data-line-number="1">split.continuous &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket) {</a>
<a class="sourceLine" id="cb1244-2" data-line-number="2">    indices =<span class="st"> </span>data<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1244-3" data-line-number="3">    input   =<span class="st"> </span>data<span class="op">$</span>dataset[indices,<span class="kw">c</span>(feature)]</a>
<a class="sourceLine" id="cb1244-4" data-line-number="4">    output  =<span class="st"> </span>data<span class="op">$</span>dataset[indices,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1244-5" data-line-number="5">    n =<span class="st"> </span><span class="kw">length</span>(input)</a>
<a class="sourceLine" id="cb1244-6" data-line-number="6">    sort.input  =<span class="st"> </span><span class="kw">sort</span>(input, <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1244-7" data-line-number="7">    <span class="cf">if</span> (n <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { </a>
<a class="sourceLine" id="cb1244-8" data-line-number="8">      avg =<span class="st"> </span>sort.input<span class="op">$</span>x</a>
<a class="sourceLine" id="cb1244-9" data-line-number="9">      loss  =<span class="st"> </span><span class="kw">split.loss</span>( loss, sort.input, output, <span class="ot">NULL</span>, <span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1244-10" data-line-number="10">                          avg, minbucket)</a>
<a class="sourceLine" id="cb1244-11" data-line-number="11">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1244-12" data-line-number="12">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb1244-13" data-line-number="13">     <span class="cf">if</span> (sort.input<span class="op">$</span>x[i] <span class="op">!=</span><span class="st"> </span>sort.input<span class="op">$</span>x[i<span class="op">+</span><span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb1244-14" data-line-number="14">        avg   =<span class="st"> </span>(sort.input<span class="op">$</span>x[i] <span class="op">+</span><span class="st"> </span>sort.input<span class="op">$</span>x[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1244-15" data-line-number="15">        left  =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&lt;</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1244-16" data-line-number="16">        right =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&gt;=</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1244-17" data-line-number="17">        loss  =<span class="st"> </span><span class="kw">split.loss</span>( loss, sort.input, output, left, right, </a>
<a class="sourceLine" id="cb1244-18" data-line-number="18">                            avg, minbucket)</a>
<a class="sourceLine" id="cb1244-19" data-line-number="19">     }</a>
<a class="sourceLine" id="cb1244-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb1244-21" data-line-number="21">    <span class="kw">optimizer</span>(feature, loss, output, data, <span class="dt">factor=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1244-22" data-line-number="22">}</a></code></pre></div>

<p>For n-observations of a continuous input variable, there are n-1 splits to consider. However, if the number of continuous observations is large, it helps to discretize, e.g., use percentile (5%, 10%, 15%, and on ).</p>
<p>For <strong>categorical input</strong>, we can choose to consider all possible splitting permutations for the categories so that for our three levels (A, B, C), we can have the following splits: LRR, RLR, RRL, LLR, RLL, LRL. That is <span class="math inline">\(P(n,n) = 6\)</span> permutations. However, imagine if we have a larger category, for example, zip codes, then it becomes computationally expensive to compute the goodness of fit for nominal categories. One way to reduce computation is to group the categorical input by the average of the output and then map the averages back to the corresponding input values (e.g., transforming to continuous) like so:</p>

<div class="sourceCode" id="cb1245"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1245-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">220</span>)</a>
<a class="sourceLine" id="cb1245-2" data-line-number="2">(<span class="dt">input =</span> <span class="kw">sample</span>(<span class="kw">c</span>(<span class="st">&#39;A&#39;</span>,<span class="st">&#39;B&#39;</span>,<span class="st">&#39;C&#39;</span>), <span class="dt">size=</span><span class="dv">10</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>##  [1] &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;C&quot; &quot;C&quot; &quot;A&quot; &quot;A&quot; &quot;B&quot;</code></pre>
<div class="sourceCode" id="cb1247"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1247-1" data-line-number="1">(<span class="dt">output =</span> <span class="kw">round</span>(<span class="kw">runif</span>( <span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">min=</span><span class="dv">20</span>, <span class="dt">max=</span><span class="dv">25</span>),<span class="dv">1</span>))</a></code></pre></div>
<pre><code>##  [1] 20.1 24.9 20.8 24.4 22.8 20.9 21.8 20.8 20.7 24.3</code></pre>
<div class="sourceCode" id="cb1249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1249-1" data-line-number="1"><span class="co"># group by average</span></a>
<a class="sourceLine" id="cb1249-2" data-line-number="2">(<span class="dt">output.grp.mean =</span> <span class="kw">tapply</span>(output, input, mean)) </a></code></pre></div>
<pre><code>##     A     B     C 
## 21.76 24.30 22.10</code></pre>
<div class="sourceCode" id="cb1251"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1251-1" data-line-number="1"><span class="co"># map the averages back to the input (transform to continuous)</span></a>
<a class="sourceLine" id="cb1251-2" data-line-number="2">output.grp.mean[input] </a></code></pre></div>
<pre><code>##     A     C     C     A     A     C     C     A     A     B 
## 21.76 22.10 22.10 21.76 21.76 22.10 22.10 21.76 21.76 24.30</code></pre>

<p>We then sort the newly transformed input accordingly and step through the splitting.</p>
<p>Before running a categorical split, it helps to acquire a complete list of all categories for all categorical inputs. The implementation below allows us to do just that:</p>

<div class="sourceCode" id="cb1253"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1253-1" data-line-number="1">get.categories &lt;-<span class="st"> </span><span class="cf">function</span>(target, dataset) {</a>
<a class="sourceLine" id="cb1253-2" data-line-number="2">  categories =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1253-3" data-line-number="3">  features =<span class="st"> </span><span class="op">!</span>(<span class="kw">names</span>(dataset) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(target))</a>
<a class="sourceLine" id="cb1253-4" data-line-number="4">  features =<span class="st"> </span><span class="kw">names</span>(dataset)[features]</a>
<a class="sourceLine" id="cb1253-5" data-line-number="5">  <span class="cf">for</span> (feature <span class="cf">in</span> features) {</a>
<a class="sourceLine" id="cb1253-6" data-line-number="6">      f =<span class="st"> </span><span class="kw">is.factor</span>(dataset[<span class="dv">1</span>,<span class="kw">c</span>(feature)])</a>
<a class="sourceLine" id="cb1253-7" data-line-number="7">      <span class="cf">if</span> (f <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1253-8" data-line-number="8">          categories[[feature]] =<span class="st"> </span><span class="kw">levels</span>(dataset[,<span class="kw">c</span>(feature)])</a>
<a class="sourceLine" id="cb1253-9" data-line-number="9">      }</a>
<a class="sourceLine" id="cb1253-10" data-line-number="10">  }</a>
<a class="sourceLine" id="cb1253-11" data-line-number="11">  categories</a>
<a class="sourceLine" id="cb1253-12" data-line-number="12">}</a>
<a class="sourceLine" id="cb1253-13" data-line-number="13"><span class="kw">get.categories</span>(<span class="st">&quot;mpg&quot;</span>, datacars)</a></code></pre></div>
<pre><code>## $cyl
## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p>We then use the complete list of categories as a reference when splitting categorical inputs. See our example implementation below:</p>

<div class="sourceCode" id="cb1255"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1255-1" data-line-number="1">split.categorical &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket, </a>
<a class="sourceLine" id="cb1255-2" data-line-number="2">                              categories) {</a>
<a class="sourceLine" id="cb1255-3" data-line-number="3">    indices =<span class="st"> </span>data<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1255-4" data-line-number="4">    input   =<span class="st"> </span>data<span class="op">$</span>dataset[indices,<span class="kw">c</span>(feature)]</a>
<a class="sourceLine" id="cb1255-5" data-line-number="5">    output  =<span class="st"> </span>data<span class="op">$</span>dataset[indices,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1255-6" data-line-number="6">    n =<span class="st"> </span><span class="kw">length</span>(input)</a>
<a class="sourceLine" id="cb1255-7" data-line-number="7">    <span class="co"># group and average by category</span></a>
<a class="sourceLine" id="cb1255-8" data-line-number="8">    output.grp.mean =<span class="st"> </span><span class="kw">tapply</span>(output, input, mean)  </a>
<a class="sourceLine" id="cb1255-9" data-line-number="9">    sort.input =<span class="st"> </span><span class="kw">sort</span>( output.grp.mean[input], <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1255-10" data-line-number="10">    <span class="cf">if</span> (n <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) { </a>
<a class="sourceLine" id="cb1255-11" data-line-number="11">      avg =<span class="st"> </span>sort.input<span class="op">$</span>x</a>
<a class="sourceLine" id="cb1255-12" data-line-number="12">      loss  =<span class="st"> </span><span class="kw">split.loss</span>( loss, sort.input, output, <span class="ot">NULL</span>, <span class="ot">NULL</span>, </a>
<a class="sourceLine" id="cb1255-13" data-line-number="13">                          avg, minbucket)</a>
<a class="sourceLine" id="cb1255-14" data-line-number="14">    } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1255-15" data-line-number="15">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>(n<span class="dv">-1</span>)) {</a>
<a class="sourceLine" id="cb1255-16" data-line-number="16">     <span class="cf">if</span> (sort.input<span class="op">$</span>x[i] <span class="op">!=</span><span class="st"> </span>sort.input<span class="op">$</span>x[i<span class="op">+</span><span class="dv">1</span>]) {</a>
<a class="sourceLine" id="cb1255-17" data-line-number="17">        avg   =<span class="st"> </span>(sort.input<span class="op">$</span>x[i] <span class="op">+</span><span class="st"> </span>sort.input<span class="op">$</span>x[i<span class="op">+</span><span class="dv">1</span>]) <span class="op">/</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1255-18" data-line-number="18">        left  =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&lt;</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1255-19" data-line-number="19">        right =<span class="st"> </span><span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&gt;=</span><span class="st"> </span>avg )</a>
<a class="sourceLine" id="cb1255-20" data-line-number="20">        l     =<span class="st"> </span><span class="kw">unique</span>( <span class="kw">names</span>( <span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&lt;</span><span class="st"> </span>avg ) ) )</a>
<a class="sourceLine" id="cb1255-21" data-line-number="21">        r     =<span class="st"> </span><span class="kw">unique</span>( <span class="kw">names</span>( <span class="kw">which</span>( sort.input<span class="op">$</span>x <span class="op">&gt;=</span><span class="st"> </span>avg ) ) )</a>
<a class="sourceLine" id="cb1255-22" data-line-number="22">        catg  =<span class="st"> </span><span class="kw">replace</span>(categories, categories <span class="op">%in%</span><span class="st"> </span>l, <span class="st">&#39;L&#39;</span>)</a>
<a class="sourceLine" id="cb1255-23" data-line-number="23">        catg  =<span class="st"> </span><span class="kw">replace</span>(catg, catg <span class="op">%in%</span><span class="st"> </span>r, <span class="st">&#39;R&#39;</span>)</a>
<a class="sourceLine" id="cb1255-24" data-line-number="24">        avg   =<span class="st"> </span><span class="kw">paste0</span>(<span class="kw">replace</span>(catg, </a>
<a class="sourceLine" id="cb1255-25" data-line-number="25">                     catg <span class="op">%in%</span><span class="st"> </span><span class="kw">setdiff</span>(categories, <span class="kw">c</span>(l,r)), <span class="st">&#39;-&#39;</span>),  </a>
<a class="sourceLine" id="cb1255-26" data-line-number="26">                     <span class="dt">collapse=</span><span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb1255-27" data-line-number="27">        loss  =<span class="st"> </span><span class="kw">split.loss</span>( loss, sort.input, output, left, right, </a>
<a class="sourceLine" id="cb1255-28" data-line-number="28">                            avg, minbucket)</a>
<a class="sourceLine" id="cb1255-29" data-line-number="29">     }</a>
<a class="sourceLine" id="cb1255-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1255-31" data-line-number="31">    <span class="kw">optimizer</span>(feature, loss, output, data, <span class="dt">factor=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1255-32" data-line-number="32">}</a></code></pre></div>

<p>Finally, we compute the <strong>goodness of fit</strong> for each feature using the following implementation:</p>

<div class="sourceCode" id="cb1256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1256-1" data-line-number="1">sp.model &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb1256-2" data-line-number="2">    cs =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1256-3" data-line-number="3">    cs<span class="op">$</span>split    =<span class="st"> </span>cs<span class="op">$</span>sse   =<span class="st"> </span>cs<span class="op">$</span>obs   =<span class="st"> </span>cs<span class="op">$</span>improve =<span class="st"> </span><span class="ot">NULL</span> </a>
<a class="sourceLine" id="cb1256-4" data-line-number="4">    cs<span class="op">$</span>l.ymean  =<span class="st"> </span>cs<span class="op">$</span>r.ymean =<span class="st"> </span>cs<span class="op">$</span>l.son =<span class="st"> </span>cs<span class="op">$</span>r.son =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1256-5" data-line-number="5">    cs<span class="op">$</span>left.indices =<span class="st"> </span><span class="kw">list</span>(); cs<span class="op">$</span>right.indices =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1256-6" data-line-number="6">    cs</a>
<a class="sourceLine" id="cb1256-7" data-line-number="7">}</a></code></pre></div>
<div class="sourceCode" id="cb1257"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1257-1" data-line-number="1">split.goodness &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1257-2" data-line-number="2">                           categories) {</a>
<a class="sourceLine" id="cb1257-3" data-line-number="3">    goodness =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1257-4" data-line-number="4">    cs =<span class="st"> </span><span class="kw">sp.model</span>()</a>
<a class="sourceLine" id="cb1257-5" data-line-number="5">    <span class="cf">for</span> (f <span class="cf">in</span> features) {</a>
<a class="sourceLine" id="cb1257-6" data-line-number="6">       input =<span class="st"> </span>data<span class="op">$</span>dataset[data<span class="op">$</span>indices,<span class="kw">c</span>(f)]</a>
<a class="sourceLine" id="cb1257-7" data-line-number="7">       <span class="cf">if</span> (<span class="kw">is.factor</span>(input)) {</a>
<a class="sourceLine" id="cb1257-8" data-line-number="8">          cat =<span class="st"> </span>categories[[f]]</a>
<a class="sourceLine" id="cb1257-9" data-line-number="9">          goodness[[f]] =<span class="st"> </span><span class="kw">split.categorical</span>(cs, f, target, data, </a>
<a class="sourceLine" id="cb1257-10" data-line-number="10">                                            minbucket, cat)</a>
<a class="sourceLine" id="cb1257-11" data-line-number="11">       } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1257-12" data-line-number="12">          goodness[[f]] =<span class="st"> </span><span class="kw">split.continuous</span>(cs, f, target,  data, </a>
<a class="sourceLine" id="cb1257-13" data-line-number="13">                                           minbucket)</a>
<a class="sourceLine" id="cb1257-14" data-line-number="14">       }</a>
<a class="sourceLine" id="cb1257-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb1257-16" data-line-number="16">    goodness</a>
<a class="sourceLine" id="cb1257-17" data-line-number="17">}</a></code></pre></div>

<p>For <strong>continuous input</strong>, we show the list of SSEs derived by iteratively splitting the data points starting from the first pair to the last pair for our <strong>wt</strong> feature. Note here that we allow a minimum of two observations per node.</p>

<div class="sourceCode" id="cb1258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1258-1" data-line-number="1">features     =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;wt&quot;</span>, <span class="st">&quot;cyl&quot;</span>)</a>
<a class="sourceLine" id="cb1258-2" data-line-number="2">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1258-3" data-line-number="3">datacars     =<span class="st"> </span>mtcars2[, <span class="kw">c</span>(target, features)  ]</a>
<a class="sourceLine" id="cb1258-4" data-line-number="4">data         =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(datacars)), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1258-5" data-line-number="5">categories   =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1258-6" data-line-number="6">ft           =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1258-7" data-line-number="7">                              categories)<span class="op">$</span>wt</a>
<a class="sourceLine" id="cb1258-8" data-line-number="8">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices  =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span>ft<span class="op">$</span>response =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1258-9" data-line-number="9"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##      feature  split obs left right   sse ymean   mse
## [1,]    &quot;wt&quot; &quot;2.26&quot;  32    6    26 391.1 20.09 35.19
##      improve perc  ntype
## [1,]  0.6527  100 &quot;node&quot;</code></pre>

<p>The minimum SSE from the list is as follows:</p>

<div class="sourceCode" id="cb1260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1260-1" data-line-number="1">ft<span class="op">$</span>sse</a></code></pre></div>
<pre><code>## [1] 391.1</code></pre>

<p>For <strong>categorical input</strong>, we use <strong>cyl</strong> feature as an example which has only three levels: 4, 6, 8. Similarly, we limit the number of observations to two per node.</p>

<div class="sourceCode" id="cb1262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1262-1" data-line-number="1">ft =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, categories)<span class="op">$</span>cyl</a>
<a class="sourceLine" id="cb1262-2" data-line-number="2">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices  =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span>ft<span class="op">$</span>response =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1262-3" data-line-number="3"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##      feature split obs left right   sse ymean   mse improve
## [1,]   &quot;cyl&quot; &quot;RLL&quot;  32   21    11 401.9 20.09 35.19  0.6431
##      perc  ntype
## [1,]  100 &quot;node&quot;</code></pre>

<p>The <strong>cyl</strong> is categorical with three sorted levels (4, 6, 8). The second column describes a split in which level 4 is assigned to the <strong>R</strong>ight, and levels 6 and 8 are assigned to the <strong>L</strong>eft.</p>
<p>The minimum SSE from the list is as follows:</p>

<div class="sourceCode" id="cb1264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1264-1" data-line-number="1">ft<span class="op">$</span>sse</a></code></pre></div>
<pre><code>## [1] 401.9</code></pre>

<p><strong>Third</strong>, it is necessary to recall that the <strong>wt</strong> feature is arbitrarily chosen for the <strong>root node</strong>. That is indeed not ideal and not optimal. To determine which feature to use for a node split, we rank each feature by <strong>importance</strong> using the least <strong>SSE</strong>. The most <strong>important</strong> feature becomes the candidate to use for the split. Otherwise, we can also use a similar formula called <strong>improvement of deviance</strong> from <strong>rpart(.)</strong>, and it is expressed like so <span class="citation">(Therneau T.M. et al. <a href="bibliography.html#ref-ref480">2019</a>)</span>: </p>
<p><span class="math display">\[\begin{align}
improve = 1 - \frac{(SS_{(left)} + SS_{(right)})}{SS_{(parent)}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SS_{(left)}\)</span> is the SSE from the <strong>left</strong> split,</li>
<li><span class="math inline">\(SS_{(right)}\)</span> is the SSE from the <strong>right</strong> split,</li>
<li><span class="math inline">\(SS_{(parent)}\)</span> is the SSE from the data points prior to split.</li>
</ul>

<div class="sourceCode" id="cb1266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1266-1" data-line-number="1">ft<span class="op">$</span>improve</a></code></pre></div>
<pre><code>## [1] 0.6431</code></pre>

<p>The <strong>improvement of deviance</strong> is implemented in our <strong>split.cost(.)</strong>. Note that the obtained improvement is computed only for the <strong>wt</strong> feature. We need to compute the improvements of the other features to compare and rank them. To do that, we use our example implementation called <strong>rank.importance(.)</strong>.</p>

<div class="sourceCode" id="cb1268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1268-1" data-line-number="1">rank.importance &lt;-<span class="st"> </span><span class="cf">function</span>(features, target,  data, minbucket, categories) {</a>
<a class="sourceLine" id="cb1268-2" data-line-number="2">    goodness =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, minbucket, categories)</a>
<a class="sourceLine" id="cb1268-3" data-line-number="3">    ranks =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1268-4" data-line-number="4">    <span class="cf">for</span> (f <span class="cf">in</span> features) {</a>
<a class="sourceLine" id="cb1268-5" data-line-number="5">        r =<span class="st"> </span>goodness[[f]]</a>
<a class="sourceLine" id="cb1268-6" data-line-number="6">        r<span class="op">$</span>left.indices  =<span class="st">  </span>r<span class="op">$</span>right.indices =<span class="st"> </span>r<span class="op">$</span>response =<span class="st"> </span>r<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1268-7" data-line-number="7">        r   =<span class="st"> </span><span class="kw">data.frame</span>(r, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1268-8" data-line-number="8">        ranks =<span class="st"> </span><span class="kw">rbind</span>(ranks, r)</a>
<a class="sourceLine" id="cb1268-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb1268-10" data-line-number="10">    ordered.idx =<span class="st"> </span><span class="kw">order</span>(<span class="kw">as.numeric</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;sse&quot;</span>)]), </a>
<a class="sourceLine" id="cb1268-11" data-line-number="11">                  stringr<span class="op">::</span><span class="kw">str_detect</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;split&quot;</span>)], <span class="st">&quot;[LR-]&quot;</span>),</a>
<a class="sourceLine" id="cb1268-12" data-line-number="12">                  ranks[,<span class="kw">c</span>(<span class="st">&quot;feature&quot;</span>)] )</a>
<a class="sourceLine" id="cb1268-13" data-line-number="13">    ranks =<span class="st"> </span><span class="kw">data.frame</span>(ranks[ordered.idx,], <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1268-14" data-line-number="14">    feature =<span class="st"> </span>ranks[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;feature&quot;</span>)] </a>
<a class="sourceLine" id="cb1268-15" data-line-number="15">    top     =<span class="st">  </span>goodness[[feature]]  </a>
<a class="sourceLine" id="cb1268-16" data-line-number="16">    <span class="kw">list</span>(<span class="st">&quot;ranks&quot;</span> =<span class="st"> </span>ranks, <span class="st">&quot;top&quot;</span> =<span class="st"> </span>top) </a>
<a class="sourceLine" id="cb1268-17" data-line-number="17">}</a></code></pre></div>

<p>Below, we use the <strong>rank.importance(.)</strong> function to obtain the ranks of features given <strong>minbucket</strong> equal to two. Note that our implementation breaks the tie so that if multiple features have the same <strong>SSE</strong>, we order the features based on type (category vs.Â continuous) and feature name (by alphabet). Other implementations such as <strong>rpart(.)</strong> may use other means to break the tie. See the list of features below ranked based on the least <strong>SSE</strong>.</p>

<div class="sourceCode" id="cb1269"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1269-1" data-line-number="1">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1269-2" data-line-number="2">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1269-3" data-line-number="3">datacars     =<span class="st"> </span>mtcars2 </a>
<a class="sourceLine" id="cb1269-4" data-line-number="4">categories   =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1269-5" data-line-number="5">data =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(datacars)), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1269-6" data-line-number="6">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">2</span>, categories)</a>
<a class="sourceLine" id="cb1269-7" data-line-number="7"><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##   feature split obs left right   sse ymean   mse improve
## 5      wt  2.26  32    6    26 391.1 20.09 35.19  0.6527
## 1     cyl   RLL  32   21    11 401.9 20.09 35.19  0.6431
## 2    disp 163.8  32   14    18 435.7 20.09 35.19  0.6130
## 3      hp   118  32   15    17 449.2 20.09 35.19  0.6011
## 4    drat  3.75  32   18    14 654.6 20.09 35.19  0.4187
## 6    qsec 18.41  32   20    12 749.9 20.09 35.19  0.3340
##   perc ntype
## 5  100  node
## 1  100  node
## 2  100  node
## 3  100  node
## 4  100  node
## 6  100  node</code></pre>

<p>The <strong>top-rank</strong> feature for the node is:</p>

<div class="sourceCode" id="cb1271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1271-1" data-line-number="1">ft =<span class="st"> </span>r<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1271-2" data-line-number="2">ft<span class="op">$</span>left.data =<span class="st"> </span>ft<span class="op">$</span>right.data =<span class="st"> </span>ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices =<span class="st"> </span></a>
<a class="sourceLine" id="cb1271-3" data-line-number="3"><span class="st">               </span>ft<span class="op">$</span>response =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1271-4" data-line-number="4"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##      feature  split obs left right   sse ymean   mse
## [1,]    &quot;wt&quot; &quot;2.26&quot;  32    6    26 391.1 20.09 35.19
##      improve perc    indices  ntype
## [1,]  0.6527  100 Integer,32 &quot;node&quot;</code></pre>

<p>The output shows that our choice of feature to use for the <strong>root node</strong> is <strong>wt</strong>, which is ranked at the top, based on the least SSE (or based on the largest improvement of deviance). The left and right columns indicate the number of observations corresponding to the split.</p>
<p>To validate, we capture a summary of our previous tree model generated by previously running the <strong>rpart(.)</strong> function.</p>

<div class="sourceCode" id="cb1273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1273-1" data-line-number="1">summ =<span class="st"> </span><span class="kw">capture.output</span>(<span class="kw">summary</span>(tree.model))</a></code></pre></div>

<p>Then we use our simple <strong>show.node(.)</strong> function to parse the summary and extract a specific node summary. The <strong>root node</strong> is our first node to show.</p>

<div class="sourceCode" id="cb1274"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1274-1" data-line-number="1"><span class="kw">show.node</span>(summ, <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [ 23 ] Node number 1: 32 observations,    complexity param=0.6527
## [ 24 ]   mean=20.09, MSE=35.19 
## [ 25 ]   left son=2 (26 obs) right son=3 (6 obs)
## [ 26 ]   Primary splits:
## [ 27 ]       wt   &lt; 2.26  to the right, improve=0.6527, (0 missing)
## [ 28 ]       cyl  splits as  RLL,       improve=0.6431, (0 missing)
## [ 29 ]       disp &lt; 163.8 to the right, improve=0.6131, (0 missing)
## [ 30 ]       hp   &lt; 118   to the right, improve=0.6011, (0 missing)
## [ 31 ]       drat &lt; 3.75  to the left,  improve=0.4187, (0 missing)
## [ 32 ]   Surrogate splits:
## [ 33 ]       disp &lt; 101.6 to the right, agree=0.969, adj=0.833, (0 split)
## [ 34 ]       hp   &lt; 92    to the right, agree=0.938, adj=0.667, (0 split)
## [ 35 ]       drat &lt; 4     to the left,  agree=0.906, adj=0.500, (0 split)
## [ 36 ]       cyl  splits as  RLL,       agree=0.844, adj=0.167, (0 split)</code></pre>

<p>The output confirms that wt is the top-ranking feature for the split with <strong>improvement of deviance</strong> equals 0.6527. The <strong>root node</strong> is split into two <strong>decision nodes</strong> with the <strong>one child node</strong> receiving 6 observations and the <strong>other child node</strong> receiving 26 observations. The split happens between a pair of input points averaging 2.26.</p>
<p>When a feature has missing observations, <strong>rpart(.)</strong> uses a <strong>Surrogate split</strong> by splitting the observations of another feature (acting as the <strong>surrogate</strong>). Our simple implementation does not account for missing observations; thus, we leave readers to investigate further handling <strong>missing observations</strong> for <strong>regression trees</strong>.</p>
<p><strong>Fourth</strong>, let us finally try to implement a <strong>regression tree</strong>. Our example implementation of <strong>Regression Tree</strong> uses a simple <strong>level-ordered</strong> algorithm, and it is flat with no additional complex rules.</p>
<p>Our example <strong>tree</strong> implementation uses a third-party library called <strong>dequer</strong> to handle queues and help achieve a <strong>level-ordered</strong> tree.</p>

<div class="sourceCode" id="cb1276"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1276-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1276-2" data-line-number="2">my.regression.tree &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, dataset,  </a>
<a class="sourceLine" id="cb1276-3" data-line-number="3">                               <span class="dt">minbucket =</span> <span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">50</span>) {</a>
<a class="sourceLine" id="cb1276-4" data-line-number="4">  split.child &lt;-<span class="st"> </span><span class="cf">function</span>(indices) {</a>
<a class="sourceLine" id="cb1276-5" data-line-number="5">    data =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span>indices, <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>dataset)</a>
<a class="sourceLine" id="cb1276-6" data-line-number="6">    <span class="kw">rank.importance</span>(features, target, data, minbucket, categories)</a>
<a class="sourceLine" id="cb1276-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1276-8" data-line-number="8">  build.level.tree &lt;-<span class="st"> </span><span class="cf">function</span>(my.stack,  <span class="dt">top.count =</span> <span class="dv">0</span>, <span class="dt">level =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1276-9" data-line-number="9">                               <span class="dt">model =</span> <span class="kw">list</span>()) {</a>
<a class="sourceLine" id="cb1276-10" data-line-number="10">    child.stack =<span class="st"> </span><span class="kw">queue</span>()</a>
<a class="sourceLine" id="cb1276-11" data-line-number="11">    <span class="cf">while</span>(<span class="kw">length</span>(my.stack)) {</a>
<a class="sourceLine" id="cb1276-12" data-line-number="12">      node =<span class="st"> </span><span class="kw">pop</span>(my.stack)</a>
<a class="sourceLine" id="cb1276-13" data-line-number="13">      top =<span class="st"> </span>node<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1276-14" data-line-number="14">      top.count =<span class="st">  </span>top.count <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1276-15" data-line-number="15">      model[[top.count]] =<span class="st"> </span>node </a>
<a class="sourceLine" id="cb1276-16" data-line-number="16">      <span class="cf">if</span> (top<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span> <span class="op">||</span><span class="st"> </span>level <span class="op">&gt;</span><span class="st"> </span>maxdepth) { </a>
<a class="sourceLine" id="cb1276-17" data-line-number="17">          model[[top.count]]<span class="op">$</span>top<span class="op">$</span>split =<span class="st"> &quot;.&quot;</span></a>
<a class="sourceLine" id="cb1276-18" data-line-number="18">          model[[top.count]]<span class="op">$</span>top<span class="op">$</span>ntype =<span class="st"> &quot;leaf&quot;</span></a>
<a class="sourceLine" id="cb1276-19" data-line-number="19">          <span class="cf">next</span> </a>
<a class="sourceLine" id="cb1276-20" data-line-number="20">      }</a>
<a class="sourceLine" id="cb1276-21" data-line-number="21">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(top<span class="op">$</span>left.indices)) {</a>
<a class="sourceLine" id="cb1276-22" data-line-number="22">         left        =<span class="st"> </span><span class="kw">split.child</span>(top<span class="op">$</span>left.indices)</a>
<a class="sourceLine" id="cb1276-23" data-line-number="23">         left<span class="op">$</span>parent =<span class="st"> </span>top.count</a>
<a class="sourceLine" id="cb1276-24" data-line-number="24">         <span class="kw">pushback</span>(child.stack, left)</a>
<a class="sourceLine" id="cb1276-25" data-line-number="25"></a>
<a class="sourceLine" id="cb1276-26" data-line-number="26">      }</a>
<a class="sourceLine" id="cb1276-27" data-line-number="27">      <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(top<span class="op">$</span>right.indices)) {</a>
<a class="sourceLine" id="cb1276-28" data-line-number="28">         right        =<span class="st"> </span><span class="kw">split.child</span>(top<span class="op">$</span>right.indices)  </a>
<a class="sourceLine" id="cb1276-29" data-line-number="29">         right<span class="op">$</span>parent =<span class="st"> </span>top.count</a>
<a class="sourceLine" id="cb1276-30" data-line-number="30">         <span class="kw">pushback</span>(child.stack, right)</a>
<a class="sourceLine" id="cb1276-31" data-line-number="31">      }</a>
<a class="sourceLine" id="cb1276-32" data-line-number="32">    }</a>
<a class="sourceLine" id="cb1276-33" data-line-number="33">    <span class="cf">if</span> (<span class="kw">length</span>(child.stack) <span class="op">&amp;&amp;</span><span class="st"> </span>level <span class="op">&lt;=</span><span class="st"> </span>maxdepth) {</a>
<a class="sourceLine" id="cb1276-34" data-line-number="34">      model =<span class="st"> </span><span class="kw">build.level.tree</span>(child.stack, top.count, level <span class="op">+</span><span class="st"> </span><span class="dv">1</span>, model) </a>
<a class="sourceLine" id="cb1276-35" data-line-number="35">    } </a>
<a class="sourceLine" id="cb1276-36" data-line-number="36">     model</a>
<a class="sourceLine" id="cb1276-37" data-line-number="37">  }</a>
<a class="sourceLine" id="cb1276-38" data-line-number="38">  my.stack =<span class="st"> </span><span class="kw">queue</span>()</a>
<a class="sourceLine" id="cb1276-39" data-line-number="39">  indices =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(dataset))</a>
<a class="sourceLine" id="cb1276-40" data-line-number="40">  categories =<span class="st"> </span><span class="kw">get.categories</span>(target, dataset)</a>
<a class="sourceLine" id="cb1276-41" data-line-number="41">  root =<span class="st"> </span><span class="kw">split.child</span>(indices); root<span class="op">$</span>parent =<span class="st"> </span><span class="dv">0</span> </a>
<a class="sourceLine" id="cb1276-42" data-line-number="42">  <span class="kw">pushback</span>(my.stack, root)</a>
<a class="sourceLine" id="cb1276-43" data-line-number="43">  my.model =<span class="st"> </span><span class="kw">build.level.tree</span>(my.stack)</a>
<a class="sourceLine" id="cb1276-44" data-line-number="44">  <span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>my.model, <span class="st">&quot;categories&quot;</span> =<span class="st"> </span>categories)</a>
<a class="sourceLine" id="cb1276-45" data-line-number="45">}</a>
<a class="sourceLine" id="cb1276-46" data-line-number="46">my.tree =<span class="st"> </span>my.regression.tree</a></code></pre></div>

<p>For displaying the table form of the tree, we use the following function:</p>

<div class="sourceCode" id="cb1277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1277-1" data-line-number="1">my.table.tree &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, <span class="dt">display_mode =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1277-2" data-line-number="2">    mlen =<span class="st"> </span><span class="kw">length</span>(my.model)</a>
<a class="sourceLine" id="cb1277-3" data-line-number="3">    tree =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1277-4" data-line-number="4">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>mlen) {</a>
<a class="sourceLine" id="cb1277-5" data-line-number="5">        r =<span class="st"> </span>my.model[[m]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1277-6" data-line-number="6">        r<span class="op">$</span>left.indices  =<span class="st"> </span>r<span class="op">$</span>right.indices =<span class="st"> </span>r<span class="op">$</span>response =<span class="st"> </span>r<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1277-7" data-line-number="7">        <span class="cf">if</span> (display_mode <span class="op">==</span><span class="st"> </span><span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1277-8" data-line-number="8">            <span class="cf">if</span> (r<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) { r<span class="op">$</span>feature =<span class="st"> &quot;&lt;leaf&gt;&quot;</span> }</a>
<a class="sourceLine" id="cb1277-9" data-line-number="9">            r<span class="op">$</span>ntype =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1277-10" data-line-number="10">        }</a>
<a class="sourceLine" id="cb1277-11" data-line-number="11">        r =<span class="st"> </span><span class="kw">data.frame</span>(r, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1277-12" data-line-number="12">        r =<span class="st"> </span><span class="kw">cbind</span>(<span class="dt">N=</span><span class="kw">as.numeric</span>(m), <span class="dt">P=</span>my.model[[m]]<span class="op">$</span>parent, r)</a>
<a class="sourceLine" id="cb1277-13" data-line-number="13">        tree =<span class="st"> </span><span class="kw">rbind</span>(tree, r)</a>
<a class="sourceLine" id="cb1277-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb1277-15" data-line-number="15">    tree</a>
<a class="sourceLine" id="cb1277-16" data-line-number="16">}</a></code></pre></div>

<p>Below, we use minbucket equal to 2 and maxdepth equal to 5 to generate a tree model.</p>

<div class="sourceCode" id="cb1278"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1278-1" data-line-number="1">my.model =<span class="st"> </span><span class="kw">my.tree</span>(features, target, datacars, <span class="dt">minbucket=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1278-2" data-line-number="2">my.tree.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model, <span class="dt">display_mode=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##     N  P feature  split obs  L  R    SSE ymean   MSE
## 1   1  0      wt   2.26  32  6 26 391.12 20.09 35.19
## 2   2  1    qsec 19.185   6  4  2  16.03 30.07  7.43
## 3   3  1    disp  266.9  26 12 14 127.32 17.79 13.33
## 4   4  2      wt  1.775   4  2  2   0.84 28.52  3.73
## 5   5  2  &lt;leaf&gt;      .   2  .  .   1.12 33.15  0.56
## 6   6  3      wt 3.3275  12  9  3  15.94 20.93  3.51
## 7   7  3    disp    450  14 12  2  33.66 15.10  6.09
## 8   8  4  &lt;leaf&gt;      .   2  .  .   0.00 30.40  0.00
## 9   9  4  &lt;leaf&gt;      .   2  .  .   0.84 26.65  0.42
## 10 10  6      hp     96   9  3  6   3.97 21.78  1.65
## 11 11  6  &lt;leaf&gt;      .   3  .  .   1.09 18.37  0.36
## 12 12  7    drat   3.18  12  7  5  19.98 15.88  2.80
## 13 13  7  &lt;leaf&gt;      .   2  .  .   0.00 10.40  0.00
## 14 14 10  &lt;leaf&gt;      .   3  .  .   1.71 23.33  0.57
## 15 15 10    qsec  16.74   6  2  4   0.99 21.00  0.38
## 16 16 12    disp    339   7  5  2   3.47 16.79  2.37
## 17 17 12      hp  254.5   5  3  2   1.36 14.62  0.68
## 18 18 15  &lt;leaf&gt;      .   2  .  .   0.84 20.35  0.42
## 19 19 15  &lt;leaf&gt;      .   4  2  2   0.08 21.32  0.04
## 20 20 16  &lt;leaf&gt;      .   5  3  2   2.27 15.92  0.67
## 21 21 16  &lt;leaf&gt;      .   2  .  .   0.12 18.95  0.06
## 22 22 17  &lt;leaf&gt;      .   3  .  .   1.04 14.10  0.35
## 23 23 17  &lt;leaf&gt;      .   2  .  .   0.32 15.40  0.16
##     improve perc
## 1  0.652661  100
## 2   0.64015   19
## 3  0.632617   81
## 4  0.943317   12
## 5         .    6
## 6  0.621527   38
## 7  0.604969   44
## 8         .    6
## 9         .    6
## 10 0.732984   28
## 11        .    9
## 12  0.40646   38
## 13        .    6
## 14        .    9
## 15 0.560841   19
## 16 0.790639   22
## 17 0.598583   16
## 18        .    6
## 19 0.423729   12
## 20 0.323477   16
## 21        .    6
## 22        .    9
## 23        .    6</code></pre>

<p>The column <strong>N</strong> represents the node number, and <strong>P</strong> represents the parent number of a node. The columns <strong>L</strong> and <strong>R</strong> represent the number of nodes in the left and right splits, respectively.</p>
<p>We previously validated the root node. Let us validate the two children (nodes 2 and 3). Moreover, let us show the following summary of the left child node (<strong>node 2</strong>).</p>

<div class="sourceCode" id="cb1280"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1280-1" data-line-number="1"><span class="kw">show.node</span>(summ, <span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [ 38 ] Node number 2: 26 observations,    complexity param=0.1947
## [ 39 ]   mean=17.79, MSE=13.33 
## [ 40 ]   left son=4 (14 obs) right son=5 (12 obs)
## [ 41 ]   Primary splits:
## [ 42 ]       disp &lt; 266.9 to the right, improve=0.6326, (0 missing)
## [ 43 ]       cyl  splits as  RRL,       improve=0.6326, (0 missing)
## [ 44 ]       hp   &lt; 136.5 to the right, improve=0.5804, (0 missing)
## [ 45 ]       wt   &lt; 3.325 to the right, improve=0.5393, (0 missing)
## [ 46 ]       qsec &lt; 18.15 to the left,  improve=0.4211, (0 missing)
## [ 47 ]   Surrogate splits:
## [ 48 ]       hp   &lt; 136.5 to the right, agree=0.962, adj=0.917, (0 split)
## [ 49 ]       wt   &lt; 3.49  to the right, agree=0.885, adj=0.750, (0 split)
## [ 50 ]       qsec &lt; 18.15 to the left,  agree=0.885, adj=0.750, (0 split)
## [ 51 ]       drat &lt; 3.58  to the left,  agree=0.846, adj=0.667, (0 split)</code></pre>

<p>Below, we show a summary of the right child node (<strong>node 3</strong>):</p>

<div class="sourceCode" id="cb1282"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1282-1" data-line-number="1"><span class="kw">show.node</span>(summ, <span class="dv">3</span>)</a></code></pre></div>
<pre><code>## [ 53 ] Node number 3: 6 observations,    complexity param=0.02533
## [ 54 ]   mean=30.07, MSE=7.426 
## [ 55 ]   left son=6 (4 obs) right son=7 (2 obs)
## [ 56 ]   Primary splits:
## [ 57 ]       qsec &lt; 19.18 to the left,  improve=0.6402, (0 missing)
## [ 58 ]       disp &lt; 78.85 to the right, improve=0.6322, (0 missing)
## [ 59 ]       wt   &lt; 1.885 to the right, improve=0.3030, (0 missing)
## [ 60 ]       hp   &lt; 65.5  to the right, improve=0.2923, (0 missing)
## [ 61 ]       drat &lt; 4.325 to the right, improve=0.2346, (0 missing)
## [ 62 ]   Surrogate splits:
## [ 63 ]       disp &lt; 78.85 to the right, agree=0.833, adj=0.5, (0 split)</code></pre>

<p><strong>Pruning a Tree</strong></p>
<p>In essence, the formation of a tree is driven by a few adjustable parameters, namely the allowable depth of the tree and the allowable number of observations per node. One of the reasons to adjust the shape of a tree is to offset <strong>overfitting</strong> and <strong>underfitting</strong>. In the previous discussion, we used <strong>maxdepth</strong> to control the depth of a tree. In a way, this is a straightforward approach to trimming a tree. Moreover another approach is to <strong>prune</strong> a tree.</p>
<p>Similar to <strong>LASSO</strong> and <strong>Ridge</strong> regularization in linear regression discussion in which we use a parameter <span class="math inline">\(\lambda\)</span> to find an optimal value to penalize or reward coefficients, we use a <strong>complexity parameter (CP)</strong> <span class="citation">(Therneau T. M. et al. <a href="bibliography.html#ref-ref1148t">1997</a>)</span> in regression trees to prune a branch (node) of a tree - we call this <strong>cost complexity pruning</strong> or <strong>weakest link pruning</strong>. The idea is to find an optimal tree - meaning that we do not want a tree to grow too complex in the same way that we do not want our regression line in linear regression to start following every point, resulting in an overfit. </p>
<p>The intuition is to prune a tree if <span class="math inline">\(R^2\)</span> increases by at least a factor of CP size. If <span class="math inline">\(R^2\)</span> does not increase by at least the amount of a CP value, then no pruning happens. In linear regression, we use adjusted <span class="math inline">\(R^2\)</span> to evaluate the significance of a predictor variable; but to the point, CP is a way to measure tree complexity and help determine which tree branch to prune.</p>
<p><span class="math display">\[\begin{align}
\alpha_{(cp)} = \frac{R(T) - R(T_t)}{|T_t| - |T| }
\end{align}\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(\alpha\)</span> is the <strong>complexity parameter (CP)</strong></li>
<li><span class="math inline">\(\mathbf{R(T)}\)</span> is the risk (computed SSE) for the entire <strong>T</strong> tree.</li>
<li><span class="math inline">\(\mathbf{R(T_t)}\)</span> is the risk for a <strong>t</strong> subtree.</li>
<li><span class="math inline">\(\mathbf{|T|}\)</span> is the number of splits in the entire tree.</li>
<li><span class="math inline">\(\mathbf{|T_t|}\)</span> is the number of splits in the <strong>t</strong> subtree.</li>
</ul>
<p>The <strong>cp</strong> value is computed as part of our example implementation of <strong>Pruning</strong> a tree:</p>

<div class="sourceCode" id="cb1284"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1284-1" data-line-number="1">tree.loss &lt;-<span class="st"> </span><span class="cf">function</span>(model) {</a>
<a class="sourceLine" id="cb1284-2" data-line-number="2">  nodes      =<span class="st"> </span>model<span class="op">$</span>N</a>
<a class="sourceLine" id="cb1284-3" data-line-number="3">  parents    =<span class="st"> </span><span class="kw">unique</span>(model<span class="op">$</span>P)</a>
<a class="sourceLine" id="cb1284-4" data-line-number="4">  nsplit     =<span class="st"> </span><span class="kw">length</span>(parents) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1284-5" data-line-number="5">  diff       =<span class="st"> </span><span class="kw">setdiff</span>(nodes, parents)</a>
<a class="sourceLine" id="cb1284-6" data-line-number="6">  idx        =<span class="st"> </span><span class="kw">which</span>( nodes <span class="op">%in%</span><span class="st"> </span>diff )</a>
<a class="sourceLine" id="cb1284-7" data-line-number="7">  leafs      =<span class="st"> </span><span class="kw">data.frame</span>(model[idx,], <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1284-8" data-line-number="8">  mse        =<span class="st"> </span><span class="kw">as.numeric</span>(leafs<span class="op">$</span>mse)</a>
<a class="sourceLine" id="cb1284-9" data-line-number="9">  obs        =<span class="st"> </span><span class="kw">as.numeric</span>(leafs<span class="op">$</span>obs)</a>
<a class="sourceLine" id="cb1284-10" data-line-number="10">  <span class="kw">sum</span>(mse <span class="op">*</span><span class="st"> </span>obs)</a>
<a class="sourceLine" id="cb1284-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1284-12" data-line-number="12">my.prune.tree &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, <span class="dt">cp =</span> <span class="fl">0.0</span> ) {</a>
<a class="sourceLine" id="cb1284-13" data-line-number="13">  prune.cost &lt;-<span class="st"> </span><span class="cf">function</span>(model, <span class="dt">cptable =</span> <span class="ot">NULL</span>, <span class="dt">subtrees =</span> <span class="kw">list</span>(), <span class="dt">cnt =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1284-14" data-line-number="14">      nodes      =<span class="st"> </span>model<span class="op">$</span>N</a>
<a class="sourceLine" id="cb1284-15" data-line-number="15">      parents    =<span class="st"> </span><span class="kw">unique</span>(model<span class="op">$</span>P)</a>
<a class="sourceLine" id="cb1284-16" data-line-number="16">      nsplit     =<span class="st"> </span><span class="kw">length</span>(parents) <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1284-17" data-line-number="17">      diff       =<span class="st"> </span><span class="kw">setdiff</span>(nodes, parents)</a>
<a class="sourceLine" id="cb1284-18" data-line-number="18">      idx        =<span class="st"> </span><span class="kw">which</span>( nodes <span class="op">%in%</span><span class="st"> </span>diff )</a>
<a class="sourceLine" id="cb1284-19" data-line-number="19">      leafs      =<span class="st"> </span><span class="kw">data.frame</span>(model[idx,], <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1284-20" data-line-number="20">      candidate.branch =<span class="st"> </span>cost =<span class="st"> </span>pruned.model =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1284-21" data-line-number="21">      min.tcost =<span class="st"> </span>min.branch =<span class="st"> </span><span class="ot">Inf</span></a>
<a class="sourceLine" id="cb1284-22" data-line-number="22">      <span class="cf">for</span> (p <span class="cf">in</span> parents) {</a>
<a class="sourceLine" id="cb1284-23" data-line-number="23">          children  =<span class="st"> </span>model[ <span class="kw">which</span>(model<span class="op">$</span>P <span class="op">==</span><span class="st"> </span>p), ]</a>
<a class="sourceLine" id="cb1284-24" data-line-number="24">          nchildren =<span class="st"> </span><span class="kw">sum</span>(leafs<span class="op">$</span>N <span class="op">%in%</span><span class="st"> </span>children<span class="op">$</span>N)</a>
<a class="sourceLine" id="cb1284-25" data-line-number="25">          <span class="cf">if</span> (nchildren <span class="op">==</span><span class="st"> </span><span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb1284-26" data-line-number="26">             idx        =<span class="st"> </span><span class="kw">which</span>(model<span class="op">$</span>N <span class="op">%in%</span><span class="st"> </span>children<span class="op">$</span>N)</a>
<a class="sourceLine" id="cb1284-27" data-line-number="27">             <span class="co"># exclude pruned children for loss compute</span></a>
<a class="sourceLine" id="cb1284-28" data-line-number="28">             new.model  =<span class="st"> </span>model[<span class="op">-</span>idx,] </a>
<a class="sourceLine" id="cb1284-29" data-line-number="29">             tc         =<span class="st"> </span><span class="kw">tree.loss</span>(new.model)</a>
<a class="sourceLine" id="cb1284-30" data-line-number="30">             <span class="cf">if</span> (min.tcost <span class="op">&gt;</span><span class="st"> </span>tc) {</a>
<a class="sourceLine" id="cb1284-31" data-line-number="31">                  new.model[<span class="kw">which</span>(new.model<span class="op">$</span>N <span class="op">==</span><span class="st"> </span>p),]<span class="op">$</span>split =<span class="st"> &#39;.&#39;</span></a>
<a class="sourceLine" id="cb1284-32" data-line-number="32">                  new.model[<span class="kw">which</span>(new.model<span class="op">$</span>N <span class="op">==</span><span class="st"> </span>p),]<span class="op">$</span>ntype =<span class="st"> &quot;leaf&quot;</span></a>
<a class="sourceLine" id="cb1284-33" data-line-number="33">                  pruned.model =<span class="st"> </span>new.model</a>
<a class="sourceLine" id="cb1284-34" data-line-number="34">                  min.tcost   =<span class="st"> </span>tc; min.branch =<span class="st"> </span>p</a>
<a class="sourceLine" id="cb1284-35" data-line-number="35">             }</a>
<a class="sourceLine" id="cb1284-36" data-line-number="36">          }</a>
<a class="sourceLine" id="cb1284-37" data-line-number="37">      }   </a>
<a class="sourceLine" id="cb1284-38" data-line-number="38">      subtrees[[<span class="kw">as.character</span>(cnt)]] =<span class="st"> </span>model</a>
<a class="sourceLine" id="cb1284-39" data-line-number="39">      cptable =<span class="st"> </span><span class="kw">rbind</span>(cptable, <span class="kw">c</span>(min.branch, min.tcost))</a>
<a class="sourceLine" id="cb1284-40" data-line-number="40">      <span class="cf">if</span> (<span class="kw">nrow</span>(pruned.model) <span class="op">==</span><span class="st"> </span><span class="dv">1</span> ) {</a>
<a class="sourceLine" id="cb1284-41" data-line-number="41">        cptable  =<span class="st"> </span><span class="kw">apply</span>(cptable, <span class="dv">2</span>, as.numeric )</a>
<a class="sourceLine" id="cb1284-42" data-line-number="42">        cptable  =<span class="st"> </span><span class="kw">apply</span>(cptable, <span class="dv">2</span>, rev)</a>
<a class="sourceLine" id="cb1284-43" data-line-number="43">        base.err =<span class="st"> </span>min.tcost</a>
<a class="sourceLine" id="cb1284-44" data-line-number="44">        rel.err  =<span class="st"> </span>cptable[,<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>base.err</a>
<a class="sourceLine" id="cb1284-45" data-line-number="45">        cptable  =<span class="st"> </span><span class="kw">data.frame</span>(     <span class="dt">cp  =</span> <span class="op">-</span><span class="kw">c</span>(<span class="kw">diff</span>(rel.err),<span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1284-46" data-line-number="46">                                <span class="dt">nsplit =</span> <span class="kw">seq</span>(<span class="dv">0</span>, <span class="kw">nrow</span>(cptable) <span class="op">-</span><span class="st"> </span><span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1284-47" data-line-number="47">                             <span class="dt">rel.error =</span> rel.err,</a>
<a class="sourceLine" id="cb1284-48" data-line-number="48">                                   <span class="dt">sse =</span> cptable[,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb1284-49" data-line-number="49">                                <span class="dt">branch =</span> cptable[,<span class="dv">1</span>] )</a>
<a class="sourceLine" id="cb1284-50" data-line-number="50">        m =<span class="st"> </span><span class="kw">nrow</span>(cptable) </a>
<a class="sourceLine" id="cb1284-51" data-line-number="51">        cptable =<span class="st"> </span>cptable[ <span class="kw">which</span>( cptable<span class="op">$</span>cp <span class="op">&gt;=</span><span class="st">  </span>cp ),]</a>
<a class="sourceLine" id="cb1284-52" data-line-number="52">        n =<span class="st"> </span><span class="kw">nrow</span>(cptable)  </a>
<a class="sourceLine" id="cb1284-53" data-line-number="53">        res =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;subtree&quot;</span> =<span class="st"> </span>subtrees[[<span class="kw">as.character</span>(m <span class="op">-</span><span class="st"> </span>n)]],</a>
<a class="sourceLine" id="cb1284-54" data-line-number="54">                   <span class="st">&quot;cptable&quot;</span> =<span class="st"> </span>cptable )</a>
<a class="sourceLine" id="cb1284-55" data-line-number="55">        <span class="kw">return</span>(res)</a>
<a class="sourceLine" id="cb1284-56" data-line-number="56">      }      </a>
<a class="sourceLine" id="cb1284-57" data-line-number="57">      <span class="kw">prune.cost</span>(pruned.model, cptable, subtrees, cnt <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1284-58" data-line-number="58">  }</a>
<a class="sourceLine" id="cb1284-59" data-line-number="59">  model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model)</a>
<a class="sourceLine" id="cb1284-60" data-line-number="60">  treeloss =<span class="st"> </span><span class="kw">tree.loss</span>(model)</a>
<a class="sourceLine" id="cb1284-61" data-line-number="61">  <span class="kw">prune.cost</span>(model, <span class="kw">c</span>(<span class="dv">0</span>, treeloss))</a>
<a class="sourceLine" id="cb1284-62" data-line-number="62">}</a></code></pre></div>

<p>Finally, let us show the cptable. Note that the branch column indicates the branch (or decision node) to prune given its corresponding <strong>cp</strong> value. See below:</p>
<div class="sourceCode" id="cb1285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1285-1" data-line-number="1">(<span class="dt">cptable =</span> <span class="kw">my.prune.tree</span>(my.model, <span class="dt">cp =</span> <span class="fl">0.0</span>)<span class="op">$</span>cptable)</a></code></pre></div>
<pre><code>##          cp nsplit rel.error     sse branch
## 1  0.652661      0  1.000000 1126.05      1
## 2  0.194702      1  0.347339  391.12      3
## 3  0.045774      2  0.152636  171.88      7
## 4  0.025328      3  0.106863  120.33      2
## 5  0.023250      4  0.081534   91.81      6
## 6  0.012488      5  0.058285   65.63      4
## 7  0.012149      6  0.045796   51.57     12
## 8  0.011647      7  0.033648   37.89     16
## 9  0.009670      8  0.022000   24.77     10
## 10 0.001801      9  0.012330   13.88     17
## 11 0.001126     10  0.010529   11.86     15
## 12 0.000000     11  0.009404   10.59      0</code></pre>
<p>The table can be validated using a similar cp table generated by <strong>rpart(.)</strong>.</p>
<div class="sourceCode" id="cb1287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1287-1" data-line-number="1">tree.model<span class="op">$</span>cptable <span class="co"># generated by rpart()</span></a></code></pre></div>
<pre><code>##          CP nsplit rel error xerror    xstd
## 1  0.652661      0  1.000000 1.0646 0.25211
## 2  0.194702      1  0.347339 0.6490 0.10462
## 3  0.045774      2  0.152636 0.3922 0.06825
## 4  0.025328      3  0.106863 0.3582 0.06410
## 5  0.023250      4  0.081534 0.2996 0.05976
## 6  0.012488      5  0.058285 0.2803 0.06724
## 7  0.012149      6  0.045796 0.2803 0.06724
## 8  0.011647      7  0.033648 0.2803 0.06724
## 9  0.009670      8  0.022000 0.2788 0.06672
## 10 0.001801      9  0.012330 0.2322 0.06739
## 11 0.001126     10  0.010529 0.2457 0.07379
## 12 0.000000     11  0.009404 0.2476 0.07793</code></pre>
<p>There is a slight discrepancy between the two results; in particular, we notice that <strong>rpart(.)</strong> is built with cross-validation. Thus, we see two other measures: the cross-validation error and standard deviation (xerror and xstd).</p>
<p><span class="math display">\[\begin{align}
rel. error = xerror + xstd
\end{align}\]</span></p>
<p>Additionally, the <strong>xerror</strong> is based on the <strong>Predictive Residual Sum Square Error (PRESS)</strong> statistic (Terry M Therneau et al.Â 2019).</p>
<p>As for deriving the value of <strong>cp</strong>, let us use cp equal to 0.1947 from the second row in the table, which involves a tree with one split. That is calculated as such:</p>
<p>Â Â Â Â Â Â Â  <span class="math inline">\(\alpha_{(cp)}\)</span> = <span class="math inline">\(\frac{R(T) - R(T_t)}{|T_t| - |T| }\)</span> = (0.3473 - 0.1526) / (2 - 1) = 0.1947.</p>
<p>Now, to demonstrate pruning using a choice of cp from the cp table, let us first create a tree with minsplit and minbucket equal to five and maxdepth equal to two. That gives us a simple and decent tree (see Figure <a href="machinelearning2.html#fig:regressiontree3">10.3</a>):</p>

<div class="sourceCode" id="cb1289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1289-1" data-line-number="1"><span class="kw">library</span>(rpart); <span class="kw">library</span>(rpart.plot)</a>
<a class="sourceLine" id="cb1289-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1289-3" data-line-number="3">datacars =<span class="st"> </span>mtcars2</a>
<a class="sourceLine" id="cb1289-4" data-line-number="4">rpart.control =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">2</span>, <span class="dt">minbucket =</span> <span class="dv">5</span>, <span class="dt">cp =</span>  <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1289-5" data-line-number="5">tree.model =<span class="st"> </span><span class="kw">rpart</span>(mpg <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> datacars, <span class="dt">control =</span> rpart.control)</a>
<a class="sourceLine" id="cb1289-6" data-line-number="6"><span class="kw">rpart.plot</span>(tree.model, <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;cp = 0&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressiontree3"></span>
<img src="DS_files/figure-html/regressiontree3-1.png" alt="Regression Tree" width="70%" />
<p class="caption">
Figure 10.3: Regression Tree
</p>
</div>

<p>The tree gives us three terminal nodes with two splits. Suppose we trim node number two of the decision tree. Node number two has two terminal nodes. To confirm that, let us show the node like so:</p>
<div class="sourceCode" id="cb1290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1290-1" data-line-number="1"><span class="kw">print</span>(tree.model)</a></code></pre></div>
<pre><code>## n= 32 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 32 1126.00 20.09  
##   2) wt&gt;=2.26 26  346.60 17.79  
##     4) disp&gt;=266.9 14   85.20 15.10 *
##     5) disp&lt; 266.9 12   42.12 20.92 *
##   3) wt&lt; 2.26 6   44.55 30.07 *</code></pre>
<p>Let us show a list of <strong>cp</strong> choices and choose one to allow us to trim a tree.</p>
<div class="sourceCode" id="cb1292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1292-1" data-line-number="1">tree.model<span class="op">$</span>cptable</a></code></pre></div>
<pre><code>##       CP nsplit rel error xerror    xstd
## 1 0.6527      0    1.0000 1.0646 0.25211
## 2 0.1947      1    0.3473 0.6490 0.10462
## 3 0.0000      2    0.1526 0.5074 0.09457</code></pre>
<p>We use the <strong>cp value</strong> equal to 0.1947 (see Figure <a href="machinelearning2.html#fig:regressiontree4">10.4</a>).</p>

<div class="sourceCode" id="cb1294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1294-1" data-line-number="1">rpart.control =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">2</span>, <span class="dt">minbucket =</span> <span class="dv">5</span>, </a>
<a class="sourceLine" id="cb1294-2" data-line-number="2">                              <span class="dt">cp =</span>  tree.model<span class="op">$</span>cptable[<span class="dv">2</span>,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb1294-3" data-line-number="3">cp.tree.model =<span class="st"> </span><span class="kw">rpart</span>(mpg <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> datacars, <span class="dt">control =</span> rpart.control)</a>
<a class="sourceLine" id="cb1294-4" data-line-number="4"><span class="kw">rpart.plot</span>(cp.tree.model, <span class="dt">main=</span><span class="kw">paste0</span>(<span class="st">&quot;cp = &quot;</span>, tree.model<span class="op">$</span>cptable[<span class="dv">2</span>,<span class="dv">1</span>]))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:regressiontree4"></span>
<img src="DS_files/figure-html/regressiontree4-1.png" alt="Regression Tree" width="60%" />
<p class="caption">
Figure 10.4: Regression Tree
</p>
</div>

<p>In the figure, node two becomes a terminal node. We can validate below:</p>
<div class="sourceCode" id="cb1295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1295-1" data-line-number="1">cp.tree.model</a></code></pre></div>
<pre><code>## n= 32 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 32 1126.00 20.09  
##   2) wt&gt;=2.26 26  346.60 17.79 *
##   3) wt&lt; 2.26 6   44.55 30.07 *</code></pre>
<p>A better way to achieve the same tree configuration is to use the <strong>prune(.)</strong> function - the idea is to avoid regenerating a tree; instead, perform <strong>pruning</strong>. See below:</p>
<div class="sourceCode" id="cb1297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1297-1" data-line-number="1">chosen.cp =<span class="st"> </span>tree.model<span class="op">$</span>cptable[<span class="dv">2</span>,<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1297-2" data-line-number="2">(<span class="dt">pruned.tree.model =</span> <span class="kw">prune</span>(tree.model, <span class="dt">cp =</span> chosen.cp))</a></code></pre></div>
<pre><code>## n= 32 
## 
## node), split, n, deviance, yval
##       * denotes terminal node
## 
## 1) root 32 1126.00 20.09  
##   2) wt&gt;=2.26 26  346.60 17.79 *
##   3) wt&lt; 2.26 6   44.55 30.07 *</code></pre>
<p>If we now get the summary, we see that <strong>Node number 2</strong> is pruned - it becomes a terminal node.</p>
<div class="sourceCode" id="cb1299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1299-1" data-line-number="1">(<span class="dt">pruned.tree =</span> <span class="kw">as.party</span>(pruned.tree.model))</a></code></pre></div>
<pre><code>## 
## Model formula:
## mpg ~ cyl + disp + hp + drat + wt + qsec
## 
## Fitted party:
## [1] root
## |   [2] wt &gt;= 2.26: 18 (n = 26, err = 347)
## |   [3] wt &lt; 2.26: 30 (n = 6, err = 45)
## 
## Number of inner nodes:    1
## Number of terminal nodes: 2</code></pre>
<p>We also can show a box plot per node using two libraries called <strong>party</strong> and <strong>partykit</strong>. See Figure <a href="machinelearning2.html#fig:prunetree1">10.5</a>.</p>
<div class="sourceCode" id="cb1301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1301-1" data-line-number="1"><span class="kw">library</span>(party); <span class="kw">library</span>(partykit)</a>
<a class="sourceLine" id="cb1301-2" data-line-number="2"><span class="kw">plot</span>(pruned.tree,<span class="kw">paste0</span>(<span class="st">&quot;cp = &quot;</span>, tree.model<span class="op">$</span>cptable[<span class="dv">2</span>,<span class="dv">1</span>]))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:prunetree1"></span>
<img src="DS_files/figure-html/prunetree1-1.png" alt="Pruned Tree" width="70%" />
<p class="caption">
Figure 10.5: Pruned Tree
</p>
</div>
<p><strong>Prediction and Cross-Validation</strong></p>
<p>In splitting or pruning a tree, we mostly rely on at least four parameters: minsplit, minbucket, maxdepth, and complexity. These four knobs allow us to build and optimize a tree model as best we can; however, with a high number of possible permutations, it takes a bit of effort to arrive at a model with optimal parameters. Therefore, to better train a model without manually tweaking knobs, we rely on <strong>cross-validation</strong>.</p>
<p><strong>First</strong>, our strategy is to perform a <strong>leave-one-out</strong> <strong>cross-validation</strong> given a dataset. Here, we use <strong>createFolds(.)</strong> function to derive several folds. We treat the first fold as our test dataset and the rest of the dataset as our training set. Labels (or response variables) are excluded from the test set.</p>

<div class="sourceCode" id="cb1302"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1302-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1302-2" data-line-number="2">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1302-3" data-line-number="3">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1302-4" data-line-number="4">datacars     =<span class="st"> </span>mtcars2</a>
<a class="sourceLine" id="cb1302-5" data-line-number="5">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(datacars<span class="op">$</span>mpg, <span class="dt">k=</span><span class="kw">nrow</span>(datacars), <span class="dt">returnTrain=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1302-6" data-line-number="6"><span class="co"># choose the first fold for our test group.</span></a>
<a class="sourceLine" id="cb1302-7" data-line-number="7">testset =<span class="st"> </span>datacars[fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1302-8" data-line-number="8"><span class="co"># choose the other folds for training group.</span></a>
<a class="sourceLine" id="cb1302-9" data-line-number="9">trainset =<span class="st"> </span>datacars[<span class="op">-</span>fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1302-10" data-line-number="10"><span class="co"># test target</span></a>
<a class="sourceLine" id="cb1302-11" data-line-number="11">test.target =<span class="st"> </span>testset<span class="op">$</span>mpg</a>
<a class="sourceLine" id="cb1302-12" data-line-number="12"><span class="co"># test predictors with no labels</span></a>
<a class="sourceLine" id="cb1302-13" data-line-number="13">test.predictors =<span class="st"> </span>testset</a>
<a class="sourceLine" id="cb1302-14" data-line-number="14">test.predictors<span class="op">$</span>mpg =<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>

<p><strong>Second</strong>, we need to be able to measure the performance of our model using a prediction function. A predicted value corresponding to an observation in our test dataset is obtained from one of the tree leaves, which holds the average target value. We use the average target value as the predicted value. To reach the leaf, we start from the root of the tree and navigate towards the target leaf by following the decision rules enforced at the decision nodes. To do that, we use the following formula below:</p>
<p><span class="math display">\[\begin{align}
\overline{Y}_{(target\ leaf)} =  \sum_{l\ \in\ L} \overline{Y}_l \times \prod_{d\ \in\ D} I\left(X_v^{(a)}, D_v^{(a)}\right) \label{eqn:eqnnumber402}\\
I\left(X_v^a{(a)}, D_v^{(a)}\right) = \begin{cases} 1 &amp; X_v^{(a)} &lt; D_v^{(a)} \\ 0 &amp; otherwise\end{cases} \label{eqn:eqnnumber403}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{I}\)</span> is an indicator function</li>
<li><span class="math inline">\(\mathbf{l}\)</span> is a leaf node</li>
<li><strong>L</strong> is a set of leaf nodes</li>
<li><strong>d</strong> is a decision node<br />
</li>
<li><strong>D</strong> is a set of ancestral decision nodes (ancestors of <strong>l</strong>, including direct parents)</li>
<li><strong>a</strong> is a feature in X input and corresponding decision node</li>
<li><strong>v</strong> is the value of the feature in X and the value of the split in the decision node</li>
<li><span class="math inline">\(\mathbf{\overline{Y}}\)</span> is the average value of a leaf (for continuous output)</li>
</ul>
<p>Let us review an example implementation of our prediction function for a model tree. Note that we skip the use of indicator function and matrix manipulation. On the other hand, our implementation uses <strong>coefficient of determination</strong> (<span class="math inline">\(\mathbf{R^2}\)</span>), RMSE, MSE, and MAE for performance measurements. Refer to <strong>Residual Sum of Squares (RSS)</strong> derived from <strong>Simple Linear Regression</strong> discussed in <strong>Statistical Computation</strong>.      </p>

<div class="sourceCode" id="cb1303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1303-1" data-line-number="1">prediction.score &lt;-<span class="st"> </span><span class="cf">function</span>(y, y.hat) {</a>
<a class="sourceLine" id="cb1303-2" data-line-number="2">  RMSE       =<span class="st"> </span>MSE =<span class="st"> </span>MAE =<span class="st"> </span>RMSLE =<span class="st"> </span>R2 =<span class="st"> </span>Bias =<span class="st"> </span>Variance =<span class="st">  </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1303-3" data-line-number="3">  RSS        =<span class="st"> </span><span class="kw">sum</span>( (y <span class="op">-</span><span class="st"> </span>y.hat)<span class="op">^</span><span class="dv">2</span> )          <span class="co"># residual sum of squares (RSS)</span></a>
<a class="sourceLine" id="cb1303-4" data-line-number="4">  ESS        =<span class="st"> </span><span class="kw">sum</span>( (y.hat <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span> )    <span class="co"># explained sum of squares (ESS)</span></a>
<a class="sourceLine" id="cb1303-5" data-line-number="5">  TSS        =<span class="st"> </span>RSS <span class="op">+</span><span class="st"> </span>ESS                     <span class="co"># total sum of squares </span></a>
<a class="sourceLine" id="cb1303-6" data-line-number="6">                                             <span class="co"># (TSS) = RSS + ESS</span></a>
<a class="sourceLine" id="cb1303-7" data-line-number="7">  <span class="cf">if</span> (TSS) {</a>
<a class="sourceLine" id="cb1303-8" data-line-number="8">  R2         =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>RSS<span class="op">/</span>TSS                   <span class="co"># R squared</span></a>
<a class="sourceLine" id="cb1303-9" data-line-number="9">  RMSE       =<span class="st"> </span><span class="kw">sqrt</span>(RSS <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(y))         <span class="co"># Root mean square error</span></a>
<a class="sourceLine" id="cb1303-10" data-line-number="10">  MSE        =<span class="st"> </span><span class="kw">mean</span>( (y <span class="op">-</span><span class="st"> </span>y.hat)<span class="op">^</span><span class="dv">2</span> )         <span class="co"># Mean square error</span></a>
<a class="sourceLine" id="cb1303-11" data-line-number="11">  MAE        =<span class="st"> </span><span class="kw">sum</span>( <span class="kw">abs</span>(y <span class="op">-</span><span class="st"> </span>y.hat) )         <span class="co"># mean absolute error</span></a>
<a class="sourceLine" id="cb1303-12" data-line-number="12">  RMSLE      =<span class="st"> </span>ModelMetrics<span class="op">::</span><span class="kw">rmsle</span>(y, y.hat) <span class="co"># Root Mean square log error</span></a>
<a class="sourceLine" id="cb1303-13" data-line-number="13">  }</a>
<a class="sourceLine" id="cb1303-14" data-line-number="14">  result =<span class="st"> </span><span class="kw">data.frame</span>( <span class="st">&quot;Rsquare&quot;</span> =<span class="st"> </span>R2,     <span class="st">&quot;MSE&quot;</span> =<span class="st"> </span>MSE, <span class="st">&quot;MAE&quot;</span> =<span class="st"> </span>MAE, </a>
<a class="sourceLine" id="cb1303-15" data-line-number="15">                          <span class="st">&quot;RMSE&quot;</span> =<span class="st"> </span>RMSE, <span class="st">&quot;RMSLE&quot;</span> =<span class="st"> </span>RMSLE)</a>
<a class="sourceLine" id="cb1303-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;fitted.values&quot;</span> =<span class="st"> </span>y.hat, <span class="st">&quot;loss&quot;</span> =<span class="st"> </span><span class="kw">round</span>(result,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1303-17" data-line-number="17">}</a></code></pre></div>
<div class="sourceCode" id="cb1304"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1304-1" data-line-number="1">my.predict  &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, x, y,  <span class="dt">resid =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1304-2" data-line-number="2">                 <span class="dt">tendency =</span> <span class="cf">function</span>(top, <span class="dt">resid =</span> <span class="ot">NULL</span>) { <span class="kw">mean</span>(top<span class="op">$</span>response) },</a>
<a class="sourceLine" id="cb1304-3" data-line-number="3">                 <span class="dt">method   =</span> prediction.score, <span class="dt">tabletree =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1304-4" data-line-number="4">  n          =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1304-5" data-line-number="5">  responses  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1304-6" data-line-number="6">  model      =<span class="st"> </span>my.model<span class="op">$</span>model</a>
<a class="sourceLine" id="cb1304-7" data-line-number="7">  categories =<span class="st"> </span>my.model<span class="op">$</span>categories</a>
<a class="sourceLine" id="cb1304-8" data-line-number="8">  <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) { model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model) }</a>
<a class="sourceLine" id="cb1304-9" data-line-number="9">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1304-10" data-line-number="10">    node =<span class="st"> </span>model[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb1304-11" data-line-number="11">    <span class="cf">while</span> (<span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1304-12" data-line-number="12">      <span class="cf">if</span> (node<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) {</a>
<a class="sourceLine" id="cb1304-13" data-line-number="13">          <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span> ) { </a>
<a class="sourceLine" id="cb1304-14" data-line-number="14">            top          =<span class="st"> </span>my.model<span class="op">$</span>model[[node<span class="op">$</span>N]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1304-15" data-line-number="15">            responses[i] =<span class="st"> </span><span class="kw">tendency</span>( top, resid )</a>
<a class="sourceLine" id="cb1304-16" data-line-number="16">          } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1304-17" data-line-number="17">            responses[i] =<span class="st"> </span>node<span class="op">$</span>ymean</a>
<a class="sourceLine" id="cb1304-18" data-line-number="18">          }</a>
<a class="sourceLine" id="cb1304-19" data-line-number="19">          <span class="cf">break</span></a>
<a class="sourceLine" id="cb1304-20" data-line-number="20">      }</a>
<a class="sourceLine" id="cb1304-21" data-line-number="21">      children    =<span class="st"> </span>model[<span class="kw">which</span>( model<span class="op">$</span>P <span class="op">==</span><span class="st"> </span>node<span class="op">$</span>N ),]</a>
<a class="sourceLine" id="cb1304-22" data-line-number="22">      cat         =<span class="st"> </span>categories[[node<span class="op">$</span>feature]]</a>
<a class="sourceLine" id="cb1304-23" data-line-number="23">      split       =<span class="st"> </span>node<span class="op">$</span>split</a>
<a class="sourceLine" id="cb1304-24" data-line-number="24">      val         =<span class="st"> </span>x[i, <span class="kw">c</span>(node<span class="op">$</span>feature)]</a>
<a class="sourceLine" id="cb1304-25" data-line-number="25">      <span class="cf">if</span> (<span class="kw">is.factor</span>(val)) {</a>
<a class="sourceLine" id="cb1304-26" data-line-number="26">        s         =<span class="st"> </span>base<span class="op">::</span><span class="kw">strsplit</span>(split,<span class="ot">NULL</span>)[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb1304-27" data-line-number="27">        direction =<span class="st"> </span>s[<span class="kw">which</span>(cat <span class="op">%in%</span><span class="st"> </span>val )]</a>
<a class="sourceLine" id="cb1304-28" data-line-number="28">        direction =<span class="st"> </span><span class="kw">ifelse</span>(direction <span class="op">==</span><span class="st"> &#39;L&#39;</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1304-29" data-line-number="29">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1304-30" data-line-number="30">        split     =<span class="st"> </span><span class="kw">as.numeric</span>(split)</a>
<a class="sourceLine" id="cb1304-31" data-line-number="31">        val       =<span class="st"> </span><span class="kw">as.numeric</span>(val)</a>
<a class="sourceLine" id="cb1304-32" data-line-number="32">        direction =<span class="st"> </span><span class="kw">ifelse</span>(val <span class="op">&lt;</span><span class="st"> </span>split, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1304-33" data-line-number="33">      }</a>
<a class="sourceLine" id="cb1304-34" data-line-number="34">      node =<span class="st"> </span>children[direction,]</a>
<a class="sourceLine" id="cb1304-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1304-36" data-line-number="36">  }</a>
<a class="sourceLine" id="cb1304-37" data-line-number="37">  <span class="kw">method</span>(y,  responses) </a>
<a class="sourceLine" id="cb1304-38" data-line-number="38">}</a></code></pre></div>

<p>Let us use the training set to build our tree model with minbucket=5 and maxdepth=3 like so:</p>

<div class="sourceCode" id="cb1305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1305-1" data-line-number="1">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1305-2" data-line-number="2">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1305-3" data-line-number="3">my.tree.model =<span class="st"> </span><span class="kw">my.tree</span>(features, target, trainset, <span class="dt">minbucket=</span><span class="dv">5</span>, <span class="dt">maxdepth=</span><span class="dv">3</span>)</a></code></pre></div>

<p>Then, we perform prediction using the test set:</p>

<div class="sourceCode" id="cb1306"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1306-1" data-line-number="1">(<span class="dt">predicted =</span> <span class="kw">my.predict</span>(my.tree.model, test.predictors, test.target))<span class="op">$</span>loss</a></code></pre></div>
<pre><code>##   Rsquare   MSE   MAE  RMSE RMSLE
## 1     0.5 2.151 1.467 1.467 0.069</code></pre>

<p>Let us train our tree model using minbucket=5 and maxdepth=3 like so:</p>

<div class="sourceCode" id="cb1308"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1308-1" data-line-number="1">my.tree.model =<span class="st"> </span><span class="kw">my.tree</span>(features, target, trainset, <span class="dt">minbucket=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>)</a></code></pre></div>
<p>Then, we perform prediction using the test set the second time:</p>

<div class="sourceCode" id="cb1309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1309-1" data-line-number="1">(<span class="dt">predicted =</span> <span class="kw">my.predict</span>(my.tree.model, test.predictors, test.target))<span class="op">$</span>loss</a></code></pre></div>
<pre><code>##   Rsquare    MSE  MAE RMSE RMSLE
## 1     0.5 0.4225 0.65 0.65  0.03</code></pre>

<p>Compared to the previous result, we get a larger <strong>RMSE</strong> this time around. The predicted values from the last prediction are as follows:</p>
<div class="sourceCode" id="cb1311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1311-1" data-line-number="1">predicted<span class="op">$</span>fitted.values</a></code></pre></div>
<pre><code>## [1] 20.35</code></pre>
<p>compared to the observed values:</p>
<div class="sourceCode" id="cb1313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1313-1" data-line-number="1">test.target</a></code></pre></div>
<pre><code>## [1] 21</code></pre>
<p>Note that the predicted value is not as close to the observed value as we expect. That is because prediction performance may not always suffice using only a single tree. As we will see in later sections, there are other ways to obtain optimal performance, such as <strong>Bagging</strong> and <strong>Boosting</strong>.</p>
<p><strong>Third</strong>, let us continue our discussion on optimizing our tree by using <strong>cross-validation (cv)</strong> to select an optimal <strong>cp</strong>. Note that this is not necessarily intended to boost performance; rather, we use <strong>cv</strong> to balance bias and variance (e.g., avoid overfitting). Here, we use our example implementation of k-fold cross-validation for a tree model. While our <strong>cv</strong> function uses prediction to measure model performance, we evaluate which <strong>cost complexity (cp)</strong> parameter value best supports our model tree. </p>

<div class="sourceCode" id="cb1315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1315-1" data-line-number="1">cross.validate &lt;-<span class="cf">function</span> (features, target, data, <span class="dt">k=</span><span class="dv">10</span>,  </a>
<a class="sourceLine" id="cb1315-2" data-line-number="2">                           <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">50</span>, cp.list) {   </a>
<a class="sourceLine" id="cb1315-3" data-line-number="3">  fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(data[,<span class="kw">c</span>(target)], <span class="dt">k =</span> k, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1315-4" data-line-number="4">  cv.result =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="kw">length</span>(cp.list), <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb1315-5" data-line-number="5">  models =<span class="st"> </span><span class="kw">list</span>(); tests =<span class="st"> </span><span class="kw">list</span>() </a>
<a class="sourceLine" id="cb1315-6" data-line-number="6">  <span class="co"># Build a set of large trees</span></a>
<a class="sourceLine" id="cb1315-7" data-line-number="7">  <span class="cf">for</span> (foldname <span class="cf">in</span> <span class="kw">names</span>(fold.indices)) {</a>
<a class="sourceLine" id="cb1315-8" data-line-number="8">        fold  =<span class="st"> </span>fold.indices[[foldname]]</a>
<a class="sourceLine" id="cb1315-9" data-line-number="9">        test  =<span class="st"> </span>data[fold,]</a>
<a class="sourceLine" id="cb1315-10" data-line-number="10">        train =<span class="st"> </span>data[<span class="op">-</span>fold,]</a>
<a class="sourceLine" id="cb1315-11" data-line-number="11">        models[[foldname]] =<span class="st"> </span><span class="kw">my.tree</span>(features, target, train, </a>
<a class="sourceLine" id="cb1315-12" data-line-number="12">                                <span class="dt">minbucket=</span>minbucket, <span class="dt">maxdepth=</span>maxdepth)</a>
<a class="sourceLine" id="cb1315-13" data-line-number="13">        tests[[foldname]]  =<span class="st"> </span>test</a>
<a class="sourceLine" id="cb1315-14" data-line-number="14">  }     </a>
<a class="sourceLine" id="cb1315-15" data-line-number="15">  <span class="co"># Prune trees up to cost complexity</span></a>
<a class="sourceLine" id="cb1315-16" data-line-number="16">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(cp.list)) {</a>
<a class="sourceLine" id="cb1315-17" data-line-number="17">    fold.predictions =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1315-18" data-line-number="18">    <span class="cf">for</span> (foldname <span class="cf">in</span> <span class="kw">names</span>(fold.indices)) { <span class="co"># cross-validate</span></a>
<a class="sourceLine" id="cb1315-19" data-line-number="19">        model   =<span class="st"> </span>models[[foldname]]</a>
<a class="sourceLine" id="cb1315-20" data-line-number="20">        test    =<span class="st"> </span>tests[[foldname]]</a>
<a class="sourceLine" id="cb1315-21" data-line-number="21">        x.label =<span class="st"> </span>test[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1315-22" data-line-number="22">        x.predictors =<span class="st"> </span>test[,<span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1315-23" data-line-number="23">        pruned.tree  =<span class="st"> </span><span class="kw">my.prune.tree</span>(model, <span class="dt">cp =</span> cp.list[i]) </a>
<a class="sourceLine" id="cb1315-24" data-line-number="24">        pruned.model =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>pruned.tree<span class="op">$</span>subtree, </a>
<a class="sourceLine" id="cb1315-25" data-line-number="25">                            <span class="st">&quot;categories&quot;</span> =<span class="st"> </span>model<span class="op">$</span>categories)</a>
<a class="sourceLine" id="cb1315-26" data-line-number="26">        predicted    =<span class="st"> </span><span class="kw">my.predict</span>(pruned.model,  x.predictors, x.label, </a>
<a class="sourceLine" id="cb1315-27" data-line-number="27">                                  <span class="dt">tabletree=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1315-28" data-line-number="28">        fold.predictions =<span class="st"> </span><span class="kw">rbind</span>(fold.predictions, <span class="kw">c</span>(cp.list[i], </a>
<a class="sourceLine" id="cb1315-29" data-line-number="29">                                <span class="kw">as.matrix</span>(predicted<span class="op">$</span>loss)))</a>
<a class="sourceLine" id="cb1315-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1315-31" data-line-number="31">    cv.result[i,] =<span class="st"> </span><span class="kw">apply</span>( fold.predictions , <span class="dv">2</span>, mean)</a>
<a class="sourceLine" id="cb1315-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb1315-33" data-line-number="33">  <span class="kw">colnames</span>(cv.result) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;cp&quot;</span>, <span class="st">&quot;Rsquare&quot;</span>, <span class="st">&quot;RMSE&quot;</span>, <span class="st">&quot;MSE&quot;</span>, <span class="st">&quot;MAE&quot;</span>, <span class="st">&quot;RMSLE&quot;</span> )</a>
<a class="sourceLine" id="cb1315-34" data-line-number="34">  cv.result</a>
<a class="sourceLine" id="cb1315-35" data-line-number="35">}</a></code></pre></div>

<p>We use <strong>10-fold</strong> cross-validation, fixing our parameters to the default values, namely minbucket=1 and maxdepth=50 to achieve the largest tree. Then initialize a set of cp values. The idea is to feed our <strong>cv</strong> function arbitrarily with a set of <strong>cp</strong> and see if any of the <strong>cp</strong> renders the lowest performance metrics (of our choice) for each corresponding pruned subtree.</p>

<div class="sourceCode" id="cb1316"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1316-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">145</span>)</a>
<a class="sourceLine" id="cb1316-2" data-line-number="2">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1316-3" data-line-number="3">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1316-4" data-line-number="4">datacars     =<span class="st"> </span>mtcars2 </a>
<a class="sourceLine" id="cb1316-5" data-line-number="5">cp.list =<span class="st"> </span><span class="kw">seq</span>(<span class="dt">from=</span><span class="fl">0.0</span>, <span class="dt">to=</span><span class="fl">0.01</span>, <span class="dt">length.out=</span><span class="dv">10</span>) <span class="co"># crude but adjustable</span></a>
<a class="sourceLine" id="cb1316-6" data-line-number="6">cvalid =<span class="st"> </span><span class="kw">cross.validate</span>(features, target, datacars, <span class="dt">k=</span><span class="dv">10</span>, <span class="dt">cp.list =</span> cp.list)</a></code></pre></div>

<p>Using the code below, we can plot our chosen performance metrics against the set of <strong>cp</strong>. See Figure <a href="machinelearning2.html#fig:cparam">10.6</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cparam"></span>
<img src="DS_files/figure-html/cparam-1.png" alt="Cost Complexity" width="100%" />
<p class="caption">
Figure 10.6: Cost Complexity
</p>
</div>
<p>The four plots show an optimal <strong>cp</strong> somewhere below 0.006. As an experiment, it may help to re-run the code with a new list of <strong>cp</strong> range that is more granular to see what the plot may show in terms of a possible lower <strong>cp</strong>.</p>
</div>
<div id="ensemble-methods" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.2</span> Ensemble Methods <a href="machinelearning2.html#ensemble-methods" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In place of <strong>pruning</strong> and <strong>cross-validation</strong> of <strong>decision trees</strong>, an alternative method to use is called <strong>ensemble</strong>. Primarily, the goal is to balance the trade-off between bias and variance.</p>
<p>There are three <strong>Ensemble methods</strong> used for decision trees, namely <strong>Bagging</strong>, <strong>Boosting</strong>, and <strong>Stacking</strong>. In this section, we discuss the basic concept of the three methods. Then, in separate sections ahead, we cover <strong>Random Forest</strong>, <strong>AdaBoost</strong>, and <strong>XBoost</strong> to illustrate their application.</p>
<p><strong>Bagging</strong> </p>
<p>The first method is called <strong>Bagging</strong>, which is a popular ensemble method introduced by Leo Breiman <span class="citation">(<a href="bibliography.html#ref-ref488">1994</a>)</span>. The name stands for <strong>Bootstrap Aggregation</strong>. The idea is to perform random selection <strong>with replacement</strong> against our data set. We bundle data points randomly selected into a sample set called <strong>bootstrap sample</strong>, which is used to construct a tree model. We repeat this random selection several times to build a list of bootstrap samples that become the basis for constructing a set of tree models used to perform average computation against all regression (or predictions) of the tree models. In effect, we can balance well the spread of our data points across several sample sets (trees) - we compensate for the high variance of individual trees by averaging. Though we lose interpretability, by this random selection with replacement, the combined models show reduced variance and low bias with higher prediction accuracy.</p>
<p>To illustrate, let X be a vector of data points.</p>
<p><span class="math display">\[\begin{align}
X =  (x_{1}, x_{2}, ..., x_{n})
\end{align}\]</span></p>
<p>Let S be bootstrap samples from which trees (T) are modeled :</p>
<p><span class="math display">\[\begin{align*}
S_{1} \in  \{x_{1}, x_{2}, ..., x_{n}\} \to T_{1} \\
S_{2} \in  \{x_{1}, x_{2}, ..., x_{n}\} \to T_{2} \\
\vdots \\
S_{b} \in  \{x_{1}, x_{2}, ..., x_{n}\} \to T_{b} 
\end{align*}\]</span></p>
<p>Let <span class="math inline">\(T_{i}(X^{(test)}, Y^{(test)})\)</span> be the tree model function to compute the average weight of a tree, where X represents predictors for the test set, and Y represents target labels for the test set. The goal is to predict Y by aggregating output from all the <strong>bagged</strong> trees.</p>
<p><span class="math display">\[\begin{align}
Y_{(aggregate)} = \frac{1}{k}\sum_{i=1}^{k} T_{i}(X^{(test)}_i, Y^{(test)}_i)
\end{align}\]</span></p>
<p><strong>First</strong>, we show our example implementation of our <strong>Bagging</strong> algorithm. Here, we introduce one parameter, namely <strong>b</strong>, which indicates the number of tree models we want to generate.</p>
<div class="sourceCode" id="cb1317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1317-1" data-line-number="1">my.bagging &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1317-2" data-line-number="2">                             <span class="dt">maxdepth=</span><span class="dv">50</span>, <span class="dt">b=</span><span class="dv">5</span>, <span class="dt">size=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1317-3" data-line-number="3">  models  =<span class="st"> </span><span class="kw">list</span>() </a>
<a class="sourceLine" id="cb1317-4" data-line-number="4">  s       =<span class="st"> </span><span class="kw">sample.int</span>(size, b <span class="op">*</span><span class="st"> </span>size, <span class="dt">replace =</span> <span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1317-5" data-line-number="5">  m       =<span class="st"> </span><span class="kw">matrix</span>(s, b, size, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1317-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb1317-7" data-line-number="7">     sample.indices =<span class="st"> </span>m[i,]</a>
<a class="sourceLine" id="cb1317-8" data-line-number="8">     samples        =<span class="st"> </span>data[sample.indices, ]</a>
<a class="sourceLine" id="cb1317-9" data-line-number="9">     model          =<span class="st"> </span><span class="kw">h.learner</span>(features, target, samples)</a>
<a class="sourceLine" id="cb1317-10" data-line-number="10">     models[[i]]    =<span class="st"> </span>model</a>
<a class="sourceLine" id="cb1317-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb1317-12" data-line-number="12">  models</a>
<a class="sourceLine" id="cb1317-13" data-line-number="13">}</a></code></pre></div>
<p><strong>Second</strong>, we generate predictions for each ensemble model. For prediction, we use <strong>RMSE</strong> as our measure of performance. We also use the <strong>apply(.)</strong> function to average the derived fitted values from each tree.</p>
<div class="sourceCode" id="cb1318"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1318-1" data-line-number="1">my.bagging.predict &lt;-<span class="st"> </span><span class="cf">function</span>(ensemble.model, x, y, <span class="dt">b=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1318-2" data-line-number="2">  predictions =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1318-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb1318-4" data-line-number="4">     model       =<span class="st"> </span>ensemble.model[[i]]</a>
<a class="sourceLine" id="cb1318-5" data-line-number="5">     predicted   =<span class="st"> </span><span class="kw">h.score</span>(model, x, y)</a>
<a class="sourceLine" id="cb1318-6" data-line-number="6">     predictions =<span class="st"> </span><span class="kw">rbind</span>(predictions, predicted<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1318-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1318-8" data-line-number="8">  y.hat =<span class="st"> </span><span class="kw">apply</span>(predictions,<span class="dv">2</span>,mean)</a>
<a class="sourceLine" id="cb1318-9" data-line-number="9">  <span class="kw">prediction.score</span>(y, y.hat)</a>
<a class="sourceLine" id="cb1318-10" data-line-number="10">}</a></code></pre></div>
<p><strong>Third</strong>, our choice of base learner is our <strong>regression tree</strong>, namely <strong>my.tree(.)</strong>. Also, our choice of score function corresponds to <strong>my.predict(.)</strong>. In other literature, such base learners produce estimates (deemed hypothetical); thus, we tend to see learner functions denoted as <strong>h(.)</strong>. Let us follow such convention, which becomes apparent in our discussion of the different ensemble methods.</p>
<div class="sourceCode" id="cb1319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1319-1" data-line-number="1">h.learner &lt;-<span class="st"> </span>my.tree</a>
<a class="sourceLine" id="cb1319-2" data-line-number="2">h.score   &lt;-<span class="st"> </span>my.predict</a></code></pre></div>
<p><strong>Fourth</strong>, we divide our dataset into train set and test set as before. We use our previous train set to generate multiple resamples for our <strong>Bagging</strong> method - that is, we resample 100 times (b=100) with replacement, given a fixed sample size that is equal to the size of the train set.</p>

<div class="sourceCode" id="cb1320"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1320-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1320-2" data-line-number="2">b=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1320-3" data-line-number="3">h.model =<span class="st"> </span><span class="kw">my.bagging</span>(features, target, trainset, <span class="dt">b=</span>b, <span class="dt">size=</span><span class="kw">nrow</span>(trainset))</a>
<a class="sourceLine" id="cb1320-4" data-line-number="4">h.pred  =<span class="st"> </span><span class="kw">my.bagging.predict</span>(h.model, test.predictors, test.target, <span class="dt">b=</span>b)</a>
<a class="sourceLine" id="cb1320-5" data-line-number="5"><span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span> =<span class="st"> </span>h.pred<span class="op">$</span>fitted.values, <span class="st">&quot;Actual&quot;</span> =<span class="st"> </span>test.target)</a></code></pre></div>
<pre><code>## Predicted    Actual 
##     20.75     21.00</code></pre>
<div class="sourceCode" id="cb1322"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1322-1" data-line-number="1">h.pred<span class="op">$</span>loss</a></code></pre></div>
<pre><code>##   Rsquare   MSE   MAE  RMSE  RMSLE
## 1     0.5 0.065 0.255 0.255 0.0117</code></pre>

<p>As we can see, we have a predicted value closer to the actual value using <strong>Bagging</strong>.</p>
<p>In the next section, we cover <strong>Random Forest</strong>, a variant of <strong>Bagging</strong>.</p>
<p><strong>Boosting</strong> </p>
<p>The second ensemble method is called <strong>Boosting</strong>, introduced by Y.Freund and R. Schapire <span class="citation">(<a href="bibliography.html#ref-ref510">1999</a>)</span>. It is considered a <strong>stagewise additive modeling</strong> technique because it trains an ensemble of models sequentially, <strong>boosting</strong> performance one model after the next. For each iteration, each model calculates residuals as output which get passed on as weights to <strong>boost</strong> the performance of the next learning model. In later sections ahead, we cover three variants of <strong>Boosting</strong>, namely <strong>AdaBoost</strong>, <strong>Gradient Boost</strong>, and <strong>XGBoost</strong> <span class="citation">(R. Schapire <a href="bibliography.html#ref-ref501">1999</a>)</span>.</p>
<p><strong>Stacking</strong> </p>
<p>Finally, the third ensemble method is called <strong>Stacking</strong>, and it is a <strong>Meta-Learning</strong> technique because we <strong>stack</strong> different heterogeneous modeling techniques such as <strong>Random Forest</strong>, <strong>SVM</strong>, and <strong>XGBoost</strong>, allowing these chosen models to learn independently, providing their corresponding predictions which we then evaluate to determine the order of accuracy.</p>
</div>
<div id="random-forest" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.3</span> Random Forest <a href="machinelearning2.html#random-forest" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Random Forest</strong> is a refinement to <strong>Bagging</strong> introduced by Leo Breiman <span class="citation">(<a href="bibliography.html#ref-ref499">2001</a>)</span>. There are two concepts introduced. For one, random forest performs a random selection of samples (called <strong>Bootstrap</strong> samples) and a random selection of the input variables (features). Using a subset of features for every node bifurcation adds uniqueness to the underlying tree; thus, the number of correlations among a sampled set (or trees) is reduced. Additionally, this handles high dimensionality. The number of features in a subset is controlled by a parameter (m) when there are (p) parameters (or variables).</p>
<p><span class="math display">\[\begin{align}
m = \sqrt{p}\ \ \ \ \ \ \text{where m is a subset of p}
\end{align}\]</span></p>
<p>For another, it also introduces <strong>out-of-bag (OOB)</strong>. Figure <a href="machinelearning2.html#fig:outofbag">10.7</a> best illustrates the idea of <strong>Random Forest</strong> with <strong>OOB</strong>. Note that while the figure shows as if a flask is pouring data points into a bag, pretend for a moment that the flask âmagicallyâ replaces a data point that leaves the flask.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:outofbag"></span>
<img src="outofbag.png" alt="Random Forest (Out-Of-Bag)" width="70%" />
<p class="caption">
Figure 10.7: Random Forest (Out-Of-Bag)
</p>
</div>
<p>In the figure, one point to emphasize is that each data point from the out-of-bag is used to test all tree models (in the ensemble) that are not trained using the data point. In essence, this cross-validation saves us from allocating a separate validation set. It becomes apparent in the implementation of our validation function later.</p>
<p>So how does <strong>Random Forest</strong> work?</p>
<p><strong>First</strong>, let us implement a variant of our base learner, <strong>my.tree(.)</strong>, and name it as <strong>my.rf.tree(.)</strong>. We then include a helper function to generate a subspace for our features. From this subspace, we randomly sample a subset of all the features (to be used by each split). We do not track re-used features in subsets; though, perhaps we leave this as an exercise. From here, we have a new regression tree function for our random forest:</p>

<div class="sourceCode" id="cb1324"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1324-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1324-2" data-line-number="2">my.rf.tree &lt;-<span class="cf">function</span>(features, target, dataset, <span class="dt">minbucket =</span> <span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">50</span>){</a>
<a class="sourceLine" id="cb1324-3" data-line-number="3">  random.subspace &lt;-<span class="st"> </span><span class="cf">function</span>(features) {</a>
<a class="sourceLine" id="cb1324-4" data-line-number="4">    p           =<span class="st"> </span><span class="kw">length</span>(features)</a>
<a class="sourceLine" id="cb1324-5" data-line-number="5">    subspaces   =<span class="st"> </span><span class="kw">combn</span>(p,<span class="kw">sqrt</span>(p)) <span class="co"># m = sqrt(p)</span></a>
<a class="sourceLine" id="cb1324-6" data-line-number="6">    n.sub       =<span class="st"> </span><span class="kw">ncol</span>(subspaces)</a>
<a class="sourceLine" id="cb1324-7" data-line-number="7">    sub.indices =<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dv">1</span><span class="op">:</span>n.sub), <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1324-8" data-line-number="8">    features[subspaces[,sub.indices]]</a>
<a class="sourceLine" id="cb1324-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb1324-10" data-line-number="10">  split.child &lt;-<span class="st"> </span><span class="cf">function</span>(indices) {</a>
<a class="sourceLine" id="cb1324-11" data-line-number="11">    sub.features =<span class="st"> </span><span class="kw">random.subspace</span>(features)</a>
<a class="sourceLine" id="cb1324-12" data-line-number="12">    data =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span>indices, <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>dataset)</a>
<a class="sourceLine" id="cb1324-13" data-line-number="13">    <span class="kw">rank.importance</span>(sub.features, target, data, minbucket, categories)</a>
<a class="sourceLine" id="cb1324-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1324-15" data-line-number="15">  build.level.tree &lt;-<span class="st"> </span><span class="cf">function</span>(my.stack,  <span class="dt">top.count =</span> <span class="dv">0</span>, <span class="dt">level =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1324-16" data-line-number="16">                               <span class="dt">model =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb1324-17" data-line-number="17">    <span class="co">#################################################</span></a>
<a class="sourceLine" id="cb1324-18" data-line-number="18">    <span class="co">### Similar content as my.regression.tree(...)</span></a>
<a class="sourceLine" id="cb1324-19" data-line-number="19">    <span class="co">#################################################</span></a>
<a class="sourceLine" id="cb1324-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb1324-21" data-line-number="21">  my.stack =<span class="st"> </span><span class="kw">queue</span>()</a>
<a class="sourceLine" id="cb1324-22" data-line-number="22">  indices =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(dataset))</a>
<a class="sourceLine" id="cb1324-23" data-line-number="23">  categories =<span class="st"> </span><span class="kw">get.categories</span>(target, dataset)</a>
<a class="sourceLine" id="cb1324-24" data-line-number="24">  root =<span class="st"> </span><span class="kw">split.child</span>(indices); root<span class="op">$</span>parent =<span class="st"> </span><span class="dv">0</span> </a>
<a class="sourceLine" id="cb1324-25" data-line-number="25">  <span class="kw">pushback</span>(my.stack, root)</a>
<a class="sourceLine" id="cb1324-26" data-line-number="26">  my.model =<span class="st"> </span><span class="kw">build.level.tree</span>(my.stack)</a>
<a class="sourceLine" id="cb1324-27" data-line-number="27">  <span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>my.model, <span class="st">&quot;categories&quot;</span> =<span class="st"> </span>categories)</a>
<a class="sourceLine" id="cb1324-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb1324-29" data-line-number="29">h.learner &lt;-<span class="st"> </span>my.rf.tree</a></code></pre></div>

<p><strong>Second</strong>, our example implementation of <strong>Random Forest</strong> uses the same implementation as <strong>my.bagging(.)</strong> function; however, we keep track of data points excluded from each sample. Note that there is a tendency for data points to be selected multiple times in random sampling with replacement. On the hand, there is also a tendency for data points to be not selected. Data points that are not selected are tracked and stored in an out-of-bag structure.</p>

<div class="sourceCode" id="cb1325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1325-1" data-line-number="1">my.random.forest &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1325-2" data-line-number="2">                             <span class="dt">maxdepth=</span><span class="dv">50</span>, <span class="dt">b=</span><span class="dv">5</span>, <span class="dt">size=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1325-3" data-line-number="3">  models  =<span class="st"> </span><span class="kw">list</span>(); oobs =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1325-4" data-line-number="4">  data.indices =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(data))</a>
<a class="sourceLine" id="cb1325-5" data-line-number="5">  sample.subspace =<span class="st"> </span><span class="kw">sample.int</span>(size, b <span class="op">*</span><span class="st"> </span>size, <span class="dt">replace =</span> <span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1325-6" data-line-number="6">  m =<span class="st"> </span><span class="kw">matrix</span>(sample.subspace, b, size, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1325-7" data-line-number="7">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb1325-8" data-line-number="8">     sample.indices =<span class="st"> </span>m[i,]</a>
<a class="sourceLine" id="cb1325-9" data-line-number="9">     samples        =<span class="st"> </span>data[sample.indices, ]</a>
<a class="sourceLine" id="cb1325-10" data-line-number="10">     oob.indices    =<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span>(data.indices <span class="op">%in%</span><span class="st"> </span>sample.indices))</a>
<a class="sourceLine" id="cb1325-11" data-line-number="11">     model          =<span class="st"> </span><span class="kw">h.learner</span>(features, target, samples)</a>
<a class="sourceLine" id="cb1325-12" data-line-number="12">     models[[i]]    =<span class="st"> </span>model</a>
<a class="sourceLine" id="cb1325-13" data-line-number="13">     oobs[[i]]      =<span class="st"> </span>oob.indices <span class="co"># out of bag indices</span></a>
<a class="sourceLine" id="cb1325-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1325-15" data-line-number="15">  <span class="kw">list</span>(<span class="st">&quot;models&quot;</span> =<span class="st"> </span>models, <span class="st">&quot;oobs&quot;</span> =<span class="st"> </span>oobs )</a>
<a class="sourceLine" id="cb1325-16" data-line-number="16">}</a></code></pre></div>

<p>Here, our base learner for <strong>h.learner(.)</strong> references <strong>my.rf.tree(.)</strong>.</p>
<p>Now, to use our function, we resample from the mtcars train set about 100 times (b=100) with replacement, given a fixed sample size equal to the size of the train set. A unique regression tree is modeled for each sample. The output is a set of 100 uniquely trained regression trees. This set is our <strong>Random Forest</strong> <strong>ensemble model</strong>.</p>
<div class="sourceCode" id="cb1326"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1326-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1326-2" data-line-number="2">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1326-3" data-line-number="3">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1326-4" data-line-number="4">b=<span class="dv">100</span></a>
<a class="sourceLine" id="cb1326-5" data-line-number="5">my.rf.model =<span class="st"> </span><span class="kw">my.random.forest</span>(features, target, trainset, </a>
<a class="sourceLine" id="cb1326-6" data-line-number="6">                               <span class="dt">b=</span>b, <span class="dt">size=</span><span class="kw">nrow</span>(trainset))</a></code></pre></div>
<p><strong>Third</strong>, we run validation against the ensemble using our implementation, namely <strong>my.rf.validate(.)</strong>. Our predictor uses <strong>h.score(.)</strong> which references our original <strong>my.predict(.)</strong> function. The prediction is measured using either <strong>SSE</strong>, <strong>MSE</strong>, or <strong>RMSE</strong>. In our case, we use <strong>MSE</strong>.</p>

<div class="sourceCode" id="cb1327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1327-1" data-line-number="1">my.rf.validate &lt;-<span class="st"> </span><span class="cf">function</span>(ensemble, features, target, data, <span class="dt">b=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1327-2" data-line-number="2">  predictions   =<span class="st"> </span>loss =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1327-3" data-line-number="3">  n             =<span class="st"> </span><span class="kw">nrow</span>(data)</a>
<a class="sourceLine" id="cb1327-4" data-line-number="4">  train.indices =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(data))</a>
<a class="sourceLine" id="cb1327-5" data-line-number="5">  y             =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1327-6" data-line-number="6">  oob.yhat      =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1327-7" data-line-number="7">  oob.error     =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1327-8" data-line-number="8">  m =<span class="st"> </span><span class="kw">matrix</span>(<span class="ot">NA</span>, n, b, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1327-9" data-line-number="9">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1327-10" data-line-number="10">    ic =<span class="st"> </span><span class="kw">as.character</span>(i)</a>
<a class="sourceLine" id="cb1327-11" data-line-number="11">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb1327-12" data-line-number="12">      model          =<span class="st"> </span>ensemble<span class="op">$</span>models[[j]]</a>
<a class="sourceLine" id="cb1327-13" data-line-number="13">      oob.indices    =<span class="st"> </span>ensemble<span class="op">$</span>oobs[[j]]</a>
<a class="sourceLine" id="cb1327-14" data-line-number="14">      <span class="co"># The existence of the data point (i) in the out-of-bag means </span></a>
<a class="sourceLine" id="cb1327-15" data-line-number="15">      <span class="co"># that the data point is not used to train the model. Thus, we</span></a>
<a class="sourceLine" id="cb1327-16" data-line-number="16">      <span class="co"># use the data point as test for prediction using the model.</span></a>
<a class="sourceLine" id="cb1327-17" data-line-number="17">      <span class="cf">if</span> (<span class="kw">sum</span>(oob.indices <span class="op">%in%</span><span class="st"> </span>i)) {</a>
<a class="sourceLine" id="cb1327-18" data-line-number="18">         test        =<span class="st"> </span>data[i,]</a>
<a class="sourceLine" id="cb1327-19" data-line-number="19">         predicted   =<span class="st"> </span><span class="kw">h.score</span>(model, test[,<span class="kw">c</span>(features)], test[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1327-20" data-line-number="20">         m[i, j]     =<span class="st"> </span>predicted<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb1327-21" data-line-number="21">      }</a>
<a class="sourceLine" id="cb1327-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb1327-23" data-line-number="23">  }</a>
<a class="sourceLine" id="cb1327-24" data-line-number="24">  mse =<span class="st"> </span>rmse =<span class="st"> </span>sse =<span class="st">  </span><span class="kw">rep</span>(<span class="dv">0</span>, b)</a>
<a class="sourceLine" id="cb1327-25" data-line-number="25">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb1327-26" data-line-number="26">    tree.yhat =<span class="st"> </span>m[,<span class="dv">1</span><span class="op">:</span>j]</a>
<a class="sourceLine" id="cb1327-27" data-line-number="27">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(<span class="kw">nrow</span>(tree.yhat))) {</a>
<a class="sourceLine" id="cb1327-28" data-line-number="28">      tree.yhat =<span class="st"> </span><span class="kw">apply</span>(tree.yhat, <span class="dv">1</span>, mean, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1327-29" data-line-number="29">    }</a>
<a class="sourceLine" id="cb1327-30" data-line-number="30">    idx =<span class="st"> </span><span class="kw">which</span>(<span class="op">!</span><span class="kw">is.na</span>(tree.yhat))</a>
<a class="sourceLine" id="cb1327-31" data-line-number="31">    y.pred  =<span class="st"> </span>tree.yhat[idx]</a>
<a class="sourceLine" id="cb1327-32" data-line-number="32">    y.true  =<span class="st"> </span>y[idx]</a>
<a class="sourceLine" id="cb1327-33" data-line-number="33">    sse[j]  =<span class="st"> </span><span class="kw">sum</span>((y.true <span class="op">-</span><span class="st"> </span>y.pred)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1327-34" data-line-number="34">    mse[j]  =<span class="st"> </span><span class="kw">mean</span>((y.true <span class="op">-</span><span class="st"> </span>y.pred)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1327-35" data-line-number="35">    rmse[j] =<span class="st"> </span><span class="kw">sqrt</span>(mse[j])</a>
<a class="sourceLine" id="cb1327-36" data-line-number="36">  }</a>
<a class="sourceLine" id="cb1327-37" data-line-number="37">  <span class="kw">list</span>(<span class="st">&quot;sse&quot;</span> =<span class="st"> </span>sse, <span class="st">&quot;mse&quot;</span> =<span class="st"> </span>mse, <span class="st">&quot;rmse&quot;</span> =<span class="st"> </span>rmse)</a>
<a class="sourceLine" id="cb1327-38" data-line-number="38">}</a></code></pre></div>

<p>The implementation of the validation above ensures that data points from out-of-bag are used only for tree models that are not trained with the data points.</p>
<p>Let us use the function to validate our <strong>ensemble model</strong>. Our goal is to determine an optimal number of trees based on the least <strong>MSE</strong>.
</p>
<div class="sourceCode" id="cb1328"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1328-1" data-line-number="1">features     =<span class="st"> </span><span class="kw">names</span>(mtcars2)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(mtcars2) <span class="op">%in%</span><span class="st"> &quot;mpg&quot;</span>)]  </a>
<a class="sourceLine" id="cb1328-2" data-line-number="2">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>)</a>
<a class="sourceLine" id="cb1328-3" data-line-number="3">my.rf.cv.result =<span class="st"> </span><span class="kw">my.rf.validate</span>(my.rf.model, features, target, trainset, <span class="dt">b=</span>b)</a></code></pre></div>

<p>Assume that we limit our ensemble with a minimum number of trees to 10.</p>
<div class="sourceCode" id="cb1329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1329-1" data-line-number="1">min.tree =<span class="st"> </span><span class="dv">10</span></a></code></pre></div>
<p>If we then plot, we see that the optimal number of trees is around 62. See Figure <a href="machinelearning2.html#fig:randomforest1">10.8</a>.</p>
<div class="sourceCode" id="cb1330"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1330-1" data-line-number="1">cp =<span class="st"> </span><span class="kw">seq</span>(min.tree, b)</a>
<a class="sourceLine" id="cb1330-2" data-line-number="2"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1330-3" data-line-number="3">mse =<span class="st"> </span>my.rf.cv.result<span class="op">$</span>mse; sse =<span class="st"> </span>my.rf.cv.result<span class="op">$</span>sse</a>
<a class="sourceLine" id="cb1330-4" data-line-number="4">mse =<span class="st"> </span>mse[(min.tree)<span class="op">:</span>b]; sse =<span class="st"> </span>sse[(min.tree)<span class="op">:</span>b]</a>
<a class="sourceLine" id="cb1330-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">main=</span><span class="st">&quot;My Forest Model&quot;</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(cp), </a>
<a class="sourceLine" id="cb1330-6" data-line-number="6">     <span class="dt">ylim=</span> <span class="kw">range</span>(mse),</a>
<a class="sourceLine" id="cb1330-7" data-line-number="7">     <span class="dt">xlab=</span><span class="st">&quot;trees&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE&quot;</span>,  <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1330-8" data-line-number="8"><span class="kw">lines</span>(cp, mse, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1330-9" data-line-number="9"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">main=</span><span class="st">&quot;My Forest Model&quot;</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(cp), </a>
<a class="sourceLine" id="cb1330-10" data-line-number="10">     <span class="dt">ylim=</span> <span class="kw">range</span>(sse),</a>
<a class="sourceLine" id="cb1330-11" data-line-number="11">     <span class="dt">xlab=</span><span class="st">&quot;trees&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;SSE&quot;</span>,  <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1330-12" data-line-number="12"><span class="kw">lines</span>(cp, sse, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:randomforest1"></span>
<img src="DS_files/figure-html/randomforest1-1.png" alt="Weights" width="90%" />
<p class="caption">
Figure 10.8: Weights
</p>
</div>
<p>Using our <strong>test set</strong> to predict using the optimal number of trees in the ensemble, we get a prediction even closer to the actual value.</p>
<div class="sourceCode" id="cb1331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1331-1" data-line-number="1">my.rf.predict &lt;-<span class="st"> </span><span class="cf">function</span>(ensemble, x.feature, x.label, <span class="dt">b=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1331-2" data-line-number="2">  predictions =<span class="st"> </span>loss =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1331-3" data-line-number="3">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb1331-4" data-line-number="4">     model       =<span class="st"> </span>ensemble<span class="op">$</span>models[[i]]</a>
<a class="sourceLine" id="cb1331-5" data-line-number="5">     predicted   =<span class="st"> </span><span class="kw">h.score</span>(model, x.feature, x.label)</a>
<a class="sourceLine" id="cb1331-6" data-line-number="6">     predictions =<span class="st"> </span><span class="kw">rbind</span>(predictions, predicted<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1331-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1331-8" data-line-number="8">  y.hat =<span class="st"> </span><span class="kw">apply</span>(predictions,<span class="dv">2</span>,mean)</a>
<a class="sourceLine" id="cb1331-9" data-line-number="9">  <span class="kw">prediction.score</span>(x.label, y.hat)</a>
<a class="sourceLine" id="cb1331-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1331-11" data-line-number="11">min.mse.b     =<span class="st"> </span><span class="kw">which.min</span>(my.rf.cv.result<span class="op">$</span>mse[<span class="dv">5</span><span class="op">:</span><span class="dv">100</span>]) <span class="op">+</span><span class="st"> </span><span class="dv">6</span></a>
<a class="sourceLine" id="cb1331-12" data-line-number="12">my.rf.result  =<span class="st"> </span><span class="kw">my.rf.predict</span>(my.rf.model, test.predictors,  </a>
<a class="sourceLine" id="cb1331-13" data-line-number="13">                              test.target, <span class="dt">b=</span>min.mse.b)</a>
<a class="sourceLine" id="cb1331-14" data-line-number="14"><span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span> =<span class="st"> </span>my.rf.result<span class="op">$</span>fitted.values,</a>
<a class="sourceLine" id="cb1331-15" data-line-number="15">  <span class="st">&quot;Actual&quot;</span>    =<span class="st"> </span>test.target )</a></code></pre></div>
<pre><code>## Predicted    Actual 
##     20.83     21.00</code></pre>
<div class="sourceCode" id="cb1333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1333-1" data-line-number="1">my.rf.result<span class="op">$</span>loss</a></code></pre></div>
<pre><code>##   Rsquare    MSE    MAE   RMSE  RMSLE
## 1     0.5 0.0295 0.1718 0.1718 0.0078</code></pre>
<p>We can also use a 3rd-party library to validate our result, namely <strong>randomForest(.)</strong>. Figure <a href="machinelearning2.html#fig:randomforest2">10.9</a> shows an optimal number of trees based on least <strong>MSE</strong>. Also, the node impurity plot shows that <strong>wt</strong>, <strong>disp</strong>, and <strong>hp</strong> are top in terms of importance.</p>
<div class="sourceCode" id="cb1335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1335-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1335-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1335-3" data-line-number="3">formula =<span class="st"> </span><span class="kw">as.formula</span>(<span class="kw">paste</span>(target, <span class="st">&quot;~&quot;</span>, <span class="kw">paste</span>(features, <span class="dt">collapse=</span><span class="st">&quot;+&quot;</span>)))</a>
<a class="sourceLine" id="cb1335-4" data-line-number="4">forest.model =<span class="st"> </span><span class="kw">randomForest</span>(formula, <span class="dt">data=</span>trainset,  <span class="dt">ntree=</span>b)</a>
<a class="sourceLine" id="cb1335-5" data-line-number="5">y =<span class="st"> </span>forest.model<span class="op">$</span>mse; x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(y))</a>
<a class="sourceLine" id="cb1335-6" data-line-number="6"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1335-7" data-line-number="7"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">main=</span><span class="st">&quot;Forest Model&quot;</span>, <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim=</span> <span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb1335-8" data-line-number="8">     <span class="dt">xlab=</span><span class="st">&quot;trees&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;MSE&quot;</span>,  <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1335-9" data-line-number="9"><span class="kw">lines</span>(x, y, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1335-10" data-line-number="10"><span class="kw">varImpPlot</span>(forest.model, <span class="dt">pch=</span><span class="dv">16</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:randomforest2"></span>
<img src="DS_files/figure-html/randomforest2-1.png" alt="Weights" width="90%" />
<p class="caption">
Figure 10.9: Weights
</p>
</div>
<p>Additionally, the final model calculates and shows the importance of the features using <strong>importance(.)</strong> and <strong>varUsed(.)</strong> functions.</p>
<div class="sourceCode" id="cb1336"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1336-1" data-line-number="1"><span class="kw">cbind</span>( <span class="kw">importance</span>(forest.model), <span class="st">&quot;varUSed&quot;</span> =<span class="st"> </span><span class="kw">varUsed</span>(forest.model))</a></code></pre></div>
<pre><code>##      IncNodePurity varUSed
## cyl         187.45      60
## disp        243.61     201
## hp          280.76     185
## drat         65.19     122
## wt          249.21     171
## qsec         29.69     128</code></pre>
<p>As shown above, we leave readers to improve <strong>my.rf.tree(.)</strong> as an exercise to generate the equivalent order of feature importance. Additionally, it helps to investigate if the <strong>Node Purity (IncNodePurity)</strong> can serve as a guide to know if we need to cut the number of features upfront before passing to <strong>randomForest(.)</strong>.</p>
<p>Finally, to validate our previous prediction, we get an almost similar result:</p>
<div class="sourceCode" id="cb1338"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1338-1" data-line-number="1">y.hat =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(forest.model, test.predictors )</a>
<a class="sourceLine" id="cb1338-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;Predicted&quot;</span> =<span class="st"> </span>y.hat,   <span class="st">&quot;Actual&quot;</span> =<span class="st"> </span>test.target )</a></code></pre></div>
<pre><code>## Predicted.Mazda RX4              Actual 
##                  21                  21</code></pre>
</div>
<div id="Adaoost" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.4</span> AdaBoost<a href="machinelearning2.html#Adaoost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>AdaBoost</strong> is short for <strong>Adaptive Boosting</strong> developed by Yoav Freund and Robert Schapire <span class="citation">(<a href="bibliography.html#ref-ref510">1999</a>)</span>. This <strong>Boosting</strong> ensemble algorithm introduces a few concepts, namely <strong>weighted errors</strong>, <strong>weak learners</strong>, and <strong>decision stumps</strong>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:boosting"></span>
<img src="boosting.png" alt="AdaBoost" width="70%" />
<p class="caption">
Figure 10.10: AdaBoost
</p>
</div>
<p>Unlike <strong>Bagging</strong>, there does not have to be bootstrapping of samples in <strong>Boosting</strong>. Instead, the entire train set is used to build <strong>decision stumps</strong>, which are one-split trees, each consisting of a root node with two children - using binary split.</p>
<p>So how does <strong>AdaBoost</strong> work?</p>
<p>The intuition behind <strong>Adaboost</strong> is commonly explained for <strong>classification</strong>. However, in <strong>regression</strong>, let us later reference a couple of papers that address regressors for boosting. The important emphasis in regression is on calculating loss function and weight. Our loss function continues to measure <strong>SSE</strong>. Additionally, it helps to salvage a few of our familiar R functions discussed in the <strong>Regression Trees</strong> section also to emphasize the process motivated by Drucker H. <span class="citation">(<a href="bibliography.html#ref-ref589h">1997</a>)</span> on the use of <strong>CART</strong> for regression. </p>
<p><strong>First</strong>, we use the mtcars dataset, splitting the dataset into a training set and a test set. Here, we use the entire training set as our sample set. We also generate the initial distribution:</p>
<div class="sourceCode" id="cb1340"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1340-1" data-line-number="1">sample.set =<span class="st"> </span>trainset</a>
<a class="sourceLine" id="cb1340-2" data-line-number="2">n =<span class="st"> </span><span class="kw">nrow</span>(sample.set); w =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>n </a>
<a class="sourceLine" id="cb1340-3" data-line-number="3">sample.weight =<span class="st"> </span><span class="kw">rep</span>(w, n) <span class="co"># Initial Proportionality</span></a>
<a class="sourceLine" id="cb1340-4" data-line-number="4"><span class="co"># only for display purpose, see output:</span></a>
<a class="sourceLine" id="cb1340-5" data-line-number="5"><span class="kw">head</span>(<span class="kw">cbind</span>(sample.set[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>)], sample.weight)) </a></code></pre></div>
<pre><code>##                    mpg    wt disp  hp sample.weight
## Mazda RX4 Wag     21.0 2.875  160 110       0.03226
## Datsun 710        22.8 2.320  108  93       0.03226
## Hornet 4 Drive    21.4 3.215  258 110       0.03226
## Hornet Sportabout 18.7 3.440  360 175       0.03226
## Valiant           18.1 3.460  225 105       0.03226
## Duster 360        14.3 3.570  360 245       0.03226</code></pre>
<p>Also, notice that we use a separate vector to keep track of sample weights, namely <strong>sample.weight</strong>.</p>

<div class="sourceCode" id="cb1342"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1342-1" data-line-number="1">sample.weight</a></code></pre></div>
<pre><code>##  [1] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226
##  [8] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226
## [15] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226
## [22] 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226 0.03226
## [29] 0.03226 0.03226 0.03226</code></pre>

<p><strong>Second</strong>, we build <strong>decision stumps</strong>. Each <strong>decision stump</strong> is built based on a chosen feature in our sample set. In the case of mtcars, we have ten features, namely <strong>wt</strong>, <strong>disp</strong>, <strong>hp</strong>, and on. Therefore, we expect around ten decision stumps. We evaluate each feature using the same criterion we used for the split function, <strong>split.goodness(.)</strong>, to split the data points for the feature - based on the least <strong>SSE</strong> value.</p>
<p>For the <strong>wt</strong> feature, the sample set is split into the following:</p>

<div class="sourceCode" id="cb1344"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1344-1" data-line-number="1">categories =<span class="st"> </span><span class="kw">get.categories</span>(<span class="st">&quot;mpg&quot;</span>, datacars)</a>
<a class="sourceLine" id="cb1344-2" data-line-number="2">data       =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span>=<span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">nrow</span>(sample.set)), <span class="st">&quot;dataset&quot;</span>=sample.set)</a>
<a class="sourceLine" id="cb1344-3" data-line-number="3">ft  =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, categories)<span class="op">$</span>wt</a>
<a class="sourceLine" id="cb1344-4" data-line-number="4">ft<span class="op">$</span>left.data =<span class="st"> </span>ft<span class="op">$</span>right.data =<span class="st"> </span>ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices =<span class="st">  </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1344-5" data-line-number="5"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##      feature  split obs left right   sse ymean  mse improve
## [1,]    &quot;wt&quot; &quot;2.26&quot;  31    6    25 380.4 20.06 36.3  0.6619
##      perc    indices   response  ntype
## [1,]   97 Integer,31 Numeric,31 &quot;node&quot;</code></pre>

<p>Note that the <strong>indices</strong> and <strong>response</strong> columns contain an array of 31 integers.</p>
<p>For <strong>cyl</strong> feature, the sample set is split into the following:</p>

<div class="sourceCode" id="cb1346"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1346-1" data-line-number="1">ft =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, categories)<span class="op">$</span>cyl</a>
<a class="sourceLine" id="cb1346-2" data-line-number="2">ft<span class="op">$</span>left.data =<span class="st"> </span>ft<span class="op">$</span>right.data =<span class="st"> </span>ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices =<span class="st">  </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1346-3" data-line-number="3"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##      feature split obs left right sse ymean  mse improve
## [1,]   &quot;cyl&quot; &quot;RLL&quot;  31   20    11 382 20.06 36.3  0.6605
##      perc    indices   response  ntype
## [1,]   97 Integer,31 Numeric,31 &quot;node&quot;</code></pre>

<p>If we have to cheat a little bit, we might be able to reuse our <strong>my.regression.tree(.)</strong> function to mimic creating a one-split decision stump for each feature like so:</p>

<div class="sourceCode" id="cb1348"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1348-1" data-line-number="1">my.wt.stump  =<span class="st"> </span><span class="kw">my.tree</span>(<span class="st">&quot;wt&quot;</span>, target, sample.set , <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1348-2" data-line-number="2">my.tree_.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.wt.stump<span class="op">$</span>model )</a></code></pre></div>
<pre><code>##   N P feat split obs L  R    SSE ymean   MSE improve perc
## 1 1 0   wt  2.26  31 6 25 380.39 20.06 36.30  0.6619   97
## 2 2 1   wt     .   6 3  3  31.05 30.07  7.43  0.3030   19
## 3 3 1   wt     .  25 9 16 159.61 17.66 13.43  0.5247   78
##   type
## 1 node
## 2 leaf
## 3 leaf</code></pre>


<div class="sourceCode" id="cb1350"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1350-1" data-line-number="1">my.cyl.stump =<span class="st"> </span><span class="kw">my.tree</span>(<span class="st">&quot;cyl&quot;</span>, target, sample.set , <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1350-2" data-line-number="2">my.tree_.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.wt.stump<span class="op">$</span>model)</a></code></pre></div>
<pre><code>##   N P feat split obs L  R    SSE ymean   MSE improve perc
## 1 1 0   wt  2.26  31 6 25 380.39 20.06 36.30  0.6619   97
## 2 2 1   wt     .   6 3  3  31.05 30.07  7.43  0.3030   19
## 3 3 1   wt     .  25 9 16 159.61 17.66 13.43  0.5247   78
##   type
## 1 node
## 2 leaf
## 3 leaf</code></pre>

<p><strong>Third</strong>, we rank each <strong>decision stump</strong> and determine the stump with the least <strong>SSE</strong>. Our <strong>my.tree(.)</strong> is already built with the algorithm to determine which feature has the least <strong>SSE</strong>. It uses <strong>rank.importance(.)</strong> to do just that. See below:</p>

<div class="sourceCode" id="cb1352"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1352-1" data-line-number="1">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, categories)</a>
<a class="sourceLine" id="cb1352-2" data-line-number="2"><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##   feature  split obs left right   sse ymean  mse improve
## 5      wt   2.26  31    6    25 380.4 20.06 36.3  0.6619
## 1     cyl    RLL  31   20    11 382.0 20.06 36.3  0.6605
## 2    disp 153.35  31   12    19 415.2 20.06 36.3  0.6310
## 3      hp    118  31   14    17 432.2 20.06 36.3  0.6159
## 4    drat   3.75  31   18    13 641.8 20.06 36.3  0.4296
## 6    qsec  18.41  31   19    12 736.5 20.06 36.3  0.3454
##   perc ntype
## 5   97  node
## 1   97  node
## 2   97  node
## 3   97  node
## 4   97  node
## 6   97  node</code></pre>

<p>As shown, <strong>wt</strong> is the top-ranked feature for the first <strong>decision stump</strong> because it has the least <strong>SSE</strong>. The other stumps are discarded.</p>
<p><strong>Fourth</strong>, calculate the error rate. Because <strong>stumps</strong> are one-split trees, they do not train well. For this reason, they are also called <strong>weak learners</strong>. The core premise around <strong>Boosting</strong> is to measure the errors corresponding to data points incorrectly predicted by weak learners and give a more favorable chance for these data points (once weighted) to be re-trained in the next iteration. In <strong>classification</strong>, it is easy to determine which data points are correctly performing because the target variable (response variable) is more categorically deterministic than if the target variable is continuous. That is where we introduce two <strong>Adaboost Regressors</strong>. The first one is called <strong>AdaBoost.R2</strong>, for which we reference H. Drucker <span class="citation">(<a href="bibliography.html#ref-ref589h">1997</a>)</span> on improving regressors using boosting techniques. The second one is called <strong>AdaBoost.RT</strong> for which we reference D.P. Solomatine and D. Shrestha <span class="citation">(<a href="bibliography.html#ref-ref600d">2004</a>)</span> on AdaBoost.RT - a boosting algorithm for regression problems. Here, we cover <strong>AdaBoost.R2</strong>.</p>
<p>In H. Druckerâs paper, for <strong>AdaBoost.R2</strong>, there are three candidate choices for a loss function, conditioned on the range [0, 1]. For illustration, we use the <strong>exponential law</strong> as our loss function:</p>
<p><span class="math display">\[\begin{align}
\epsilon_i = L_i^{(t)} = 1 - exp\left[\frac{-|\hat{y_i} - y_i|}{D} \right]\ \ \ \ \ \ \ \ where\ D\ = sup|\hat{y_i} - y_i|,\ \ \ \ i\ in\ 1\ ..\ n
\end{align}\]</span></p>
<p>Here, let us use our <strong>my.predict(.)</strong> function from <strong>regression tree</strong> for <strong>h.score(.)</strong>. Our example implementation of the <strong>AdaBoost.R2</strong> Loss function is shown below. We have to predict our fitted values against the stump using the same mtcars train set as the sample set.</p>

<div class="sourceCode" id="cb1354"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1354-1" data-line-number="1">h.score &lt;-<span class="st"> </span>my.predict</a>
<a class="sourceLine" id="cb1354-2" data-line-number="2">adaboost.loss &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, stump, sample.set) {</a>
<a class="sourceLine" id="cb1354-3" data-line-number="3">   x          =<span class="st"> </span>sample.set[,<span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1354-4" data-line-number="4">   y          =<span class="st"> </span>sample.set[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1354-5" data-line-number="5">   predicted  =<span class="st"> </span><span class="kw">h.score</span>(stump, x, y)</a>
<a class="sourceLine" id="cb1354-6" data-line-number="6">   yhat       =<span class="st"> </span>predicted<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb1354-7" data-line-number="7">   e          =<span class="st"> </span><span class="kw">abs</span>( yhat <span class="op">-</span><span class="st"> </span>y ) </a>
<a class="sourceLine" id="cb1354-8" data-line-number="8">   loss       =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">exp</span>( <span class="op">-</span>e <span class="op">/</span><span class="st"> </span><span class="kw">max</span>(e) )</a>
<a class="sourceLine" id="cb1354-9" data-line-number="9">   <span class="kw">list</span>(<span class="st">&quot;yhat&quot;</span> =<span class="st"> </span>yhat, <span class="st">&quot;loss&quot;</span> =<span class="st"> </span>loss )</a>
<a class="sourceLine" id="cb1354-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1354-11" data-line-number="11">(<span class="dt">Lt =</span> <span class="kw">adaboost.loss</span>(features, target, my.wt.stump, sample.set ))</a></code></pre></div>
<pre><code>## $yhat
##  [1] 17.66 17.66 17.66 17.66 17.66 17.66 17.66 17.66 17.66
## [10] 17.66 17.66 17.66 17.66 17.66 17.66 17.66 30.07 30.07
## [19] 30.07 17.66 17.66 17.66 17.66 17.66 30.07 30.07 30.07
## [28] 17.66 17.66 17.66 17.66
## 
## $loss
##  [1] 0.36875 0.50737 0.40259 0.13346 0.05881 0.37049 0.60480
##  [8] 0.50737 0.19113 0.01910 0.15933 0.04838 0.28741 0.63212
## [15] 0.63212 0.33483 0.27486 0.04488 0.41022 0.41076 0.25734
## [22] 0.28741 0.45149 0.19113 0.31688 0.42887 0.04488 0.22601
## [29] 0.24497 0.30677 0.40259</code></pre>

<p><strong>Fifth</strong>, calculate the average loss using the following equation (note that this is a weighted expectation - thus, the use of the summation):</p>
<p><span class="math display">\[\begin{align}
\overline{L^{(t)}} = \sum_i^n L_i^{(t)} p_i^{(t)}\ \ \ \ 
\ \ \ \ \ \ \ where\ p_i^{(t)}\text{ =  probability distribution.}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1356"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1356-1" data-line-number="1">(<span class="dt">Lt.avg =</span> <span class="kw">sum</span>(Lt<span class="op">$</span>loss <span class="op">*</span><span class="st"> </span>sample.weight))</a></code></pre></div>
<pre><code>## [1] 0.3083</code></pre>
<p><strong>Sixth</strong>, our next step is to use the average weight to evaluate the <strong>confidence</strong> level of our stump. In other literature, this is also described as the <strong>amount of say</strong>, the <strong>voting power</strong>, or the <strong>weight</strong> of a stump which is expressed in the following equation:</p>
<p><span class="math display">\[\begin{align}
\beta^{(t)} = \left( \frac{\overline{L^{(t)}}}{1 - \overline{L^{(t)}}} \right)\ \ \ \ \ 
\text{where } \mathbf{t} \text{ corresponds to the iteration index}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1358"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1358-1" data-line-number="1">( <span class="dt">Bt =</span> Lt.avg <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Lt.avg) )</a></code></pre></div>
<pre><code>## [1] 0.4457</code></pre>
<p><strong>Seventh</strong>, we update our sample weights using the following equation. The new sample weights follow a new probability distribution (<strong>P</strong>) for the data points.</p>
<p><span class="math display">\[\begin{align}
w_i^{(t+1)} = \frac{w_i}{z_t}^{(t)}\beta^{(t)}{}^{\left( 1 - L_i^{(t)}\right)}
\ \ \ \ \ where\ z_t\ \text{is a normalizer}
\end{align}\]</span></p>

<div class="sourceCode" id="cb1360"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1360-1" data-line-number="1">new.sample.weight =<span class="st"> </span>sample.weight <span class="op">*</span><span class="st"> </span>Bt <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>Lt<span class="op">$</span>loss)</a>
<a class="sourceLine" id="cb1360-2" data-line-number="2">(<span class="dt">normalized.weight =</span> <span class="dt">P =</span> new.sample.weight <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(new.sample.weight))</a></code></pre></div>
<pre><code>##  [1] 0.02993 0.02605 0.02893 0.03787 0.04080 0.02988 0.02363
##  [8] 0.02605 0.03574 0.04245 0.03690 0.04123 0.03246 0.02300
## [15] 0.02300 0.03096 0.03287 0.04137 0.02871 0.02870 0.03345
## [22] 0.03246 0.02755 0.03574 0.03152 0.02818 0.04137 0.03452
## [29] 0.03387 0.03184 0.02893</code></pre>

<p><strong>Eight</strong>, we use this new distribution to resample our data set. To do that, we create a cumulative version to map our data points to the distribution.</p>

<div class="sourceCode" id="cb1362"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1362-1" data-line-number="1">(<span class="dt">cumulative.weight =</span> <span class="dt">P =</span> <span class="kw">cumsum</span>(normalized.weight))</a></code></pre></div>
<pre><code>##  [1] 0.02993 0.05598 0.08491 0.12278 0.16358 0.19346 0.21709
##  [8] 0.24314 0.27889 0.32134 0.35824 0.39947 0.43194 0.45493
## [15] 0.47793 0.50889 0.54177 0.58314 0.61185 0.64055 0.67400
## [22] 0.70647 0.73402 0.76976 0.80128 0.82946 0.87084 0.90536
## [29] 0.93923 0.97107 1.00000</code></pre>

<p>We collect a new sample set based on the proportionality, using random numbers between 0 and 1. The idea is to draw N random numbers, each mapped according to the corresponding cumulative weights of the data point. Because some weights are larger than the others, some random numbers have a higher probability of falling under the larger weights. In essence, this is how we favor samples that are performing poorly in the sample set and thus give them a higher probability of being re-sampled in the next iteration.</p>

<div class="sourceCode" id="cb1364"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1364-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1364-2" data-line-number="2">rnum =<span class="st"> </span><span class="kw">sort</span>(<span class="kw">runif</span>(<span class="dt">n=</span>n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1364-3" data-line-number="3"><span class="kw">head</span>(<span class="kw">cbind</span>(sample.set[,<span class="kw">c</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>)], cumulative.weight, rnum))</a></code></pre></div>
<pre><code>##                    mpg    wt disp  hp cumulative.weight
## Mazda RX4 Wag     21.0 2.875  160 110           0.02993
## Datsun 710        22.8 2.320  108  93           0.05598
## Hornet 4 Drive    21.4 3.215  258 110           0.08491
## Hornet Sportabout 18.7 3.440  360 175           0.12278
## Valiant           18.1 3.460  225 105           0.16358
## Duster 360        14.3 3.570  360 245           0.19346
##                     rnum
## Mazda RX4 Wag     0.0509
## Datsun 710        0.1469
## Hornet 4 Drive    0.1766
## Hornet Sportabout 0.2804
## Valiant           0.3359
## Duster 360        0.4323</code></pre>

<p>Notice above that the <strong>rnum</strong> is not correctly mapped yet to the corresponding <strong>cumulative weights</strong>. To do that, we use the below example implementation as part of resampling the distribution:</p>

<div class="sourceCode" id="cb1366"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1366-1" data-line-number="1">sampling.distribution &lt;-<span class="st"> </span><span class="cf">function</span>(sample.set, sample.weight, <span class="dt">seed=</span><span class="dv">142</span>) {</a>
<a class="sourceLine" id="cb1366-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">nrow</span>(sample.set)</a>
<a class="sourceLine" id="cb1366-3" data-line-number="3">  indices =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1366-4" data-line-number="4">  cumulative.weight =<span class="st"> </span><span class="kw">cumsum</span>(sample.weight)</a>
<a class="sourceLine" id="cb1366-5" data-line-number="5">  <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1366-6" data-line-number="6">  random.number =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1366-7" data-line-number="7">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1366-8" data-line-number="8">    indices[i] =<span class="st"> </span><span class="kw">which</span>(random.number[i] <span class="op">&lt;</span><span class="st"> </span>cumulative.weight)[<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1366-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb1366-10" data-line-number="10">  sample.set[indices,]</a>
<a class="sourceLine" id="cb1366-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb1366-12" data-line-number="12"><span class="kw">head</span>(<span class="kw">sampling.distribution</span>(sample.set, normalized.weight, <span class="dv">1</span>))</a></code></pre></div>
<pre><code>##                 mpg cyl  disp  hp drat    wt  qsec
## Merc 280       19.2   6 167.6 123 3.92 3.440 18.30
## Merc 450SL     17.3   8 275.8 180 3.07 3.730 17.60
## Honda Civic    30.4   4  75.7  52 4.93 1.615 18.52
## Ferrari Dino   19.7   6 145.0 175 3.62 2.770 15.50
## Merc 240D      24.4   4 146.7  62 3.69 3.190 20.00
## Ford Pantera L 15.8   8 351.0 264 4.22 3.170 14.50</code></pre>

<p><strong>Ninth</strong>, we repeat the process starting from step two by processing the newly re-sampled sample set with new weights and building the next <strong>decision stump</strong>. Alternatively, we can use our example implementation of <strong>AdaBoost Regressor</strong>, pulling together all the steps above into one function. Our stopping criterion is based on an average loss larger than 0.5 <span class="citation">(Drucker H. <a href="bibliography.html#ref-ref589h">1997</a>)</span>.</p>

<div class="sourceCode" id="cb1368"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1368-1" data-line-number="1">h.learner &lt;-<span class="st"> </span>my.tree</a>
<a class="sourceLine" id="cb1368-2" data-line-number="2">my.adaboost.R &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, sample.set, <span class="dt">estimators=</span><span class="dv">10</span>) {</a>
<a class="sourceLine" id="cb1368-3" data-line-number="3">  stumps =<span class="st"> </span><span class="kw">list</span>();  betas =<span class="st"> </span><span class="kw">list</span>(); yi =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1368-4" data-line-number="4">  n =<span class="st"> </span><span class="kw">nrow</span>(sample.set)</a>
<a class="sourceLine" id="cb1368-5" data-line-number="5">  w =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>n </a>
<a class="sourceLine" id="cb1368-6" data-line-number="6">  sample.weight =<span class="st"> </span><span class="kw">rep</span>(w, n) <span class="co"># Proportionality</span></a>
<a class="sourceLine" id="cb1368-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>estimators) {</a>
<a class="sourceLine" id="cb1368-8" data-line-number="8">     data      =<span class="st"> </span><span class="kw">sampling.distribution</span>(sample.set, sample.weight)</a>
<a class="sourceLine" id="cb1368-9" data-line-number="9">     my.stump  =<span class="st"> </span><span class="kw">h.learner</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1368-10" data-line-number="10">     predicted =<span class="st"> </span><span class="kw">adaboost.loss</span>(features, target, my.stump, data)</a>
<a class="sourceLine" id="cb1368-11" data-line-number="11">     et        =<span class="st"> </span><span class="kw">sum</span>(predicted<span class="op">$</span>loss <span class="op">*</span><span class="st"> </span>sample.weight) <span class="co"># avg/weighted loss</span></a>
<a class="sourceLine" id="cb1368-12" data-line-number="12">     Bt                =<span class="st"> </span>et <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>et) </a>
<a class="sourceLine" id="cb1368-13" data-line-number="13">     <span class="cf">if</span> (<span class="kw">is.nan</span>(Bt) <span class="op">||</span><span class="st"> </span>et <span class="op">&gt;=</span><span class="st"> </span><span class="fl">0.5</span>) <span class="cf">break</span></a>
<a class="sourceLine" id="cb1368-14" data-line-number="14">     stumps[[t]]       =<span class="st"> </span>my.stump</a>
<a class="sourceLine" id="cb1368-15" data-line-number="15">     betas[[t]]        =<span class="st"> </span>Bt</a>
<a class="sourceLine" id="cb1368-16" data-line-number="16">     yi[[t]]           =<span class="st"> </span>predicted<span class="op">$</span>yhat</a>
<a class="sourceLine" id="cb1368-17" data-line-number="17">     sample.weight     =<span class="st"> </span>sample.weight <span class="op">*</span><span class="st"> </span>Bt<span class="op">^</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>predicted<span class="op">$</span>loss)</a>
<a class="sourceLine" id="cb1368-18" data-line-number="18">     sample.weight     =<span class="st"> </span>sample.weight <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(sample.weight)</a>
<a class="sourceLine" id="cb1368-19" data-line-number="19">     sample.set        =<span class="st"> </span>data</a>
<a class="sourceLine" id="cb1368-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb1368-21" data-line-number="21">  <span class="kw">list</span>(<span class="st">&quot;stumps&quot;</span> =<span class="st"> </span>stumps, <span class="st">&quot;betas&quot;</span> =<span class="st"> </span>betas, <span class="st">&quot;yi&quot;</span> =<span class="st"> </span>yi)</a>
<a class="sourceLine" id="cb1368-22" data-line-number="22">}</a></code></pre></div>

<p>Here, our base learner <strong>h.learner(.)</strong> references our original regression function, namely <strong>my.tree(.)</strong>.</p>
<p>Note in <strong>boosting</strong> that we have a choice of <strong>resampling</strong> from our original sample set or a choice of <strong>re-weighing</strong> our original sample set instead. In our implementation, we have chosen to perform <strong>resampling</strong>. The choice of <strong>re-weighing</strong> is illustrated in a later section in the context of <strong>AdaBoost Classifier</strong>.</p>

<div class="sourceCode" id="cb1369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1369-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1369-2" data-line-number="2">sample.set =<span class="st"> </span>trainset</a>
<a class="sourceLine" id="cb1369-3" data-line-number="3">my.adaboost.model =<span class="st"> </span><span class="kw">my.adaboost.R</span>(features, target, sample.set, <span class="dt">estimators=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1369-4" data-line-number="4"><span class="co"># example of first stump</span></a>
<a class="sourceLine" id="cb1369-5" data-line-number="5">my.table.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.adaboost.model<span class="op">$</span>stumps[[<span class="dv">1</span>]]<span class="op">$</span>model) </a></code></pre></div>
<pre><code>##   N P feat  split obs  L  R    SSE ymean   MSE improve  %
## 1 1 0   wt 2.3925  31 10 21 308.30 19.86 56.54  0.8241 97
## 2 2 1 disp      .  10  8  2  17.59 29.75  8.91  0.8027 31
## 3 3 1 disp      .  21 16  5  71.23 15.15 10.44  0.6750 66
##   type
## 1 node
## 2 leaf
## 3 leaf</code></pre>
<div class="sourceCode" id="cb1371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1371-1" data-line-number="1">my.adaboost.model<span class="op">$</span>betas[[<span class="dv">1</span>]]   <span class="co"># example of corresponding beta</span></a></code></pre></div>
<pre><code>## [1] 0.3368</code></pre>
<div class="sourceCode" id="cb1373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1373-1" data-line-number="1">my.adaboost.model<span class="op">$</span>yi[[<span class="dv">1</span>]]      <span class="co"># example of corresponding y (ith=1)</span></a></code></pre></div>
<pre><code>##  [1] 15.15 15.15 15.15 29.75 29.75 29.75 15.15 15.15 15.15
## [10] 15.15 29.75 15.15 15.15 29.75 15.15 29.75 15.15 29.75
## [19] 29.75 15.15 15.15 15.15 29.75 15.15 15.15 15.15 15.15
## [28] 29.75 15.15 15.15 15.15</code></pre>

<p><strong>Finally</strong>, for a model prediction for <strong>Adaboost Regression</strong>, we use <strong>weighted median</strong> <span class="citation">(Drucker H. <a href="bibliography.html#ref-ref589h">1997</a>)</span>.</p>
<p><span class="math display">\[\begin{align}
h_f = inf\left\{y \in Y : \sum_{t:h_t \le y} \log_e\left(\frac{1}{\beta_t}\right) 
    \ge \frac{1}{2} \sum_t \log_e \left( \frac{1}{\beta_t}\right)
\right\}
\end{align}\]</span></p>
<p>Our example implementation of the <strong>weighted median</strong> is like so:</p>

<div class="sourceCode" id="cb1375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1375-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))} <span class="co"># exp(1) = 2.718282</span></a>
<a class="sourceLine" id="cb1375-2" data-line-number="2">my.adaboost.predict &lt;-<span class="st"> </span><span class="cf">function</span>( features, target, adaboost.model, </a>
<a class="sourceLine" id="cb1375-3" data-line-number="3">                                 sample.set) {</a>
<a class="sourceLine" id="cb1375-4" data-line-number="4">  n  =<span class="st"> </span><span class="kw">nrow</span>(sample.set);   b =<span class="st"> </span><span class="dv">0</span>;  hf =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1375-5" data-line-number="5">  estimators =<span class="st"> </span><span class="kw">length</span>(adaboost.model<span class="op">$</span>stumps)</a>
<a class="sourceLine" id="cb1375-6" data-line-number="6">  B =<span class="st"> </span>adaboost.model<span class="op">$</span>betas</a>
<a class="sourceLine" id="cb1375-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>estimators) { b =<span class="st"> </span>b <span class="op">+</span><span class="st"> </span><span class="kw">ln</span> (<span class="dv">1</span><span class="op">/</span><span class="st"> </span>B[[t]]) }</a>
<a class="sourceLine" id="cb1375-8" data-line-number="8">  b =<span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1375-9" data-line-number="9">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1375-10" data-line-number="10">    <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>estimators) {</a>
<a class="sourceLine" id="cb1375-11" data-line-number="11">      stump     =<span class="st"> </span>adaboost.model<span class="op">$</span>stumps[[t]]</a>
<a class="sourceLine" id="cb1375-12" data-line-number="12">      yi        =<span class="st"> </span>adaboost.model<span class="op">$</span>yi[[t]]  <span class="co"># predicted y-hat (yi)</span></a>
<a class="sourceLine" id="cb1375-13" data-line-number="13">      predicted =<span class="st"> </span><span class="kw">adaboost.loss</span>(features, target, stump, sample.set[i,])</a>
<a class="sourceLine" id="cb1375-14" data-line-number="14">      ht        =<span class="st"> </span>predicted<span class="op">$</span>yhat</a>
<a class="sourceLine" id="cb1375-15" data-line-number="15">      s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1375-16" data-line-number="16">      <span class="cf">for</span> (p <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(yi)) <span class="cf">if</span> (ht <span class="op">&lt;=</span><span class="st"> </span>yi[p]) s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span><span class="kw">ln</span>(<span class="dv">1</span><span class="op">/</span>B[[t]])</a>
<a class="sourceLine" id="cb1375-17" data-line-number="17">      <span class="cf">if</span> (s <span class="op">&gt;=</span><span class="st"> </span>b) hf[i] =<span class="st"> </span>ht</a>
<a class="sourceLine" id="cb1375-18" data-line-number="18">    }</a>
<a class="sourceLine" id="cb1375-19" data-line-number="19">  }</a>
<a class="sourceLine" id="cb1375-20" data-line-number="20">  hf</a>
<a class="sourceLine" id="cb1375-21" data-line-number="21">} </a></code></pre></div>
<div class="sourceCode" id="cb1376"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1376-1" data-line-number="1">yhat =<span class="st"> </span><span class="kw">my.adaboost.predict</span>(features, target, my.adaboost.model,  testset)</a>
<a class="sourceLine" id="cb1376-2" data-line-number="2"><span class="kw">cbind</span>(<span class="st">&quot;Predicted&quot;</span> =<span class="st"> </span>yhat, <span class="st">&quot;True&quot;</span> =<span class="st"> </span>test.target)</a></code></pre></div>
<pre><code>##      Predicted True
## [1,]     21.42   21</code></pre>

<p>As an exercise, we leave readers to play around with different random seeds for the random <strong>re-sampling</strong>, namely <strong>sampling.distribution(.)</strong>, and see how it affects prediction performance.</p>
<p>We leave readers to investigate G. Ridgeway et al.âs paper in which a variant of the <strong>Adaboost regressor</strong> uses probabilistic classifier and probabilistic prediction, allowing the transformation of a sample set into one with a categorical target variable.</p>
<p>Let us discuss <strong>Gradient Boost</strong> next which provides an alternative technique to <strong>regression</strong>.</p>
</div>
<div id="gradient-boost" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.5</span> Gradient Boost <a href="machinelearning2.html#gradient-boost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Gradient Boost</strong>, also called <strong>Gradient Boost Machine (GBM)</strong>, was developed by Jerome H. Friedman <span class="citation">(<a href="bibliography.html#ref-ref621j">1999</a><a href="bibliography.html#ref-ref621j">b</a>)</span>. The idea originated from Leo Breiman <span class="citation">(<a href="bibliography.html#ref-ref501">1999</a>)</span>, who came to realize, by observing <strong>Adaboost</strong>, that the algorithm characterizes gradient descent in function space, particularly in the <strong>loss</strong> function space, e.g.:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\nabla_{F(x)} Lik(y, F(x))}_{\text{negative gradient}} = -\left[\frac{\partial Lik(y, F(x))}{\partial F(x)}\right]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
where\ \ \
\underbrace{Lik(y, F(x))}_{\text{loss function}} 
= \frac{1}{2}\left(y -\hat{y}\right)^2 \ \
and\ \ \underbrace{F(x) = \hat{y}}_{\text{y-hat}}.
\end{align}\]</span></p>
<p>Similar to <strong>Adaboost</strong>, <strong>Gradient Boost</strong> is also sequential in processing an ensemble of tree models as it learns from one model to the next. In addition, it introduces a coordinate-wise gradient-based learning technique using a <strong>learning rate</strong> denoted by the <strong>eta</strong> symbol <span class="math inline">\(\eta\)</span>. See Figure <a href="machinelearning2.html#fig:gradientlearning">10.11</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gradientlearning"></span>
<img src="gradientlearning.png" alt="Gradient Learning" width="100%" />
<p class="caption">
Figure 10.11: Gradient Learning
</p>
</div>
<p>The idea is to start with an arbitrary initial value (usually the sample average). Then, the goal is to calculate an optimal path to the target value such that the initial value eventually reaches (or reasonably matches) the target value iteratively at step-wise increments.</p>
<p>So how does <strong>Gradient Boost</strong> work?</p>
<p><strong>First</strong>, let us use the same mtcars training set as before in which <strong>mpg</strong> is our dependent variable (our target variable), and then generate our initial predicted value. Here, our initial estimate is merely the average of our target values derived from the following equation:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
F_0(X) &amp;= \text{arg}\ \underset{\gamma}{min} \sum_{i=1}^n Lik(y_i, \gamma) 
\ \ \ \ \ where\ \ \ \ \ \ Lik(y_i, \gamma)= \frac{1}{2} \left(y_i - \gamma\right)^2\\
&amp;= \text{arg}\ \underset{\gamma}{min} \sum_{i=1}^n \frac{1}{2} \left(y_i - \gamma\right)^2 = \frac{1}{n}\sum_{i=1}^n y_i = \bar{y}
\end{array} \label{eqn:eqnnumber404}
\end{align}\]</span></p>
<p>The symbol gamma <span class="math inline">\((\gamma)\)</span> represents the predicted value. At the same time, the <span class="math inline">\(\mathbf{F_m(x)}\)</span> represents the best weak-learner model by finding the <strong>minimal gamma</strong>. The loss function is required to be differentiable such that by derivation, we can calculate the value of gamma. For example, given target values (<strong>mpg</strong>) from the mtcars set, we perform a derivative calculation to get the following equivalent average:</p>
<p><span class="math display">\[\begin{align*}
\frac{1}{2}(21.0 - \gamma)^2\ +\ ...\ +\ \frac{1}{2}(21.4 - \gamma)^2 &amp;= 0&amp;\\
(21.0 - \gamma)(-1)\ +\ ...\ +\ (21.4 - \gamma)(-1) &amp;= 0&amp;\leftarrow \text{(by derivation)}\\
\gamma = (21.0\ +\ ...\ +\ 21.4)/32 &amp;= 20.06&amp;\leftarrow\text{(base model)}\  F_0(x) = \bar{y}\\ 
\end{align*}\]</span></p>

<div class="sourceCode" id="cb1378"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1378-1" data-line-number="1">sample.set =<span class="st"> </span>trainset</a>
<a class="sourceLine" id="cb1378-2" data-line-number="2">sample.predictors =<span class="st"> </span>sample.set[, <span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1378-3" data-line-number="3">sample.target     =<span class="st"> </span>sample.set[, <span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1378-4" data-line-number="4">(<span class="dt">target.mean      =</span> <span class="kw">mean</span>(sample.target))</a></code></pre></div>
<pre><code>## [1] 20.06</code></pre>

<p><strong>Second</strong>, we calculate the <strong>error</strong> between our initial estimated target and the actual target. To calculate the error, we use the following formula, for <span class="math inline">\(i = 1\ ..\ n\)</span>:</p>
<p><span class="math display">\[\begin{align}
r_{im} &amp;= - \left[\frac{\partial Lik(y_i, F_{m-1}(x_i))}{\partial F_{m-1}(x_i)}\right] = - \frac{\partial}{\partial(\hat{y})} \left[\frac{1}{2}\left(y -\hat{y}\right)^2 \right] = (y - \hat{y})\\ 
\nonumber \\
&amp;where\ \ \ \ F_{m-1}(x_i) \equiv F_0(x_i) \equiv \hat{y} \equiv \gamma
\end{align}\]</span></p>
<p>By partial derivation of the loss function with respect to <span class="math inline">\(\hat{y}\)</span>, the equation is simplified to the following simple residual equation: <span class="math inline">\(r_{im} = (y - \hat{y})\)</span>.</p>

<div class="sourceCode" id="cb1380"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1380-1" data-line-number="1">(<span class="dt">target.residual =</span> sample.target <span class="op">-</span><span class="st"> </span>target.mean)  <span class="co"># rim = (y - y.hat)</span></a></code></pre></div>
<pre><code>##  [1]  0.9387  2.7387  1.3387 -1.3613 -1.9613 -5.7613  4.3387
##  [8]  2.7387 -0.8613 -2.2613 -3.6613 -2.7613 -4.8613 -9.6613
## [15] -9.6613 -5.3613 12.3387 10.3387 13.8387  1.4387 -4.5613
## [22] -4.8613 -6.7613 -0.8613  7.2387  5.9387 10.3387 -4.2613
## [29] -0.3613 -5.0613  1.3387</code></pre>

<p>Let us call this error our <strong>target residual</strong> or <strong>pseudo residual</strong>. In the context of gradient descent in function space, this initial target residual is our starting point, and the expectation is that the residual decreases as we fit more regression models. Fitting regression to residual is indeed a concept we have not discussed thus far - an idea introduced in boosting in the context of regression. It becomes apparent next.</p>
<p><strong>Third</strong>, note that <strong>boosting</strong> is agnostic in the choice of <strong>base regressor</strong> to use. In our case, like <strong>Adaboost.R2</strong>, we continue to choose our implementation of a <strong>regression tree</strong> for <strong>Gradient Boost</strong>, namely <strong>my.tree(.)</strong>. Unlike <strong>Adaboost.R2</strong>, we do not have to build <strong>stumps</strong>. Our <strong>trees</strong> can grow to decent depths. There is no need to grow the trees to the maximum. A decent depth in our case is around five - but this is not to be taken as a hard rule.</p>

<div class="sourceCode" id="cb1382"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1382-1" data-line-number="1">comprehensive.dataset =<span class="st"> </span>datacars</a>
<a class="sourceLine" id="cb1382-2" data-line-number="2">sample.set =<span class="st"> </span><span class="kw">cbind</span>(<span class="st">&quot;mpg&quot;</span> =<span class="st"> </span>target.residual, sample.predictors)</a>
<a class="sourceLine" id="cb1382-3" data-line-number="3">my.gbm.model =<span class="st"> </span><span class="kw">my.tree</span>(features, target, sample.set, <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>)</a></code></pre></div>

<p>We use the target residual as our dependent variable in building a tree model. Here, we fit our tree model to the target residuals. In the process, the leaf nodes are formed containing the corresponding target residuals for this model. For every leaf that contains multiple target residuals, we take the average of those residuals, which becomes the target value of each leaf. While that is already done in our <strong>my.tree(.)</strong>, it helps to show and explain the equation in the context of the <strong>GBM</strong> algorithm:</p>
<p><span class="math display">\[\begin{align}
\gamma_{jm} = \text{arg}\ \underset{\gamma}{min} \sum_{xi \in L_{j}} Lik(y_i, F_{m-1}(x_i) + \gamma),
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(\mathbf{F_{m-1}(x)}\)</span> represents our <strong>regressor model</strong> - in this early step, it is the base model, namely <span class="math inline">\(\mathbf{F_0(x)}\)</span>, which also happens to be our initial target residual. If we follow the same idea discussed in the first step by derivation and minimization with respect to <strong>gamma</strong>, we find that each leaf (indexed by <strong>j</strong>) in the model (indexed by <strong>m</strong>) produces an <strong>average gradient</strong>. It is thus essential to note that, in our specific implementation of the regression tree, our <strong>my.tree(.)</strong> function indeed already performs a more straightforward process of averaging residuals local to a leaf (or a region). For example, assume that a couple of residuals happen to fall under the leaf (j) of a tree model (m); therefore, we have the following:</p>
<p><span class="math display">\[\begin{align}
\frac{1}{2}(y_1 - (F_0(x_1) + \gamma_{jm} ))^2\ + \frac{1}{2}(y_2 - (F_0(x_2) + \gamma_{jm} ))^2&amp;= 0&amp;\\
\frac{1}{2}(y_1 - (20.06 + \gamma_{jm} ))^2\ + \frac{1}{2}(y_2 - (20.06  + \gamma_{jm} ))^2&amp;= 0&amp;\\
(y_1 - 20.06 - \gamma_{jm})(-1)\ + (y_2 - 20.06 - \gamma_{jm})(-1) &amp;= 0&amp;\leftarrow \text{(by derivation)}\\
\gamma_{jm} = [(y_1\ - \ 20.06) + (y_2\ - \ 20.06)]/2 &amp;&amp;\leftarrow\text{(score = leaf average)}
\end{align}\]</span></p>
<p>Each leaf across all models thus holds such corresponding <span class="math inline">\(\gamma_{jm}\)</span> to represent the score as a base learner in ensemble models. Otherwise, it represents the predicted/estimated value so that if we are to predict an unseen data, we flow in the direction of the <strong>average gradient</strong> from the root node of the tree model, producing a predicted <strong>pseudo-residual</strong> like so:</p>

<div class="sourceCode" id="cb1383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1383-1" data-line-number="1"><span class="co"># leaf average (gamma)</span></a>
<a class="sourceLine" id="cb1383-2" data-line-number="2">(<span class="dt">residual.output1 =</span> <span class="kw">my.predict</span>(my.gbm.model, sample.predictors, </a>
<a class="sourceLine" id="cb1383-3" data-line-number="3">                                  target.residual)<span class="op">$</span>fitted.values)</a></code></pre></div>
<pre><code>##  [1]  1.2637  2.7387  1.2637 -1.1113 -1.9613 -5.9613  4.3387
##  [8]  2.7387 -0.8613 -2.2613 -4.1413 -4.1413 -4.1413 -9.6613
## [15] -9.6613 -5.9613 12.3387 10.3387 13.8387  1.2637 -4.1413
## [22] -4.1413 -5.9613 -1.1113  7.2387  5.9387 10.3387 -4.6613
## [29] -0.3613 -4.6613  1.2637</code></pre>

<p>We use the gradient boost tree model, namely <strong>my.gbm.model</strong>, to predict the next residual. Note hereafter that we expect the predicted output from the fit to be an estimated residual - and that is true for all subsequent gradient boost trees that we build. We use the <strong>my.predict(.)</strong> function to produce the expected residual output.</p>
<p>Alternatively, we can combine all that and formulate our example implementation of a <strong>weak-learner regressor</strong>. We use the new target residual to re-construct our sample set, replacing the previous target residual with the new residual as the next target. Our base learner uses <strong>my.tree(.)</strong> and <strong>my.predict(.)</strong> from <strong>Regression Tree</strong> fitting a model to residual.</p>

<div class="sourceCode" id="cb1385"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1385-1" data-line-number="1">h.learner &lt;-<span class="st"> </span>my.tree</a>
<a class="sourceLine" id="cb1385-2" data-line-number="2">h.score   &lt;-<span class="st"> </span>my.predict</a>
<a class="sourceLine" id="cb1385-3" data-line-number="3">weak.learner &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, x, y) {</a>
<a class="sourceLine" id="cb1385-4" data-line-number="4">    data =<span class="st"> </span><span class="kw">cbind</span>(y, x) <span class="co"># target residual (y)</span></a>
<a class="sourceLine" id="cb1385-5" data-line-number="5">    <span class="kw">names</span>(data)[<span class="dv">1</span>] =<span class="st"> </span>target</a>
<a class="sourceLine" id="cb1385-6" data-line-number="6">    model     =<span class="st"> </span><span class="kw">h.learner</span>(features, target, data,  <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1385-7" data-line-number="7">    (<span class="dt">pseudo.res  =</span> <span class="kw">h.score</span>(model, x, y)<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1385-8" data-line-number="8">    <span class="kw">list</span>(<span class="st">&quot;model&quot;</span> =<span class="st"> </span>model, <span class="st">&quot;predicted.residual&quot;</span> =<span class="st"> </span>pseudo.res)</a>
<a class="sourceLine" id="cb1385-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1385-10" data-line-number="10">h =<span class="st"> </span>weak.learner <span class="co"># hypohetical estimator function</span></a></code></pre></div>
<div class="sourceCode" id="cb1386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1386-1" data-line-number="1">wk.model         =<span class="st"> </span><span class="kw">h</span>(features, target, sample.predictors, target.residual)</a>
<a class="sourceLine" id="cb1386-2" data-line-number="2">residual.output1 =<span class="st"> </span>wk.model<span class="op">$</span>predicted.residual <span class="co"># leaf average (gamma)</span></a></code></pre></div>

<p><strong>Fourth</strong>, with our predicted residual, we generate the next new model (<span class="math inline">\(F_{m}(x)\)</span>) for our <strong>ensemble prediction</strong> using the following equation:</p>
<p><span class="math display">\[\begin{align}
F_1(X) = F_0(X) + \eta \times r_1\ \ \ \ \ \ \ \ \ where\ \eta \text{ is learning rate, and } r_1  =  h_1(X)
\end{align}\]</span></p>
<p>For illustration, we choose our <strong>learning rate</strong> to be 0.01.</p>

<div class="sourceCode" id="cb1387"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1387-1" data-line-number="1">l.r =<span class="st"> </span><span class="fl">0.01</span> <span class="co"># learning rate (scaling constant)</span></a>
<a class="sourceLine" id="cb1387-2" data-line-number="2">(<span class="dt">predicted.target =</span> target.mean <span class="op">+</span><span class="st"> </span>l.r <span class="op">*</span><span class="st"> </span>residual.output1)</a></code></pre></div>
<pre><code>##  [1] 20.07 20.07 20.07 20.01 20.07 20.01 20.07 20.07 20.07
## [10] 20.07 20.01 20.01 20.01 20.01 20.01 20.01 20.19 20.15
## [19] 20.19 20.07 20.01 20.01 20.01 20.01 20.15 20.15 20.15
## [28] 20.01 20.07 20.01 20.07</code></pre>

<p><strong>Fifth</strong>, we then calculate the next target residual as before. Here, we repeat the process from step 2.</p>

<div class="sourceCode" id="cb1389"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1389-1" data-line-number="1">(<span class="dt">target.residual =</span> sample.target <span class="op">-</span><span class="st"> </span>predicted.target)</a></code></pre></div>
<pre><code>##  [1]  0.9301  2.7301  1.3301 -1.3117 -1.9699 -5.7117  4.3301
##  [8]  2.7301 -0.8699 -2.2699 -3.6117 -2.7117 -4.8117 -9.6117
## [15] -9.6117 -5.3117 12.2078 10.2541 13.7078  1.4301 -4.5117
## [22] -4.8117 -6.7117 -0.8117  7.1541  5.8541 10.2541 -4.2117
## [29] -0.3699 -5.0117  1.3301</code></pre>

<p><strong>Sixth</strong>, we build our next weak learner model using our new target residual, similar to step 3.</p>

<div class="sourceCode" id="cb1391"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1391-1" data-line-number="1">wk.model         =<span class="st"> </span><span class="kw">h</span>(features, target, sample.predictors, target.residual)</a>
<a class="sourceLine" id="cb1391-2" data-line-number="2">residual.output2 =<span class="st"> </span>wk.model<span class="op">$</span>predicted.residual</a></code></pre></div>

<p><strong>Seventh</strong>, and just as before, we calculate our new estimated target following step 4, but this time, we add the first predicted residual, scaled using a learning rate, with the second predicted residual, also scaled with the same learning rate. With that, we obtain the following:</p>

<div class="sourceCode" id="cb1392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1392-1" data-line-number="1">l.r =<span class="st"> </span><span class="fl">0.01</span> <span class="co"># learning rate (scaling constant)</span></a>
<a class="sourceLine" id="cb1392-2" data-line-number="2">(<span class="dt">predicted.target =</span> target.mean <span class="op">+</span><span class="st"> </span>l.r <span class="op">*</span><span class="st"> </span>residual.output1 <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1392-3" data-line-number="3"><span class="st">                    </span>l.r <span class="op">*</span><span class="st"> </span>residual.output2)</a></code></pre></div>
<pre><code>##  [1] 20.05 20.10 20.05 19.99 20.05 19.95 20.10 20.10 20.05
## [10] 20.05 19.99 19.99 19.99 19.95 19.95 19.95 20.30 20.25
## [19] 20.30 20.10 19.99 19.99 19.95 19.99 20.25 20.18 20.25
## [28] 19.95 20.05 19.95 20.10</code></pre>

<p>The <strong>predicted.target</strong> is our <strong>ensemble prediction</strong>, derived as the summation of the residual outputs of our weak learners and is expressed as such:</p>
<p><span class="math display">\[\begin{align}
F_2(X) = F_1(X) + \eta \times r_2\ \ \ \ \ \ \ \ \ where\ \eta \text{ is learning rate, and } r_2  =  h_2(X)
\end{align}\]</span></p>
<p>Notice that each estimated target gets closer in the direction of the true target values.</p>
<p><strong>Finally</strong>, we iterate the entire process starting from step 3 until a stopping criterion.</p>
<p>All that can be summarized using the below <strong>Gradient Boost</strong> algorithm <span class="citation">(Friedman J. <a href="bibliography.html#ref-ref621j">1999</a><a href="bibliography.html#ref-ref621j">b</a>)</span>:</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in \mathbb{R}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M\\
\ \ \ \text{a loss function}: Lik(y, \hat{y}) = \frac{1}{2}(y - \hat{y})^2\\
\ \ \ \text{a weak-learner base regressor}: h(x)\\
\ \ \ \text{a learning rate}: \eta\\
\mathbf{Algorithm}:\\
\ \ \ F_0(X) = \text{arg}\ \underset{\gamma}{min} \sum_{x=1}^n Lik(y_i, \gamma )
\ \ \ &amp;\text{(initialize)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ \ r_{im} = -\left[\frac{\partial Lik(y_i, F(x_i))}{\partial F(x_i)}\right]_{F(X) = F_{m-1}(X)} = y - \hat{y}&amp;\text{(pseudo-residuals)}\\
\ \ \ \ \ \ \ \text{fit model using } h_m(\{x_i,r_{im}\}_{i=1}^n)\ &amp;\text{(fit model to pseudo-residual)} \\
\ \ \ \ \ \ \ \text{for each}\ (L)_j \in (T)_m\ &amp;\text{where (L)eaf and (T)ree}  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \gamma_{jm} = \text{arg}\ \underset{\gamma}{min} \sum_{x \in L_{jm}} Lik(y, F_{m-1}(x) + \gamma) &amp; \text{(leaf residual)}\\
\ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ F_m(x) = F_{m-1}(x) + 
\begin{cases}
\eta\ h_m(x)  \\ 
\gamma_{m}\ h_m(x) 
\end{cases} 
&amp;
\begin{cases}
\text{regression tree} \\
\text{otherwise, other regressors}
\end{cases} \\
\ \ \ \text{end loop} \\
\ \ \ \text{Output }F_m(x)
\end{array}
\]</span>
</p>
<p>And below is the example implementation of the algorithm:</p>

<div class="sourceCode" id="cb1394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1394-1" data-line-number="1">my.gbm &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">machines=</span><span class="dv">5</span>, <span class="dt">learning.rate=</span><span class="fl">0.01</span>) {</a>
<a class="sourceLine" id="cb1394-2" data-line-number="2">  mse &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>) }</a>
<a class="sourceLine" id="cb1394-3" data-line-number="3">  x =<span class="st"> </span>data[,<span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1394-4" data-line-number="4">  y =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1394-5" data-line-number="5">  eta =<span class="st"> </span>learning.rate</a>
<a class="sourceLine" id="cb1394-6" data-line-number="6">  F0 =<span class="st"> </span>y.hat =<span class="st"> </span>y.mean =<span class="st"> </span><span class="kw">mean</span>(y) <span class="co"># Initial residual (target.mean)</span></a>
<a class="sourceLine" id="cb1394-7" data-line-number="7">  tot.predicted =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="kw">length</span>(y), machines, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1394-8" data-line-number="8">  residuals     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, machines)</a>
<a class="sourceLine" id="cb1394-9" data-line-number="9">  old.mse       =<span class="st"> </span><span class="ot">Inf</span>; model =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1394-10" data-line-number="10">  <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>machines) {</a>
<a class="sourceLine" id="cb1394-11" data-line-number="11">    residual      =<span class="st"> </span>y <span class="op">-</span><span class="st"> </span>y.hat  <span class="co"># negative gradient (r.im) (target.residual)</span></a>
<a class="sourceLine" id="cb1394-12" data-line-number="12">    wk.model      =<span class="st"> </span><span class="kw">h</span>(features, target, x, residual)</a>
<a class="sourceLine" id="cb1394-13" data-line-number="13">    tot.predicted[,m] =<span class="st"> </span>wk.model<span class="op">$</span>predicted.residual</a>
<a class="sourceLine" id="cb1394-14" data-line-number="14">    hms           =<span class="st"> </span><span class="kw">apply</span>(tot.predicted, <span class="dv">1</span>, sum) </a>
<a class="sourceLine" id="cb1394-15" data-line-number="15">    y.hat         =<span class="st"> </span>y.mean <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>hms  <span class="co"># Fm = F0 + n * h_1(x)  ... </span></a>
<a class="sourceLine" id="cb1394-16" data-line-number="16">    <span class="cf">if</span> (old.mse <span class="op">==</span><span class="st"> </span><span class="kw">mse</span>( y <span class="op">-</span><span class="st"> </span>y.hat )) <span class="cf">break</span></a>
<a class="sourceLine" id="cb1394-17" data-line-number="17">    residuals[m]      =<span class="st"> </span><span class="kw">mse</span>( y <span class="op">-</span><span class="st"> </span>y.hat )</a>
<a class="sourceLine" id="cb1394-18" data-line-number="18">    old.mse =<span class="st"> </span>residuals[m]</a>
<a class="sourceLine" id="cb1394-19" data-line-number="19">  }</a>
<a class="sourceLine" id="cb1394-20" data-line-number="20">  <span class="co"># final predicted target ( y.hat is Fm )</span></a>
<a class="sourceLine" id="cb1394-21" data-line-number="21">  <span class="kw">list</span>(<span class="st">&quot;mse&quot;</span> =<span class="st"> </span>residuals, <span class="st">&quot;y.hat&quot;</span> =<span class="st"> </span>y.hat, <span class="st">&quot;model&quot;</span> =<span class="st"> </span>wk.model<span class="op">$</span>model )</a>
<a class="sourceLine" id="cb1394-22" data-line-number="22">}</a></code></pre></div>

<p>Let us use the <strong>implementation</strong> to learn a trainset.</p>

<div class="sourceCode" id="cb1395"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1395-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb1395-2" data-line-number="2">sample.set =<span class="st"> </span>trainset </a>
<a class="sourceLine" id="cb1395-3" data-line-number="3">ensemble.model =<span class="st"> </span><span class="kw">my.gbm</span>(features, target, sample.set,  <span class="dt">machine=</span><span class="dv">50</span>, </a>
<a class="sourceLine" id="cb1395-4" data-line-number="4">                        <span class="dt">learning.rate=</span><span class="dv">1</span>)</a></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gradientboost1"></span>
<img src="DS_files/figure-html/gradientboost1-1.png" alt="Gradient Boost (Regression)" width="70%" />
<p class="caption">
Figure 10.12: Gradient Boost (Regression)
</p>
</div>
<p>If we compare the variance between the target and the estimate, we obtain the following:</p>

<div class="sourceCode" id="cb1396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1396-1" data-line-number="1"><span class="kw">summary</span>(sample.target)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    10.4    15.3    19.2    20.1    22.8    33.9</code></pre>
<div class="sourceCode" id="cb1398"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1398-1" data-line-number="1"><span class="kw">summary</span>(ensemble.model<span class="op">$</span>y.hat)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##    10.4    15.3    19.2    20.1    22.8    33.9</code></pre>

<p>In our implementation of <strong>Gradient Boost</strong>, our weak learner uses <strong>CART</strong> - regression tree. However, we have not allowed a way to control the minbucket and maxdepth hyperparameters, let alone calibrate the complexity parameter. They are made constant. The following section introduces an algorithm that allows us to adjust <strong>knobs</strong> to calibrate our models.</p>
<p>Note that there are other variants of Boosting techniques, namely <strong>LogitBoost</strong>, <strong>BrownBoost</strong>, <strong>CatBoost</strong>, <strong>LightGBM</strong>, and <strong>LPBoost</strong>. We leave readers to investigate other boosting variants.</p>
</div>
<div id="xgboost" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.6</span> XGBoost <a href="machinelearning2.html#xgboost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To recognize <strong>XGBoost</strong> as the dream algorithm for most <strong>ML enthusiasts</strong> is probably a true statement - after all, it is also recognized to win competitions.</p>
<p><strong>XGBoost</strong> stands for <strong>Extreme Gradient Boost</strong>, and Tianqi Chen developed it to assess every <strong>nook and cranny</strong> of <strong>Gradient Boost</strong>. The idea is to implement a roster of tools and techniques to address optimizing the <strong>Gradient Boost</strong> algorithm at scale, along with many other features it offers. For example, we use a <strong>CART booster (or learner)</strong> for <strong>Gradient Boost</strong> such that the basic hyperparameters to tune are minbucket, minsplit, maxdepth, complexity parameter, and many others, including a choice of sampling (and sub-sampling). <strong>XGBoost</strong> finds the means to calibrate those hyperparameters and use <strong>Regularization</strong> such as <strong>LASSO or Ridge regularization</strong> in <strong>Linear Regression</strong>. For another example, <strong>Gradient Boost</strong> relies on sequentially boosting weak learners. <strong>XGBoost</strong> finds the means to parallelize the process. On top of that, it also looks into efficiently utilizing computational resources by handling caching optimization and parallel processing in a distributed manner.</p>
<p>To illustrate, let us use a third-party R implementation of the <strong>XGBoost</strong> called <strong>xgboost(.)</strong> made available through <strong>open source</strong>.</p>

<div class="sourceCode" id="cb1400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1400-1" data-line-number="1"><span class="kw">library</span>(xgboost)</a>
<a class="sourceLine" id="cb1400-2" data-line-number="2">m.trainset =<span class="st"> </span><span class="kw">apply</span>( <span class="kw">as.matrix</span>(trainset), <span class="dv">2</span>, as.numeric)</a>
<a class="sourceLine" id="cb1400-3" data-line-number="3">xgboost.model =<span class="st"> </span><span class="kw">xgboost</span>(<span class="dt">data =</span> m.trainset, <span class="dt">label =</span> trainset<span class="op">$</span>mpg,  <span class="dt">verbose =</span> <span class="dv">0</span>,</a>
<a class="sourceLine" id="cb1400-4" data-line-number="4">                        <span class="dt">max.depth =</span> <span class="dv">2</span>, <span class="dt">eta =</span> <span class="dv">1</span>, <span class="dt">nthread =</span> <span class="dv">2</span>, <span class="dt">nrounds =</span> <span class="dv">50</span>, </a>
<a class="sourceLine" id="cb1400-5" data-line-number="5">                        <span class="dt">objective =</span> <span class="st">&quot;reg:squarederror&quot;</span>, <span class="dt">eval_metric=</span><span class="st">&quot;rmse&quot;</span>)</a></code></pre></div>

<p>The algorithm offers a list of <strong>objective</strong> options (based on <strong>xgboost</strong> R documentation):</p>
<p><span class="math display">\[
\begin{array}{lll}
\text{reg:squarederror} &amp; \text{reg:squaredlogerror} &amp; \text{reg:logistic} \\
\text{binary:logistic} &amp; \text{binary:hinge} &amp; \text{count:poisson} \\
\text{survival:cox} &amp; \text{survival:aft} &amp; \text{multi:softmax} \\
\text{multi:softprob} &amp; \text{rank:pairwise} &amp; \text{rank:ndcg} \\
\text{rank:map} &amp; \text{reg:gamma} &amp; \text{reg:tweedie} \\
\end{array}
\]</span></p>
<p>Correspondingly, it also offers the following evaluation metrics, depending upon the chosen <strong>objective</strong> function:</p>
<p><span class="math display">\[
\begin{array}{lll}
\text{rmse} &amp; \text{rmsle} &amp; \text{mae} \\
\text{mape} &amp; \text{logloss} &amp; \text{mlogloss} \\
\text{error} &amp; \text{auc} &amp; \text{ndcg} \\
\text{map} &amp; \text{poisson-nloglik} &amp; \text{gamma-nloglik} \\
\text{cox-nloglik} &amp; \text{gamma-deviance} &amp; \text{tweedie-noglik} \\
\end{array}
\]</span></p>
<p>See Figure <a href="machinelearning2.html#fig:xgboost1">10.13</a> for the plot of the <strong>RMSE</strong> result.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:xgboost1"></span>
<img src="DS_files/figure-html/xgboost1-1.png" alt="XGBoost (Regression)" width="70%" />
<p class="caption">
Figure 10.13: XGBoost (Regression)
</p>
</div>
<p>We leave readers to investigate the many features and combinations of hyperparameters used by <strong>XGBoost</strong> to understand its opportunities and potential. The key takeaway in this section is that <strong>XGBoost</strong> can be viewed as an implementation of some of the theories we covered in previous chapters, namely <strong>Numerical Linear Algebra</strong>, <strong>Probability and Distribution</strong>, <strong>Statistical Computation</strong>, and <strong>Bayesian Computation</strong>.</p>
<p>Additionally, we leave readers to investigate and evaluate <strong>LightGBM</strong>.</p>
</div>
<div id="generalized-linear-modeling-glm" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.7</span> Generalized Linear Modeling (GLM)  <a href="machinelearning2.html#generalized-linear-modeling-glm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In conventional linear regression, it is common to fit models on datasets in which the continuous response variable, namely <strong>y</strong>, is characterized by a linear function. Such a response variable commonly follows a normal distribution:</p>
<p><span class="math display">\[\begin{align}
f(x) = y = x^T\beta\ \ \ \ \rightarrow where\ y \ \sim\ \mathcal{N}(\mu = x^TB, \text{sd} =\sigma)\ \ \ \ 
\end{align}\]</span></p>
<p>where <strong>x</strong> represents dependent covariates and <span class="math inline">\(\beta\)</span> represents weights or coefficients.</p>
<p>In other cases, the response variable may be characterized by some non-linear function following an <strong>exponential</strong> family of distributions, e.g., Binomial, Poisson, Bamma, and on. To accommodate such cases, McCullagh P. and Nelder J.A. <span class="citation">(<a href="bibliography.html#ref-ref737m">1983</a>)</span> popularized the concept of <strong>General Linear Models (GLM)</strong>. For example, <strong>GLM</strong> supports a response variable that may follow a <strong>Binomial distribution</strong> like so:</p>
<p><span class="math display">\[\begin{align}
f(x) = \mu = x^T\beta\ \ \ \ \rightarrow where\ \mu \ \sim\ \mathcal{Bin}(n, \rho)\ \ \ \ 
\end{align}\]</span></p>
<p>or it may follow an <strong>Exponential distribution</strong> like so:</p>
<p><span class="math display">\[\begin{align}
f(x) = \mu = x^T\beta\ \ \ \ \rightarrow where\ \mu \ \sim\ \mathcal{Exp}(\lambda)\ \ \ \ 
\end{align}\]</span></p>
<p>or even follow a <strong>Poission distribution</strong> like so:</p>
<p><span class="math display">\[\begin{align}
f(x) = \mu = x^T\beta\ \ \ \ \rightarrow where\ \mu \ \sim\ \mathcal{Pois}(\lambda)\ \ \ \ 
\end{align}\]</span></p>
<p>Each of these <strong>exponential</strong> distributions tends towards a central tendency. Our expressions above use the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) to indicate that <strong>every point</strong> in <strong>y</strong> represents the center or the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) of a family of distribution. For example, the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) of Normal distribution in Linear Regression is simply the fitted values (<span class="math inline">\(\hat{y_i}\)</span>). See <strong>Point-Estimate vs Stochastic Estimate</strong> Figure in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under <strong>Bayesian Inference</strong> Section. On the other hand, the <strong>mean</strong> of Binomial distribution in Logistic Regression is formed by the following <strong>sigmoid</strong> equation:</p>
<p><span class="math display">\[\begin{align}
\mu = P(y=1|x) = \frac{exp(y)}{1 + exp(y)}
\end{align}\]</span></p>
<p>This <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) creates what we see as the <strong>Sigmoid</strong> curve, which is non-linear. <strong>GLM</strong> transforms the non-linear relationship between the independent variables and the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) into one that is linear by using a <strong>link function</strong> denoted as <strong>g(.)</strong>. In the case of <strong>logistic regression</strong>, the <strong>link function</strong> is expressed as:</p>
<p><span class="math display">\[\begin{align}
g(\mu) = \frac{\mu}{1 - \mu}
\end{align}\]</span></p>
<p>For other exponential distributions, see Figure <a href="machinelearning2.html#fig:glm">10.14</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:glm"></span>
<img src="glm.png" alt="Generalized Linear Models" width="70%" />
<p class="caption">
Figure 10.14: Generalized Linear Models
</p>
</div>
<p>Let us list a few <strong>exponential</strong> distributions that are common in many literatures. See Table <a href="machinelearning2.html#tab:linkfunction">10.1</a>.</p>

<table>
<caption><span id="tab:linkfunction">Table 10.1: </span>General Linear Model</caption>
<colgroup>
<col width="13%" />
<col width="17%" />
<col width="35%" />
<col width="17%" />
<col width="16%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Family</th>
<th align="left">Link Name</th>
<th align="left">Mean Function</th>
<th align="left">Link Function</th>
<th align="left">Support</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Gaussian</td>
<td align="left">Identity</td>
<td align="left"><span class="math inline">\(\mu = y = x^T\beta\)</span></td>
<td align="left"><span class="math inline">\(\mu\)</span></td>
<td align="left"><span class="math inline">\((-\infty, +\infty)\)</span></td>
</tr>
<tr class="even">
<td align="left">Inverse Gaussian</td>
<td align="left">Inverse Squared</td>
<td align="left"><span class="math inline">\(\mu = (x^T\beta)^{-1/2}\)</span></td>
<td align="left"><span class="math inline">\(\mu^{-2}\)</span></td>
<td align="left"><span class="math inline">\((0, +\infty)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Bernoulli</td>
<td align="left">Logit (Logistic Unit)</td>
<td align="left"><span class="math inline">\(\mu = \frac{exp(x^T\beta)}{1+exp(x^T\beta)}\)</span></td>
<td align="left"><span class="math inline">\(\log_e(\mu/(1 - \mu)\)</span></td>
<td align="left">[0,1]</td>
</tr>
<tr class="even">
<td align="left">Binomial</td>
<td align="left">Logit</td>
<td align="left"><span class="math inline">\(\mu = \frac{exp(x^T\beta)}{1+exp(x^T\beta)}\)</span></td>
<td align="left"><span class="math inline">\(\log_e(\mu/(1 - \mu)\)</span></td>
<td align="left">[0,N]</td>
</tr>
<tr class="odd">
<td align="left">Binomial</td>
<td align="left">Probit (Probab. Unit)</td>
<td align="left"><span class="math inline">\(\mu = \phi(x^T\beta)\)</span></td>
<td align="left"><span class="math inline">\(\phi^{-1}(\mu)\)</span></td>
<td align="left">[0,1]</td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left">Natural Log</td>
<td align="left"><span class="math inline">\(\mu=exp(x^T\beta)\)</span></td>
<td align="left"><span class="math inline">\(\log_e(\mu)\)</span></td>
<td align="left"><span class="math inline">\([0,+\infty)\)</span></td>
</tr>
<tr class="odd">
<td align="left">Exponential</td>
<td align="left">Negative Inverse</td>
<td align="left"><span class="math inline">\(\mu=-(x^T\beta)^{-1}\)</span></td>
<td align="left"><span class="math inline">\(-\mu^{-1}\)</span></td>
<td align="left"><span class="math inline">\((0,+\infty)\)</span></td>
</tr>
</tbody>
</table>

<p>In the next two sections, we demonstrate the use of <strong>GLM</strong> using the <strong>glm(.)</strong> function with which we model a family of distributions specific to <strong>Exponential Distributions</strong> such as <strong>Binomial and Poisson Distribution</strong>. We also introduce <strong>Binary Classification</strong> through <strong>Logistic Regression</strong> and <strong>Poisson Regression</strong>.</p>
</div>
<div id="logisticregression" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.8</span> Logistic Regression (GLM)<a href="machinelearning2.html#logisticregression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Linear Regression</strong>, when fitting a linear model, we optimize the <span class="math inline">\(\beta\)</span>s parameters such that the corresponding response variable, the fitted (<span class="math inline">\(\hat{y}\)</span>) values, are derived - forming a <strong>fitted line</strong>. We commonly use <strong>Sum Squared Error (SSE)</strong> as our loss function. That is because the response variable is expected to be <strong>continuous</strong>. We fit a linear model to find the least square - we call this minimizing the loss function. </p>
<p>In <strong>Logistic Regression</strong> (also called <strong>Logit Regression</strong>), when fitting a logistic model, similar to <strong>linear regression</strong>, we also optimize the <span class="math inline">\(\beta\)</span> parameters such that the corresponding response variable, the fitted <span class="math inline">\(\hat{y}\)</span> values (also called <strong>logit</strong> values), are derived - also forming a <strong>fitted line</strong>. Note that it may look as if the response variable is <strong>linearly</strong> associated with the predictor variables. This is made possible only because the response variable holds the <strong>logit-transformed</strong> values (<strong>logit</strong> values) of its binomial probabilities. In other words, the response variable - assumed to be dichotomous - goes into two transformations to achieve a linear association with the predictor variables. For example, we start with the common equation below:</p>
<p><span class="math display">\[\begin{align}
Y = f(X) = \beta^TX
\end{align}\]</span></p>
<p>We then transform <strong>Y</strong> into a binomial probability with range [0, 1] using the <strong>inverse logit</strong> equation:</p>
<p><span class="math display">\[\begin{align}
\text{logit}^{-1}\left(\beta^TX\right) = \frac{exp(\beta^TX)}{1 + exp(\beta^TX)} 
\end{align}\]</span></p>
<p>That is derived from the fact that the <strong>invert logit</strong> is a binomial probability of observing a successful event and can also be expressed as such:</p>
<p><span class="math display">\[\begin{align}
\mu = P(y = 1|x) = \frac{exp(y)}{1 + exp(y)}
\ \ \ \ \ \ \text{where } \mu \sim Bin(n,p)\ \ \ and\ \ \ \ Y = f(X) = \beta^TX 
\end{align}\]</span></p>
<p>Here, the <strong>rho</strong> (<span class="math inline">\(\rho\)</span>) symbol indicates the probability of success, given <strong>n</strong> number of trials.</p>
<p>We see a <strong>sigmoid</strong> curve if we plot the binomial probability. Because a <strong>sigmoid</strong> curve remains to be non-linear in form, our next step is to use <strong>log-odds</strong> to transform the probability into <strong>log-odds unit</strong> using the following equation: </p>
<p><span class="math display">\[\begin{align}
g(\mu) = \text{logit}(\mu) = \log_e \left(\frac{\mu}{1 - \mu}\right) = \beta^TX = \beta_0 + \beta_1 x_1 + ... + \beta_n x_n
\end{align}\]</span></p>
<p>where <span class="math inline">\(g(\mu)\)</span> is a <strong>link function</strong>.</p>
<p>In terms of <strong>goodness of fit</strong>, it is worth noting that when we fit a logistic model, we use the following <strong>inverse logit</strong> function below, in which we obtain a p-hat (<span class="math inline">\(\hat{p}\)</span>) for <strong>logistic regression</strong> similar to the common y-hat (<span class="math inline">\(\hat{y}\)</span>) for <strong>linear regression</strong>. Note that our p-hat (<span class="math inline">\(\hat{p}\)</span>) is also equivalent to the mean (<span class="math inline">\(\mu\)</span>), pointing to the fact that our binomial distribution - geometrically presented as a <strong>sigmoid curve</strong> - has a central tendency such that every point along the curve ends up to be a mean (<span class="math inline">\(\mu_i\)</span>), center to the noise model. </p>
<p><span class="math display">\[\begin{align}
\hat{p} = \mu = \text{logit}^{-1}\left(\beta^TX\right)
\end{align}\]</span></p>
<p>Additionally, let us not forget that <span class="math inline">\(\hat{p}\)</span> is the estimated probability of an event being successful. Therefore, to account for events that are not successful, we have the following equation:</p>
<p><span class="math display">\[\begin{align}
\hat{q} = 1 - \hat{p}
\end{align}\]</span></p>
<p>It becomes apparent when we compute for the <strong>MLE</strong> for our loss function. </p>
<p>As always, our goal in fitting a model is to optimize the <span class="math inline">\(\beta\)</span> parameters. It is achieved by using <strong>maximum likelihood estimate (MLE)</strong> as our loss function because of the non-linearity of our <strong>logistic</strong> model. However, we use its counterpart <strong>negative log-likelihood (NLL)</strong> instead to keep the common notion of minimizing a loss function. When using <strong>MLE</strong>, we tend to maximize the likelihood estimate - similar to maximizing the loss function, which may not be sensible. If we use <strong>NLL</strong>, it is natural to aim for the least <strong>NLL</strong>; thus, minimizing the loss function.   </p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = \text{arg}\ \underset{\beta}{\text{min}}\
\underbrace{\mathcal{L}\text{oss}\left\{   -
\sum_{i=1}^n \left[
 \underbrace{y_i \log_e \left(\mu\right)}_{\text{if Y=1}}
 + 
 \underbrace{ (1 - y_i) \log_e \left( \text{logit}\left(\frac{\mu}{ 1 - \mu}\right)\right)}_{\text{if Y=0}}
 \right]
 \right\} 
}_{\text{NLL}}
\end{align}\]</span></p>
<p>where: <span class="math inline">\(\mu = \text{logit}^{-1}\left(\beta^TX\right)\)</span>.</p>
<p>The equation for the loss function above can be written in a simpler form:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta} = \text{arg}\ \underset{\beta}{\text{min}}\ \mathcal{L}\text{oss}\left\{ -  
\sum_{i=1}^n \left[
  y_i \log_e \left(\mu\right) +  (1 - y_i) \log_e \left( 1 -\mu \right) 
 \right]
 \right\} 
\end{align}\]</span></p>
<p>As long as the <strong>loss function</strong> is differentiable, we should be able to perform partial derivatives with respect to each <span class="math inline">\(\beta_j\)</span> parameter like so:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \beta_j} =- \sum_{i=1}^n \left[
  y_i \log_e \left(\mu\right) +  (1 - y_i) \log_e \left( 1 -\mu \right) 
 \right] = 0
\end{align}\]</span></p>
<p>We leave readers to investigate the use of <strong>grid search</strong>, or <strong>Monte Carlo</strong>, to simulate different combinations of the <span class="math inline">\(\beta\)</span> parameters. In reference to <strong>Bayesian Computation</strong> and <strong>Deep Neural Network</strong> chapters, we can also look for other optimization strategies, namely <strong>Laplace Approximation</strong> and <strong>Gradient Descent</strong> or <strong>Newton Raphson</strong> from the <strong>Linear Algebra</strong> chapter.  </p>
<p>Note that the <strong>NLL Loss function</strong> is also termed the <strong>Cross-Entropy Loss Function</strong> in <strong>Deep Neural Network</strong>.</p>
<p>In fitting a model, each combination of <span class="math inline">\(\beta\)</span> parameters corresponds to a line geometrically. As we adjust our <span class="math inline">\(\beta\)</span> parameters, the line that is formed becomes a reference model to obtain our <strong>yhat</strong> (<span class="math inline">\(\hat{y}\)</span>) - we can call this <strong>logit-hat</strong>. However, we do not use the <strong>logit-hat</strong> (<span class="math inline">\(\hat{y}\)</span>) to calculate the <strong>Least-Squares</strong>. Instead, we convert the <strong>logit-hat</strong> to its equivalent estimated probability - the <strong>p-hat</strong> - using the <strong>inverse logit</strong> function. It is this probability for which we minimize our NLL loss function. Once we find the least NLL, then the corresponding <span class="math inline">\(\beta\)</span> parameters are the model components for the final fit. Here we use <strong>plogis(.)</strong> for the conversion. See also the equivalent <strong>inverse.logit(.)</strong> in <strong>Statistical Computation</strong> chapter. Now, to do that, the following helper functions are required: </p>
<div class="sourceCode" id="cb1401"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1401-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}</a>
<a class="sourceLine" id="cb1401-2" data-line-number="2">mle &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">sum</span>(<span class="kw">ln</span>(x)) }</a>
<a class="sourceLine" id="cb1401-3" data-line-number="3">nll &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="op">-</span><span class="kw">mle</span>(x) }</a></code></pre></div>
<p>We can then convert the <strong>logit-hat</strong> (<span class="math inline">\(\hat{p}\)</span>) to its <strong>inverse-logit</strong> form - the <strong>p-hat</strong> (<span class="math inline">\(\hat{p}\)</span>) values - to which we apply <strong>NLL</strong>.</p>
<div class="sourceCode" id="cb1402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1402-1" data-line-number="1">loss &lt;-<span class="st"> </span><span class="cf">function</span>(logit, target) {</a>
<a class="sourceLine" id="cb1402-2" data-line-number="2">      p.hat =<span class="st"> </span>logit </a>
<a class="sourceLine" id="cb1402-3" data-line-number="3">      prob =<span class="st"> </span><span class="kw">ifelse</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, p.hat, (<span class="dt">q.hat =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p.hat))</a>
<a class="sourceLine" id="cb1402-4" data-line-number="4">      <span class="kw">nll</span>(prob)  <span class="co"># Negative Log Likelihood</span></a>
<a class="sourceLine" id="cb1402-5" data-line-number="5">}</a></code></pre></div>
<p>Let us now use a simple mtcars dataset to illustrate <strong>logistic regression</strong>. Here, we discuss <strong>Binary Logistic</strong> regression because our dependent variable is binary (e.g. <strong>vs</strong>). We fit a model using <strong>glm(.)</strong> for logistic regression â note that <strong>glmnet(.)</strong> can be used which includes <strong>Regularization</strong>:</p>

<div class="sourceCode" id="cb1403"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1403-1" data-line-number="1">logit.model =<span class="st"> </span><span class="kw">glm</span>(vs <span class="op">~</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>disp, <span class="dt">data=</span>trainset, </a>
<a class="sourceLine" id="cb1403-2" data-line-number="2">                  <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span>logit))</a></code></pre></div>

<p>See <strong>Inverse Logit (Sigmoid)</strong> Figure in Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under <strong>Logistic Regression</strong> Section for a sample of the fitted <strong>sigmoid</strong> curve.</p>
<p>With the fitted model, we should be able to obtain the fitted values. The fitted values are the log odds of our <strong>p-hat</strong> <span class="math inline">\(\hat{p}\)</span> probabilities.</p>

<div class="sourceCode" id="cb1404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1404-1" data-line-number="1">logit.hat =<span class="st"> </span><span class="kw">fitted</span>(logit.model) </a>
<a class="sourceLine" id="cb1404-2" data-line-number="2"><span class="kw">round</span>(logit.hat,<span class="dv">3</span>)</a></code></pre></div>
<pre><code>##  [1] 0.631 0.742 0.878 0.500 0.003 0.806 0.000 0.994 0.951
## [10] 0.771 0.771 0.039 0.020 0.022 0.003 0.002 0.001 0.983
## [19] 0.979 0.971 0.857 0.043 0.046 0.000 0.004 0.971 0.821
## [28] 0.317 0.039 0.000 0.836</code></pre>

<p>The <strong>logit-hat</strong> is already based on the optimized model. We can see the final <strong>MLE</strong> and <strong>NLL</strong> obtained from this fit.</p>

<div class="sourceCode" id="cb1406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1406-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;NLL&quot;</span> =<span class="st"> </span><span class="kw">loss</span>(logit.hat, trainset<span class="op">$</span>vs), <span class="st">&quot;MLE&quot;</span> =<span class="st"> </span><span class="op">-</span><span class="kw">loss</span>(logit.hat, trainset<span class="op">$</span>vs))</a></code></pre></div>
<pre><code>##    NLL    MLE 
##  7.493 -7.493</code></pre>

<p>We can validate the loss using the <strong>logLik(.)</strong> function against the model like so:</p>

<div class="sourceCode" id="cb1408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1408-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;NLL&quot;</span> =<span class="st"> </span><span class="op">-</span><span class="kw">logLik</span>(logit.model), <span class="st">&quot;MLE&quot;</span> =<span class="st"> </span><span class="kw">logLik</span>(logit.model))</a></code></pre></div>
<pre><code>##    NLL    MLE 
##  7.493 -7.493</code></pre>

<p>Now, assume that we have optimized <span class="math inline">\(\beta\)</span> parameters to represent our model after fitting the logistic model (after minimizing our loss function) - that also means obtaining the fitted (<span class="math inline">\(\hat{y}\)</span>) values. The fitted binomial model (also called the error model or noise model) gives us the following coefficients (along with the confidence interval).</p>
<div class="sourceCode" id="cb1410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1410-1" data-line-number="1"><span class="kw">coefficients</span>(logit.model)</a></code></pre></div>
<pre><code>## (Intercept)          wt          hp        disp 
##     5.32430     2.04414    -0.06624    -0.01787</code></pre>
<div class="sourceCode" id="cb1412"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1412-1" data-line-number="1"><span class="kw">confint.default</span>(logit.model)</a></code></pre></div>
<pre><code>##                2.5 %    97.5 %
## (Intercept) -2.25902 12.907619
## wt          -1.20904  5.297312
## hp          -0.13836  0.005882
## disp        -0.05265  0.016913</code></pre>
<p>Note that the coefficients are in the <strong>log-odds</strong> unit. To get the <strong>odds ratio (OR)</strong>, we just exponentiate the logit coefficients like so: </p>
<div class="sourceCode" id="cb1414"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1414-1" data-line-number="1">(<span class="dt">OR =</span> <span class="kw">exp</span>(<span class="kw">coefficients</span>(logit.model)))</a></code></pre></div>
<pre><code>## (Intercept)          wt          hp        disp 
##    205.2647      7.7225      0.9359      0.9823</code></pre>
<p>This means that if <strong>hp</strong> and <strong>disp</strong> are constant, then <strong>wt</strong> is 7.7225 times more likely to be in the <strong>S (Straight) engine</strong> class.</p>
<p>In terms of optimizing specific parameters to fit a model, the <strong>goodness of fit</strong> in <strong>Logistic Regression</strong> does not use the common <strong>R-squared</strong>. Instead, we use a variant of it called <strong>Pseudo R-squared</strong>. There are a few variants that we can investigate, of which we list only three variants from the following contributors, namely <strong>McFadden</strong>, <strong>Cox &amp; Snell</strong>, and <strong>NagelKerke (Cragg-Uhler)</strong>. In our discussion, let us use <strong>McFaddenâs R-squared</strong> expressed like so:     </p>
<p><span class="math display">\[\begin{align}
R^2 = 1 - \frac{\log_e Lik(M_{Full})}{\log_e Lik(M_{Null})}
\ \ \ \ \ \ where \ \ \ \ \
\begin{array}{l}
\text{M}_{(Full)}\text{ = model with predictors}\\
\text{M}_{(Null)}\text{ = model without predictors}
\end{array} \label{eqn:eqnnumber405}
\end{align}\]</span></p>
<p>A <strong>model without predictors</strong> represents a horizontal line intercepting the <strong>y-axis</strong>. In other words, such a model makes all the <strong>logit-hat</strong> values correspond to the <span class="math inline">\(\beta_0\)</span> coefficient (the intercept) values.</p>
<div class="sourceCode" id="cb1416"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1416-1" data-line-number="1">null.model =<span class="st"> </span><span class="kw">glm</span>(vs <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data=</span>trainset, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span>logit))</a>
<a class="sourceLine" id="cb1416-2" data-line-number="2">logit.null =<span class="st"> </span><span class="kw">fitted</span>(null.model)</a></code></pre></div>
<p>We now calculate the <strong>McFaddenâs R-squared</strong>:</p>
<div class="sourceCode" id="cb1417"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1417-1" data-line-number="1">(<span class="dt">R.squared  =</span> <span class="kw">c</span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">logLik</span>(logit.model) <span class="op">/</span><span class="st"> </span><span class="kw">logLik</span>(null.model) ) )</a></code></pre></div>
<pre><code>## [1] 0.6489</code></pre>
<p>and to validate, we have:</p>
<div class="sourceCode" id="cb1419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1419-1" data-line-number="1">model.full =<span class="st"> </span><span class="op">-</span><span class="kw">loss</span>(logit.hat, trainset<span class="op">$</span>vs)  <span class="co"># MLE for the fit.</span></a>
<a class="sourceLine" id="cb1419-2" data-line-number="2">model.null =<span class="st"> </span><span class="op">-</span><span class="kw">loss</span>(logit.null, trainset<span class="op">$</span>vs) <span class="co"># MLE for the mean.</span></a>
<a class="sourceLine" id="cb1419-3" data-line-number="3">(<span class="dt">R.squared =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(model.full) <span class="op">/</span><span class="st"> </span>model.null)</a></code></pre></div>
<pre><code>## [1] 0.6489</code></pre>
<p>As for the three <span class="math inline">\(\mathbf{R^2}\)</span> variants, let us use a 3rd party package called <strong>rcompanion</strong> and use the function <strong>nagelkerke(.)</strong> like so:</p>
<div class="sourceCode" id="cb1421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1421-1" data-line-number="1"><span class="kw">library</span>(rcompanion)</a>
<a class="sourceLine" id="cb1421-2" data-line-number="2"><span class="kw">nagelkerke</span>(logit.model)<span class="op">$</span>Models</a></code></pre></div>
<pre><code>##                                                                    
## Model: &quot;glm, vs ~ wt + hp + disp, binomial(link = logit), trainset&quot;
## Null:  &quot;glm, vs ~ 1, binomial(link = logit), trainset&quot;</code></pre>
<div class="sourceCode" id="cb1423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1423-1" data-line-number="1"><span class="kw">nagelkerke</span>(logit.model)<span class="op">$</span>Pseudo.R.squared.for.model.vs.null</a></code></pre></div>
<pre><code>##                              Pseudo.R.squared
## McFadden                               0.6489
## Cox and Snell (ML)                     0.5908
## Nagelkerke (Cragg and Uhler)           0.7902</code></pre>
<p>As in the case of <span class="math inline">\(\mathbf{Pseudo\ R^2}\)</span>, a value of one indicates the best fit; otherwise, a value of zero indicates the worst fit. Our result above is a good fit at 0.6489.</p>
<p>In terms of using <strong>Logistic Regression</strong> as <strong>Binary Classification</strong>, we can convert the <strong>p-hat</strong> into its <strong>dichotomous</strong> form based on a 0.5 threshold like so:</p>
<p><span class="math display">\[
h(z) = \begin{cases} 
1 &amp; \text{if}\ \hat{p} &gt;= 0.5\ \ \ \ \text{when z &gt;= 0} \\
0 &amp; \text{if}\ \hat{p} &lt; 0.5\ \ \ \ \ \ \ \text{when z &lt; 0}
\end{cases}
\]</span></p>
<p>where: <span class="math inline">\(z = \text{logit}\)</span> and <span class="math inline">\(\hat{p} \text{= inverse-logit values (probabilities)}\)</span>.</p>
<div class="sourceCode" id="cb1425"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1425-1" data-line-number="1">h &lt;-<span class="st"> </span><span class="cf">function</span>(logit, target) { </a>
<a class="sourceLine" id="cb1425-2" data-line-number="2">      p.hat =<span class="st"> </span><span class="kw">plogis</span>(logit)</a>
<a class="sourceLine" id="cb1425-3" data-line-number="3">      prob =<span class="st"> </span><span class="kw">ifelse</span>(target <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, p.hat, <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p.hat)</a>
<a class="sourceLine" id="cb1425-4" data-line-number="4">      <span class="kw">ifelse</span>(prob <span class="op">&gt;=</span><span class="fl">0.5</span>, <span class="dv">1</span>, <span class="dv">0</span> )</a>
<a class="sourceLine" id="cb1425-5" data-line-number="5">}</a></code></pre></div>
<p>Using our previous model generated from <strong>glm(.)</strong>, we can then classify the fitted <strong>logit</strong> values like so:</p>
<div class="sourceCode" id="cb1426"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1426-1" data-line-number="1">(<span class="dt">fitted.class =</span> <span class="kw">h</span>(logit.hat, trainset<span class="op">$</span>vs))</a></code></pre></div>
<pre><code>##  [1] 0 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 0 1
## [29] 0 0 1</code></pre>
<p>Apart from using <span class="math inline">\(\mathbf{Pseudo\ R^2}\)</span> to measure <strong>goodness of fit</strong>, it also helps to measure the strength of association not only based on model performance (e.g., McFaddenâs full model vs.Â null model) but also based on predicted and observed outcomes. To test if our <strong>model</strong> performs <strong>accurately</strong>, we can use techniques described in the <strong>Statistical Computation</strong> chapter under the <strong>Goodness of Fit</strong> section, namely <strong>Wald test</strong>, <strong>Likelihood Ratio test</strong>, and <strong>Lagrange Multiplier test</strong>. Additionally, we also introduce three measures.</p>
<ul>
<li><strong>Deviance</strong> test</li>
</ul>
<p>The <strong>Deviance test</strong> is a measure between the <strong>fitted model</strong> and the <strong>saturated model</strong>. Whereas a <strong>null</strong> model is the most <strong>restrictive model</strong> (See Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under <strong>Inference for Regression</strong> Section), a <strong>saturated</strong> model is more general in that it has <strong>n</strong> parameters equivalent to the sample size with a likelihood equal to 1.</p>
<p><span class="math display">\[\begin{align}
G^2 = 2\sum_{i=1}^n O_i \log_e \left(\frac{O_i}{E_i}\right) = -2 \log_e \mathcal{L} \left(\hat{\beta}\right)
\end{align}\]</span></p>
<p>Using this test, small deviance suggests a good fit. For why we use -2 in the equation, refer to <strong>Wilkâs Theorem</strong>.</p>
<ul>
<li><strong>Pearsonâs Chi-square</strong> test </li>
</ul>
<p>The <strong>Pearsonâs Chi-square</strong> test is a widely used test of <strong>statistical significance</strong> between observed (<strong>y</strong>) and expected (<span class="math inline">\(\mathbf{\hat{y}}\)</span>) models.</p>
<p><span class="math display">\[\begin{align}
X^2_{(Pearson)} = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i} 
\end{align}\]</span></p>
<p>Using this test, a small <strong>Chi-square</strong> with <strong>p-value</strong> closer to one suggests a good fit. Otherwise, a large <strong>Chi-square</strong> with <strong>p-value</strong> less than 0.05 suggests a poor fit.</p>
<ul>
<li><strong>Hosmer-Lemeshow</strong> test </li>
</ul>
<p>The <strong>Hosmer-Lemeshow</strong> is expressed like so:</p>
<p><span class="math display">\[\begin{align}
X^2_{(HL)} = \sum_{i=1}^n \frac{(O_i - E_i)^2}{E_i(1 - \frac{E_i}{n_i})}
\end{align}\]</span></p>
<p>This test is known to render a good fit if the group is set to ten (default). We can use a third-party library called <strong>ResourceSelection</strong> and use the <strong>hoslem.test(.)</strong> function to run the test. See below:</p>
<div class="sourceCode" id="cb1428"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1428-1" data-line-number="1"><span class="kw">library</span>(ResourceSelection)</a>
<a class="sourceLine" id="cb1428-2" data-line-number="2">hl =<span class="st"> </span><span class="kw">hoslem.test</span>(trainset<span class="op">$</span>vs, <span class="kw">fitted</span>(logit.model), <span class="dt">g=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1428-3" data-line-number="3"><span class="kw">cbind</span>(hl<span class="op">$</span>expected, hl<span class="op">$</span>observed)</a></code></pre></div>
<pre><code>##                      yhat0    yhat1 y0 y1
## [3.23e-07,0.00106] 3.99880 0.001197  4  0
## (0.00106,0.00344]  2.99161 0.008386  3  0
## (0.00344,0.0219]   2.95444 0.045563  3  0
## (0.0219,0.0432]    2.87855 0.121455  3  0
## (0.0432,0.5]       2.13627 0.863735  1  2
## (0.5,0.771]        1.08517 2.914832  2  2
## (0.771,0.821]      0.37354 1.626456  1  1
## (0.821,0.878]      0.42893 2.571072  0  3
## (0.878,0.971]      0.10823 2.891768  0  3
## (0.971,0.994]      0.04446 2.955536  0  3</code></pre>
<p><strong>Stukel test</strong> and <strong>Osius-Rojek test</strong> are recent alternatives that offer benefits over <strong>Hosmer-Lemeshow test</strong>. We leave readers to investigate the two tests.</p>
<p>As for <strong>Deviance test</strong> and <strong>Pearsonâs Chi-square test</strong>, the next section uses both tests in the context of <strong>Poisson regression</strong>.</p>
<p>Finally, in terms of <strong>Prediction</strong> for <strong>Logistic Regression</strong> classification, let us use the model we trained previously. Our response variable (<strong>vs</strong>) represents types of engine car, which is a binomial target having only two values, namely <strong>V-shaped engine</strong> or <strong>Straight engine</strong>. We can predict and determine the probability of a given new car having an <strong>S engine</strong> given our fitted model.</p>

<div class="sourceCode" id="cb1430"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1430-1" data-line-number="1">logit.predicted =<span class="st"> </span><span class="kw">predict.glm</span>(logit.model, test.predictors, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb1430-2" data-line-number="2"> prob.predicted =<span class="st"> </span>P =<span class="st"> </span><span class="kw">plogis</span>(logit.predicted)</a>
<a class="sourceLine" id="cb1430-3" data-line-number="3">car =<span class="st"> </span><span class="kw">names</span>(P);  <span class="kw">names</span>(P) =<span class="st"> </span><span class="ot">NULL</span>; </a>
<a class="sourceLine" id="cb1430-4" data-line-number="4">est.class =<span class="st"> </span><span class="kw">ifelse</span>(P <span class="op">&gt;=</span><span class="dv">0</span>, <span class="st">&quot;S Engine&quot;</span>, <span class="st">&quot;V Engine&quot;</span>)</a>
<a class="sourceLine" id="cb1430-5" data-line-number="5"><span class="kw">c</span>(<span class="st">&quot;Car&quot;</span> =<span class="st"> </span>car, <span class="st">&quot;Probability&quot;</span> =<span class="st"> </span><span class="kw">round</span>(P,<span class="dv">2</span>), <span class="st">&quot;Predicted Class&quot;</span> =<span class="st"> </span>est.class)</a></code></pre></div>
<pre><code>##              Car      Probability  Predicted Class 
## &quot;Ford Pantera L&quot;            &quot;0.5&quot;       &quot;S Engine&quot;</code></pre>

<p>Before we go to <strong>SVM</strong> classification, let us extend (perhaps complete) our understanding of <strong>GLM</strong> by introducing <strong>Poisson Regression</strong>. While the section should fall under <strong>Regression</strong>, let us focus on the <strong>loss function</strong> as this topic becomes a more critical subject in subsequent classification discussions.</p>
</div>
<div id="poisson" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.1.9</span> Poisson Regression (GLM)<a href="machinelearning2.html#poisson" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Poisson Regression</strong> models events that occur at a certain rate and is used to forecast counts. Alternatively, we also can use <strong>Negative Binomial Regression</strong> to model the same. As is the case, our goal is to optimize <span class="math inline">\(\beta\)</span> parameters to achieve a good fit. To do that, we also start with the common equation below: </p>
<p><span class="math display">\[\begin{align}
Y = f(X) = \beta^TX
\end{align}\]</span></p>
<p>We then transform <strong>Y</strong> into a Poisson probability with range [0, <span class="math inline">\(+\infty\)</span>), using the exponential equation:</p>
<p><span class="math display">\[\begin{align}
\mu = exp(\beta^TX)\ \ \ \ \ where\ \mu\ \sim Pois(\lambda) 
\end{align}\]</span></p>
<p>Each point in the probability model (the curve line) corresponds to the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) - the central tendency of the Poisson distribution - which indicates the probability of observing the number of events occurring at a given period.</p>
<p>To achieve linearity of the relationship between independent variables and the dependent variable, we convert the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) to <strong>natural log</strong> like so:</p>
<p><span class="math display">\[\begin{align}
g(\mu) = \log_e(\mu) = \beta^TX 
\end{align}\]</span></p>
<p>where <span class="math inline">\(g(\mu)\)</span> is a <strong>link function</strong>, also called the <strong>natural link</strong>.</p>
<p>To now optimize the fit, we use <strong>MLE</strong> similar to <strong>Logistic Regression</strong>. Here, we express the <strong>log-likelihood</strong> using the below expression:</p>
<p><span class="math display">\[\begin{align}
Lik(\beta) = \prod_{i=1}^n \frac{\lambda_i^{Y_i} e^{-\lambda_i} }{Y_i!}
\ \ \ \ \ \rightarrow\ \ \ \ \ \ \
\log_e Lik(\beta) = \sum_{i=1}^n y_i \log_e \lambda_i -  \log_e y_i! - \lambda_i
\end{align}\]</span></p>
<p>Our example implementation of the log-likelihood is as follows:</p>
<div class="sourceCode" id="cb1432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1432-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}</a>
<a class="sourceLine" id="cb1432-2" data-line-number="2">poisson.likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(x, lambda) {</a>
<a class="sourceLine" id="cb1432-3" data-line-number="3"> (lambda<span class="op">^</span>x <span class="op">*</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>lambda)) <span class="op">/</span><span class="st"> </span><span class="kw">factorial</span>(x) </a>
<a class="sourceLine" id="cb1432-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1432-5" data-line-number="5">poisson.loglik &lt;-<span class="st"> </span><span class="cf">function</span>(x,lambda) {</a>
<a class="sourceLine" id="cb1432-6" data-line-number="6">  <span class="kw">sum</span>(x <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(lambda) <span class="op">-</span><span class="st"> </span><span class="kw">ln</span>(<span class="kw">factorial</span>(x)) <span class="op">-</span><span class="st"> </span>lambda) </a>
<a class="sourceLine" id="cb1432-7" data-line-number="7">}</a></code></pre></div>
<p>Our <strong>loss function</strong> becomes the following equation below. Here we use <strong>NLL</strong> to minimize the loss:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta_j} = \text{arg}\ \underset{\beta_j}{\text{min}}\
\underbrace{\mathcal{L}\text{oss}\left\{   
\lambda_{i=1}^n x_{ij}
\left[Y_i - exp\left(\beta^TX\right)\right] 
 \right\} 
}_{\text{NLL}}
\end{align}\]</span></p>
<p>As long as the <strong>loss function</strong> is differentiable, we should be able to perform partial derivatives with respect to each <span class="math inline">\(\beta_j\)</span> parameter like so:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial \mathcal{L}}{\partial \beta_j} = \lambda_{i=1}^n x_{ij}
\left[Y_i - exp\left(\beta^TX\right)\right] = 0
\end{align}\]</span></p>
<p>Only if differentiation cannot solve the optimization can we resort to the use of other <strong>iterative algorithms</strong> such as <strong>Gradient Descent</strong> or <strong>Newton Raphson</strong>.  </p>
<p>Now to illustrate <strong>Poisson Regression</strong>, let us simulate a dataset. We start with the following equation:</p>
<p><span class="math display">\[\begin{align}
\log_e(y) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
\ \ \ \ \ \ \rightarrow\ \ \ \ \  \mu = \exp(\beta_0 + \beta_1 x_1 + \beta_2 x_2)
\end{align}\]</span></p>
<div class="sourceCode" id="cb1433"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1433-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1433-2" data-line-number="2">N     =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1433-3" data-line-number="3">beta =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1433-4" data-line-number="4">x1     =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1433-5" data-line-number="5">x2     =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n =</span> N, <span class="dt">min =</span> <span class="dv">0</span>, <span class="dt">max =</span> <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1433-6" data-line-number="6">mu    =<span class="st"> </span><span class="kw">exp</span>(beta[<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>beta[<span class="dv">2</span>] <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>beta[<span class="dv">3</span>] <span class="op">*</span><span class="st"> </span>x2)</a></code></pre></div>
<p>Using the central tendency denoted as <strong>mu</strong> (<span class="math inline">\(\mu\)</span>), we generate a random outcome that follows a <strong>Poisson distribution</strong> like we use <strong>rpois(.)</strong> function so that our <strong>link function</strong> is <strong>g(.)</strong> = <strong>rpois(.)</strong>:</p>
<div class="sourceCode" id="cb1434"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1434-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1434-2" data-line-number="2">y  =<span class="st">  </span><span class="kw">rpois</span>(<span class="dt">n=</span>N, <span class="dt">lambda =</span> mu)</a></code></pre></div>
<p>Then, we fit three models. Note that one property of a <strong>Poisson distribution</strong> is to have the mean equal to the variance. However, as we cannot guarantee from our sample dataset. Let us use <strong>quasi-Poisson</strong> to handle <strong>overdispersion</strong> or <strong>underdispersion</strong> concerns - See Chapter <strong>6</strong> (<strong>Statistical Computation</strong>) under <strong>Dispersion</strong> Subsection under <strong>Regression Analysis</strong> Section.    </p>

<div class="sourceCode" id="cb1435"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1435-1" data-line-number="1">pois.model  =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2, <span class="dt">family=</span><span class="kw">quasipoisson</span>(<span class="dt">link=</span>log))</a>
<a class="sourceLine" id="cb1435-2" data-line-number="2">pois1.model =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x1, <span class="dt">family=</span><span class="kw">quasipoisson</span>(<span class="dt">link=</span>log))</a>
<a class="sourceLine" id="cb1435-3" data-line-number="3">pois2.model =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span>x2, <span class="dt">family=</span><span class="kw">quasipoisson</span>(<span class="dt">link=</span>log))</a></code></pre></div>

<p>We are now ready to plot. See Figure <a href="machinelearning2.html#fig:poissonreg">10.15</a>.</p>

<div class="sourceCode" id="cb1436"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1436-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x1), <span class="dt">ylim=</span><span class="kw">range</span>(y),</a>
<a class="sourceLine" id="cb1436-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;Frequency&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Interval&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Poisson Regression&quot;</span>)</a>
<a class="sourceLine" id="cb1436-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1436-4" data-line-number="4"><span class="kw">points</span>(x1,y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1436-5" data-line-number="5"><span class="kw">points</span>(x2,y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb1436-6" data-line-number="6">n      =<span class="st"> </span><span class="kw">length</span>(pois.model<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1436-7" data-line-number="7">y.hat  =<span class="st"> </span>pois.model<span class="op">$</span>fitted.values </a>
<a class="sourceLine" id="cb1436-8" data-line-number="8">y.hat1 =<span class="st"> </span>pois1.model<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb1436-9" data-line-number="9">y.hat2 =<span class="st"> </span>pois2.model<span class="op">$</span>fitted.values </a>
<a class="sourceLine" id="cb1436-10" data-line-number="10">x      =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>, <span class="dt">length.out =</span> n)</a>
<a class="sourceLine" id="cb1436-11" data-line-number="11">pois  =<span class="st"> </span>smoothingSpline =<span class="st"> </span><span class="kw">smooth.spline</span>(x, <span class="kw">sort</span>(y.hat), <span class="dt">spar=</span><span class="fl">1.00</span>)</a>
<a class="sourceLine" id="cb1436-12" data-line-number="12">pois1  =<span class="st"> </span>smoothingSpline =<span class="st"> </span><span class="kw">smooth.spline</span>(x, <span class="kw">sort</span>(y.hat1), <span class="dt">spar=</span><span class="fl">1.00</span>)</a>
<a class="sourceLine" id="cb1436-13" data-line-number="13">pois2  =<span class="st"> </span>smoothingSpline =<span class="st"> </span><span class="kw">smooth.spline</span>(x, <span class="kw">sort</span>(y.hat2), <span class="dt">spar=</span><span class="fl">1.00</span>)</a>
<a class="sourceLine" id="cb1436-14" data-line-number="14"><span class="kw">lines</span>( pois1 , <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1436-15" data-line-number="15"><span class="kw">lines</span>( pois2 , <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1436-16" data-line-number="16"><span class="kw">lines</span>( pois , <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1436-17" data-line-number="17"><span class="kw">legend</span>(<span class="dv">0</span>, <span class="dv">250</span>, </a>
<a class="sourceLine" id="cb1436-18" data-line-number="18">   <span class="kw">c</span>( <span class="st">&quot;pois.model (y ~ x1 + x2)&quot;</span>,  <span class="st">&quot;pois1.model (y ~ x1)&quot;</span>, </a>
<a class="sourceLine" id="cb1436-19" data-line-number="19">      <span class="st">&quot;pois2.model (y ~ x2)&quot;</span> ),</a>
<a class="sourceLine" id="cb1436-20" data-line-number="20">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), </a>
<a class="sourceLine" id="cb1436-21" data-line-number="21">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:poissonreg"></span>
<img src="DS_files/figure-html/poissonreg-1.png" alt="Poisson Regression" width="70%" />
<p class="caption">
Figure 10.15: Poisson Regression
</p>
</div>

<p>Let us analyze the <strong>pois.model</strong>:</p>

<div class="sourceCode" id="cb1437"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1437-1" data-line-number="1">(<span class="dt">summary.pois =</span> <span class="kw">summary</span>(pois.model))</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y ~ x1 + x2, family = quasipoisson(link = log))
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.152  -0.841   0.083   0.575   3.431  
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   1.0151     0.0564    18.0   &lt;2e-16 ***
## x1            1.9878     0.0532    37.4   &lt;2e-16 ***
## x2            2.9702     0.0589    50.4   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for quasipoisson family taken to be 1.052)
## 
##     Null deviance: 5347.02  on 99  degrees of freedom
## Residual deviance:  102.47  on 97  degrees of freedom
## AIC: NA
## 
## Number of Fisher Scoring iterations: 4</code></pre>

<p>Note that the <strong>summary</strong> shows the <strong>Dispersion</strong> measure as 1.0521. That is the adjusted <strong>Dispersion</strong> because of <strong>Quasi-Poisson distribution</strong>; otherwise, the value stays at 1.</p>
<p>The <strong>Deviance Residual</strong> shows values close to zero, indicating a balance bias, stretching evenly towards opposite directions. On the other hand, the unit variance for <strong>Poisson Distribution</strong> is as follows <span class="citation">(A. Colin Cameron et al. <a href="bibliography.html#ref-ref1104a">1995</a>; Zhirui Ye et al. <a href="bibliography.html#ref-ref1090z">2012</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
D(y, u) = 2\left(y \log_e\frac{y}{\mu} - y + \mu\right)
\end{align}\]</span></p>

<div class="sourceCode" id="cb1439"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1439-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}</a>
<a class="sourceLine" id="cb1439-2" data-line-number="2">deviance.test &lt;-<span class="st"> </span><span class="cf">function</span>(y,mu) {</a>
<a class="sourceLine" id="cb1439-3" data-line-number="3">    <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>(y <span class="op">*</span><span class="st"> </span><span class="kw">ln</span>(<span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="dv">1</span>, y<span class="op">/</span>mu)) <span class="op">-</span><span class="st"> </span>y <span class="op">+</span><span class="st"> </span>mu ) </a>
<a class="sourceLine" id="cb1439-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1439-5" data-line-number="5">residual.dev =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">deviance.test</span>(y, y.hat)) <span class="op">*</span><span class="st">  </span><span class="kw">ifelse</span>(y <span class="op">&gt;</span><span class="st"> </span>y.hat, <span class="dv">1</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1439-6" data-line-number="6"><span class="kw">summary</span>(residual.dev)</a></code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  -2.152  -0.841   0.083  -0.040   0.575   3.431</code></pre>

<p>The <strong>R-squared based on Pearsonâs residual</strong> shows the following computation:</p>
<p><span class="math display">\[\begin{align}
R^2_{(chi-square)} = \sum_{i=1}^n \frac{(y_i - \hat{y}_i)^2}{\hat{y}_i}\ \ \ \ \text{(Pearson&#39;s Chi-square)}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1441"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1441-1" data-line-number="1">pearson.rsquare1 =<span class="st"> </span><span class="kw">residuals</span>(pois.model, <span class="dt">type=</span><span class="st">&quot;pearson&quot;</span>)</a>
<a class="sourceLine" id="cb1441-2" data-line-number="2">pearson.rsquare2 =<span class="st"> </span>(y <span class="op">-</span><span class="st"> </span>y.hat) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(y.hat)</a>
<a class="sourceLine" id="cb1441-3" data-line-number="3"><span class="kw">all.equal</span>(pearson.rsquare1, pearson.rsquare2)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Additionally, all coefficients in the summary indicates <strong>statistical significance</strong>. We can use <strong>Chi-square test</strong> with the <strong>null model</strong> to validate that further. </p>

<div class="sourceCode" id="cb1443"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1443-1" data-line-number="1">null.model =<span class="st"> </span><span class="kw">glm</span>(y <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">family=</span><span class="kw">quasipoisson</span>(<span class="dt">link=</span>log))</a>
<a class="sourceLine" id="cb1443-2" data-line-number="2"><span class="kw">anova</span>(null.model, pois.model, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)</a></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: y ~ 1
## Model 2: y ~ x1 + x2
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    
## 1        99       5347                         
## 2        97        102  2     5245   &lt;2e-16 ***
## ---
## Signif. codes:  
## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>The result indicates that <strong>x1</strong> and <strong>x2</strong> are statistically significant predictors of <strong>y</strong>.</p>
<p>In terms of <strong>prediction</strong>, we use <strong>predict.glm(.)</strong> to predict the <strong>frequency</strong> outcome for the <strong>response</strong> variable.</p>

<div class="sourceCode" id="cb1445"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1445-1" data-line-number="1">(<span class="dt">count =</span> <span class="kw">predict.glm</span>(pois.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x1 =</span> <span class="fl">0.6</span>, <span class="dt">x2=</span><span class="fl">0.6</span>), </a>
<a class="sourceLine" id="cb1445-2" data-line-number="2">                     <span class="dt">type=</span><span class="st">&quot;response&quot;</span>))</a></code></pre></div>
<pre><code>##     1 
## 54.05</code></pre>

<p>Alternatively, we can also obtain the <strong>link</strong> outcome; otherwise, the log of the <strong>frequency</strong> outcome.</p>

<div class="sourceCode" id="cb1447"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1447-1" data-line-number="1">log.count =<span class="st"> </span><span class="kw">predict.glm</span>(pois.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x1 =</span> <span class="fl">0.6</span>, <span class="dt">x2=</span><span class="fl">0.6</span>), </a>
<a class="sourceLine" id="cb1447-2" data-line-number="2">                        <span class="dt">type=</span><span class="st">&quot;link&quot;</span>)</a>
<a class="sourceLine" id="cb1447-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;link&quot;</span> =<span class="st"> </span>log.count, <span class="st">&quot;log(response)&quot;</span> =<span class="st"> </span><span class="kw">log</span>(count))</a></code></pre></div>
<pre><code>##          link.1 log(response).1 
##            3.99            3.99</code></pre>

</div>
</div>
<div id="binary-classification-supervised" class="section level2 hasAnchor">
<h2><span class="header-section-number">10.2</span> Binary Classification (Supervised)<a href="machinelearning2.html#binary-classification-supervised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>For <strong>Regression</strong> - specifically <strong>Linear Regression</strong>, we tend to use <strong>sum squared error (SSE)</strong> to minimize our loss function and use <strong>Mean Square Error (MSE)</strong> or <strong>Root Mean Square Error (RMSE)</strong> to evaluate performance for which the response variable is continuous.</p>
<p>For <strong>Classification</strong>, we minimize our loss (or gain) function based on measures derived from <strong>Information Theory</strong> such as <strong>Gini Index (GI)</strong> and <strong>Entropy (H)</strong> and use <strong>confusion matrix</strong> as a reference to evaluate performance for which the response variable is categorical. We score the performance based on <strong>accuracy</strong>, <strong>sensitivity</strong>, <strong>specificity</strong>, and <strong>F1 score</strong>.</p>
<div id="linear-svm-sgdpegasos" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.1</span> Linear SVM (SGD/PEGASOS)  <a href="machinelearning2.html#linear-svm-sgdpegasos" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Support Vector Machine (SVM)</strong> is a classic <strong>Computational Learning</strong> technique used for <strong>classification</strong>. As long as data points are <strong>linearly separable</strong>, we can use a simple <strong>linear SVM (LSVM)</strong> to perform <strong>classification</strong>. If that is not the case, then perhaps there are other ways. Figure <a href="machinelearning2.html#fig:svm2">10.16</a> shows example techniques of how to separate data points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:svm2"></span>
<img src="svm2.png" alt="Support Vector Machine" width="90%" />
<p class="caption">
Figure 10.16: Support Vector Machine
</p>
</div>
<p>Modulus transformation is a good illustration of how one can use even basic mathematics to separate data points. For example, in separating even numbers from odd numbers, we use modulus operations. Additionally, Figure <a href="machinelearning2.html#fig:svm1">10.17</a> shows SVM techniques in separating data points.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:svm1"></span>
<img src="svm1.png" alt="Support Vector Machine" width="90%" />
<p class="caption">
Figure 10.17: Support Vector Machine
</p>
</div>
<p>In this section, let us discuss <strong>classification</strong> using <strong>linear SVM</strong>. Note that <strong>LSVM</strong> is about modeling an optimal <strong>decision boundary</strong> and not about modeling probability in the same way we do with <strong>General Linear Models</strong>. It is different from <strong>linear regression</strong> in which we measure the <strong>minimum</strong> horizontal distance - the residual - from the data point to the line. It is also far different from the <strong>Principal Component Analysis (PCA)</strong> in which we measure the <strong>maximum</strong> distance - the variance - from the projected data point to the point of origin along the <strong>Principal Component</strong> axis (see Figure <a href="machinelearning1.html#fig:olspca">9.49</a>). In <strong>linear SVM</strong>, we measure the <strong>minimum</strong> orthogonal distance - a normal line - from the data point to a hyperplane. See Figure <a href="machinelearning2.html#fig:linearsvm">10.18</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearsvm"></span>
<img src="linearsvm.png" alt="Linear SVM" width="80%" />
<p class="caption">
Figure 10.18: Linear SVM
</p>
</div>
<p>Note that <strong>LSVM</strong> relies on concepts we cover in <strong>linear algebra</strong> around vector calculation. Understanding <strong>Euclidian norm</strong>, <strong>Dot Product</strong>, <strong>Magnitude</strong>, <strong>Direction</strong>, <strong>Projection</strong>, and many others are essential in building our intuition on <strong>SVM</strong>.</p>
<p>To understand the concept, we start with the standard <strong>linear equation</strong> (where <strong>m</strong> is the slope and <strong>b</strong> is the intercept) with a slight modification (where <span class="math inline">\(\mathbf{x_2}\)</span> takes the place of <strong>y</strong>):</p>
<p><span class="math display">\[\begin{align}
y = mx + b\ \ \ \ \ \rightarrow\ \ \ \ \ \ \ x_2 =  m x_1 + b
\end{align}\]</span></p>
<p>In an alternative format, we can re-arrange the equation by moving <span class="math inline">\(\mathbf{x_2}\)</span> to the right side and, at the same time, show a generalized equation for the line like so:</p>
<p><span class="math display">\[\begin{align}
0 = mx_1 - x_2 + b \ \ \ \ \ \ \rightarrow\ \ \ \ \ \ \ \ \ \
\underbrace{\mathbf{w}^T\mathbf{x} + b = 0}_{\text{1-D hyperplane}}
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> - <strong>norm</strong> to the hyperplane - and <span class="math inline">\(\mathbf{x}\)</span> are vectors such that:</p>
<p><span class="math display">\[
\mathbf{w} = 
\left[\begin{array}{r}
m \\
-1
\end{array}\right]
\ \ \ \ 
\text{and}
\ \ \ \ 
\mathbf{x} = 
\left[\begin{array}{r}
x_1 \\
x_2
\end{array}\right].
\]</span>
The <strong>b</strong> term becomes a <strong>bias</strong> coefficient that displaces the <strong>hyperplane</strong> from point of origin , namely <strong>O = </strong> <span class="math inline">\(\mathbf{[0,0]}^T\)</span>, if <strong>b &gt; 0</strong>.</p>
<p>Assume <strong>b=0</strong> and the slope is <strong>m=2</strong> based on <strong>rise=2</strong> and <strong>run=1</strong>. We then have the following:</p>

<div class="sourceCode" id="cb1449"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1449-1" data-line-number="1">b =<span class="st"> </span><span class="dv">0</span>; m =<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1449-2" data-line-number="2">x1 =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>, <span class="dt">length.out=</span><span class="dv">10</span>)  <span class="co"># some continuous obs for feature x1</span></a>
<a class="sourceLine" id="cb1449-3" data-line-number="3">x2  =<span class="st"> </span>m <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>b   </a></code></pre></div>

<p>The equation should equal to zero: <span class="math inline">\(m x1 - x2 = \mathbf{w}^T\mathbf{x} = 0\)</span></p>

<div class="sourceCode" id="cb1450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1450-1" data-line-number="1">w =<span class="st"> </span><span class="kw">c</span>(m, <span class="dv">-1</span>);  x =<span class="st"> </span><span class="kw">rbind</span>(x1, x2)</a>
<a class="sourceLine" id="cb1450-2" data-line-number="2">v1 =<span class="st"> </span>m <span class="op">*</span><span class="st"> </span>x1 <span class="op">-</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1450-3" data-line-number="3">v2 =<span class="st"> </span><span class="kw">c</span>( <span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>b )</a>
<a class="sourceLine" id="cb1450-4" data-line-number="4"><span class="kw">data.frame</span>(<span class="st">&quot;sum v1&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(v1), <span class="st">&quot;sum v2&quot;</span> =<span class="st"> </span><span class="kw">sum</span>(v2), <span class="st">&quot;equal&quot;</span> =<span class="st"> </span><span class="kw">all.equal</span>(v1, v2))</a></code></pre></div>
<pre><code>##   sum.v1 sum.v2 equal
## 1      0      0  TRUE</code></pre>

<p>Based on the equation above, we can see the plotted hyperplane in Figure <a href="machinelearning2.html#fig:hyperplane">10.19</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane"></span>
<img src="DS_files/figure-html/hyperplane-1.png" alt="Hyperplane" width="70%" />
<p class="caption">
Figure 10.19: Hyperplane
</p>
</div>
<p>As we can see, the vector <strong>w</strong> points in the <strong>direction</strong> denoted by (<span class="math inline">\(w_1\)</span>,<span class="math inline">\(w_2\)</span>) = (2, -1). To derive the size (magnitude) of <strong>w</strong>, we use the following euclidean formula (recall <strong>euclidean norm</strong>):</p>
<p><span class="math display">\[\begin{align}
\|\mathbf{w}\| = \sqrt{w_1^2 + w_2^2 + w^2_3 +\ ...\ + w^2_n} \\
= \sqrt{(2)^2 +(-1)^2} = \sqrt{5}
\end{align}\]</span></p>
<p>Given a vector, namely <strong>p (4,2)</strong>, let us compute the magnitude of the projection of <strong>p</strong> into the subspace <strong>w</strong>. Let us denote the projection of <strong>p</strong> as <strong>q</strong>. Here, we use the formula:</p>
<p><span class="math display">\[\begin{align}
c_{(scale)} = \frac{\mathbf{w} \cdot \mathbf{p}}{\| \mathbf{w} \|^2} =
\frac{
\left[\begin{array}{r} 2\\-1 \end{array}\right] \cdot
\left[\begin{array}{r} 4\\2 \end{array}\right] \label{eqn:eqnnumber406}
}
{(\sqrt{5})^2} = \frac{2\times4 + (-1)\times 2}{5} = 1.2
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathbf{q}_{(proj)} = c_{(scale)} \times \mathbf{w} = 
1.2 \times 
\left[\begin{array}{r} 2\\-1 \end{array}\right] = 
\left[\begin{array}{r} 2.4\\-1.2 \end{array}\right]  \label{eqn:eqnnumber407}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1452-1" data-line-number="1">proj &lt;-<span class="st"> </span><span class="cf">function</span>(p, v) {</a>
<a class="sourceLine" id="cb1452-2" data-line-number="2">  v.size =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(v<span class="op">^</span><span class="dv">2</span>))  <span class="co"># magnitude</span></a>
<a class="sourceLine" id="cb1452-3" data-line-number="3">  c.scale =<span class="st"> </span>(<span class="kw">t</span>(v) <span class="op">%*%</span><span class="st"> </span>p) <span class="op">/</span><span class="st"> </span>v.size<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1452-4" data-line-number="4">  <span class="kw">c</span>(c.scale) <span class="op">*</span><span class="st"> </span>v</a>
<a class="sourceLine" id="cb1452-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1452-6" data-line-number="6">p =<span class="st"> </span><span class="kw">c</span>(<span class="dv">4</span>,<span class="dv">2</span>); (<span class="dt">q =</span> <span class="kw">proj</span>(p, w))</a></code></pre></div>
<pre><code>## [1]  2.4 -1.2</code></pre>
<p>Using similar computation, we can obtain the projection of <strong>r</strong>, denoted as <strong>s</strong>, like so:</p>
<p><span class="math display">\[
\mathbf{s}_{(proj)} = \left[\begin{array}{r} -0.8\\ 0.4 \end{array}\right] 
\]</span></p>
<div class="sourceCode" id="cb1454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1454-1" data-line-number="1">r =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">4</span>); (<span class="dt">s =</span> <span class="kw">proj</span>(r, w))</a></code></pre></div>
<pre><code>## [1] -0.8  0.4</code></pre>
<p>Let us see the plot in Figure <a href="machinelearning2.html#fig:hyperplane1">10.20</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane1"></span>
<img src="DS_files/figure-html/hyperplane1-1.png" alt="Projections and Magnitude" width="70%" />
<p class="caption">
Figure 10.20: Projections and Magnitude
</p>
</div>
<p>The magnitude of the projection <strong>(q)</strong> of <strong>p</strong> and project <strong>(s)</strong> of <strong>r</strong> are:</p>
<p><span class="math display">\[\begin{align*}
\|\mathbf{q}\| = \sqrt{2.4^2 + (-1.2)^2} = 2.683282\ \ \ \ \ \ \ 
\|\mathbf{s}\| = \sqrt{(-0.8)^2 + (0.4)^2} = 0.8944272
\end{align*}\]</span></p>
<p>In other words, the orthogonal distance from point <strong>p</strong> to the hyperplane equals <span class="math inline">\(\|\mathbf{q}\|\)</span>. Similarly, the orthogonal distance from point <strong>r</strong> to the hyperplane equals <span class="math inline">\(\|\mathbf{s}\|\)</span>. Therefore, to note, the distance from any random point to the hyperplane can be computed based on its projection to <strong>w</strong>. Equivalently, <span class="math inline">\(\|\mathbf{q}\|\)</span> = <span class="math inline">\(\|\mathbf{p}\|\)</span> and <span class="math inline">\(\|\mathbf{s}\|\)</span> = <span class="math inline">\(\|\mathbf{r}\|\)</span>.</p>
<p>We can calculate the <strong>margin</strong> of each point across a hyperplane. For example:</p>
<p><span class="math display">\[\begin{align*}
\text{M}_{p} = 2 \| \mathbf{p}\| = 5.366563\ \ \ \ \ \
\text{M}_{r} = 2 \| \mathbf{r}\| = 1.788854\ \ \ \ \ \
 where: \text{M = margin}
\end{align*}\]</span></p>
<p>See Figure <a href="machinelearning2.html#fig:hyperplane2">10.21</a> for the plot of the margins.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hyperplane2"></span>
<img src="DS_files/figure-html/hyperplane2-1.png" alt="Margins and Hyperplanes" width="70%" />
<p class="caption">
Figure 10.21: Margins and Hyperplanes
</p>
</div>
<p>Given that we can calculate <strong>distance</strong> and <strong>margin</strong>, our next goal for <strong>LSVM</strong> is to find the optimal <strong>decision boundary</strong> - the <span class="math inline">\(H_0\)</span> hyperplane. It can be achieved based on the following steps:</p>
<p><strong>First</strong>, data points are classified based on the following formula:</p>
<p><span class="math display">\[\begin{align}
g(x) = \mathbf{w}^Tx + b
\end{align}\]</span></p>
<p><span class="math display">\[
where: g(x) = \begin{cases}
\ge +1 &amp; \text{(+) data points on or above the }H_{(+)} \\
0  &amp; \text{the }H_0 \text{ hyperplane} \\
\le-1 &amp; \text{(-) data points on or below the }H_{(-)}
\end{cases}
\]</span></p>
<p>Any data point classified as positive (+) respects the following equation:</p>
<p><span class="math display">\[\begin{align}
\mathbf{w}^Tx + b \ge +1
\end{align}\]</span></p>
<p>Any data point classified as negative (-) respects the following equation:</p>
<p><span class="math display">\[\begin{align}
\mathbf{w}^Tx + b \le -1
\end{align}\]</span></p>
<p>For mathematical convenience, we combine the two equations into one <strong>constraining</strong> equation by introducing an extra variable, namely <strong>y</strong> where <span class="math inline">\(y \in (-1,+1)\)</span> like so:</p>
<p><span class="math display">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1\ \ \  \equiv\ \ \  y_i (\mathbf{w}^T\mathbf{x_i} + b) -1 \ge 0
\end{align}\]</span></p>
<p>The equation above imposes a <strong>hard margin</strong> constraint such that it allows us to classify any <strong>x</strong> data points so that if <span class="math inline">\(\mathbf{w}^T\mathbf{x} + b\)</span> is positive and <span class="math inline">\(y = +1\)</span>, then <strong>x</strong> is correctly classified. Similarly if <span class="math inline">\(\mathbf{w}^T\mathbf{x} + b\)</span> is negative and <span class="math inline">\(y = -1\)</span>, then <strong>x</strong> is correctly classified.</p>
<p>However, there may be cases when <strong>x</strong> data points do not respect the equation above and thus may render misclassification, which happens when such data points fall between the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes. For example:</p>
<p><span class="math display">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  &lt; 1\ \ \ \ \leftarrow\ \ \ \text{mis-classified}
\end{align}\]</span></p>
<p>Such data points may cause the entire dataset not to be <strong>linearly separable</strong>. To mitigate the situation, we can relax the constraint by <strong>softening</strong> the margin allowing a few data points to <strong>cross over</strong> to the other side of the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> boundaries. For example, positive (+) data points can cross over to the other side of the <span class="math inline">\(H_{(+)}\)</span> hyperplane in the same way that negative (-) data points can cross over to the other side of the <span class="math inline">\(H_{(-)}\)</span> hyperplane. In doing so, we end up with a <strong>soft margin</strong> expressed like so:</p>
<p><span class="math display">\[\begin{align}
y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1 - \xi_i \ \ \  \equiv\ \ \  y_i (\mathbf{w}^T\mathbf{x} + b) -(1 - \xi_i) \ge 0
\end{align}\]</span></p>
<p>We introduce a <strong>slack variable</strong> denoted by the <strong>xi</strong> (<span class="math inline">\(\xi\)</span>) symbol. The <strong>slack variable</strong> serves as a measure of <strong>crossover error</strong> - not necessarily a <strong>misclassification error</strong> because, for example, positive (+) data points may still find themselves between the <strong>decision boundary</strong> and the <span class="math inline">\(H_{(+)}\)</span> boundary in that they are therefore still correctly classified. However, as data points get farther away from the borders and deeper to the other side, the error progresses to a <strong>misclassification error</strong>. Under such conditions, each data point that crosses over <strong>incurs some loss</strong> - calculated through <strong>Hinge Loss</strong>. We continue this discussion further in step four. </p>
<p><strong>Second</strong>, data points that are located along the <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes are called the <strong>support vectors</strong> because they <strong>support</strong> the <strong>decision boundary</strong> and they help to dictate the separability distance between two classes. The idea, therefore, is to identify those <strong>support vectors</strong>. To do that, we need to rely on distance measurement, which requires us to review three equations derived from the original equation, namely <span class="math inline">\(g(x) =\mathbf{w}^T\mathbf{x} + b\)</span>. The three equations are expressed as such:</p>
<p><span class="math display">\[\begin{align}
\underbrace{\mathbf{w}^Tx + b = +1}_{\text{equation for the }H_{(+)}\text{ hyperplane}}\ \ \ \ \ \ \ \ \ \ 
\underbrace{\mathbf{w}^Tx + b = -1}_{\text{equation for the }H_{(-)}\text{ hyperplane}}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\underbrace{\mathbf{w}^Tx + b = 0}_{\text{equation for the }H_{0}\text{ hyperplane}}
\end{align}\]</span></p>
<p>We have shown earlier how to project a vector to <strong>w</strong> and derive the magnitude of the vector. Alternatively, we also can use the following equation (based on <strong>Pythagoras</strong>) to compute the distance of any point to a hyperplane directly.</p>
<p><span class="math display">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|} \ \ \ \ \leftarrow \text{based on}\ \ \ \ \ \
\underbrace{\frac{|ax +bx + c|}{\sqrt{a^2 + b^2}}}_{\text{euclidean distance}}
\end{align}\]</span></p>
<p>For example, we can solve for the distance of both <span class="math inline">\(\|\mathbf{p}\|\)</span> and <span class="math inline">\(\|\mathbf{r}\|\)</span> like so:</p>
<div class="sourceCode" id="cb1456"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1456-1" data-line-number="1">Distance &lt;-<span class="st"> </span><span class="cf">function</span>(x, w, b) {</a>
<a class="sourceLine" id="cb1456-2" data-line-number="2">  numer =<span class="st"> </span><span class="kw">abs</span>( <span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>b )</a>
<a class="sourceLine" id="cb1456-3" data-line-number="3">  denom =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w<span class="op">^</span><span class="dv">2</span>))  </a>
<a class="sourceLine" id="cb1456-4" data-line-number="4">  numer <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb1456-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1456-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;dist(p)&quot;</span> =<span class="st"> </span><span class="kw">Distance</span>(p, w, b), <span class="st">&quot;dist(r)&quot;</span> =<span class="st"> </span><span class="kw">Distance</span>(r, w, b))</a></code></pre></div>
<pre><code>## dist(p) dist(r) 
##  2.6833  0.8944</code></pre>
<p>Now, using the same distance equation, we can determine the distance of both <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes to the <span class="math inline">\(H_0\)</span> hyperplane.</p>
<p>For <span class="math inline">\(H_{(+)}\)</span> hyperplane, we have the following distance from which our (+) support vectors are found:</p>
<p><span class="math display">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|}
= \frac{|+1|}{\|\mathbf{w}\|}
\ \ \ \ \ \leftarrow\ \ \ \ \underbrace{\mathbf{w}^Tx + b = +1}_{\text{equation for the }H_{(+)}\text{ hyperplane}}
\end{align}\]</span></p>
<p>For <span class="math inline">\(H_{(-)}\)</span> hyperplane, we have the following distance from which our (-) support vectors are found:</p>
<p><span class="math display">\[\begin{align}
D(x, w, b) = \frac{|\mathbf{w}^T\mathbf{x} + b|}{\|\mathbf{w}\|}
= \frac{|-1|}{\|\mathbf{w}\|}
\ \ \ \ \ \leftarrow\ \ \ \ \underbrace{\mathbf{w}^Tx + b = -1}_{\text{equation for the }H_{(-)}\text{ hyperplane}}
\end{align}\]</span></p>
<p>From here, it is easy to prove that the <strong>margin</strong> - the distance between <span class="math inline">\(H_{(+)}\)</span> and <span class="math inline">\(H_{(-)}\)</span> hyperplanes - of our hyperplane is equal to the following:</p>
<p><span class="math display">\[\begin{align}
\text{margin} = D_{H_{(+)}} + D_{H_{(-)}} = \frac{|+1|}{\|\mathbf{w}\|} +  \frac{|-1|}{\|\mathbf{w}\|} = \frac{2}{\|\mathbf{w}\|}
\end{align}\]</span></p>
<p>In other words, our (+) <strong>support vectors</strong> are a <strong>margin</strong> distant-away from (-) <strong>support vectors</strong>.</p>
<p>Alternatively, the width of the margin is calculated based on subtracting the closest opposing support vectors. See Figure <a href="machinelearning2.html#fig:optimizingsvm">10.22</a>.</p>
<p><span class="math display">\[\begin{align}
\text{width}_{\left(v_{(+)} - v_{(-)}\right)} = \frac{\mathbf{w}}{\|\mathbf{w}\|} \cdot \left(v_{(+)} - v_{(-)}\right) = \frac{2}{\|\mathbf{w}\|} 
\end{align}\]</span></p>
<p>For example:</p>
<div class="sourceCode" id="cb1458"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1458-1" data-line-number="1">width &lt;-<span class="st"> </span><span class="cf">function</span>(v.pos, v.neg, w) {</a>
<a class="sourceLine" id="cb1458-2" data-line-number="2">  u =<span class="st">  </span>w <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1458-3" data-line-number="3">  <span class="kw">abs</span>(<span class="kw">c</span>(u <span class="op">%*%</span><span class="st"> </span>( v.pos <span class="op">-</span><span class="st"> </span>v.neg )))</a>
<a class="sourceLine" id="cb1458-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1458-5" data-line-number="5">w.norm =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">-1</span>); v.pos =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>); v.neg =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1458-6" data-line-number="6"><span class="kw">all.equal</span>( <span class="kw">width</span>(v.pos, v.neg, w.norm),  <span class="dv">2</span> <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(w.norm<span class="op">^</span><span class="dv">2</span>)) )</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>
<p>Also, notice in the equation above that if we minimize <strong>w</strong>, the <strong>margin</strong> is effectively maximized. It is our optimization problem, expressed like so:</p>
<p><span class="math display">\[\begin{align}
\text{max}\ \frac{2}{\|\mathbf{w} \|} \equiv \text{min}\  \|\mathbf{w} \|
\end{align}\]</span></p>
<p><strong>Third</strong>, we need to find the optimal <strong>decision-boundary</strong>. To do that, all we need to do is perform the optimization equation below (with a caveat such that minimizing the <strong>w</strong> is subject to a constraint):</p>
<p><span class="math display">\[\begin{align}
\text{min}\  \|\mathbf{w}\|\ \ \ \ \leftarrow\ \ \ \ \ \ \ \text{subject to:}\ \ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1
\end{align}\]</span></p>
<p>For mathematical convenience, we can perform mathematical manipulation using other variants of the minimization term, keeping the proportionality intact.</p>
<p><span class="math display">\[\begin{align}
\text{min} \left\{\|\mathbf{w}\| \propto  \|\mathbf{w}\|^2 \propto  \frac{1}{2}\|\mathbf{w}\|^2 \right\} \equiv \text{min} \left\{ \frac{1}{2}\|\mathbf{w}\|^2 \right\}
\end{align}\]</span></p>
<p>Here, without losing proportionality, we transform the equation into a <strong>convex function</strong> expressed as the square of <span class="math inline">\(\|\mathbf{w}\|\)</span>, divided by 2. This transformation allows us an easy way to work with <strong>lagrangian multiplier</strong> optimization, which we will cover later.</p>
<p>Also, we can adjust the constraint to address <strong>scale variant</strong> concerns by dividing it by the <strong>w</strong> norm so that we have the following:</p>
<p><span class="math display">\[\begin{align}
y_i \left[D(x_i, w, b)\right] \ge 1
\ \ \ \ \ \leftarrow\ \ \ \ \ 
y_i \left(\frac{\mathbf{w}^T\mathbf{x_i} + b}{\|\mathbf{w}\|}\right)  \ge 1
\ \ \ \ \ \leftarrow\ \ \ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b)  \ge 1
\end{align}\]</span></p>
<p>With all that said, our minimization formula for a perfectly separable dataset becomes:</p>
<p><span class="math display">\[\begin{align}
 \text{arg}\ \underset{\mathbf{w},b}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2 \right\}\ \ \ \ \mathbf{s.t.}\ \ \ y_i 
 \left[D(x_i, w, b)\right] \ge 1,\ \ \ \ \ where\ \forall i = 1,..,n.
\end{align}\]</span></p>
<p>Note that the distance formula depends upon the values of <strong>w</strong> and <strong>b</strong> - they are the parameters of our <span class="math inline">\(H_0\)</span> hyperplane. As long as we know that the data points are <strong>linearly separable</strong>, we can generate a list of arbitrary <span class="math inline">\(H_0\)</span> hyperplanes described by their corresponding set of <strong>w</strong> and <strong>b</strong> values. However, only one of the <span class="math inline">\(H_0\)</span> hyperplanes can be a qualified candidate based upon the <strong>margin</strong> that ensures the maximum separability of the (+) and (-) support vectors.</p>
<p>Here, by minimizing the value of <strong>w</strong> (and <strong>b</strong>), we maximize the <strong>margin</strong>.</p>
<p>See Figure <a href="machinelearning2.html#fig:optimizingsvm">10.22</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:optimizingsvm"></span>
<img src="optimizingsvm.png" alt="Optimal Hyperplane" width="60%" />
<p class="caption">
Figure 10.22: Optimal Hyperplane
</p>
</div>
<p><strong>Fourth</strong>, recall in step one that if our dataset is not <strong>linearly separable</strong>, perhaps we can relax the <strong>hard margin</strong> a bit; instead, we can impose a <strong>softer margin</strong> to permit data points to cross over the <strong>decision boundary</strong>. Consequently, however, by crossing over to the other side, the data points are forced to <strong>incur</strong> some loss, represented by a <strong>slack variable</strong> (<span class="math inline">\(\xi\)</span>).</p>
<p>Now, a <strong>slack variable</strong> (<span class="math inline">\(\xi_i\)</span>) acts as a <strong>loss function</strong> - specifically, the <strong>Hinge Loss</strong> function - that allows a <strong>cross-over</strong> violation such that the deeper the cross-over to the wrong side (meaning, the larger the violation), the more significant the loss.</p>
<p>For a good intuition, it helps to show two <strong>loss functions</strong>, namely <strong>logistic loss</strong> function and <strong>hinge loss</strong> function. See Figure <a href="machinelearning2.html#fig:hingeloss">10.23</a>. </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:hingeloss"></span>
<img src="hingeloss.png" alt="Loss Functions" width="90%" />
<p class="caption">
Figure 10.23: Loss Functions
</p>
</div>
<p>Here, we use <strong>Hinge Loss</strong> for <strong>SVM</strong>, expressed bellow, to measure the amount of <strong>loss</strong> for data points.</p>
<p><span class="math display">\[\begin{align}
\xi_i = 
\underbrace{\text{max} \left\{0, 1 - y_i (\mathbf{w}^T\mathbf{x_i} + b) \right\}}_{\text{hinge loss function}}
\ \ \ \ \text{based on}\ \ \ y_i (\mathbf{w}^T\mathbf{x_i} + b) &lt; 1\ \ \leftarrow\  \text{(mis-classified)}
\end{align}\]</span></p>
<p>A simple intuition behind this is to equate (<span class="math inline">\(\mathbf{w}^T\mathbf{x_i} + b\)</span>) with a y-hat (<span class="math inline">\(\mathbf{\hat{y}}\)</span>) outcome such that we can then formulate the outcome this way:</p>
<p><span class="math display">\[\begin{align}
y_i (\hat{y_i}) = 
\begin{cases}
\ge +1 &amp; \text{if}\ (y_i = +1, \hat{y_i} \ge +1) \ or\ (y_i=-1,\hat{y_i} \le -1) \\
\le -1 &amp; \text{if}\ (y_i = -1, \hat{y_i} \ge +1) \ or\ (y_i=+1,\hat{y_i} \le -1) \\
\end{cases} \label{eqn:eqnnumber408}
\end{align}\]</span></p>
<p>Therefore:</p>
<p><span class="math display">\[\begin{align}
\xi_i = 
\text{max} \left\{0, 1 - y_i (\hat{y}_i) \right\} 
\begin{cases}
=0 &amp; \text{max} \left\{0, 1 - (\ge+1) \right\}\ \ \leftarrow \text{(zero loss)}\\
&gt;0 &amp; \text{max} \left\{0, 1 - (\le-1) \right\}\ \ \leftarrow \text{(with loss)}\\
\end{cases} \label{eqn:eqnnumber409}
\end{align}\]</span></p>
<p>We now go back to our optimization equation and plug in the <strong>slack variable</strong> (<span class="math inline">\(\xi_i\)</span>), which gets minimized also. See below:</p>
<p><span class="math display">\[\begin{align}
\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2  +  \sum_{i=1}^n\xi_i^p\right\}
\ \ \ \mathbf{\text{s.t}}\ \ \text{const.}
\begin{cases}
y_i\left[D(x_i, w, b)\right] \ge 1 -  \xi_i\\
\xi_i \ge 0
\end{cases}
\ \ \ \ \forall i = 1,..,n \label{eqn:eqnnumber410}
\end{align}\]</span></p>
<p>Notice the inclusion of an exponent <strong>p</strong> to the <strong>Hinge Loss</strong>. It takes care of outliers by increasing the <strong>loss</strong> exponentially, thus pushing them further away from the border.</p>
<p>Now, because we use a <strong>softer margin</strong>, in the process of maximizing the <strong>margin</strong>, there may be chances that more data points may forcibly get trapped between the borders, accumulating a larger number of misclassified data points than necessary. A way to avoid that is to have a <strong>trade-off</strong> between how much misclassification we can take versus how large our margin should be. We apply a <strong>cost</strong> as a penalty represented by <strong>C</strong> or by the <strong>lambda</strong> <span class="math inline">\(\lambda\)</span> parameter to regulate the trade-off. We have two choices like so.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{1}{2}\|\mathbf{w}\|^2  +  C\sum_{i=1}^n\xi_i^p\right\}}_{\text{1st choice}}
\ \ \ \ \ \ \ \ \ \ \ \ \ 
\underbrace{\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \frac{\lambda}{2}\|\mathbf{w}\|^2  +  \sum_{i=1}^n\xi_i^p\right\}}_{\text{2nd choice}}
\end{align}\]</span></p>
<p>Here, we choose to add <strong>lambda</strong> (<span class="math inline">\(\lambda\)</span>) to our equation, still subject to the aforementioned constraints.</p>
<p>The final equation for our <strong>objective (cost) function</strong> below is what we call a solution to the <strong>primal optimization problem</strong>. By <strong>primal</strong>, it means that the original variables (e.g. <strong>w</strong> and <strong>b</strong>) continue to be used for optimization. Also, recall <strong>Ridge Regression</strong> under the <strong>General Modeling</strong> section in relation to the equation below:</p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\mathbf{w}) = 
\text{arg}\ \underset{\mathbf{w},b, \xi_i}{\text{min}} \left\{ \underbrace{\frac{\lambda}{2}\|\mathbf{w}\|^2}_{
\begin{array}{l}
    \text{ridge-like} \\
    \text{regularizer}
\end{array}
    }  +
\underbrace{\sum_{i=1}^n
\overbrace{\text{max} \left\{0, 1 - y_i (\mathbf{w}^T\mathbf{x_i} + b) \right\}^p}^{
\text{hinge }(\xi_i)}}_{\text{loss}}
\right\} \label{eqn:eqnnumber411}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\ \ \ \mathbf{\text{subject to }}\ \ 
\begin{cases}
y_i\left[D(x_i, w, b)\right] \ge 1 -  \xi_i\\
\xi_i \ge 0
\end{cases}
\ \ \ \ \forall i = 1,..,n \label{eqn:eqnnumber412}
\end{align}\]</span></p>
<p>On the other hand, we can rewrite the same problem into another form called the <strong>dual optimization formulation</strong>. We use such <strong>formulation</strong> in line with the optimization method using <strong>Lagrange multiplier</strong> (See Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). This topic will be more discussed when dealing with <strong>kernel non-linear SVM</strong>.</p>
<p><strong>Fifth</strong>, for <strong>primal optimization problem</strong>, we can use <strong>Gradient Descent</strong> or <strong>Stochastic Gradient Descent (SGD)</strong>, or both. Given the <strong>non-differentiable</strong> nature of the <strong>Hinge Loss</strong>, we need to use <strong>sub-gradient</strong> instead. We use one called <strong>PEGASOS</strong>,, an acronym for <strong>Primal Estimated sub-GrAdient SOlver for SVM</strong>. </p>
<p>For the <strong>update rule</strong> using the <strong>PEGASOS</strong> algorithm, we have the following:</p>

<p><span class="math display">\[\begin{align}
\mathbf{w}^{(t+1)} &amp;= \mathbf{w}^{(t)} - \eta^{(t)} \nabla_{\mathbf{w}}  \mathcal{J}(\mathbf{w}^{(t)}) \\
&amp;= (1  - \eta^{(t)} \lambda)\mathbf{w}^{(t)} + \eta^{(t)}\ \mathbf{1}[\ y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i]\ y^{(t)}_i x^{(t)}_i
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align*}
\text{where: }\\
\ \ \ \eta \text{ is learning rate }\\
\ \ \ \lambda \text{ is a regularization parameter }\\
\end{align*}\]</span>
</p>
<p>with an <strong>indicator function</strong> being:</p>

<p><span class="math display">\[\begin{align}
\mathbf{1}[\ y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i]= 
\begin{cases}
1 &amp;  y^{(t)}_i \mathbf{w}^T \mathbf{x}^{(t)}_i &lt; 1\\
0   &amp; \text{otherwise}
\end{cases} \label{eqn:eqnnumber413}
\end{align}\]</span>
</p>
<p>Here, we take the <strong>gradient</strong> (<span class="math inline">\(\nabla\)</span>) of the cost function <span class="math inline">\(\mathcal{J}(\mathbf{w})\)</span> with respect to <strong>w</strong>. Then we multiply the gradient by the learning rate (<span class="math inline">\(\eta\)</span>). We then update <span class="math inline">\(\mathbf{w}\)</span> and repeat the process until convergence. The algorithm eventually convergences because our objective function is <strong>convex</strong>.</p>
<p>Below are two juxtaposed algorithms for review. One is with a generic gradient descent algorithm, and the other is the <strong>PEGASOS</strong> algorithm introduced by Shalev-Shwartz S. et al. <span class="citation">(<a href="bibliography.html#ref-ref694s">2007</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{lll}
\mathbf{\text{Gradient Descent Algorithm}} \\
\\
\text{Initialize } \mathbf{w}_0\ \text{ and } \eta \\
\text{loop}\ t\ in\ 1:\ \text{T} \\
\ \ \ \text{Calculate}\ \mathbf{gradient } (\nabla_\mathbf{w})\\
\ \ \ \ \ \ \ of\ \mathcal{J}(\mathbf{w}^{(t)}) \\
\ \ \ \text{Update}\ \mathbf{w}\\
\ \ \ \ \ \ \  e.g.\\
\ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} - \eta \nabla_{\mathbf{w}} \mathcal{J}(\mathbf{w}^{(t)})\\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T)} \\ {} \\ {} \\ {} 
\end{array} 
\left|
\begin{array}{ll}
\mathbf{\text{PEGASOS Algorithm}} \\
\text{(Shai Shalev-Shwartz et al, 2007)}\\
\\
\text{Input:}\ \ S, \lambda, \text{T} \\
\text{Set}\ \mathbf{w}_1 = 0\\
\text{loop}\ t\ in\ 1,2,...,\text{T}  \\
\ \ \ \ \text{Choose}\ i^{(t)}\ \in \{1,...,|S|\}\ \ \text{(unif. random)} \\  
\ \ \ \ \text{Set}\ \eta^{(t)} = \frac{1}{t\lambda}\\
\ \ \ \ \text{if}\ \ y_{i_t} (\mathbf{w}^T x_i^{(t)}) &lt; 1\ \text{then} \\
\ \ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = (1 - \eta^{(t)} \lambda) \mathbf{w}^{(t)} + \eta^{(t)} y_i^{(t)} x_i^{(t)}\\
\ \ \ \ \text{else}\\
\ \ \ \ \ \ \ \ \mathbf{w}^{(t+1)} = (1 - \eta^{(t)} \lambda) \mathbf{w}^{(t)} \\
\ \ \ \ \text{end if}\\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T+1)} 
\end{array}
\right.
\]</span>
</p>
<p><strong>PEGASOS</strong> performs <strong>sub-gradient descent stochastically</strong> as it randomly selects an observation from the dataset. Here, we use <strong>sample.int(.)</strong> to simulate random sampling of the index (<span class="math inline">\(i_t\)</span>).</p>
<p>Below is an example implementation of <strong>PEGASOS</strong> for <strong>SVM</strong>:</p>

<div class="sourceCode" id="cb1460"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1460-1" data-line-number="1">my.pegasos.svm &lt;-<span class="st"> </span><span class="cf">function</span>(data, <span class="dt">lambda=</span><span class="fl">0.01</span>, <span class="dt">limit=</span><span class="dv">15000</span>) {</a>
<a class="sourceLine" id="cb1460-2" data-line-number="2">  x   =<span class="st"> </span><span class="kw">cbind</span>(<span class="dv">1</span>, data[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>)])  </a>
<a class="sourceLine" id="cb1460-3" data-line-number="3">  y   =<span class="st"> </span>data[,<span class="kw">c</span>(<span class="dv">3</span>)]</a>
<a class="sourceLine" id="cb1460-4" data-line-number="4">  n   =<span class="st"> </span><span class="kw">nrow</span>(x); p =<span class="st"> </span><span class="kw">ncol</span>(x)</a>
<a class="sourceLine" id="cb1460-5" data-line-number="5">  w   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p) </a>
<a class="sourceLine" id="cb1460-6" data-line-number="6">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb1460-7" data-line-number="7">      i  =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1460-8" data-line-number="8">      xi =<span class="st"> </span>x[i,]; yi =<span class="st"> </span>y[i]</a>
<a class="sourceLine" id="cb1460-9" data-line-number="9">      eta =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(t <span class="op">*</span><span class="st"> </span>lambda)      <span class="co"># learning rate</span></a>
<a class="sourceLine" id="cb1460-10" data-line-number="10">      <span class="cf">if</span> ( yi <span class="op">*</span><span class="st"> </span>(<span class="kw">t</span>(w) <span class="op">%*%</span><span class="st"> </span>xi) <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span>) { <span class="co"># s.t. (1 - yi * (t(w) %*% xi) &gt;= 0)</span></a>
<a class="sourceLine" id="cb1460-11" data-line-number="11">        w =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>w  <span class="op">+</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span>xi</a>
<a class="sourceLine" id="cb1460-12" data-line-number="12">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1460-13" data-line-number="13">        w =<span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>eta <span class="op">*</span><span class="st"> </span>lambda) <span class="op">*</span><span class="st"> </span>w </a>
<a class="sourceLine" id="cb1460-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb1460-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1460-16" data-line-number="16">  w  </a>
<a class="sourceLine" id="cb1460-17" data-line-number="17">}</a></code></pre></div>

<p>With the optimized <strong>w</strong> coefficients (or weights), we can implement the hyperplanes like so:</p>

<div class="sourceCode" id="cb1461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1461-1" data-line-number="1">svm.hyperplanes &lt;-<span class="st"> </span><span class="cf">function</span>(svm.model) {</a>
<a class="sourceLine" id="cb1461-2" data-line-number="2"> b      =<span class="st">  </span>svm.model[<span class="dv">1</span>]; rise =<span class="st"> </span>svm.model[<span class="dv">2</span>]; run =<span class="st"> </span>svm.model[<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb1461-3" data-line-number="3"> H0.int =<span class="st">  </span><span class="op">-</span>b <span class="op">/</span><span class="st"> </span>run    </a>
<a class="sourceLine" id="cb1461-4" data-line-number="4"> Hp.int =<span class="st">  </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>b) <span class="op">/</span><span class="st"> </span>run </a>
<a class="sourceLine" id="cb1461-5" data-line-number="5"> Hn.int =<span class="st"> </span><span class="op">-</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>b) <span class="op">/</span><span class="st"> </span>run</a>
<a class="sourceLine" id="cb1461-6" data-line-number="6"> <span class="kw">list</span>(<span class="st">&quot;slope&quot;</span> =<span class="st"> </span><span class="op">-</span>rise  <span class="op">/</span><span class="st"> </span>run, </a>
<a class="sourceLine" id="cb1461-7" data-line-number="7">      <span class="st">&quot;H0.int&quot;</span> =<span class="st"> </span>H0.int, <span class="st">&quot;Hp.int&quot;</span> =<span class="st"> </span>Hp.int, <span class="st">&quot;Hn.int&quot;</span> =<span class="st"> </span>Hn.int)</a>
<a class="sourceLine" id="cb1461-8" data-line-number="8">}</a></code></pre></div>

<p>Let us use the <strong>implementation</strong> to learn the classifier. Below is a sample dataset to use:</p>

<div class="sourceCode" id="cb1462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1462-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1462-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">20</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1462-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1462-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1462-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1462-6" data-line-number="6">y         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2))</a>
<a class="sourceLine" id="cb1462-7" data-line-number="7">data      =<span class="st"> </span><span class="kw">cbind</span>(x, y)</a>
<a class="sourceLine" id="cb1462-8" data-line-number="8">svm.pegasos.model =<span class="st"> </span><span class="kw">my.pegasos.svm</span>(data) </a>
<a class="sourceLine" id="cb1462-9" data-line-number="9">hplanes   =<span class="st"> </span><span class="kw">svm.hyperplanes</span>(svm.pegasos.model)</a></code></pre></div>

<p>We now plot and see Figure <a href="machinelearning2.html#fig:gdsvm">10.24</a>.</p>
<div class="sourceCode" id="cb1463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1463-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1463-2" data-line-number="2">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span><span class="st">&quot;LSVM (Sub-Gradient Descent)&quot;</span>)</a>
<a class="sourceLine" id="cb1463-3" data-line-number="3"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1463-4" data-line-number="4"><span class="kw">points</span>(x, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1463-5" data-line-number="5"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>H0.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">1</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1463-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hp.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>)</a>
<a class="sourceLine" id="cb1463-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">a=</span>hplanes<span class="op">$</span>Hn.int, <span class="dt">b=</span>hplanes<span class="op">$</span>slope, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gdsvm"></span>
<img src="DS_files/figure-html/gdsvm-1.png" alt="LSVM (Sub-Gradient Descent)" width="70%" />
<p class="caption">
Figure 10.24: LSVM (Sub-Gradient Descent)
</p>
</div>
<p><strong>Sixth</strong>, for prediction, we can use the model to classify any new or missing data.</p>
<p><span class="math display">\[\begin{align}
h(x; \mathbf{w}) = \text{sign}(g(x)) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)
\ \ \ \ \leftarrow\ \ \ \
\begin{cases}
+1 &amp; \mathbf{w}^T\mathbf{x} \ge 0\\ 
-1 &amp; \mathbf{w}^T\mathbf{x} &lt;0
\end{cases} \label{eqn:eqnnumber414}
\end{align}\]</span></p>
<p>Below is an example of predicting the class of a given set of data.</p>

<div class="sourceCode" id="cb1464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1464-1" data-line-number="1">x.new =<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st"> </span><span class="dv">-2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st">  </span><span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1464-2" data-line-number="2">               <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st">  </span><span class="dv">2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st"> </span><span class="dv">-2</span>))</a>
<a class="sourceLine" id="cb1464-3" data-line-number="3">my.svm.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(w, x) {</a>
<a class="sourceLine" id="cb1464-4" data-line-number="4">    <span class="kw">c</span>(<span class="kw">sign</span>(w <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)))</a>
<a class="sourceLine" id="cb1464-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1464-6" data-line-number="6">h =<span class="st"> </span><span class="kw">my.svm.prediction</span>(svm.pegasos.model, x.new)</a>
<a class="sourceLine" id="cb1464-7" data-line-number="7">pred =<span class="st"> </span><span class="kw">ifelse</span>(h <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>)</a>
<a class="sourceLine" id="cb1464-8" data-line-number="8"><span class="kw">print</span>(<span class="kw">cbind</span>(x.new, pred), <span class="dt">quote=</span><span class="ot">FALSE</span>, <span class="dt">right=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##      intercept x1 x2     pred
## [1,]         1 -2  2 positive
## [2,]         1  2 -2 negative</code></pre>

<p><strong>Finally</strong>, to evaluate performance, we can also use <strong>Chi-squared test</strong> in the same way we did with <strong>Poisson Regression</strong>. See the previous section for the method of evaluation.</p>
</div>
<div id="kernel-svm-smo" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.2</span> Kernel SVM (SMO)  <a href="machinelearning2.html#kernel-svm-smo" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>This section (<strong>SMO-based SVM</strong>) and the next section (<strong>SDCA-based SVM</strong>) may require an understanding of the <strong>theory of duality</strong> starting with <strong>simplex method</strong>, <strong>dual simplex</strong>, and <strong>primal and dual formulations</strong> from Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under Subsections <strong>Simplex Method</strong>, <strong>Dual Method</strong>, and <strong>Primal Dual</strong> respectively under <strong>Polynomial Regression</strong> Section.</p>
<p>In this section, we take the case in which our <strong>optimization problem</strong> is written into a different formulation - the <strong>dual optimization formulation</strong>. Here, we cover two complementing <strong>algorithms</strong> to solve for <strong>SVM</strong>, namely <strong>Lagrangian Multiplier method</strong> and <strong>Sequential Minimal Optimization (SMO)</strong>.</p>
<p><strong>First</strong>, we begin our discussion by introducing the <strong>Lagrangian duality</strong>, a variant of the <strong>Fenchel duality</strong> for optimization problems.  </p>
<p>Based on <strong>Duality Theory</strong> and <strong>Lagrange Multiplier</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>), we obtain the following equation: </p>
<p><span class="math display">\[\begin{align}
Lik(X, \alpha) = f(X) - \sum_{i=1}^n \alpha_i \times (g_i(X) - c_i)
\end{align}\]</span></p>
<p>Now, if only the dataset is perfectly separable with a <strong>hard margin</strong>, then our equation from the <strong>primal formulation</strong> in the previous section equates to the following <strong>primal Lagrangian formulation</strong>:</p>
<p><span class="math display">\[\begin{align}
Lik(w, b, \alpha_i) =
\underbrace{\frac{1}{2} \|\mathbf{w}\|^2}_{f(\mathbf{w},b)} 
- \sum_{i=1}^n \alpha_i 
\underbrace{\left(y_i \left( \mathbf{w}^T\mathbf{\vec{x}_i} + b\right) - 1\right)}_{g(w,b) - c_i}
\end{align}\]</span></p>
<p><strong>Second</strong>, if we then relax the <strong>hard margin</strong> by using a penalty (<span class="math inline">\(\xi_i\)</span>) where <strong>p=1</strong>, then our <strong>primal Lagrangian formulation</strong> becomes:</p>

<p><span class="math display">\[\begin{align}
Lik(w, b, \xi_i, \alpha_i) = \left[\frac{1}{2}\|\mathbf{w}\|^2 + C \sum_{i=1}^n \xi_i \right] - \sum_{i=1}^n\left[ \alpha_i \left\{  y_i \left( \mathbf{w}^T\mathbf{\vec{x}_i} + b\right) - (1 - \xi_i )\right\} \right] - \sum_{i=1}^n \alpha_i \left( \xi_i \right) 
\end{align}\]</span>
</p>
<p>Here, we use <strong>C</strong> as our regularization parameter. In this formulation, the <strong>alpha</strong> (<span class="math inline">\(\alpha\)</span>) symbol represents a set of <strong>Lagrange multipliers</strong>.</p>
<p>We can then take the <strong>partial derivative</strong> of the function with respect to <strong>w</strong>, <strong>b</strong>, and <span class="math inline">\(\xi_i\)</span> respectively:</p>
<p><span class="math display">\[\begin{align}
{}&amp;\nabla_w Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial \mathbf{w}} = \mathbf{w} -  \sum_{i=1}^n \alpha_i  y_i \mathbf{\vec{x}_i}   = 0
\ \ \ \ \rightarrow\ \ \ \ \ \mathbf{w} = \sum_{i=1}^n \alpha_i y_i \mathbf{\vec{x}_i} \\
\nonumber \\
&amp;\nabla_b Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial b} = - \sum_{i=1}^n \alpha_i  y_i = 0\ \ \ \ \rightarrow\ \ \ \ \ \sum_{i=1}^n \alpha_i  y_i =  0
\nonumber \\
&amp;\nabla_{\xi_i} Lik(w, b, \xi_i, \alpha_i) = \frac{\partial L}{\partial \xi_i} = C - \alpha_i = 0\ \ \ \ \rightarrow\ \ \ \ \ C  =  \alpha_i
\end{align}\]</span></p>
<p>By substituting the derived <strong>w</strong> to the <strong>Lagrangian formulation</strong> (dropping constants) where <strong>w</strong>=<span class="math inline">\(\left(\sum_{i=1}^n \alpha_i y_i \mathbf{x}_i\right)\)</span>, we get the following:</p>
<p><span class="math display">\[\begin{align}
Lik(w, b, \alpha_i) 
{}&amp;= \frac{1}{2} \left(\sum_{i=1} \alpha_i y_i \mathbf{\vec{x}_i}\right) \cdot \left(\sum_{j=1} \alpha_j y_j \mathbf{\vec{x}_j}\right) \\
&amp;- \left(\sum_{i=1} \alpha_i y_i \mathbf{\vec{x}_i}\right) \cdot \left(\sum_{j=1} \alpha_j y_j \mathbf{\vec{x}_j}\right) - \sum \alpha_i y_i b + \sum \alpha_i.
\end{align}\]</span></p>
<p>where <span class="math inline">\(\sum \alpha_i y_i b = 0\)</span></p>
<p>Simplifying the equation, we end up with the <strong>Dual Lagrangian formulation</strong>. By <strong>Dual</strong>, we disregard the use of the <strong>original variables</strong> (e.g., <strong>w</strong> and <strong>b</strong>) as minimized in the <strong>primal optimization formulation</strong> and use a <strong>dot product of two vectors</strong> for optimization instead in which we have the following:</p>
<p><span class="math display">\[\begin{align}
Lik(w, b, \alpha_i)  = \sum \alpha_i - \frac{1}{2} \sum_{i=1} \sum_{j=1} \alpha_i \alpha_j y_i y_j (\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}) 
\end{align}\]</span></p>
<p>conditioned on the following <strong>Karush-Khun-Tucker (KKT)</strong> constraints where <strong>p=1</strong>:  </p>
<p><span class="math display">\[\begin{align}
\text{* Optimality (Stationarity) } &amp; \nabla_w Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n\\
 &amp; \nabla_b Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n\\
  &amp; \nabla_{\xi_i} Lik(w, b, \xi_i, \alpha_i)  = 0, &amp; i=1,...,n\\
\text{* Primal Feasibility } &amp; y_i(\mathbf{w}^T \mathbf{\vec{x}_i} + b)  \ge 1,  &amp; i=1,...,n \\
  &amp;  \xi_i \ge 0, &amp; i=1,...,n \\
\text{* Dual Feasibility } &amp; \alpha_i \ge 0, &amp; i=1,...,n \\
  &amp;  C - \alpha_i \ge 0, &amp; i=1,...,n \\
\text{* Complementary Slackness } &amp; \alpha_i\left(y_i(\mathbf{w}^T \mathbf{\vec{x}_i} + b) - 1 + \xi_i\right) = 0, &amp; i=1,...,n \\
  &amp;  (C - \alpha_i)\ \xi_i = 0, &amp; i=1,...,n 
\end{align}\]</span></p>
<p><strong>Third</strong>, with all that, the final equation for our <strong>objective (cost) function</strong> below is what we call solution to the <strong>dual optimization problem</strong>. The <strong>dual formulation</strong> tells us to maximize the <strong>dot product of the vectors</strong> instead, namely <span class="math inline">\((\mathbf{x_i} \cdot \mathbf{x_j})\)</span>: </p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align*}
\ \ \mathbf{subject\ to }\ \
\begin{cases}
0 \le \alpha_i \le C \\
\sum_{i=1}^n \alpha_i y_i = 0
\end{cases}
,\ \forall i=1,...,n
\end{align*}\]</span></p>
<p>For classification, we can use the below <strong>prediction function</strong>.</p>
<p><span class="math display">\[\begin{align}
h\left(\mathbf{x}\right) = 
\underbrace{\sum_{i=1}^n \alpha_i y_i \left(\mathbf{x}  \cdot \mathbf{\vec{x}_i}  \right) + b}_{\text{dual}}
\ \ \ \ \leftarrow\ \ \ \ \ \
\underbrace{\text{sign}(\mathbf{w}^T \mathbf{x} + b)}_{\text{primal }}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, we get to the point in which we modify the objective function and prediction function such that they support <strong>Kernel functions</strong>. The transformation of the equation is indeed straightforward. We pass the dot-product of the two vectors to a <strong>Kernel function</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \ \mathbf{K}\left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
h\left(\mathbf{x}\right)  = 
\underbrace{\text{sign}\left(\sum_{i=1}^n \alpha_i y_i\ \mathbf{K}\left( \mathbf{x} \cdot \mathbf{\vec{x}_i}   \right) + b\right)}_{\text{dual}}
\ \ \ \ \leftarrow\ \ \ \ \ \
\underbrace{\text{sign}(\mathbf{w}^T \mathbf{K}(\mathbf{x}) + b)}_{\text{primal }}
\end{align}\]</span></p>
<p>The choice of <strong>Kernel function</strong> is based on the nature of the dataset. If the data follows some linear characteristics, perhaps a <strong>Linear Kernel</strong> is appropriate. Below are three <strong>Kernels</strong> used for <strong>SVM</strong>:</p>
<p><strong>Linear Kernel</strong> - if data linearly demonstrates separability, then we use the kernel below: </p>
<p><span class="math display">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j} \right)  
\end{align}\]</span></p>
<p><strong>Polynomimal Kernel</strong> - if data demonstrates separability in a polynomial fashion (given <strong>d</strong> degrees), then we use the kernel below: </p>
<p><span class="math display">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j} + c \right)^d
\ \ \ \ \ \text{where }\ \mathbf{c}\ \text{is constant}
\end{align}\]</span></p>
<p><strong>Radial Basis Kernel (RBF)</strong> - if data demonstrates separability in a gaussian fashion, then we can use the kernel below:  </p>
<p><span class="math display">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\|^2 \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{2\sigma^2}\ and\ \sigma\ \text{is variance}
\end{align}\]</span></p>
<p><strong>Laplacian Kernel</strong> - this kernel is a variant of <strong>RBF</strong> kernel. </p>
<p><span class="math display">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\| \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{\sigma}\ and\ \sigma\ \text{is variance}
\end{align}\]</span></p>
<p><strong>Exponential Kernel</strong> - this kernel is another variant of <strong>RBF</strong> kernel. </p>
<p><span class="math display">\[\begin{align}
\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j}\right) = exp\left( - \gamma \| \mathbf{\vec{x}_i} - \mathbf{\vec{x}_j}\| \right)
\ \ \ \ \ where\ \ \gamma = \frac{1}{2\sigma^2}\ and\ \sigma\ \text{is variance}
\end{align}\]</span></p>
<p><strong>Fifth</strong>, we now come down to the <strong>algorithm</strong>. Here, let us discuss an algorithm called <strong>Sequential Minimal Optimization (SMO)</strong> introduced by John C. Platt <span class="citation">(<a href="bibliography.html#ref-ref718j">1998</a>)</span>. A simplified variant of <strong>SMO</strong> is found in a Stanford CS229 course <span class="citation">(<a href="bibliography.html#ref-ref729s">2009</a>)</span>.</p>
<p>We start with the objective function <span class="math inline">\(\mathcal{J}(\lambda)\)</span> in the <strong>fourth</strong> step and the two constraints in the <strong>third</strong> step. The intent is to solve for the <strong>lagrangian multipliers</strong>, e.g. (<span class="math inline">\(\alpha_i, \alpha_j)\)</span>, so that given <span class="math inline">\(\alpha = (\alpha_1, \alpha_2, \alpha_3,\ ...\ \alpha_n)\)</span>, the <strong>objective function</strong> allows us to solve only one pair of elements at a time; meaning, the goal is to heuristically pick an arbitrary pair of <span class="math inline">\(\alpha\)</span> elements, e.g. <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\alpha_j\)</span>, such that in solving for their values using <strong>SMO</strong>, we respect the following <strong>KKT</strong> constraints:</p>
<p><span class="math display">\[\begin{align}
\alpha_i = 0\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) \ge 1 \\
0 &lt; \alpha_i &lt; C\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) = 1 \\
\alpha_i = C\ \ \ \ &amp;\rightarrow y_i f(\mathbf{\vec{x}_i}) \le 1 
\end{align}\]</span></p>
<p>To illustrate, we use Figure <a href="machinelearning2.html#fig:smo">10.25</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo"></span>
<img src="smo.png" alt="Dual Constraint" width="90%" />
<p class="caption">
Figure 10.25: Dual Constraint
</p>
</div>
<p>The first constraint (inequality constraint), namely <span class="math inline">\(0 \le \alpha_i \le C\)</span> and <span class="math inline">\(0 \le \alpha_j \le C\)</span>, bounds the diagonal lines within the boxes shown in the figure. The second constraint (linear equality constraint), namely <span class="math inline">\(\sum_{i=1}^n \alpha_i y_i = 0\)</span>, limits us along the diagonal lines in the figure.</p>
<p>To enforce the first constraint, we start with the following expression:</p>
<p><span class="math display">\[\begin{align}
\gamma = \alpha_i + s\ \alpha_j
\end{align}\]</span></p>
<p>Here, let <span class="math inline">\(s = y_i y_j\)</span> and let the labels <span class="math inline">\(y_i\)</span> and <span class="math inline">\(y_j\)</span> to have values between (-1,1) respectively. Therefore, <span class="math inline">\(s = -1\)</span> if <span class="math inline">\(y_i \ne y_j\)</span> and <span class="math inline">\(s = +1\)</span> if <span class="math inline">\(y_i = y_j\)</span>. So that:</p>
<p><span class="math display">\[\begin{align}
\text{if}\ s = -1 \text{ and }  \gamma &gt; 0, &amp;\text{then} &amp;min(\alpha_j) = 0, &amp;max(\alpha_j) = C - \gamma \\
\text{if}\ s = -1 \text{ and }  \gamma &lt; 0, &amp;\text{then} &amp;min(\alpha_j) = -\gamma, &amp;max(\alpha_j) = C\\
\text{if}\ s = +1 \text{ and } \gamma &lt; C, &amp;\text{then} &amp;min(\alpha_j) = 0, &amp;max(\alpha_j) = \gamma\\
\text{if}\ s = +1 \text{ and }  \gamma &gt; C, &amp;\text{then} &amp;min(\alpha_j) = \gamma - C, &amp;max(\alpha_j) = C
\end{align}\]</span></p>
<p>Combining the conditions above, we obtain the following upper (H) and lower (L) bounds:</p>
<p><span class="math display">\[\begin{align}
If\ y_i \ne y_j,&amp; L = max(0, \alpha_j^{(t)} - \alpha_i^{(t)}),&amp; H=min(C, C+ \alpha_j^{(t)} - \alpha_1^{(t)})\\
If\ y_i = y_j,&amp; L = max(0, \alpha_i^{(t)} + \alpha_j^{(t)} -C),&amp; H=min(C, \alpha_i^{(t)} + \alpha_j^{(t)})
\end{align}\]</span></p>
<p>To enforce the second constraint along the diagonal line, we update the second alpha (<span class="math inline">\(\alpha_j\)</span>) using the following formula:</p>
<p><span class="math display">\[\begin{align}
\alpha_j^{(t+1)} = \alpha_j^{(t)} - \frac{y_j\left(E_i  - E_j\right)}{\eta} 
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
E_i &amp;= f(\mathbf{\vec{x}}_i) - y_i \\
\eta &amp;= 2\ \mathbf{K}\left( \mathbf{\vec{x}}_i, \mathbf{\vec{x}}_j   \right) - \mathbf{K}\left(  \mathbf{\vec{x}}_i, \mathbf{\vec{x}}_i \right) - \mathbf{K}\left(  \mathbf{\vec{x}}_j, \mathbf{\vec{x}}_j  \right)
\end{align}\]</span></p>
<p>Note that the <strong>eta</strong> (<span class="math inline">\(\eta\)</span>) symbol represents second derivative of the objective function along the diagonal line <span class="citation">(Platt J.C. <a href="bibliography.html#ref-ref718j">1998</a>)</span>. Here, we incorporate the use of <strong>Kernel function</strong>. Also, note that <span class="math inline">\(\mathbf{E_i}\)</span> is the error between the true label and the estimated output on the ith observation. See <strong>prediction function</strong> discussed above, namely <span class="math inline">\(h(\mathbf{x})\)</span> for <span class="math inline">\(f(\mathbf{\vec{x}}_i)\)</span>.</p>
<p>The next step is to get the new value of the second alpha (<span class="math inline">\(\alpha_j\)</span>) by clipping it within the range [L, H].</p>
<p><span class="math display">\[\begin{align}
\alpha_j^{(t+1)} = \begin{cases}
H &amp; if\ \alpha_j^{(t+1)} &gt; H\\
\alpha_j^{(t+1)} &amp; if\ L \le \alpha_j^{(t+1)} \le H\\ 
L &amp;\ if\ \alpha_j^{(t+1)} &lt; L
\end{cases}  \label{eqn:eqnnumber415}
\end{align}\]</span></p>
<p>Equivalently, to get the new value of the first alpha (<span class="math inline">\(\alpha_i\)</span>), we use the <strong>y</strong> labels along with the second alpha (<span class="math inline">\(\alpha_i\)</span>):</p>
<p><span class="math display">\[\begin{align}
\alpha_i^{(t+1)} = \alpha_i^{(t)} + s\left(a_j^{(t)} - a_j^{(t+1)}\right)
\ \ \ \ \ \ where\ \ \ \ \ s = y_i y_j
\end{align}\]</span></p>
<p>Now, to classify labels, we also need to calculate the intercept <strong>b</strong>. Here, we first compute for the <span class="math inline">\(\mathbf{b}_1\)</span> and <span class="math inline">\(\mathbf{b}_2\)</span> thresholds.</p>
<p>Let the following be:</p>
<p><span class="math display">\[\begin{align}
\mathbf{w}_i = y_i\left(\alpha_i^{(t+1)} - \alpha_i^{(t)}\right)
\ \ \ \ \ \ \ \ \ \ 
\mathbf{w}_j = y_j\left(\alpha_j^{(t+1)} - \alpha_j^{(t)}\right)
\end{align}\]</span></p>
<p>therefore:</p>
<p><span class="math display">\[\begin{align}
b_1 =  E_i + 
\mathbf{w}_i \mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_i} \right) + 
\mathbf{w}_j \mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j} \right) + b^{(t)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
b_2 = E_j +
\mathbf{w}_i\mathbf{K}\left( \mathbf{\vec{x}_i}, \mathbf{\vec{x}_j} \right) +
\mathbf{w}_j \mathbf{K}\left( \mathbf{\vec{x}_j}, \mathbf{\vec{x}_j} \right) + b^{(t)}
\end{align}\]</span></p>
<p>then, we clip our intercept <strong>b</strong> like so:</p>
<p><span class="math display">\[\begin{align}
b^{(t+1)} = \begin{cases}
b_1 &amp; if\ 0\ &lt; \alpha_i &lt; C \\
b_2 &amp; if\ 0\ &lt; \alpha_j &lt; C \\
(b_1 + b_2)/2 &amp; otherwise
\end{cases} \label{eqn:eqnnumber416}
\end{align}\]</span></p>
<p>Finally, we also need to handle our <strong>error cache</strong> required during enforcing the second constraint above, taking care of non-bound multipliers (<span class="math inline">\(\alpha_k\)</span>) such that <span class="math inline">\(k \ne i\)</span> and <span class="math inline">\(k \ne j\)</span>.</p>
<p><span class="math display">\[\begin{align}
E^{(t+1)}_k = E^{(t)}_k  + 
\mathbf{w}_i\mathbf{K}(\mathbf{\vec{x}}_i, \mathbf{\vec{x}}_k) + 
\mathbf{w}_j\mathbf{K}(\mathbf{\vec{x}}_j, \mathbf{\vec{x}}_k) + b^{(t)} - b^{(t+1)}
\end{align}\]</span></p>
<p>Below is our example implementation of <strong>SMO</strong> in R code following the <strong>pseudo-code</strong> in Platt J.C. <span class="citation">(<a href="bibliography.html#ref-ref718j">1998</a>)</span> and motivated by a python code from jonchar.net with modification. The implementation has three procedures, namely <strong>take.step(.)</strong>, <strong>examine.example(.)</strong>, and a main function - in our case, we have <strong>my.smo.svm(.)</strong> for main.</p>
<p>Our implementation starts with a helper function, namely <strong>roll(.)</strong>, for the random starting point of a vector.</p>
<div class="sourceCode" id="cb1466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1466-1" data-line-number="1">roll &lt;-<span class="st"> </span><span class="cf">function</span>(x, start) {</a>
<a class="sourceLine" id="cb1466-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb1466-3" data-line-number="3">  <span class="cf">if</span> (start <span class="op">&lt;</span><span class="st"> </span><span class="dv">1</span> <span class="op">||</span><span class="st"> </span>start <span class="op">&gt;=</span><span class="st"> </span>n) <span class="kw">return</span>(x)</a>
<a class="sourceLine" id="cb1466-4" data-line-number="4">  x.e =<span class="st"> </span>x[(n <span class="op">-</span><span class="st"> </span>start <span class="op">+</span><span class="st"> </span><span class="dv">1</span>)<span class="op">:</span>n]</a>
<a class="sourceLine" id="cb1466-5" data-line-number="5">  x.b =<span class="st"> </span>x[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span>start)]</a>
<a class="sourceLine" id="cb1466-6" data-line-number="6">  <span class="kw">c</span>(x.e, x.b)</a>
<a class="sourceLine" id="cb1466-7" data-line-number="7">}</a></code></pre></div>
<p>Then, we have two <strong>basic</strong> kernel functions - a linear kernel and a Gaussian kernel, respectively.</p>

<div class="sourceCode" id="cb1467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1467-1" data-line-number="1">linear.kernel &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2, <span class="dt">b =</span> <span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1467-2" data-line-number="2">    x1 <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x2)  <span class="op">+</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1467-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb1467-4" data-line-number="4">radial.kernel &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2, <span class="dt">sigma=</span><span class="dv">1</span>) { <span class="co"># Gaussian Kernel</span></a>
<a class="sourceLine" id="cb1467-5" data-line-number="5">    <span class="cf">if</span> (<span class="kw">is.null</span>(<span class="kw">nrow</span>(x1)) <span class="op">||</span><span class="st"> </span><span class="kw">dim</span>(x1) <span class="op">==</span><span class="st"> </span><span class="dv">1</span> ) {</a>
<a class="sourceLine" id="cb1467-6" data-line-number="6">        z =<span class="st"> </span><span class="op">-</span><span class="kw">sweep</span>(x2, <span class="dv">2</span>, x1, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb1467-7" data-line-number="7">        x.norm =<span class="st"> </span><span class="kw">apply</span>( z , <span class="dv">1</span>, <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) })  </a>
<a class="sourceLine" id="cb1467-8" data-line-number="8">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1467-9" data-line-number="9">    <span class="cf">if</span> (<span class="kw">is.null</span>(<span class="kw">nrow</span>(x2)) <span class="op">||</span><span class="st"> </span><span class="kw">dim</span>(x2) <span class="op">==</span><span class="st"> </span><span class="dv">1</span> ) {</a>
<a class="sourceLine" id="cb1467-10" data-line-number="10">        z =<span class="st"> </span><span class="op">-</span><span class="kw">sweep</span>(x1, <span class="dv">2</span>, x2, <span class="st">&#39;-&#39;</span>)</a>
<a class="sourceLine" id="cb1467-11" data-line-number="11">        x.norm =<span class="st"> </span><span class="kw">apply</span>( z , <span class="dv">1</span>, <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) }) </a>
<a class="sourceLine" id="cb1467-12" data-line-number="12">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1467-13" data-line-number="13">        pu =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb1467-14" data-line-number="14">        N =<span class="st"> </span><span class="kw">nrow</span>(x1)</a>
<a class="sourceLine" id="cb1467-15" data-line-number="15">        <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1467-16" data-line-number="16">             u  =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(x1[i,],N),N, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1467-17" data-line-number="17">             a  =<span class="st">  </span><span class="kw">array</span>( <span class="kw">as.matrix</span>(u  <span class="op">-</span><span class="st"> </span>x2), <span class="kw">c</span>(N,<span class="dv">2</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1467-18" data-line-number="18">             pu =<span class="st"> </span><span class="kw">c</span>(pu, a)</a>
<a class="sourceLine" id="cb1467-19" data-line-number="19">        }</a>
<a class="sourceLine" id="cb1467-20" data-line-number="20">        pu     =<span class="st"> </span><span class="kw">array</span>(pu, <span class="kw">c</span>(N,<span class="dv">2</span>,N))</a>
<a class="sourceLine" id="cb1467-21" data-line-number="21">        x.norm =<span class="st"> </span><span class="kw">apply</span>(pu, <span class="kw">c</span>(<span class="dv">3</span>,<span class="dv">1</span>), <span class="cf">function</span>(x) { <span class="kw">sqrt</span>(<span class="kw">sum</span>(x<span class="op">^</span><span class="dv">2</span>)) })</a>
<a class="sourceLine" id="cb1467-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb1467-23" data-line-number="23">    <span class="kw">exp</span>( <span class="op">-</span><span class="st"> </span>(x.norm<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>sigma<span class="op">^</span><span class="dv">2</span>) )</a>
<a class="sourceLine" id="cb1467-24" data-line-number="24">}</a></code></pre></div>

<p>More importantly, we implement our objective function and prediction (decision) functions:</p>

<div class="sourceCode" id="cb1468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1468-1" data-line-number="1">J =<span class="st"> </span>objective.function &lt;-<span class="st"> </span><span class="cf">function</span>(alphas, kernel, x, y) {</a>
<a class="sourceLine" id="cb1468-2" data-line-number="2">  a =<span class="st"> </span>alphas; <span class="kw">sum</span>(a) <span class="op">-</span><span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sum</span>((a<span class="op">*</span>a) <span class="op">*</span><span class="st"> </span>(y<span class="op">*</span>y) <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(x,x))</a>
<a class="sourceLine" id="cb1468-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb1468-4" data-line-number="4">h =<span class="st"> </span>prediction.function &lt;-<span class="st"> </span><span class="cf">function</span>(alphas, kernel, x1, x2, y, b) {</a>
<a class="sourceLine" id="cb1468-5" data-line-number="5">  (alphas <span class="op">*</span><span class="st"> </span>y) <span class="op">%*%</span><span class="st"> </span><span class="kw">kernel</span>( x1, x2)  <span class="op">-</span><span class="st"> </span>b</a>
<a class="sourceLine" id="cb1468-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb1468-7" data-line-number="7">f =<span class="st"> </span>evaluate &lt;-<span class="st"> </span><span class="cf">function</span>(i) {</a>
<a class="sourceLine" id="cb1468-8" data-line-number="8">    <span class="kw">h</span>(alphas, kernel, x, <span class="kw">t</span>(<span class="kw">c</span>(x[i,])), y, b)</a>
<a class="sourceLine" id="cb1468-9" data-line-number="9">}</a></code></pre></div>

<p>The <strong>take.step(.)</strong> procedure implements the <strong>update rules</strong> for <span class="math inline">\(\alpha_i\)</span> and <span class="math inline">\(\alpha_j\)</span>, including the <strong>b</strong> threshold and <strong>error cache</strong>.</p>

<div class="sourceCode" id="cb1469"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1469-1" data-line-number="1">take.step &lt;-<span class="st"> </span><span class="cf">function</span>(i1, i2, E2) {</a>
<a class="sourceLine" id="cb1469-2" data-line-number="2">  <span class="cf">if</span> (i1 <span class="op">==</span><span class="st"> </span>i2) { <span class="kw">return</span>(<span class="dv">0</span>) }</a>
<a class="sourceLine" id="cb1469-3" data-line-number="3">  alph1 =<span class="st"> </span>alphas[i1]; alph2 =<span class="st"> </span>alphas[i2]</a>
<a class="sourceLine" id="cb1469-4" data-line-number="4">  y1    =<span class="st"> </span>y[i1];      y2    =<span class="st"> </span>y[i2]</a>
<a class="sourceLine" id="cb1469-5" data-line-number="5">  s     =<span class="st"> </span>y1 <span class="op">*</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1469-6" data-line-number="6">  <span class="co"># Compute L, H</span></a>
<a class="sourceLine" id="cb1469-7" data-line-number="7">  <span class="cf">if</span> (y1 <span class="op">!=</span><span class="st"> </span>y2) {</a>
<a class="sourceLine" id="cb1469-8" data-line-number="8">    L =<span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, alph2 <span class="op">-</span><span class="st"> </span>alph1); H =<span class="st"> </span><span class="kw">min</span>(C, C <span class="op">+</span><span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>alph1)</a>
<a class="sourceLine" id="cb1469-9" data-line-number="9">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-10" data-line-number="10">  <span class="cf">if</span> (y1 <span class="op">==</span><span class="st"> </span>y2) {</a>
<a class="sourceLine" id="cb1469-11" data-line-number="11">    L =<span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, alph1 <span class="op">+</span><span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>C); H =<span class="st"> </span><span class="kw">min</span>(C, alph1 <span class="op">+</span><span class="st"> </span>alph2)</a>
<a class="sourceLine" id="cb1469-12" data-line-number="12">  } </a>
<a class="sourceLine" id="cb1469-13" data-line-number="13">  <span class="cf">if</span> (L <span class="op">==</span><span class="st"> </span>H) {</a>
<a class="sourceLine" id="cb1469-14" data-line-number="14">    <span class="kw">return</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1469-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1469-16" data-line-number="16">  <span class="cf">if</span> (eps <span class="op">&lt;</span><span class="st"> </span>alph1 <span class="op">&amp;&amp;</span><span class="st"> </span>alph1 <span class="op">&lt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span>eps) {</a>
<a class="sourceLine" id="cb1469-17" data-line-number="17">      E1 =<span class="st"> </span>errors[i1]</a>
<a class="sourceLine" id="cb1469-18" data-line-number="18">  } <span class="cf">else</span> { </a>
<a class="sourceLine" id="cb1469-19" data-line-number="19">      E1 =<span class="st"> </span><span class="kw">f</span>(i1) <span class="op">-</span><span class="st"> </span>y1  </a>
<a class="sourceLine" id="cb1469-20" data-line-number="20">  }</a>
<a class="sourceLine" id="cb1469-21" data-line-number="21">  <span class="co"># Computer kernels</span></a>
<a class="sourceLine" id="cb1469-22" data-line-number="22">  k11 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), <span class="kw">t</span>(x[i1,]))</a>
<a class="sourceLine" id="cb1469-23" data-line-number="23">  k12 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), <span class="kw">t</span>(x[i2,]))</a>
<a class="sourceLine" id="cb1469-24" data-line-number="24">  k22 =<span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i2,]), <span class="kw">t</span>(x[i2,]))</a>
<a class="sourceLine" id="cb1469-25" data-line-number="25">  eta =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span>k12 <span class="op">-</span><span class="st"> </span>k11 <span class="op">-</span><span class="st"> </span>k22</a>
<a class="sourceLine" id="cb1469-26" data-line-number="26">  <span class="cf">if</span> (eta <span class="op">&lt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1469-27" data-line-number="27">    a2 =<span class="st"> </span>alph2 <span class="op">-</span><span class="st"> </span>y2 <span class="op">*</span><span class="st"> </span>(E1 <span class="op">-</span><span class="st"> </span>E2) <span class="op">/</span><span class="st"> </span>eta</a>
<a class="sourceLine" id="cb1469-28" data-line-number="28">    <span class="cf">if</span> (a2 <span class="op">&lt;</span><span class="st"> </span>L) { a2 =<span class="st"> </span>L } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-29" data-line-number="29">    <span class="cf">if</span> (a2 <span class="op">&gt;</span><span class="st"> </span>H) { a2 =<span class="st"> </span>H }</a>
<a class="sourceLine" id="cb1469-30" data-line-number="30">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1469-31" data-line-number="31">    adj.alphas =<span class="st"> </span>alphas</a>
<a class="sourceLine" id="cb1469-32" data-line-number="32">    adj.alphas[i2] =<span class="st"> </span>L</a>
<a class="sourceLine" id="cb1469-33" data-line-number="33">    L.obj =<span class="st"> </span><span class="kw">J</span>(adj.alphas, kernel, x, y)</a>
<a class="sourceLine" id="cb1469-34" data-line-number="34">    adj.alphas[i2] =<span class="st"> </span>H</a>
<a class="sourceLine" id="cb1469-35" data-line-number="35">    H.obj =<span class="st"> </span><span class="kw">J</span>(adj.alphas, kernel, x, y)</a>
<a class="sourceLine" id="cb1469-36" data-line-number="36">    <span class="cf">if</span> (L.obj <span class="op">&gt;</span><span class="st"> </span>H.obj <span class="op">+</span><span class="st"> </span>eps) { a2 =<span class="st"> </span>L } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-37" data-line-number="37">    <span class="cf">if</span> (L.obj <span class="op">&lt;</span><span class="st"> </span>H.obj <span class="op">-</span><span class="st"> </span>eps) { a2 =<span class="st"> </span>H } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1469-38" data-line-number="38">      { a2 =<span class="st"> </span>alph2 }</a>
<a class="sourceLine" id="cb1469-39" data-line-number="39">  }</a>
<a class="sourceLine" id="cb1469-40" data-line-number="40">  <span class="cf">if</span> (a2 <span class="op">&lt;</span><span class="st"> </span><span class="fl">1e-8</span>)     { a2 =<span class="st"> </span><span class="dv">0</span> } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1469-41" data-line-number="41">  <span class="cf">if</span> (a2 <span class="op">&gt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span><span class="fl">1e-8</span>) { a2 =<span class="st"> </span>C }</a>
<a class="sourceLine" id="cb1469-42" data-line-number="42">  <span class="cf">if</span> (<span class="kw">abs</span>(a2 <span class="op">-</span><span class="st"> </span>alph2) <span class="op">&lt;</span><span class="st"> </span>eps <span class="op">*</span><span class="st"> </span>(a2 <span class="op">+</span><span class="st"> </span>alph2 <span class="op">+</span><span class="st"> </span>eps)) { <span class="kw">return</span>(<span class="dv">0</span>) }</a>
<a class="sourceLine" id="cb1469-43" data-line-number="43">  <span class="co"># update alpha_i</span></a>
<a class="sourceLine" id="cb1469-44" data-line-number="44">  a2 =<span class="st"> </span><span class="kw">c</span>(a2)</a>
<a class="sourceLine" id="cb1469-45" data-line-number="45">  a1 =<span class="st"> </span><span class="kw">c</span>(alph1 <span class="op">+</span><span class="st"> </span>s <span class="op">*</span><span class="st"> </span>(alph2 <span class="op">-</span><span class="st"> </span>a2))</a>
<a class="sourceLine" id="cb1469-46" data-line-number="46">  <span class="co"># update new b threshold</span></a>
<a class="sourceLine" id="cb1469-47" data-line-number="47">  w1 =<span class="st"> </span><span class="kw">c</span>(y1 <span class="op">*</span><span class="st"> </span>(a1 <span class="op">-</span><span class="st"> </span>alph1));  w2 =<span class="st"> </span><span class="kw">c</span>(y2 <span class="op">*</span><span class="st"> </span>(a2 <span class="op">-</span><span class="st"> </span>alph2))</a>
<a class="sourceLine" id="cb1469-48" data-line-number="48">  b1 =<span class="st"> </span><span class="kw">c</span>(E1 <span class="op">+</span><span class="st"> </span>w1 <span class="op">*</span><span class="st"> </span>k11 <span class="op">+</span><span class="st"> </span>w2 <span class="op">*</span><span class="st"> </span>k12 <span class="op">+</span><span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1469-49" data-line-number="49">  b2 =<span class="st"> </span><span class="kw">c</span>(E2 <span class="op">+</span><span class="st"> </span>w1 <span class="op">*</span><span class="st"> </span>k12 <span class="op">+</span><span class="st"> </span>w2 <span class="op">*</span><span class="st"> </span>k22 <span class="op">+</span><span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1469-50" data-line-number="50">  <span class="cf">if</span> ( <span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a1 <span class="op">&amp;&amp;</span><span class="st"> </span>a1 <span class="op">&lt;</span><span class="st"> </span>C) {</a>
<a class="sourceLine" id="cb1469-51" data-line-number="51">     b.new =<span class="st"> </span>b1</a>
<a class="sourceLine" id="cb1469-52" data-line-number="52">  } <span class="cf">else</span></a>
<a class="sourceLine" id="cb1469-53" data-line-number="53">  <span class="cf">if</span> ( <span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a2 <span class="op">&amp;&amp;</span><span class="st"> </span>a2 <span class="op">&lt;</span><span class="st"> </span>C) {</a>
<a class="sourceLine" id="cb1469-54" data-line-number="54">     b.new =<span class="st"> </span>b2</a>
<a class="sourceLine" id="cb1469-55" data-line-number="55">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1469-56" data-line-number="56">    b.new =<span class="st"> </span>(b1 <span class="op">+</span><span class="st"> </span>b2) <span class="op">*</span><span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb1469-57" data-line-number="57">  }</a>
<a class="sourceLine" id="cb1469-58" data-line-number="58">  <span class="co"># update alphas</span></a>
<a class="sourceLine" id="cb1469-59" data-line-number="59">  alphas[i1] &lt;&lt;-<span class="st"> </span>a1</a>
<a class="sourceLine" id="cb1469-60" data-line-number="60">  alphas[i2] &lt;&lt;-<span class="st"> </span>a2</a>
<a class="sourceLine" id="cb1469-61" data-line-number="61">  <span class="co"># update error cache</span></a>
<a class="sourceLine" id="cb1469-62" data-line-number="62">  <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a1 <span class="op">&amp;&amp;</span><span class="st"> </span>a1 <span class="op">&lt;</span><span class="st"> </span>C) { errors[i1] &lt;&lt;-<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb1469-63" data-line-number="63">  <span class="cf">if</span> (<span class="dv">0</span> <span class="op">&lt;</span><span class="st"> </span>a2 <span class="op">&amp;&amp;</span><span class="st"> </span>a2 <span class="op">&lt;</span><span class="st"> </span>C) { errors[i2] &lt;&lt;-<span class="st"> </span><span class="dv">0</span> }</a>
<a class="sourceLine" id="cb1469-64" data-line-number="64">  alph.indices =<span class="st"> </span><span class="kw">which</span>( <span class="op">!</span>(<span class="kw">seq</span>(n) <span class="op">%in%</span><span class="st"> </span><span class="kw">c</span>(i1,i2)) )</a>
<a class="sourceLine" id="cb1469-65" data-line-number="65">  errors[alph.indices] &lt;&lt;-<span class="st"> </span>errors[alph.indices] <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1469-66" data-line-number="66"><span class="st">        </span>w1 <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i1,]), x[alph.indices,]) <span class="op">+</span></a>
<a class="sourceLine" id="cb1469-67" data-line-number="67"><span class="st">        </span>w2 <span class="op">*</span><span class="st"> </span><span class="kw">kernel</span>(<span class="kw">t</span>(x[i2,]), x[alph.indices,]) <span class="op">+</span><span class="st"> </span>b <span class="op">-</span><span class="st"> </span>b.new</a>
<a class="sourceLine" id="cb1469-68" data-line-number="68">  <span class="co"># update b threshold  </span></a>
<a class="sourceLine" id="cb1469-69" data-line-number="69">  b &lt;&lt;-<span class="st"> </span><span class="kw">c</span>(b.new)</a>
<a class="sourceLine" id="cb1469-70" data-line-number="70">  <span class="kw">return</span> (<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1469-71" data-line-number="71">}</a></code></pre></div>

<p>As the functionâs name implies, we examine a given observation (or example) based on the second index.</p>

<div class="sourceCode" id="cb1470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1470-1" data-line-number="1">examine.example &lt;-<span class="st"> </span><span class="cf">function</span>(i2) {</a>
<a class="sourceLine" id="cb1470-2" data-line-number="2">  y2     =<span class="st"> </span>y[i2]</a>
<a class="sourceLine" id="cb1470-3" data-line-number="3">  alph2  =<span class="st"> </span>alphas[i2] </a>
<a class="sourceLine" id="cb1470-4" data-line-number="4">  <span class="cf">if</span> (eps <span class="op">&lt;</span><span class="st"> </span>alph2  <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&lt;</span><span class="st"> </span>C <span class="op">-</span><span class="st"> </span>eps) {</a>
<a class="sourceLine" id="cb1470-5" data-line-number="5">      E2 =<span class="st"> </span>errors[i2]</a>
<a class="sourceLine" id="cb1470-6" data-line-number="6">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1470-7" data-line-number="7">      E2 =<span class="st"> </span><span class="kw">f</span>(i2) <span class="op">-</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1470-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1470-9" data-line-number="9">  r2     =<span class="st"> </span>E2 <span class="op">*</span><span class="st"> </span>y2</a>
<a class="sourceLine" id="cb1470-10" data-line-number="10">  <span class="cf">if</span> ((r2 <span class="op">&lt;</span><span class="st"> </span><span class="op">-</span>tol <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&lt;</span><span class="st"> </span>C) <span class="op">||</span><span class="st"> </span>(r2 <span class="op">&gt;</span><span class="st"> </span>tol <span class="op">&amp;&amp;</span><span class="st"> </span>alph2 <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>)) {</a>
<a class="sourceLine" id="cb1470-11" data-line-number="11">    i1.indices =<span class="st"> </span><span class="kw">which</span>( alphas <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>alphas <span class="op">!=</span><span class="st"> </span>C )</a>
<a class="sourceLine" id="cb1470-12" data-line-number="12">    m =<span class="st"> </span><span class="kw">length</span>(i1.indices)</a>
<a class="sourceLine" id="cb1470-13" data-line-number="13">    <span class="co"># Consider heuristics in terms of max error deltas</span></a>
<a class="sourceLine" id="cb1470-14" data-line-number="14">    <span class="cf">if</span> (m <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1470-15" data-line-number="15">      <span class="cf">if</span> (E2 <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-16" data-line-number="16">        i1 =<span class="st"> </span><span class="kw">which.min</span>(errors)</a>
<a class="sourceLine" id="cb1470-17" data-line-number="17">      } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb1470-18" data-line-number="18">      <span class="cf">if</span> (E2 <span class="op">&lt;=</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-19" data-line-number="19">        i1 =<span class="st"> </span><span class="kw">which.max</span>(errors)</a>
<a class="sourceLine" id="cb1470-20" data-line-number="20">      }</a>
<a class="sourceLine" id="cb1470-21" data-line-number="21">      <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-22" data-line-number="22">    }</a>
<a class="sourceLine" id="cb1470-23" data-line-number="23">    <span class="co"># Consider heuristics in terms of non-zero and non-C alphas</span></a>
<a class="sourceLine" id="cb1470-24" data-line-number="24">    <span class="cf">if</span> (m <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1470-25" data-line-number="25">      rand.i =<span class="st"> </span><span class="kw">sample.int</span>(m, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1470-26" data-line-number="26">      rolled.indices =<span class="st"> </span><span class="kw">roll</span>(i1.indices, rand.i)</a>
<a class="sourceLine" id="cb1470-27" data-line-number="27">      <span class="cf">for</span> (i1 <span class="cf">in</span> rolled.indices) {</a>
<a class="sourceLine" id="cb1470-28" data-line-number="28">        <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-29" data-line-number="29">      }</a>
<a class="sourceLine" id="cb1470-30" data-line-number="30">    }</a>
<a class="sourceLine" id="cb1470-31" data-line-number="31">    <span class="co"># Otherwise, consider all observations</span></a>
<a class="sourceLine" id="cb1470-32" data-line-number="32">    rand.i =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1470-33" data-line-number="33">    <span class="cf">for</span> (i1 <span class="cf">in</span> <span class="kw">roll</span>(<span class="kw">seq</span>(n), rand.i)) {</a>
<a class="sourceLine" id="cb1470-34" data-line-number="34">        <span class="cf">if</span> (<span class="kw">take.step</span>(i1, i2, E2)) { <span class="kw">return</span>(<span class="dv">1</span>) }</a>
<a class="sourceLine" id="cb1470-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1470-36" data-line-number="36">  }</a>
<a class="sourceLine" id="cb1470-37" data-line-number="37">  <span class="kw">return</span>(<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1470-38" data-line-number="38">}</a></code></pre></div>

<p>Our training function is <strong>my.smo.svm(.)</strong> and it learns data for classification.</p>

<div class="sourceCode" id="cb1471"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1471-1" data-line-number="1">train =<span class="st"> </span>my.smo.svm &lt;-<span class="st"> </span><span class="cf">function</span>(x, <span class="dt">C=</span><span class="dv">1</span>, <span class="dt">tol=</span><span class="fl">10e-3</span>, <span class="dt">eps=</span><span class="fl">10e-3</span>) {</a>
<a class="sourceLine" id="cb1471-2" data-line-number="2">  n           =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1471-3" data-line-number="3">  num.changed =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-4" data-line-number="4">  examine.all =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1471-5" data-line-number="5">  <span class="cf">while</span> (num.changed <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span> <span class="op">||</span><span class="st"> </span>examine.all ) {</a>
<a class="sourceLine" id="cb1471-6" data-line-number="6">    num.changed =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-7" data-line-number="7">    <span class="cf">if</span> (examine.all) {</a>
<a class="sourceLine" id="cb1471-8" data-line-number="8">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1471-9" data-line-number="9">        num.changed =<span class="st"> </span>num.changed <span class="op">+</span><span class="st"> </span><span class="kw">examine.example</span>(i)</a>
<a class="sourceLine" id="cb1471-10" data-line-number="10">      }</a>
<a class="sourceLine" id="cb1471-11" data-line-number="11">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1471-12" data-line-number="12">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="kw">which</span>( alphas <span class="op">!=</span><span class="st"> </span><span class="dv">0</span> <span class="op">&amp;</span><span class="st"> </span>alphas <span class="op">!=</span><span class="st"> </span>C )) {</a>
<a class="sourceLine" id="cb1471-13" data-line-number="13">        num.changed =<span class="st"> </span>num.changed <span class="op">+</span><span class="st"> </span><span class="kw">examine.example</span>(i)</a>
<a class="sourceLine" id="cb1471-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb1471-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb1471-16" data-line-number="16">    <span class="cf">if</span> (examine.all <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1471-17" data-line-number="17">      examine.all =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1471-18" data-line-number="18">    } <span class="cf">else</span> <span class="cf">if</span> (num.changed <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb1471-19" data-line-number="19">      examine.all =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1471-20" data-line-number="20">    }</a>
<a class="sourceLine" id="cb1471-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb1471-22" data-line-number="22">  <span class="kw">list</span>(<span class="st">&quot;alphas&quot;</span> =<span class="st"> </span>alphas, <span class="st">&quot;kernel&quot;</span> =<span class="st"> </span>kernel, <span class="st">&quot;x&quot;</span> =<span class="st"> </span>x, <span class="st">&quot;y&quot;</span> =<span class="st"> </span>y, <span class="st">&quot;b&quot;</span> =<span class="st"> </span>b)</a>
<a class="sourceLine" id="cb1471-23" data-line-number="23">}</a></code></pre></div>

<p>Then, we have a function to plot the result of our <strong>SVM</strong>:</p>

<div class="sourceCode" id="cb1472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1472-1" data-line-number="1">plot.svm &lt;-<span class="st"> </span><span class="cf">function</span>(title, model) {</a>
<a class="sourceLine" id="cb1472-2" data-line-number="2">  alphas =<span class="st"> </span>model<span class="op">$</span>alphas; kernel =<span class="st"> </span>model<span class="op">$</span>kernel</a>
<a class="sourceLine" id="cb1472-3" data-line-number="3">  x =<span class="st"> </span>model<span class="op">$</span>x; y =<span class="st"> </span>model<span class="op">$</span>y; b =<span class="st"> </span>model<span class="op">$</span>b</a>
<a class="sourceLine" id="cb1472-4" data-line-number="4">  x.test =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(x[,<span class="dv">1</span>]), <span class="kw">max</span>(x[,<span class="dv">1</span>]), <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1472-5" data-line-number="5">  y.test =<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(x[,<span class="dv">2</span>]), <span class="kw">max</span>(x[,<span class="dv">2</span>]), <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb1472-6" data-line-number="6">  grid =<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="dv">100</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>), <span class="dv">100</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1472-7" data-line-number="7">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(x.test)) {</a>
<a class="sourceLine" id="cb1472-8" data-line-number="8">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(y.test)) {</a>
<a class="sourceLine" id="cb1472-9" data-line-number="9">        grid[j,i] =<span class="st"> </span><span class="kw">c</span>(<span class="kw">h</span>(alphas, kernel, x, <span class="kw">t</span>(<span class="kw">c</span>(x.test[j],y.test[i])), y, b))</a>
<a class="sourceLine" id="cb1472-10" data-line-number="10">      }</a>
<a class="sourceLine" id="cb1472-11" data-line-number="11">  }</a>
<a class="sourceLine" id="cb1472-12" data-line-number="12">  <span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x[,<span class="dv">1</span>]), <span class="dt">ylim=</span><span class="kw">range</span>(x[,<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb1472-13" data-line-number="13">     <span class="dt">ylab=</span><span class="st">&quot;x2&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;x1&quot;</span>, <span class="dt">main=</span>title)</a>
<a class="sourceLine" id="cb1472-14" data-line-number="14">  <span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1472-15" data-line-number="15">  <span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>,<span class="dt">v=</span><span class="dv">0</span>,<span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1472-16" data-line-number="16">  <span class="kw">points</span>(x[,<span class="dv">1</span>],x[,<span class="dv">2</span>], <span class="dt">pch=</span><span class="dv">20</span>,  <span class="dt">col=</span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span><span class="dv">-1</span>, </a>
<a class="sourceLine" id="cb1472-17" data-line-number="17">                                          <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>))</a>
<a class="sourceLine" id="cb1472-18" data-line-number="18">  no_opt =<span class="st"> </span><span class="kw">which</span>(<span class="kw">abs</span>(alphas) <span class="op">&gt;</span><span class="st"> </span>eps)</a>
<a class="sourceLine" id="cb1472-19" data-line-number="19">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(no_opt)) {</a>
<a class="sourceLine" id="cb1472-20" data-line-number="20">      <span class="kw">points</span>(x[no_opt[i],<span class="dv">1</span>], x[no_opt[i],<span class="dv">2</span>], <span class="dt">pch=</span><span class="dv">1</span>, <span class="dt">cex=</span><span class="fl">1.2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb1472-21" data-line-number="21">  }</a>
<a class="sourceLine" id="cb1472-22" data-line-number="22">  <span class="kw">contour</span>(<span class="dt">x =</span> x.test, <span class="dt">y =</span> y.test, <span class="dt">method =</span> <span class="st">&quot;edge&quot;</span>, <span class="dt">lwd=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">1</span>),</a>
<a class="sourceLine" id="cb1472-23" data-line-number="23">        grid, <span class="dt">levels=</span><span class="kw">c</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>), <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1472-24" data-line-number="24">        <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;blue&quot;</span>),</a>
<a class="sourceLine" id="cb1472-25" data-line-number="25">        <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1472-26" data-line-number="26">}</a></code></pre></div>

<p>Also, for fix parameters, let us have the following global variables:</p>

<div class="sourceCode" id="cb1473"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1473-1" data-line-number="1">tol    =<span class="st"> </span><span class="fl">10e-3</span> <span class="co"># tolerance</span></a>
<a class="sourceLine" id="cb1473-2" data-line-number="2">eps    =<span class="st"> </span><span class="fl">10e-3</span> <span class="co"># constraint limit</span></a></code></pre></div>

<p>Now, to use our implementation, let us use the following dataset:</p>

<div class="sourceCode" id="cb1474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1474-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1474-2" data-line-number="2">N         =<span class="st"> </span><span class="dv">100</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1474-3" data-line-number="3">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1474-4" data-line-number="4">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1474-5" data-line-number="5">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1474-6" data-line-number="6">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2)))</a></code></pre></div>

<p>Finally, let us run our implementation and plot. See Figure <a href="machinelearning2.html#fig:smo1">10.26</a>.</p>

<div class="sourceCode" id="cb1475"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1475-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1475-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1475-3" data-line-number="3">kernel =<span class="st"> </span>linear.kernel</a>
<a class="sourceLine" id="cb1475-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1475-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1475-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1475-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1475-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1475-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (linear)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo1"></span>
<img src="DS_files/figure-html/smo1-1.png" alt="SMO (Linear)" width="70%" />
<p class="caption">
Figure 10.26: SMO (Linear)
</p>
</div>

<p>Below, we show learning <strong>SVM-SMO</strong> using radial kernel. See Figure <a href="machinelearning2.html#fig:smo2">10.27</a>.</p>

<div class="sourceCode" id="cb1476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1476-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1476-2" data-line-number="2">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1476-3" data-line-number="3">pi.set =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb1476-4" data-line-number="4">x1.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1476-5" data-line-number="5">x2.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1476-6" data-line-number="6">x1.inner.rand =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1476-7" data-line-number="7">x2.inner.rand =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1476-8" data-line-number="8">x1.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)  </a>
<a class="sourceLine" id="cb1476-9" data-line-number="9">x2.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1476-10" data-line-number="10">x1.inner.line =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)  </a>
<a class="sourceLine" id="cb1476-11" data-line-number="11">x2.inner.line =<span class="st"> </span><span class="fl">0.5</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1476-12" data-line-number="12">x.line    =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand), </a>
<a class="sourceLine" id="cb1476-13" data-line-number="13">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1476-14" data-line-number="14">x         =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.outer.rand, x2.outer.rand), </a>
<a class="sourceLine" id="cb1476-15" data-line-number="15">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1476-16" data-line-number="16">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, n))</a></code></pre></div>
<div class="sourceCode" id="cb1477"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1477-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1477-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1477-3" data-line-number="3">kernel =<span class="st"> </span>radial.kernel</a>
<a class="sourceLine" id="cb1477-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1477-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1477-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1477-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1477-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1477-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (Radial)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo2"></span>
<img src="DS_files/figure-html/smo2-1.png" alt="SMO (Radial)" width="70%" />
<p class="caption">
Figure 10.27: SMO (Radial)
</p>
</div>

<p>Below, we show another <strong>SVM-SMO</strong> learning using radial kernel. See Figure <a href="machinelearning2.html#fig:smo3">10.28</a>.</p>

<div class="sourceCode" id="cb1478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1478-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1478-2" data-line-number="2">n =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1478-3" data-line-number="3">pi.set =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,  pi, <span class="dt">length.out=</span>n)</a>
<a class="sourceLine" id="cb1478-4" data-line-number="4">x1.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb1478-5" data-line-number="5">x2.outer.rand =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) </a>
<a class="sourceLine" id="cb1478-6" data-line-number="6">x1.inner.rand =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1478-7" data-line-number="7">x2.inner.rand =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="kw">runif</span>(n, <span class="fl">-0.5</span>, <span class="fl">0.5</span>) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1478-8" data-line-number="8">x1.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set)</a>
<a class="sourceLine" id="cb1478-9" data-line-number="9">x2.outer.line =<span class="st"> </span><span class="dv">2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) </a>
<a class="sourceLine" id="cb1478-10" data-line-number="10">x1.inner.line =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">cos</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1478-11" data-line-number="11">x2.inner.line =<span class="st"> </span><span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">sin</span>(pi.set) <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1478-12" data-line-number="12">x.line    =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand), </a>
<a class="sourceLine" id="cb1478-13" data-line-number="13">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1478-14" data-line-number="14">x         =<span class="st"> </span><span class="kw">rbind</span>(<span class="kw">cbind</span>(x1.outer.rand, x2.outer.rand), </a>
<a class="sourceLine" id="cb1478-15" data-line-number="15">                  <span class="kw">cbind</span>(x1.inner.rand, x2.inner.rand)) </a>
<a class="sourceLine" id="cb1478-16" data-line-number="16">y         =<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, n))</a></code></pre></div>
<div class="sourceCode" id="cb1479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1479-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1479-2" data-line-number="2">n      =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1479-3" data-line-number="3">kernel =<span class="st"> </span>radial.kernel</a>
<a class="sourceLine" id="cb1479-4" data-line-number="4">alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1479-5" data-line-number="5">b      =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1479-6" data-line-number="6">C      =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1479-7" data-line-number="7">errors =<span class="st"> </span><span class="kw">h</span>(alphas, kernel, x, x, y, b) <span class="op">-</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb1479-8" data-line-number="8">my.smo.model =<span class="st"> </span><span class="kw">my.smo.svm</span>(x)</a>
<a class="sourceLine" id="cb1479-9" data-line-number="9"><span class="kw">plot.svm</span>(<span class="st">&quot;SMO (Radial)&quot;</span>, my.smo.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:smo3"></span>
<img src="DS_files/figure-html/smo3-1.png" alt="SMO (Radial)" width="70%" />
<p class="caption">
Figure 10.28: SMO (Radial)
</p>
</div>

<p>We leave readers to adjust the random seed, number of samples, and the <strong>C</strong> parameter as an exercise.</p>
</div>
<div id="sdca-based-svm" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.2.3</span> SDCA-based SVM <a href="machinelearning2.html#sdca-based-svm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Another algorithm to mention for <strong>dual optimization</strong> is the <strong>Stochastic Dual Coordinate Ascent (SDCA)</strong> analyzed by Shai Shalev-Shwartz et al. <span class="citation">(<a href="bibliography.html#ref-ref745c">2008</a>)</span> along with reference to <strong>DCA</strong> as proposed by Hsieh C. et al. <span class="citation">(<a href="bibliography.html#ref-ref745c">2008</a>)</span>.</p>
<p>There are variants of <strong>SDCA</strong>, all intended to enhance the algorithm in parallel processing, distributed processing, and using mini-batch.  </p>
<p>In relation to the <strong>Dual Lagrangian formulation</strong>, we rewrite the second term of the objective function in the <strong>fourth</strong> step of <strong>SMO</strong>: </p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \left(\mathbf{\vec{x}_i} \cdot \mathbf{\vec{x}_j}\right) \right\} 
\end{align}\]</span></p>
<p>such that we have the following:</p>
<p><span class="math display">\[\begin{align}
 \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j (\mathbf{x_i} \cdot \mathbf{x_j}) 
&amp;=  \frac{1}{2} \left(\sum_{i=1}^n  \alpha_i y_i \mathbf{x_i} \right)
 \left( \sum_{j=1}^n \alpha_j y_j \mathbf{x_j} \right)\\
&amp;=  \frac{1}{2} \mathbf{w} \cdot \mathbf{w}\\ 
&amp;= \frac{1}{2}\|\mathbf{w}\|^2_2  
\end{align}\]</span></p>
<p>where <span class="math inline">\(\frac{1}{2} \left(\sum_{i=1}^n \alpha_i y_i \mathbf{x_i} \right)\)</span> is based on result of the partial derivative of the <strong>dual form</strong> with respect to <strong>w</strong>.</p>
<p>Our objective function is therefore rewritten this way (note here that we are using the <span class="math inline">\(\lambda &gt; 0\)</span> as our regularizer):</p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i- \frac{\lambda}{2} \|\mathbf{w}\|^2 \right\} 
\end{align}\]</span></p>
<p>Now consider the delta of <span class="math inline">\(\alpha_i\)</span> denoted as <span class="math inline">\(\Delta^{(\alpha)}_i\)</span> so that <span class="math inline">\(\alpha^{(t+1)}_i = \alpha_i^{(t)} + \Delta\alpha_i\)</span>, we then have the dual form for the ith observation: <span class="math inline">\(\mathbf{D(\alpha_i + \Delta^{(\alpha)}_i)}\)</span>. Equivalently, our primal form is updated with the following delta <span class="math inline">\((\lambda n)^{(-1)}\Delta^{(\alpha)} x_i\)</span> so that we therefore have <span class="math inline">\(\mathbf{w}^{(t+1)} = \mathbf{w}^{(t)} + (\lambda n)^{(-1)}\Delta^{(\alpha)} x_i\)</span>.</p>
<p>We then update our objective function:</p>
<p><span class="math display">\[\begin{align}
\mathcal{J}(\alpha) = 
\text{arg}\ \underset{\alpha_i \ge 0}{\text{max}} 
\left\{\sum_{i=1}^n \alpha_i + \Delta^{(\alpha)}) - \frac{\lambda}{2} \|\mathbf{w} + (\lambda n)^{-1} \Delta^{(\alpha)} x_i\|^2 \right\} 
\end{align}\]</span></p>
<p>With all that, we now have our <strong>update rules</strong> for <strong>SDCA</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\Delta_i^{(\alpha)} {}&amp;=  y_i\ \underbrace{
           \text{max} \left(0, \text{min} \left(1, \frac{1 - 
             y_i x_i^T \mathbf{w}^{(t-1)}}{\|x_i\|^2_2/(\lambda n)}\right) + y_i \alpha_i^{(t-1)} \right)
}_{\text{hinge-loss in closed form}}
             - \alpha_i^{(t-1)}\\
\alpha_{i}^{(t+1)} &amp;= \alpha_i^{(t)} + \Delta_i^{(\alpha)}\ \ \ \ \ \ \ \leftarrow \text{(dual update)} \\
\mathbf{w}^{(t+1)} &amp;= \mathbf{w}^{(t)} + \frac{\Delta_i^{(\alpha)}}{\lambda n} x_i\ \ \ \leftarrow \text{(primal update)}  
\end{align}\]</span></p>
<p>For the algorithm, we focus on the <strong>SDCA-Perm</strong> variant from Shai Shalev-Shwatz and Tong Zhang <span class="citation">(<a href="bibliography.html#ref-ref710s">2013</a>)</span> in solving the <strong>dual problem</strong> of <strong>SVM</strong> using the <strong>hinge-loss</strong> and <strong>random option</strong>. Below is the algorithm for review.</p>
<p><span class="math display">\[
 \begin{array}{ll}
\mathbf{\text{SDCA Algorithm}} \\
\text{Shalev-Schwartz et al. (arxiv 1209.1873v2 2013)}\\
\\
\text{Input:}\ \ S, \lambda, \text{T} \\
\text{Set}\ \mathbf{w}_0 = z + \frac{1}{\lambda n} \sum_{i=1}^n \alpha^{0} x_i\\
\text{loop}\ t\ in\ 1,2,...,\text{T}  \\
\ \ \ \ \text{Choose}\ i_t\ \in \{1,...,|S|\}\ \ \leftarrow \ \ \text{(uniformly random)}\\  
\ \ \ \ \Delta_i^{(\alpha)} = y_i\ \text{max} \left(0, \text{min} \left(1, \frac{1 - 
             y_i x_i^T \mathbf{w}^{(t-1)}}{\|x_i\|^2_2/(\lambda n)}\right) + y_i \alpha_i^{(t-1)} \right) - \alpha_i^{(t-1)}\\
\ \ \ \ \alpha_i^{(t+1)} = \alpha_i^{(t)} + \Delta_i^{(\alpha)}\ \ \ \ \ \ \ \ \ \ \leftarrow \text{(dual update)} \\
\ \ \ \ \mathbf{w}^{(t+1)} =  \mathbf{w}^{(t)} + \frac{\Delta_i^{(\alpha)}}{\lambda n} x_i\ \ \ \ \ \ \leftarrow \text{(primal update)} \\
\text{end loop} \\
\text{Ouput}\ \mathbf{w}^{(T+1)} 
\end{array}
\]</span></p>
<p>Below is an example implementation of <strong>SDCA</strong> for <strong>SVM</strong>. Here, we use <strong>sample.int(.)</strong> to simulate random sampling of the index (<span class="math inline">\(i^{(t)}\)</span>).</p>

<div class="sourceCode" id="cb1480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1480-1" data-line-number="1">my.sdca.svm &lt;-<span class="st"> </span><span class="cf">function</span>(x, y, <span class="dt">lambda=</span><span class="fl">0.01</span>, <span class="dt">limit=</span><span class="dv">15000</span>) {</a>
<a class="sourceLine" id="cb1480-2" data-line-number="2">  n     =<span class="st"> </span><span class="kw">nrow</span>(x); </a>
<a class="sourceLine" id="cb1480-3" data-line-number="3">  x =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), x)</a>
<a class="sourceLine" id="cb1480-4" data-line-number="4">  p =<span class="st"> </span><span class="kw">ncol</span>(x)</a>
<a class="sourceLine" id="cb1480-5" data-line-number="5">  w     =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, p)</a>
<a class="sourceLine" id="cb1480-6" data-line-number="6">  alphas =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1480-7" data-line-number="7">  <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>limit) {</a>
<a class="sourceLine" id="cb1480-8" data-line-number="8">      i  =<span class="st"> </span><span class="kw">sample.int</span>(n, <span class="dt">size=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1480-9" data-line-number="9">      xi =<span class="st"> </span>x[i,]; yi =<span class="st"> </span>y[i]</a>
<a class="sourceLine" id="cb1480-10" data-line-number="10">      norm_x  =<span class="st"> </span><span class="kw">sqrt</span>(<span class="kw">sum</span>(xi<span class="op">^</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1480-11" data-line-number="11">      delta.alpha_i =<span class="st"> </span>yi <span class="op">*</span><span class="st"> </span><span class="kw">max</span>(<span class="dv">0</span>, <span class="kw">min</span>(<span class="dv">1</span>, ( (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span><span class="kw">t</span>(xi) <span class="op">%*%</span><span class="st"> </span>w) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1480-12" data-line-number="12"><span class="st">                </span>lambda <span class="op">*</span><span class="st"> </span>n<span class="op">/</span><span class="st"> </span>norm_x )) <span class="op">+</span><span class="st"> </span>yi <span class="op">*</span><span class="st"> </span>alphas[i]) <span class="op">-</span><span class="st"> </span>alphas[i]</a>
<a class="sourceLine" id="cb1480-13" data-line-number="13">      alphas[i] =<span class="st"> </span>alphas[i] <span class="op">+</span><span class="st"> </span>delta.alpha_i</a>
<a class="sourceLine" id="cb1480-14" data-line-number="14">      w       =<span class="st"> </span>w <span class="op">+</span><span class="st"> </span>( delta.alpha_i <span class="op">*</span><span class="st"> </span>xi) <span class="op">/</span><span class="st"> </span>(lambda <span class="op">*</span><span class="st"> </span>n )</a>
<a class="sourceLine" id="cb1480-15" data-line-number="15">  }</a>
<a class="sourceLine" id="cb1480-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;alphas&quot;</span> =<span class="st"> </span>alphas, <span class="st">&quot;w&quot;</span> =<span class="st"> </span>w)</a>
<a class="sourceLine" id="cb1480-17" data-line-number="17">}</a></code></pre></div>

<p>Let us use the same linear dataset as we did in <strong>SMO</strong> previously to use our implementation.</p>

<div class="sourceCode" id="cb1481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1481-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1481-2" data-line-number="2">eps =<span class="st"> </span><span class="fl">10e-3</span></a>
<a class="sourceLine" id="cb1481-3" data-line-number="3">N         =<span class="st"> </span><span class="dv">20</span>; v =<span class="st"> </span><span class="dv">1</span> <span class="co"># variance</span></a>
<a class="sourceLine" id="cb1481-4" data-line-number="4">x1.blue   =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); x2.blue =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); y1 =<span class="st"> </span><span class="kw">rep</span>( <span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1481-5" data-line-number="5">x1.red    =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N,  <span class="dv">2</span>, v); x2.red  =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>N, <span class="dv">-2</span>, v); y2 =<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">1</span>, N)</a>
<a class="sourceLine" id="cb1481-6" data-line-number="6">x         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(x1.blue, x1.red), <span class="kw">c</span>(x2.blue, x2.red)) </a>
<a class="sourceLine" id="cb1481-7" data-line-number="7">y         =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">c</span>(y1, y2))</a></code></pre></div>

<p>Moreover, because our algorithm is stochastic, let us use a random seed for our implementation.</p>

<div class="sourceCode" id="cb1482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1482-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">152</span>)</a>
<a class="sourceLine" id="cb1482-2" data-line-number="2">svm.sdca.model =<span class="st"> </span><span class="kw">my.sdca.svm</span>(x,y)</a>
<a class="sourceLine" id="cb1482-3" data-line-number="3">hplanes        =<span class="st"> </span><span class="kw">svm.hyperplanes</span>(svm.sdca.model<span class="op">$</span>w)</a></code></pre></div>

<p>We then plot. See Figure <a href="machinelearning2.html#fig:sdcasvm">10.29</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:sdcasvm"></span>
<img src="DS_files/figure-html/sdcasvm-1.png" alt="SVM (Stochastic Dual Coordinate Ascent)" width="70%" />
<p class="caption">
Figure 10.29: SVM (Stochastic Dual Coordinate Ascent)
</p>
</div>

<p><strong>Finally</strong>, for prediction, we can use the model to classify any new or missing data.</p>
<p><span class="math display">\[\begin{align}
h(x; \mathbf{w}) = \text{sign}(g(x)) = \text{sign}(\mathbf{w}^T \mathbf{x} + b)\ \ \ \leftarrow\ \ \ \
\begin{cases}
+1 &amp; \mathbf{w}^T \mathbf{x} \ge 0\\
-1 &amp; \mathbf{w}^T \mathbf{x} &lt; 0\\
\end{cases} \label{eqn:eqnnumber417}
\end{align}\]</span></p>
<div class="sourceCode" id="cb1483"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1483-1" data-line-number="1">x.new =<span class="st"> </span><span class="kw">rbind</span>( <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st"> </span><span class="dv">-2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st">  </span><span class="dv">2</span>), </a>
<a class="sourceLine" id="cb1483-2" data-line-number="2">               <span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span> =<span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;x1&quot;</span> =<span class="st">  </span><span class="dv">2</span>, <span class="st">&quot;x2&quot;</span> =<span class="st"> </span><span class="dv">-2</span>))</a>
<a class="sourceLine" id="cb1483-3" data-line-number="3">my.sdca.prediction &lt;-<span class="st"> </span><span class="cf">function</span>(w, x) {</a>
<a class="sourceLine" id="cb1483-4" data-line-number="4">    <span class="kw">c</span>(<span class="kw">sign</span>(w  <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(x)))</a>
<a class="sourceLine" id="cb1483-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb1483-6" data-line-number="6">h =<span class="st"> </span><span class="kw">my.sdca.prediction</span>(svm.sdca.model<span class="op">$</span>w, x.new)</a>
<a class="sourceLine" id="cb1483-7" data-line-number="7">pred =<span class="st"> </span><span class="kw">ifelse</span>(h <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;positive&quot;</span>, <span class="st">&quot;negative&quot;</span>)</a>
<a class="sourceLine" id="cb1483-8" data-line-number="8"><span class="kw">print</span>(<span class="kw">cbind</span>(x.new, pred), <span class="dt">quote=</span><span class="ot">FALSE</span>, <span class="dt">right=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##      intercept x1 x2     pred
## [1,]         1 -2  2 positive
## [2,]         1  2 -2 negative</code></pre>
<p>We showed a sample Kernel functions in our <strong>SMO-based SVM</strong>. We leave readers to investigate Kernel functions in use with <strong>SDCA-based SVM</strong> and as an exercise, modify our example implementation above. Also, there are variants of SDCA worth investigating with the introduction of <strong>mini-batching</strong>, along with properties that make the algorithm adaptive and distributed.</p>
<p>Also, for other variants of <strong>SVM</strong>, we leave readers to investigate <strong>Top-K multiclass SVM</strong> for a high accuracy multiclass <strong>SVM</strong> algorithm with <strong>top k</strong> error prioritization, and <strong>Ensemble SVM</strong> to support <strong>ensemble models</strong> for <strong>SVM models</strong>. Note that an example application for <strong>Top-K</strong> multi-classification is around <strong>image/object classification and image/object segmentation</strong>.</p>
<p>As for <strong>prediction performance</strong> for <strong>SVM</strong> methods, we can reference <strong>ConfusionMatrix</strong> for metrics such as <strong>specificity</strong>, <strong>sensitivity</strong>, <strong>accuracy</strong>, <strong>F1 score</strong>, and others.</p>
</div>
</div>
<div id="multi-class-classification-supervised" class="section level2 hasAnchor">
<h2><span class="header-section-number">10.3</span> Multi-class Classification (Supervised) <a href="machinelearning2.html#multi-class-classification-supervised" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Most of our discussions are around <strong>binary</strong> classifications in previous sections. Our goal now is to demonstrate <strong>multivariate</strong> classification. Here, we revisit <strong>Decision Trees</strong> and review <strong>goodness of split</strong> based on the purity (or impurity) of nodes. Our goal is to group observations of variables into <strong>homogeneous</strong> instances (or nodes) which have low degrees of impurity.</p>
<p>A starting point is to discuss <strong>Naive Bayes</strong> as a simple classifier.</p>
<div id="bayesian-classification" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.1</span> Bayesian Classification <a href="machinelearning2.html#bayesian-classification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us start the discussion of <strong>multi-classification</strong> with <strong>Naive Bayes</strong> as a simple classifier. Recall the Bayes formula derived from Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>):</p>
<p><span class="math display">\[\begin{align}
P(\text{Class|Features}) =
\frac{P(\text{Features|Class}) P(\text{Class}) }{P(\text{Features}) }
\ \ \rightarrow \ \ 
P(Y|X) =  \frac{P(X|Y)P(Y)}{P(X)}
\end{align}\]</span></p>
<p>We can then expand the formula to accommodate a case in which our posterior distribution is based on the condition of multiple distributions.</p>
<p><span class="math display">\[\begin{align}
P(y_k|x_{1}, x_{2}, x_{3},...,x_{n}) = 
\frac{\left[\prod_{i=1}^{n}P(x_{i}|y_k)\right]\cdot P(y_k)}
{\prod_{i=1}^{n}P(x_{i})}
\end{align}\]</span></p>
<p>Expanding the <strong>likelihood</strong> and <strong>prior</strong>, we get the following:</p>
<p><span class="math display">\[\begin{align}
\underbrace{P(x_i = j|y_i = k) = \frac{\sum_i^N\mathbf{I}(x_i=j, y_i = k)}{\sum_i^N\mathbf{I}(y_i = k) }}_{\text{likelihood}}
\ \ \ \ \ \ \ \ \ \ \ \ \
\underbrace{P(y_i = k) = \frac{\sum_i^N\mathbf{I}(y_i = k)}{N}}_{\text{prior}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(X\)</span> represents a set of <strong>features</strong> that are continuous or discrete</li>
<li><strong>y</strong> being the class or label</li>
<li><strong>k</strong> is a specific value of a class in Y.</li>
<li><strong>j</strong> is a specific value of a feature in X.</li>
</ul>
<p>For a toy example, let us use the common <strong>frequency (probabilities) table</strong> to illustrate <strong>naive bayesian classification</strong>. Suppose we are looking to form a new basketball team to represent a national basketball competition. To form the team, we need quality players with specific requirements. - the idea is to classify whether a candidate is a <strong>Class A</strong> caliber player, a <strong>Class B</strong> caliber player, and so on, based on the following <strong>features</strong>:</p>
<ul>
<li>SPG - Scores Per Game<br />
</li>
<li>RPG - Rebounds Per Game<br />
</li>
<li>APG - Assists Per Game<br />
</li>
<li>FTPG - Free Throws per Game</li>
</ul>
<p>A qualified candidate is with high SPG, high RPG, or high APG if one gets double-digit statistics, e.g., 20 points per game and ten assists per game. Table <a href="machinelearning2.html#tab:nbaclass">10.2</a> shows 200 <strong>observed</strong> candidates with the qualifications we seek:</p>
<table>
<caption><span id="tab:nbaclass">Table 10.2: </span>National Basketball Competition</caption>
<thead>
<tr class="header">
<th align="right">Player (y)</th>
<th align="right">FTPG (<span class="math inline">\(x_{1}\)</span>)</th>
<th align="right">APG (<span class="math inline">\(x_{2}\)</span>)</th>
<th align="right">RPG (<span class="math inline">\(x_{3}\)</span>)</th>
<th align="right">SPG (<span class="math inline">\(x_{4}\)</span>)</th>
<th align="right">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Class A</td>
<td align="right">5 (0.50)</td>
<td align="right">9 (0.90)</td>
<td align="right">7 (0.70)</td>
<td align="right">10 (1.00)</td>
<td align="right"><span class="math inline">\(\mathbf{10}\)</span> (0.05)</td>
</tr>
<tr class="even">
<td align="right">Class B</td>
<td align="right">10 (0.20)</td>
<td align="right">40 (0.80)</td>
<td align="right">25 (0.50)</td>
<td align="right">25 (0.50)</td>
<td align="right"><span class="math inline">\(\mathbf{50}\)</span> (0.25)</td>
</tr>
<tr class="odd">
<td align="right">Class C</td>
<td align="right">28 (0.20)</td>
<td align="right">70 (0.50)</td>
<td align="right">84 (0.60)</td>
<td align="right">14 (0.01)</td>
<td align="right"><span class="math inline">\(\mathbf{140}\)</span> (0.70)</td>
</tr>
<tr class="even">
<td align="right">Total</td>
<td align="right">43 (0.22)</td>
<td align="right">119 (0.60)</td>
<td align="right">116 (0.58)</td>
<td align="right">49 (0.24)</td>
<td align="right"><span class="math inline">\(\mathbf{200}\)</span> (1.00)</td>
</tr>
</tbody>
</table>
<p>Of the overall 200 candidates in Table <a href="machinelearning2.html#tab:nbaclass">10.2</a>, we see only 5% are <strong>Class A</strong> caliber players. Of the 10 <strong>Class A</strong> caliber players, only 5 achieve high <strong>FTPG</strong>.</p>
<p>With reference to Table <a href="machinelearning2.html#tab:nbaclass">10.2</a>, we can formulate the probabilities. It can be shown that the <strong>probability</strong> of getting players with high free throws per game (<strong>FTPG</strong>) is 22% and that the probability of players with high scores per game (<strong>SPG</strong>) is 24%. They are expressed accordingly - they form the marginal probability, namely <span class="math inline">\(P(X) \equiv P(Features)\)</span>:</p>
<p><span class="math display">\[
P(x_1 = FTPG) = \frac{43}{200} = 22\%\ \ \ \ \ \ \ \ \ \ \ \ \
P(x_4 = SPG) = \frac{49}{200} = 24\%
\]</span></p>
<p>We can also show that the <strong>likelihood</strong> of a high <strong>FTPG</strong> given <strong>Class A</strong> caliber player is 50% and that the <strong>likelihood</strong> of a high <strong>SP</strong> given <strong>Class A</strong> caliber player is 100%. They are also expressed accordingly - and they form the <strong>likelihood</strong>, namely <span class="math inline">\(P(x|y) \equiv P(Features|\text{Class})\)</span>:</p>
<p><span class="math display">\[\begin{align*}
P(x_1 = FTPG | y = \text{Class A}) = \frac{5}{10} = 50\% \\
P(x_4 = SPG | y = \text{Class A}) = \frac{10}{10} = 100\%
\end{align*}\]</span></p>
<p>Now the probability of getting a <strong>Class A</strong> player given all the <strong>features</strong> is expressed as such:</p>
<p><span class="math display">\[\begin{align}
P(\text{Class A}&amp;|FTPG, APG, RPG, SPG)  \nonumber \\ 
&amp;=\frac{P(FTPG|A)\ P(APG|A)\ P(RPG|A) \ P(SPG|A)\ P(A)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)} \\
&amp;= \frac{0.50\times 0.90\times 0.70\times 1.00\times 0.05}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.8571708 \nonumber
\end{align}\]</span></p>
<p>If we are looking for <strong>Class B</strong> players, we will be estimating it this way:</p>
<p><span class="math display">\[\begin{align}
P(\text{Class B}&amp;|FTPG, APG, RPG, SPG)  \nonumber \\ 
&amp;=\frac{P(FTPG|B)\ P(APG|B)\ P(RPG|B)\ P(SPG|B)\ P(B)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)} \\
&amp;= \frac{0.20 \times 0.80\times 0.50\times 0.50\times 0.25}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.5442355 \nonumber
\end{align}\]</span></p>
<p>If we are looking for <strong>Class C</strong> players, we will be estimating it this way:</p>
<p><span class="math display">\[\begin{align}
P(\text{Class C}&amp;|FTPG, APG, RPG, SPG)   \nonumber \\ 
&amp;=\frac{P(FTPG|C)\ P(APG|C)\ P(RPG|C)\ P(SPG|C)\ P(C)}
{P(FTPG)\ P(APG)\ P(RPG)\ P(SPG)} \\
&amp;= \frac{0.20\times 0.50\times 0.60\times 0.01\times 0.70}{0.22\times 0.60\times 0.58\times 0.24} \nonumber \\
&amp;= 0.02285789 \nonumber
\end{align}\]</span></p>
<p>Therefore, it can be shown that with the 200 available candidates, we have about 85.72% probability of getting a <strong>Class A</strong> caliber player given the presented statistics, about 54.42% probability of getting a <strong>Class B</strong> caliber player, and about 2.29% probability of getting a <strong>Class C</strong> caliber player.</p>
<p>Now, assume a training set claims to have the following probabilities for high <strong>FTPG</strong>, high <strong>APG</strong>, and high <strong>RPG</strong>, but zero probability for having low in <strong>SPG</strong>.</p>

<p><span class="math display">\[
\begin{array}{rl}
P(x_1 = FTPG | \text{A}) = 0.50\\
P(x_1 = APG | \text{A}) =  0.90\\
P(x_1 = RPG | \text{A}) =  0.70\\
P(x_1 = SPG | \text{A}) =  0.00\\
\end{array}\ \ 
\begin{array}{rl}
P(x_1 = FTPG | \text{B}) = 0.20\\
P(x_1 = APG | \text{B}) =  0.80\\
P(x_1 = RPG | \text{B}) =  0.50\\
P(x_1 = SPG | \text{B}) =  0.00\\
\end{array}\ \ 
\begin{array}{rl}
P(x_1 = FTPG | \text{C}) = 0.20\\
P(x_1 = APG | \text{C}) =  0.50\\
P(x_1 = RPG | \text{C}) =  0.60\\
P(x_1 = SPG | \text{C}) =  0.00\\
\end{array}
\]</span>
</p>
<p>If even one of the high statistics per game has zero frequency, then the probability of predicting such a class is always zero. It is a disadvantage for <strong>Bayes Classification</strong>. Here, we use <strong>Laplacian Smoothing</strong> as a solution to correct such a disadvantage. </p>
<p>Basically, for <strong>Naive Bayes</strong> which has the form <span class="math inline">\(P(y_i = k|x_i) \propto P(x_i|y_i = k)P(y_i = k)\)</span>, we add <span class="math inline">\(\lambda\)</span> like so:</p>
<p><span class="math display">\[\begin{align}
\underbrace{P(x_i = j|y_i = k) = \frac{\sum_i^N\mathbf{I}(x_i=j, y_i = k)  + \lambda}{\sum_i^N\mathbf{I}(y_i = k) + J\lambda}}_{\text{likelihood}}
\ \ \ \ \
\underbrace{P(y_i = k) = \frac{\sum_i^N\mathbf{I}(y_i = k) + \lambda}{N + K\lambda}}_{\text{prior}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>J</strong> is the number of categorical values for <span class="math inline">\(x_i\)</span>. Here, we only have two (low or high).</li>
<li><strong>K</strong> is the number of categorical values for <span class="math inline">\(y_i\)</span>. Here, we have three (A, B, C).</li>
</ul>
<p>So that if <span class="math inline">\(\lambda = 1\)</span> (arbitrarily choose a small number)</p>
<p><span class="math display">\[
\begin{array}{lrr}
P(x_1 = FTPG | \text{A}) &amp;= \frac{5 + 1}{10 + 2\times1} = 0.50\\
P(x_1 = APG | \text{A}) &amp;= \frac{9 + 1}{10 + 2\times1}  = 0.83\\
P(x_1 = RPG | \text{A}) &amp;= \frac{7 + 1}{10 + 2\times1}  = 0.67\\
P(x_1 = SPG | \text{A}) &amp;= \frac{0 + 1}{10 + 2\times1}  = 0.08\\
\end{array}\ \ \ \ \ \ 
\begin{array}{lrr}
P(y_k = \text{A}) = \frac{10 + 1}{200 + 3\times1} = 0.0541872\\
P(y_k = \text{B}) = \frac{50 + 1}{200 + 3\times1}  = 0.2512315\\
P(y_k = \text{C}) = \frac{140 + 1}{200 + 3\times1}  = 0.6945813\\
\end{array}
\]</span>
Notice now that we have a small probability assigned to <span class="math inline">\(P(x_1 = SPG | \text{A})\)</span>. We leave readers to compute the other probabilities.</p>
</div>
<div id="classification-trees" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.2</span> Classification Trees <a href="machinelearning2.html#classification-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, it helps to refresh our understanding of <strong>Decision Trees</strong>. Recall in <strong>Regression trees</strong> the use of <strong>SSE</strong> as basis for our <strong>loss function</strong> to determine <strong>goodness of fit</strong>. We now extend the concept by using other <strong>splitting criteria</strong> based from <strong>probability theory</strong> and <strong>information theory</strong>. In the context of <strong>Classification Trees</strong> when dealing with <strong>categorical</strong> target variables, we consider basic metrics such <strong>Entropy</strong>, <strong>Gini Index</strong>, <strong>Gini Impurity</strong>, and <strong>Information Gain</strong> for our <strong>loss (and gain) function</strong>. In the same context, we use <strong>classification error rate</strong> to validate performance.     </p>
<p><strong>CHAID, ID3, C4.5, C5.0</strong></p>
<p>Apart from <strong>CART</strong> developed in 1984, other decision tree algorithms are available for review. Three such algorithms were introduced by <strong>Ross Quinlan</strong> <span class="citation">(<a href="bibliography.html#ref-ref559jr">1986</a>, <a href="bibliography.html#ref-ref563jr">1996</a>)</span>. The algorithms differ in addressing the following core criteria and strategy, among other considerations: <strong>splitting criterion</strong>, <strong>stopping criterion</strong>, and <strong>pruning strategy</strong>.</p>
<p>Note that <strong>splitting criterion</strong> is based on measures of node impurity. Also, note in the <strong>Decision Tree</strong> section that we put a discussion of <strong>CART</strong> in the context of <strong>Regression</strong> and <strong>bifurcation</strong> (binary split); however, in the context of <strong>Classification</strong>, the implementation of <strong>Cart</strong> supports multiway split along with the use of <strong>Gini Index (also called Gini Impurity)</strong> to determine the split.</p>
<p>Here, instead of detailing classic classification tree algorithms, let us briefly introduce them <span class="citation">(Hssina B. et al. <a href="bibliography.html#ref-ref580h">2014</a>)</span>:</p>
<ul>
<li><p><strong>CHAID</strong> is called <strong>Chi-square automatic interaction detector</strong> and was introduced by Gordon Kass in 1980. The algorithm uses <strong>Chi-square</strong> as <strong>splitting criterion</strong>. </p></li>
<li><p><strong>ID3</strong> is called <strong>Iterative Dichotomiser 3</strong> introduced by R. Quinlan in 1983-86. The algorithm uses <strong>Information gain</strong> as the <strong>splitting criterion</strong> and supports <strong>multiway</strong> split. </p></li>
<li><p><strong>C4.5</strong> is a successor of <strong>ID3</strong> introduced by R. Quinlan in 1993. The algorithm uses <strong>Gain Ratio</strong> as the <strong>splitting criterion</strong>. It enforces a rule-based algorithm that supports continuous and categorical attributes, missing data, post-pruning, and <strong>multiway</strong> split.  </p></li>
<li><p><strong>C5.0</strong> is a successor of <strong>C4.5</strong> introduced by R. Quinlan R. in 1994. The algorithm uses <strong>Information Gain</strong> as the <strong>splitting criterion</strong>. It comes with improved speed and memory efficiency, among other enhancements. It supports discrete, continuous, date and time, timestamp, and categorical attributes.  </p></li>
</ul>
<p>In our discussions ahead, we illustrate the use of <strong>Gini Index</strong> and <strong>Information Gain</strong> as we build our classification tree.</p>
<p>Recall under the <strong>Regression Trees</strong> Subsection under <strong>Regression</strong> Section the use of <strong>CART</strong> to perform tree regression. Here, we use the same method with the intent to perform tree classification. A simple tree classification using <strong>CART</strong> is shown in Figure <a href="machinelearning2.html#fig:classificationtree1">10.30</a> using an R dataset called <strong>iris</strong>.</p>
<div class="sourceCode" id="cb1485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1485-1" data-line-number="1">pacman<span class="op">::</span><span class="kw">p_load</span>(rpart,rpart.plot)</a>
<a class="sourceLine" id="cb1485-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1485-3" data-line-number="3"><span class="kw">data</span>(iris)</a>
<a class="sourceLine" id="cb1485-4" data-line-number="4">rpart.control =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">minsplit=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">15</span>, <span class="dt">minbucket =</span> <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1485-5" data-line-number="5">tree.model =<span class="st"> </span><span class="kw">rpart</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> iris, <span class="dt">control =</span> rpart.control)</a>
<a class="sourceLine" id="cb1485-6" data-line-number="6"><span class="kw">rpart.plot</span>(tree.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:classificationtree1"></span>
<img src="DS_files/figure-html/classificationtree1-1.png" alt="Classification Tree" width="70%" />
<p class="caption">
Figure 10.30: Classification Tree
</p>
</div>
<p>To illustrate tree classification, we start by building a tree and splitting tree nodes.</p>
<p><strong>First</strong>, let us view the structure of our dataset, namely <strong>iris</strong>. Here, we have three independent variables, namely <strong>Sepal.Length</strong>, <strong>Sepal.Width</strong>, <strong>Petal.Length</strong>, and <strong>Petal.Width</strong>. Our target (or dependent) variable is <strong>Species</strong>. See below:</p>

<div class="sourceCode" id="cb1486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1486-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">56</span>)</a>
<a class="sourceLine" id="cb1486-2" data-line-number="2"><span class="kw">str</span>(iris, <span class="dt">width=</span><span class="dv">56</span>, <span class="dt">strict.width=</span><span class="st">&quot;wrap&quot;</span>)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    150 obs. of  5 variables:
## $ Sepal.Length: num 5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9
##    ...
## $ Sepal.Width : num 3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9
##    3.1 ...
## $ Petal.Length: num 1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4
##    1.5 ...
## $ Petal.Width : num 0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2
##    0.1 ...
## $ Species : Factor w/ 3 levels
##    &quot;setosa&quot;,&quot;versicolor&quot;,..: 1 1 1 1 1 1 1 1 1 1 ...</code></pre>

<p><strong>Second</strong>, we also use Figure <a href="machinelearning2.html#fig:regressiontree">10.2</a> similar to <strong>Regression Trees</strong> under <strong>Regression</strong> Section. Note that our target variable is <strong>categorical</strong> in this case. Also, to illustrate, let us choose one of the features, namely <strong>Sepal.Length</strong> and arbitrarily split the observations perhaps somewhere in the middle (say at index 70).</p>

<div class="sourceCode" id="cb1488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1488-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1488-2" data-line-number="2">datacars =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1488-3" data-line-number="3">sepal.length =<span class="st"> </span>datacars[[<span class="st">&quot;Sepal.Length&quot;</span>]]</a>
<a class="sourceLine" id="cb1488-4" data-line-number="4">(<span class="dt">sorted.input =</span> <span class="kw">sort</span>(sepal.length, <span class="dt">index.return =</span> <span class="ot">TRUE</span>))<span class="op">$</span>x</a></code></pre></div>
<pre><code>##   [1] 4.3 4.4 4.4 4.4 4.5 4.6 4.6 4.6 4.6 4.7 4.7 4.8 4.8 4.8 4.8 4.8
##  [17] 4.9 4.9 4.9 4.9 4.9 4.9 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0 5.0
##  [33] 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.1 5.2 5.2 5.2 5.2 5.3 5.4 5.4
##  [49] 5.4 5.4 5.4 5.4 5.5 5.5 5.5 5.5 5.5 5.5 5.5 5.6 5.6 5.6 5.6 5.6
##  [65] 5.6 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.7 5.8 5.8 5.8 5.8 5.8 5.8 5.8
##  [81] 5.9 5.9 5.9 6.0 6.0 6.0 6.0 6.0 6.0 6.1 6.1 6.1 6.1 6.1 6.1 6.2
##  [97] 6.2 6.2 6.2 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.3 6.4 6.4 6.4 6.4
## [113] 6.4 6.4 6.4 6.5 6.5 6.5 6.5 6.5 6.6 6.6 6.7 6.7 6.7 6.7 6.7 6.7
## [129] 6.7 6.7 6.8 6.8 6.8 6.9 6.9 6.9 6.9 7.0 7.1 7.2 7.2 7.2 7.3 7.4
## [145] 7.6 7.7 7.7 7.7 7.7 7.9</code></pre>
<div class="sourceCode" id="cb1490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1490-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1490-2" data-line-number="2">split.input &lt;-<span class="st"> </span><span class="cf">function</span>(sorted.input, idx, <span class="dt">is.factor =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1490-3" data-line-number="3">  n =<span class="st"> </span><span class="kw">length</span>(sorted.input<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb1490-4" data-line-number="4">  left.indices  =<span class="st">  </span>sorted.input<span class="op">$</span>ix[<span class="dv">1</span><span class="op">:</span>idx]</a>
<a class="sourceLine" id="cb1490-5" data-line-number="5">  right.indices =<span class="st">  </span>sorted.input<span class="op">$</span>ix[(idx<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>n]</a>
<a class="sourceLine" id="cb1490-6" data-line-number="6">  <span class="kw">list</span>(<span class="st">&quot;left&quot;</span> =<span class="st"> </span>left.indices, <span class="st">&quot;right&quot;</span> =<span class="st"> </span>right.indices )</a>
<a class="sourceLine" id="cb1490-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1490-8" data-line-number="8">split.index =<span class="st"> </span><span class="dv">70</span></a>
<a class="sourceLine" id="cb1490-9" data-line-number="9">(<span class="dt">split =</span> <span class="kw">split.input</span>(sorted.input, split.index ))</a></code></pre></div>
<pre><code>## $left
##  [1]  14   9  39  43  42   4   7  23  48   3  30  12  13  25  31  46
## [17]   2  10  35  38  58 107   5   8  26  27  36  41  44  50  61  94
## [33]   1  18  20  22  24  40  45  47  99  28  29  33  60  49   6  11
## [49]  17  21  32  85  34  37  54  81  82  90  91  65  67  70  89  95
## [65] 122  16  19  56  80  96
## 
## $right
##  [1]  97 100 114  15  68  83  93 102 115 143  62  71 150  63  79  84
## [17]  86 120 139  64  72  74  92 128 135  69  98 127 149  57  73  88
## [33] 101 104 124 134 137 147  52  75 112 116 129 133 138  55 105 111
## [49] 117 148  59  76  66  78  87 109 125 141 145 146  77 113 144  53
## [65] 121 140 142  51 103 110 126 130 108 131 106 118 119 123 136 132</code></pre>

<p>The left split has a total of 70 observations, and the right split has a total of 80 observations.</p>
<p><strong>Third</strong>, we determine the number of classes in our target variable. Below, we have three classes in the <strong>Species</strong> target variable, namely <strong>Setosa</strong>, <strong>Versicolor</strong>, and <strong>Virginica</strong>.</p>
<div class="sourceCode" id="cb1492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1492-1" data-line-number="1"><span class="kw">levels</span>(iris<span class="op">$</span>Species) </a></code></pre></div>
<pre><code>## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>
<p><strong>Fourth</strong>, we then determine the distribution of the classes between the two splits.</p>

<div class="sourceCode" id="cb1494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1494-1" data-line-number="1"><span class="kw">library</span>(plyr)</a>
<a class="sourceLine" id="cb1494-2" data-line-number="2">left =<span class="st"> </span>iris[split<span class="op">$</span>left,]</a>
<a class="sourceLine" id="cb1494-3" data-line-number="3">right =<span class="st"> </span>iris[split<span class="op">$</span>right,]</a>
<a class="sourceLine" id="cb1494-4" data-line-number="4">iris.dist =<span class="st"> </span><span class="kw">cbind</span>(<span class="kw">count</span>(left, <span class="st">&#39;Species&#39;</span>), <span class="kw">count</span>(right, <span class="st">&#39;Species&#39;</span>),</a>
<a class="sourceLine" id="cb1494-5" data-line-number="5">            <span class="kw">count</span>(iris, <span class="st">&#39;Species&#39;</span>))[,<span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">4</span>,<span class="dv">6</span>)]</a>
<a class="sourceLine" id="cb1494-6" data-line-number="6"><span class="kw">colnames</span>(iris.dist) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Class&quot;</span>, <span class="st">&quot;Left Count&quot;</span>, <span class="st">&quot;Right Count&quot;</span>, <span class="st">&quot;Total Count&quot;</span>)</a>
<a class="sourceLine" id="cb1494-7" data-line-number="7">iris.dist</a></code></pre></div>
<pre><code>##        Class Left Count Right Count Total Count
## 1     setosa         49           1          50
## 2 versicolor         19          31          50
## 3  virginica          2          48          50</code></pre>

<p><strong>Fifth</strong>, to know if the split is good, we need to use specific metrics. For Regression, we use <strong>SSE</strong>. For classification, let us illustrate the use of <strong>Gini Index (GI)</strong> and <strong>Gini Information Gain (IG)</strong>.</p>
<p><span class="math display">\[\begin{align}
GI_{\text{(gini index)}} &amp;=\sum_{k=1}^K P_k \times ( 1 - P_k) &amp;\ \ \ \ \ \text{where K = number of classes}  \\
&amp;=1 - \sum_{k=1}^K\left[P(Y=y_k)\right]^2  
\end{align}\]</span></p>
<p>For a perfect split, we have the following <strong>Gini Index</strong>:</p>
<p><span class="math display">\[
\begin{array}{ll}
GI_{(perfect)} &amp;= 1 - \left[ P(Y=\text{setosa})^2 + P(Y=\text{versicola})^2 + P(Y=\text{virginica})^2\right] \\
&amp;= 1 - \left[ \left(\frac{50}{150}\right)^2 + \left(\frac{50}{150}\right)^2 + \left(\frac{50}{150}\right)^2\right]\\
&amp;= 0.6666667
\end{array}
\]</span></p>
<p>For each split in our example above, we have the following <strong>Gini Index</strong>:</p>
<p><span class="math display">\[\begin{align}
GI_{(left)} &amp;= 1 - [ P(Y=\text{setosa}|X_{(left)})^2 + \nonumber\\
&amp;\ \ \ \ P(Y=\text{versicola}|X_{(left)})^2 + P(Y=\text{virginica}|X_{(left)})^2 ] \\
&amp;= 1 - \left[ \left(\frac{49}{70}\right)^2 + \left(\frac{19}{70}\right)^2 + \left(\frac{2}{70}\right)^2\right] \nonumber \\
&amp;= 0.4355102 \nonumber
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
GI_{(right)} &amp;= 1 - [ P(Y=\text{setosa}|X_{(right)})^2 + P(Y=\text{versicola}|X_{(right)})^2 + \nonumber \\
&amp;\ \ \ \ P(Y=\text{virginica}|X_{(right)})^2 ] \\
&amp;= 1 - \left[ \left(\frac{1}{80}\right)^2 + \left(\frac{31}{80}\right)^2 + \left(\frac{48}{80}\right)^2\right] \nonumber\\
&amp;= 0.4896875 \nonumber
\end{align}\]</span></p>
<p>To now get the <strong>goodness of split</strong>, we use the following formula: </p>
<p><span class="math display">\[\begin{align}
IG_{(gini)} = GI_{(perfect)} - \left[ P(left) \times GI_{(left)} + P(right) \times GI_{(right)}  \right]
\end{align}\]</span></p>
<p>where <span class="math inline">\(P(left)\)</span> is the <strong>weight of impurity of the left split</strong> and <span class="math inline">\(P(right)\)</span> is the <strong>weight of impurity of the right split</strong>.</p>
<p><span class="math display">\[
\begin{array}{lll}
P(left) = \frac{49 + 19 + 2}{150} = \frac{70}{150} &amp;\ \ \ \ \ &amp;P(left) = \frac{1 + 31 + 48}{150} = \frac{80}{150}
\end{array} 
\]</span></p>
<p>Therefore, our <strong>Gini Gain</strong> which calculates the amount of <strong>impurity</strong> removed is:</p>
<p><span class="math display">\[
\begin{array}{ll}
IG_{(gini)} &amp;= 0.6666667 - \left[ \frac{70}{150}\times(0.4355102) + \frac{80}{150} \times (0.4896875) \right]\\
&amp;= 0.2022619
\end{array}
\]</span></p>
<p>Below, we have our example implementation of <strong>Gini Gain</strong>:</p>

<div class="sourceCode" id="cb1496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1496-1" data-line-number="1">gini =<span class="st"> </span>gini.index &lt;-<span class="st"> </span><span class="cf">function</span>(y, lev) {</a>
<a class="sourceLine" id="cb1496-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1496-3" data-line-number="3">  gi =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1496-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> lev) {</a>
<a class="sourceLine" id="cb1496-5" data-line-number="5">    cl.n   =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1496-6" data-line-number="6">    gi =<span class="st"> </span>gi <span class="op">+</span><span class="st"> </span>(cl.n <span class="op">/</span><span class="st"> </span>n)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1496-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1496-8" data-line-number="8">  <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>gi</a>
<a class="sourceLine" id="cb1496-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1496-10" data-line-number="10">gain =<span class="st"> </span>gini.gain &lt;-<span class="st"> </span><span class="cf">function</span>(parent, left, right, lev) {</a>
<a class="sourceLine" id="cb1496-11" data-line-number="11">  left.n =<span class="st"> </span><span class="kw">length</span>(left)</a>
<a class="sourceLine" id="cb1496-12" data-line-number="12">  right.n =<span class="st"> </span><span class="kw">length</span>(right)</a>
<a class="sourceLine" id="cb1496-13" data-line-number="13">  total.n =<span class="st"> </span>left.n <span class="op">+</span><span class="st"> </span>right.n</a>
<a class="sourceLine" id="cb1496-14" data-line-number="14">  p.left =<span class="st"> </span>left.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1496-15" data-line-number="15">  p.right =<span class="st"> </span>right.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1496-16" data-line-number="16">  <span class="kw">gini</span>(parent, lev) <span class="op">-</span><span class="st"> </span>(p.left <span class="op">*</span><span class="st"> </span><span class="kw">gini</span>(left, lev)  <span class="op">+</span><span class="st"> </span>p.right <span class="op">*</span><span class="st"> </span><span class="kw">gini</span>(right, lev))</a>
<a class="sourceLine" id="cb1496-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb1496-18" data-line-number="18">y =<span class="st"> </span>iris<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1496-19" data-line-number="19">lev =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1496-20" data-line-number="20"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Gini Gain: &quot;</span>, <span class="kw">gain</span> (y, y[split<span class="op">$</span>left], y[split<span class="op">$</span>right], lev)))</a></code></pre></div>
<pre><code>## [1] &quot;Gini Gain: 0.202261904761905&quot;</code></pre>

<p><strong>Sixth</strong>, alternatively, we can also use <strong>Entropy (H)</strong> and <strong>Entropy Information Gain (IG)</strong> to measure the <strong>goodness of fit</strong>.</p>
<p><span class="math display">\[\begin{align}
H_{\text{(entropy)}} = - \sum_{i=1}^K P_k \times\ \log_e(P_k)
\end{align}\]</span></p>
<p>For a perfect split, we have the following <strong>Entropy</strong>:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rl}
H{\text{(perfect)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa) \times\ \log_e P(Y = setosa) +\\
P(Y = versicola) \times\ \log_e P(Y = versicola) +\\
P(Y = virginica) \times\ \log_e P(Y = virginica)
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{50}{150} \log_e \frac{50}{150} +
\frac{50}{150} \log_e \frac{50}{150}+
\frac{50}{150} \log_e \frac{50}{150}
\right]\\
&amp;= - \left[-1.098612\right] = 1.098612
\end{array}
\end{align*}\]</span></p>
<p>For each split in our example above, we have the following <strong>Entropy</strong>:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{rl}
H{\text{(left)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa|X_{(left)}) \times\ \log_e P(Y = setosa|X_{(left)}) +\\
P(Y = versicola|X_{(left)}) \times\ \log_e P(Y = versicola|X_{(left)}) +\\
P(Y = virginica|X_{(left)}) \times\ \log_e P(Y = virginica|X_{(left)})
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{49}{70} \log_e \frac{49}{70} +
\frac{19}{70} \log_e \frac{19}{70}+
\frac{2}{70} \log_e \frac{2}{70}
\right]\\
&amp;= -\left[-0.705212\right] = 0.705212
\\
\\
H{\text{(right)}} &amp;= - \left[
\begin{array}{ll}
P(Y = setosa|X_{(right)}) \times\ \log_e P(Y = setosa|X_{(right)}) +\\
P(Y = versicola|X_{(right)}) \times\ \log_e P(Y = versicola|X_{(right)}) +\\
P(Y = virginica|X_{(right)}) \times\ \log_e P(Y = virginica|X_{(right)})
\end{array}
\right]\\
\\
&amp;= - \left[
\frac{1}{80} \log_e \frac{1}{80} +
\frac{31}{80} \log_e \frac{31}{80}+
\frac{48}{80} \log_e \frac{48}{80}
\right]\\
&amp;= -\left[-0.728636\right] = 0.728636
\end{array}
\end{align*}\]</span></p>
<p>To now get the <strong>goodness of split</strong>, we use the following formula:</p>
<p><span class="math display">\[\begin{align}
IG_{(entropy)} &amp;= H_{(perfect)} - \left[ P(left) \times H_{(left)} + P(right) \times H_{(right)}  \right]\\
&amp;= 1.098612 - \left[ \frac{70}{150} (0.705212) + \frac{80}{150} (0.728636) \right] \nonumber \\
&amp;= 0.3809072 \nonumber
\end{align}\]</span></p>
<p>Below, we have our example implementation of <strong>Entropy Gain</strong>:</p>

<div class="sourceCode" id="cb1498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1498-1" data-line-number="1">entropy &lt;-<span class="st"> </span><span class="cf">function</span>(y, lev) {</a>
<a class="sourceLine" id="cb1498-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1498-3" data-line-number="3">  e =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1498-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> lev) {</a>
<a class="sourceLine" id="cb1498-5" data-line-number="5">    cl.n   =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>k)</a>
<a class="sourceLine" id="cb1498-6" data-line-number="6">    p =<span class="st"> </span>cl.n <span class="op">/</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb1498-7" data-line-number="7">    e =<span class="st"> </span>e <span class="op">+</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span><span class="kw">log</span>(p, <span class="kw">exp</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1498-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb1498-9" data-line-number="9">  <span class="op">-</span>e</a>
<a class="sourceLine" id="cb1498-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb1498-11" data-line-number="11">entropy.gain &lt;-<span class="st"> </span><span class="cf">function</span>(parent, left, right, lev) {</a>
<a class="sourceLine" id="cb1498-12" data-line-number="12">  left.n =<span class="st"> </span><span class="kw">length</span>(left)</a>
<a class="sourceLine" id="cb1498-13" data-line-number="13">  right.n =<span class="st"> </span><span class="kw">length</span>(right)</a>
<a class="sourceLine" id="cb1498-14" data-line-number="14">  total.n =<span class="st"> </span>left.n <span class="op">+</span><span class="st"> </span>right.n</a>
<a class="sourceLine" id="cb1498-15" data-line-number="15">  p.left =<span class="st"> </span>left.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1498-16" data-line-number="16">  p.right =<span class="st"> </span>right.n <span class="op">/</span><span class="st"> </span>total.n</a>
<a class="sourceLine" id="cb1498-17" data-line-number="17">  <span class="kw">entropy</span>(parent, lev) <span class="op">-</span><span class="st"> </span>(p.left <span class="op">*</span><span class="st"> </span><span class="kw">entropy</span>(left, lev)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1498-18" data-line-number="18"><span class="st">                            </span>p.right <span class="op">*</span><span class="st"> </span><span class="kw">entropy</span>(right, lev))</a>
<a class="sourceLine" id="cb1498-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb1498-20" data-line-number="20">y =<span class="st"> </span>iris<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1498-21" data-line-number="21">lev =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1498-22" data-line-number="22"><span class="kw">print</span>(<span class="kw">paste0</span>(<span class="st">&quot;Entropy Gain: &quot;</span>, </a>
<a class="sourceLine" id="cb1498-23" data-line-number="23">             <span class="kw">entropy.gain</span> (y, y[split<span class="op">$</span>left], y[split<span class="op">$</span>right], lev)))</a></code></pre></div>
<pre><code>## [1] &quot;Entropy Gain: 0.38090751345448&quot;</code></pre>

<p><strong>Seventh</strong>, let us now revisit our implementation of <strong>Regression Trees</strong> and modify a few of the functions we used to build a <strong>Regression Tree</strong>. For intuition of the few functions, reference our detailed discussion in the <strong>Regression Trees</strong> Section.</p>
<p>The first function to modify is the <strong>split.loss(.)</strong> function. The modification replaces the loss function that uses the <strong>SSE</strong> measure with the <strong>Gini Index</strong> and <strong>Gini Gain</strong> measure. Also, the minimum bucket applies not only to the tree node but also to the individual classes so that if only one class has observations greater than <strong>minbucket</strong>, then no further split is needed. The measure of <strong>improvement</strong> is measured using the <strong>Gini Gain</strong> of parents and children like so:   </p>
<p><span class="math display">\[\begin{align}
improve = \frac{IG_{(splits)}}{IG_{(parent)}}  
\end{align}\]</span></p>

<div class="sourceCode" id="cb1500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1500-1" data-line-number="1">class.probs &lt;-<span class="st"> </span><span class="cf">function</span>(y) {</a>
<a class="sourceLine" id="cb1500-2" data-line-number="2">  K       =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1500-3" data-line-number="3">  total   =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1500-4" data-line-number="4">  probs   =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, K); i =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1500-5" data-line-number="5">  <span class="cf">for</span> (lev <span class="cf">in</span> levs) { i =<span class="st"> </span>i <span class="op">+</span><span class="st"> </span><span class="dv">1</span>; probs[i] =<span class="st"> </span><span class="kw">sum</span>(y <span class="op">==</span><span class="st"> </span>lev) <span class="op">/</span><span class="st"> </span>total } </a>
<a class="sourceLine" id="cb1500-6" data-line-number="6">  probs</a>
<a class="sourceLine" id="cb1500-7" data-line-number="7">}</a>
<a class="sourceLine" id="cb1500-8" data-line-number="8">split.loss &lt;-<span class="st"> </span><span class="cf">function</span>(loss, sort.input, output, left, right, </a>
<a class="sourceLine" id="cb1500-9" data-line-number="9">                       avg, minbucket ) {</a>
<a class="sourceLine" id="cb1500-10" data-line-number="10">  cs          =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1500-11" data-line-number="11">  n           =<span class="st"> </span><span class="kw">length</span>(output)</a>
<a class="sourceLine" id="cb1500-12" data-line-number="12">  probs       =<span class="st"> </span><span class="kw">class.probs</span>(output)</a>
<a class="sourceLine" id="cb1500-13" data-line-number="13">  parent.gain =<span class="st"> </span><span class="kw">gini</span>(output, levs)</a>
<a class="sourceLine" id="cb1500-14" data-line-number="14">  <span class="cf">if</span> (<span class="kw">length</span>(left) <span class="op">&gt;=</span><span class="st"> </span>minbucket <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(right) <span class="op">&gt;=</span><span class="st"> </span>minbucket ) {</a>
<a class="sourceLine" id="cb1500-15" data-line-number="15">    <span class="co"># no split if only one class has &gt; minbucket</span></a>
<a class="sourceLine" id="cb1500-16" data-line-number="16">    <span class="cf">if</span> (<span class="kw">sum</span>( probs <span class="op">*</span><span class="st"> </span>n <span class="op">&gt;</span><span class="st"> </span>minbucket ) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1500-17" data-line-number="17">      left.idx    =<span class="st"> </span>sort.input<span class="op">$</span>ix[left]</a>
<a class="sourceLine" id="cb1500-18" data-line-number="18">      right.idx   =<span class="st"> </span>sort.input<span class="op">$</span>ix[<span class="op">-</span>left]</a>
<a class="sourceLine" id="cb1500-19" data-line-number="19">      o1 =<span class="st"> </span>output[left.idx]; o2 =<span class="st"> </span>output[right.idx]</a>
<a class="sourceLine" id="cb1500-20" data-line-number="20">      child.probs =<span class="st"> </span><span class="kw">round</span>(probs[<span class="kw">which.max</span>(probs)]<span class="op">*</span><span class="dv">100</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1500-21" data-line-number="21">      child.class =<span class="st"> </span>levs[<span class="kw">which.max</span>(probs)]</a>
<a class="sourceLine" id="cb1500-22" data-line-number="22">      child.gain  =<span class="st"> </span><span class="kw">gain</span>(output, o1, o2, levs)</a>
<a class="sourceLine" id="cb1500-23" data-line-number="23">      child.improve    =<span class="st">  </span>child.gain <span class="op">/</span><span class="st"> </span>parent.gain</a>
<a class="sourceLine" id="cb1500-24" data-line-number="24">      o.len =<span class="st"> </span>n; l.len =<span class="st"> </span><span class="kw">length</span>(o1); r.len =<span class="st"> </span><span class="kw">length</span>(o2)</a>
<a class="sourceLine" id="cb1500-25" data-line-number="25">      cs<span class="op">$</span>split   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>split, avg)</a>
<a class="sourceLine" id="cb1500-26" data-line-number="26">      cs<span class="op">$</span>Pr      =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>Pr, child.probs)</a>
<a class="sourceLine" id="cb1500-27" data-line-number="27">      cs<span class="op">$</span>class   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>class, child.class)</a>
<a class="sourceLine" id="cb1500-28" data-line-number="28">      cs<span class="op">$</span>gain    =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>gain, <span class="kw">round</span>(child.gain,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1500-29" data-line-number="29">      cs<span class="op">$</span>improve =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>improve, <span class="kw">round</span>(child.improve<span class="op">*</span><span class="dv">100</span>,<span class="dv">3</span>))</a>
<a class="sourceLine" id="cb1500-30" data-line-number="30">      cs<span class="op">$</span>obs     =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>obs, o.len);  </a>
<a class="sourceLine" id="cb1500-31" data-line-number="31">      cs<span class="op">$</span>l.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>l.son, l.len); cs<span class="op">$</span>r.son   =<span class="st"> </span><span class="kw">c</span>(cs<span class="op">$</span>r.son, r.len)</a>
<a class="sourceLine" id="cb1500-32" data-line-number="32">      cs<span class="op">$</span>left.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>left.idx</a>
<a class="sourceLine" id="cb1500-33" data-line-number="33">      cs<span class="op">$</span>right.indices[[<span class="kw">as.character</span>(avg)]] =<span class="st"> </span>right.idx</a>
<a class="sourceLine" id="cb1500-34" data-line-number="34">    }</a>
<a class="sourceLine" id="cb1500-35" data-line-number="35">  }</a>
<a class="sourceLine" id="cb1500-36" data-line-number="36">  cs</a>
<a class="sourceLine" id="cb1500-37" data-line-number="37">}</a></code></pre></div>

<p>The second function to modify is our <strong>optimizer</strong> function which references the <strong>minimum.loss(.)</strong> function in <strong>Regression</strong>. Here in <strong>Classification</strong>, we replace the optimizer with <strong>maximum.gain(.)</strong> because our measure is now to maximize the gain.</p>

<div class="sourceCode" id="cb1501"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1501-1" data-line-number="1">maximum.gain &lt;-<span class="st"> </span><span class="cf">function</span>(feature, loss, output, data, factor) {</a>
<a class="sourceLine" id="cb1501-2" data-line-number="2">    cs =<span class="st"> </span>loss</a>
<a class="sourceLine" id="cb1501-3" data-line-number="3">    indices      =<span class="st"> </span>data<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1501-4" data-line-number="4">    data         =<span class="st"> </span>data<span class="op">$</span>dataset[indices,]</a>
<a class="sourceLine" id="cb1501-5" data-line-number="5">    parent.gain =<span class="st"> </span><span class="kw">gini</span>(output, levs)</a>
<a class="sourceLine" id="cb1501-6" data-line-number="6">    <span class="cf">if</span> (<span class="op">!</span><span class="kw">is.null</span>(cs<span class="op">$</span>gain) <span class="op">&amp;&amp;</span><span class="st"> </span><span class="kw">length</span>(output) <span class="op">&gt;</span><span class="st"> </span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb1501-7" data-line-number="7">        max.idx  =<span class="st"> </span><span class="kw">which.max</span>(cs<span class="op">$</span>gain)  </a>
<a class="sourceLine" id="cb1501-8" data-line-number="8">        avg      =<span class="st"> </span><span class="kw">as.character</span>(cs<span class="op">$</span>split[max.idx])</a>
<a class="sourceLine" id="cb1501-9" data-line-number="9">        perc     =<span class="st"> </span><span class="kw">round</span>(cs<span class="op">$</span>obs[max.idx] <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1501-10" data-line-number="10">        lson     =<span class="st"> </span>cs<span class="op">$</span>l.son[max.idx]</a>
<a class="sourceLine" id="cb1501-11" data-line-number="11">        rson     =<span class="st"> </span>cs<span class="op">$</span>r.son[max.idx]</a>
<a class="sourceLine" id="cb1501-12" data-line-number="12">        lindices =<span class="st"> </span>cs<span class="op">$</span>left.indices[[avg]];  lindices  =<span class="st"> </span>indices[lindices]</a>
<a class="sourceLine" id="cb1501-13" data-line-number="13">        rindices =<span class="st"> </span>cs<span class="op">$</span>right.indices[[avg]]; rindices  =<span class="st"> </span>indices[rindices]</a>
<a class="sourceLine" id="cb1501-14" data-line-number="14">        node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span> =<span class="st"> </span>feature,       <span class="st">&quot;split&quot;</span>  =<span class="st"> </span>avg,  </a>
<a class="sourceLine" id="cb1501-15" data-line-number="15">         <span class="st">&quot;obs&quot;</span>           =<span class="st"> </span>cs<span class="op">$</span>obs[max.idx],     <span class="st">&quot;left&quot;</span>  =<span class="st"> </span>lson, </a>
<a class="sourceLine" id="cb1501-16" data-line-number="16">         <span class="st">&quot;right&quot;</span>         =<span class="st"> </span>rson,                  <span class="st">&quot;Pr&quot;</span>  =<span class="st"> </span>cs<span class="op">$</span>Pr[max.idx],       </a>
<a class="sourceLine" id="cb1501-17" data-line-number="17">         <span class="st">&quot;class&quot;</span>         =<span class="st"> </span>cs<span class="op">$</span>class[max.idx],   <span class="st">&quot;gain&quot;</span>  =<span class="st"> </span>cs<span class="op">$</span>gain[max.idx],           </a>
<a class="sourceLine" id="cb1501-18" data-line-number="18">         <span class="st">&quot;improve&quot;</span>       =<span class="st"> </span>cs<span class="op">$</span>improve[max.idx], <span class="st">&quot;perc&quot;</span>          =<span class="st"> </span>perc,</a>
<a class="sourceLine" id="cb1501-19" data-line-number="19">         <span class="st">&quot;left.indices&quot;</span>  =<span class="st"> </span>lindices,            <span class="st">&quot;right.indices&quot;</span> =<span class="st"> </span>rindices,</a>
<a class="sourceLine" id="cb1501-20" data-line-number="20">         <span class="st">&quot;indices&quot;</span>       =<span class="st"> </span>indices,             </a>
<a class="sourceLine" id="cb1501-21" data-line-number="21">         <span class="st">&quot;response&quot;</span>      =<span class="st"> </span><span class="kw">as.character</span>(output),</a>
<a class="sourceLine" id="cb1501-22" data-line-number="22">         <span class="st">&quot;ntype&quot;</span>         =<span class="st"> &quot;node&quot;</span>)</a>
<a class="sourceLine" id="cb1501-23" data-line-number="23">    } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1501-24" data-line-number="24">        probs         =<span class="st"> </span><span class="kw">class.probs</span>(output)</a>
<a class="sourceLine" id="cb1501-25" data-line-number="25">        parent.probs  =<span class="st"> </span><span class="kw">round</span>(probs[<span class="kw">which.max</span>(probs)]<span class="op">*</span><span class="dv">100</span>,<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1501-26" data-line-number="26">        parent.class  =<span class="st"> </span>levs[<span class="kw">which.max</span>(probs)]</a>
<a class="sourceLine" id="cb1501-27" data-line-number="27">        node =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;feature&quot;</span>  =<span class="st">  </span>feature,   <span class="st">&quot;split&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1501-28" data-line-number="28">            <span class="st">&quot;obs&quot;</span>      =<span class="st"> </span><span class="kw">length</span>(output),  <span class="st">&quot;left&quot;</span> =<span class="st"> &quot;.&quot;</span>,</a>
<a class="sourceLine" id="cb1501-29" data-line-number="29">            <span class="st">&quot;right&quot;</span>    =<span class="st"> &quot;.&quot;</span>,               <span class="st">&quot;Pr&quot;</span> =<span class="st"> </span>parent.probs,</a>
<a class="sourceLine" id="cb1501-30" data-line-number="30">            <span class="st">&quot;class&quot;</span>    =<span class="st"> </span>parent.class,    <span class="st">&quot;gain&quot;</span> =<span class="st"> </span><span class="kw">round</span>(parent.gain,<span class="dv">4</span>),</a>
<a class="sourceLine" id="cb1501-31" data-line-number="31">            <span class="st">&quot;improve&quot;</span>  =<span class="st"> &quot;.&quot;</span>, </a>
<a class="sourceLine" id="cb1501-32" data-line-number="32">            <span class="st">&quot;perc&quot;</span>     =<span class="st"> </span><span class="kw">round</span>( <span class="kw">length</span>(output) <span class="op">/</span><span class="st"> </span>sample <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">0</span>),</a>
<a class="sourceLine" id="cb1501-33" data-line-number="33">            <span class="st">&quot;indices&quot;</span>  =<span class="st"> </span>indices,    <span class="st">&quot;response&quot;</span>  =<span class="st"> </span><span class="kw">as.character</span>(output),</a>
<a class="sourceLine" id="cb1501-34" data-line-number="34">            <span class="st">&quot;ntype&quot;</span>   =<span class="st"> &quot;leaf&quot;</span>)</a>
<a class="sourceLine" id="cb1501-35" data-line-number="35">    }</a>
<a class="sourceLine" id="cb1501-36" data-line-number="36">    node</a>
<a class="sourceLine" id="cb1501-37" data-line-number="37">}</a>
<a class="sourceLine" id="cb1501-38" data-line-number="38">optimizer &lt;-<span class="st"> </span>maximum.gain</a></code></pre></div>

<p>The third function to modify is the <strong>split.continuous(.)</strong>, using the <strong>maximum.gain(.)</strong> function above.</p>

<div class="sourceCode" id="cb1502"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1502-1" data-line-number="1">split.continuous &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket) {</a>
<a class="sourceLine" id="cb1502-2" data-line-number="2">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1502-3" data-line-number="3">    <span class="co">### Similar content as split.continuous (...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1502-4" data-line-number="4">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1502-5" data-line-number="5">}</a></code></pre></div>

<p>The fourth function to modify is the <strong>split.categorical(.)</strong>, using the <strong>maximum.gain(.)</strong> function above. This requires the <strong>get.categories(.)</strong> function to derive the full categorical list.</p>

<div class="sourceCode" id="cb1503"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1503-1" data-line-number="1">split.categorical &lt;-<span class="st"> </span><span class="cf">function</span>(loss, feature, target, data, minbucket, </a>
<a class="sourceLine" id="cb1503-2" data-line-number="2">                              categories) {</a>
<a class="sourceLine" id="cb1503-3" data-line-number="3">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1503-4" data-line-number="4">    <span class="co">### Similar content as split.categorical(...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1503-5" data-line-number="5">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1503-6" data-line-number="6">}</a></code></pre></div>

<p>The fifth function to modify is the <strong>split.goodness(.)</strong> function.</p>

<div class="sourceCode" id="cb1504"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1504-1" data-line-number="1">sp.model &lt;-<span class="st"> </span><span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb1504-2" data-line-number="2">    cs =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1504-3" data-line-number="3">    cs<span class="op">$</span>split    =<span class="st"> </span>cs<span class="op">$</span>gain     =<span class="st"> </span>cs<span class="op">$</span>obs   =<span class="st"> </span>cs<span class="op">$</span>improve =<span class="st"> </span><span class="ot">NULL</span> </a>
<a class="sourceLine" id="cb1504-4" data-line-number="4">    cs<span class="op">$</span>Pr       =<span class="st"> </span>cs<span class="op">$</span>class    =<span class="st"> </span>cs<span class="op">$</span>l.son =<span class="st"> </span>cs<span class="op">$</span>r.son   =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1504-5" data-line-number="5">    cs<span class="op">$</span>left.indices =<span class="st"> </span><span class="kw">list</span>(); cs<span class="op">$</span>right.indices =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1504-6" data-line-number="6">    cs</a>
<a class="sourceLine" id="cb1504-7" data-line-number="7">}</a></code></pre></div>
<div class="sourceCode" id="cb1505"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1505-1" data-line-number="1">split.goodness &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">minbucket =</span> <span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1505-2" data-line-number="2">                           categories) {</a>
<a class="sourceLine" id="cb1505-3" data-line-number="3">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1505-4" data-line-number="4">    <span class="co">### Similar content as split.goodness(...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1505-5" data-line-number="5">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1505-6" data-line-number="6">}</a></code></pre></div>

<p>To test the <strong>goodness of split</strong>, we apply the function <strong>split.goodness(.)</strong> like so:</p>

<div class="sourceCode" id="cb1506"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1506-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1506-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1506-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1506-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1506-5" data-line-number="5">sample      =<span class="st"> </span><span class="kw">nrow</span>(datacars)</a>
<a class="sourceLine" id="cb1506-6" data-line-number="6">categories  =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1506-7" data-line-number="7">data      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1506-8" data-line-number="8">ft =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">2</span>, </a>
<a class="sourceLine" id="cb1506-9" data-line-number="9">                    categories)<span class="op">$</span>Petal.Length</a>
<a class="sourceLine" id="cb1506-10" data-line-number="10">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices  =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1506-11" data-line-number="11"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##             feature  split obs left right Pr    class   gain improve
## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150   50   100 33 &quot;setosa&quot; 0.3333      50
##      perc      response  ntype
## [1,]  100 Character,150 &quot;node&quot;</code></pre>

<p>Then finally, the last function to modify is the <strong>rank.importance(.)</strong> function to support the ranking of <strong>improvement</strong> based on <strong>Gini Gain</strong>.</p>

<div class="sourceCode" id="cb1508"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1508-1" data-line-number="1">rank.importance &lt;-<span class="st"> </span><span class="cf">function</span>(features, target,  data, minbucket, categories) {</a>
<a class="sourceLine" id="cb1508-2" data-line-number="2">    goodness =<span class="st"> </span><span class="kw">split.goodness</span>(features, target, data, minbucket, categories)</a>
<a class="sourceLine" id="cb1508-3" data-line-number="3">    ranks =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1508-4" data-line-number="4">    <span class="cf">for</span> (f <span class="cf">in</span> features) {</a>
<a class="sourceLine" id="cb1508-5" data-line-number="5">        r =<span class="st"> </span>goodness[[f]]</a>
<a class="sourceLine" id="cb1508-6" data-line-number="6">        r<span class="op">$</span>left.indices  =<span class="st">  </span>r<span class="op">$</span>right.indices =<span class="st"> </span>r<span class="op">$</span>response =<span class="st"> </span>r<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1508-7" data-line-number="7">        r   =<span class="st"> </span><span class="kw">data.frame</span>(r, <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1508-8" data-line-number="8">        ranks =<span class="st"> </span><span class="kw">rbind</span>(ranks, r)</a>
<a class="sourceLine" id="cb1508-9" data-line-number="9">    }</a>
<a class="sourceLine" id="cb1508-10" data-line-number="10">    ordered.idx =<span class="st"> </span><span class="kw">order</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;improve&quot;</span>)],</a>
<a class="sourceLine" id="cb1508-11" data-line-number="11">                stringr<span class="op">::</span><span class="kw">str_detect</span>(ranks[,<span class="kw">c</span>(<span class="st">&quot;split&quot;</span>)], <span class="st">&quot;[LR-]&quot;</span>),</a>
<a class="sourceLine" id="cb1508-12" data-line-number="12">                <span class="dt">decreasing=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb1508-13" data-line-number="13">    ranks =<span class="st"> </span><span class="kw">data.frame</span>(ranks[ordered.idx,], <span class="dt">stringsAsFactors =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1508-14" data-line-number="14">    feature =<span class="st"> </span>ranks[<span class="dv">1</span>,<span class="kw">c</span>(<span class="st">&quot;feature&quot;</span>)] </a>
<a class="sourceLine" id="cb1508-15" data-line-number="15">    top     =<span class="st">  </span>goodness[[feature]]  </a>
<a class="sourceLine" id="cb1508-16" data-line-number="16">    <span class="kw">list</span>(<span class="st">&quot;ranks&quot;</span> =<span class="st"> </span>ranks, <span class="st">&quot;top&quot;</span> =<span class="st"> </span>top) </a>
<a class="sourceLine" id="cb1508-17" data-line-number="17">}</a></code></pre></div>

<p>Using the modified implementation, we can rank our classification such that we get the top feature to use for the optimal split:</p>

<div class="sourceCode" id="cb1509"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1509-1" data-line-number="1">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">2</span>, categories)</a>
<a class="sourceLine" id="cb1509-2" data-line-number="2">my.rank =<span class="st"> </span><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##        feature split obs   L   R Pr  class  gain improve   % ntyp
## 3 Petal.Length  2.45 150  50 100 33 setosa 0.333   50.00 100 node
## 4  Petal.Width   0.8 150  50 100 33 setosa 0.333   50.00 100 node
## 1 Sepal.Length  5.45 150  52  98 33 setosa 0.228   34.16 100 node
## 2  Sepal.Width  3.35 150 113  37 33 setosa 0.127   19.04 100 node</code></pre>
<div class="sourceCode" id="cb1511"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1511-1" data-line-number="1">ft =<span class="st"> </span>r<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1511-2" data-line-number="2">ft<span class="op">$</span>left.indices =<span class="st"> </span>ft<span class="op">$</span>right.indices =<span class="st"> </span>ft<span class="op">$</span>indices =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1511-3" data-line-number="3"><span class="kw">print</span>(<span class="kw">t</span>(<span class="kw">as.matrix</span>(ft)), <span class="dt">right=</span><span class="ot">TRUE</span>, <span class="dt">quote=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##             feature  split obs left right Pr    class   gain improve
## [1,] &quot;Petal.Length&quot; &quot;2.45&quot; 150   50   100 33 &quot;setosa&quot; 0.3333      50
##      perc      response  ntype
## [1,]  100 Character,150 &quot;node&quot;</code></pre>

<p>To validate, let us print a summary of the result from our previous tree.model:</p>

<div class="sourceCode" id="cb1513"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1513-1" data-line-number="1">summ =<span class="st"> </span><span class="kw">capture.output</span>(<span class="kw">summary</span>(tree.model))</a></code></pre></div>
<div class="sourceCode" id="cb1514"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1514-1" data-line-number="1"><span class="kw">show.node</span>(summ,<span class="dv">1</span>, <span class="dt">lnum=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## Node number 1: 150 observations,    complexity param=0.5
##   predicted class=setosa      expected loss=0.6667  P(node) =1
##     class counts:    50    50    50
##    probabilities: 0.333 0.333 0.333 
##   left son=2 (50 obs) right son=3 (100 obs)
##   Primary splits:
##       Petal.Length &lt; 2.45 to the left,  improve=50.00, (0 missing)
##       Petal.Width  &lt; 0.8  to the left,  improve=50.00, (0 missing)
##       Sepal.Length &lt; 5.45 to the left,  improve=34.16, (0 missing)
##       Sepal.Width  &lt; 3.35 to the right, improve=19.04, (0 missing)
##   Surrogate splits:
##       Petal.Width  &lt; 0.8  to the left,  agree=1.000, adj=1.00, (0 split)
##       Sepal.Length &lt; 5.45 to the left,  agree=0.920, adj=0.76, (0 split)
##       Sepal.Width  &lt; 3.35 to the right, agree=0.833, adj=0.50, (0 split)</code></pre>

<p>Let us now implement a new base learner for our classification tree. Here, we use <strong>my.regression.tree(.)</strong> with no modification. We only rename the function as <strong>my.classification.tree(.)</strong>, but uses <strong>rank.importance(.)</strong> as modified above for classification.</p>

<div class="sourceCode" id="cb1516"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1516-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1516-2" data-line-number="2">my.classification.tree &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, dataset,  <span class="dt">minbucket =</span> <span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1516-3" data-line-number="3">                                   <span class="dt">maxdepth=</span><span class="dv">50</span>) {</a>
<a class="sourceLine" id="cb1516-4" data-line-number="4">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1516-5" data-line-number="5">    <span class="co">### Similar content as my.regression.tree (...) in Regression Tree</span></a>
<a class="sourceLine" id="cb1516-6" data-line-number="6">    <span class="co">#################################################################</span></a>
<a class="sourceLine" id="cb1516-7" data-line-number="7">}</a></code></pre></div>
<div class="sourceCode" id="cb1517"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1517-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1517-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1517-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1517-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1517-5" data-line-number="5">my.model =<span class="st"> </span><span class="kw">h.learner</span>(features, target, datacars, <span class="dt">minbucket=</span><span class="dv">2</span>, <span class="dt">maxdepth=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1517-6" data-line-number="6"><span class="co"># See Regression Tree Section for my.table.tree(.)</span></a>
<a class="sourceLine" id="cb1517-7" data-line-number="7">my.tree.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model, <span class="dt">display_mode=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##   N P      feature split obs  L   R  Pr      class  gain improv perc
## 1 1 0 Petal.Length  2.45 150 50 100  33     setosa 0.333     50  100
## 2 2 1       &lt;leaf&gt;     .  50  .   . 100     setosa 0.000      .   33
## 3 3 1  Petal.Width  1.75 100 54  46  50 versicolor 0.390 77.939   67
## 4 4 3 Petal.Length  4.95  54 48   6  91 versicolor 0.082 49.031   36
## 5 5 3       &lt;leaf&gt;     .  46  .   .  98  virginica 0.042      .   31
## 6 6 4       &lt;leaf&gt;     .  48  .   .  98 versicolor 0.041      .   32
## 7 7 4       &lt;leaf&gt;     .   6  .   .  67  virginica 0.444      .    4</code></pre>

<p>In terms of prediction, one can construct the same prediction function, namely <strong>my.predict(.)</strong>, which we implemented for <strong>Regression Trees</strong>. A modified version of the function is implemented in a section covering <strong>AdaBoost</strong> for classification.</p>
</div>
<div id="ensemble-methods-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.3</span> Ensemble Methods <a href="machinelearning2.html#ensemble-methods-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Using <strong>Classification Trees</strong>, similar to <strong>Regression</strong>, we have ensemble methods available for classification, namely <strong>Random Forest</strong>, <strong>Adaboost</strong>, and <strong>Gradient Boost</strong>. For a review of the methods, recall the use of <strong>MSE</strong> for regression to evaluate individual trees of an ensemble. For classification, in our case, we can reference a <strong>Confusion Matrix</strong> to evaluate models based on <strong>specificity</strong>, <strong>sensitivity</strong>, <strong>accuracy</strong>, <strong>F1 score</strong>, and others.</p>
<p>To illustrate, let us continue to use the <strong>iris</strong> train set and test set.</p>

<div class="sourceCode" id="cb1519"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1519-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb1519-2" data-line-number="2">features     =<span class="st"> </span><span class="kw">names</span>(iris)[<span class="kw">which</span>(<span class="op">!</span><span class="kw">names</span>(iris) <span class="op">%in%</span><span class="st"> &quot;Species&quot;</span>)]  </a>
<a class="sourceLine" id="cb1519-3" data-line-number="3">target       =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1519-4" data-line-number="4">datacars     =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1519-5" data-line-number="5">fold.indices =<span class="st"> </span><span class="kw">createFolds</span>(datacars<span class="op">$</span>Species, <span class="dt">k =</span> <span class="dv">10</span>, <span class="dt">returnTrain =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb1519-6" data-line-number="6"><span class="co"># choose the first fold for our test group.</span></a>
<a class="sourceLine" id="cb1519-7" data-line-number="7">test =<span class="st"> </span>datacars[fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1519-8" data-line-number="8"><span class="co"># choose the other folds for training group.</span></a>
<a class="sourceLine" id="cb1519-9" data-line-number="9">train =<span class="st"> </span>datacars[<span class="op">-</span>fold.indices<span class="op">$</span>Fold01,]</a>
<a class="sourceLine" id="cb1519-10" data-line-number="10"><span class="co"># test target</span></a>
<a class="sourceLine" id="cb1519-11" data-line-number="11">test.class =<span class="st"> </span>test<span class="op">$</span>Species</a>
<a class="sourceLine" id="cb1519-12" data-line-number="12">test<span class="op">$</span>Species =<span class="st"> </span><span class="ot">NULL</span></a></code></pre></div>

<p>Now, recall our discussion of ensemble methods under <strong>Ensemble Methods</strong> Subsection under <strong>Regression</strong> Section. The concept of <strong>bagging</strong> and <strong>boosting</strong> remains the same for <strong>classification</strong>. Learning a tree model by random sampling - so-called <strong>bootstrapping</strong> - and then testing using out of bag applies to <strong>classification</strong> also. Similarly, learning a tree model by <strong>boosting</strong> weak learners, e.g., <strong>AdaBoost</strong>, or using the gradient method to speed up the learning process, e.g., <strong>Gradient Boost</strong>, also applies to <strong>classification</strong>.</p>
<p>The next few sections review how ensemble methods use the train set and test set for classification. Note that details of the ensemble methods are already covered in <strong>regression</strong>; thus, the following sections complement the intuition with the use of 3rd-party R packages and emphasize the classification performance of the methods.</p>
</div>
<div id="random-forest-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.4</span> Random Forest <a href="machinelearning2.html#random-forest-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We reference <strong>Random Forest</strong> Subsection under <strong>Regression</strong> Section for a detailed intuition of <strong>Bagging</strong> using <strong>Random Forest</strong> as our <strong>Ensemble</strong> algorithm. With modification to our implementation, we replace the <strong>regressor</strong> with our own implementation of <strong>Random Forest classifier</strong>. For example, we use both <strong>my.random.forest(.)</strong> and <strong>my.rf.tree(.)</strong> functions; however, content of <strong>build.level.tree(.)</strong> uses a classifier, namely <strong>my.classification.tree(.)</strong>.</p>
<p>In this section, we extend our discussion of <strong>Random Forest</strong> by covering the modelâs performance. Using our train set and test set, we learn the model like so:</p>
<div class="sourceCode" id="cb1520"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1520-1" data-line-number="1"><span class="kw">library</span>(randomForest)</a>
<a class="sourceLine" id="cb1520-2" data-line-number="2">ntree =<span class="st"> </span><span class="dv">5</span></a>
<a class="sourceLine" id="cb1520-3" data-line-number="3">rf.model =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1520-4" data-line-number="4">                        <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1520-5" data-line-number="5">rf.model<span class="op">$</span>confusion</a></code></pre></div>
<pre><code>##            setosa versicolor virginica class.error
## setosa         43          0         0      0.0000
## versicolor      0         36         5      0.1220
## virginica       0          5        31      0.1389</code></pre>
<p>Based on the model, our confusion matrix above indicates that the model fits well for setosa class, given a classification error of 0. The virginica class shows a 0.1389 classification error.</p>
<p>Then, we plot the <strong>variable importance</strong>. It is worth mentioning that if the dataset is for regression, the importance of a variable is measured based on the increase of <strong>Node Impurity</strong> denoted by <strong>IncNodePurity</strong>; however, for classification, it is measured based on the mean decrease of <strong>Gini Index</strong> and <strong>Accuracy</strong> denoted by <strong>MeanDecreaseGini</strong> and <strong>MeanDecreaseAccuracy</strong> respectively.</p>
<div class="sourceCode" id="cb1522"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1522-1" data-line-number="1"><span class="kw">cbind</span>(<span class="kw">importance</span>(rf.model), <span class="st">&quot;varUsed&quot;</span> =<span class="st"> </span><span class="kw">varUsed</span>(rf.model))</a></code></pre></div>
<pre><code>##              setosa versicolor virginica MeanDecreaseAccuracy
## Sepal.Length  0.000    -0.3128    0.8773              1.02317
## Sepal.Width   0.000    -1.1180    1.1180             -0.08649
## Petal.Length  1.118     3.1824    1.6423              2.03300
## Petal.Width   4.372     3.7990    3.8253              4.58091
##              MeanDecreaseGini varUsed
## Sepal.Length           11.602      11
## Sepal.Width             1.091       4
## Petal.Length           17.058       9
## Petal.Width            59.251      17</code></pre>
<div class="sourceCode" id="cb1524"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1524-1" data-line-number="1"><span class="kw">varImpPlot</span>(rf.model)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:varImpPlot"></span>
<img src="DS_files/figure-html/varImpPlot-1.png" alt="Variable Importance" width="70%" />
<p class="caption">
Figure 10.31: Variable Importance
</p>
</div>
<p>It also helps to mention that the measures above explain how variables may contribute in importance the lower the values. In practice, however, the interpretation of such values gets lost in the complexity of how <strong>Random Forest</strong> builds trees, especially if we deal with thousands of trees.</p>
<p>Instead, we can look into the <strong>out-of-bag</strong> error. Note that it is not necessary for us to have a separate train set and test set because we are setting aside a portion of the dataset (called <strong>out-of-bag</strong>) for validation. We can use the entire dataset to fit our model while, internally, the algorithm sets aside an <strong>out-of-bag</strong> set to test the model. The error from the test is called the <strong>out-of-bag</strong> error rate or <strong>out-of-bag</strong> estimate. It reflects an estimate of what a prediction error may look like should we predict a class given new data.</p>
<div class="sourceCode" id="cb1525"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1525-1" data-line-number="1">rf.model<span class="op">$</span>err.rate</a></code></pre></div>
<pre><code>##          OOB setosa versicolor virginica
## [1,] 0.02381      0    0.06667    0.0000
## [2,] 0.10667      0    0.12500    0.1923
## [3,] 0.13000      0    0.20588    0.1875
## [4,] 0.11504      0    0.20000    0.1471
## [5,] 0.08333      0    0.12195    0.1389</code></pre>
<p>We also use <strong>OOB error rate</strong> to evaluate the performance of different configurations of our random Tree, e.g., using different <strong>mtry</strong> - this is the number of variables to try and test at split when building our random forest tree.</p>

<div class="sourceCode" id="cb1527"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1527-1" data-line-number="1">rf.model1 =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1527-2" data-line-number="2">                         <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1527-3" data-line-number="3">rf.model2 =<span class="st"> </span><span class="kw">randomForest</span>(Species <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> train, <span class="dt">importance=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb1527-4" data-line-number="4">                         <span class="dt">ntree=</span>ntree, <span class="dt">mtry=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb1527-5" data-line-number="5">(<span class="dt">OOB.rates =</span> <span class="kw">cbind</span>(<span class="st">&quot;OOB (mtry=1)&quot;</span> =<span class="st"> </span>rf.model1<span class="op">$</span>err.rate[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb1527-6" data-line-number="6">      <span class="st">&quot;OOB (mtry=2)&quot;</span> =<span class="st"> </span>rf.model<span class="op">$</span>err.rate[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb1527-7" data-line-number="7">      <span class="st">&quot;OOB (mtry=3)&quot;</span> =<span class="st"> </span>rf.model2<span class="op">$</span>err.rate[,<span class="dv">1</span>]))</a></code></pre></div>
<pre><code>##      OOB (mtry=1) OOB (mtry=2) OOB (mtry=3)
## [1,]      0.06667      0.02381      0.06000
## [2,]      0.08974      0.10667      0.04706
## [3,]      0.10526      0.13000      0.06000
## [4,]      0.10619      0.11504      0.05405
## [5,]      0.09677      0.08333      0.06667</code></pre>
<div class="sourceCode" id="cb1529"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1529-1" data-line-number="1">min.rate.idx =<span class="st"> </span><span class="kw">which.min</span>(OOB.rates[ntree,])</a></code></pre></div>

<p>It shows that tuning the tree with 3 variable(s) (e.g.Â mtry=3) renders the least error at 6.67%.</p>
<p>Lastly, to complement our evaluation, we can still split our dataset and use a separate test set to measure the performance of predictions. Here, we make a prediction and generate the AUC score using the performance(.) function from the ROCR library. For prediction using our test set, we use the following code:</p>
<div class="sourceCode" id="cb1530"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1530-1" data-line-number="1">test.pred  =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(rf.model, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&quot;response&quot;</span>)</a>
<a class="sourceLine" id="cb1530-2" data-line-number="2"><span class="kw">table</span>(<span class="dt">observed=</span>test.class,<span class="dt">predicted=</span>test.pred)</a></code></pre></div>
<pre><code>##             predicted
## observed     setosa versicolor virginica
##   setosa          5          0         0
##   versicolor      0          5         0
##   virginica       0          0         5</code></pre>
<p>We then evaluate the prediction performance by plotting and reviewing the <strong>AUC</strong>. See Figure <a href="machinelearning2.html#fig:rfroc">10.32</a>.</p>

<div class="sourceCode" id="cb1532"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1532-1" data-line-number="1"><span class="kw">library</span>(ROCR)</a>
<a class="sourceLine" id="cb1532-2" data-line-number="2">color =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;blue&quot;</span>, <span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb1532-3" data-line-number="3">test.votes   =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(rf.model, <span class="dt">newdata=</span>test, <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)</a>
<a class="sourceLine" id="cb1532-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb1532-5" data-line-number="5">      <span class="dt">xlab=</span><span class="st">&quot;False Positive Rate&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;True Positive Rate&quot;</span>, </a>
<a class="sourceLine" id="cb1532-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;AUC (Random Forest Performance)&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1532-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1532-8" data-line-number="8"><span class="kw">abline</span>( <span class="dt">a=</span><span class="dv">0</span>, <span class="dt">b=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb1532-9" data-line-number="9">auc =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(levs))</a>
<a class="sourceLine" id="cb1532-10" data-line-number="10"><span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(levs)) {</a>
<a class="sourceLine" id="cb1532-11" data-line-number="11">  truth   =<span class="st"> </span><span class="kw">ifelse</span>(test.class <span class="op">==</span><span class="st"> </span>levs[k], <span class="dv">1</span>, <span class="dv">0</span>) </a>
<a class="sourceLine" id="cb1532-12" data-line-number="12">  pred    =<span class="st"> </span><span class="kw">prediction</span>(test.votes[,k], truth)</a>
<a class="sourceLine" id="cb1532-13" data-line-number="13">  perf    =<span class="st"> </span><span class="kw">performance</span>(pred, <span class="st">&quot;tpr&quot;</span>, <span class="st">&quot;fpr&quot;</span>)</a>
<a class="sourceLine" id="cb1532-14" data-line-number="14">  auc[k]  =<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)<span class="op">@</span>y.values</a>
<a class="sourceLine" id="cb1532-15" data-line-number="15">  <span class="kw">plot</span>(perf, <span class="dt">col=</span>color[k], <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1532-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb1532-17" data-line-number="17"><span class="kw">legend</span>(<span class="fl">0.38</span>, <span class="fl">0.28</span>, <span class="dt">legend=</span><span class="kw">c</span>(  </a>
<a class="sourceLine" id="cb1532-18" data-line-number="18">      <span class="kw">paste0</span>(levs[<span class="dv">1</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">1</span>],<span class="st">&quot;)&quot;</span>), </a>
<a class="sourceLine" id="cb1532-19" data-line-number="19">      <span class="kw">paste0</span>(levs[<span class="dv">2</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">2</span>],<span class="st">&quot;)&quot;</span>),</a>
<a class="sourceLine" id="cb1532-20" data-line-number="20">      <span class="kw">paste0</span>(levs[<span class="dv">3</span>],<span class="st">&quot; (auc=&quot;</span>,auc[<span class="dv">3</span>],<span class="st">&quot;)&quot;</span>) ), </a>
<a class="sourceLine" id="cb1532-21" data-line-number="21">    <span class="dt">col=</span>color,  <span class="dt">pch=</span><span class="dv">20</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb1532-22" data-line-number="22"><span class="kw">legend</span>(<span class="fl">0.65</span>, <span class="fl">0.70</span>, </a>
<a class="sourceLine" id="cb1532-23" data-line-number="23">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;0.90 - 1.00 = excellent&quot;</span>,  <span class="st">&quot;0.80 - 0.90 = good&quot;</span>, </a>
<a class="sourceLine" id="cb1532-24" data-line-number="24">              <span class="st">&quot;0.70 - 0.80 = fair&quot;</span>,   <span class="st">&quot;0.60 - 0.70 = poor&quot;</span>,</a>
<a class="sourceLine" id="cb1532-25" data-line-number="25">              <span class="st">&quot;0.50 - 0.60 = fail&quot;</span> ),</a>
<a class="sourceLine" id="cb1532-26" data-line-number="26">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">16</span>,<span class="dv">16</span>,<span class="dv">16</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rfroc"></span>
<img src="DS_files/figure-html/rfroc-1.png" alt="AUC (Random Forest Performance)" width="70%" />
<p class="caption">
Figure 10.32: AUC (Random Forest Performance)
</p>
</div>

<p>The figure shows that the model predicts excellently, having <strong>AUC</strong> values within the range 0.90 - 1.00.</p>
</div>
<div id="AdaBoost" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.5</span> AdaBoost &amp; SAMME<a href="machinelearning2.html#AdaBoost" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue to discuss <strong>classification</strong> using one of the <strong>Ensemble</strong> methods called <strong>Boosting</strong>. Our next introductory classification algorithm is <strong>AdaBoost</strong> which we also covered under <strong>AdaBoost</strong> Subsection under <strong>Regression</strong> Section.  </p>
<p>Recall in <strong>AdaBoost Regression</strong> the introduction of <strong>AdaBoost.R2</strong> <span class="citation">(Drucker H. <a href="bibliography.html#ref-ref589h">1997</a>)</span>. For <strong>Regression</strong>, we also can use <strong>SAMME.R</strong> as an alternative algorithm which is not covered under <strong>Regression</strong> in <strong>Computational Learning I</strong>; however, in this section, we discuss <strong>SAMME</strong> for classification instead.</p>
<p>Also, we introduce <strong>AdaBoost.M2 (adam2)</strong> for classification. Though we do not cover <strong>AdaBoost.M1 (adam1)</strong> in this section, it helps to be aware of such precursors.</p>
<p>Here, we compare the two multi-classification algorithms, namely <strong>AdaBoost.M2</strong> and <strong>SAMME</strong>. Below is the <strong>AdaBoost.M2</strong> algorithm <span class="citation">(Yoav Freund, Robert E Schapire <a href="bibliography.html#ref-ref616y">1996</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{Let} B = \{{(i, y)}:i\ \in\ \{1,...,n\}, y \ne y_i\}\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ weights: D^{(0)}_i = \frac{1}{n},\ i=1,2,...,n\ \ \ \ \ \ \ \text{(initialize distribution)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ \ S^{(m)} \leftarrow \text{Resample from original training set } X^{(0)} \text{ with }D^{(m)}\\
\ \ \ \ \ \ \ h^{(m)}\  \leftarrow \text{Train a weak classifier on } S^{(m)} \text{ then }\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{calculate the pseudo-loss of }  h^{(m)}: \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \epsilon^{(m)} = \frac{1}{2} \sum_{(i,y) \in B}^n D^{(m)}_{(i,y)} \left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\right)\\
\ \ \ \ \ \ \ \text{Compute contribution for this classifier}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \beta^{(m)} =   \epsilon^{(m)}/(1 - \epsilon^{(m)}) 
    \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  \text{(Voting Power)}\\
\ \ \ \ \ \ \ \text{Update weights on training points}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \alpha^{(m)} = (1/2)  \left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\right)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ D^{(m+1)}_{(i,y)} \leftarrow D^{(m)}_{(i,y)} \cdot (\beta^{(m)})^{\alpha^{(m)}}, i=1,2,...,n\\
\ \ \ \ \ \ \ \text{Normalize weights such that } \sum_{i=1}^n D^{(m+1)}_{(i,y)} = 1 \\
\ \ \ \text{end loop} \\
\ \ \ \text{Output }H(\mathbf{x}) = \text{arg}\ \underset{y \in Y}{\text{max}}\left(\sum_{m=1}^M \log_e \frac{1}{\beta^{(m)}}h^{(m)}(x, y)\right)
\end{array} 
\]</span>
</p>
<p>Moreover, below is the <strong>SAMME</strong> algorithm. The name is an acronym for <strong>S</strong>tagewise <strong>A</strong>dditive <strong>M</strong>odeling Using <strong>M</strong>ulti-class <strong>E</strong>xponential loss function. It is also a multi-classification algorithm <span class="citation">(Ji Zhu et al. <a href="bibliography.html#ref-ref602j">2006</a>)</span>.</p>

<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\ 
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ weights: w^{(0)}_i = \frac{1}{n},\ i=1,2,...,n \ \ \ \ \ \ \ \ \text{(initialize equal weights)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ \ S^{(m)} \leftarrow \text{Resample from original training set } X^{(0)} \text{ with }w^{(m)}\\
\ \ \ \ \ \ \ h^{(m)}\  \leftarrow \text{Train a weak classifier on } S^{(m)} \text{ then }\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{compute the following pseudo-loss of } h^{(m)}: \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \epsilon^{(m)} = 
    \sum_{i=1}^n w^{(m)}_i \mathbf{1}\{h^{(m)}(x_i) \ne y_i \} / \sum_{i=1}^n w_i^{(m)}\\
\ \ \ \ \ \ \ \text{Compute contribution for this classifier}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \beta^{(m)} = 
     \log_e \frac{1 - \epsilon^{(m)}}{\epsilon^{(m)}} + \log_e(K-1)
           \ \ \ \ \ \ \ \ \ \ \  \text{(Voting Power)}\\
\ \ \ \ \ \ \ \text{Update weights on training points}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \alpha^{(m)} = \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) \ne y_i \}) \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ w^{(m+1)}_i \leftarrow (w^{(m)}_i)^{\alpha^{(m)}}, i=1,2,...,n\\
\ \ \ \ \ \ \ \text{Normalize weights such that } \sum_{i=1}^n w^{(m+1)}_i = 1 \\
\ \ \ \text{end loop} \\
\ \ \ \text{Output }H(\mathbf{x}) = arg\ \underset{k}{max}\left(\sum_{m=1}^M \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) = k \}\right)
\end{array} 
\]</span>
</p>
<p>One important note is that a base learnerâs choice for <strong>h(x)</strong> is allowed for both algorithms. Any base learner classifier is supported, namely, Decision Trees, SVM, Logistic Regression, and so on. The idea is to use the base learner to fit models that are not required to be well fitted - hence, a weak fit - and we use our algorithm to <strong>boost</strong> for a better fit. In our case, we use our <strong>Classification Tree</strong> that we previously implemented and introduced as our base learner.</p>
<p><strong>First</strong>, we start with the idea that <strong>Boosting</strong> for <strong>Classification Trees</strong> uses weak base learners called <strong>Stumps</strong>. A <strong>stump</strong> is a tree with only one split and can be built using our classifier as an illustration. Note that our classifier uses <strong>Gini Index</strong> and <strong>Gini Gain</strong> to evaluate <strong>goodness of split</strong> for a <strong>stump</strong>. Alternatively, we can use <strong>Entropy</strong> and <strong>Entropy Gain</strong>, detailed under the <strong>Classification Trees</strong> section. </p>
<div class="sourceCode" id="cb1533"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1533-1" data-line-number="1"><span class="kw">library</span>(dequer)</a>
<a class="sourceLine" id="cb1533-2" data-line-number="2">h.learner =<span class="st"> </span>my.classification.tree <span class="co"># our base learner classifier</span></a></code></pre></div>
<p>For example, we write the following code to generate a stump considering only the <strong>Petal.Length</strong> features using the <strong>iris</strong> dataset:</p>

<div class="sourceCode" id="cb1534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1534-1" data-line-number="1">my.petal.stump =<span class="st"> </span><span class="kw">h.learner</span>(<span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Species&quot;</span>, train, <span class="dt">minbucket=</span><span class="dv">1</span>, </a>
<a class="sourceLine" id="cb1534-2" data-line-number="2">                           <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1534-3" data-line-number="3">my.tree.model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.petal.stump<span class="op">$</span>model, <span class="dt">display_mode=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>##   N P      feature split obs  L  R  Pr      class  gain improve perc
## 1 1 0 Petal.Length  2.45 135 45 90  33     setosa 0.333      50   90
## 2 2 1       &lt;leaf&gt;     .  45  .  . 100     setosa 0.000       .   30
## 3 3 1       &lt;leaf&gt;     .  90 40 50  50 versicolor 0.361    72.2   60</code></pre>

<p>In terms of ranking the top feature to use for the split of a <strong>stump</strong>, we use the same <strong>rank.importance(.)</strong> function:</p>

<div class="sourceCode" id="cb1536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1536-1" data-line-number="1">features    =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1536-2" data-line-number="2">target      =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1536-3" data-line-number="3">levs        =<span class="st"> </span><span class="kw">levels</span>(iris[,target])</a>
<a class="sourceLine" id="cb1536-4" data-line-number="4">datacars    =<span class="st"> </span>iris</a>
<a class="sourceLine" id="cb1536-5" data-line-number="5">categories  =<span class="st"> </span><span class="kw">get.categories</span>(target, datacars)</a>
<a class="sourceLine" id="cb1536-6" data-line-number="6">data      =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;indices&quot;</span> =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample), <span class="st">&quot;dataset&quot;</span> =<span class="st"> </span>datacars)</a>
<a class="sourceLine" id="cb1536-7" data-line-number="7">r =<span class="st"> </span><span class="kw">rank.importance</span>(features, target, data, <span class="dt">minbucket=</span><span class="dv">1</span>, categories)</a>
<a class="sourceLine" id="cb1536-8" data-line-number="8">my.rank =<span class="st"> </span><span class="kw">as.data.frame</span>(r<span class="op">$</span>ranks)</a></code></pre></div>
<pre><code>##        feature split obs   L   R Pr  class  gain improve perc ntyp
## 3 Petal.Length  2.45 150  50 100 33 setosa 0.333   50.00  100 node
## 4  Petal.Width   0.8 150  50 100 33 setosa 0.333   50.00  100 node
## 1 Sepal.Length  5.45 150  52  98 33 setosa 0.228   34.16  100 node
## 2  Sepal.Width  3.35 150 113  37 33 setosa 0.127   19.04  100 node</code></pre>

<p>The result shows <strong>Petal.length</strong> feature as the top choice based on the <strong>Gini Gain</strong> metrics. Alternatively, we can use the <strong>improve</strong> metrics.</p>
<p><strong>Second</strong>, for <strong>prediction</strong>, we continue to use <strong>my.predict(.)</strong> with slight modification. Here, we expect a comparison of <strong>categorical</strong> output between the estimate and true value.</p>

<div class="sourceCode" id="cb1538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1538-1" data-line-number="1">prediction.score &lt;-<span class="st"> </span><span class="cf">function</span>(y, y.hat) {</a>
<a class="sourceLine" id="cb1538-2" data-line-number="2">  result =<span class="st"> </span><span class="kw">ifelse</span>(y <span class="op">==</span><span class="st"> </span>y.hat, <span class="dv">1</span>, <span class="dv">-1</span>)</a>
<a class="sourceLine" id="cb1538-3" data-line-number="3">  <span class="kw">list</span>(<span class="st">&quot;fitted.values&quot;</span> =<span class="st"> </span>y.hat, <span class="st">&quot;result&quot;</span> =<span class="st"> </span>result)</a>
<a class="sourceLine" id="cb1538-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb1538-5" data-line-number="5">my.predict  &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, x, y,  <span class="dt">resid =</span> <span class="ot">NULL</span>,</a>
<a class="sourceLine" id="cb1538-6" data-line-number="6">                        <span class="dt">tendency =</span> <span class="cf">function</span>(top, <span class="dt">resid =</span> <span class="ot">NULL</span>) { top<span class="op">$</span>class },</a>
<a class="sourceLine" id="cb1538-7" data-line-number="7">                        <span class="dt">method   =</span> prediction.score, <span class="dt">tabletree =</span> <span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb1538-8" data-line-number="8">  n          =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1538-9" data-line-number="9">  responses  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1538-10" data-line-number="10">  model      =<span class="st"> </span>my.model<span class="op">$</span>model</a>
<a class="sourceLine" id="cb1538-11" data-line-number="11">  categories =<span class="st"> </span>my.model<span class="op">$</span>categories</a>
<a class="sourceLine" id="cb1538-12" data-line-number="12">  <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) { model =<span class="st"> </span><span class="kw">my.table.tree</span>(my.model<span class="op">$</span>model) }</a>
<a class="sourceLine" id="cb1538-13" data-line-number="13">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1538-14" data-line-number="14">    node =<span class="st"> </span>model[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb1538-15" data-line-number="15">    <span class="cf">while</span> (<span class="ot">TRUE</span>) {</a>
<a class="sourceLine" id="cb1538-16" data-line-number="16">      <span class="cf">if</span> (node<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) {</a>
<a class="sourceLine" id="cb1538-17" data-line-number="17">          <span class="cf">if</span> (tabletree <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span> ) { </a>
<a class="sourceLine" id="cb1538-18" data-line-number="18">            top          =<span class="st"> </span>my.model<span class="op">$</span>model[[node<span class="op">$</span>N]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1538-19" data-line-number="19">            responses[i] =<span class="st"> </span><span class="kw">tendency</span>( top, resid )</a>
<a class="sourceLine" id="cb1538-20" data-line-number="20">          } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1538-21" data-line-number="21">            responses[i] =<span class="st"> </span>node<span class="op">$</span>class</a>
<a class="sourceLine" id="cb1538-22" data-line-number="22">          }</a>
<a class="sourceLine" id="cb1538-23" data-line-number="23">          <span class="cf">break</span></a>
<a class="sourceLine" id="cb1538-24" data-line-number="24">      }</a>
<a class="sourceLine" id="cb1538-25" data-line-number="25">      children    =<span class="st"> </span>model[<span class="kw">which</span>( model<span class="op">$</span>P <span class="op">==</span><span class="st"> </span>node<span class="op">$</span>N ),]</a>
<a class="sourceLine" id="cb1538-26" data-line-number="26">      cat         =<span class="st"> </span>categories[[node<span class="op">$</span>feature]]</a>
<a class="sourceLine" id="cb1538-27" data-line-number="27">      split       =<span class="st"> </span>node<span class="op">$</span>split</a>
<a class="sourceLine" id="cb1538-28" data-line-number="28">      val         =<span class="st"> </span>x[i, <span class="kw">c</span>(node<span class="op">$</span>feature)]</a>
<a class="sourceLine" id="cb1538-29" data-line-number="29">      <span class="cf">if</span> (<span class="kw">is.factor</span>(val)) {</a>
<a class="sourceLine" id="cb1538-30" data-line-number="30">        s         =<span class="st"> </span>base<span class="op">::</span><span class="kw">strsplit</span>(split,<span class="ot">NULL</span>)[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb1538-31" data-line-number="31">        direction =<span class="st"> </span>s[<span class="kw">which</span>(cat <span class="op">%in%</span><span class="st"> </span>val )]</a>
<a class="sourceLine" id="cb1538-32" data-line-number="32">        direction =<span class="st"> </span><span class="kw">ifelse</span>(direction <span class="op">==</span><span class="st"> &#39;L&#39;</span>, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1538-33" data-line-number="33">      } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb1538-34" data-line-number="34">        split     =<span class="st"> </span><span class="kw">as.numeric</span>(split)</a>
<a class="sourceLine" id="cb1538-35" data-line-number="35">        val       =<span class="st"> </span><span class="kw">as.numeric</span>(val)</a>
<a class="sourceLine" id="cb1538-36" data-line-number="36">        direction =<span class="st"> </span><span class="kw">ifelse</span>(val <span class="op">&lt;</span><span class="st"> </span>split, <span class="dv">1</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1538-37" data-line-number="37">      }</a>
<a class="sourceLine" id="cb1538-38" data-line-number="38">      node =<span class="st"> </span>children[direction,]</a>
<a class="sourceLine" id="cb1538-39" data-line-number="39">    }</a>
<a class="sourceLine" id="cb1538-40" data-line-number="40">  }</a>
<a class="sourceLine" id="cb1538-41" data-line-number="41">  <span class="kw">method</span>(y,  responses) </a>
<a class="sourceLine" id="cb1538-42" data-line-number="42">}</a>
<a class="sourceLine" id="cb1538-43" data-line-number="43">h.score =<span class="st"> </span>my.predict </a></code></pre></div>

<p>To test our <strong>prediction</strong> function, we use one of our stump trees like so:</p>

<div class="sourceCode" id="cb1539"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1539-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1539-2" data-line-number="2"><span class="kw">h.score</span>(my.petal.stump, test, test.class)</a></code></pre></div>
<pre><code>## $fitted.values
##  [1] &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;    
##  [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## [11] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## 
## $result
##  [1]  1  1  1  1  1  1  1  1  1  1 -1 -1 -1 -1 -1</code></pre>

<p><strong>Third</strong>, we also use <strong>sampling.distribution(.)</strong>. Here, we pick a new set of samples from our original dataset; but the sample is a weighted sample based on the calculated distribution.</p>

<div class="sourceCode" id="cb1541"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1541-1" data-line-number="1">sampling.distribution &lt;-<span class="st"> </span><span class="cf">function</span>(sample.set, sample.weight, <span class="dt">seed=</span><span class="dv">142</span>) {</a>
<a class="sourceLine" id="cb1541-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">nrow</span>(sample.set)</a>
<a class="sourceLine" id="cb1541-3" data-line-number="3">  indices =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, n)</a>
<a class="sourceLine" id="cb1541-4" data-line-number="4">  cumulative.weight =<span class="st"> </span><span class="kw">cumsum</span>(sample.weight) <span class="co"># Distribution</span></a>
<a class="sourceLine" id="cb1541-5" data-line-number="5">  <span class="kw">set.seed</span>(seed)</a>
<a class="sourceLine" id="cb1541-6" data-line-number="6">  random.number =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span>n, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)   </a>
<a class="sourceLine" id="cb1541-7" data-line-number="7">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb1541-8" data-line-number="8">    indices[i] =<span class="st"> </span><span class="kw">which</span>(random.number[i] <span class="op">&lt;</span><span class="st"> </span>cumulative.weight)[<span class="dv">1</span>] </a>
<a class="sourceLine" id="cb1541-9" data-line-number="9">  }</a>
<a class="sourceLine" id="cb1541-10" data-line-number="10">  sample.set[indices,]</a>
<a class="sourceLine" id="cb1541-11" data-line-number="11">}</a></code></pre></div>

<p>Our sample weight, denoted as <strong>w</strong>, is used eventually to construct our distribution based on the cumulative weight with which we sample our original train set for the next fit.</p>

<div class="sourceCode" id="cb1542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1542-1" data-line-number="1">sample.set =<span class="st"> </span>train</a>
<a class="sourceLine" id="cb1542-2" data-line-number="2">n =<span class="st"> </span><span class="kw">nrow</span>(sample.set); w =<span class="st"> </span><span class="dv">1</span><span class="op">/</span>n</a>
<a class="sourceLine" id="cb1542-3" data-line-number="3">sample.weight =<span class="st"> </span><span class="kw">rep</span>(w, n)</a>
<a class="sourceLine" id="cb1542-4" data-line-number="4">sampled.set =<span class="st"> </span><span class="kw">cbind</span>(sample.set, sample.weight)</a></code></pre></div>
<pre><code>##   Sepal.Len Sepa.Width Petal.Len Petal.Width Species Sampl.Weight
## 1       5.1        3.5       1.4         0.2  setosa     0.007407
## 2       4.9        3.0       1.4         0.2  setosa     0.007407
## 3       4.7        3.2       1.3         0.2  setosa     0.007407
## 4       4.6        3.1       1.5         0.2  setosa     0.007407
## 5       5.0        3.6       1.4         0.2  setosa     0.007407
## 6       5.4        3.9       1.7         0.4  setosa     0.007407</code></pre>

<p><strong>Fourth</strong>, calculate the <strong>pseudo-loss</strong> of our hypothesis, namely <span class="math inline">\(\mathbf{h^{(t)}}\)</span>. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\epsilon^{(m)} = \frac{1}{2} \sum_{(i,y) \in B}^n D^{(m)}_{(i,y)} \left(1 - h^{(m)}(x_i, y_i) + h^{(m)}(x_i, y)\right) 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\epsilon^{(m)} = 
    \sum_{i=1}^n w^{(m)}_i \mathbf{1}\{h^{(m)}(x_i) \ne y_i \} / \sum_{i=1}^n w_i^{(m)} 
\end{align}\]</span></p>
<p><strong>Fifth</strong>, calculate the contribution (or power of say). For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\beta^{(m)} =   \epsilon^{(m)}/(1 - \epsilon^{(m)}) 
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\beta^{(m)} = 
     \frac{1}{2} \log_e \frac{1 - \epsilon^{(m)}}{\epsilon^{(m)}} + \log_e(K-1)
\end{align}\]</span></p>
<p><strong>Sixth</strong>, we now update the weights. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\alpha^{(m)} = (1/2)  \left(1 + h^{(m)}(x_i, y_i) - h^{(m)}(x_i, y)\right)\\
D^{(m+1)}_{(i,y)} \leftarrow D^{(m)}_{(i,y)} \cdot (\beta^{(m)})^{\alpha^{(m)}}, i=1,2,...,n
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
\alpha^{(m)} = \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) \ne y_i \}) \\
w^{(m+1)}_i \leftarrow (w^{(m)}_i)^{\alpha^{(m)}}, i=1,2,...,n
\end{align}\]</span></p>
<p><strong>Finally</strong>, we calculate our prediction output. For <strong>AdaBoost.M2</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
H(\mathbf{x}) = \text{arg}\ \underset{y \in Y}{\text{max}}\left(\sum_{m=1}^M \log_e \frac{1}{\beta^{(m)}}h^{(m)}(x, y)\right)
\end{align}\]</span></p>
<p>For <strong>SAMME</strong>, we have:</p>
<p><span class="math display">\[\begin{align}
H(\mathbf{x}) = arg\ \underset{k}{max}\left(\sum_{m=1}^M \beta^{(m)} \cdot \mathbf{1}\{h^{(m)}(x_i) = k \}\right)
\end{align}\]</span></p>
<p>Now, for illustration, let us use <strong>SAMME</strong> as our algorithm for an example implementation of multi-classification.</p>

<div class="sourceCode" id="cb1544"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1544-1" data-line-number="1">my.adaboost.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, D, <span class="dt">estimators =</span> <span class="dv">100</span>) {</a>
<a class="sourceLine" id="cb1544-2" data-line-number="2">    n        =<span class="st"> </span><span class="kw">nrow</span>(D)</a>
<a class="sourceLine" id="cb1544-3" data-line-number="3">    w        =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">1</span><span class="op">/</span>n, n)</a>
<a class="sourceLine" id="cb1544-4" data-line-number="4">    levs     =<span class="st"> </span><span class="kw">levels</span>(D[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1544-5" data-line-number="5">    K        =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1544-6" data-line-number="6">    K.log    =<span class="st"> </span><span class="kw">log</span>(K <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, <span class="kw">exp</span>(<span class="dv">1</span>)) <span class="co"># 2.718282</span></a>
<a class="sourceLine" id="cb1544-7" data-line-number="7">    model =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1544-8" data-line-number="8">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>estimators) {</a>
<a class="sourceLine" id="cb1544-9" data-line-number="9">        sample.set =<span class="st"> </span><span class="kw">sampling.distribution</span>(D, w, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1544-10" data-line-number="10">        x =<span class="st"> </span>sample.set[, <span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1544-11" data-line-number="11">        y =<span class="st"> </span><span class="kw">as.character</span>(sample.set[, <span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1544-12" data-line-number="12">        h.model =<span class="st"> </span><span class="kw">h.learner</span>(features, target, sample.set, </a>
<a class="sourceLine" id="cb1544-13" data-line-number="13">                            <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1544-14" data-line-number="14">        h.pred  =<span class="st"> </span><span class="kw">h.score</span>(h.model, x, y)</a>
<a class="sourceLine" id="cb1544-15" data-line-number="15">        errors  =<span class="st"> </span>(h.pred<span class="op">$</span>fitted.values <span class="op">!=</span><span class="st"> </span>y) <span class="op">*</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1544-16" data-line-number="16">        epsilon =<span class="st"> </span><span class="kw">sum</span>(w <span class="op">*</span><span class="st"> </span>errors) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w)</a>
<a class="sourceLine" id="cb1544-17" data-line-number="17">        beta    =<span class="st"> </span><span class="kw">log</span>( (<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>epsilon) <span class="op">/</span><span class="st"> </span>epsilon, <span class="kw">exp</span>(<span class="dv">1</span>)) <span class="op">*</span><span class="st"> </span>K.log</a>
<a class="sourceLine" id="cb1544-18" data-line-number="18">        w       =<span class="st"> </span>w<span class="op">^</span>(beta <span class="op">*</span><span class="st"> </span>errors)</a>
<a class="sourceLine" id="cb1544-19" data-line-number="19">        w       =<span class="st"> </span>w <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(w) <span class="co"># normalize  </span></a>
<a class="sourceLine" id="cb1544-20" data-line-number="20">        model[[m]] =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;learner&quot;</span> =<span class="st"> </span>h.model, <span class="st">&quot;beta&quot;</span> =<span class="st"> </span>beta)</a>
<a class="sourceLine" id="cb1544-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1544-22" data-line-number="22">    <span class="kw">list</span>(<span class="st">&quot;levs&quot;</span> =<span class="st"> </span>levs, <span class="st">&quot;model&quot;</span> =<span class="st"> </span>model)</a>
<a class="sourceLine" id="cb1544-23" data-line-number="23">}</a></code></pre></div>
<div class="sourceCode" id="cb1545"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1545-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1545-2" data-line-number="2">target   =<span class="st"> &quot;Species&quot;</span></a>
<a class="sourceLine" id="cb1545-3" data-line-number="3">my.model =<span class="st"> </span><span class="kw">my.adaboost.SAMME</span>(features, target, train, <span class="dt">estimators =</span> <span class="dv">5</span>)</a></code></pre></div>

<p>Let us also write our example implementation of a prediction function using <strong>SAMME</strong>.</p>

<div class="sourceCode" id="cb1546"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1546-1" data-line-number="1">H =<span class="st"> </span>my.predict.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(models, x, y) {</a>
<a class="sourceLine" id="cb1546-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1546-3" data-line-number="3">  y.max  =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(y))</a>
<a class="sourceLine" id="cb1546-4" data-line-number="4">  levs   =<span class="st"> </span>models<span class="op">$</span>levs</a>
<a class="sourceLine" id="cb1546-5" data-line-number="5">  K      =<span class="st"> </span><span class="kw">length</span>(levs)</a>
<a class="sourceLine" id="cb1546-6" data-line-number="6">  class  =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1546-7" data-line-number="7">  <span class="cf">for</span> (model <span class="cf">in</span> models<span class="op">$</span>model) {</a>
<a class="sourceLine" id="cb1546-8" data-line-number="8">    pred =<span class="st"> </span><span class="kw">h.score</span>(model<span class="op">$</span>learner, x, y)</a>
<a class="sourceLine" id="cb1546-9" data-line-number="9">    beta.I =<span class="st"> </span><span class="kw">ifelse</span>(pred<span class="op">$</span>result <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, model<span class="op">$</span>beta, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb1546-10" data-line-number="10">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1546-11" data-line-number="11">      h =<span class="st"> </span>(levs[k] <span class="op">==</span><span class="st"> </span>pred<span class="op">$</span>fitted.values)</a>
<a class="sourceLine" id="cb1546-12" data-line-number="12">      class[,k] =<span class="st"> </span>class[,k] <span class="op">+</span><span class="st">  </span>beta.I <span class="op">*</span><span class="st"> </span>h </a>
<a class="sourceLine" id="cb1546-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb1546-14" data-line-number="14">  }</a>
<a class="sourceLine" id="cb1546-15" data-line-number="15">  y.pred =<span class="st"> </span><span class="kw">apply</span>(class, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1546-16" data-line-number="16">  <span class="kw">list</span>(<span class="st">&quot;y.hat&quot;</span> =<span class="st"> </span>levs[y.pred], <span class="st">&quot;classes&quot;</span> =<span class="st"> </span>levs)</a>
<a class="sourceLine" id="cb1546-17" data-line-number="17">}</a>
<a class="sourceLine" id="cb1546-18" data-line-number="18">(<span class="dt">predicted =</span> <span class="kw">H</span>(my.model, test, test.class))</a></code></pre></div>
<pre><code>## $y.hat
##  [1] &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;     &quot;setosa&quot;    
##  [6] &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot; &quot;versicolor&quot;
## [11] &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot;  &quot;virginica&quot; 
## 
## $classes
## [1] &quot;setosa&quot;     &quot;versicolor&quot; &quot;virginica&quot;</code></pre>

<p>Then, to evaluate using <strong>confusion matrix</strong>, we have the following implementation:</p>

<div class="sourceCode" id="cb1548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1548-1" data-line-number="1"><span class="kw">library</span>(caret)</a>
<a class="sourceLine" id="cb1548-2" data-line-number="2">my.evaluate.SAMME &lt;-<span class="st"> </span><span class="cf">function</span>(target, predicted) {</a>
<a class="sourceLine" id="cb1548-3" data-line-number="3">   y.hat         =<span class="st"> </span><span class="kw">as.factor</span>(predicted<span class="op">$</span>y.hat)</a>
<a class="sourceLine" id="cb1548-4" data-line-number="4">   <span class="kw">levels</span>(y.hat) =<span class="st"> </span>predicted<span class="op">$</span>classes</a>
<a class="sourceLine" id="cb1548-5" data-line-number="5">   conf.tab      =<span class="st"> </span><span class="kw">table</span>(y.hat, <span class="kw">as.factor</span>(target))</a>
<a class="sourceLine" id="cb1548-6" data-line-number="6">   accuracy      =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(conf.tab)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(conf.tab)</a>
<a class="sourceLine" id="cb1548-7" data-line-number="7">   <span class="kw">list</span>(<span class="st">&quot;conf.table&quot;</span> =<span class="st"> </span>conf.tab, <span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">round</span>(accuracy <span class="op">*</span><span class="st"> </span><span class="dv">100</span>,<span class="dv">2</span>))</a>
<a class="sourceLine" id="cb1548-8" data-line-number="8">}</a>
<a class="sourceLine" id="cb1548-9" data-line-number="9">(<span class="dt">my.outcome =</span> <span class="kw">my.evaluate.SAMME</span>(test.class, predicted))</a></code></pre></div>
<pre><code>## $conf.table
##             
## y.hat        setosa versicolor virginica
##   setosa          5          0         0
##   versicolor      0          5         0
##   virginica       0          0         5
## 
## $accuracy
## [1] 100</code></pre>

<p>where <strong>row-vectors</strong> are the predictions and <strong>column-vectors</strong> are the reference.</p>
<p>Our metrics are based on using a <strong>confusion matrix</strong> to derive our calculation for <strong>accuracy</strong> per class in a multi-classification setting. A multiclass <strong>accuracy</strong> is formulated below:</p>

<p><span class="math display">\[\begin{align}
Accuracy_{(avg)} = \frac{1}{K} \sum_{k=1}^K \frac{TP_k + TN_k}{TP_k + TN_k + FP_k + FN_k} 
\ \ \ \ \ \text{where K is the number of classes} 
\end{align}\]</span>
</p>
<p>The formula is calculated based on the <strong>confusion matrix</strong> above. Also, a simple way to calculate <strong>accuracy</strong> using a matrix is shown below:</p>
<div class="sourceCode" id="cb1550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1550-1" data-line-number="1">cmat =<span class="st"> </span>my.outcome<span class="op">$</span>conf.table</a>
<a class="sourceLine" id="cb1550-2" data-line-number="2">accuracy =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">diag</span>(cmat)) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span>(cmat)</a>
<a class="sourceLine" id="cb1550-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;accuracy&quot;</span> =<span class="st"> </span><span class="kw">round</span>(accuracy <span class="op">*</span><span class="st"> </span><span class="dv">100</span>, <span class="dv">2</span>), <span class="st">&quot;error&quot;</span> =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>accuracy)</a></code></pre></div>
<pre><code>## accuracy    error 
##      100        0</code></pre>
<p>Note that the accuracy above may seem too accurate if it is 100%. Overfitting can be avoided using regularization.</p>
<p>Other score metrics to consider for evaluation include <strong>precision</strong> and <strong>recall</strong>.</p>
<div class="sourceCode" id="cb1552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1552-1" data-line-number="1">precision =<span class="st"> </span><span class="kw">diag</span>(cmat) <span class="op">/</span><span class="st"> </span><span class="kw">rowSums</span>(cmat)</a>
<a class="sourceLine" id="cb1552-2" data-line-number="2">recall    =<span class="st"> </span><span class="kw">diag</span>(cmat) <span class="op">/</span><span class="st"> </span><span class="kw">colSums</span>(cmat)</a>
<a class="sourceLine" id="cb1552-3" data-line-number="3"><span class="kw">rbind</span>(precision, recall)</a></code></pre></div>
<pre><code>##           setosa versicolor virginica
## precision      1          1         1
## recall         1          1         1</code></pre>
<p>As an exercise, we leave readers to play around with different random seeds for the random resampling, namely sampling.distribution(.), and see how it affects prediction performance.</p>
<p>Under the <strong>Regression</strong> Section for <strong>AdaBoost</strong>, the choice of sampling set is based on <strong>resampling</strong>. Here, we demonstrate the use of <strong>re-weighing</strong>.</p>
</div>
<div id="logitboost-j-classes" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.6</span> LogitBoost (J Classes)<a href="machinelearning2.html#logitboost-j-classes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us extend our discussion of <strong>Logistic Regression</strong> to cover <strong>Multi Classification</strong>. Here, we introduce <strong>LogitBoost</strong> as an <strong>ensemble method</strong> using <strong>Logistic Regression</strong> for <strong>Multi Classification</strong>. We tailor our discussion based on the <strong>LogitBoost (J classes)</strong> algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani <span class="citation">(<a href="bibliography.html#ref-ref685p">2012</a>)</span> - note here that we use <strong>K</strong> for the number of classes.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M \\
\mathbf{Algorithm}:\\
\ \ \ F_{0,k}(x_i) = 0, P_{0,k}(x_i) = 1/K\ for\ k=1,...,K\ and\ i=1,...,n\ \ \ &amp;\text{(initialize)} \\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\ 
\ \ \ \ \ \ \text{loop}\ k\ in\ 1:\ K \\
\ \ \ \ \ \ \ \ \ \ \ \ \text{Compute working responses and weights}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ z_{ik} = \frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )} &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) ) &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \text{Fit model using } f_{m,k}(\{x_i,z_{ik},  w_{ik}\}_{i=1}^n)\ \text{by a weighted} &amp; \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{least-squares regression of}\ z_{ik}\ \text{to}\ x_i\ \text{with weights}\ w_{ik}\\
\ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ \text{Set}\ f_{m,k}(x_i)\ = \frac{K-1}{K} \left(f_{m-1,k}(x_i) - \frac{1}{K}\sum_{l=1}^K f_{m-1,l}(x_i)\right) &amp;   \text{i = 1,...,n}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ F_{m,k}(x_i)\ = F_{m-1,k}(x_i) + f_{m,k}(x_i) &amp;   \text{i = 1,...,n}\\
\ \ \ \ \ \ \ P_{m,k}(x_i) = exp(F_{m,k}(x_i))/\sum_{l=1}^K exp(F_{m,l}(x_i))
  &amp;  \text{i = 1,...,n}\\
\ \ \ \text{end loop}\\
\ \ \ \text{Output}\ \text{arg}\ \underset{k}{\text{max}}\ F_{M,k}(x)
\end{array}
\]</span></p>
<p>The algorithm is based on <strong>Newton method</strong> for optimization. An alternative method called <strong>Gradient Descent</strong> is covered in the next section under <strong>Gradient Boost</strong>.</p>
<p>Here, we work on the basis of probabilistic estimates expressed like so:</p>
<p><span class="math display">\[\begin{align}
\hat{p}_k = P(y_i=k|x_i) = \frac{exp(F_k(x_i))}{1 + exp(F_k(x_i))},
\end{align}\]</span></p>
<p>so that given a dataset, namely <span class="math inline">\(\{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\)</span> where <strong>K</strong> is the number of classes, our goal is to compute the probability of each of the <strong>K</strong> classes, e.g. <span class="math inline">\(\hat{p}_k \in \mathbb{R}^k\)</span>, such that one of them is the most likely to be the <strong>k</strong> class for <span class="math inline">\(y_i\)</span>. That is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\hat{y}_i|x_i = \text{arg}\ \underset{k}{\text{max}}\ \hat{p}_k(x_i)
\ \ \ \ \ where\ \sum_{k=1}^K \hat{p}_k = 1
\end{align}\]</span></p>
<p>For the <strong>LogitBoost Loss Function</strong>, we use <strong>negative log likelihood (NLL)</strong>:</p>
<p><span class="math display">\[\begin{align}
\mathcal{L(y, F(x))} = \sum_{i=1}^n \left\{ - \sum_{k=1}^K y_{ik}\ \log_e \hat{p}_{k}(x_i)\right\}
\end{align}\]</span></p>
<p>We minimize the <strong>Loss Function</strong> by taking the 1st (Jacobian) and second (Hessian) derivatives of the loss function with respect to <span class="math inline">\(\mathbf{F_k(x_i)}\)</span>.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\nabla_{F_{m,l}(x_i)} Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}{\partial F_{m,l}(x_i)}\right]_{\{F_{m,l}(x) = F_{m-1,l}(x)\}^K_{l=1}} \\
&amp;= y_{ik} - P_{m-1,k}(x_i)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\underbrace{\nabla_{F_{m,l}(x_i)}^2 Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial^2 Lik(\{y_{il}, F_{m,l}(x_i)\}^K_{l=1})}{\partial F_{m,l}(x_i)^2}\right]_{\{F_{m,l}(x) = F_{m-1,l}(x)\}^K_{l=1}} \\
&amp;= P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )
\end{align}\]</span></p>
<p>Then we formulate our <strong>working response</strong>, namely <span class="math inline">\(\mathbf{z_{ik}}\)</span>, and <strong>weight</strong>, namely <span class="math inline">\(\mathbf{w_{ik}}\)</span>, based on the result of the derivatives:</p>
<p><span class="math display">\[\begin{align}
z_{ik} = \frac{y_{ik} - P_{m-1,k}(x_i)}{P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )}
\ \ \ \ \ \ \ \ \ \ \ \
w_{ik} = P_{m-1,k}(x_i)( 1 - P_{m-1,k}(x_i) )
\end{align}\]</span></p>
<p>Note that <span class="math inline">\(w_{ik}\)</span> can become numerically unstable as it approaches zero. In our example implementation below, our quick patch is to apply the following condition:</p>
<p><span class="math display">\[\begin{align*}
w_{ik}\ \ \ \rightarrow
\begin{cases}
\text{1e-15}  &amp; if\ w_{ik} = 0\\
w_{ik} &amp; if\ w_{ik} \ne 0
\end{cases}
\end{align*}\]</span></p>
<p>Finally, <strong>LogitBoost</strong> is a greedy stage-wise additive model such that in terms of our base learner, namely <span class="math inline">\(\mathbf{f_{k,m}(x_i)}\)</span>, we perform the following expressions below iteratively:</p>
<p><span class="math display">\[\begin{align}
f_{m,k}(x_i)\ &amp;= &amp;\frac{K-1}{K} \left(f_{m-1,k}(x_i) - \frac{1}{K}\sum_{l=1}^K f_{m-1,l}(x_i)\right)\\
F_{m,k}(x_i) &amp;= &amp;F_{m-1,k}(x_i) + f_{m,k}(x_i) 
\end{align}\]</span></p>
<p>Below is an example implementation of <strong>LogitBoost (J classes)</strong>. We use <strong>rpart(.)</strong> for the fit - a regression tree for the base learner.</p>

<div class="sourceCode" id="cb1554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1554-1" data-line-number="1"><span class="kw">library</span>(rpart)</a>
<a class="sourceLine" id="cb1554-2" data-line-number="2">my.logitboost &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">machines=</span><span class="dv">5</span>) {</a>
<a class="sourceLine" id="cb1554-3" data-line-number="3">  x       =<span class="st"> </span><span class="kw">as.matrix</span>(data[,<span class="kw">c</span>(features)])</a>
<a class="sourceLine" id="cb1554-4" data-line-number="4">  y       =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1554-5" data-line-number="5">  n       =<span class="st"> </span><span class="kw">nrow</span>(x)</a>
<a class="sourceLine" id="cb1554-6" data-line-number="6">  M       =<span class="st"> </span>machines</a>
<a class="sourceLine" id="cb1554-7" data-line-number="7">  classes =<span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1554-8" data-line-number="8">  K       =<span class="st"> </span><span class="kw">length</span>(classes)</a>
<a class="sourceLine" id="cb1554-9" data-line-number="9">  F_k     =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-10" data-line-number="10">  p_k     =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">1</span><span class="op">/</span>K, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-11" data-line-number="11">  y_ik    =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-12" data-line-number="12">  h       =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1554-13" data-line-number="13">  cntrl =<span class="st"> </span><span class="kw">rpart.control</span>(<span class="dt">cp=</span><span class="dv">0</span>, <span class="dt">maxdepth=</span><span class="dv">1</span>, <span class="dt">minsplit=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1554-14" data-line-number="14">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {   y_ik[,k]   =<span class="st"> </span><span class="kw">as.numeric</span>(y <span class="op">==</span><span class="st"> </span>classes[k]) }</a>
<a class="sourceLine" id="cb1554-15" data-line-number="15">  <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb1554-16" data-line-number="16">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1554-17" data-line-number="17">       w_ik    =<span class="st"> </span>( p_k <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_k))</a>
<a class="sourceLine" id="cb1554-18" data-line-number="18">       z_ik    =<span class="st"> </span>(y_ik <span class="op">-</span><span class="st"> </span>p_k) <span class="op">/</span><span class="st"> </span><span class="kw">ifelse</span>( w_ik <span class="op">&gt;</span><span class="st"> </span><span class="dv">0</span>, w_ik, <span class="fl">1e-15</span>)</a>
<a class="sourceLine" id="cb1554-19" data-line-number="19">       f_model =<span class="st"> </span><span class="kw">rpart</span>(z_ik[,k] <span class="op">~</span><span class="st"> </span>x, <span class="dt">weights =</span> w_ik[,k], <span class="dt">control =</span> cntrl)</a>
<a class="sourceLine" id="cb1554-20" data-line-number="20">       h[,k]   =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(f_model)</a>
<a class="sourceLine" id="cb1554-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1554-22" data-line-number="22">    f_mk  =<span class="st"> </span>(K<span class="dv">-1</span>)<span class="op">/</span>K <span class="op">*</span><span class="st"> </span>( h <span class="op">-</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>K <span class="op">*</span><span class="st"> </span><span class="kw">apply</span>(h, <span class="dv">1</span>, sum))</a>
<a class="sourceLine" id="cb1554-23" data-line-number="23">    F_k  =<span class="st"> </span>F_k <span class="op">+</span><span class="st">  </span>f_mk</a>
<a class="sourceLine" id="cb1554-24" data-line-number="24">    p_k =<span class="st"> </span><span class="kw">exp</span>(F_k) <span class="op">/</span><span class="st"> </span><span class="kw">apply</span>(<span class="kw">exp</span>(F_k), <span class="dv">1</span>, sum) </a>
<a class="sourceLine" id="cb1554-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb1554-26" data-line-number="26">  <span class="kw">apply</span>(F_k, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1554-27" data-line-number="27">}</a></code></pre></div>

<p>Let us use the implementation and compare the difference between the actual target values and the fitted values generated by <strong>logitBoost</strong> using m equal to 1, 5, and 10.</p>

<div class="sourceCode" id="cb1555"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1555-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1555-2" data-line-number="2">target   =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1555-3" data-line-number="3">y        =<span class="st"> </span><span class="kw">as.numeric</span>(train[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1555-4" data-line-number="4">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1555-5" data-line-number="5"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.3529&quot;</code></pre>
<div class="sourceCode" id="cb1557"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1557-1" data-line-number="1">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb1557-2" data-line-number="2"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.5&quot;</code></pre>
<div class="sourceCode" id="cb1559"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1559-1" data-line-number="1">fitted.values =<span class="st"> </span><span class="kw">my.logitboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">10</span>)</a>
<a class="sourceLine" id="cb1559-2" data-line-number="2"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] &quot;Mean relative difference: 0.3333&quot;</code></pre>

<p>As an experiment, we leave readers to tune maxdepth and maxsplit. Moreover, we leave readers to modify the implementation to preserve the <strong>weak learner models</strong>, e.g., <strong>f_model</strong>, and use them to predict using the test set. From there, we use <strong>Confusion Matrix</strong> to evaluate the performance of the <strong>logitBoost</strong> ensemble itself.</p>
<p>There are other variants of <strong>LogitBoost</strong> that address numerical stability. One is called <strong>Adaptive Base Class (ABC) LogitBoost</strong> formulated by Ping Li (from Cornell University). We leave readers to investigate <strong>ABC-LogitBoost</strong> and its application to <strong>image</strong> classification <span class="citation">(Ping Li <a href="bibliography.html#ref-ref685p">2012</a>)</span>.</p>
</div>
<div id="gradient-boost-1" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.7</span> Gradient Boost <a href="machinelearning2.html#gradient-boost-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We continue to extend our concept of <strong>Boosting</strong> by using <strong>Gradient Boost</strong>. A detailed intuitive view of the concept of <strong>Gradient Boosting</strong> is covered under the <strong>Regression</strong> Section. The same intuition applies to <strong>Classification</strong>.</p>
<p>Recall in <strong>Gradient Boosting for Regression</strong> that we have the following <strong>square error</strong> in our <strong>loss function</strong>:</p>
<p><span class="math display">\[
\underbrace{Lik(y, F(x))}_{\text{loss function}} 
= \frac{1}{2}\left(y -\hat{y}\right)^2 \ \
and\ \ \underbrace{F(x) = \hat{y}}_{\text{y-hat}}.
\]</span></p>
<p>In this section, we discuss <strong>K-class Logistic Gradient Boost</strong> introduced by Jerome Friedman <span class="citation">(<a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>, <a href="bibliography.html#ref-ref677j">2001</a>, <a href="bibliography.html#ref-ref679j">2002</a>)</span> in the context of <strong>Multi-Classification</strong>. Here, we start with our <strong>loss function</strong> based on <strong>cross-entropy</strong> in the form below where we measure the <strong>loss</strong> for each <strong>k</strong> class - hence, it is notable to point out the notation: <span class="math inline">\(\{y_k, F_k(x)\}^K_{k=1}\)</span>. Our <strong>loss function</strong> is thus written as:</p>
<p><span class="math display">\[\begin{align}
\underbrace{Lik(\{y_k, F_k(x)\}^K_{k=1})}_{\text{loss function}} 
= - \sum_{k=1}^K y_k\log_e P_k(x) 
\end{align}\]</span></p>
<p>where <strong>K</strong> is the number of classes and <span class="math inline">\(P_k\)</span> is a <strong>softmax</strong> function expressed as:</p>
<p><span class="math display">\[\begin{align}
\underbrace{P_k(x) = \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}}_{\text{softmax}}
\ \ \ \ \ \ \ \leftarrow\ \ \ \ \ \ 
F_{(k\ or\ l)}(x) = \underbrace{o_{(k\ or\ l)} = \frac{P(y_{(k\ or\ l)} = 1|x)}{P (y_{(k\ or\ l)} = 0|x)}}_{\text{log odds}}
\end{align}\]</span></p>
<p>Expanding our <strong>loss function</strong>, we get:</p>

<p><span class="math display">\[\begin{align}
\underbrace{Lik(\{y_k, F_k(x)\}^K_1)}_{\text{loss function}} 
= -  \sum_{k=1}^K 
y_k \log_e \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}
=  \sum_{i=1}^n \underbrace{ \left(-\sum_{k=1}^K 
\underbrace{\mathbf{1}\{class = k\}}_{y_k} \log_e \underbrace{ \frac{e^{o_k}}{\sum_{l=1}^K e^{o_l}}}_{\text{softmax}}\right)}_{\begin{array}{c}\text{negative log-likelihood loss}\\ \text{(cross-entropy)}\end{array}}   \label{eqn:eqnnumber419}
\end{align}\]</span>
</p>
<p>Note that our <strong>log-odds</strong> undergo the following logistic transformation in reference to another <strong>Boosting method</strong> called <strong>LogitBoost</strong>, which uses <strong>Quasi-Newton (or Newton Raphson)</strong> method along with <strong>Hessian</strong> calculation <span class="citation">(Friedman J. <a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>. The multinomial log-likelihood loss is generalized into <strong>LogitBoost</strong> for multi-classification. </p>
<p><span class="math display">\[\begin{align}
F_k(x) = \log_e p_k(x) - \frac{1}{K}\sum_{l=1}^K \log_e p_l(x)
\end{align}\]</span></p>
<p>Using the <strong>loss function</strong>, we derive its <strong>negative gradient</strong>. Let us first take the derivative of our <strong>softmax</strong> with respect to <strong>log odds</strong>.</p>
<p><span class="math display">\[\begin{align}
\text{if }i=j\ \ \ \ \rightarrow
\frac{\partial P_i}{\partial o_k} = 
\frac{\partial \frac{e^{o_k}}{\sum_{j=1}^K e^{o_j}}}{\partial o_k} &amp;= 
   \frac{e^{o_k} \left(\sum_{k=1}^K e^{o_k} - e^{o_j} \right)}{(\sum_{k=1}^K)^2} =
   P_i \left(1 - P_j\right)\\
\text{if }i\ne j\ \ \ \ \rightarrow
\frac{\partial P_i}{\partial o_k} = 
\frac{\partial \frac{e^{o_k}}{\sum_{j=1}^K e^{o_j}}}{\partial o_k} &amp;=  
\frac{0 - e^o{_j} e^{o_i}}{\left(\sum_{k-1}^K e^{o_k}\right)} = - P_j P_i 
\end{align}\]</span></p>
<p>therefore:</p>
<p><span class="math display">\[\begin{align}
\frac{\partial P_i}{\partial o_k} =
\begin{cases} 
P_i( 1 - P_i) &amp; if\ i = j\\
- P_i P_j &amp; if\ i \ne j
\end{cases}  \label{eqn:eqnnumber420}
\end{align}\]</span></p>
<p>We then take the <strong>negative derivative</strong> of our <strong>loss function</strong> with respect to <strong>log-odds</strong>.</p>
<p><span class="math display">\[\begin{align}
r_{ik} = \underbrace{\nabla_{F_l(x_i)} Lik(\{y_{il}, F_l(x_i)\}^K_{l=1})}_{\text{negative gradient}} &amp;= -\left[\frac{\partial Lik(\{y_{il}, F_l(x_i)\}^K_{l=1})}{\partial F_l(x_i)}\right]_{\{F_l(x) = F_{l,m-1}(x)\}^K_{l=1}} \\
&amp;= -\sum_{k=1}^K y_k(x) \left(\frac{ \partial \log_e P_k(x) }{\partial o_k}\right)\\
&amp;= - y_i ( 1- P_i) - \sum_{k\ne i}  y_k \frac{1}{P_k} ( - P_i P_j  )\\
&amp;= y_{ik} - P \left(\sum_k y_k\right)\ \ \ \ \leftarrow\ \left(\sum_k y_k\right)  = 1\\
&amp;= y_{ik} - P_{k,m-1}(x_i)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{(residual)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(F_{l,m-1}(x_i) \equiv F_{l,0}(x_i) \equiv \hat{y}_l \equiv \gamma_l\)</span>.</p>
<p>Let us implement the <strong>log-odds function</strong> and <strong>softmax function</strong>. We perform a logistic transform, a.l.a <strong>softmax</strong>:</p>

<div class="sourceCode" id="cb1561"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1561-1" data-line-number="1">softmax &lt;-<span class="st"> </span><span class="cf">function</span>(F_k) {</a>
<a class="sourceLine" id="cb1561-2" data-line-number="2">    n   =<span class="st"> </span><span class="kw">nrow</span>(F_k)</a>
<a class="sourceLine" id="cb1561-3" data-line-number="3">    K   =<span class="st"> </span><span class="kw">ncol</span>(F_k)</a>
<a class="sourceLine" id="cb1561-4" data-line-number="4">    p_k =<span class="st"> </span><span class="kw">matrix</span>( <span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1561-5" data-line-number="5">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1561-6" data-line-number="6">        p_k[,k] =<span class="st"> </span><span class="kw">exp</span>(F_k[,k]) <span class="op">/</span><span class="st"> </span><span class="kw">apply</span>(<span class="kw">exp</span>(F_k), <span class="dv">1</span>, sum)</a>
<a class="sourceLine" id="cb1561-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb1561-8" data-line-number="8">    p_k</a>
<a class="sourceLine" id="cb1561-9" data-line-number="9">}</a></code></pre></div>

<p>In the case of using our <strong>classification tree</strong> as our <strong>base learner</strong>, each leaf (or region) produces the following score transformation:</p>
<p><span class="math display">\[
\begin{array}{lrr}
\ \ \ \ \ \ \ \text{for each}\ (L)_j \in (T)_m\ &amp;\text{where (L)eaf and (T)ree}  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \gamma_{jkm} = 
\frac{K - 1}{K} \frac{\sum_{x_i} \in {L_{jkm}}^{rim}}{\sum_{x_i} \in {L_{jkm}}^{|r_{im}|(1 - |r_{im}|)}} \\
\ \ \ \ \ \ \ \text{end loop} 
\end{array}
\]</span></p>
<p>The calculation of the score is implemented as such:</p>

<div class="sourceCode" id="cb1562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1562-1" data-line-number="1">transform &lt;-<span class="st"> </span><span class="cf">function</span>(my.model, <span class="dt">prob =</span> <span class="ot">NULL</span>) {</a>
<a class="sourceLine" id="cb1562-2" data-line-number="2">    J     =<span class="st"> </span><span class="kw">length</span>(my.model<span class="op">$</span>model)</a>
<a class="sourceLine" id="cb1562-3" data-line-number="3">    numer =<span class="st"> </span><span class="dv">0</span>; denom =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1562-4" data-line-number="4">    <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>J) {</a>
<a class="sourceLine" id="cb1562-5" data-line-number="5">        top =<span class="st"> </span>my.model<span class="op">$</span>model[[j]]<span class="op">$</span>top</a>
<a class="sourceLine" id="cb1562-6" data-line-number="6">        <span class="cf">if</span> (top<span class="op">$</span>ntype <span class="op">==</span><span class="st"> &quot;leaf&quot;</span>) {</a>
<a class="sourceLine" id="cb1562-7" data-line-number="7">            indices  =<span class="st"> </span>top<span class="op">$</span>indices</a>
<a class="sourceLine" id="cb1562-8" data-line-number="8">            y_j      =<span class="st"> </span>top<span class="op">$</span>response</a>
<a class="sourceLine" id="cb1562-9" data-line-number="9">            p_j      =<span class="st"> </span>prob[indices]</a>
<a class="sourceLine" id="cb1562-10" data-line-number="10">            gamma    =<span class="st"> </span>(J<span class="dv">-1</span>) <span class="op">/</span><span class="st"> </span>J  <span class="op">*</span><span class="st"> </span>( <span class="kw">sum</span>(y_j) <span class="op">/</span><span class="st"> </span><span class="kw">sum</span> (<span class="kw">abs</span>(p_j) <span class="op">*</span><span class="st"> </span><span class="kw">abs</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p_j)))</a>
<a class="sourceLine" id="cb1562-11" data-line-number="11">            my.model<span class="op">$</span>model[[j]]<span class="op">$</span>top<span class="op">$</span>ymean =<span class="st"> </span>gamma</a>
<a class="sourceLine" id="cb1562-12" data-line-number="12">        }</a>
<a class="sourceLine" id="cb1562-13" data-line-number="13">    }</a>
<a class="sourceLine" id="cb1562-14" data-line-number="14">    my.model</a>
<a class="sourceLine" id="cb1562-15" data-line-number="15">}</a></code></pre></div>

<p>Finally, <strong>Gradient Boost</strong> is a greedy stage-wise additive model such that in terms of our base learner, namely <span class="math inline">\(\mathbf{\gamma_{jkm}(x_i)}\)</span>, we perform the following expressions iteratively:</p>
<p><span class="math display">\[\begin{align}
F_{k,m}(x) = F_{k,m-1}(x) + \sum_{j=1}^J \gamma_{jkm}\mathbf{1}\{x \in L_{jkm}\}
\ \ \ \ \ where\ \ \text{(where J is number of leafs)}
\end{align}\]</span></p>
<p>The algorithm below is a variant of <strong>Gradient Boosting</strong> with modification to allow for <strong>Multi-Classification</strong> (See Algorithm 6: <span class="math inline">\(L_k\)</span>_TreeBoost, J. Friedman <span class="citation">(<a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>). Here, we use the term <strong>L</strong>eaf referring to <strong>R</strong>egion for a <strong>J-terminal</strong> to emphasize using decision trees for base learners.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,K\}\}_{i=1}^n\\
\ \ \ \text{number of machines}: M\\
\mathbf{Algorithm}:\\
\ \ \ F_0(X) = \text{arg}\ \underset{\gamma}{min} \sum_{x=1}^n Lik(y_i, \gamma )
\ \ \ \text{(initialize)}\\
\ \ \ \text{loop}\ m\ in\ 1:\ M \\
\ \ \ \ \ \ P_k(x) = \frac{e^{F_k}}{\sum_{l=1}^K e^{F_l}},\ \ \ \  for\ k = 1,..,K \\
\ \ \ \ \ \ \text{loop}\ k\ in\ 1:\ K\\
\ \ \ \ \ \ \ \ \ \ \ \ \ r_{im} = y_{ik} - P_{k,m-1}(x_i)\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{fit model using } h_m(\{x_i,r_{im}\}_{i=1}^n)\ \ \ \ \ \ \ \text{(fit model to pseudo-residuals)} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{for each}\ (L)_j \in (T)_m\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where (L)eaf and (T)ree}  \\
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \gamma_{jkm} = 
\frac{K - 1}{K} \frac{\sum_{x_i \in L_{jkm}}{rim}}{\sum_{x_i \in L_{jkm}}{|r_{im}|(1 - |r_{im}|)}}
\ \ \ \ \text{(leaf residual)}\\
\ \ \ \ \ \ \ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \ \ \ \ \ \ \ \ \ \ F_{k,m}(x) = F_{k,m-1}(x) + \sum_{j=1}^J \gamma_{jkm}\mathbf{1}\{x \in L_{jkm}\} \ \ \ \ \ \ \text{(J is number of leafs)}\\
\ \ \ \ \ \ \ \text{end loop} \\
\ \ \ \text{end loop}\\
\ \ \text{Output } F_{k,m}(x) 
\end{array}
\]</span></p>
<p>Below is our example implementation of Algorithm 6: <span class="math inline">\(L_k\)</span>_TreeBoost <span class="citation">(J. Friedman <a href="bibliography.html#ref-ref670j">1999</a><a href="bibliography.html#ref-ref670j">a</a>)</span>:</p>

<div class="sourceCode" id="cb1563"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1563-1" data-line-number="1"><span class="co"># Here, we use regression tree and prediction </span></a>
<a class="sourceLine" id="cb1563-2" data-line-number="2"><span class="co"># from Computational Learning I chapter.</span></a>
<a class="sourceLine" id="cb1563-3" data-line-number="3">h.learner &lt;-<span class="st"> </span>my.regression.tree  </a>
<a class="sourceLine" id="cb1563-4" data-line-number="4">h.score   &lt;-<span class="st"> </span>my.predict          </a>
<a class="sourceLine" id="cb1563-5" data-line-number="5">tendency  &lt;-<span class="st"> </span><span class="cf">function</span>(top, resid) { top<span class="op">$</span>ymean }</a>
<a class="sourceLine" id="cb1563-6" data-line-number="6">my.gradientboost &lt;-<span class="st"> </span><span class="cf">function</span>(features, target, data, <span class="dt">machines=</span><span class="dv">50</span>, </a>
<a class="sourceLine" id="cb1563-7" data-line-number="7">                      <span class="dt">minbucket=</span><span class="dv">1</span>, <span class="dt">maxdepth=</span><span class="dv">4</span>) {</a>
<a class="sourceLine" id="cb1563-8" data-line-number="8">    x          =<span class="st"> </span>data[,<span class="kw">c</span>(features)]</a>
<a class="sourceLine" id="cb1563-9" data-line-number="9">    y          =<span class="st"> </span>data[,<span class="kw">c</span>(target)]</a>
<a class="sourceLine" id="cb1563-10" data-line-number="10">    n          =<span class="st"> </span><span class="kw">length</span>(y)</a>
<a class="sourceLine" id="cb1563-11" data-line-number="11">    classes    =<span class="st"> </span><span class="kw">levels</span>(y)</a>
<a class="sourceLine" id="cb1563-12" data-line-number="12">    K          =<span class="st"> </span><span class="kw">length</span>(classes)</a>
<a class="sourceLine" id="cb1563-13" data-line-number="13">    M          =<span class="st"> </span>machines</a>
<a class="sourceLine" id="cb1563-14" data-line-number="14">    y_ik       =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-15" data-line-number="15">    y.hat      =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, n, K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-16" data-line-number="16">    F_k        =<span class="st"> </span><span class="kw">matrix</span>( <span class="dv">0</span>, <span class="dt">nrow=</span>n, <span class="dt">ncol=</span>K, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1563-17" data-line-number="17">    f_model    =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb1563-18" data-line-number="18">    <span class="kw">names</span>(F_k) =<span class="st"> </span>classes</a>
<a class="sourceLine" id="cb1563-19" data-line-number="19">    <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) { </a>
<a class="sourceLine" id="cb1563-20" data-line-number="20">      y_ik[,k]      =<span class="st"> </span><span class="kw">as.numeric</span>(y <span class="op">==</span><span class="st"> </span>classes[k])</a>
<a class="sourceLine" id="cb1563-21" data-line-number="21">    }</a>
<a class="sourceLine" id="cb1563-22" data-line-number="22">    <span class="cf">for</span> (m <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>M) {</a>
<a class="sourceLine" id="cb1563-23" data-line-number="23">      p_k =<span class="st"> </span><span class="kw">softmax</span>(F_k)</a>
<a class="sourceLine" id="cb1563-24" data-line-number="24">      <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>K) {</a>
<a class="sourceLine" id="cb1563-25" data-line-number="25">        y.hat[,k]        =<span class="st"> </span>residual =<span class="st"> </span>y_ik[,k] <span class="op">-</span><span class="st"> </span>p_k[,k] </a>
<a class="sourceLine" id="cb1563-26" data-line-number="26">        data[,<span class="kw">c</span>(target)] =<span class="st"> </span>residual</a>
<a class="sourceLine" id="cb1563-27" data-line-number="27">        f_model[[k]]     =<span class="st"> </span><span class="kw">h.learner</span>(features, target, data, minbucket, </a>
<a class="sourceLine" id="cb1563-28" data-line-number="28">                                     maxdepth)</a>
<a class="sourceLine" id="cb1563-29" data-line-number="29">        f_model[[k]]     =<span class="st"> </span><span class="kw">transform</span>(f_model[[k]], p_k[,k])</a>
<a class="sourceLine" id="cb1563-30" data-line-number="30">        gamma_jkm        =<span class="st"> </span><span class="kw">h.score</span>(f_model[[k]], x, y.hat[,k] )<span class="op">$</span>fitted.values</a>
<a class="sourceLine" id="cb1563-31" data-line-number="31">        F_k[,k]          =<span class="st"> </span>F_k[,k] <span class="op">+</span><span class="st"> </span>gamma_jkm </a>
<a class="sourceLine" id="cb1563-32" data-line-number="32">      }</a>
<a class="sourceLine" id="cb1563-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb1563-34" data-line-number="34">    p_k =<span class="st"> </span><span class="kw">softmax</span>(F_k)</a>
<a class="sourceLine" id="cb1563-35" data-line-number="35">    <span class="kw">apply</span>(p_k, <span class="dv">1</span>, which.max)</a>
<a class="sourceLine" id="cb1563-36" data-line-number="36">}</a></code></pre></div>

<p>Let us use the implementation and compare the difference between the actual target values and the fitted values generated by <strong>Gradient Boost</strong> using m equal to 1.</p>

<div class="sourceCode" id="cb1564"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1564-1" data-line-number="1">features =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sepal.Length&quot;</span>, <span class="st">&quot;Sepal.Width&quot;</span>, <span class="st">&quot;Petal.Length&quot;</span>, <span class="st">&quot;Petal.Width&quot;</span>)</a>
<a class="sourceLine" id="cb1564-2" data-line-number="2">target   =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Species&quot;</span>)</a>
<a class="sourceLine" id="cb1564-3" data-line-number="3">y        =<span class="st"> </span><span class="kw">as.numeric</span>(train[,<span class="kw">c</span>(target)])</a>
<a class="sourceLine" id="cb1564-4" data-line-number="4">fitted.values =<span class="st"> </span><span class="kw">my.gradientboost</span>(features, target, train, <span class="dt">machines=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1564-5" data-line-number="5"><span class="kw">all.equal</span>(y, fitted.values)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>We leave readers to enhance the implementation with emphasis on fixing the overfitting.</p>
</div>
<div id="k-next-neighbors-knn" class="section level3 hasAnchor">
<h3><span class="header-section-number">10.3.8</span> K-Next Neighbors (KNN)  <a href="machinelearning2.html#k-next-neighbors-knn" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before we jump to a new topic, it helps to also finally give merit to the <strong>K-Next Neighbors (KNN)</strong> classification method being one of the traditional classification algorithms. <strong>KNN</strong> may be confused with <strong>K-means</strong>, which are two separate methods. The former is a classification method, and the latter is a clustering method.</p>
<p><strong>KNN</strong> has the following simple algorithm, which allows for unknown data, e.g., test data, to be classified given already learned or classified data. In the algorithm below, we are asked to determine the class to which the new data belongs. Note here that <strong>C</strong> is the number of categories (or classes) and <strong>K</strong> is the K nearest neighbors.</p>
<p><span class="math display">\[
\begin{array}{ll}
\mathbf{Input}:\\
\ \ \ \text{dataset}: \{{(x_i,y_i)}:x_i\ \in\ X, y_i \in Y = \{1,...,C\}\}_{i=1}^n\\
\ \ \ \text{unknown dataset}: \{{(z_i)}:z_i\ \in\ Z \}_{i=1}^m\\
\ \ \ \text{number of nearest data points}: K\\
\mathbf{Algorithm}:\\
\ \ \ \text{loop}\ j\ in\ 1:\ m\\
\ \ \ \ \ \ \ \text{loop}\ i\ in\ 1:\ n\\
\ \ \ \ \ \ \ \ \ \ \ \text{Calculate:}\ Distance(x_i, z_j) \\
\ \ \ \ \ \ \ \ \ \ \ \text{Sort the calculated distances in ascending order} \\
\ \ \ \ \ \ \ \ \ \ \ \text{Choose the top K least distances (nearest neighbors)} \\
\ \ \ \ \ \ \ \ \ \ \ \text{The most frequent class, e.g. c,  in the K nearest}\\
\ \ \ \ \ \ \ \ \ \ \ \text{neighbors gets assigned to}\ z_j \\
\ \ \ \ \ \ \ \text{end loop}\\
\ \ \ \text{end loop}\\
\end{array}
\]</span></p>
<p>In the algorithm, any data measurement as necessary can be used, e.g. <strong>Euclidean</strong>, <strong>Manhattan</strong>, etc.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machinelearning1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="machinelearning3.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
