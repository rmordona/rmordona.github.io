<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>11.4 Time-Series Forecasting  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="11.4 Time-Series Forecasting  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="11.4 Time-Series Forecasting  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza Ordoña" />


<meta name="date" content="2023-02-20" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="11.3-natural-language-processing-nlp.html"/>
<link rel="next" href="11.5-recommender-systems.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i>Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i>Caveat and Disclaimer</a></li>
<li class="chapter" data-level="" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i>About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.1" data-path="0.1-notation.html"><a href="0.1-notation.html"><i class="fa fa-check"></i><b>0.1</b> Notation</a></li>
<li class="chapter" data-level="0.2" data-path="0.2-number-system.html"><a href="0.2-number-system.html"><i class="fa fa-check"></i><b>0.2</b> Number System</a></li>
<li class="chapter" data-level="0.3" data-path="0.3-implementation.html"><a href="0.3-implementation.html"><i class="fa fa-check"></i><b>0.3</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="1-numericalmethods.html"><a href="1-numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="1.1-closed-form-equation.html"><a href="1.1-closed-form-equation.html"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="1.2-analytical-and-numerical-solutions.html"><a href="1.2-analytical-and-numerical-solutions.html"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="1.3-significant-figures.html"><a href="1.3-significant-figures.html"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="1.4-accuracy.html"><a href="1.4-accuracy.html"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="1.5-precision.html"><a href="1.5-precision.html"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="1.6-stability-and-sensitivity.html"><a href="1.6-stability-and-sensitivity.html"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="1.7-stiffness-and-implicitness.html"><a href="1.7-stiffness-and-implicitness.html"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="1.8-conditioning-and-posedness.html"><a href="1.8-conditioning-and-posedness.html"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="2-linearalgebra.html"><a href="2-linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="2.1-system-of-linear-equations.html"><a href="2.1-system-of-linear-equations.html"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="2.2-scalar-vector-and-matrix-tensor.html"><a href="2.2-scalar-vector-and-matrix-tensor.html"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="2.3-transposition-and-multiplication.html"><a href="2.3-transposition-and-multiplication.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="2.4-magnitude-direction-unit-vectors.html"><a href="2.4-magnitude-direction-unit-vectors.html"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="2.5-linear-combination-and-independence.html"><a href="2.5-linear-combination-and-independence.html"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="2.6-space-span-and-basis.html"><a href="2.6-space-span-and-basis.html"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="2.7-determinants.html"><a href="2.7-determinants.html"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="2.8-minors-cofactors-and-adjugate-forms.html"><a href="2.8-minors-cofactors-and-adjugate-forms.html"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="2.9-inverse-form-and-row-echelon-form.html"><a href="2.9-inverse-form-and-row-echelon-form.html"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="2.10-linear-transformations.html"><a href="2.10-linear-transformations.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="2.11-rank-and-nullity.html"><a href="2.11-rank-and-nullity.html"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="2.12-singularity-and-triviality.html"><a href="2.12-singularity-and-triviality.html"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="2.13-orthogonality-and-orthonormality.html"><a href="2.13-orthogonality-and-orthonormality.html"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="2.14-eigenvectors-and-eigenvalues.html"><a href="2.14-eigenvectors-and-eigenvalues.html"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><a href="2.15-matrix-reconstruction-using-eigenvalues-and-eigenvectors.html"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="2.16-diagonalizability-of-a-matrix.html"><a href="2.16-diagonalizability-of-a-matrix.html"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="2.17-trace-of-a-square-matrix.html"><a href="2.17-trace-of-a-square-matrix.html"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="2.18-algebraic-and-geometric-multiplicity.html"><a href="2.18-algebraic-and-geometric-multiplicity.html"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="2.19-types-of-matrices.html"><a href="2.19-types-of-matrices.html"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="2.20-matrix-factorization.html"><a href="2.20-matrix-factorization.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="2.21-software-libraries.html"><a href="2.21-software-libraries.html"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="2.22-summary.html"><a href="2.22-summary.html"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="3-numericallinearalgebra.html"><a href="3-numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="3.1-iteration-and-convergence.html"><a href="3.1-iteration-and-convergence.html"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html"><a href="3.2-approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="3.3-approximating-root-and-fixed-point-by-iteration.html"><a href="3.3-approximating-root-and-fixed-point-by-iteration.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqns by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newton’s Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broyden’s Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html"><a href="3.4-approximating-solutions-to-systems-of-eqs-by-iteration-ax-b.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="3.5-polynomialregression.html"><a href="3.5-polynomialregression.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="3.6-approximating-polynomial-functions-by-series-expansion.html"><a href="3.6-approximating-polynomial-functions-by-series-expansion.html"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Horner’s method </a></li>
<li class="chapter" data-level="3.7.9" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="3.7-polynomialinterpolation.html"><a href="3.7-polynomialinterpolation.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="3.8-polynomialsmoothing.html"><a href="3.8-polynomialsmoothing.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="3.9-polynomial-optimization.html"><a href="3.9-polynomial-optimization.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="3.10-summary-1.html"><a href="3.10-summary-1.html"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="4-numericalcalculus.html"><a href="4-numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="4.1-introductory-calculus.html"><a href="4.1-introductory-calculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="4.2-approximation-by-numerical-integration.html"><a href="4.2-approximation-by-numerical-integration.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="4.3-approximation-by-numerical-differentiation.html"><a href="4.3-approximation-by-numerical-differentiation.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Euler’s Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Euler’s Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heun’s Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="4.4-approximation-using-ordinary-differential-equations.html"><a href="4.4-approximation-using-ordinary-differential-equations.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="4.5-approximation-using-functional-differential-equations.html"><a href="4.5-approximation-using-functional-differential-equations.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="4.6-approximation-using-partial-differential-equations.html"><a href="4.6-approximation-using-partial-differential-equations.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burger’s Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="4.7-approximation-using-fourier-series-and-transform.html"><a href="4.7-approximation-using-fourier-series-and-transform.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="4.8-summary-2.html"><a href="4.8-summary-2.html"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="5-numericalprobability.html"><a href="5-numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="5.1-approximation-based-on-random-chances.html"><a href="5.1-approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="5.2-distribution.html"><a href="5.2-distribution.html"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="5.3-mass-and-density.html"><a href="5.3-mass-and-density.html"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="5.4-probability.html"><a href="5.4-probability.html"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="5.5-probability-density-function-pdf.html"><a href="5.5-probability-density-function-pdf.html"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="5.6-probability-mass-function-pmf.html"><a href="5.6-probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="5.7-cumulative-distribution-function-cdf.html"><a href="5.7-cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="5.8-special-functions.html"><a href="5.8-special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="5.9-distributiontypes.html"><a href="5.9-distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="5.10-summary-3.html"><a href="5.10-summary-3.html"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="6-statistics.html"><a href="6-statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="6.1-descriptive-statistics.html"><a href="6.1-descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="6.2-inferential-statistics.html"><a href="6.2-inferential-statistics.html"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearson’s Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="6.3-the-significance-of-difference.html"><a href="6.3-the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="6.4-post-hoc-analysis.html"><a href="6.4-post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffe’s Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisher’s Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukey’s Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnett’s Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncan’s Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="6.5-multiple-comparison-tests.html"><a href="6.5-multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="6.6-statistical-modeling.html"><a href="6.6-statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="6.7-regression-analysis.html"><a href="6.7-regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="6.8-the-significance-of-regression.html"><a href="6.8-the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="6.9-inference-for-regression.html"><a href="6.9-inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="6.10-summary-4.html"><a href="6.10-summary-4.html"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="7-bayesian.html"><a href="7-bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="7.1-probability-1.html"><a href="7.1-probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="7.2-probability-rules.html"><a href="7.2-probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#naïve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> Naïve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="7.3-bayes-theorem.html"><a href="7.3-bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="7.4-conjugacy.html"><a href="7.4-conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="7.5-information-theory.html"><a href="7.5-information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensen’s Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="7.6-bayesianinference.html"><a href="7.6-bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="8-bayesian2.html"><a href="8-bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="8.1-bayesian-models.html"><a href="8.1-bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="8.2-simulation-and-sampling.html"><a href="8.2-simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="8.3-bayesian-analysis.html"><a href="8.3-bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="8.4-summary-5.html"><a href="8.4-summary-5.html"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="9-machinelearning1.html"><a href="9-machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="9.1-observation-and-measurement.html"><a href="9.1-observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="9.2-input-data.html"><a href="9.2-input-data.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="9.2-input-data.html"><a href="9.2-input-data.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="9.2-input-data.html"><a href="9.2-input-data.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="9.2-input-data.html"><a href="9.2-input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="9.2-input-data.html"><a href="9.2-input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="9.2-input-data.html"><a href="9.2-input-data.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="9.2-input-data.html"><a href="9.2-input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="9.2-input-data.html"><a href="9.2-input-data.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="9.3-primitive-methods.html"><a href="9.3-primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="9.4-distance-metrics.html"><a href="9.4-distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="9.5-exploratory-data-analysis.html"><a href="9.5-exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="9.6-featureengineering.html"><a href="9.6-featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="9.7-general-modeling.html"><a href="9.7-general-modeling.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="9.8-supervised-vs.unsupervised-learning.html"><a href="9.8-supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>9.8</b> Supervised vs. Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="9.9-summary-6.html"><a href="9.9-summary-6.html"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="10-machinelearning2.html"><a href="10-machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="10.1-regression.html"><a href="10.1-regression.html"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="10.1-regression.html"><a href="10.1-regression.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="10.1-regression.html"><a href="10.1-regression.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="10.1-regression.html"><a href="10.1-regression.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="10.1-regression.html"><a href="10.1-regression.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="10.1-regression.html"><a href="10.1-regression.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="10.1-regression.html"><a href="10.1-regression.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="10.1-regression.html"><a href="10.1-regression.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="10.1-regression.html"><a href="10.1-regression.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="10.1-regression.html"><a href="10.1-regression.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="10.2-binary-classification-supervised.html"><a href="10.2-binary-classification-supervised.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="10.3-multi-class-classification-supervised.html"><a href="10.3-multi-class-classification-supervised.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="11-machinelearning3.html"><a href="11-machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="11.1-clustering-unsupervised.html"><a href="11.1-clustering-unsupervised.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="11.2-meta-learning.html"><a href="11.2-meta-learning.html"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="11.3-natural-language-processing-nlp.html"><a href="11.3-natural-language-processing-nlp.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="11.4-time-series-forecasting.html"><a href="11.4-time-series-forecasting.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="11.5-recommender-systems.html"><a href="11.5-recommender-systems.html"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="12-deeplearning1.html"><a href="12-deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="12.1-simple-perceptron.html"><a href="12.1-simple-perceptron.html"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="12.2-adaptive-linear-neuron-adaline.html"><a href="12.2-adaptive-linear-neuron-adaline.html"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="12.3-multi-layer-perceptron-mlp.html"><a href="12.3-multi-layer-perceptron-mlp.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="12.4-convolutional-neural-network-cnn.html"><a href="12.4-convolutional-neural-network-cnn.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="13-deeplearning2.html"><a href="13-deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="13.1-residual-network-resnet.html"><a href="13.1-residual-network-resnet.html"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="13.2-recurrent-neural-network-rnn.html"><a href="13.2-recurrent-neural-network-rnn.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="13.3-deep-stacked-rnn.html"><a href="13.3-deep-stacked-rnn.html"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="13.4-deep-stacked-bidirectional-rnn.html"><a href="13.4-deep-stacked-bidirectional-rnn.html"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="13.5-transformer-neural-network-tnn.html"><a href="13.5-transformer-neural-network-tnn.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="13.6-applications-using-tnn-and-rnn.html"><a href="13.6-applications-using-tnn-and-rnn.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="13.7-generative-adversarial-network-gan.html"><a href="13.7-generative-adversarial-network-gan.html"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="13.8-deep-reinforcement-network-dqn.html"><a href="13.8-deep-reinforcement-network-dqn.html"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="13.9-summary-8.html"><a href="13.9-summary-8.html"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="14-distributedcomputation.html"><a href="14-distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="14.1-integration-and-interoperability.html"><a href="14.1-integration-and-interoperability.html"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="14.2-ml-pipelines.html"><a href="14.2-ml-pipelines.html"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="14.3-open-standards.html"><a href="14.3-open-standards.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="14.4-general-summary.html"><a href="14.4-general-summary.html"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i>Appendix A</a><ul>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i>Trigonometry</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i>Logarithms</a></li>
<li class="chapter" data-level="" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i>Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i>Appendix B</a><ul>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i>On Random chances</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i>On Replacements</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i>On Permutations and Combinations</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i>On Conditional Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i>The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i>On Dependent and Independent Events</a></li>
<li class="chapter" data-level="" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i>On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i>Appendix C</a></li>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html"><i class="fa fa-check"></i>Appendix D</a><ul>
<li class="chapter" data-level="" data-path="appendix-d.html"><a href="appendix-d.html#lubridate-library"><i class="fa fa-check"></i>Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="time-series-forecasting" class="section level2 hasAnchor">
<h2><span class="header-section-number">11.4</span> Time-Series Forecasting <a href="11.4-time-series-forecasting.html#time-series-forecasting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now switch context to discuss <strong>Time-Series Forecasting</strong> - an area that is also equally important. In this context, a probable typical case here is forecasting sales.</p>
<p>Note that most - if not all - of our previous discussions underscored static data - a snapshot of an incident. Here, we discuss data that carry changes over time - in a <strong>Time-Series</strong> fashion. Our goal is to identify patterns (e.g., Trend, Seasonality, among others) that may allow us to forecast accurately.</p>
<p>As we tackle other concepts, it is good to be familiar with terms used in <strong>Forecasting</strong>. Note that when plotting our data, some terms apply to the <strong>Time-Series</strong> behavior along the <strong>Y-axis</strong>, e.g. <strong>trend</strong>, <strong>drift</strong>, and <strong>stationary</strong> terms; while the <strong>X-axis</strong> reflects fix time interval.</p>
<ul>
<li><p><strong>Period</strong> - refers to the duration of one full cycle.</p></li>
<li><p><strong>Frequency</strong> - refers to the number of complete cycles.</p></li>
<li><p><strong>Time Lag</strong> - refers to the elapsed time between two related events.</p></li>
<li><p><strong>Trend</strong> - A series with a trend that follows an increasing or decreasing pattern. The <strong>trajectory</strong> remains constant, meaning there is no shift in the intercept.</p></li>
<li><p><strong>Drift</strong> - A series in which the <strong>trajectory</strong> shifts (affecting the intercept).</p></li>
<li><p><strong>Seasonality</strong> - A series has Seasonality if it follows a pattern that repeats, such as <strong>weekly</strong>, <strong>monthly</strong>, <strong>quarterly</strong>, and <strong>yearly</strong>.</p></li>
<li><p><strong>Stationary</strong> - A stationary series follows a pattern in which the mean (<span class="math inline">\(\mu\)</span>) and variance (<span class="math inline">\(\sigma^2\)</span>) along the <strong>Y</strong> axis are constant over time. This property may be regarded as <strong>strongly stationary</strong>. If there is an observed change in variance (<span class="math inline">\(\sigma^2\)</span>) over time while the mean (<span class="math inline">\(\mu\)</span>) remains constant, this may be regarded as <strong>weakly stationary</strong>. In the context of <strong>seasonality</strong> and <strong>trend</strong>, let us describe <strong>stationary</strong> to mean a series with no <strong>trend</strong> nor <strong>seasonality</strong> patterns.  </p></li>
<li><p><strong>Spike</strong> / <strong>Peak</strong> - follows sudden jumps - almost treated as <strong>outliers</strong>.</p></li>
<li><p><strong>Differencing</strong> - this method transforms a series from non-stationary to stationary. It also means removing trend and seasonality patterns. </p></li>
<li><p><strong>Correlation</strong> - this is a method of correlating a series between differing time lags. It can serve to identify Stationary vs non-stationary series. If the outcome suggests a non-stationary series, then the series may have a trend, a pattern of Seasonality, or even both.</p></li>
</ul>
<p>With such terms being described, a good starting point is to construct our data beginning with the idea that our data comes from some random sampling - by that, we mean data with noise.</p>

<div class="sourceCode" id="cb1678"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1678-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1678-2" data-line-number="2"><span class="co"># simulate seasonality  - 3 cycles (3 years)</span></a>
<a class="sourceLine" id="cb1678-3" data-line-number="3">(<span class="dt">x =</span> <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>))  </a></code></pre></div>
<pre><code>##  [1] 0.45127 0.78378 0.70968 0.38174 0.63632 0.70135
##  [7] 0.64044 0.26668 0.81542 0.98299 0.02727 0.83749
## [13] 0.60324 0.56745 0.82005 0.25157 0.50549 0.86754
## [19] 0.95818 0.54570 0.13958 0.95534 0.39249 0.26849
## [25] 0.57221 0.91214 0.93429 0.88049 0.94569 0.81499
## [31] 0.03278 0.94271 0.94774 0.90209 0.55227 0.22489</code></pre>

<p>To simulate the idea of <strong>running total</strong>, let us use the function <strong>cumsum(.)</strong> to generate our <strong>cumulative sum</strong>.</p>

<div class="sourceCode" id="cb1680"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1680-1" data-line-number="1">(<span class="dt">x =</span> <span class="kw">cumsum</span>(x))</a></code></pre></div>
<pre><code>##  [1]  0.4513  1.2350  1.9447  2.3265  2.9628  3.6641
##  [7]  4.3046  4.5713  5.3867  6.3697  6.3969  7.2344
## [13]  7.8377  8.4051  9.2252  9.4767  9.9822 10.8498
## [19] 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096
## [25] 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694
## [31] 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718</code></pre>

<p>Here, we need to use 3rd-party libraries as listed below to be able to illustrate the idea:</p>
<div class="sourceCode" id="cb1682"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1682-1" data-line-number="1"><span class="kw">library</span>(lubridate)</a>
<a class="sourceLine" id="cb1682-2" data-line-number="2"><span class="kw">library</span>(tseries)</a>
<a class="sourceLine" id="cb1682-3" data-line-number="3"><span class="kw">library</span>(forecast)</a>
<a class="sourceLine" id="cb1682-4" data-line-number="4"><span class="kw">library</span>(fpp)</a>
<a class="sourceLine" id="cb1682-5" data-line-number="5"><span class="kw">library</span>(caret)</a></code></pre></div>
<p>Our next step is to convert our <strong>Cumulative sum</strong> into a <strong>Time-Series</strong> object using the <strong>st(.)</strong> function, specifying the total frequency that leads to the accumulation.</p>

<div class="sourceCode" id="cb1683"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1683-1" data-line-number="1">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>)))</a></code></pre></div>
<pre><code>##          Jan     Feb     Mar     Apr     May     Jun
## 2018  0.4513  1.2350  1.9447  2.3265  2.9628  3.6641
## 2019  7.8377  8.4051  9.2252  9.4767  9.9822 10.8498
## 2020 14.6818 15.5939 16.5282 17.4087 18.3544 19.1694
##          Jul     Aug     Sep     Oct     Nov     Dec
## 2018  4.3046  4.5713  5.3867  6.3697  6.3969  7.2344
## 2019 11.8080 12.3537 12.4932 13.4486 13.8411 14.1096
## 2020 19.2021 20.1449 21.0926 21.9947 22.5470 22.7718</code></pre>

<p>There are two considerations when dealing with <strong>Time-Series</strong> data, among many others. One consideration is to be able to fill the gap when there is <strong>missing</strong> data. Another consideration is to perform <strong>smoothing</strong>.</p>
<p>When it comes to <strong>missing</strong> data, recall our discussion on <strong>Missingness and Imputation</strong> covered under <strong>Exploratory Data Analysis</strong> section in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>). We also discuss <strong>Kalman Filter</strong> under the <strong>Bayesian Models</strong> section in Chapter <strong>8</strong> (<strong>Bayesian Computation II</strong>). We leave readers to review the mentioned sections.</p>
<p>For <strong>smoothing</strong>, e.g., <strong>interpolation</strong>, the next section introduces the use of <strong>STL</strong>.</p>
<div id="seasonal-trend-decomposition-using-loess-stl" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.1</span> Seasonal Trend Decomposition using LOESS (STL)  <a href="11.4-time-series-forecasting.html#seasonal-trend-decomposition-using-loess-stl" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We now look into <strong>smoothing methods</strong> using <strong>STL</strong>. Given the <strong>Time-Series</strong> object we generated in the previous section, our goal is to see <strong>trends</strong> and <strong>seasonality</strong>, including performing <strong>differencing</strong> to see the delta (or lag) between periods.</p>
<p>To illustrate, let us use <strong>stl(.)</strong> function to plot not just <strong>trend</strong> and <strong>season</strong>, but also including the <strong>remainder</strong> (see Figure <a href="11.4-time-series-forecasting.html#fig:tsplot1">11.17</a>).</p>

<div class="sourceCode" id="cb1685"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1685-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot1"></span>
<img src="DS_files/figure-html/tsplot1-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.17: Time-Series Plot
</p>
</div>

<p>Notice in the figure that our data shows a constantly increasing trend. The <strong>seasonal</strong> trend also shows an increase per season — ignore the drops for a moment. If this were a sales trend and we were an investor, we would like what we are seeing.</p>
<p>Let us change our random data by introducing a range between -2 and 1. Now, let us review the plot (see Figure <a href="11.4-time-series-forecasting.html#fig:tsplot2">11.18</a>).</p>

<div class="sourceCode" id="cb1686"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1686-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">width=</span><span class="dv">70</span>)</a>
<a class="sourceLine" id="cb1686-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1686-3" data-line-number="3">x =<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">2</span>, <span class="dt">max=</span><span class="dv">1</span>))</a>
<a class="sourceLine" id="cb1686-4" data-line-number="4">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>))</a></code></pre></div>
<pre><code>##       Jan     Feb     Mar     Apr     May     Jun     Jul     Aug
## 1 -0.6462 -0.2949 -0.1658 -1.0206 -1.1116 -1.0076 -1.0863 -2.2862
## 2 -2.4870 -2.7846 -2.3245 -3.5698 -4.0533 -3.4507 -2.5761 -2.9390
## 3 -5.9547 -5.2183 -4.4154 -3.7739 -2.9369 -2.4919 -4.3936 -3.5654
##       Sep     Oct     Nov     Dec
## 1 -1.8399 -0.8910 -2.8092 -2.2967
## 2 -4.5203 -3.6543 -4.4768 -5.6713
## 3 -2.7222 -2.0159 -2.3591 -3.6845</code></pre>
<div class="sourceCode" id="cb1688"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1688-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot2"></span>
<img src="DS_files/figure-html/tsplot2-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.18: Time-Series Plot
</p>
</div>

<p>By introducing a higher probability of generating negative values, we see a decreasing trend (e.g., more loss in sales). We may not prefer this sales trend.</p>
<p>Finally, let us introduce a more significant gap in range and review (see Figure <a href="11.4-time-series-forecasting.html#fig:tsplot3">11.19</a>).</p>

<div class="sourceCode" id="cb1689"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1689-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2021</span>)</a>
<a class="sourceLine" id="cb1689-2" data-line-number="2">x =<span class="st"> </span><span class="kw">cumsum</span>(<span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">12</span> <span class="op">*</span><span class="st"> </span><span class="dv">3</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">10</span>, <span class="dt">max=</span><span class="dv">10</span>))</a>
<a class="sourceLine" id="cb1689-3" data-line-number="3">(<span class="dt">TS =</span> <span class="kw">ts</span>(x, <span class="dt">frequency =</span> <span class="dv">12</span>))</a></code></pre></div>
<pre><code>##        Jan      Feb      Mar      Apr      May      Jun      Jul
## 1  -0.9747   4.7009   8.8946   6.5295   9.2559  13.2829  16.0916
## 2  26.7534  28.1024  34.5035  29.5349  29.6448  36.9956  46.1592
## 3  43.6353  51.8781  60.5640  68.1737  77.0876  83.3874  74.0429
##        Aug      Sep      Oct      Nov      Dec
## 1  11.4252  17.7337  27.3934  17.9388  24.6886
## 2  47.0732  39.8648  48.9716  46.8214  42.1911
## 3  82.8971  91.8519  99.8937 100.9390  95.4367</code></pre>
<div class="sourceCode" id="cb1691"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1691-1" data-line-number="1"><span class="kw">stl</span>(TS, <span class="st">&quot;periodic&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsplot3"></span>
<img src="DS_files/figure-html/tsplot3-1.png" alt="Time-Series Plot" width="70%" />
<p class="caption">
Figure 11.19: Time-Series Plot
</p>
</div>

<p>While the trend may look similar to the first plot, notice that the <strong>remainder</strong> section of our plot shows some more significant <strong>swings</strong> or <strong>fluctuations</strong>.</p>
<p>The three plots we reviewed all come from results using the <strong>stl(.)</strong> function. <strong>STL</strong> stands for <strong>Seasonal Trend Decomposition using LOESS</strong>. The idea behind <strong>STL</strong> is to be able to <strong>decompose</strong> our raw data using <strong>smoothing</strong> methods and <strong>translate</strong> into four meaningful insights, accounting for <strong>rolling total</strong>, <strong>trend</strong>, <strong>seasonality</strong> and <strong>remainder</strong>. Also, recall our discussion on <strong>Scatterplot Smoothing</strong>, introducing <strong>LOESS</strong> for smoothing, in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>).</p>
</div>
<div id="forecasting-models" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.2</span> Forecasting Models <a href="11.4-time-series-forecasting.html#forecasting-models" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Let us cover a few simple classic <strong>Forecasting</strong> methods we can use, namely:</p>
<ul>
<li><p><strong>Naïve Forecasting</strong> - this is the simplest forecasting in which the forecast is based on the previous (or final) value. For example, based on the recent data in just the previous section, the final sales for Dec 2020 is 95.4367. This value becomes the forecast for the next value for Jan of 2021. </p></li>
<li><p><strong>Seasonal Naïve Forecasting</strong> - this method follows the same as <strong>Naïve Forecasting</strong>; however, the forecast for Dec 2021 is based on the value taken from the same previous season. Therefore, the forecast for Jan 2021 will follow based on the season value from Jan 2020, which is 43.6353.</p></li>
<li><p><strong>Linear Trend Projection</strong> - this method follows the <strong>Least-Squares</strong> model in which a straight <strong>trend-line</strong> stretches in increasing or decreasing fashion fitting through data. The forecast follows the next value that falls on the line. See <strong>Least-Squares</strong> discussion in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). </p></li>
<li><p><strong>Moving Average</strong> - this method is mainly used in trading. There are three types of moving average: </p>
<ul>
<li><p><strong>Simple Moving Average (SMA)</strong> - this provides the average of N-periods: <span class="math inline">\(SMA = \frac{1}{N}\sum_{i=1}^N A_i\)</span> where <strong>A</strong> is the average for a period.</p></li>
<li><p><strong>Weighted Moving Average(WMA)</strong> - this provides the average based on the formula: <span class="math inline">\(WMA = \frac{1}{N}\sum_{i=1}^N A_i \times W_i\)</span> where <strong>W</strong> is a given (or arbitrary) weighting factor. </p></li>
<li><p><strong>Exponential Moving Average (EMA)</strong> - this provides the average based on the formula: <span class="math inline">\(\text{EMA}^{T+1} = (\text{P} - \text{EMA}^T) \times \left(\frac{2}{N + 1}\right) + \text{EMA}^T\)</span> where <strong>P</strong> is previous EMA and <strong>P</strong> is today’s price.</p></li>
</ul></li>
</ul>
<p>The next few sections cover a couple of advanced methods dealing with <strong>Time-Series</strong> forecasting using our generated <strong>Time-Series</strong> object.</p>
</div>
<div id="time-series-linear-model-tslm" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.3</span> Time-Series Linear Model (TSLM)  <a href="11.4-time-series-forecasting.html#time-series-linear-model-tslm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>With the given <strong>Time-Series</strong> object generated using <strong>stl(.)</strong>, let us model our forecast using linear regression. Note here that the <strong>tslm(.)</strong> function is equivalent to <strong>lm(.)</strong> for <strong>linear regression</strong> modeling in which we fit a line. Here, fitting a line considers trends and seasonality in a <strong>Time-Series</strong> fashion. Moreover, we use the <strong>forecast(.)</strong> function to forecast, determining future values.</p>
<p>There are four ways to fit a model to our data and forecast based on the model.</p>
<p><strong>Fitting TS Model with the linear trend only</strong></p>

<div class="sourceCode" id="cb1692"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1692-1" data-line-number="1">h =<span class="st"> </span><span class="dv">3</span> <span class="co"># forecast 3 periods ahead  in the horizon</span></a>
<a class="sourceLine" id="cb1692-2" data-line-number="2">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>trend)</a>
<a class="sourceLine" id="cb1692-3" data-line-number="3">(<span class="dt">forecasts1 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## Jan 4          93.13 82.69 103.6 76.89 109.4
## Feb 4          95.85 85.36 106.3 79.54 112.2
## Mar 4          98.57 88.03 109.1 82.18 115.0</code></pre>

<p><strong>TS Model with seasonality only</strong></p>

<div class="sourceCode" id="cb1694"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1694-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>season)</a>
<a class="sourceLine" id="cb1694-2" data-line-number="2">(<span class="dt">forecasts2 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast  Lo 80 Hi 80  Lo 95 Hi 95
## Jan 4          23.14 -27.46 73.74 -56.11 102.4
## Feb 4          28.23 -22.37 78.83 -51.02 107.5
## Mar 4          34.65 -15.95 85.25 -44.59 113.9</code></pre>

<p><strong>TS Linear Model with both linear trend and seasonality</strong></p>

<div class="sourceCode" id="cb1696"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1696-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>trend <span class="op">+</span><span class="st"> </span>season)</a>
<a class="sourceLine" id="cb1696-2" data-line-number="2">(<span class="dt">forecasts3 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, <span class="dt">h=</span>h))</a></code></pre></div>
<pre><code>##       Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## Jan 4          87.54 73.85 101.2 66.08 109.0
## Feb 4          92.63 78.94 106.3 71.17 114.1
## Mar 4          99.06 85.37 112.7 77.60 120.5</code></pre>

<p><strong>TS Linear Model using fourier series, where K = max. order of fourier terms.</strong></p>

<div class="sourceCode" id="cb1698"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1698-1" data-line-number="1">tslm.model =<span class="st"> </span><span class="kw">tslm</span>(TS <span class="op">~</span><span class="st">  </span>forecast<span class="op">::</span><span class="kw">fourier</span>(TS, <span class="dt">K=</span><span class="dv">4</span>))</a>
<a class="sourceLine" id="cb1698-2" data-line-number="2">(<span class="dt">forecasts4 =</span> forecast<span class="op">::</span><span class="kw">forecast</span>(tslm.model, </a>
<a class="sourceLine" id="cb1698-3" data-line-number="3">              <span class="kw">data.frame</span>(forecast<span class="op">::</span><span class="kw">fourier</span>(TS, <span class="dt">K=</span><span class="dv">4</span>, <span class="dt">h=</span>h))))</a></code></pre></div>
<pre><code>##       Point Forecast  Lo 80 Hi 80  Lo 95  Hi 95
## Jan 4          27.10 -19.16 73.37 -45.16  99.36
## Feb 4          25.46 -20.80 71.73 -46.80  97.72
## Mar 4          35.94 -10.32 82.21 -36.32 108.20</code></pre>

<p>We then plot our forecast corresponding to each model (see Figures <a href="11.4-time-series-forecasting.html#fig:tslmplot1">11.20</a> and <a href="11.4-time-series-forecasting.html#fig:tslmplot2">11.21</a> ).</p>

<div class="sourceCode" id="cb1700"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1700-1" data-line-number="1"><span class="kw">library</span>(patchwork)</a>
<a class="sourceLine" id="cb1700-2" data-line-number="2">p1 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts1, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Trend&quot;</span>) </a>
<a class="sourceLine" id="cb1700-3" data-line-number="3">p2 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts2, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Season&quot;</span>)</a>
<a class="sourceLine" id="cb1700-4" data-line-number="4">p1 <span class="op">/</span><span class="st"> </span>p2  <span class="co"># display two plots vertically</span></a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tslmplot1"></span>
<img src="DS_files/figure-html/tslmplot1-1.png" alt="Forecast (using STLM)" width="70%" />
<p class="caption">
Figure 11.20: Forecast (using STLM)
</p>
</div>
<div class="sourceCode" id="cb1701"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1701-1" data-line-number="1">p3 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts3, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Trend + Season&quot;</span>)</a>
<a class="sourceLine" id="cb1701-2" data-line-number="2">p4 =<span class="st"> </span><span class="kw">autoplot</span>(forecasts4, <span class="dt">ylab=</span><span class="st">&quot;Forecast&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Fourier(K=4)&quot;</span>)</a>
<a class="sourceLine" id="cb1701-3" data-line-number="3">p3 <span class="op">/</span><span class="st"> </span>p4 <span class="co"># display two plots vertically </span></a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tslmplot2"></span>
<img src="DS_files/figure-html/tslmplot2-1.png" alt="Forecast (using STLM)" width="70%" />
<p class="caption">
Figure 11.21: Forecast (using STLM)
</p>
</div>

<p>All the plots show our forecast around the fourth period, estimated with confidence levels.</p>
</div>
<div id="autoregressive-integrated-moving-average-arima" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.4</span> AutoRegressive Integrated Moving Average (ARIMA)  <a href="11.4-time-series-forecasting.html#autoregressive-integrated-moving-average-arima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>ARIMA</strong> is also called the <strong>Box-Jenkins</strong> method formulated by George Box and Gwilym Jenkins in 1976, which allows us to <strong>model</strong> stationary time-series. <strong>ARIMA</strong> uses three parameters: </p>
<ul>
<li><strong>p</strong> is non-seasonal parameter for AR order</li>
<li><strong>d</strong> is non-seasonal parameter for differencing (de-trending)</li>
<li><strong>q</strong> is non-seasonal parameter for MA order</li>
</ul>
<p>Below are <strong>ARIMA</strong> model examples that can represent different time series:</p>
<p><span class="math display" id="eq:equate1130039" id="eq:equate1130038" id="eq:equate1130037" id="eq:equate1130036">\[\begin{align}
ARIMA(1, 0, 0) &amp;\rightarrow &amp;Y_t = \beta_1  Y_{t-1} + \mathcal{E}_t \tag{11.37} \\
ARIMA(2, 0, 0) &amp;\rightarrow &amp;Y_t = \beta_1  Y_{t-1} + \beta_2  Y_{t-2} + \mathcal{E}_t \tag{11.38} \\
ARIMA(1, 1, 0) &amp;\rightarrow &amp;Y_t = \beta_1 \Delta Y_{t-1}  + \mathcal{E}_t
               \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ where\ \Delta Y_{t-1} = Y_t - Y_{t-1} \tag{11.39} \\
ARIMA(1, 1, 2) &amp;\rightarrow &amp;Y_t = \beta_1 \Delta Y_{t-1} + \theta_1 \mathcal{E}_{t-1} + \theta_2 \mathcal{E}_{t-2} + \mathcal{E}_t \tag{11.40} 
\end{align}\]</span></p>
<p>To understand <strong>ARIMA</strong>, we need to break it down into three methods, namely <strong>Autoregression (AR)</strong>, <strong>Integration (I)</strong>, and <strong>Moving Average (MA)</strong>.</p>
<p><strong>First</strong>, the <strong>Autoregression (AR)</strong> method allows us to forecast (or predict) the future in reference to the past. Our goal is to achieve a future state that regresses the previous state. We can model that statement based on the following linear equation with an assumed intercept in the form of a mean (<span class="math inline">\(\mu\)</span>) and a white noise denoted by <span class="math inline">\(\mathcal{E}_t\)</span>:</p>
<p><span class="math display" id="eq:equate1130040">\[\begin{align}
AR(p) = \mu + \underbrace{\left(\sum_{i=1}^p \beta_i y_{t-i}\right)}_\text{AR(p)} + \mathcal{E}_t = \mu + \beta_1y_{t-1} + \beta_2y_{t-2} \ + ... +\ \beta_p y_{t-p}  + \mathcal{E}_t   \tag{11.41} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(y_{t - p}\)</span> is the state value at <strong>(t-p)</strong> time-period (or the <strong>lag</strong>) in the past and the mean (<span class="math inline">\(\mu\)</span>) is the overall mean. The parameter <strong>p</strong> is the order of terms to use.</p>
<p>To illustrate, let us use <strong>first-order</strong> using the following equation:</p>
<p><span class="math display" id="eq:equate1130041">\[\begin{align}
AR(p=1) = \mu + \beta_1 y_{t-1} + \mathcal{E}_t \tag{11.42} 
\end{align}\]</span></p>
<p>Here, for a simple example, assume that our previous state corresponds to a value of 25. Assume that to be more conservative, we weight that value by using a multiplying factor of 0.5 — the <span class="math inline">\(\beta_1\)</span> coefficient for the <strong>first-order</strong> term. In addition, the overall average value is 20, which is our starting point or baseline.</p>
<p><span class="math display">\[
y_{t - 1} = 25\ \ \ \ \ \ \ \beta_1 = 0.5\ \ \ \ \ \ \mu = 20 
\]</span></p>
<p>Therefore, our predicted value <span class="math inline">\(\hat{S}\)</span> becomes:</p>
<p><span class="math display" id="eq:equate1130042">\[\begin{align}
\hat{S}(1) = y_t &amp;= \mu + \beta_1 y_{t-1}  \tag{11.43} \\
&amp;=20 + 0.5\times 25   \nonumber \\
&amp;=32.5 \nonumber
\end{align}\]</span></p>
<p>If only we know the actual state at the period (t), then we should be able to determine the amount of hidden information (whether white noise or any confounding factors) not covered by our prediction. For example, assume that the error is <span class="math inline">\(\mathcal{E}_t = 2\)</span>, then the actual state value computes to about the following:</p>
<p><span class="math display" id="eq:equate1130043">\[\begin{align}
S(1) = y_t &amp;= \mu + \beta_1 y_{t-1}  + \mathcal{E}_t \tag{11.44} \\
&amp;=20 + 0.5\times 25  + 2  \nonumber\\
&amp;=34.5   \nonumber
\end{align}\]</span></p>
<p>In this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as <strong>AR(1)</strong>, meaning that our autoregression model is based on the <strong>first-order</strong> term. Moreover, if we have to consider two periods (or lags) in the past, we can denote that as <strong>AR(2)</strong> so that the <strong>second-order</strong> formula then becomes:</p>
<p><span class="math display" id="eq:equate1130044">\[\begin{align}
AR(p=2) =  = \mu + \beta_1 y_{t-1} + \beta_2 y_{t-2}  + \mathcal{E}_t  \tag{11.45} 
\end{align}\]</span></p>
<p>We can determine the proper order (<strong>p</strong>) to use for <strong>AR(p)</strong> by using <strong>PACF</strong>, which will be covered later.</p>
<p><strong>Second</strong>, the <strong>Moving Average (MA)</strong> method allows us to deal with previous state errors instead of state values, as covered in our first step above for <strong>AR(p)</strong>. Our <strong>MA</strong> equation then looks like so:</p>
<p><span class="math display" id="eq:equate1130045">\[\begin{align}
MA(q) = \mu + \underbrace{\left(\sum_{i=1}^q \theta _i\mathcal{E}_{t-i}\right)}_\text{MA(q)} + \mathcal{E}_t = \mu + \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1} +  \theta_2 \mathcal{E}_{t-2}\ + ... +\ 
 \theta_q \mathcal{E}_{t-q} \tag{11.46} 
\end{align}\]</span></p>
<p>where <span class="math inline">\(\mathcal{E}_{t - q}\)</span> is the error at <strong>(t-q)</strong> time-period (or the <strong>lag</strong>) in the past, and the mean (<span class="math inline">\(\mu\)</span>) is the overall mean. The parameter <strong>q</strong> is the order of terms to use.</p>
<p>To illustrate, let us use <strong>first-order</strong> using the following equation:</p>
<p><span class="math display" id="eq:equate1130046">\[\begin{align}
MA(q = 1) = \mu + \theta_1 \mathcal{E}_{t-1}  \tag{11.47} 
\end{align}\]</span></p>
<p>Here, we continue to use our previous example in which we have a previous state corresponding to a value of 25. However, given that our overall average value is 20, we know that there is a 5-point error (or difference) in our previous state. Moreover, we still assume that to be more conservative, we weigh our error (not the state value) by using a multiplying factor of 2.5.</p>
<p><span class="math display">\[
y_{t - 1} = 25\ \ \ \ \ \ \ \theta_1 = 2.5\ \ \ \ \ \ \mu= 20\ \ \ \ \ \ \ \ \mathcal{E}_{t-1} = 5 
\]</span></p>
<p>Therefore, our predicted value <span class="math inline">\(\hat{S}\)</span> becomes:</p>
<p><span class="math display" id="eq:equate1130047">\[\begin{align}
\hat{S}(1) = y_t &amp;= \mu + \theta_1 \mathcal{E}_{t-1}  \tag{11.48} \\ 
&amp;=20 + 2.5\times 5  \nonumber \\
&amp;=32.5 \nonumber
\end{align}\]</span></p>
<p>A similar situation applies should we know the true value. Assume an error difference (white noise) of 2 at period (<strong>t</strong>); therefore, our true value becomes:</p>
<p><span class="math display" id="eq:equate1130048">\[\begin{align}
S(1) = y_t&amp;= \mu+ \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1}     \tag{11.49} \\
&amp;=20 + 2  + 2.5\times 5  \nonumber \\
&amp;=34.5 \nonumber
\end{align}\]</span></p>
<p>Also, in this particular example, we can say that we have modeled our forecast using only one period in the past. We can denote that as <strong>MA(1)</strong>, meaning that our moving average model is based on the <strong>first-order</strong>. Moreover, if we have to consider two periods in the past, we can denote that as <strong>MA(2)</strong> so that the <strong>second-order</strong> formula then becomes:</p>
<p><span class="math display" id="eq:equate1130049">\[\begin{align}
MA(q = 2) = \mu + \mathcal{E}_t + \theta_1 \mathcal{E}_{t-1} +  \theta_2 \mathcal{E}_{t-2} \tag{11.50} 
\end{align}\]</span></p>
<p>We can determine the proper order (<strong>q</strong>) for <strong>MA(q)</strong> by using <strong>ACF</strong>, which is discussed later.</p>
<p><strong>Third</strong>, the <strong>Integrated (I)</strong> method allows us to convert our <strong>Time-Series</strong> data from non-stationary to stationary series by a simple method called <strong>Differencing</strong>. Here, we denote <span class="math inline">\(\mathbf{I(d)}\)</span> as a function that performs differencing for a given <strong>Time-Series</strong> such that the parameter <strong>d</strong> indicates the degree of differencing (or how many iterations to take to difference the series).</p>
<p><span class="math display" id="eq:equate1130050">\[\begin{align}
I(d) = \{Y_{(t+1)} - Y_t\}_{t=1}^{(T-1)} \tag{11.51} 
\end{align}\]</span></p>
<p>To illustrate, the <strong>Time-Series</strong> data below can transform into the following new vector by <strong>first-degree</strong> differencing, e.g., <strong>I(1)</strong>:</p>
<p><span class="math display">\[
y = (10,9,5,7,6,4)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(1)} = (-1,-4, 2, -1,-2)
\]</span></p>
<p>If it is essential to perform <strong>second-degree</strong> differencing, e.g. <strong>I(2)</strong>, then we can iterate again:</p>
<p><span class="math display">\[
y = (10,9,5,7,6,4)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(1)} = (-1,-4, 2, -1,-2)\ \ \ \ \ \rightarrow\ \ \ \ 
z^{(2)} = (-3,6, -3, -1)
\]</span>
We can use any <strong>degree</strong> of differencing, e.g., <strong>I(d)</strong>, as we see necessary.</p>
<p><strong>Fourth</strong>, we can combine all three methods to form <strong>ARIMA(p,d,q)</strong>with the three given methods. The parameters allow us to fine-tune our formulae for an accurate forecast.</p>
<p><span class="math display" id="eq:equate1130051">\[\begin{align}
\text{ARIMA}(p, d, q) =  \mu + 
\underbrace{\left(\sum_{i=1}^p \beta_i y_{t-i}\right)}_\text{AR(p)} + 
\underbrace{\left(\sum_{i=1}^q \theta _i\mathcal{E}_{t-i}\right)}_\text{MA(q)} + \mathcal{E}_t \tag{11.52} 
\end{align}\]</span></p>
<p>Note that if we exclude the <strong>differencing</strong>, e.g., <strong>I(d)</strong>, our model becomes the <strong>ARMA(p,q)</strong> model. This model assumes that our <strong>Time-Series</strong> data is already stationary.</p>
<p><strong>Finally</strong>, to determine the number of orders to use for <strong>AR(p)</strong> and <strong>MA(q)</strong>, we have to rely on <strong>Autocorrelation</strong>, which allows us to compare the correlation of a time-series data between two time periods. Here, we introduce <strong>AutoCorrelation Function (ACF)</strong> and <strong>Partial AutoCorrelation Function (PACF)</strong>.</p>
<p><strong>AutoCorrelation Function (ACF)</strong>  </p>
<p>To start, we first determine the value of the <strong>q</strong> parameter for <strong>MA</strong>. In this case, we use <strong>ACF</strong>. We can illustrate this using the equation below:</p>
<p><span class="math display" id="eq:equate1130052">\[\begin{align}
ACF(k) = \frac{\sum_{t=k+1}^T \left(y_t - \bar{y}\right)\left(y_{t -k} - \bar{y}\right)}
   {\sum_{t=1}^T \left(y_t - \bar{y}\right)^2} \tag{11.53} 
\end{align}\]</span></p>
<p>Our example implementation of the <strong>ACF</strong> equation is as follows:</p>

<div class="sourceCode" id="cb1702"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1702-1" data-line-number="1">my.acf &lt;-<span class="st"> </span><span class="cf">function</span>(y) {</a>
<a class="sourceLine" id="cb1702-2" data-line-number="2">   N =<span class="st"> </span><span class="kw">length</span>(y)  </a>
<a class="sourceLine" id="cb1702-3" data-line-number="3">   y.mean =<span class="st"> </span><span class="kw">mean</span>(y)</a>
<a class="sourceLine" id="cb1702-4" data-line-number="4">   acf =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N   )</a>
<a class="sourceLine" id="cb1702-5" data-line-number="5">   <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">0</span><span class="op">:</span><span class="st"> </span>(N<span class="dv">-1</span>) ) {</a>
<a class="sourceLine" id="cb1702-6" data-line-number="6">     numer =<span class="st"> </span><span class="dv">0</span>; denom =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb1702-7" data-line-number="7">     <span class="cf">for</span> (t <span class="cf">in</span> (k<span class="op">+</span><span class="dv">1</span>)<span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1702-8" data-line-number="8">        numer =<span class="st"> </span>numer <span class="op">+</span><span class="st"> </span>(y[t] <span class="op">-</span><span class="st"> </span>y.mean) <span class="op">*</span><span class="st"> </span>(y[t<span class="op">-</span>k] <span class="op">-</span><span class="st"> </span>y.mean)</a>
<a class="sourceLine" id="cb1702-9" data-line-number="9">     }</a>
<a class="sourceLine" id="cb1702-10" data-line-number="10">     <span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1702-11" data-line-number="11">        denom =<span class="st"> </span>denom <span class="op">+</span><span class="st"> </span>(y[t] <span class="op">-</span><span class="st"> </span>y.mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb1702-12" data-line-number="12">     }</a>
<a class="sourceLine" id="cb1702-13" data-line-number="13">     acf[k <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] =<span class="st"> </span>numer <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb1702-14" data-line-number="14">   }</a>
<a class="sourceLine" id="cb1702-15" data-line-number="15">   acf</a>
<a class="sourceLine" id="cb1702-16" data-line-number="16">}</a></code></pre></div>

<p>To use the function, let us generate some random time-series and plot (see Figure <a href="11.4-time-series-forecasting.html#fig:tsdata">11.22</a>).</p>

<div class="sourceCode" id="cb1703"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1703-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1703-2" data-line-number="2">N =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb1703-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1703-4" data-line-number="4">y =<span class="st">  </span><span class="kw">cos</span>( <span class="kw">seq</span>(<span class="dv">0</span>, N, <span class="dt">length.out =</span> N)) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb1703-5" data-line-number="5">y =<span class="st"> </span><span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb1703-6" data-line-number="6">y =<span class="st"> </span><span class="kw">as.vector</span>(y)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsdata"></span>
<img src="DS_files/figure-html/tsdata-1.png" alt="Stationary (Gaussian Noise)" width="70%" />
<p class="caption">
Figure 11.22: Stationary (Gaussian Noise)
</p>
</div>

<p>Using our own implementation of <strong>ACF</strong>, we get the following:</p>

<div class="sourceCode" id="cb1704"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1704-1" data-line-number="1">acf1 =<span class="st"> </span><span class="kw">my.acf</span>(y );  <span class="kw">head</span>(acf1, <span class="dt">n=</span><span class="dv">30</span>) <span class="co"># display only first 30 values</span></a></code></pre></div>
<pre><code>##  [1]  1.00000  0.21814 -0.27968 -0.31197 -0.18164  0.12728  0.18185
##  [8]  0.17614 -0.05514 -0.34622 -0.20759 -0.01356  0.15699  0.14616
## [15]  0.12170 -0.11613 -0.23486 -0.06625  0.18138  0.22938 -0.02141
## [22] -0.14976 -0.15499 -0.12128  0.16216  0.17166  0.06235 -0.09066
## [29] -0.16201 -0.05920</code></pre>

<p>We can validate with <strong>acf(.)</strong> function from the built-in <strong>stats</strong> R package:</p>

<div class="sourceCode" id="cb1706"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1706-1" data-line-number="1">p =<span class="st"> </span>stats<span class="op">::</span><span class="kw">acf</span>(y, <span class="dt">type=</span><span class="st">&quot;correlation&quot;</span> , <span class="dt">plot=</span><span class="ot">FALSE</span>, <span class="dt">lag.max=</span>N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1706-2" data-line-number="2">acf2 =<span class="st"> </span><span class="kw">as.vector</span>(p<span class="op">$</span>acf);  <span class="kw">head</span>(acf1, <span class="dt">n=</span><span class="dv">30</span>) <span class="co"># display only first 30 values</span></a></code></pre></div>
<pre><code>##  [1]  1.00000  0.21814 -0.27968 -0.31197 -0.18164  0.12728  0.18185
##  [8]  0.17614 -0.05514 -0.34622 -0.20759 -0.01356  0.15699  0.14616
## [15]  0.12170 -0.11613 -0.23486 -0.06625  0.18138  0.22938 -0.02141
## [22] -0.14976 -0.15499 -0.12128  0.16216  0.17166  0.06235 -0.09066
## [29] -0.16201 -0.05920</code></pre>

<p>Now to compare if both are equal:</p>

<div class="sourceCode" id="cb1708"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1708-1" data-line-number="1"><span class="kw">all.equal</span>(acf1 , acf2 )</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>Additionally, we also accompany <strong>ACF</strong> with an estimation of our confidence level to help with our <strong>cut-off</strong>. However, first, let us recall the equation and corresponding implementation:</p>
<p><span class="math display" id="eq:equate1130053">\[\begin{align}
\pm z \times \left(\sigma / \sqrt{T}\right)
\ \ \ \ \ \ \text{where z-score} = 1.96 \text{ and }\sigma = 1 \tag{11.54} 
\end{align}\]</span></p>

<div class="sourceCode" id="cb1710"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1710-1" data-line-number="1">conf.interval &lt;-<span class="st"> </span><span class="cf">function</span>(y, alpha) {</a>
<a class="sourceLine" id="cb1710-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(y); z =<span class="st"> </span>alpha; sigma =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb1710-3" data-line-number="3">  <span class="kw">c</span>(sigma,<span class="op">-</span>sigma) <span class="op">*</span><span class="st"> </span>z<span class="op">/</span><span class="kw">sqrt</span>(N) </a>
<a class="sourceLine" id="cb1710-4" data-line-number="4">}</a></code></pre></div>

<p>A confidence level of 95% has a z=score of 1.96. This maps to the following ACF boundaries.</p>

<div class="sourceCode" id="cb1711"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1711-1" data-line-number="1">(<span class="dt">cl =</span> <span class="kw">conf.interval</span>(y, <span class="fl">1.96</span>))</a></code></pre></div>
<pre><code>## [1]  0.196 -0.196</code></pre>

<p>We now plot <strong>ACF</strong> - see Figure <a href="11.4-time-series-forecasting.html#fig:acfplot">11.23</a>. Notice that the correlation <strong>tails off</strong> for one with seasonality.</p>

<div class="sourceCode" id="cb1713"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1713-1" data-line-number="1">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(acf1))</a>
<a class="sourceLine" id="cb1713-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(x), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>),  <span class="dt">xaxt=</span><span class="st">&quot;n&quot;</span>,</a>
<a class="sourceLine" id="cb1713-3" data-line-number="3">      <span class="dt">xlab=</span><span class="st">&quot;Lag&quot;</span>,  <span class="dt">ylab=</span><span class="st">&quot;ACF&quot;</span>, </a>
<a class="sourceLine" id="cb1713-4" data-line-number="4">     <span class="dt">main=</span><span class="st">&quot;ACF Plot&quot;</span>,  <span class="dt">frame=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1713-5" data-line-number="5"><span class="kw">axis</span>(<span class="dt">side =</span> <span class="dv">1</span>, <span class="dt">at =</span> <span class="kw">seq</span>(<span class="dv">0</span>, N), <span class="dt">tcl =</span> <span class="fl">-0.3</span>, <span class="dt">labels =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb1713-6" data-line-number="6"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb1713-7" data-line-number="7"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb1713-8" data-line-number="8"><span class="kw">lines</span>(x, acf1, <span class="dt">type=</span><span class="st">&#39;h&#39;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb1713-9" data-line-number="9"><span class="kw">abline</span>(<span class="dt">h=</span><span class="kw">conf.interval</span>(y, <span class="fl">1.96</span>), <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:acfplot"></span>
<img src="DS_files/figure-html/acfplot-1.png" alt="ACF Plot" width="70%" />
<p class="caption">
Figure 11.23: ACF Plot
</p>
</div>

<p>With the confidence level drawn horizontally in the figure, any <strong>ACF</strong> value between <span class="math inline">\(\pm\)</span> (0.196) is considered as having zero correlation for a <strong>lag</strong>; and thus gets discarded.</p>
<p>Therefore, to finally obtain the order for our <strong>q</strong> parameter for <strong>MA</strong>, we get:</p>

<div class="sourceCode" id="cb1714"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1714-1" data-line-number="1">(<span class="dt">MA.q =</span> <span class="kw">which</span>(<span class="op">!</span><span class="kw">between</span>(acf1, cl[<span class="dv">2</span>], cl[<span class="dv">1</span>])))</a></code></pre></div>
<pre><code>##  [1]  1  2  3  4 10 11 17 20 47 56</code></pre>

<p><strong>Partial AutoCorrelation Function (PACF)</strong>  </p>
<p>Next, we use <strong>PACF</strong> to determine the value for the <strong>p</strong> parameter for <strong>AR</strong>. The basic idea of <strong>PACF</strong> starts with the equation below in which the value for <span class="math inline">\(Y_t\)</span> can be explained directly by the <strong>pth</strong> term, namely <span class="math inline">\(Y_{t-k}\)</span>. In other words, we suggest that there is some autocorrelation between <span class="math inline">\(Y_t\)</span> and <span class="math inline">\(Y_{t-k}\)</span>.</p>
<p><span class="math display" id="eq:equate1130054">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2}  + \beta_3 Y_{t-3}\ + ... +\ \beta_k Y_{t-k} \tag{11.55} 
\end{align}\]</span></p>
<p>Therefore, we can discard <span class="math inline">\(Y_{t-1},\  Y_{t-2},\ Y_{t-3}, ...,\ Y_{t-k+1}\)</span> for a <strong>Time-Series</strong> at lag <strong>k</strong>. What we are interested is the coefficient at lag <strong>k</strong>, namely <span class="math inline">\(\beta_k\)</span> from the term <span class="math inline">\(\beta_k Y_{t-k}\)</span>.</p>
<p>The value for <strong>PACF</strong> is obtained from the coefficient of the terminating term of the following equation.</p>
<p><span class="math display" id="eq:equate1130055">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2}  + \beta_3 Y_{t-3}\ + ... +\ \beta_k Y_{t-k} \tag{11.56} 
\end{align}\]</span></p>
<p>The terminating term of the equation above is based on a given <strong>Kth</strong> order. For example, <strong>PACF(K=2)</strong> equals the <span class="math inline">\(\beta_2\)</span> coefficient from the following equation:</p>
<p><span class="math display" id="eq:equate1130056">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2} \tag{11.57} 
\end{align}\]</span></p>
<p><strong>PACF(K=4)</strong> equals the <span class="math inline">\(\beta_4\)</span> coefficient from the following equation:</p>
<p><span class="math display" id="eq:equate1130057">\[\begin{align}
Y_t = \beta_0 + \beta_1 Y_{t-1}  + \beta_2 Y_{t-2} + \beta_3 Y_{t-3}  + \beta_4 Y_{t-4} \tag{11.58} 
\end{align}\]</span></p>
<p>Now, let us introduce three ways to calculate <strong>PACF</strong> for a system of equations. Each equation is generated for each value of K, namely <span class="math inline">\(k = 1,2,3,...,N\)</span>.</p>
<p>The first way is to use symmetric invertible <strong>Toeplitz matrix</strong>, which is simply a list of systems of equations — <strong>Yule-Walker</strong> equations. A generic <strong>Toeplitz matrix</strong> is shown below: </p>
<p><span class="math display" id="eq:eqnnumber503">\[\begin{align}
\underbrace{ 
\left[ 
\begin{array}{lllllll}
1 &amp; \rho_{1} &amp; \rho_{2 } &amp; \cdots &amp; \rho_{k-2} &amp; \rho_{k-1} \\
 \rho_{1}  &amp; 1 &amp; \rho_{1 } &amp; \cdots &amp; \rho_{k-3} &amp; \rho_{k-2} \\
 \rho_{2} &amp; \rho_{1} &amp; 1 &amp; \cdots &amp; \rho_{k-4} &amp; \rho_{k-3} \\
\vdots &amp;  \vdots &amp; \vdots&amp; \ddots &amp;\vdots &amp; \vdots\\
\rho_{k-2} &amp; \rho_{k-3}  &amp; \rho_{k-4} &amp; \cdots &amp; 1 &amp; \rho_{1} \\
\rho_{k-1}  &amp; \rho_{k-2} &amp; \rho_{k-3}&amp; \cdots &amp; \rho_{1} &amp; 1 \\
\end{array}
\right]
}_{\text{R}}
\underbrace{
\left[
\begin{array}{l}
\beta_1 \\
\beta_2 \\
\beta_3 \\
\vdots \\
\beta_{k-1} \\
\beta_k \\
\end{array}
\right]}_{\beta}
=
\underbrace{
\left[
\begin{array}{l}
\rho_1 \\
\rho_2 \\
\rho_3 \\
\vdots \\
\rho_{k-1} \\
\rho_k \\
\end{array}
\right]}_{\rho} \tag{11.59}
\end{align}\]</span></p>
<p>Without going through derivations, we compute for the coefficients like so:</p>
<p><span class="math display" id="eq:equate1130058">\[\begin{align}
\beta = R^{-1}\rho \tag{11.60} 
\end{align}\]</span></p>
<p>To illustrate, given <strong>k=5</strong>, we only consider the <span class="math inline">\(k \times x\)</span> dimension of <strong>R</strong> and the kth dimension of <span class="math inline">\(\rho\)</span> to solve for the <strong>PACF</strong> value at lag 5.</p>
<p><span class="math display" id="eq:equate1130059">\[\begin{align}
\underbrace{\beta}_{(1:k)} = \underbrace{R^{-1}}_{(1:k,1:k)}\underbrace{\rho}_{(1:k)} \tag{11.61} 
\end{align}\]</span></p>
<p>Let us use the vector of <strong>ACF</strong> we generated previously to create a <strong>Toeplitz matrix</strong> given <strong>k=5</strong>:</p>

<div class="sourceCode" id="cb1716"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1716-1" data-line-number="1">k=<span class="dv">5</span></a>
<a class="sourceLine" id="cb1716-2" data-line-number="2">R   =<span class="st"> </span><span class="kw">toeplitz</span>(acf1) </a>
<a class="sourceLine" id="cb1716-3" data-line-number="3">(<span class="dt">R.kk =</span> R[<span class="dv">1</span><span class="op">:</span>k, <span class="dv">1</span><span class="op">:</span>k])  <span class="co"># display only 5x5 toeplitz matrix for illustration</span></a></code></pre></div>
<pre><code>##         [,1]    [,2]    [,3]    [,4]    [,5]
## [1,]  1.0000  0.2181 -0.2797 -0.3120 -0.1816
## [2,]  0.2181  1.0000  0.2181 -0.2797 -0.3120
## [3,] -0.2797  0.2181  1.0000  0.2181 -0.2797
## [4,] -0.3120 -0.2797  0.2181  1.0000  0.2181
## [5,] -0.1816 -0.3120 -0.2797  0.2181  1.0000</code></pre>

<p>Then we use <strong>solve(.)</strong> to invert the matrix and solve for the coefficients.</p>

<div class="sourceCode" id="cb1718"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1718-1" data-line-number="1">beta.kk =<span class="st"> </span>acf1[<span class="dv">2</span><span class="op">:</span>(k <span class="op">+</span><span class="st"> </span><span class="dv">1</span> )]</a>
<a class="sourceLine" id="cb1718-2" data-line-number="2">(<span class="dt">coeffs =</span> <span class="kw">as.vector</span>( <span class="kw">solve</span>(R.kk) <span class="op">%*%</span><span class="st"> </span>beta.kk))</a></code></pre></div>
<pre><code>## [1]  0.20877 -0.33304 -0.11718 -0.19982  0.07212</code></pre>

<p>Out of the obtained coefficients, namely <span class="math inline">\(\beta_1\)</span> = 0.2088, <span class="math inline">\(\beta_2\)</span> = -0.333, <span class="math inline">\(\beta_3\)</span> = -0.1172, <span class="math inline">\(\beta_4\)</span> = -0.1998, and <span class="math inline">\(\beta_5\)</span> = 0.0721, we consider only PACF(k=5) = <span class="math inline">\(\beta_5\)</span> = 0.0721.</p>
<p>For an example implementation of <strong>PACF</strong>, we have the following:</p>

<div class="sourceCode" id="cb1720"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1720-1" data-line-number="1">my.pacf &lt;-<span class="st"> </span><span class="cf">function</span>(R, rho) {</a>
<a class="sourceLine" id="cb1720-2" data-line-number="2">  N =<span class="st"> </span><span class="kw">length</span>(rho) </a>
<a class="sourceLine" id="cb1720-3" data-line-number="3">  pacf =<span class="st"> </span><span class="ot">NULL</span></a>
<a class="sourceLine" id="cb1720-4" data-line-number="4">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>N) {</a>
<a class="sourceLine" id="cb1720-5" data-line-number="5">    beta =<span class="st"> </span><span class="kw">solve</span>(R[<span class="dv">1</span><span class="op">:</span>k, <span class="dv">1</span><span class="op">:</span>k]) <span class="op">%*%</span><span class="st"> </span>rho[<span class="dv">1</span><span class="op">:</span>k]</a>
<a class="sourceLine" id="cb1720-6" data-line-number="6">    pacf =<span class="st"> </span><span class="kw">c</span>(pacf, beta[k])</a>
<a class="sourceLine" id="cb1720-7" data-line-number="7">  }</a>
<a class="sourceLine" id="cb1720-8" data-line-number="8">  pacf</a>
<a class="sourceLine" id="cb1720-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb1720-10" data-line-number="10">P   =<span class="st"> </span><span class="kw">length</span>(acf1) </a>
<a class="sourceLine" id="cb1720-11" data-line-number="11">rho =<span class="st"> </span>acf1[<span class="dv">2</span><span class="op">:</span>P]</a>
<a class="sourceLine" id="cb1720-12" data-line-number="12"><span class="kw">head</span>(<span class="kw">my.pacf</span>(R, rho), <span class="dt">n=</span><span class="dv">30</span>)  <span class="co"># for k=1,2,...,N</span></a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p><strong>Alternatively</strong>, to solve for <strong>PACF</strong> without using <strong>Toeplitz matrix</strong>, we can use the <strong>Levinson-Durbin</strong> Algorithm. The Algorithm comes in two forms, one of which obtains the backward prediction coefficients <span class="citation">(Borchers B. <a href="bibliography.html#ref-ref1048b">2001</a>)</span>. </p>
<p>Step 1: <span class="math inline">\(\beta_{1} = \frac{\rho_1}{\rho_0},\ \ \ P_1 = \beta_{1}\)</span></p>
<p>Step 2: For <span class="math inline">\(n = 2,3,...,p\)</span>, compute for the updates:</p>
<p><span class="math display" id="eq:equate1130062" id="eq:equate1130061" id="eq:equate1130060">\[\begin{align}
\beta_{nn} &amp;= 
   \frac{\rho_n - \sum_{k=1}^{n-1} \beta_{n-1,k} \rho_{n-k}}
         {1 - \sum_{k=1}^{n-1} \beta_{n-1,k} \rho_k} \tag{11.62} \\
\beta_{nk} &amp;= \beta_{n-1} - \beta_{nn}\ \beta_{n-1, n-k},\ \ \ \  \ where\ \ \ \ \ {k=1,2,...,n-1}  \tag{11.63} \\
P_{n} &amp;= \beta_{nn} \tag{11.64} 
\end{align}\]</span></p>
<p>We leave readers to investigate the algorithm to obtain the forward prediction coefficients as an exercise.</p>
<p>Below is an example implementation of the <strong>Levinson-Durbin</strong> algorithm (motivated by the original R script from Ross Ihaka’s Statistical 726 Course <span class="citation">(<a href="bibliography.html#ref-ref1023r">n.d.</a>)</span> with modification following the algorithm above):</p>

<div class="sourceCode" id="cb1722"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1722-1" data-line-number="1">my.Levinson.Durbin =<span class="st"> </span><span class="cf">function</span>(rho, <span class="dt">lag.max =</span> <span class="dv">2</span>) {</a>
<a class="sourceLine" id="cb1722-2" data-line-number="2">    N =<span class="st"> </span><span class="kw">length</span>(rho)</a>
<a class="sourceLine" id="cb1722-3" data-line-number="3">    beta =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, N <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1722-4" data-line-number="4">    pacf =<span class="st"> </span>beta[<span class="dv">1</span>] =<span class="st"> </span>rho[<span class="dv">2</span>] <span class="op">/</span><span class="st"> </span>rho[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb1722-5" data-line-number="5">    rho =<span class="st"> </span>rho[<span class="dv">2</span><span class="op">:</span>(lag.max<span class="op">+</span><span class="dv">1</span>)]</a>
<a class="sourceLine" id="cb1722-6" data-line-number="6">    pacf =<span class="st"> </span><span class="kw">c</span>(beta[<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb1722-7" data-line-number="7">    <span class="cf">for</span> (n <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>lag.max) {</a>
<a class="sourceLine" id="cb1722-8" data-line-number="8">        beta.n   =<span class="st"> </span>beta[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)]</a>
<a class="sourceLine" id="cb1722-9" data-line-number="9">        rho.n   =<span class="st"> </span>rho[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)]  </a>
<a class="sourceLine" id="cb1722-10" data-line-number="10">        beta[n]  =<span class="st"> </span>( rho[n] <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>( beta.n <span class="op">*</span><span class="st"> </span><span class="kw">rev</span>(rho.n) )) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb1722-11" data-line-number="11"><span class="st">                     </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">sum</span>( beta.n <span class="op">*</span><span class="st">  </span>rho.n))</a>
<a class="sourceLine" id="cb1722-12" data-line-number="12">        beta[<span class="dv">1</span><span class="op">:</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)] =<span class="st"> </span>beta.n <span class="op">-</span><span class="st"> </span>beta[n] <span class="op">*</span><span class="st"> </span><span class="kw">rev</span>(beta.n)</a>
<a class="sourceLine" id="cb1722-13" data-line-number="13">        pacf =<span class="st"> </span><span class="kw">c</span>(pacf, beta[n])</a>
<a class="sourceLine" id="cb1722-14" data-line-number="14">    }</a>
<a class="sourceLine" id="cb1722-15" data-line-number="15">    pacf </a>
<a class="sourceLine" id="cb1722-16" data-line-number="16">}</a>
<a class="sourceLine" id="cb1722-17" data-line-number="17">pacf1 =<span class="st"> </span><span class="kw">my.Levinson.Durbin</span>(acf1, <span class="dt">lag.max =</span> (P<span class="dv">-1</span>))</a>
<a class="sourceLine" id="cb1722-18" data-line-number="18"><span class="kw">head</span>(pacf1, <span class="dt">n=</span><span class="dv">30</span>)</a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p>Alternatively, we can still use <strong>pacf(.)</strong> function with type equal to <strong>partial</strong>:</p>

<div class="sourceCode" id="cb1724"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1724-1" data-line-number="1">pacf2 =<span class="st"> </span><span class="kw">as.vector</span>( <span class="kw">pacf</span>(y, <span class="dt">type=</span><span class="st">&quot;partial&quot;</span>, <span class="dt">lag.max=</span>(P<span class="dv">-1</span>), <span class="dt">plot=</span><span class="ot">FALSE</span>)<span class="op">$</span>acf)</a>
<a class="sourceLine" id="cb1724-2" data-line-number="2"><span class="kw">head</span>(pacf2, <span class="dt">n=</span><span class="dv">30</span>)</a></code></pre></div>
<pre><code>##  [1]  0.218137 -0.343610 -0.184589 -0.185733  0.072116 -0.017177
##  [7]  0.153925 -0.078614 -0.223876 -0.086100 -0.138184 -0.046556
## [13] -0.062338  0.154348 -0.134410 -0.033726 -0.048870  0.076038
## [19] -0.006173 -0.113236 -0.042865 -0.095876 -0.077892  0.083693
## [25] -0.041574 -0.001806 -0.005386 -0.024463 -0.066546 -0.008387</code></pre>

<p>Let us validate:</p>

<div class="sourceCode" id="cb1726"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1726-1" data-line-number="1"><span class="kw">all.equal</span>(pacf1, pacf2)</a></code></pre></div>
<pre><code>## [1] TRUE</code></pre>

<p>We now plot <strong>PACF</strong> - see Figure <a href="11.4-time-series-forecasting.html#fig:pacfplot">11.24</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:pacfplot"></span>
<img src="DS_files/figure-html/pacfplot-1.png" alt="PACF Plot" width="70%" />
<p class="caption">
Figure 11.24: PACF Plot
</p>
</div>

<p>Notice that both <strong>ACF</strong> and <strong>PACF</strong> plots show some behavior in which the first couple of lags are beyond the confidence levels, indicating a strong correlation of the <strong>first lag</strong> lags at least with the current time, e.g., <span class="math inline">\(Y_t\)</span>. Then the rest of the lags <strong>tail off</strong> — meaning the correlation diminishes gradually. On the other hand, there are cases in which the plots show a strong correlation at the beginning, then suddenly <strong>cuts off</strong>. For example, in the <strong>ACF</strong> plot, if we see a strong correlation at the beginning of the lag, e.g., lag 1, and <strong>cuts off</strong> for the subsequent lags, then we can interpret that to mean that <strong>AR(1)</strong> or <strong>MA(1)</strong> is most likely the configuration we seek in which <strong>p</strong> for <strong>AR</strong> or <strong>q</strong> for <strong>MA</strong> follows a parameter value of 1.</p>
<p>Other literature explains signs of <strong>Seasonality</strong> or <strong>Trend</strong> reflected in the plots. As an exercise, we leave readers to investigate other examples of <strong>ACF</strong> and <strong>PACF</strong> plots and review their interpretations.</p>
</div>
<div id="multiplicative-seasonal-arima-sarima" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.5</span> Multiplicative Seasonal ARIMA (SARIMA) <a href="11.4-time-series-forecasting.html#multiplicative-seasonal-arima-sarima" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>ARIMA</strong> models are mainly used for <strong>stationary</strong> time series. To address <strong>non-stationary</strong> time-series, we can use <strong>SARIMA</strong>, which is written in the following expression:</p>
<p><span class="math display" id="eq:equate1130063">\[\begin{align}
\text{ARIMA}\underbrace{(p,d,q)}_{\text{non-seasonal}} \times \underbrace{(P,D,Q)}_{\text{seasonal}} m \tag{11.65} 
\end{align}\]</span>
where:</p>
<p><span class="math display">\[\begin{align*}
\begin{array}{ll}
\mathbf{p}\ \text{is non-seasonal param for AR order} &amp; 
\mathbf{P}\ \text{is seasonal param for AR order} \\
\mathbf{d}\ \text{is non-seasonal param for differencing} &amp; 
\mathbf{D}\ \text{is seasonal param for differencing} \\
\mathbf{q}\ \text{is non-seasonal param for MA order} &amp; 
\mathbf{Q}\ \text{is seasonal param for MA order}
\end{array}\\
\mathbf{m}\ \text{number of periods to cover for a given frequency or season}
\end{align*}\]</span></p>
<p>To understand <strong>SARIMA</strong>, let us introduce the <strong>Backshift operator</strong> denoted simply as <strong>B</strong>, which shifts a given period one period back. Below are examples of backshifting a given period:</p>
<p><span class="math display" id="eq:equate1130064">\[\begin{align}
B (Y_t) = Y_{t - 1}\ \ \ \ \ \ \ \ \ \
B (Y_{t-1}) = Y_{t - 2}\ \ \ \ \ \ \ \ \ \ \
B^2 (Y_t) = B (Y_{t-1}) = Y_{t - 2}\ \ \ \ \ \  \tag{11.66} 
\end{align}\]</span></p>
<p>Another example is to use the same notation for differencing:</p>
<p><span class="math display" id="eq:equate1130065">\[\begin{align}
\underbrace{Y_t - Y_{t - 1} = ( 1 - B) y_t}_{\text{1st difference}}\ \ \ \ \ \ \ \ 
\underbrace{Y_t - Y_{t - 1} - Y_{t - 2} = ( 1 - B)^2 y_t}_{\text{2nd difference}} \tag{11.67} 
\end{align}\]</span></p>
<p>The last example is to use the same notation to backshift a season to <strong>m</strong> periods:</p>
<p><span class="math display" id="eq:equate1130066">\[\begin{align}
(1 - B^m)y_t \tag{11.68} 
\end{align}\]</span></p>
<p>where <strong>m</strong> is the number of periods to backshift in a given season.</p>
<p><strong>Backshift operator</strong> follows both the algebraic and polynomial rules so that given AR(1) and AR(2), respectively, as an example, we start with the usual equations:</p>
<p><span class="math display" id="eq:equate1130067">\[\begin{align}
y_t = \mu + \beta_1 y_{t-1}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
y_t = \mu + \beta_1 y_{t-1} + \beta_2 y_{t-2} \tag{11.69} 
\end{align}\]</span></p>
<p>We then re-arrange our equation</p>
<p><span class="math display" id="eq:eqnnumber504">\[\begin{align}
\begin{array}{ll}
y_t - \beta_1 y_{t-1}   &amp;= \mu \\
(1 - \beta_1 B  )y_t &amp;= \mu 
\end{array}\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \
\begin{array}{ll}
y_t - \beta_1 y_{t-1} - \beta_2 y_{t-2} &amp;= \mu \\
(1 - \beta_1 B - \beta_2 B^2)y_t &amp;= \mu 
\end{array} \tag{11.70}
\end{align}\]</span></p>
<p>The corresponding polynomial for the <strong>backshift operation</strong> becomes:</p>
<p><span class="math display" id="eq:equate1130068">\[\begin{align}
\beta (B) = (1 - \beta_1 B)\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
\beta (B) = (1 - \beta_1 B - \beta_2 B^2) \tag{11.71} 
\end{align}\]</span></p>
<p>so then we get the final polynomial notation (which applies for both AR(1) and AR(2) generically):</p>
<p><span class="math display">\[
\beta(B) y_t = \mu
\]</span></p>
<p>In terms of <strong>SARIMA</strong> modeling, we start with a notation as expressed below:</p>
<p><span class="math display">\[\mathbf{\text{SARIMA}(2,1,1)(1,2,1)_{4}}\]</span>.</p>
<p>Here, we break the notation into the following:</p>
<p><span class="math display" id="eq:eqnnumber505">\[\begin{align}
\underbrace{
\begin{array}{rllll} 
AR(2)\ &amp;\rightarrow\ y_t &amp;= \beta_1 y_{t-1} + \beta_2 y_{t-2}\\
I(1)\ &amp;\rightarrow\ \Delta y_t &amp;= y_t -  y_{t-1}\\
MA(1)\ &amp;\rightarrow\ \mathcal{E}_t &amp;= \theta_1 \mathcal{E}_{t-1}\\
\end{array}}_{\text{non-seasonal}} \left|
\underbrace{
\begin{array}{rllll}
AR(1)\ &amp;\rightarrow\ y_t &amp;= \alpha_1 y_{t-1} \\
I(2)\ &amp;\rightarrow\ \Delta y_t &amp;= y_t -  y_{t-1} - y_{t-2}\\
MA(2)\ &amp;\rightarrow\ \mathcal{E}_t &amp;= \phi_1 \mathcal{E}_{t-1}  \\
\end{array}}_{\text{seasonal}}
\right. \tag{11.72}
\end{align}\]</span></p>
<p>Using the <strong>backshift notation</strong>, we write the following:</p>
<p><span class="math display" id="eq:eqnnumber506">\[\begin{align}
\underbrace{
\begin{array}{rll}
AR(2)\ &amp;\rightarrow\ (1 -\beta_1 B - \beta_2 B^2) y_t \\
I(1)\ &amp;\rightarrow\ (1 - B)y_t\\
MA(1)\ &amp;\rightarrow\ (1 - \theta_1 B )\mathcal{E}_t   \\
\end{array}}_{\text{non-seasonal}} 
\left|
\underbrace{
\begin{array}{rll}
AR(1)\ &amp;\rightarrow\ (1 - \alpha B^4)y_t  \\
I(2)\ &amp;\rightarrow\ (1 - B^4)^2y_t\\
MA(2)\ &amp;\rightarrow\ (1 -\phi_1 B^4) \mathcal{E}_t \\
\end{array}}_{\text{seasonal}}
\right. \tag{11.73}
\end{align}\]</span></p>
<p>Therefore, the final <strong>SARIMA</strong> model, namely <span class="math inline">\(\text{SARIMA}(2,1,1)(1,2,1)_{\mathbf{4}}\)</span> corresponds to the following operation.</p>
<p><span class="math display" id="eq:equate1130069">\[\begin{align}
(1 -\beta_1 B - \beta_2 B^2) (1 - B)(1 - \alpha B^4)(1 - B^4)^2y_t = (1 - \theta_1 B )(1 -\phi_1 B^4) \mathcal{E}_t \tag{11.74} 
\end{align}\]</span></p>
</div>
<div id="time-series-decomposition" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.6</span> Time-Series Decomposition <a href="11.4-time-series-forecasting.html#time-series-decomposition" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In Chapter <strong>4</strong> (<strong>Numerical Calculus</strong>)}, we covered <strong>Fourier Series and Transformation</strong>. We expressed the idea that a large wave can be represented as a convolution of smaller waves such that we can, as an example, mathematically decompose the large wave equation into a series of sinusoidal terms.</p>
<p>In this section, the same idea applies. However, the opposite applies in that combining terms is equivalent to applying the effect of <strong>smoothing</strong> the series.</p>
<p>We can use smoothing methods such as splines and local regression to decompose trend, seasonality, and random noise.</p>
<p><span class="math display" id="eq:equate1130070">\[\begin{align}
yT = f(sT, tT, rT) = sT + tT + rT \tag{11.75} 
\end{align}\]</span></p>
<p>When components are multiplicative, then use <span class="math inline">\(\log_e(.)\)</span>.</p>
<p><span class="math display" id="eq:equate1130071">\[\begin{align}
yT = f(sT, tT, rT) = log(sT) + log(tT) + log(rT) \tag{11.76} 
\end{align}\]</span></p>
<p>To illustrate, let us concoct a dataset that covers 24 periods:</p>

<div class="sourceCode" id="cb1728"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1728-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb1728-2" data-line-number="2">N =<span class="st"> </span><span class="dv">24</span></a>
<a class="sourceLine" id="cb1728-3" data-line-number="3">e =<span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb1728-4" data-line-number="4">(<span class="dt">y =</span>  <span class="kw">cos</span>( <span class="kw">seq</span>(<span class="dv">0</span>, N, <span class="dt">length.out =</span> N)) <span class="op">+</span><span class="st"> </span>e )</a></code></pre></div>
<pre><code>##  [1]  1.3770  0.8048 -1.5916 -2.1303 -3.3094  1.2044  1.9389  0.2930
##  [9]  1.2851 -0.8821 -1.3850  1.3734  2.1954  0.1697 -0.5775  0.8016
## [17]  1.1534 -2.5945 -1.2912  0.6182  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>For a monthly period, we can change the <strong>frequency</strong> to 12. Since our data set has a stretch of 24 periods, then we cover two years:</p>

<div class="sourceCode" id="cb1730"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1730-1" data-line-number="1">(<span class="dt">ts.data =</span> <span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">12</span>))</a></code></pre></div>
<pre><code>##          Jan     Feb     Mar     Apr     May     Jun     Jul     Aug
## 2018  1.3770  0.8048 -1.5916 -2.1303 -3.3094  1.2044  1.9389  0.2930
## 2019  2.1954  0.1697 -0.5775  0.8016  1.1534 -2.5945 -1.2912  0.6182
##          Sep     Oct     Nov     Dec
## 2018  1.2851 -0.8821 -1.3850  1.3734
## 2019  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>Furthermore, for a quarterly period, we can change the <strong>frequency</strong> to 4. Similarly, since our data stretches to 24 periods, then we cover six years:</p>

<div class="sourceCode" id="cb1732"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1732-1" data-line-number="1">(<span class="dt">ts.data =</span> <span class="kw">ts</span> (<span class="dt">data =</span> y, <span class="dt">start =</span> <span class="kw">c</span>(<span class="dv">2018</span>, <span class="dv">1</span>),  <span class="dt">frequency=</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  1.3770  0.8048 -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012 -0.2509  0.3510</code></pre>

<p>Let us now decompose by <strong>additive</strong> method:</p>

<div class="sourceCode" id="cb1734"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1734-1" data-line-number="1">(<span class="dt">ts.decomposed =</span> <span class="kw">decompose</span>( ts.data , <span class="st">&quot;additive&quot;</span>))</a></code></pre></div>
<pre><code>## $x
##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  1.3770  0.8048 -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012 -0.2509  0.3510
## 
## $seasonal
##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018  0.6753 -0.4334 -0.5115  0.2695
## 2019  0.6753 -0.4334 -0.5115  0.2695
## 2020  0.6753 -0.4334 -0.5115  0.2695
## 2021  0.6753 -0.4334 -0.5115  0.2695
## 2022  0.6753 -0.4334 -0.5115  0.2695
## 2023  0.6753 -0.4334 -0.5115  0.2695
## 
## $trend
##          Qtr1     Qtr2     Qtr3     Qtr4
## 2018       NA       NA -0.97084 -1.50667
## 2019 -1.01542 -0.27120  0.60603  0.91954
## 2020  0.24325 -0.03718  0.21165  0.45691
## 2021  0.68931  0.71877  0.51704  0.04127
## 2022 -0.39347 -0.50561 -0.45519 -0.04488
## 2023  0.42212  0.51877       NA       NA
## 
## $random
##          Qtr1     Qtr2     Qtr3     Qtr4
## 2018       NA       NA -0.10928 -0.89318
## 2019 -2.96928  1.90897  1.84429 -0.89605
## 2020  0.36654 -0.41150 -1.08518  0.64700
## 2021  0.83073 -0.11568 -0.58310  0.49082
## 2022  0.87151 -1.65549 -0.32457  0.39357
## 2023  0.64265  0.01586       NA       NA
## 
## $figure
## [1]  0.6753 -0.4334 -0.5115  0.2695
## 
## $type
## [1] &quot;additive&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;decomposed.ts&quot;</code></pre>

<p>Now, let us try to reconstruct by adding:</p>

<div class="sourceCode" id="cb1736"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1736-1" data-line-number="1">(ts.decomposed<span class="op">$</span>seasonal <span class="op">+</span><span class="st"> </span>ts.decomposed<span class="op">$</span>trend <span class="op">+</span><span class="st"> </span>ts.decomposed<span class="op">$</span>random )</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018      NA      NA -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012      NA      NA</code></pre>

<p>Let us also decompose by a <strong>multiplicative</strong> method and try to reconstruct by multiplying. We should see the same time-series data.</p>

<div class="sourceCode" id="cb1738"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1738-1" data-line-number="1">ts.decomposed =<span class="st"> </span><span class="kw">decompose</span>( ts.data , <span class="st">&quot;multiplicative&quot;</span>)</a>
<a class="sourceLine" id="cb1738-2" data-line-number="2">(ts.decomposed<span class="op">$</span>seasonal <span class="op">*</span><span class="st"> </span>ts.decomposed<span class="op">$</span>trend <span class="op">*</span><span class="st"> </span>ts.decomposed<span class="op">$</span>random )</a></code></pre></div>
<pre><code>##         Qtr1    Qtr2    Qtr3    Qtr4
## 2018      NA      NA -1.5916 -2.1303
## 2019 -3.3094  1.2044  1.9389  0.2930
## 2020  1.2851 -0.8821 -1.3850  1.3734
## 2021  2.1954  0.1697 -0.5775  0.8016
## 2022  1.1534 -2.5945 -1.2912  0.6182
## 2023  1.7401  0.1012      NA      NA</code></pre>

<p>We can then plot and see what it looks like. See Figure <a href="11.4-time-series-forecasting.html#fig:tsdecompose">11.25</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tsdecompose"></span>
<img src="DS_files/figure-html/tsdecompose-1.png" alt="Time-Series Decomposition" width="70%" />
<p class="caption">
Figure 11.25: Time-Series Decomposition
</p>
</div>

<p>In the next section, let us perform prediction using <strong>ARIMA/SARIMA</strong>.</p>
</div>
<div id="stl-with-aicbic" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.7</span> STL with AIC/BIC<a href="11.4-time-series-forecasting.html#stl-with-aicbic" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In our recent discussion on <strong>Arima and Sarima</strong>, we have introduced models in the form of linear equations. Such linear equations, in a <strong>Linear Model</strong>, as discussed in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under <strong>AIC</strong> and <strong>BIC</strong>, can be transformed such that the <strong>coefficients</strong> are evaluated for <strong>relevance</strong>.</p>
<p>To illustrate, let us use <strong>stlf(.)</strong> to apply <strong>ARIMA</strong> method and <strong>AIC</strong> for <strong>information criteria</strong> to our recent <strong>ts.data</strong> time series in the previous section. Here, we try to forecast what it looks like for the first two quarters in the year following the time series, namely 2024.</p>

<div class="sourceCode" id="cb1740"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1740-1" data-line-number="1"><span class="kw">library</span>(forecast)</a>
<a class="sourceLine" id="cb1740-2" data-line-number="2"><span class="kw">stlf</span>(ts.data, <span class="dt">h=</span><span class="dv">2</span>,  <span class="dt">method=</span><span class="st">&#39;arima&#39;</span>, <span class="dt">ic=</span><span class="st">&#39;bic&#39;</span>)</a></code></pre></div>
<pre><code>##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2024 Q1         0.7659 -0.9617 2.493 -1.876 3.408
## 2024 Q2        -0.4565 -2.1840 1.271 -3.099 2.186</code></pre>

<p>Another method of interest is <strong>Error Trend Seasonality (ETS)</strong>. Again, let us use <strong>AIC</strong> this time with <strong>MAE</strong> for optimization criteria.</p>

<div class="sourceCode" id="cb1742"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1742-1" data-line-number="1"><span class="kw">stlf</span>(ts.data, <span class="dt">h=</span><span class="dv">2</span>,  <span class="dt">method=</span><span class="st">&#39;ets&#39;</span>, <span class="dt">ic=</span><span class="st">&#39;aic&#39;</span>, <span class="dt">opt.crit=</span><span class="st">&#39;mae&#39;</span>)</a></code></pre></div>
<pre><code>##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2024 Q1        1.20850 -0.6654 3.082 -1.657 4.074
## 2024 Q2       -0.01387 -1.8878 1.860 -2.880 2.852</code></pre>

<p>We leave readers to investigate <strong>ETS</strong>.</p>
</div>
<div id="multivariate-time-series" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.8</span> Multivariate Time-Series<a href="11.4-time-series-forecasting.html#multivariate-time-series" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In real-world scenarios, time series can be complex. This section is to account for multivariate features in a time series which we can base on the following linear equation with two independent variables and one dependent variable:</p>
<p><span class="math display" id="eq:equate1130072">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 \tag{11.77} 
\end{align}\]</span></p>
<p>We can concoct a simple multivariate data point (using sine with noise) like so:</p>

<div class="sourceCode" id="cb1744"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1744-1" data-line-number="1">N     =<span class="st"> </span><span class="dv">12</span></a>
<a class="sourceLine" id="cb1744-2" data-line-number="2">x1    =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,N) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N,<span class="dv">0</span>,<span class="dv">1</span>)     </a>
<a class="sourceLine" id="cb1744-3" data-line-number="3">x2    =<span class="st"> </span><span class="kw">rep</span>( <span class="kw">sin</span>(<span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">4</span>)), <span class="dv">3</span>) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(N, <span class="dv">0</span>, <span class="dv">1</span>) </a>
<a class="sourceLine" id="cb1744-4" data-line-number="4">beta0 =<span class="st"> </span><span class="fl">0.5</span>; beta1 =<span class="st"> </span><span class="fl">0.7</span>; beta2 =<span class="st"> </span><span class="fl">1.7</span></a>
<a class="sourceLine" id="cb1744-5" data-line-number="5">y     =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>beta2 <span class="op">*</span><span class="st"> </span>x2</a>
<a class="sourceLine" id="cb1744-6" data-line-number="6">(<span class="dt">m    =</span> <span class="kw">as.matrix</span>(<span class="kw">cbind</span>(x1,x2, y)))</a></code></pre></div>
<pre><code>##           x1      x2      y
##  [1,]  1.834  0.5559  2.729
##  [2,]  2.199  0.9853  3.714
##  [3,]  4.298 -0.4192  2.796
##  [4,]  4.937 -0.3096  3.429
##  [5,]  4.853  1.7500  6.872
##  [6,]  6.110  0.4042  5.465
##  [7,]  6.187 -0.1599  4.559
##  [8,]  7.256 -1.4828  3.059
##  [9,] 10.095 -0.3386  6.991
## [10,] 12.435  1.1624 11.181
## [11,] 11.388 -0.2296  8.081
## [12,] 12.291 -0.7346  7.855</code></pre>

<p>Then converting to time-series, our <strong>time-series</strong> starts from the year 2018 and stretches through 2020 (on a quarterly basis):</p>

<div class="sourceCode" id="cb1746"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1746-1" data-line-number="1">(<span class="dt">ts.multiv =</span> <span class="kw">ts</span>(m, <span class="dt">start =</span><span class="kw">c</span>(<span class="dv">2018</span>,<span class="dv">1</span>), <span class="dt">frequency=</span><span class="dv">4</span>))</a></code></pre></div>
<pre><code>##             x1      x2      y
## 2018 Q1  1.834  0.5559  2.729
## 2018 Q2  2.199  0.9853  3.714
## 2018 Q3  4.298 -0.4192  2.796
## 2018 Q4  4.937 -0.3096  3.429
## 2019 Q1  4.853  1.7500  6.872
## 2019 Q2  6.110  0.4042  5.465
## 2019 Q3  6.187 -0.1599  4.559
## 2019 Q4  7.256 -1.4828  3.059
## 2020 Q1 10.095 -0.3386  6.991
## 2020 Q2 12.435  1.1624 11.181
## 2020 Q3 11.388 -0.2296  8.081
## 2020 Q4 12.291 -0.7346  7.855</code></pre>

<p>We can then plot and see what it looks like. See Figure <a href="11.4-time-series-forecasting.html#fig:multivseries">11.26</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:multivseries"></span>
<img src="DS_files/figure-html/multivseries-1.png" alt="Multivariate Time-Series" width="70%" />
<p class="caption">
Figure 11.26: Multivariate Time-Series
</p>
</div>

<p>Let us fit our model to data and predict (forecast what it looks like in the year 2021, covering the first two quarters):</p>

<div class="sourceCode" id="cb1748"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1748-1" data-line-number="1">ts.fit =<span class="st"> </span><span class="kw">tslm</span>(ts.multiv <span class="op">~</span><span class="st"> </span>trend <span class="op">+</span><span class="st"> </span>season)    </a>
<a class="sourceLine" id="cb1748-2" data-line-number="2">(<span class="dt">f  =</span> <span class="kw">forecast</span>(ts.fit, <span class="dt">h=</span><span class="dv">2</span>))   </a></code></pre></div>
<pre><code>## x1
##         Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## 2021 Q1          13.83 11.79 15.86 10.43 17.23
## 2021 Q2          15.15 13.12 17.19 11.75 18.55
## 
## x2
##         Point Forecast   Lo 80 Hi 80  Lo 95 Hi 95
## 2021 Q1         0.4175 -0.8653 1.700 -1.726 2.561
## 2021 Q2         0.6124 -0.6704 1.895 -1.532 2.756
## 
## y
##         Point Forecast Lo 80 Hi 80 Lo 95 Hi 95
## 2021 Q1          10.89 8.225 13.56 6.437 15.34
## 2021 Q2          12.15 9.481 14.81 7.693 16.60</code></pre>

<p>Note that <strong>Multivariate Time-Series</strong> is different from <strong>Multiple Time-Series</strong>. The former pertains to dependent variables, namely <strong>X</strong>, while the latter pertains to independent variables, namely <strong>Y</strong>. Additionally, the latter considers different time series in different state spaces, depending on different factors and conditions; hence <strong>multiple series</strong>. We cover that in the next section.</p>
</div>
<div id="forecasting-considerations" class="section level3 hasAnchor">
<h3><span class="header-section-number">11.4.9</span> Forecasting Considerations<a href="11.4-time-series-forecasting.html#forecasting-considerations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are many considerations to make when dealing with time-series data and forecasting. Our time-series data is mixed with noise and many other factors in real-world scenarios. Our goal is to perform some level of pre-processing to clean our data.</p>
<p><strong>Denoising Time-series</strong></p>
<p>One of the essential preparations to make is to reduce the amount of noise from our data. There are many methods of denoising time series. Each method may be more specific and relative to certain domains. In general, however, we are listing below a few suggested methods:</p>
<ul>
<li><strong>Rolling and Expanding Window</strong> method</li>
<li><strong>Wavelet Transform</strong> method</li>
<li><strong>Spline Smoothing</strong> method</li>
<li><strong>Convolution</strong> method</li>
</ul>
<p>We leave readers to investigate each of the mentioned denoising methods.</p>
<p><strong>Sampling Time-Series</strong></p>
<p>Another important step to make is when sampling our data. Take <strong>market trading</strong> as an example. Certain period is affected by <strong>Job Employment status</strong>, <strong>10-year notes status</strong>, <strong>market cycles</strong>, <strong>industry cycle</strong>, <strong>fiscal reports</strong>, and now including the recent <strong>Covid-19 Pandemic</strong>. When sampling for data, do we need to skip the year 2020, considering the unique condition compared to any other periods? For this reason, there are cases when we need to sample and model our forecast based on different conditions, <strong>multiple series</strong> in state space.</p>
<p><strong>Lag features</strong></p>
<p><strong>Lags</strong> in time series can be helpful in some cases with which we can generate new features. Using our original multivariate time series, we can use the <strong>Lags</strong> from the target variable, namely <strong>y</strong>, by shifting one lag and using the result as a new feature, namely <strong>x3</strong>.</p>

<div class="sourceCode" id="cb1750"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1750-1" data-line-number="1">m</a></code></pre></div>
<pre><code>##           x1      x2      y
##  [1,]  1.834  0.5559  2.729
##  [2,]  2.199  0.9853  3.714
##  [3,]  4.298 -0.4192  2.796
##  [4,]  4.937 -0.3096  3.429
##  [5,]  4.853  1.7500  6.872
##  [6,]  6.110  0.4042  5.465
##  [7,]  6.187 -0.1599  4.559
##  [8,]  7.256 -1.4828  3.059
##  [9,] 10.095 -0.3386  6.991
## [10,] 12.435  1.1624 11.181
## [11,] 11.388 -0.2296  8.081
## [12,] 12.291 -0.7346  7.855</code></pre>

<p>Below is an example of shifting <strong>lag</strong>:</p>

<div class="sourceCode" id="cb1752"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1752-1" data-line-number="1"><span class="kw">library</span>(data.table)</a>
<a class="sourceLine" id="cb1752-2" data-line-number="2">(<span class="dt">x3 =</span> <span class="kw">shift</span>(m[,<span class="dv">3</span>], <span class="dt">n=</span><span class="dv">1</span>, <span class="dt">fill=</span><span class="ot">NA</span>, <span class="dt">type=</span><span class="st">&quot;lag&quot;</span>) )</a></code></pre></div>
<pre><code>##  [1]     NA  2.729  3.714  2.796  3.429  6.872  5.465  4.559  3.059
## [10]  6.991 11.181  8.081</code></pre>

<p>We then incorporate the shifted series back into the matrix in the form of a new feature.</p>

<div class="sourceCode" id="cb1754"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1754-1" data-line-number="1"><span class="kw">as.matrix</span>(<span class="kw">data.frame</span>(<span class="dt">x1 =</span> m[,<span class="dv">1</span>], <span class="dt">x2 =</span> m[,<span class="dv">2</span>], <span class="dt">x3 =</span> x3, <span class="dt">y =</span> m[,<span class="dv">3</span>]))</a></code></pre></div>
<pre><code>##           x1      x2     x3      y
##  [1,]  1.834  0.5559     NA  2.729
##  [2,]  2.199  0.9853  2.729  3.714
##  [3,]  4.298 -0.4192  3.714  2.796
##  [4,]  4.937 -0.3096  2.796  3.429
##  [5,]  4.853  1.7500  3.429  6.872
##  [6,]  6.110  0.4042  6.872  5.465
##  [7,]  6.187 -0.1599  5.465  4.559
##  [8,]  7.256 -1.4828  4.559  3.059
##  [9,] 10.095 -0.3386  3.059  6.991
## [10,] 12.435  1.1624  6.991 11.181
## [11,] 11.388 -0.2296 11.181  8.081
## [12,] 12.291 -0.7346  8.081  7.855</code></pre>

<p>From there, we can perform forecasting.</p>
<p>The number of shifts can be determined perhaps based on cycles or frequencies:</p>

<div class="sourceCode" id="cb1756"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1756-1" data-line-number="1"><span class="kw">cycle</span>(ts.multiv)</a></code></pre></div>
<pre><code>##      Qtr1 Qtr2 Qtr3 Qtr4
## 2018    1    2    3    4
## 2019    1    2    3    4
## 2020    1    2    3    4</code></pre>
<div class="sourceCode" id="cb1758"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1758-1" data-line-number="1"><span class="kw">frequency</span>(ts.multiv)</a></code></pre></div>
<pre><code>## [1] 4</code></pre>

<p><strong>Panel data</strong> and <strong>Pooled data</strong></p>
<p>We end our discussion of <strong>Time-Series</strong> in this section by introducing <strong>Panel data</strong>, also called <strong>Longitudinal data</strong>, borrowed from <strong>Statistics and Econometrics</strong> theory. This type of data combines both <strong>Time-Series</strong> data and <strong>Cross-Sectional</strong> data. To explain the difference between the two, as an example for the former, <strong>Time-Series</strong> data is a collection or group of observations for a single variable, e.g., market stock price, over time. As for the latter, we have a collection of observations for multiple variables, e.g., companies and job employment status, over the same period.</p>
<p>In some cases, <strong>Pooled data</strong> is interpreted as <strong>Panel data</strong>. However, to be more concrete, when we have a collection of <strong>cross-sectional</strong> data over time, we refer to it as <strong>Pooled data</strong>. If we have a <strong>cross-sectional</strong> data sampled multiple times, we regard it as <strong>Panel data</strong>. For example, it becomes <strong>Pooled data</strong> for cross-sectional data collected over two years. It then becomes <strong>Panel data</strong> for cross-sectional data collected every five years - e.g., collecting various factors affecting climate change every five years.</p>
<p>Also, in some cases, we find terms such as <strong>Stacked Time-Series</strong>, <strong>Stacked Cross-Section</strong>, and <strong>Pooled data</strong>. As the term implies, multiple <strong>Time-Series</strong> data can be stacked on top of each other (if visually plotted). Similarly, the same applies to multiple <strong>Cross-Sectional</strong> data <strong>Stacked Time-Series</strong>.</p>
<p><strong>Fixed Effect Model vs. Random Effect Model</strong></p>
<p>It helps to be familiar when dealing with <strong>Time-Series</strong>, with two models mainly introduced in <strong>Statistics</strong> and <strong>Econometrics</strong>. The first is <strong>Fixed Effects</strong> model, which describes modeling data that are fixed in nature. Examples of such data that are constant and not changing are the sex and nationality of an individual. On the other hand, <strong>Random Effect Model</strong> changes over time, such as market price.</p>
<p>For date utilities that we can use for forecasting, please see the Appendix for the <strong>lubridate</strong> package.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="11.3-natural-language-processing-nlp.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="11.5-recommender-systems.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
