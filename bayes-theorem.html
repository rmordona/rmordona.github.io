<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Bayes Theorem  | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Bayes Theorem  | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Bayes Theorem  | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="probability-rules.html"/>
<link rel="next" href="conjugacy.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="number-system.html"><a href="number-system.html"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>1</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="1.1" data-path="approximation-based-on-random-chances.html"><a href="approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>1.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="1.2" data-path="distribution.html"><a href="distribution.html"><i class="fa fa-check"></i><b>1.2</b> Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="mass-and-density.html"><a href="mass-and-density.html"><i class="fa fa-check"></i><b>1.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1.4</b> Probability  </a></li>
<li class="chapter" data-level="1.5" data-path="probability-density-function-pdf.html"><a href="probability-density-function-pdf.html"><i class="fa fa-check"></i><b>1.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="1.6" data-path="probability-mass-function-pmf.html"><a href="probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>1.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="1.7" data-path="cumulative-distribution-function-cdf.html"><a href="cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>1.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="1.8" data-path="special-functions.html"><a href="special-functions.html"><i class="fa fa-check"></i><b>1.8</b> Special Functions</a><ul>
<li class="chapter" data-level="1.8.1" data-path="special-functions.html"><a href="special-functions.html#gamma-function"><i class="fa fa-check"></i><b>1.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="1.8.2" data-path="special-functions.html"><a href="special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>1.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="1.8.3" data-path="special-functions.html"><a href="special-functions.html#digamma-function"><i class="fa fa-check"></i><b>1.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="1.8.4" data-path="special-functions.html"><a href="special-functions.html#beta-function"><i class="fa fa-check"></i><b>1.8.4</b> Beta function </a></li>
<li class="chapter" data-level="1.8.5" data-path="special-functions.html"><a href="special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>1.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="1.8.6" data-path="special-functions.html"><a href="special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>1.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="1.8.7" data-path="special-functions.html"><a href="special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>1.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="1.8.8" data-path="special-functions.html"><a href="special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>1.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="1.8.9" data-path="special-functions.html"><a href="special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>1.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="1.8.10" data-path="special-functions.html"><a href="special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>1.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="distributiontypes.html"><a href="distributiontypes.html"><i class="fa fa-check"></i><b>1.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="1.9.1" data-path="distributiontypes.html"><a href="distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>1.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="1.9.2" data-path="distributiontypes.html"><a href="distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>1.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="1.9.3" data-path="distributiontypes.html"><a href="distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="1.9.4" data-path="distributiontypes.html"><a href="distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>1.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="1.9.5" data-path="distributiontypes.html"><a href="distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>1.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="1.9.6" data-path="distributiontypes.html"><a href="distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="1.9.7" data-path="distributiontypes.html"><a href="distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>1.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="1.9.8" data-path="distributiontypes.html"><a href="distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>1.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="1.9.9" data-path="distributiontypes.html"><a href="distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>1.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="1.9.10" data-path="distributiontypes.html"><a href="distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>1.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="1.9.11" data-path="distributiontypes.html"><a href="distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>1.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="1.9.12" data-path="distributiontypes.html"><a href="distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>1.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="1.9.13" data-path="distributiontypes.html"><a href="distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>1.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="1.9.14" data-path="distributiontypes.html"><a href="distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>1.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="1.9.15" data-path="distributiontypes.html"><a href="distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>1.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="1.9.16" data-path="distributiontypes.html"><a href="distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>1.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="1.9.17" data-path="distributiontypes.html"><a href="distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>1.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="1.9.18" data-path="distributiontypes.html"><a href="distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>1.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="1.9.19" data-path="distributiontypes.html"><a href="distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>1.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="1.9.20" data-path="distributiontypes.html"><a href="distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>1.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.9.21" data-path="distributiontypes.html"><a href="distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>1.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="1.9.22" data-path="distributiontypes.html"><a href="distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>1.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="1.9.23" data-path="distributiontypes.html"><a href="distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>1.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="1.9.24" data-path="distributiontypes.html"><a href="distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>1.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>2</b> Statistical Computation</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>2.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>2.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="2.1.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>2.1.3</b> Variability </a></li>
<li class="chapter" data-level="2.1.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>2.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="2.1.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>2.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html"><i class="fa fa-check"></i><b>2.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="2.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html"><i class="fa fa-check"></i><b>2.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>2.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>2.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="2.3.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>2.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>2.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="2.3.5" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>2.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.6" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>2.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.7" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>2.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="2.3.8" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>2.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="2.3.9" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>2.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="2.3.10" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>2.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html"><i class="fa fa-check"></i><b>2.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="2.4.1" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>2.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="2.4.2" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>2.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html"><i class="fa fa-check"></i><b>2.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>2.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>2.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>2.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>2.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>2.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>2.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="2.5.7" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>2.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="2.5.8" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>2.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>2.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>2.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="2.6.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>2.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="2.6.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>2.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="2.6.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>2.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>2.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="2.7.1" data-path="regression-analysis.html"><a href="regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.7.2" data-path="regression-analysis.html"><a href="regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>2.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="2.7.3" data-path="regression-analysis.html"><a href="regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>2.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="2.7.4" data-path="regression-analysis.html"><a href="regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>2.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="2.7.5" data-path="regression-analysis.html"><a href="regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>2.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="2.7.6" data-path="regression-analysis.html"><a href="regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>2.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="2.7.7" data-path="regression-analysis.html"><a href="regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>2.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html"><i class="fa fa-check"></i><b>2.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="2.8.1" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>2.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="2.8.3" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>2.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="2.8.4" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>2.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="2.8.5" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>2.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="2.8.6" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>2.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="2.8.7" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>2.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="2.8.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>2.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="2.8.9" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>2.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>2.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="2.9.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>2.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="2.9.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>2.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="2.9.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>2.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>3</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-1.html"><a href="probability-1.html"><i class="fa fa-check"></i><b>3.1</b> Probability </a><ul>
<li class="chapter" data-level="3.1.1" data-path="probability-1.html"><a href="probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>3.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-1.html"><a href="probability-1.html#joint-probability"><i class="fa fa-check"></i><b>3.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-1.html"><a href="probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>3.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-1.html"><a href="probability-1.html#negation-probability"><i class="fa fa-check"></i><b>3.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-1.html"><a href="probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>3.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-rules.html"><a href="probability-rules.html"><i class="fa fa-check"></i><b>3.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>3.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>3.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>3.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-rules.html"><a href="probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>3.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-rules.html"><a href="probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>3.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>3.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>3.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>3.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="3.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>3.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="3.3.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>3.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="3.3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>3.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="3.3.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>3.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugacy.html"><a href="conjugacy.html"><i class="fa fa-check"></i><b>3.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="3.4.1" data-path="conjugacy.html"><a href="conjugacy.html#precision"><i class="fa fa-check"></i><b>3.4.1</b> Precision </a></li>
<li class="chapter" data-level="3.4.2" data-path="conjugacy.html"><a href="conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>3.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="3.4.3" data-path="conjugacy.html"><a href="conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.4" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.5" data-path="conjugacy.html"><a href="conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.6" data-path="conjugacy.html"><a href="conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.7" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.8" data-path="conjugacy.html"><a href="conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>3.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="3.4.9" data-path="conjugacy.html"><a href="conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.10" data-path="conjugacy.html"><a href="conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.11" data-path="conjugacy.html"><a href="conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.12" data-path="conjugacy.html"><a href="conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.13" data-path="conjugacy.html"><a href="conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>3.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="3.4.14" data-path="conjugacy.html"><a href="conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>3.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="information-theory.html"><a href="information-theory.html"><i class="fa fa-check"></i><b>3.5</b> Information Theory </a><ul>
<li class="chapter" data-level="3.5.1" data-path="information-theory.html"><a href="information-theory.html#information"><i class="fa fa-check"></i><b>3.5.1</b> Information </a></li>
<li class="chapter" data-level="3.5.2" data-path="information-theory.html"><a href="information-theory.html#entropy"><i class="fa fa-check"></i><b>3.5.2</b> Entropy </a></li>
<li class="chapter" data-level="3.5.3" data-path="information-theory.html"><a href="information-theory.html#gini-index"><i class="fa fa-check"></i><b>3.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="3.5.4" data-path="information-theory.html"><a href="information-theory.html#information-gain"><i class="fa fa-check"></i><b>3.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="3.5.5" data-path="information-theory.html"><a href="information-theory.html#mutual-information"><i class="fa fa-check"></i><b>3.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="3.5.6" data-path="information-theory.html"><a href="information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>3.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="3.5.7" data-path="information-theory.html"><a href="information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>3.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="bayesianinference.html"><a href="bayesianinference.html"><i class="fa fa-check"></i><b>3.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="3.6.1" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>3.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="3.6.2" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>3.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="3.6.3" data-path="bayesianinference.html"><a href="bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>3.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="3.6.4" data-path="bayesianinference.html"><a href="bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>3.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="3.6.5" data-path="bayesianinference.html"><a href="bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>3.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>4</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-models.html"><a href="bayesian-models.html"><i class="fa fa-check"></i><b>4.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-models.html"><a href="bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>4.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-models.html"><a href="bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>4.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-models.html"><a href="bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>4.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="4.1.4" data-path="bayesian-models.html"><a href="bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>4.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="4.1.5" data-path="bayesian-models.html"><a href="bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>4.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="4.1.6" data-path="bayesian-models.html"><a href="bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>4.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="4.1.7" data-path="bayesian-models.html"><a href="bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>4.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="4.1.8" data-path="bayesian-models.html"><a href="bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>4.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="4.1.9" data-path="bayesian-models.html"><a href="bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>4.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="4.1.10" data-path="bayesian-models.html"><a href="bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>4.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="4.1.11" data-path="bayesian-models.html"><a href="bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>4.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html"><i class="fa fa-check"></i><b>4.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="4.2.1" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="4.2.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="4.2.3" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.4" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>4.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.5" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.6" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>4.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="4.2.7" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>4.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="4.2.8" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="4.2.9" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>4.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>4.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>4.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="4.3.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>4.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="4.3.4" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>4.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>5</b> Computational Learning I</a><ul>
<li class="chapter" data-level="5.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html"><i class="fa fa-check"></i><b>5.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="5.1.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>5.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="5.1.2" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>5.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="5.1.3" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>5.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="5.1.4" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>5.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="input-data.html"><a href="input-data.html"><i class="fa fa-check"></i><b>5.2</b> Input Data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="input-data.html"><a href="input-data.html#structured-data"><i class="fa fa-check"></i><b>5.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="input-data.html"><a href="input-data.html#non-structured-data"><i class="fa fa-check"></i><b>5.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="input-data.html"><a href="input-data.html#statistical-data"><i class="fa fa-check"></i><b>5.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="5.2.4" data-path="input-data.html"><a href="input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>5.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="5.2.5" data-path="input-data.html"><a href="input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>5.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="5.2.6" data-path="input-data.html"><a href="input-data.html#data-lake"><i class="fa fa-check"></i><b>5.2.6</b> Data lake</a></li>
<li class="chapter" data-level="5.2.7" data-path="input-data.html"><a href="input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>5.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="5.2.8" data-path="input-data.html"><a href="input-data.html#multimedia-md"><i class="fa fa-check"></i><b>5.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="primitive-methods.html"><a href="primitive-methods.html"><i class="fa fa-check"></i><b>5.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="5.3.1" data-path="primitive-methods.html"><a href="primitive-methods.html#weighting"><i class="fa fa-check"></i><b>5.3.1</b> Weighting</a></li>
<li class="chapter" data-level="5.3.2" data-path="primitive-methods.html"><a href="primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>5.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3.3" data-path="primitive-methods.html"><a href="primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>5.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="5.3.4" data-path="primitive-methods.html"><a href="primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>5.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="5.3.5" data-path="primitive-methods.html"><a href="primitive-methods.html#centering"><i class="fa fa-check"></i><b>5.3.5</b> Centering </a></li>
<li class="chapter" data-level="5.3.6" data-path="primitive-methods.html"><a href="primitive-methods.html#scaling"><i class="fa fa-check"></i><b>5.3.6</b> Scaling </a></li>
<li class="chapter" data-level="5.3.7" data-path="primitive-methods.html"><a href="primitive-methods.html#transforming"><i class="fa fa-check"></i><b>5.3.7</b> Transforming</a></li>
<li class="chapter" data-level="5.3.8" data-path="primitive-methods.html"><a href="primitive-methods.html#clipping"><i class="fa fa-check"></i><b>5.3.8</b> Clipping </a></li>
<li class="chapter" data-level="5.3.9" data-path="primitive-methods.html"><a href="primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>5.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="distance-metrics.html"><a href="distance-metrics.html"><i class="fa fa-check"></i><b>5.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="distance-metrics.html"><a href="distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>5.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="5.4.2" data-path="distance-metrics.html"><a href="distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>5.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="5.4.3" data-path="distance-metrics.html"><a href="distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>5.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="5.4.4" data-path="distance-metrics.html"><a href="distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>5.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="5.4.5" data-path="distance-metrics.html"><a href="distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>5.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="5.4.6" data-path="distance-metrics.html"><a href="distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>5.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="5.4.7" data-path="distance-metrics.html"><a href="distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>5.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="5.4.8" data-path="distance-metrics.html"><a href="distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>5.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>5.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>5.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="5.5.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>5.5.2</b> Association</a></li>
<li class="chapter" data-level="5.5.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>5.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="5.5.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>5.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="5.5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>5.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="5.5.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>5.5.6</b> Covariance </a></li>
<li class="chapter" data-level="5.5.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>5.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="5.5.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>5.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="5.5.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>5.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="5.5.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>5.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="5.5.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>5.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="5.5.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="5.5.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>5.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="5.5.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>5.5.14</b> Discretization </a></li>
<li class="chapter" data-level="5.5.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>5.5.15</b> Stratification </a></li>
<li class="chapter" data-level="5.5.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>5.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="5.5.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>5.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="featureengineering.html"><a href="featureengineering.html"><i class="fa fa-check"></i><b>5.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="5.6.1" data-path="featureengineering.html"><a href="featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>5.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="5.6.2" data-path="featureengineering.html"><a href="featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>5.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="5.6.3" data-path="featureengineering.html"><a href="featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>5.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="5.6.4" data-path="featureengineering.html"><a href="featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>5.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="5.6.5" data-path="featureengineering.html"><a href="featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>5.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="5.6.6" data-path="featureengineering.html"><a href="featureengineering.html#featureselection"><i class="fa fa-check"></i><b>5.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="5.6.7" data-path="featureengineering.html"><a href="featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>5.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="5.6.8" data-path="featureengineering.html"><a href="featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>5.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="general-modeling.html"><a href="general-modeling.html"><i class="fa fa-check"></i><b>5.7</b> General Modeling</a><ul>
<li class="chapter" data-level="5.7.1" data-path="general-modeling.html"><a href="general-modeling.html#training-learning"><i class="fa fa-check"></i><b>5.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="5.7.2" data-path="general-modeling.html"><a href="general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>5.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="5.7.3" data-path="general-modeling.html"><a href="general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>5.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="5.7.4" data-path="general-modeling.html"><a href="general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>5.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="5.7.5" data-path="general-modeling.html"><a href="general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>5.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="5.7.6" data-path="general-modeling.html"><a href="general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>5.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="5.7.7" data-path="general-modeling.html"><a href="general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>5.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="5.7.8" data-path="general-modeling.html"><a href="general-modeling.html#regularization"><i class="fa fa-check"></i><b>5.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="supervised-vs.unsupervised-learning.html"><a href="supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>5.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="5.9" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>6</b> Appendix</a><ul>
<li class="chapter" data-level="6.1" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>6.1</b> Appendix A</a><ul>
<li class="chapter" data-level="6.1.1" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>6.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="6.1.2" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i><b>6.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="6.1.3" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i><b>6.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i><b>6.2</b> Appendix B</a><ul>
<li class="chapter" data-level="6.2.1" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>6.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="6.2.2" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>6.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="6.2.3" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>6.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="6.2.4" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>6.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="6.2.5" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>6.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="6.2.6" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>6.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="6.2.7" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>6.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i><b>6.3</b> Appendix C</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bayes-theorem" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.3</span> Bayes Theorem <a href="bayes-theorem.html#bayes-theorem" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The formula for Bayes Rule (or Bayes Theorem) is expressed this way <span class="citation">(Bruyninckx H. <a href="bibliography.html#ref-ref930h">2002</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
\text{Posterior} = \frac{\text{Likelihood}\times \text{Prior}}{\text{Marginal Likelihood}}\ \ \ \ \rightarrow\ \ \ \ \ \ P(X|Y) = \frac{P(Y|X)\times P(X)}{P(Y)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(P(Y|X)P(X)\)</span> is an expanded version of joint probability:</p>
<p><span class="math display">\[\begin{align}
P(X,Y) = P(Y|X)\times P(X)
\end{align}\]</span></p>
<p>and where <span class="math inline">\(P(Y)\)</span> is the observed probability (the evidence) and can be expanded as:</p>
<p><span class="math display">\[\begin{align}
P(Y) =  P(Y|X)\times P(X) + P(Y|X&#39;)\times P(X&#39;)
\end{align}\]</span></p>
<p>Note that <strong>marginal likelihood</strong> is also called <strong>evidence</strong>.</p>
<p>Therefore, we have:</p>
<p><span class="math display">\[\begin{align}
P(X|Y) = \frac{P(X,Y)}{P(Y)} = \frac{P(Y|X)\times P(X)}{P(Y|X)\times P(X) + P(Y|X&#39;)\times P(X&#39;)} 
\end{align}\]</span></p>
<p>To illustrate, let us use Figure . Note that, hereafter, we also use the figure to illustrate other concepts around <strong>estimation</strong> and <strong>inference</strong> in later chapters, such as the <strong>Variational Inference</strong>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bayestheorem"></span>
<img src="bayestheorem.png" alt="Bi-variate Mixture Model" width="70%" />
<p class="caption">
Figure 3.1: Bi-variate Mixture Model
</p>
</div>

<p>Figure  shows diagram of a bivariate mixture model with three circular contours representing a cluster (or classification), namely <span class="math inline">\(\mathbf{c} = \{ c_1, c_2, c_3\}\)</span>. There are two random variables, namely <span class="math inline">\(X1\)</span> and <span class="math inline">\(X2\)</span>. Each random variable is characterized by a univariate gaussian mixture distribution - it is a mixture of possibly three individual <strong>gaussian distributions</strong>.</p>
<p>We start with the <strong>prior</strong> of the <strong>Bayes Theorem</strong>, which is written, in this case, as the initial proportion of each classification like so:</p>
<p><span class="math display">\[\begin{align}
P(c_1) = \pi_1 = \frac{ c_1}{ \sum_i^k c_i}\ \ \ \ \ \ \ \ \ \ \ \
P(c_2) = \pi_2 = \frac{ c_2}{ \sum_i^k c_i}\ \ \ \ \ \ \ \ \ \ \ \
P(c_3) = \pi_3 = \frac{ c_3}{ \sum_i^k c_i}
\end{align}\]</span></p>
<p>Then we follow that by computing the <strong>likelihood</strong>. For that, we first calculate the probability of a random event (a random variable) belonging to a cluster.</p>
<p><span class="math display">\[\begin{align}
\begin{array}{ll}
P(x^{(1)}|c_1) \sim \mathcal{N}\left(\mu^{(1)}_1, \sigma^{2(1)}_1\right) &amp;
P(x^{(2)}|c_1) \sim \mathcal{N}\left(\mu^{(2)}_1, \sigma^{2(2)}_1\right)\\
P(x^{(1)}|c_2) \sim \mathcal{N}\left(\mu^{(1)}_2, \sigma^{2(1)}_2\right) &amp;
P(x^{(2)}|c_2) \sim \mathcal{N}\left(\mu^{(2)}_2, \sigma^{2(2)}_2\right)\\
P(x^{(1)}|c_3) \sim \mathcal{N}\left(\mu^{(1)}_3, \sigma^{2(1)}_3\right) &amp;
P(x^{(2)}|c_3) \sim \mathcal{N}\left(\mu^{(2)}_3, \sigma^{2(2)}_3\right)\\
\end{array} \label{eqn:eqnnumber47}
\end{align}\]</span></p>
<p>Then we construct the <strong>likelihood</strong>:</p>
<p><span class="math display">\[\begin{align}
\begin{array}{l}
P(X|c_1)  = P(x^{(1)},x^{(2)}| c_1 ) = P(x^{(1)}| c_1 )P(x^{(2)}| c_1 )\\
P(X|c_2)  = P(x^{(1)},x^{(2)}| c_2 ) = P(x^{(1)}| c_2 )P(x^{(2)}| c_2 )\\
P(X|c_3)  = P(x^{(1)},x^{(2)}| c_3 ) = P(x^{(1)}| c_3 )P(x^{(2)}| c_3 )
\end{array} \label{eqn:eqnnumber48}
\end{align}\]</span></p>
<p>Finally, we construct the <strong>posterior</strong>:</p>
<p><span class="math display">\[\begin{align}
P(c_1|X) = \frac{P(X|c_1)P(c_1)}
{\sum_i P(X|c_i)P(c_i)}
\ \ \ \ \ \ \ \ 
P(c_2|X) = \frac{P(X|c_2)P(c_2)}
{\sum_i P(X|c_i)P(c_i)} \nonumber \\
P(c_3|X) = \frac{P(X|c_3)P(c_3)}
{\sum_i P(X|c_i)P(c_i)}
\end{align}\]</span></p>
<p>To illustrate the <strong>Bayes Theorem</strong>, suppose we perform an analysis of strange flu-like symptoms and a particular pathogen currently floating around a small local community. Suppose, after reviewing historical records, that 25% of the locals reported similar flu-like symptoms in the past. However, in testing individuals, we find that 10% are positive for the pathogen. Additionally, 3% of the locals with flu-like symptoms also carry the pathogen. Let us determine the probability of locals being positive for flu-like symptoms.</p>
<p>Let X be locals with flu-like symptoms and Y be locals tested positive for a particular pathogen. Therefore:</p>
<p><span class="math display">\[\begin{align*}
P(X) {}&amp;= 25\% = 0.25\\
P(Y) &amp;= 10\% = 0.10\\
P(Y|X) &amp;= 3\% = 0.03 
\end{align*}\]</span></p>
<p>Using <strong>Bayes Theorem</strong>, we compute the following:</p>
<p><span class="math display">\[
P(X|Y) = \frac{P(Y|X)\times P(X)}{P(Y)}
= \frac{(0.03)\times (0.25)}{(0.10)} = 0.075 = 7.5\% 
\]</span></p>
<p>That means that the probability of developing flu-like symptoms for subjects found to have the pathogen is 7.5%.</p>
<div id="naÃ¯ve-bayes" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.1</span> NaÃ¯ve Bayes <a href="bayes-theorem.html#naÃ¯ve-bayes" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>NaÃ¯ve Bayes</strong> is the normalized form of <strong>Bayes Theorem</strong> formula.</p>
<p>Recall the following Bayes Formula:</p>
<p><span class="math display">\[\begin{align}
P(X|Y) = \frac{P(X,Y)}{P(Y)} = \frac{P(Y|X) \times P(X)}{P(Y|X) \times P(X) + P(Y|X&#39;)\times P(X&#39;)} 
\end{align}\]</span></p>
<p>With multivariate probabilities, the equation expands into the following:</p>
<p><span class="math display">\[\begin{align}
P(X_{1}, X_{2},X_{3},...|Y) = \frac{P(X_{1},X_{2},X_{3},...,Y)}{P(Y)}
\end{align}\]</span></p>
<p>where <span class="math inline">\(P(Y)\)</span> is a normalizing (scaling) factor.</p>
<p>For discrete densities, the normalizing factor is summed up using the following expression:</p>
<p><span class="math display">\[\begin{align}
P(Y) = \sum_{X_{i}=1}^{N_{1}}\sum_{X_{2}=1}^{N_{2}}
\sum_{X_{3}=1}^{N_{3}} ... P(X_{1},X_{2},X_{3},...,Y)
\end{align}\]</span></p>
<p>For continuous densities, the normalizing factor integrates using the following expression:</p>
<p><span class="math display">\[\begin{align}
P(Y) = \int_{-\infty}^{\infty}\int_{-\infty}^{\infty}
\int_{-\infty}^{\infty} ... P(X_{1},X_{2},X_{3},...,Y)...dX_{3}dX_{2}dX_{1}
\end{align}\]</span></p>
<p>Note that every observed density added becomes a challenge to integrate. As a result, it can lead to being <strong>intractable</strong>. Fortunately, we use <span class="math inline">\(P(Y)\)</span> for normalization, and so with or without it, the proportionality remains intact (which we can use for approximation). Because of that, we end up with the below <strong>NaÃ¯ve</strong> formula:</p>
<p><span class="math display">\[\begin{align}
\underbrace{P(X|Y)}_\text{posterior}\ \propto\ \underbrace{P(Y|X)}_\text{likelihood} \times \underbrace{P(X)}_\text{prior}
\end{align}\]</span></p>
<p>It is also common to see the following equivalent notation to emphasize evidence versus hypothesis:</p>
<p><span class="math display">\[\begin{align}
P(H|E) \propto P(E|H) \times P(H)
\ \ \ \ \ \leftarrow\ \ \ \ \ \ \ P(H|E)  = \frac{ P(E|H) \times P(H) }{ P(E)}
\end{align}\]</span></p>
<p>The notation starts with the premise that we need to evaluate the state (or probability) of a <strong>hypothesis</strong> given a set of observations - our evidence. This is our <strong>posterior</strong> probability denoted as <span class="math inline">\(P(H|E)\)</span>. Our <strong>posterior</strong> hypothesis remains to be <strong>weak</strong> until proven to hold. Here, <strong>weak</strong> may refer to being subjective (or even anecdotal) in describing (or proving) a piece of evidence. Our goal is apparently to collect more evidence (e.g., perform more clinical tests). If we can collect more (objective) evidence backed by our (subjective) <strong>prior</strong> hypothesis, we can give weight to the collected evidence, generating a newly updated <strong>posterior</strong> knowledge which becomes less <strong>weak</strong> and closer to describing a piece of objective evidence; nonetheless, it is a newly updated information. In a sense, we begin to see the scheme of things. A <strong>prior</strong> knowledge becomes a <strong>posterior</strong> knowledge for every new evidence, and a <strong>posterior</strong> knowledge becomes a <strong>prior</strong> knowledge for the next evidence.</p>
<p>To illustrate, when testing positive for symptoms (or signs) of a particular disease (whether cancer or COVID-19 infection as an example), we find that complex solutions tend to root or reference back the below fundamental equation. Note that <strong>symptoms</strong> (or signs) of disease remains <strong>hypothetical</strong> until tested positive:</p>
<p><span class="math display">\[\begin{align}
P(\text{symptom}|\mathbf{+}) \propto  
P(\mathbf{+}|\text{symptom}) \times P(\text{symptom})
\end{align}\]</span></p>
<p>where normalizing factor <span class="math inline">\(P(+)\)</span> is omitted:</p>
<p><span class="math display">\[\begin{align}
P(+) = P(\mathbf{+}|\text{symptom})\times P(\text{symptom}) +  
P(\mathbf{+}|\text{no symptom})\times  P(\text{no symptom})
\end{align}\]</span></p>
<p>Let us continue to further our discussion of <strong>hypothesis</strong> in the next section. Note that our description of <strong>hypothesis</strong> in the next section focuses on <strong>parameter estimation</strong> using the theta <span class="math inline">\(\theta\)</span> symbol.</p>
</div>
<div id="likelihood" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.2</span> Likelihood<a href="bayes-theorem.html#likelihood" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, it is essential to distinguish two statements that can help us understand <strong>likelihood</strong>: </p>
<ol style="list-style-type: decimal">
<li>The likelihood of observing data given a sampled distribution.</li>
<li>The most likely distribution that produces observed data.</li>
</ol>
<p>Both statements focus around the parameter theta <span class="math inline">\(\theta\)</span> for a <strong>data distribution</strong>. The symbol theta <span class="math inline">\(\theta\)</span> represents a <strong>set (or vector) of parameter quantities</strong>, particularly the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) and <strong>variance</strong> (<span class="math inline">\(\sigma^2\)</span>) for a <strong>normal distribution</strong>. Note that other types of distributions have their own corresponding set of parameters for <strong>theta</strong>, <span class="math inline">\(\theta\)</span>. Here, we use <strong>normal distribution</strong> to explain a case.</p>
<p>We begin our discussion by mentioning - in the Bayesian sense - that there is uncertainty in the parameter theta <span class="math inline">\(\theta\)</span>. The uncertainty comes about because of the idea, for example, that the average height (the mean height) of the world population is difficult to exact because it requires a global census which is impossible to achieve. For that reason, we settle on our observations - a sampled data limited in terms of the distribution it follows in comparison to the actual distribution of an entire population. We use Figure  as reference.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihood"></span>
<img src="likelihood.png" alt="Likelihood" width="100%" />
<p class="caption">
Figure 3.2: Likelihood
</p>
</div>

<p>It is fair to assume that, given a distribution characterizes by <span class="math inline">\(\theta = (\mu=2, \sigma=1)\)</span>, the <strong>likelihood</strong> of observing data (e.g.Â x=3) is 0.2419707. This is illustrated by the left-side diagram.</p>

<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb7-1" data-line-number="1"><span class="kw">dnorm</span>(<span class="dt">x=</span><span class="dv">3</span>, <span class="dt">mean=</span><span class="dv">2</span>, <span class="dt">sd=</span> <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.2419707</code></pre>

<p>It is also fair to assume that, given a list of distribution - for example, a distribution characterized by <span class="math inline">\(\theta = (\mu=2, \sigma=1)\)</span> and another distribution characterized by <span class="math inline">\(\theta = (\mu=3, \sigma=1)\)</span>, the one distribution that most likely produces the data (e.g.Â x=3) has an assumed <strong>maximum likelihood estimate (MLE)</strong> that is equal to 0.3989423. This is illustrated by the right-side diagram which references our second statement.</p>

<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb9-1" data-line-number="1"><span class="kw">dnorm</span>(<span class="dt">x=</span><span class="dv">3</span>, <span class="dt">mean=</span><span class="dv">3</span>, <span class="dt">sd=</span> <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.3989423</code></pre>

<p>As we note, we assume an initial <strong>MLE</strong> in the diagram. We know that this is not necessarily accurate because of the insufficient number of sampling distributions. Regarding <strong>likelihood</strong>, we are most often interested in knowing which <strong>sample distribution</strong> closely relates to the <strong>actual distribution</strong> (such as one representing the population). We can achieve such a close estimate by way of <strong>MLE</strong>. We expand on the concept of <strong>MLE</strong> in the <strong>Bayesian Inference</strong> section.</p>
<p>As we expound further, <strong>likelihood</strong> comes into the picture every time we have limited access to a complete set of data, albeit the parameter <span class="math inline">\(\theta\)</span> is known. To illustrate, assume by luck that we have a distribution that is as complete a set as we can gather. Consider this to be almost the actual distribution. Ignore that we are using <strong>sampling</strong> to generate our actual distribution. Let us take the mean and variance - our parameter model - like so:</p>

<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb11-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb11-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb11-3" data-line-number="3">true.distribution =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span><span class="dv">1000</span>, <span class="dt">prob=</span><span class="ot">NULL</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb11-4" data-line-number="4">( <span class="dt">theta =</span> <span class="kw">c</span>(<span class="st">&quot;mean&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(true.distribution), </a>
<a class="sourceLine" id="cb11-5" data-line-number="5">            <span class="st">&quot;sd&quot;</span>   =<span class="st"> </span><span class="kw">sd</span>(true.distribution)))</a></code></pre></div>
<pre><code>##     mean       sd 
## 25.70800 14.44269</code></pre>

<p>The characteristic of the true distribution is recorded as such: <span class="math inline">\(\theta\)</span> = (<span class="math inline">\(\mu\)</span> = 25.71, <span class="math inline">\(\sigma\)</span> = 14.44).</p>
<p>Suppose now that we have limited access to the actual data in that we only have a significantly small portion of the data set. Let us start with three small samples. See below:</p>

<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb13-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb13-2" data-line-number="2"><span class="co"># suppose  each sample is IID (identical distribution)</span></a>
<a class="sourceLine" id="cb13-3" data-line-number="3">sample1 =<span class="st"> </span><span class="kw">sample</span>(true.distribution, <span class="dt">size=</span><span class="dv">7</span>, <span class="dt">prob=</span><span class="ot">NULL</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb13-4" data-line-number="4">sample2 =<span class="st"> </span><span class="kw">sample</span>(true.distribution, <span class="dt">size=</span><span class="dv">4</span>, <span class="dt">prob=</span><span class="ot">NULL</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb13-5" data-line-number="5">sample3 =<span class="st"> </span><span class="kw">sample</span>(true.distribution, <span class="dt">size=</span><span class="dv">3</span>, <span class="dt">prob=</span><span class="ot">NULL</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb13-6" data-line-number="6">samples =<span class="st"> </span><span class="kw">matrix</span>(</a>
<a class="sourceLine" id="cb13-7" data-line-number="7">     <span class="kw">c</span>( <span class="kw">paste0</span>(sample1, <span class="dt">collapse=</span><span class="st">&quot;, &quot;</span>), <span class="kw">round</span>(<span class="kw">mean</span>(sample1),<span class="dv">3</span>),  </a>
<a class="sourceLine" id="cb13-8" data-line-number="8">        <span class="kw">round</span>(<span class="kw">sd</span>(sample1),<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb13-9" data-line-number="9">        <span class="kw">paste0</span>(sample2, <span class="dt">collapse=</span><span class="st">&quot;, &quot;</span>), <span class="kw">round</span>(<span class="kw">mean</span>(sample2),<span class="dv">3</span>),  </a>
<a class="sourceLine" id="cb13-10" data-line-number="10">        <span class="kw">round</span>(<span class="kw">sd</span>(sample2),<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb13-11" data-line-number="11">        <span class="kw">paste0</span>(sample3, <span class="dt">collapse=</span><span class="st">&quot;, &quot;</span>), <span class="kw">round</span>(<span class="kw">mean</span>(sample3),<span class="dv">3</span>),  </a>
<a class="sourceLine" id="cb13-12" data-line-number="12">        <span class="kw">round</span>(<span class="kw">sd</span>(sample3),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb13-13" data-line-number="13">      ) , <span class="dt">nrow=</span><span class="dv">3</span>, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb13-14" data-line-number="14"><span class="kw">colnames</span>(samples) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;observations&quot;</span>, <span class="st">&quot;sample mean&quot;</span>, <span class="st">&quot;sample sd&quot;</span>)</a>
<a class="sourceLine" id="cb13-15" data-line-number="15"><span class="kw">rownames</span>(samples) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;sample.1&quot;</span>, <span class="st">&quot;sample.2&quot;</span>, <span class="st">&quot;sample.3&quot;</span>)</a>
<a class="sourceLine" id="cb13-16" data-line-number="16"><span class="kw">as.data.frame</span>(samples)</a></code></pre></div>
<pre><code>##                       observations sample mean sample sd
## sample.1 30, 10, 14, 9, 35, 18, 41      22.429    12.817
## sample.2             3, 27, 24, 17       17.75    10.689
## sample.3                18, 36, 29      27.667     9.074</code></pre>

<p>Notice how the mean and standard deviation for each sample are very disproportionate compared to the actual mean and standard deviation of the true data distribution. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:muvar"></span>
<img src="DS_files/figure-html/muvar-1.png" alt="Disproportioned parameters" width="70%" />
<p class="caption">
Figure 3.3: Disproportioned parameters
</p>
</div>

<p>In Figure , we have one single observation, namely <span class="math inline">\(x=14\)</span>. Let us get the likelihood of the four distributions:</p>

<pre><code>##            sample mean sample sd likelihood
## population      25.708    14.443 0.01988653
## sample.1        22.429    12.817 0.02507315
## sample.2        17.750    10.689 0.03509510
## sample.3        27.667     9.074 0.01414169</code></pre>

<p>At first glance, the data, x=14, is likely to be observed from each of the three sample distributions; however, we see that <strong>sample.2</strong> has the most likelihood of observing data (x=14) at 0.0141417. Therefore, we can assume that <strong>sample.2</strong> is the closest estimate (or can be a good representative) of the actual distribution. In other words, <strong>sample.2</strong> has a better parameter model that can characterize (or can be proportional to) the actual distribution.</p>
<p>However, we have to note that our assumption is based only on one observation (x=14) and on only three samplings. Our goal is to find a better parameter model proportional to that of the actual distribution, but we need sufficient observations and sufficient samplings.</p>
<p>We need to maximize the <strong>likelihood</strong> that a set of observed data (not just one data point) belongs to a sample distribution (of a list of sample distributions) represented by theta <span class="math inline">\(\theta\)</span> that hopefully can characterize the actual distribution. As we pointed out, we can achieve this using <strong>MLE</strong>.</p>
<p>Now, <strong>likelihood</strong> is expressed in the form of a <strong>PDF</strong> formula:</p>
<p><span class="math display">\[\begin{align}
Lik(\theta|X) \equiv P(X|\theta)\ \ \ \leftarrow \ \ f(x; \theta)\ \ or\ \ f(x; \mu, \sigma^2)
\end{align}\]</span></p>
<p>The <span class="math inline">\(Lik(\theta|X)\)</span> is the likelihood function for the probability of a random event to occur, given that we have observed such occurrence.</p>
<p>Notice that the parameters in the <strong>likelihood</strong> notation get swapped (for semantics), and we use the equivalence (<span class="math inline">\(\equiv\)</span>) notation. While the meaning or interpretation is different, we compute the likelihood using the equation from <strong>Density function</strong>. However, to avoid confusing this equation as a <strong>Density function</strong>, we can call it a <strong>Likelihood function</strong>. Also, a <strong>Density function</strong> forms the curve (as we discussed in Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>)), and the <strong>probability</strong> forms the area in the curve. In the case of <strong>Likelihood function</strong>, it is a function of theta <span class="math inline">\(\theta\)</span> conditioned on the observed data.</p>
<p>We can perform point estimates for marginal sampling density or joint density likelihood like so:</p>
<p><span class="math display">\[\begin{align}
 Lik(\mu, \sigma^2  |  X = x_1) {}&amp;\equiv P(X = x_1| \mu, \sigma^2 ) &amp; \text{marginal-density likelihood} \\ 
 \nonumber \\
 Lik(\mu, \sigma^2  |  x_1,\ ....,\ x_n) &amp;\equiv P(x_1,\ ....,\ x_n| \mu, \sigma^2 )  &amp; \text{joint-density likelihood}
\end{align}\]</span></p>
<p><strong>MLE</strong> works best with <strong>joint-density likelihood</strong>.</p>
<p>The formula for both likelihoods for different types of densities is provided in later sections under <strong>Conjugacy</strong> and also in Table . The formula helps to compute the <strong>joint-density</strong> likelihood.</p>
<p>Recall that the <span class="math inline">\(\mu\)</span> parameter controls the <strong>location</strong> of the <strong>bell-shape</strong> curve and that the <span class="math inline">\(\sigma\)</span> parameter controls the <strong>scale</strong> (or spread) of the curve. In terms of evaluating multiple sampling densities, it is easy to visualize the shape of each sample density curve based on the location (via the mean) and the spread (via the variance).</p>
<p>To illustrate the likelihood of observing a single data point, namely <span class="math inline">\(x = 71\)</span>, given known constant standard deviation, namely <span class="math inline">\(\sigma = 2.5\)</span> and an unknown mean <span class="math inline">\(\mu\)</span>, let us treat <span class="math inline">\(\mu\)</span> as a random variable and plug in a few random values for the mean like so: <span class="math inline">\(\mu = (67,\ 68.5,\ 70.5,\ 72.5)\)</span>. Then, we use Figure  to show four bell-shaped curves for each of the random <span class="math inline">\(\mu\)</span> values.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihoodmu"></span>
<img src="DS_files/figure-html/likelihoodmu-1.png" alt="Likelihood (constant variance but variant mean)" width="70%" />
<p class="caption">
Figure 3.4: Likelihood (constant variance but variant mean)
</p>
</div>

<p>The likelihood estimate of observing <span class="math inline">\(x=71\)</span> for each of the means <span class="math inline">\(\mu\)</span> is as follows:</p>
<p><span class="math inline">\(Lik\mu_1\)</span> = 0.0443683, <span class="math inline">\(Lik\mu_2\)</span> = 0.0967883, <span class="math inline">\(Lik\mu_3\)</span> = 0.1564171, <span class="math inline">\(Lik\mu_4\)</span> = 0.1332898.</p>
<p>We derive that from the following example implementation of <strong>likelihood</strong> using our <strong>likelihood function</strong> for normal distribution:</p>

<div class="sourceCode" id="cb16"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb16-1" data-line-number="1">normal.likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(x, mean, sd) { <span class="co"># for normal outcome</span></a>
<a class="sourceLine" id="cb16-2" data-line-number="2">   variance =<span class="st"> </span>sd<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb16-3" data-line-number="3">   (<span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="kw">sqrt</span>( <span class="dv">2</span> <span class="op">*</span><span class="st"> </span>pi <span class="op">*</span><span class="st"> </span>variance ))) <span class="op">*</span><span class="st">  </span></a>
<a class="sourceLine" id="cb16-4" data-line-number="4"><span class="st">         </span><span class="kw">exp</span>(<span class="op">-</span>(x <span class="op">-</span><span class="st"> </span>mean)<span class="op">^</span><span class="dv">2</span><span class="op">/</span>(<span class="dv">2</span> <span class="op">*</span><span class="st"> </span>variance))</a>
<a class="sourceLine" id="cb16-5" data-line-number="5">}</a>
<a class="sourceLine" id="cb16-6" data-line-number="6"><span class="co"># log likelihood for joint density</span></a>
<a class="sourceLine" id="cb16-7" data-line-number="7">loglikelihood &lt;-<span class="st"> </span><span class="cf">function</span>(density) {</a>
<a class="sourceLine" id="cb16-8" data-line-number="8">  loglik =<span class="st"> </span><span class="kw">log</span>(density, <span class="kw">exp</span>(<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb16-9" data-line-number="9">  <span class="op">-</span><span class="kw">sum</span>(loglik) <span class="co"># for iid</span></a>
<a class="sourceLine" id="cb16-10" data-line-number="10">}</a></code></pre></div>

<p>For <strong>binomial likelihood</strong>, we have the following implementation:</p>

<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb17-1" data-line-number="1">binomial.likelihood &lt;-<span class="st"> </span><span class="cf">function</span>(x, rho, n) { <span class="co"># for binomial outcome</span></a>
<a class="sourceLine" id="cb17-2" data-line-number="2">   rho<span class="op">^</span>x <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>rho)<span class="op">^</span>(n <span class="op">-</span><span class="st"> </span>x) <span class="co"># rho = probability of success</span></a>
<a class="sourceLine" id="cb17-3" data-line-number="3">}</a></code></pre></div>

<p>For <strong>poisson likelihood</strong>, see Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Poisson Regression</strong> Subsection under <strong>Regression</strong> Section.</p>
<p>Using the <strong>normal likehood</strong>, we get the following:</p>

<div class="sourceCode" id="cb18"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb18-1" data-line-number="1">(<span class="dt">likelihood =</span> <span class="kw">normal.likelihood</span>(x, <span class="dt">mean =</span> mean, <span class="dt">sd =</span> sigma))</a></code></pre></div>
<pre><code>## [1] 0.04436833 0.09678829 0.15641708 0.13328984</code></pre>

<p>From there, we take the maximum likelihood (assumed <strong>MLE</strong>) for the mean:</p>

<div class="sourceCode" id="cb20"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb20-1" data-line-number="1">(<span class="dt">MLE =</span> <span class="kw">max</span>(likelihood))</a></code></pre></div>
<pre><code>## [1] 0.1564171</code></pre>

<p>Finally, we get the optimal paramater - the <strong>mean</strong> <span class="math inline">\(\hat{\mu}\)</span> - based on the <strong>MLE</strong>:</p>

<div class="sourceCode" id="cb22"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb22-1" data-line-number="1">(<span class="dt">mean.est =</span> mean[ <span class="kw">which.max</span>(likelihood) ] ) <span class="co"># MLE</span></a></code></pre></div>
<pre><code>## [1] 70.5</code></pre>

<p>Because we deal with a random variable for <strong>mean</strong>, we can also visualize its distribution. Note that this is <strong>mean distribution</strong> and not <strong>data distribution</strong>. We show the <strong>ML</strong>, which is located at the peak of the bell-shaped curve where the slope is zero. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlemean"></span>
<img src="DS_files/figure-html/mlemean-1.png" alt="Likelihood (parameter mean distribution)" width="70%" />
<p class="caption">
Figure 3.5: Likelihood (parameter mean distribution)
</p>
</div>

<p>In other words, by maximizing the likelihood, we can get a parameter model, namely the mean <span class="math inline">\(\mu\)</span> = 70.5 and standard deviation <span class="math inline">\(\sigma\)</span> = 2.5, that describes the sample distribution as a possible representative of (and possibly proportional to) our actual distribution. We can also say that this distribution is the one that produced our observed data.</p>
<p>Next, to illustrate the likelihood of observing a single data point, namely <span class="math inline">\(x = 71\)</span>, given known constant mean, namely <span class="math inline">\(\mu = 71\)</span> and an unknown variance <span class="math inline">\(\sigma^2\)</span>, let us treat <span class="math inline">\(\sigma\)</span> as a random variable and plug-in a few random values like so: <span class="math inline">\(\sigma = (1.5,\ 2.0,\ 2.5,\ 3.5)\)</span>. See Figure  for each of the random <span class="math inline">\(\sigma\)</span> values.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihoodsd"></span>
<img src="DS_files/figure-html/likelihoodsd-1.png" alt="Likelihood (constant variance but variant mean)" width="70%" />
<p class="caption">
Figure 3.6: Likelihood (constant variance but variant mean)
</p>
</div>

<p>Similarly, the probability of observing <span class="math inline">\(x=71\)</span> for each of the variance <span class="math inline">\(\sigma^2\)</span> is as follows:</p>
<p><span class="math inline">\(Lik\sigma_1\)</span> = 0.2659615, <span class="math inline">\(Lik\sigma_2\)</span> = 0.1994711, <span class="math inline">\(Lik\sigma_3\)</span> = 0.1595769, <span class="math inline">\(Lik\sigma_4\)</span> = 0.1139835.</p>

<div class="sourceCode" id="cb24"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb24-1" data-line-number="1">(<span class="dt">likelihood =</span> <span class="kw">normal.likelihood</span>(x, <span class="dt">mean =</span> mean, <span class="dt">sd =</span> sd))</a></code></pre></div>
<pre><code>## [1] 0.2659615 0.1994711 0.1595769 0.1139835</code></pre>

<p>From there, we take the maximum likelihood (alsu assumed <strong>MLE</strong>) for <strong>variance</strong>:</p>

<div class="sourceCode" id="cb26"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb26-1" data-line-number="1">(<span class="dt">MLE =</span> <span class="kw">max</span>(likelihood))</a></code></pre></div>
<pre><code>## [1] 0.2659615</code></pre>

<p>Finally, we get the optimal parameter - the <strong>variance</strong> <span class="math inline">\(\hat{\sigma}^2\)</span> estimate (or <strong>sd</strong> <span class="math inline">\(\hat{\sigma}\)</span> estimate for that matter) - based on our <strong>MLE</strong>:</p>

<div class="sourceCode" id="cb28"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb28-1" data-line-number="1">(<span class="dt">sd.est =</span> sd[ <span class="kw">which.max</span>(likelihood) ] ) <span class="co">#MLE</span></a></code></pre></div>
<pre><code>## [1] 1.5</code></pre>

<p>Because we deal with a random variable for <strong>variance</strong>, we can also visualize its distribution and show the assumed <strong>MLE</strong> at the peak of the curve. See Figure .</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:mlesigma"></span>
<img src="DS_files/figure-html/mlesigma-1.png" alt="Likelihood (parameter variance distribution)" width="70%" />
<p class="caption">
Figure 3.7: Likelihood (parameter variance distribution)
</p>
</div>

<p>For a joint-density likelihood, let us introduce <strong>negative log-likelihood (NLL)</strong> - a counterpart version of <strong>ML</strong>. We use <strong>NLL</strong> to avoid multiplication overflows (but more importantly, for our loss function - see Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under the <strong>General Modeling</strong> section. Also, for more discussion around <strong>maximum estimation</strong> and <strong>negative log-likelihood (NLL)</strong>, see <strong>parameter estimation</strong> in a later section.</p>


<p>To compute for joint-density likelihood, let us first suppose that the <strong>known</strong> parameters are derived from an actual distribution (these parameters become our reference for validation):</p>

<div class="sourceCode" id="cb30"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb30-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;known mean&quot;</span>=true.mean, <span class="st">&quot;known sd&quot;</span>=<span class="st"> </span>true.sigma)</a></code></pre></div>
<pre><code>## known mean   known sd 
##  51.714286   2.146985</code></pre>

<p>Next, let us perform sampling. In the later section, we discuss a list of more advanced <strong>sampling techniques</strong> such as the <strong>Markov Chain Monte Carlo (MCMC)</strong> technique using <strong>JAGS</strong>. For now, to perform a simple sampling, let us use the <strong>rnorm()</strong> function:</p>

<div class="sourceCode" id="cb32"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb32-1" data-line-number="1">(<span class="dt">sample =</span> <span class="kw">round</span>( <span class="kw">rnorm</span>(<span class="dt">n =</span> <span class="dv">10</span>, <span class="dt">mean =</span> true.mean, <span class="dt">sd =</span> true.sigma), <span class="dv">3</span>))</a></code></pre></div>
<pre><code>##  [1] 49.630 53.150 52.016 48.569 55.204 50.986 54.946 50.071 50.095 54.288</code></pre>

<p>The sample may provide the initial estimates of the parameters (they are quite close to the known parameters):</p>

<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb34-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;initial mean&quot;</span> =<span class="st"> </span><span class="kw">mean</span>(sample), <span class="st">&quot;initial sd&quot;</span>=<span class="st"> </span><span class="kw">sqrt</span>( <span class="kw">var</span>(sample) ) )</a></code></pre></div>
<pre><code>## initial mean   initial sd 
##    51.895500     2.384206</code></pre>

<p>Suppose now that we only know the variance <span class="math inline">\(\sigma\)</span> = 2.1469849. The mean <span class="math inline">\(\mu\)</span> is unknown. Let us compute for the mean estimate <span class="math inline">\(\hat{\mu}\)</span> of the true mean <span class="math inline">\(\mu\)</span>.</p>

<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb36-1" data-line-number="1">loglike.mean =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb36-2" data-line-number="2"><span class="cf">for</span> (mu <span class="cf">in</span> mean) {</a>
<a class="sourceLine" id="cb36-3" data-line-number="3">  lk.mean  =<span class="st"> </span><span class="kw">loglikelihood</span>( <span class="kw">normal.likelihood</span>(sample, </a>
<a class="sourceLine" id="cb36-4" data-line-number="4">                                      <span class="dt">mean=</span>mu, <span class="dt">sd =</span> true.sigma) )</a>
<a class="sourceLine" id="cb36-5" data-line-number="5">  loglike.mean =<span class="st"> </span><span class="kw">cbind</span>(loglike.mean, lk.mean )</a>
<a class="sourceLine" id="cb36-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb36-7" data-line-number="7">loglike.mean =<span class="st"> </span><span class="kw">round</span>(loglike.mean, <span class="dv">3</span>)</a></code></pre></div>

<p>The overall <strong>NLL</strong> for <strong>mean</strong> and its corresponding value <span class="math inline">\(\mu^*\)</span>:</p>

<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb37-1" data-line-number="1">mean.nll  =<span class="st"> </span><span class="kw">round</span>( <span class="kw">min</span>(loglike.mean), <span class="dv">3</span>) </a>
<a class="sourceLine" id="cb37-2" data-line-number="2">mean.est =<span class="st"> </span><span class="kw">round</span>( mean[ <span class="kw">which.min</span>(loglike.mean) ], <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb37-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;NLL&quot;</span>=<span class="st"> </span>mean.nll, <span class="st">&quot;mean&quot;</span> =<span class="st"> </span>mean.est)</a></code></pre></div>
<pre><code>##    NLL   mean 
## 22.662 52.406</code></pre>

<p>Next, suppose that we only know the mean <span class="math inline">\(\mu\)</span> = 51.7142857. The variance <span class="math inline">\(\sigma\)</span> is unknown. Let us solve for the <span class="math inline">\(\sigma\)</span>.</p>

<div class="sourceCode" id="cb39"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb39-1" data-line-number="1">loglike.sd =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb39-2" data-line-number="2"><span class="cf">for</span> (sd <span class="cf">in</span> sigma) {</a>
<a class="sourceLine" id="cb39-3" data-line-number="3">  lk.sigma  =<span class="st"> </span><span class="kw">loglikelihood</span>( <span class="kw">normal.likelihood</span>(sample, </a>
<a class="sourceLine" id="cb39-4" data-line-number="4">                                        <span class="dt">mean=</span> true.mean, <span class="dt">sd =</span> sd) )</a>
<a class="sourceLine" id="cb39-5" data-line-number="5">  loglike.sd =<span class="st"> </span><span class="kw">cbind</span>(loglike.sd, <span class="kw">min</span>(lk.sigma) )</a>
<a class="sourceLine" id="cb39-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb39-7" data-line-number="7">loglike.sd =<span class="st"> </span><span class="kw">round</span>(loglike.sd, <span class="dv">3</span>)</a></code></pre></div>

<p>The overall <strong>NLL</strong> for <strong>sd</strong> and its corresponding value <span class="math inline">\(\sigma^*\)</span>:</p>

<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb40-1" data-line-number="1">sd.nll    =<span class="st"> </span><span class="kw">round</span> ( <span class="kw">min</span>(loglike.sd), <span class="dv">3</span>) </a>
<a class="sourceLine" id="cb40-2" data-line-number="2">sd.est    =<span class="st"> </span><span class="kw">round</span>( sigma[ <span class="kw">which.min</span>(loglike.sd)], <span class="dv">3</span>) </a>
<a class="sourceLine" id="cb40-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;NLL&quot;</span>=<span class="st"> </span>sd.nll, <span class="st">&quot;sd&quot;</span>=<span class="st"> </span>sd.est)</a></code></pre></div>
<pre><code>##    NLL     sd 
## 22.388  2.219</code></pre>

<p>Here is a table of the likelihood for mean and variance with corresponding overall <strong>NLL</strong> (see Table ).</p>

<table>
<caption><span id="tab:likelihoodtab">Table 3.1: </span>Parameter Estimation</caption>
<thead>
<tr class="header">
<th align="right">mean</th>
<th align="right">minimum NLL (<span class="math inline">\(\theta_{\mu}^*\)</span>)</th>
<th align="right">sd</th>
<th align="right">minimum NLL (<span class="math inline">\(\theta_{\sigma}^*\)</span>)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">53.835</td>
<td align="right">26.458</td>
<td align="right">2.595</td>
<td align="right">22.549</td>
</tr>
<tr class="even">
<td align="right">48.233</td>
<td align="right">36.929</td>
<td align="right">1.691</td>
<td align="right">23.446</td>
</tr>
<tr class="odd">
<td align="right">45.789</td>
<td align="right">62.821</td>
<td align="right">3.500</td>
<td align="right">23.819</td>
</tr>
<tr class="even">
<td align="right">53.083</td>
<td align="right">23.908</td>
<td align="right">1.515</td>
<td align="right">24.559</td>
</tr>
<tr class="odd">
<td align="right">57.030</td>
<td align="right">50.976</td>
<td align="right">1.992</td>
<td align="right">22.568</td>
</tr>
<tr class="even">
<td align="right">55.226</td>
<td align="right">34.408</td>
<td align="right">1.352</td>
<td align="right">26.292</td>
</tr>
<tr class="odd">
<td align="right">51.128</td>
<td align="right">23.019</td>
<td align="right">2.219</td>
<td align="right">22.388</td>
</tr>
<tr class="even">
<td align="right">51.353</td>
<td align="right">22.698</td>
<td align="right">2.633</td>
<td align="right">22.584</td>
</tr>
<tr class="odd">
<td align="right">52.406</td>
<td align="right">22.662</td>
<td align="right">1.817</td>
<td align="right">22.960</td>
</tr>
<tr class="even">
<td align="right">49.060</td>
<td align="right">31.100</td>
<td align="right">2.156</td>
<td align="right">22.410</td>
</tr>
</tbody>
</table>

<p>where:</p>
<p>minimum NLL (<span class="math inline">\(\theta_{\mu}^*\)</span>) <span class="math inline">\(\rightarrow\)</span> min (-<span class="math inline">\(nl Lik (\theta_{\mu}^* = 52.406| X )) =22.662\)</span> and</p>
<p>minimum NLL (<span class="math inline">\(\theta_{\sigma}^*\)</span>) <span class="math inline">\(\rightarrow\)</span> min (-<span class="math inline">\(nl Lik (\theta_{\sigma}^* = 2.219 | X )) =22.388\)</span>.</p>
<p>Let us introduce a few family of joint likelihood formulas (suppose data is <strong>IID</strong>):</p>

<table>
<caption><span id="tab:familylik">Table 3.2: </span>Family of Likelihood</caption>
<colgroup>
<col width="5%" />
<col width="48%" />
<col width="38%" />
<col width="7%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Family</th>
<th align="left"><span class="math inline">\(Lik (\theta\)</span>|<span class="math inline">\(x_1,...,x_n\)</span>)</th>
<th align="left"><span class="math inline">\(logLik (\theta\)</span>|<span class="math inline">\(x_1,...,x_n\)</span>)</th>
<th align="left"><span class="math inline">\(\theta\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Uniform</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^n\frac{1}{\theta}, 0 \le x_i \le \theta\)</span></td>
<td align="left"><span class="math inline">\(-n \cdot ln(\theta)\)</span></td>
<td align="left"><span class="math inline">\(\theta\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"><span class="math inline">\(\text{ }\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}} \cdot exp\left(-\frac{(x_i - \mu)^2}{2\sigma^2}\right)\)</span></td>
<td align="left"><span class="math inline">\(\frac{n}{2}ln(2\pi\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n(x_i - \mu)^2\)</span></td>
<td align="left"><span class="math inline">\(\mu,\sigma^2\)</span></td>
</tr>
<tr class="even">
<td align="left">Binomial</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^m \binom{n}{x_i}\cdot \rho^{x_i}(1 - \rho)^{n-x_i}\)</span></td>
<td align="left"><span class="math inline">\(ln \binom{n}{x} + x ln(\rho) + (n - x) ln (1 - \rho)\)</span></td>
<td align="left"><span class="math inline">\(\rho\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\rho^X( 1 - \rho)^{n-X}, X = \sum_{i=1}^n x_i\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Geometric</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^n \rho^n( 1 - \rho)^{x_i - n}\)</span></td>
<td align="left"><span class="math inline">\(n\cdot ln (\rho) + \left(\sum_{i=1}^n x_i - n \right) ln ( 1 - \rho)\)</span></td>
<td align="left"><span class="math inline">\(\rho\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\rho^n ( 1 - \rho)^{\sum_{i=1} (x_i - n)}\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Poisson</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^n \frac{\lambda^{x_i}}{x_i!} \cdot exp(-\lambda)\)</span></td>
<td align="left"><span class="math inline">\(\sum_{i=1}^n\left( x_i ln(\lambda) - ln(x_i!) - \lambda \right )\)</span></td>
<td align="left"><span class="math inline">\(\lambda\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\frac{\lambda \sum_{i=1}^n x_i}{\prod_{i=1}^n x_i} \cdot exp(-n \lambda)\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Exponential</td>
<td align="left"><span class="math inline">\(\prod_{i=1}^n \lambda \cdot exp(-\lambda x), x\ge 0\)</span></td>
<td align="left"><span class="math inline">\(-n ln(\lambda) - \frac{1}{\lambda} \sum_{i=1}^n x_i, 0 &lt; \lambda &lt; \infty\)</span></td>
<td align="left"><span class="math inline">\(\lambda\)</span></td>
</tr>
<tr class="odd">
<td align="left"></td>
<td align="left"><span class="math inline">\(\frac{1}{\lambda^n} \cdot exp\left(\frac{-\sum_{i=1}^n x_i}{\lambda}\right)\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>Note that the likelihood for Normal Distribution can also be written as:</p>
<p><span class="math display">\[
Lik (\theta|x_1,...,x_n) = 
\left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n \cdot exp \left[\frac{-1}{2\sigma^2} \sum_{i=1}^n (x_i - \mu)^2\right]
\]</span></p>
<p>One other topic relevant to <strong>Likelihood</strong> is around <strong>base rate fallacy</strong>, which is often the cause of <strong>missing pertinent evidence</strong> (e.g., due to subjective (bias) decisions such as omitting relevant information).</p>
<p><strong>Base Rate Fallacy</strong> can also be called <strong>base rate neglect</strong> or <strong>base rate bias</strong>. It is Likelihood not measured based on frequency or commonality; rather based on the causal bias for which the Likelihood of an event to occur is based on information from other events based on premonition or irrational belief or feeling.</p>
<p>We leave readers to investigate this topic further, including the <strong>Uncertainty Quantification</strong>, which discusses <strong>Aleatoric uncertainty</strong> (Statistical uncertainty) and <strong>Epistemic uncertainty</strong> (Systematic uncertainty) - balancing knowledge vs.Â practice.</p>
</div>
<div id="posterior-probability" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.3</span> Posterior Probability  <a href="bayes-theorem.html#posterior-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term <strong>Posterior</strong> can be defined generally as <strong>that which comes after</strong>. In <strong>Bayesian</strong> statistics, a <strong>Posterior probability</strong>, also called <strong>a-posteriori probability</strong>, refers to the <strong>proportion of uncertainty</strong> of a random event <strong>after</strong> evidence is taken into consideration. Other terms are relevant to <strong>Posterior</strong> such as <strong>posterior odds, posterior uncertainty, posterior density, or posterior distribution</strong>.</p>
<p>A posterior probability, <span class="math inline">\(P(\theta|X)\)</span>, is obtained using the following formula:</p>
<p><span class="math display">\[\begin{align}
\underbrace{P(\theta|X)}_\text{posterior}\ \propto\ \underbrace{Lik(\theta|X)}_\text{likelihood} \times \underbrace{P(\theta)}_\text{prior}\ \ \ \ \ where\ \ \ \ Lik(\theta|X) \equiv P(X|\theta)
\end{align}\]</span></p>
<p>Or for multiple observations, we have the following notation:</p>
<p><span class="math display">\[\begin{align}
P(\theta|x_1,...x_n) \propto  Lik(\theta|x_1,...x_n) \times P(\theta) 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li>the <strong>sampling density (likelihood)</strong> represents the new evidence (new observation)</li>
<li>the <strong>prior</strong> is a-priori probability that represents weight for the <strong>likelihood</strong></li>
<li>the <strong>posterior</strong> is a-posteriori probability that represents the new summary (new model) for theta (<span class="math inline">\(\theta\)</span>), e.g.Â the mean and variance that characterize an updated <strong>posterior</strong> normal distribution.</li>
</ul>
<p>Note that for <strong>IIDs</strong>, we can drop the normalizing factor: <span class="math inline">\(P(x_1,...,x_n)\)</span>.</p>
<p>Now, recall the following equivalent equation again when testing positive for symptoms of a certain disease:</p>
<p><span class="math display">\[\begin{align}
P(\text{symptom}|\mathbf{+})_{(posterior)} \propto  
P(\mathbf{+}|\text{symptom}) \times P(\text{symptom})
\end{align}\]</span></p>
<p>In previous sections, we analyze flu-like symptoms in the presence of a particular pathogen in a small local community. We denote flu-like symptoms with theta (<span class="math inline">\(\theta\)</span>). We also denote the presence of the pathogen with the variable <strong>X</strong>. There is a previous recording of about 25% of the local community experiencing similar strange flu-like symptoms in that particular case. That is a <strong>prior</strong> knowledge.</p>
<p>Based on our posterior probability computation in our previous case, we know that the resulting probability is 7.5%, <span class="math inline">\(P(\theta|X) = 7.5\%\)</span>. Therefore, we can make this our new prior knowledge to further our analysis.</p>
<p><span class="math display">\[\begin{align}
P(\theta)\ \ \leftarrow P(\theta|X)\ \ \ \ \ \ \ where\ \ new\ P(\theta) = 7.5\%
\end{align}\]</span></p>
<p>If we are to perform another analysis today, granting today we find a piece of new evidence that 11% are found positive with the pathogen, and 4% of the locals with flu-like symptoms also carry the pathogen, then we have a new posterior probability using the new prior knowledge and new evidence:</p>
<p><span class="math display">\[
P(\theta|X) = \frac{Lik(\theta|X)P(\theta)}{P(X)}
= \frac{(0.04)(0.075)}{(0.11)} = 0.0273 = 2.73\%
\]</span></p>
</div>
<div id="prior-probability" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.3.4</span> Prior Probability  <a href="bayes-theorem.html#prior-probability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The term <strong>Prior</strong> can be defined generally as <strong>that which comes before</strong>. In <strong>Bayesian</strong> statistics, a <strong>Prior probability</strong>, also called <strong>a-priori probability</strong>, refers to the <strong>proportion of uncertainty</strong> of a random event <strong>before</strong> evidence is taken into consideration. There are other terms relevant to <strong>prior</strong> such as <strong>prior odds, prior uncertainty, prior density, or prior distribution</strong>.</p>
<p>A prior probability, <span class="math inline">\(P(\theta)\)</span>, is formulated based on <strong>prior knowledge</strong> and is applied to a <strong>likelihood</strong> of observing data given theta (<span class="math inline">\(\theta\)</span>).</p>
<p><span class="math display">\[\begin{align}
P(\theta|x_1,...x_n) \propto  Lik(\theta|x_1,...x_n)\times P(\theta)\ \ \leftarrow\ \ \ \ \
\text{posterior} \propto \text{likelihood} \times  \text{prior}
\end{align}\]</span></p>
<p>For example, recall the following equivalent equation when testing positive (<span class="math inline">\(\mathbf{+}\)</span>) for symptoms of a particular disease:</p>
<p><span class="math display">\[\begin{align}
P(\text{symptom}|\mathbf{+}) \propto  
P(\mathbf{+}|\text{symptom})\times  P(\text{symptom})_{(prior)}
\end{align}\]</span></p>
<p>We may show cold-like or flu-like symptoms when contracting certain diseases, for an illustrative example. These symptoms feed into our initial <strong>hypothetical</strong> prognosis. This prognosis describes an initial temporary belief of the state or condition of our health until we visit our general physician for professional diagnosis to know the cause of our symptoms - actual evidence of what we contracted. Our prognosis is mere <strong>hypothetical</strong> and becomes an initial <strong>prior</strong> knowledge. However, we can be creative and start imagining all sorts of other <strong>hypothetical</strong> predispositions of our condition, but that, of course, does not help our physician to reach a better recommendation or conclusion. To help guide our physician, we have to provide our most accurate condition or risk a less accurate diagnosis. In other words, we start with the premise that our prior is merely an <strong>uninformed</strong> belief - our prognosis. It can be regarded as a <strong>weakly informed prior</strong>. Such weak belief requires a doctorâs diagnostics. Therefore, more diagnostic evidence allows us to offset our belief and thus creates a more evidence-based, diagnostic-based (or data-based) posterior result. Otherwise, partial analysis (likelihood) and prognosis (prior belief) may need to be combined for a proper posterior if the diagnostic analysis is insufficient.</p>
<p>Also, mathematically, our <strong>prior distribution</strong> becomes <strong>improper</strong> if we end up with an infinite range of prognostic possibilities, meaning that our prior does not integrate to 1. An <strong>improper prior</strong> happens when corresponding prior probability does not integrate to a finite number:</p>
<p><span class="math display">\[\begin{align}
P(\theta) = \int f(\theta) d\theta = +\infty
\end{align}\]</span></p>
<p>Nonetheless, as long as the posterior probability is <strong>proper</strong> - meaning that it integrates to 1 - then our prior probability does not have to be <strong>proper</strong>. We continue to extend our discussion on <strong>informative prior</strong> in the <strong>Hyperparameters</strong> section.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="probability-rules.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="conjugacy.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
