<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 6 Statistical Computation | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 6 Statistical Computation | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Statistical Computation | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-14" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="numericalprobability.html"/>
<link rel="next" href="bayesian.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="index.html"><a href="index.html#acknowledgment-and-motivations"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="index.html"><a href="index.html#caveat"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="index.html"><a href="index.html#about-the-author"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="mathematical-notation.html"><a href="mathematical-notation.html#notation"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="mathematical-notation.html"><a href="mathematical-notation.html#number-system"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="mathematical-notation.html"><a href="mathematical-notation.html#implementation"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalmethods.html"><a href="numericalmethods.html"><i class="fa fa-check"></i><b>1</b> Direct and Indirect Methods</a><ul>
<li class="chapter" data-level="1.1" data-path="numericalmethods.html"><a href="numericalmethods.html#closed-form-equation"><i class="fa fa-check"></i><b>1.1</b> Closed-form equation</a></li>
<li class="chapter" data-level="1.2" data-path="numericalmethods.html"><a href="numericalmethods.html#analytical-and-numerical-solutions"><i class="fa fa-check"></i><b>1.2</b> Analytical and Numerical solutions  </a></li>
<li class="chapter" data-level="1.3" data-path="numericalmethods.html"><a href="numericalmethods.html#significant-figures"><i class="fa fa-check"></i><b>1.3</b> Significant figures</a></li>
<li class="chapter" data-level="1.4" data-path="numericalmethods.html"><a href="numericalmethods.html#accuracy"><i class="fa fa-check"></i><b>1.4</b> Accuracy</a></li>
<li class="chapter" data-level="1.5" data-path="numericalmethods.html"><a href="numericalmethods.html#precision"><i class="fa fa-check"></i><b>1.5</b> Precision </a></li>
<li class="chapter" data-level="1.6" data-path="numericalmethods.html"><a href="numericalmethods.html#stability-and-sensitivity"><i class="fa fa-check"></i><b>1.6</b> Stability and Sensitivity  </a></li>
<li class="chapter" data-level="1.7" data-path="numericalmethods.html"><a href="numericalmethods.html#stiffness-and-implicitness"><i class="fa fa-check"></i><b>1.7</b> Stiffness and Implicitness  </a></li>
<li class="chapter" data-level="1.8" data-path="numericalmethods.html"><a href="numericalmethods.html#conditioning-and-posedness"><i class="fa fa-check"></i><b>1.8</b> Conditioning and Posedness  </a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="linearalgebra.html"><a href="linearalgebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra I</a><ul>
<li class="chapter" data-level="2.1" data-path="linearalgebra.html"><a href="linearalgebra.html#system-of-linear-equations"><i class="fa fa-check"></i><b>2.1</b> System of Linear Equations</a></li>
<li class="chapter" data-level="2.2" data-path="linearalgebra.html"><a href="linearalgebra.html#scalar-vector-and-matrix-tensor"><i class="fa fa-check"></i><b>2.2</b> Scalar, Vector, and Matrix, Tensor</a></li>
<li class="chapter" data-level="2.3" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition-and-multiplication"><i class="fa fa-check"></i><b>2.3</b> Transposition and Multiplication</a><ul>
<li class="chapter" data-level="2.3.1" data-path="linearalgebra.html"><a href="linearalgebra.html#transposition"><i class="fa fa-check"></i><b>2.3.1</b> Transposition</a></li>
<li class="chapter" data-level="2.3.2" data-path="linearalgebra.html"><a href="linearalgebra.html#dot-product"><i class="fa fa-check"></i><b>2.3.2</b> Dot Product</a></li>
<li class="chapter" data-level="2.3.3" data-path="linearalgebra.html"><a href="linearalgebra.html#hadamard-product"><i class="fa fa-check"></i><b>2.3.3</b> Hadamard Product</a></li>
<li class="chapter" data-level="2.3.4" data-path="linearalgebra.html"><a href="linearalgebra.html#kronecker-product"><i class="fa fa-check"></i><b>2.3.4</b> Kronecker Product</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linearalgebra.html"><a href="linearalgebra.html#magnitude-direction-unit-vectors"><i class="fa fa-check"></i><b>2.4</b> Magnitude, Direction, Unit Vectors</a></li>
<li class="chapter" data-level="2.5" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-combination-and-independence"><i class="fa fa-check"></i><b>2.5</b> Linear Combination and Independence</a></li>
<li class="chapter" data-level="2.6" data-path="linearalgebra.html"><a href="linearalgebra.html#space-span-and-basis"><i class="fa fa-check"></i><b>2.6</b> Space, Span, and Basis</a></li>
<li class="chapter" data-level="2.7" data-path="linearalgebra.html"><a href="linearalgebra.html#determinants"><i class="fa fa-check"></i><b>2.7</b> Determinants </a></li>
<li class="chapter" data-level="2.8" data-path="linearalgebra.html"><a href="linearalgebra.html#minors-cofactors-and-adjugate-forms"><i class="fa fa-check"></i><b>2.8</b> Minors, Cofactors, and Adjugate Forms</a></li>
<li class="chapter" data-level="2.9" data-path="linearalgebra.html"><a href="linearalgebra.html#inverse-form-and-row-echelon-form"><i class="fa fa-check"></i><b>2.9</b> Inverse Form and Row-Echelon Form</a></li>
<li class="chapter" data-level="2.10" data-path="linearalgebra.html"><a href="linearalgebra.html#linear-transformations"><i class="fa fa-check"></i><b>2.10</b> Linear Transformations</a><ul>
<li class="chapter" data-level="2.10.1" data-path="linearalgebra.html"><a href="linearalgebra.html#scaling"><i class="fa fa-check"></i><b>2.10.1</b> Scaling </a></li>
<li class="chapter" data-level="2.10.2" data-path="linearalgebra.html"><a href="linearalgebra.html#transvection-shearing"><i class="fa fa-check"></i><b>2.10.2</b> Transvection (Shearing)  </a></li>
<li class="chapter" data-level="2.10.3" data-path="linearalgebra.html"><a href="linearalgebra.html#rotation"><i class="fa fa-check"></i><b>2.10.3</b> Rotation </a></li>
<li class="chapter" data-level="2.10.4" data-path="linearalgebra.html"><a href="linearalgebra.html#reflection"><i class="fa fa-check"></i><b>2.10.4</b> Reflection </a></li>
<li class="chapter" data-level="2.10.5" data-path="linearalgebra.html"><a href="linearalgebra.html#projection"><i class="fa fa-check"></i><b>2.10.5</b> Projection </a></li>
<li class="chapter" data-level="2.10.6" data-path="linearalgebra.html"><a href="linearalgebra.html#translation"><i class="fa fa-check"></i><b>2.10.6</b> Translation </a></li>
<li class="chapter" data-level="2.10.7" data-path="linearalgebra.html"><a href="linearalgebra.html#dilation-and-composition"><i class="fa fa-check"></i><b>2.10.7</b> Dilation and Composition  </a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="linearalgebra.html"><a href="linearalgebra.html#rank-and-nullity"><i class="fa fa-check"></i><b>2.11</b> Rank and Nullity  </a></li>
<li class="chapter" data-level="2.12" data-path="linearalgebra.html"><a href="linearalgebra.html#singularity-and-triviality"><i class="fa fa-check"></i><b>2.12</b> Singularity and Triviality  </a></li>
<li class="chapter" data-level="2.13" data-path="linearalgebra.html"><a href="linearalgebra.html#orthogonality-and-orthonormality"><i class="fa fa-check"></i><b>2.13</b> Orthogonality and Orthonormality  </a></li>
<li class="chapter" data-level="2.14" data-path="linearalgebra.html"><a href="linearalgebra.html#eigenvectors-and-eigenvalues"><i class="fa fa-check"></i><b>2.14</b> Eigenvectors and Eigenvalues  </a></li>
<li class="chapter" data-level="2.15" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-reconstruction-using-eigenvalues-and-eigenvectors"><i class="fa fa-check"></i><b>2.15</b> Matrix Reconstruction using Eigenvalues and Eigenvectors</a></li>
<li class="chapter" data-level="2.16" data-path="linearalgebra.html"><a href="linearalgebra.html#diagonalizability-of-a-matrix"><i class="fa fa-check"></i><b>2.16</b> Diagonalizability of a Matrix </a></li>
<li class="chapter" data-level="2.17" data-path="linearalgebra.html"><a href="linearalgebra.html#trace-of-a-square-matrix"><i class="fa fa-check"></i><b>2.17</b> Trace of a Square Matrix </a></li>
<li class="chapter" data-level="2.18" data-path="linearalgebra.html"><a href="linearalgebra.html#algebraic-and-geometric-multiplicity"><i class="fa fa-check"></i><b>2.18</b> Algebraic and Geometric Multiplicity</a></li>
<li class="chapter" data-level="2.19" data-path="linearalgebra.html"><a href="linearalgebra.html#types-of-matrices"><i class="fa fa-check"></i><b>2.19</b> Types of Matrices</a></li>
<li class="chapter" data-level="2.20" data-path="linearalgebra.html"><a href="linearalgebra.html#matrix-factorization"><i class="fa fa-check"></i><b>2.20</b> Matrix Factorization </a><ul>
<li class="chapter" data-level="2.20.1" data-path="linearalgebra.html"><a href="linearalgebra.html#eigen-spectral-decomposition"><i class="fa fa-check"></i><b>2.20.1</b> Eigen (Spectral) Decomposition  </a></li>
<li class="chapter" data-level="2.20.2" data-path="linearalgebra.html"><a href="linearalgebra.html#ludecomposition"><i class="fa fa-check"></i><b>2.20.2</b> LU Decomposition (Doolittle Algorithm)</a></li>
<li class="chapter" data-level="2.20.3" data-path="linearalgebra.html"><a href="linearalgebra.html#ldu-factorization"><i class="fa fa-check"></i><b>2.20.3</b> LDU Factorization </a></li>
<li class="chapter" data-level="2.20.4" data-path="linearalgebra.html"><a href="linearalgebra.html#qr-factorization-gram-schmidt-householder-and-givens"><i class="fa fa-check"></i><b>2.20.4</b> QR Factorization (Gram-Schmidt, Householder, and Givens) </a></li>
<li class="chapter" data-level="2.20.5" data-path="linearalgebra.html"><a href="linearalgebra.html#cholesky-factorization"><i class="fa fa-check"></i><b>2.20.5</b> Cholesky Factorization </a></li>
<li class="chapter" data-level="2.20.6" data-path="linearalgebra.html"><a href="linearalgebra.html#svd-factorization"><i class="fa fa-check"></i><b>2.20.6</b> SVD Factorization </a></li>
<li class="chapter" data-level="2.20.7" data-path="linearalgebra.html"><a href="linearalgebra.html#jordan-decomposition"><i class="fa fa-check"></i><b>2.20.7</b> Jordan Decomposition </a></li>
<li class="chapter" data-level="2.20.8" data-path="linearalgebra.html"><a href="linearalgebra.html#other-decomposition"><i class="fa fa-check"></i><b>2.20.8</b> Other Decomposition</a></li>
</ul></li>
<li class="chapter" data-level="2.21" data-path="linearalgebra.html"><a href="linearalgebra.html#software-libraries"><i class="fa fa-check"></i><b>2.21</b> Software libraries    </a></li>
<li class="chapter" data-level="2.22" data-path="linearalgebra.html"><a href="linearalgebra.html#summary"><i class="fa fa-check"></i><b>2.22</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html"><i class="fa fa-check"></i><b>3</b> Numerical Linear Algebra II</a><ul>
<li class="chapter" data-level="3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#iteration-and-convergence"><i class="fa fa-check"></i><b>3.1</b> Iteration and Convergence </a></li>
<li class="chapter" data-level="3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-eigenvalues-and-eigenvectors-by-iteration-av-lambda-v"><i class="fa fa-check"></i><b>3.2</b> Approximating Eigenvalues and EigenVectors by Iteration (<span class="math inline">\(Av = \lambda v\)</span>)</a><ul>
<li class="chapter" data-level="3.2.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#power-method"><i class="fa fa-check"></i><b>3.2.1</b> Power Method </a></li>
<li class="chapter" data-level="3.2.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#inverse-power-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.2</b> Inverse Power Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#rayleigh-quotient-method-using-lu-decomposition"><i class="fa fa-check"></i><b>3.2.3</b> Rayleigh Quotient Method (using LU Decomposition)</a></li>
<li class="chapter" data-level="3.2.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#qr-method-using-qr-decomposition-by-givens"><i class="fa fa-check"></i><b>3.2.4</b> QR Method (using QR Decomposition by Givens)</a></li>
<li class="chapter" data-level="3.2.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-eigenvalue-method-using-jacobi-rotation"><i class="fa fa-check"></i><b>3.2.5</b> Jacobi Eigenvalue Method (using Jacobi Rotation)</a></li>
<li class="chapter" data-level="3.2.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#arnoldi-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.6</b> Arnoldi Method (using Gram-Schmidt in Krylov Subspace) </a></li>
<li class="chapter" data-level="3.2.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lanczos-method-using-gram-schmidt-in-krylov-subspace"><i class="fa fa-check"></i><b>3.2.7</b> Lanczos Method (using Gram-Schmidt in Krylov Subspace)</a></li>
<li class="chapter" data-level="3.2.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fine-tuning-of-iteration-and-convergence"><i class="fa fa-check"></i><b>3.2.8</b> Fine-Tuning of Iteration and Convergence</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-root-and-fixed-point-by-iteration"><i class="fa fa-check"></i><b>3.3</b> Approximating Root and Fixed-Point by Iteration</a><ul>
<li class="chapter" data-level="3.3.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#root-finding-method-fx-0"><i class="fa fa-check"></i><b>3.3.1</b> Root-Finding Method (<span class="math inline">\(f(x) = 0\)</span>) </a></li>
<li class="chapter" data-level="3.3.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#fixed-point-method-fx-x"><i class="fa fa-check"></i><b>3.3.2</b> Fixed-Point Method (<span class="math inline">\(f(x) = x\)</span>) </a></li>
<li class="chapter" data-level="3.3.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bisection-method"><i class="fa fa-check"></i><b>3.3.3</b> Bisection Method </a></li>
<li class="chapter" data-level="3.3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-raphson-method-using-the-tangent-line"><i class="fa fa-check"></i><b>3.3.4</b> Newton-Raphson Method (using the Tangent Line)</a></li>
<li class="chapter" data-level="3.3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#secant-method-using-the-secant-line"><i class="fa fa-check"></i><b>3.3.5</b> Secant Method (using the Secant Line)</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-solutions-to-systems-of-eqs-by-iteration-ax-b"><i class="fa fa-check"></i><b>3.4</b> Approximating Solutions to Systems of Eqs by Iteration (<span class="math inline">\(Ax = b\)</span>)</a><ul>
<li class="chapter" data-level="3.4.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#krylovmethods"><i class="fa fa-check"></i><b>3.4.1</b> Krylov Methods</a></li>
<li class="chapter" data-level="3.4.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#gmres-generalized-minimal-residual"><i class="fa fa-check"></i><b>3.4.2</b> GMRES (Generalized Minimal Residual)  </a></li>
<li class="chapter" data-level="3.4.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#conjugate-gradient-method-cg"><i class="fa fa-check"></i><b>3.4.3</b> Conjugate Gradient Method (CG)  </a></li>
<li class="chapter" data-level="3.4.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#jacobi-and-gauss-seidel-method"><i class="fa fa-check"></i><b>3.4.4</b> Jacobi and Gauss-Seidel Method </a></li>
<li class="chapter" data-level="3.4.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#successive-over-relaxation-sor-method"><i class="fa fa-check"></i><b>3.4.5</b> Successive Over-Relaxation (SOR) Method  </a></li>
<li class="chapter" data-level="3.4.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newtons-method"><i class="fa fa-check"></i><b>3.4.6</b> Newtonâs Method </a></li>
<li class="chapter" data-level="3.4.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#broydens-method"><i class="fa fa-check"></i><b>3.4.7</b> Broydenâs Method </a></li>
<li class="chapter" data-level="3.4.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bfgs-broyden-fletcher-goldfarb-shanno-method"><i class="fa fa-check"></i><b>3.4.8</b> BFGS (Broyden-Fletcher-Goldfarb-Shanno) method </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialregression"><i class="fa fa-check"></i><b>3.5</b> Approximating Polynomial Functions by Regression</a><ul>
<li class="chapter" data-level="3.5.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#least-squares"><i class="fa fa-check"></i><b>3.5.1</b> Least-Squares </a></li>
<li class="chapter" data-level="3.5.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#linear-regression"><i class="fa fa-check"></i><b>3.5.2</b> Linear Regression </a></li>
<li class="chapter" data-level="3.5.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#higherdegreepolynomials"><i class="fa fa-check"></i><b>3.5.3</b> Higher Degree Polynomials</a></li>
<li class="chapter" data-level="3.5.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#non-linear-regression"><i class="fa fa-check"></i><b>3.5.4</b> Non-Linear Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#approximating-polynomial-functions-by-series-expansion"><i class="fa fa-check"></i><b>3.6</b> Approximating Polynomial Functions by Series Expansion </a></li>
<li class="chapter" data-level="3.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialinterpolation"><i class="fa fa-check"></i><b>3.7</b> Approximating Polynomial Functions by Interpolation</a><ul>
<li class="chapter" data-level="3.7.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.1</b> Polynomial interpolation </a></li>
<li class="chapter" data-level="3.7.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.7.2</b> Lagrange interpolation </a></li>
<li class="chapter" data-level="3.7.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-interpolation"><i class="fa fa-check"></i><b>3.7.3</b> Newton interpolation </a></li>
<li class="chapter" data-level="3.7.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-forward-interpolation"><i class="fa fa-check"></i><b>3.7.4</b> Newton Forward interpolation </a></li>
<li class="chapter" data-level="3.7.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#newton-backward-interpolation"><i class="fa fa-check"></i><b>3.7.5</b> Newton Backward interpolation </a></li>
<li class="chapter" data-level="3.7.6" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#interpolation-considerations"><i class="fa fa-check"></i><b>3.7.6</b> Interpolation Considerations</a></li>
<li class="chapter" data-level="3.7.7" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lebesque-constant"><i class="fa fa-check"></i><b>3.7.7</b> Lebesque Constant </a></li>
<li class="chapter" data-level="3.7.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#horners-method"><i class="fa fa-check"></i><b>3.7.8</b> Hornerâs method </a></li>
<li class="chapter" data-level="3.7.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#piecewise-polynomial-interpolation"><i class="fa fa-check"></i><b>3.7.9</b> Piecewise Polynomial Interpolation </a></li>
<li class="chapter" data-level="3.7.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#b-spline-interpolation"><i class="fa fa-check"></i><b>3.7.10</b> B-Spline interpolation </a></li>
<li class="chapter" data-level="3.7.11" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bspline"><i class="fa fa-check"></i><b>3.7.11</b> B-Spline Regression</a></li>
<li class="chapter" data-level="3.7.12" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#p-spline-regression"><i class="fa fa-check"></i><b>3.7.12</b> P-Spline Regression </a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomialsmoothing"><i class="fa fa-check"></i><b>3.8</b> Approximating Polynomial Functions by Smoothing</a><ul>
<li class="chapter" data-level="3.8.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#bin-smoothing"><i class="fa fa-check"></i><b>3.8.1</b> Bin Smoothing </a></li>
<li class="chapter" data-level="3.8.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#kernel-smoothing"><i class="fa fa-check"></i><b>3.8.2</b> Kernel Smoothing </a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#polynomial-optimization"><i class="fa fa-check"></i><b>3.9</b> Polynomial Optimization </a><ul>
<li class="chapter" data-level="3.9.1" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#simplexmethod"><i class="fa fa-check"></i><b>3.9.1</b> Simplex Method</a></li>
<li class="chapter" data-level="3.9.2" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#dualsimplex"><i class="fa fa-check"></i><b>3.9.2</b> Dual Simplex</a></li>
<li class="chapter" data-level="3.9.3" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#primaldual"><i class="fa fa-check"></i><b>3.9.3</b> Primal-Dual Formulation</a></li>
<li class="chapter" data-level="3.9.4" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#lagrange-multiplier"><i class="fa fa-check"></i><b>3.9.4</b> Lagrange Multiplier </a></li>
<li class="chapter" data-level="3.9.5" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#karush-khun-tucker-conditions"><i class="fa fa-check"></i><b>3.9.5</b> Karush-Khun-Tucker Conditions </a></li>
</ul></li>
<li class="chapter" data-level="3.10" data-path="numericallinearalgebra.html"><a href="numericallinearalgebra.html#summary-1"><i class="fa fa-check"></i><b>3.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="numericalcalculus.html"><a href="numericalcalculus.html"><i class="fa fa-check"></i><b>4</b> Numerical Calculus</a><ul>
<li class="chapter" data-level="4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#introductory-calculus"><i class="fa fa-check"></i><b>4.1</b> Introductory Calculus</a><ul>
<li class="chapter" data-level="4.1.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#function"><i class="fa fa-check"></i><b>4.1.1</b> Function</a></li>
<li class="chapter" data-level="4.1.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#slopes"><i class="fa fa-check"></i><b>4.1.2</b> Slopes</a></li>
<li class="chapter" data-level="4.1.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#limits"><i class="fa fa-check"></i><b>4.1.3</b> Limits</a></li>
<li class="chapter" data-level="4.1.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#derivatives"><i class="fa fa-check"></i><b>4.1.4</b> Derivatives</a></li>
<li class="chapter" data-level="4.1.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#integrals"><i class="fa fa-check"></i><b>4.1.5</b> Integrals </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-integration"><i class="fa fa-check"></i><b>4.2</b> Approximation by Numerical Integration </a><ul>
<li class="chapter" data-level="4.2.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#newton-cotes-quadrature"><i class="fa fa-check"></i><b>4.2.1</b> Newton-Cotes Quadrature </a></li>
<li class="chapter" data-level="4.2.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#composite-and-adaptive-quadrature"><i class="fa fa-check"></i><b>4.2.2</b> Composite and Adaptive Quadrature </a></li>
<li class="chapter" data-level="4.2.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#gaussianquadrature"><i class="fa fa-check"></i><b>4.2.3</b> Gaussian Quadrature</a></li>
<li class="chapter" data-level="4.2.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#romberg-integration"><i class="fa fa-check"></i><b>4.2.4</b> Romberg integration </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-by-numerical-differentiation"><i class="fa fa-check"></i><b>4.3</b> Approximation by Numerical Differentiation </a><ul>
<li class="chapter" data-level="4.3.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#order-of-accuracy"><i class="fa fa-check"></i><b>4.3.1</b> Order of Accuracy</a></li>
<li class="chapter" data-level="4.3.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference"><i class="fa fa-check"></i><b>4.3.2</b> Finite Difference </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-ordinary-differential-equations"><i class="fa fa-check"></i><b>4.4</b> Approximation using Ordinary Differential Equations  </a><ul>
<li class="chapter" data-level="4.4.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-explicit"><i class="fa fa-check"></i><b>4.4.1</b> Eulerâs Method (Explicit) </a></li>
<li class="chapter" data-level="4.4.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#eulers-method-implicit"><i class="fa fa-check"></i><b>4.4.2</b> Eulerâs Method (Implicit)</a></li>
<li class="chapter" data-level="4.4.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#heuns-method"><i class="fa fa-check"></i><b>4.4.3</b> Heunâs Method </a></li>
<li class="chapter" data-level="4.4.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#runge-kutta-method"><i class="fa fa-check"></i><b>4.4.4</b> Runge-Kutta Method </a></li>
<li class="chapter" data-level="4.4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#shooting-method"><i class="fa fa-check"></i><b>4.4.5</b> Shooting Method </a></li>
<li class="chapter" data-level="4.4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-difference-method"><i class="fa fa-check"></i><b>4.4.6</b> Finite Difference Method  </a></li>
<li class="chapter" data-level="4.4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#finite-element-method-based-on-wrm-and-vm"><i class="fa fa-check"></i><b>4.4.7</b> Finite Element Method (based on WRM and VM) </a></li>
<li class="chapter" data-level="4.4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#least-square-method-using-wrm"><i class="fa fa-check"></i><b>4.4.8</b> Least-Square Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.9" data-path="numericalcalculus.html"><a href="numericalcalculus.html#galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.9</b> Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.10" data-path="numericalcalculus.html"><a href="numericalcalculus.html#petrov-galerkin-method-using-wrm"><i class="fa fa-check"></i><b>4.4.10</b> Petrov-Galerkin Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.11" data-path="numericalcalculus.html"><a href="numericalcalculus.html#rayleigh-ritz-method-using-wrm"><i class="fa fa-check"></i><b>4.4.11</b> Rayleigh-Ritz Method (using WRM)</a></li>
<li class="chapter" data-level="4.4.12" data-path="numericalcalculus.html"><a href="numericalcalculus.html#subdomain-method-using-subdomains"><i class="fa fa-check"></i><b>4.4.12</b> Subdomain Method (using subdomains)</a></li>
<li class="chapter" data-level="4.4.13" data-path="numericalcalculus.html"><a href="numericalcalculus.html#collocation-method-using-direct-location-points"><i class="fa fa-check"></i><b>4.4.13</b> Collocation Method (using direct location points) </a></li>
<li class="chapter" data-level="4.4.14" data-path="numericalcalculus.html"><a href="numericalcalculus.html#weighted-residual-summary"><i class="fa fa-check"></i><b>4.4.14</b> Weighted Residual Summary </a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-functional-differential-equations"><i class="fa fa-check"></i><b>4.5</b> Approximation using Functional Differential Equations </a><ul>
<li class="chapter" data-level="4.5.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-functions"><i class="fa fa-check"></i><b>4.5.1</b> Variational Functions </a></li>
<li class="chapter" data-level="4.5.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#variational-methods"><i class="fa fa-check"></i><b>4.5.2</b> Variational Methods </a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-partial-differential-equations"><i class="fa fa-check"></i><b>4.6</b> Approximation using Partial Differential Equations </a><ul>
<li class="chapter" data-level="4.6.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-laplace-equation-elliptic-pde"><i class="fa fa-check"></i><b>4.6.1</b> The Laplace Equation (Elliptic PDE)  </a></li>
<li class="chapter" data-level="4.6.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-heat-equation-parabolic-pde"><i class="fa fa-check"></i><b>4.6.2</b> The Heat equation (Parabolic PDE)  </a></li>
<li class="chapter" data-level="4.6.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-wave-equation-hyperbolic-pde"><i class="fa fa-check"></i><b>4.6.3</b> The Wave equation (Hyperbolic PDE)  </a></li>
<li class="chapter" data-level="4.6.4" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-crank-nicolson-equation"><i class="fa fa-check"></i><b>4.6.4</b> The Crank-Nicolson Equation </a></li>
<li class="chapter" data-level="4.6.5" data-path="numericalcalculus.html"><a href="numericalcalculus.html#the-burgers-equation"><i class="fa fa-check"></i><b>4.6.5</b> The Burgerâs Equation </a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="numericalcalculus.html"><a href="numericalcalculus.html#approximation-using-fourier-series-and-transform"><i class="fa fa-check"></i><b>4.7</b> Approximation using Fourier Series And Transform </a><ul>
<li class="chapter" data-level="4.7.1" data-path="numericalcalculus.html"><a href="numericalcalculus.html#discrete-fourier-transform-dft"><i class="fa fa-check"></i><b>4.7.1</b> Discrete Fourier Transform (DFT)  </a></li>
<li class="chapter" data-level="4.7.2" data-path="numericalcalculus.html"><a href="numericalcalculus.html#inverse-discrete-fourier-transformation-idft"><i class="fa fa-check"></i><b>4.7.2</b> Inverse Discrete Fourier Transformation (IDFT)  </a></li>
<li class="chapter" data-level="4.7.3" data-path="numericalcalculus.html"><a href="numericalcalculus.html#fast-fourier-transform-fft"><i class="fa fa-check"></i><b>4.7.3</b> Fast Fourier Transform (FFT)  </a></li>
</ul></li>
<li class="chapter" data-level="4.8" data-path="numericalcalculus.html"><a href="numericalcalculus.html#summary-2"><i class="fa fa-check"></i><b>4.8</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>5</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="5.1" data-path="numericalprobability.html"><a href="numericalprobability.html#approximation-based-on-random-chances"><i class="fa fa-check"></i><b>5.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="5.2" data-path="numericalprobability.html"><a href="numericalprobability.html#distribution"><i class="fa fa-check"></i><b>5.2</b> Distribution</a></li>
<li class="chapter" data-level="5.3" data-path="numericalprobability.html"><a href="numericalprobability.html#mass-and-density"><i class="fa fa-check"></i><b>5.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="5.4" data-path="numericalprobability.html"><a href="numericalprobability.html#probability"><i class="fa fa-check"></i><b>5.4</b> Probability  </a></li>
<li class="chapter" data-level="5.5" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-density-function-pdf"><i class="fa fa-check"></i><b>5.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="5.6" data-path="numericalprobability.html"><a href="numericalprobability.html#probability-mass-function-pmf"><i class="fa fa-check"></i><b>5.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="5.7" data-path="numericalprobability.html"><a href="numericalprobability.html#cumulative-distribution-function-cdf"><i class="fa fa-check"></i><b>5.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="5.8" data-path="numericalprobability.html"><a href="numericalprobability.html#special-functions"><i class="fa fa-check"></i><b>5.8</b> Special Functions</a><ul>
<li class="chapter" data-level="5.8.1" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-function"><i class="fa fa-check"></i><b>5.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="5.8.2" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>5.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="5.8.3" data-path="numericalprobability.html"><a href="numericalprobability.html#digamma-function"><i class="fa fa-check"></i><b>5.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="5.8.4" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-function"><i class="fa fa-check"></i><b>5.8.4</b> Beta function </a></li>
<li class="chapter" data-level="5.8.5" data-path="numericalprobability.html"><a href="numericalprobability.html#incomplete-beta-function"><i class="fa fa-check"></i><b>5.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="5.8.6" data-path="numericalprobability.html"><a href="numericalprobability.html#regularized-beta-function"><i class="fa fa-check"></i><b>5.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="5.8.7" data-path="numericalprobability.html"><a href="numericalprobability.html#hypergeometric-function"><i class="fa fa-check"></i><b>5.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="5.8.8" data-path="numericalprobability.html"><a href="numericalprobability.html#continued-fraction"><i class="fa fa-check"></i><b>5.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="5.8.9" data-path="numericalprobability.html"><a href="numericalprobability.html#dirac-delta-function"><i class="fa fa-check"></i><b>5.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="5.8.10" data-path="numericalprobability.html"><a href="numericalprobability.html#kronecker-delta-function"><i class="fa fa-check"></i><b>5.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="5.9" data-path="numericalprobability.html"><a href="numericalprobability.html#distributiontypes"><i class="fa fa-check"></i><b>5.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="5.9.1" data-path="numericalprobability.html"><a href="numericalprobability.html#bernoulli-distribution"><i class="fa fa-check"></i><b>5.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="5.9.2" data-path="numericalprobability.html"><a href="numericalprobability.html#binomial-distribution"><i class="fa fa-check"></i><b>5.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="5.9.3" data-path="numericalprobability.html"><a href="numericalprobability.html#multinomial-distribution"><i class="fa fa-check"></i><b>5.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="5.9.4" data-path="numericalprobability.html"><a href="numericalprobability.html#geometric-distribution"><i class="fa fa-check"></i><b>5.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="5.9.5" data-path="numericalprobability.html"><a href="numericalprobability.html#beta-distribution"><i class="fa fa-check"></i><b>5.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="5.9.6" data-path="numericalprobability.html"><a href="numericalprobability.html#dirichlet-distribution"><i class="fa fa-check"></i><b>5.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="5.9.7" data-path="numericalprobability.html"><a href="numericalprobability.html#exponential-distribution"><i class="fa fa-check"></i><b>5.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="5.9.8" data-path="numericalprobability.html"><a href="numericalprobability.html#gamma-distribution"><i class="fa fa-check"></i><b>5.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="5.9.9" data-path="numericalprobability.html"><a href="numericalprobability.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>5.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="5.9.10" data-path="numericalprobability.html"><a href="numericalprobability.html#weibull-distribution"><i class="fa fa-check"></i><b>5.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="5.9.11" data-path="numericalprobability.html"><a href="numericalprobability.html#poisson-distribution"><i class="fa fa-check"></i><b>5.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="5.9.12" data-path="numericalprobability.html"><a href="numericalprobability.html#pareto-distribution"><i class="fa fa-check"></i><b>5.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="5.9.13" data-path="numericalprobability.html"><a href="numericalprobability.html#normal-distribution"><i class="fa fa-check"></i><b>5.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="5.9.14" data-path="numericalprobability.html"><a href="numericalprobability.html#wald-distribution"><i class="fa fa-check"></i><b>5.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="5.9.15" data-path="numericalprobability.html"><a href="numericalprobability.html#log-normal-distribution"><i class="fa fa-check"></i><b>5.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="5.9.16" data-path="numericalprobability.html"><a href="numericalprobability.html#uniform-distribution"><i class="fa fa-check"></i><b>5.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="5.9.17" data-path="numericalprobability.html"><a href="numericalprobability.html#t-distribution"><i class="fa fa-check"></i><b>5.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="5.9.18" data-path="numericalprobability.html"><a href="numericalprobability.html#f-distribution"><i class="fa fa-check"></i><b>5.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="5.9.19" data-path="numericalprobability.html"><a href="numericalprobability.html#chi-square-distribution"><i class="fa fa-check"></i><b>5.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="5.9.20" data-path="numericalprobability.html"><a href="numericalprobability.html#wishartdistribution"><i class="fa fa-check"></i><b>5.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="5.9.21" data-path="numericalprobability.html"><a href="numericalprobability.html#lkj-distribution"><i class="fa fa-check"></i><b>5.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="5.9.22" data-path="numericalprobability.html"><a href="numericalprobability.html#mixture-distribution"><i class="fa fa-check"></i><b>5.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="5.9.23" data-path="numericalprobability.html"><a href="numericalprobability.html#non-parametric-distribution"><i class="fa fa-check"></i><b>5.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="5.9.24" data-path="numericalprobability.html"><a href="numericalprobability.html#multi-dimensional-density"><i class="fa fa-check"></i><b>5.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="5.10" data-path="numericalprobability.html"><a href="numericalprobability.html#summary-3"><i class="fa fa-check"></i><b>5.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>6</b> Statistical Computation</a><ul>
<li class="chapter" data-level="6.1" data-path="statistics.html"><a href="statistics.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="6.1.1" data-path="statistics.html"><a href="statistics.html#visual-representation"><i class="fa fa-check"></i><b>6.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="6.1.2" data-path="statistics.html"><a href="statistics.html#central-tendency"><i class="fa fa-check"></i><b>6.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="6.1.3" data-path="statistics.html"><a href="statistics.html#variability"><i class="fa fa-check"></i><b>6.1.3</b> Variability </a></li>
<li class="chapter" data-level="6.1.4" data-path="statistics.html"><a href="statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>6.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="6.1.5" data-path="statistics.html"><a href="statistics.html#five-number-summary"><i class="fa fa-check"></i><b>6.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="statistics.html"><a href="statistics.html#inferential-statistics"><i class="fa fa-check"></i><b>6.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="6.3" data-path="statistics.html"><a href="statistics.html#the-significance-of-difference"><i class="fa fa-check"></i><b>6.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="6.3.1" data-path="statistics.html"><a href="statistics.html#hypothesis"><i class="fa fa-check"></i><b>6.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="6.3.2" data-path="statistics.html"><a href="statistics.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>6.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="6.3.3" data-path="statistics.html"><a href="statistics.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>6.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="6.3.4" data-path="statistics.html"><a href="statistics.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>6.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="6.3.5" data-path="statistics.html"><a href="statistics.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>6.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.6" data-path="statistics.html"><a href="statistics.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>6.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="6.3.7" data-path="statistics.html"><a href="statistics.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>6.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="6.3.8" data-path="statistics.html"><a href="statistics.html#wilcoxon-test"><i class="fa fa-check"></i><b>6.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="6.3.9" data-path="statistics.html"><a href="statistics.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>6.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="6.3.10" data-path="statistics.html"><a href="statistics.html#friedman-test"><i class="fa fa-check"></i><b>6.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="statistics.html"><a href="statistics.html#post-hoc-analysis"><i class="fa fa-check"></i><b>6.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="6.4.1" data-path="statistics.html"><a href="statistics.html#bonferroni-correction"><i class="fa fa-check"></i><b>6.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="6.4.2" data-path="statistics.html"><a href="statistics.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>6.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="statistics.html"><a href="statistics.html#multiple-comparison-tests"><i class="fa fa-check"></i><b>6.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="6.5.1" data-path="statistics.html"><a href="statistics.html#scheffes-test"><i class="fa fa-check"></i><b>6.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="6.5.2" data-path="statistics.html"><a href="statistics.html#fishers-test"><i class="fa fa-check"></i><b>6.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="6.5.3" data-path="statistics.html"><a href="statistics.html#tukeys-test"><i class="fa fa-check"></i><b>6.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="6.5.4" data-path="statistics.html"><a href="statistics.html#newman-keul-test"><i class="fa fa-check"></i><b>6.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="6.5.5" data-path="statistics.html"><a href="statistics.html#games-howell-test"><i class="fa fa-check"></i><b>6.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="6.5.6" data-path="statistics.html"><a href="statistics.html#dunnetts-test"><i class="fa fa-check"></i><b>6.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="6.5.7" data-path="statistics.html"><a href="statistics.html#duncans-test"><i class="fa fa-check"></i><b>6.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="6.5.8" data-path="statistics.html"><a href="statistics.html#meta-analysis-test"><i class="fa fa-check"></i><b>6.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="statistics.html"><a href="statistics.html#statistical-modeling"><i class="fa fa-check"></i><b>6.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="6.6.1" data-path="statistics.html"><a href="statistics.html#model-specification"><i class="fa fa-check"></i><b>6.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="6.6.2" data-path="statistics.html"><a href="statistics.html#statistical-interaction"><i class="fa fa-check"></i><b>6.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="6.6.3" data-path="statistics.html"><a href="statistics.html#dummy-variables"><i class="fa fa-check"></i><b>6.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="6.6.4" data-path="statistics.html"><a href="statistics.html#model-selection"><i class="fa fa-check"></i><b>6.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="statistics.html"><a href="statistics.html#regression-analysis"><i class="fa fa-check"></i><b>6.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="6.7.1" data-path="statistics.html"><a href="statistics.html#assumptions"><i class="fa fa-check"></i><b>6.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="6.7.2" data-path="statistics.html"><a href="statistics.html#correlation-coefficients"><i class="fa fa-check"></i><b>6.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="6.7.3" data-path="statistics.html"><a href="statistics.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>6.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="6.7.4" data-path="statistics.html"><a href="statistics.html#normality-and-leverage"><i class="fa fa-check"></i><b>6.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="6.7.5" data-path="statistics.html"><a href="statistics.html#collinearity"><i class="fa fa-check"></i><b>6.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="6.7.6" data-path="statistics.html"><a href="statistics.html#dispersion"><i class="fa fa-check"></i><b>6.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="6.7.7" data-path="statistics.html"><a href="statistics.html#diagnostic-plots"><i class="fa fa-check"></i><b>6.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="statistics.html"><a href="statistics.html#the-significance-of-regression"><i class="fa fa-check"></i><b>6.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="6.8.1" data-path="statistics.html"><a href="statistics.html#simple-linear-regression"><i class="fa fa-check"></i><b>6.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="6.8.2" data-path="statistics.html"><a href="statistics.html#multilinear-regression"><i class="fa fa-check"></i><b>6.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="6.8.3" data-path="statistics.html"><a href="statistics.html#logistic-regression"><i class="fa fa-check"></i><b>6.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="6.8.4" data-path="statistics.html"><a href="statistics.html#poisson-regression"><i class="fa fa-check"></i><b>6.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="6.8.5" data-path="statistics.html"><a href="statistics.html#cox-regression"><i class="fa fa-check"></i><b>6.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="6.8.6" data-path="statistics.html"><a href="statistics.html#polynomial-regression"><i class="fa fa-check"></i><b>6.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="6.8.7" data-path="statistics.html"><a href="statistics.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>6.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="6.8.8" data-path="statistics.html"><a href="statistics.html#spline-smoothing"><i class="fa fa-check"></i><b>6.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="6.8.9" data-path="statistics.html"><a href="statistics.html#loess-and-lowess"><i class="fa fa-check"></i><b>6.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="6.9" data-path="statistics.html"><a href="statistics.html#inference-for-regression"><i class="fa fa-check"></i><b>6.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="6.9.1" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>6.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="6.9.2" data-path="statistics.html"><a href="statistics.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>6.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="6.9.3" data-path="statistics.html"><a href="statistics.html#confidence-interval"><i class="fa fa-check"></i><b>6.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="6.10" data-path="statistics.html"><a href="statistics.html#summary-4"><i class="fa fa-check"></i><b>6.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>7</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="7.1" data-path="bayesian.html"><a href="bayesian.html#probability-1"><i class="fa fa-check"></i><b>7.1</b> Probability </a><ul>
<li class="chapter" data-level="7.1.1" data-path="bayesian.html"><a href="bayesian.html#marginal-probability"><i class="fa fa-check"></i><b>7.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="7.1.2" data-path="bayesian.html"><a href="bayesian.html#joint-probability"><i class="fa fa-check"></i><b>7.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="7.1.3" data-path="bayesian.html"><a href="bayesian.html#conditional-probability"><i class="fa fa-check"></i><b>7.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="7.1.4" data-path="bayesian.html"><a href="bayesian.html#negation-probability"><i class="fa fa-check"></i><b>7.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="7.1.5" data-path="bayesian.html"><a href="bayesian.html#combination-of-probabilities"><i class="fa fa-check"></i><b>7.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="bayesian.html"><a href="bayesian.html#probability-rules"><i class="fa fa-check"></i><b>7.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="7.2.1" data-path="bayesian.html"><a href="bayesian.html#law-of-total-probability"><i class="fa fa-check"></i><b>7.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="7.2.2" data-path="bayesian.html"><a href="bayesian.html#law-of-total-expectation"><i class="fa fa-check"></i><b>7.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="7.2.3" data-path="bayesian.html"><a href="bayesian.html#law-of-total-variance"><i class="fa fa-check"></i><b>7.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="7.2.4" data-path="bayesian.html"><a href="bayesian.html#law-of-total-covariance"><i class="fa fa-check"></i><b>7.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="7.2.5" data-path="bayesian.html"><a href="bayesian.html#law-of-large-numbers"><i class="fa fa-check"></i><b>7.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="7.2.6" data-path="bayesian.html"><a href="bayesian.html#central-limit-theorem"><i class="fa fa-check"></i><b>7.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="7.2.7" data-path="bayesian.html"><a href="bayesian.html#rule-of-independence"><i class="fa fa-check"></i><b>7.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="7.2.8" data-path="bayesian.html"><a href="bayesian.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>7.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="7.2.9" data-path="bayesian.html"><a href="bayesian.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>7.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="bayesian.html"><a href="bayesian.html#bayes-theorem"><i class="fa fa-check"></i><b>7.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="7.3.1" data-path="bayesian.html"><a href="bayesian.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>7.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="7.3.2" data-path="bayesian.html"><a href="bayesian.html#likelihood"><i class="fa fa-check"></i><b>7.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="7.3.3" data-path="bayesian.html"><a href="bayesian.html#posterior-probability"><i class="fa fa-check"></i><b>7.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="7.3.4" data-path="bayesian.html"><a href="bayesian.html#prior-probability"><i class="fa fa-check"></i><b>7.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="bayesian.html"><a href="bayesian.html#conjugacy"><i class="fa fa-check"></i><b>7.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="7.4.1" data-path="bayesian.html"><a href="bayesian.html#precision-1"><i class="fa fa-check"></i><b>7.4.1</b> Precision </a></li>
<li class="chapter" data-level="7.4.2" data-path="bayesian.html"><a href="bayesian.html#conjugate-prior"><i class="fa fa-check"></i><b>7.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="7.4.3" data-path="bayesian.html"><a href="bayesian.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.4" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.5" data-path="bayesian.html"><a href="bayesian.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>7.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="7.4.6" data-path="bayesian.html"><a href="bayesian.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.7" data-path="bayesian.html"><a href="bayesian.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>7.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="7.4.8" data-path="bayesian.html"><a href="bayesian.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>7.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="7.4.9" data-path="bayesian.html"><a href="bayesian.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.10" data-path="bayesian.html"><a href="bayesian.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>7.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="7.4.11" data-path="bayesian.html"><a href="bayesian.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.12" data-path="bayesian.html"><a href="bayesian.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>7.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="7.4.13" data-path="bayesian.html"><a href="bayesian.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>7.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="7.4.14" data-path="bayesian.html"><a href="bayesian.html#hyperparameters"><i class="fa fa-check"></i><b>7.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="bayesian.html"><a href="bayesian.html#information-theory"><i class="fa fa-check"></i><b>7.5</b> Information Theory </a><ul>
<li class="chapter" data-level="7.5.1" data-path="bayesian.html"><a href="bayesian.html#information"><i class="fa fa-check"></i><b>7.5.1</b> Information </a></li>
<li class="chapter" data-level="7.5.2" data-path="bayesian.html"><a href="bayesian.html#entropy"><i class="fa fa-check"></i><b>7.5.2</b> Entropy </a></li>
<li class="chapter" data-level="7.5.3" data-path="bayesian.html"><a href="bayesian.html#gini-index"><i class="fa fa-check"></i><b>7.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="7.5.4" data-path="bayesian.html"><a href="bayesian.html#information-gain"><i class="fa fa-check"></i><b>7.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="7.5.5" data-path="bayesian.html"><a href="bayesian.html#mutual-information"><i class="fa fa-check"></i><b>7.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="7.5.6" data-path="bayesian.html"><a href="bayesian.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>7.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="7.5.7" data-path="bayesian.html"><a href="bayesian.html#jensens-inequality"><i class="fa fa-check"></i><b>7.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="bayesian.html"><a href="bayesian.html#bayesianinference"><i class="fa fa-check"></i><b>7.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="7.6.1" data-path="bayesian.html"><a href="bayesian.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>7.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="7.6.2" data-path="bayesian.html"><a href="bayesian.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>7.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="7.6.3" data-path="bayesian.html"><a href="bayesian.html#laplace-approximation"><i class="fa fa-check"></i><b>7.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="7.6.4" data-path="bayesian.html"><a href="bayesian.html#expectation-maximization-em"><i class="fa fa-check"></i><b>7.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="7.6.5" data-path="bayesian.html"><a href="bayesian.html#variational-inference"><i class="fa fa-check"></i><b>7.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>8</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="8.1" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-models"><i class="fa fa-check"></i><b>8.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="8.1.1" data-path="bayesian2.html"><a href="bayesian2.html#belief-propagation"><i class="fa fa-check"></i><b>8.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="8.1.2" data-path="bayesian2.html"><a href="bayesian2.html#expectation-propagation"><i class="fa fa-check"></i><b>8.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="8.1.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain"><i class="fa fa-check"></i><b>8.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="8.1.4" data-path="bayesian2.html"><a href="bayesian2.html#hidden-markov-model"><i class="fa fa-check"></i><b>8.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="8.1.5" data-path="bayesian2.html"><a href="bayesian2.html#dynamic-system-model"><i class="fa fa-check"></i><b>8.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="8.1.6" data-path="bayesian2.html"><a href="bayesian2.html#bayes-filter"><i class="fa fa-check"></i><b>8.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="8.1.7" data-path="bayesian2.html"><a href="bayesian2.html#kalman-filter"><i class="fa fa-check"></i><b>8.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="8.1.8" data-path="bayesian2.html"><a href="bayesian2.html#extended-kalman-filter"><i class="fa fa-check"></i><b>8.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="8.1.9" data-path="bayesian2.html"><a href="bayesian2.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>8.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="8.1.10" data-path="bayesian2.html"><a href="bayesian2.html#particle-filter"><i class="fa fa-check"></i><b>8.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="8.1.11" data-path="bayesian2.html"><a href="bayesian2.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>8.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="bayesian2.html"><a href="bayesian2.html#simulation-and-sampling"><i class="fa fa-check"></i><b>8.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="8.2.1" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>8.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="8.2.2" data-path="bayesian2.html"><a href="bayesian2.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>8.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="8.2.3" data-path="bayesian2.html"><a href="bayesian2.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>8.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.4" data-path="bayesian2.html"><a href="bayesian2.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>8.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.5" data-path="bayesian2.html"><a href="bayesian2.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>8.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="8.2.6" data-path="bayesian2.html"><a href="bayesian2.html#gibbs-sampling"><i class="fa fa-check"></i><b>8.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="8.2.7" data-path="bayesian2.html"><a href="bayesian2.html#importance-sampling"><i class="fa fa-check"></i><b>8.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="8.2.8" data-path="bayesian2.html"><a href="bayesian2.html#rejection-sampling"><i class="fa fa-check"></i><b>8.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="8.2.9" data-path="bayesian2.html"><a href="bayesian2.html#jags-modeling"><i class="fa fa-check"></i><b>8.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="bayesian2.html"><a href="bayesian2.html#bayesian-analysis"><i class="fa fa-check"></i><b>8.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="8.3.1" data-path="bayesian2.html"><a href="bayesian2.html#autocorrelation"><i class="fa fa-check"></i><b>8.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="8.3.2" data-path="bayesian2.html"><a href="bayesian2.html#predictive-probability"><i class="fa fa-check"></i><b>8.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="8.3.3" data-path="bayesian2.html"><a href="bayesian2.html#posterior-interval"><i class="fa fa-check"></i><b>8.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="8.3.4" data-path="bayesian2.html"><a href="bayesian2.html#bayes-factor"><i class="fa fa-check"></i><b>8.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="bayesian2.html"><a href="bayesian2.html#summary-5"><i class="fa fa-check"></i><b>8.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>9</b> Computational Learning I</a><ul>
<li class="chapter" data-level="9.1" data-path="machinelearning1.html"><a href="machinelearning1.html#observation-and-measurement"><i class="fa fa-check"></i><b>9.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="9.1.1" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-measurements"><i class="fa fa-check"></i><b>9.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="9.1.2" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>9.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="9.1.3" data-path="machinelearning1.html"><a href="machinelearning1.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>9.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="9.1.4" data-path="machinelearning1.html"><a href="machinelearning1.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>9.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="machinelearning1.html"><a href="machinelearning1.html#input-data"><i class="fa fa-check"></i><b>9.2</b> Input Data</a><ul>
<li class="chapter" data-level="9.2.1" data-path="machinelearning1.html"><a href="machinelearning1.html#structured-data"><i class="fa fa-check"></i><b>9.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="9.2.2" data-path="machinelearning1.html"><a href="machinelearning1.html#non-structured-data"><i class="fa fa-check"></i><b>9.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="9.2.3" data-path="machinelearning1.html"><a href="machinelearning1.html#statistical-data"><i class="fa fa-check"></i><b>9.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="9.2.4" data-path="machinelearning1.html"><a href="machinelearning1.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>9.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="9.2.5" data-path="machinelearning1.html"><a href="machinelearning1.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>9.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="9.2.6" data-path="machinelearning1.html"><a href="machinelearning1.html#data-lake"><i class="fa fa-check"></i><b>9.2.6</b> Data lake</a></li>
<li class="chapter" data-level="9.2.7" data-path="machinelearning1.html"><a href="machinelearning1.html#natural-language-nl"><i class="fa fa-check"></i><b>9.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="9.2.8" data-path="machinelearning1.html"><a href="machinelearning1.html#multimedia-md"><i class="fa fa-check"></i><b>9.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="machinelearning1.html"><a href="machinelearning1.html#primitive-methods"><i class="fa fa-check"></i><b>9.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="9.3.1" data-path="machinelearning1.html"><a href="machinelearning1.html#weighting"><i class="fa fa-check"></i><b>9.3.1</b> Weighting</a></li>
<li class="chapter" data-level="9.3.2" data-path="machinelearning1.html"><a href="machinelearning1.html#smoothing"><i class="fa fa-check"></i><b>9.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="9.3.3" data-path="machinelearning1.html"><a href="machinelearning1.html#normalizing"><i class="fa fa-check"></i><b>9.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="9.3.4" data-path="machinelearning1.html"><a href="machinelearning1.html#standardizing"><i class="fa fa-check"></i><b>9.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="9.3.5" data-path="machinelearning1.html"><a href="machinelearning1.html#centering"><i class="fa fa-check"></i><b>9.3.5</b> Centering </a></li>
<li class="chapter" data-level="9.3.6" data-path="machinelearning1.html"><a href="machinelearning1.html#scaling-1"><i class="fa fa-check"></i><b>9.3.6</b> Scaling </a></li>
<li class="chapter" data-level="9.3.7" data-path="machinelearning1.html"><a href="machinelearning1.html#transforming"><i class="fa fa-check"></i><b>9.3.7</b> Transforming</a></li>
<li class="chapter" data-level="9.3.8" data-path="machinelearning1.html"><a href="machinelearning1.html#clipping"><i class="fa fa-check"></i><b>9.3.8</b> Clipping </a></li>
<li class="chapter" data-level="9.3.9" data-path="machinelearning1.html"><a href="machinelearning1.html#regularizing"><i class="fa fa-check"></i><b>9.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="machinelearning1.html"><a href="machinelearning1.html#distance-metrics"><i class="fa fa-check"></i><b>9.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="9.4.1" data-path="machinelearning1.html"><a href="machinelearning1.html#cosine-similarity"><i class="fa fa-check"></i><b>9.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="9.4.2" data-path="machinelearning1.html"><a href="machinelearning1.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>9.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="9.4.3" data-path="machinelearning1.html"><a href="machinelearning1.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>9.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="9.4.4" data-path="machinelearning1.html"><a href="machinelearning1.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>9.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="9.4.5" data-path="machinelearning1.html"><a href="machinelearning1.html#hamming-distance"><i class="fa fa-check"></i><b>9.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="9.4.6" data-path="machinelearning1.html"><a href="machinelearning1.html#mahalanobis-distance"><i class="fa fa-check"></i><b>9.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="9.4.7" data-path="machinelearning1.html"><a href="machinelearning1.html#precision-and-accuracy"><i class="fa fa-check"></i><b>9.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="9.4.8" data-path="machinelearning1.html"><a href="machinelearning1.html#auc-on-roc"><i class="fa fa-check"></i><b>9.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="machinelearning1.html"><a href="machinelearning1.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>9.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="9.5.1" data-path="machinelearning1.html"><a href="machinelearning1.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>9.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="9.5.2" data-path="machinelearning1.html"><a href="machinelearning1.html#association"><i class="fa fa-check"></i><b>9.5.2</b> Association</a></li>
<li class="chapter" data-level="9.5.3" data-path="machinelearning1.html"><a href="machinelearning1.html#pattern-discovery"><i class="fa fa-check"></i><b>9.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="9.5.4" data-path="machinelearning1.html"><a href="machinelearning1.html#null-invariance"><i class="fa fa-check"></i><b>9.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="9.5.5" data-path="machinelearning1.html"><a href="machinelearning1.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>9.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="9.5.6" data-path="machinelearning1.html"><a href="machinelearning1.html#covariance"><i class="fa fa-check"></i><b>9.5.6</b> Covariance </a></li>
<li class="chapter" data-level="9.5.7" data-path="machinelearning1.html"><a href="machinelearning1.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>9.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="9.5.8" data-path="machinelearning1.html"><a href="machinelearning1.html#dominating-factors"><i class="fa fa-check"></i><b>9.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="9.5.9" data-path="machinelearning1.html"><a href="machinelearning1.html#missingness-and-imputation"><i class="fa fa-check"></i><b>9.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="9.5.10" data-path="machinelearning1.html"><a href="machinelearning1.html#confounding-variable"><i class="fa fa-check"></i><b>9.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="9.5.11" data-path="machinelearning1.html"><a href="machinelearning1.html#data-leakage"><i class="fa fa-check"></i><b>9.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="9.5.12" data-path="machinelearning1.html"><a href="machinelearning1.html#one-hot-encoding"><i class="fa fa-check"></i><b>9.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="9.5.13" data-path="machinelearning1.html"><a href="machinelearning1.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>9.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="9.5.14" data-path="machinelearning1.html"><a href="machinelearning1.html#discretization"><i class="fa fa-check"></i><b>9.5.14</b> Discretization </a></li>
<li class="chapter" data-level="9.5.15" data-path="machinelearning1.html"><a href="machinelearning1.html#stratification"><i class="fa fa-check"></i><b>9.5.15</b> Stratification </a></li>
<li class="chapter" data-level="9.5.16" data-path="machinelearning1.html"><a href="machinelearning1.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>9.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="9.5.17" data-path="machinelearning1.html"><a href="machinelearning1.html#embedding"><i class="fa fa-check"></i><b>9.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="9.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureengineering"><i class="fa fa-check"></i><b>9.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="9.6.1" data-path="machinelearning1.html"><a href="machinelearning1.html#machine-learning-features"><i class="fa fa-check"></i><b>9.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="9.6.2" data-path="machinelearning1.html"><a href="machinelearning1.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="9.6.3" data-path="machinelearning1.html"><a href="machinelearning1.html#principal-component-analysis"><i class="fa fa-check"></i><b>9.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="9.6.4" data-path="machinelearning1.html"><a href="machinelearning1.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="9.6.5" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-construction"><i class="fa fa-check"></i><b>9.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="9.6.6" data-path="machinelearning1.html"><a href="machinelearning1.html#featureselection"><i class="fa fa-check"></i><b>9.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="9.6.7" data-path="machinelearning1.html"><a href="machinelearning1.html#feature-transformation"><i class="fa fa-check"></i><b>9.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="9.6.8" data-path="machinelearning1.html"><a href="machinelearning1.html#model-specification-1"><i class="fa fa-check"></i><b>9.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="machinelearning1.html"><a href="machinelearning1.html#general-modeling"><i class="fa fa-check"></i><b>9.7</b> General Modeling</a><ul>
<li class="chapter" data-level="9.7.1" data-path="machinelearning1.html"><a href="machinelearning1.html#training-learning"><i class="fa fa-check"></i><b>9.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="9.7.2" data-path="machinelearning1.html"><a href="machinelearning1.html#validation-tuning"><i class="fa fa-check"></i><b>9.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="9.7.3" data-path="machinelearning1.html"><a href="machinelearning1.html#testing-assessing"><i class="fa fa-check"></i><b>9.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="9.7.4" data-path="machinelearning1.html"><a href="machinelearning1.html#cross-validation-cv"><i class="fa fa-check"></i><b>9.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="9.7.5" data-path="machinelearning1.html"><a href="machinelearning1.html#bias-and-variance"><i class="fa fa-check"></i><b>9.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="9.7.6" data-path="machinelearning1.html"><a href="machinelearning1.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>9.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="9.7.7" data-path="machinelearning1.html"><a href="machinelearning1.html#global-and-local-minima"><i class="fa fa-check"></i><b>9.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="9.7.8" data-path="machinelearning1.html"><a href="machinelearning1.html#regularization"><i class="fa fa-check"></i><b>9.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="machinelearning1.html"><a href="machinelearning1.html#supervised-vs.unsupervised-learning"><i class="fa fa-check"></i><b>9.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="9.9" data-path="machinelearning1.html"><a href="machinelearning1.html#summary-6"><i class="fa fa-check"></i><b>9.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="machinelearning2.html"><a href="machinelearning2.html"><i class="fa fa-check"></i><b>10</b> Computational Learning II</a><ul>
<li class="chapter" data-level="10.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression"><i class="fa fa-check"></i><b>10.1</b> Regression (Supervised)</a><ul>
<li class="chapter" data-level="10.1.1" data-path="machinelearning2.html"><a href="machinelearning2.html#regression-trees"><i class="fa fa-check"></i><b>10.1.1</b> Regression Trees </a></li>
<li class="chapter" data-level="10.1.2" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods"><i class="fa fa-check"></i><b>10.1.2</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.1.3" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest"><i class="fa fa-check"></i><b>10.1.3</b> Random Forest </a></li>
<li class="chapter" data-level="10.1.4" data-path="machinelearning2.html"><a href="machinelearning2.html#Adaoost"><i class="fa fa-check"></i><b>10.1.4</b> AdaBoost</a></li>
<li class="chapter" data-level="10.1.5" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost"><i class="fa fa-check"></i><b>10.1.5</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.1.6" data-path="machinelearning2.html"><a href="machinelearning2.html#xgboost"><i class="fa fa-check"></i><b>10.1.6</b> XGBoost </a></li>
<li class="chapter" data-level="10.1.7" data-path="machinelearning2.html"><a href="machinelearning2.html#generalized-linear-modeling-glm"><i class="fa fa-check"></i><b>10.1.7</b> Generalized Linear Modeling (GLM)  </a></li>
<li class="chapter" data-level="10.1.8" data-path="machinelearning2.html"><a href="machinelearning2.html#logisticregression"><i class="fa fa-check"></i><b>10.1.8</b> Logistic Regression (GLM)</a></li>
<li class="chapter" data-level="10.1.9" data-path="machinelearning2.html"><a href="machinelearning2.html#poisson"><i class="fa fa-check"></i><b>10.1.9</b> Poisson Regression (GLM)</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="machinelearning2.html"><a href="machinelearning2.html#binary-classification-supervised"><i class="fa fa-check"></i><b>10.2</b> Binary Classification (Supervised)</a><ul>
<li class="chapter" data-level="10.2.1" data-path="machinelearning2.html"><a href="machinelearning2.html#linear-svm-sgdpegasos"><i class="fa fa-check"></i><b>10.2.1</b> Linear SVM (SGD/PEGASOS)  </a></li>
<li class="chapter" data-level="10.2.2" data-path="machinelearning2.html"><a href="machinelearning2.html#kernel-svm-smo"><i class="fa fa-check"></i><b>10.2.2</b> Kernel SVM (SMO)  </a></li>
<li class="chapter" data-level="10.2.3" data-path="machinelearning2.html"><a href="machinelearning2.html#sdca-based-svm"><i class="fa fa-check"></i><b>10.2.3</b> SDCA-based SVM </a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="machinelearning2.html"><a href="machinelearning2.html#multi-class-classification-supervised"><i class="fa fa-check"></i><b>10.3</b> Multi-class Classification (Supervised) </a><ul>
<li class="chapter" data-level="10.3.1" data-path="machinelearning2.html"><a href="machinelearning2.html#bayesian-classification"><i class="fa fa-check"></i><b>10.3.1</b> Bayesian Classification </a></li>
<li class="chapter" data-level="10.3.2" data-path="machinelearning2.html"><a href="machinelearning2.html#classification-trees"><i class="fa fa-check"></i><b>10.3.2</b> Classification Trees </a></li>
<li class="chapter" data-level="10.3.3" data-path="machinelearning2.html"><a href="machinelearning2.html#ensemble-methods-1"><i class="fa fa-check"></i><b>10.3.3</b> Ensemble Methods </a></li>
<li class="chapter" data-level="10.3.4" data-path="machinelearning2.html"><a href="machinelearning2.html#random-forest-1"><i class="fa fa-check"></i><b>10.3.4</b> Random Forest </a></li>
<li class="chapter" data-level="10.3.5" data-path="machinelearning2.html"><a href="machinelearning2.html#AdaBoost"><i class="fa fa-check"></i><b>10.3.5</b> AdaBoost &amp; SAMME</a></li>
<li class="chapter" data-level="10.3.6" data-path="machinelearning2.html"><a href="machinelearning2.html#logitboost-j-classes"><i class="fa fa-check"></i><b>10.3.6</b> LogitBoost (J Classes)</a></li>
<li class="chapter" data-level="10.3.7" data-path="machinelearning2.html"><a href="machinelearning2.html#gradient-boost-1"><i class="fa fa-check"></i><b>10.3.7</b> Gradient Boost </a></li>
<li class="chapter" data-level="10.3.8" data-path="machinelearning2.html"><a href="machinelearning2.html#k-next-neighbors-knn"><i class="fa fa-check"></i><b>10.3.8</b> K-Next Neighbors (KNN)  </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="11" data-path="machinelearning3.html"><a href="machinelearning3.html"><i class="fa fa-check"></i><b>11</b> Computational Learning III</a><ul>
<li class="chapter" data-level="11.1" data-path="machinelearning3.html"><a href="machinelearning3.html#clustering-unsupervised"><i class="fa fa-check"></i><b>11.1</b> Clustering (Unsupervised) </a><ul>
<li class="chapter" data-level="11.1.1" data-path="machinelearning3.html"><a href="machinelearning3.html#k-means-clustering"><i class="fa fa-check"></i><b>11.1.1</b> K-means (clustering) </a></li>
<li class="chapter" data-level="11.1.2" data-path="machinelearning3.html"><a href="machinelearning3.html#hierarchical-clustering"><i class="fa fa-check"></i><b>11.1.2</b> Hierarchical (clustering) </a></li>
<li class="chapter" data-level="11.1.3" data-path="machinelearning3.html"><a href="machinelearning3.html#dbscan-clustering"><i class="fa fa-check"></i><b>11.1.3</b> DBSCAN (clustering) </a></li>
<li class="chapter" data-level="11.1.4" data-path="machinelearning3.html"><a href="machinelearning3.html#quality-of-clustering"><i class="fa fa-check"></i><b>11.1.4</b> Quality of Clustering</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="machinelearning3.html"><a href="machinelearning3.html#meta-learning"><i class="fa fa-check"></i><b>11.2</b> Meta-Learning </a></li>
<li class="chapter" data-level="11.3" data-path="machinelearning3.html"><a href="machinelearning3.html#natural-language-processing-nlp"><i class="fa fa-check"></i><b>11.3</b> Natural Language Processing (NLP)  </a><ul>
<li class="chapter" data-level="11.3.1" data-path="machinelearning3.html"><a href="machinelearning3.html#pre-processing-texts"><i class="fa fa-check"></i><b>11.3.1</b> Pre-Processing Texts</a></li>
<li class="chapter" data-level="11.3.2" data-path="machinelearning3.html"><a href="machinelearning3.html#ranking-and-scoring"><i class="fa fa-check"></i><b>11.3.2</b> Ranking and Scoring </a></li>
<li class="chapter" data-level="11.3.3" data-path="machinelearning3.html"><a href="machinelearning3.html#document-similarity"><i class="fa fa-check"></i><b>11.3.3</b> Document Similarity </a></li>
<li class="chapter" data-level="11.3.4" data-path="machinelearning3.html"><a href="machinelearning3.html#linguistic-analysis"><i class="fa fa-check"></i><b>11.3.4</b> Linguistic Analysis </a></li>
<li class="chapter" data-level="11.3.5" data-path="machinelearning3.html"><a href="machinelearning3.html#lexical-analysis"><i class="fa fa-check"></i><b>11.3.5</b> Lexical Analysis </a></li>
<li class="chapter" data-level="11.3.6" data-path="machinelearning3.html"><a href="machinelearning3.html#semantic-analysis"><i class="fa fa-check"></i><b>11.3.6</b> Semantic Analysis </a></li>
<li class="chapter" data-level="11.3.7" data-path="machinelearning3.html"><a href="machinelearning3.html#named-entity-recognition-ner"><i class="fa fa-check"></i><b>11.3.7</b> Named Entity Recognition (NER)  </a></li>
<li class="chapter" data-level="11.3.8" data-path="machinelearning3.html"><a href="machinelearning3.html#sentiment-and-opinion-analysis"><i class="fa fa-check"></i><b>11.3.8</b> Sentiment and Opinion Analysis  </a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-forecasting"><i class="fa fa-check"></i><b>11.4</b> Time-Series Forecasting </a><ul>
<li class="chapter" data-level="11.4.1" data-path="machinelearning3.html"><a href="machinelearning3.html#seasonal-trend-decomposition-using-loess-stl"><i class="fa fa-check"></i><b>11.4.1</b> Seasonal Trend Decomposition using LOESS (STL)  </a></li>
<li class="chapter" data-level="11.4.2" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-models"><i class="fa fa-check"></i><b>11.4.2</b> Forecasting Models </a></li>
<li class="chapter" data-level="11.4.3" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-linear-model-tslm"><i class="fa fa-check"></i><b>11.4.3</b> Time-Series Linear Model (TSLM)  </a></li>
<li class="chapter" data-level="11.4.4" data-path="machinelearning3.html"><a href="machinelearning3.html#autoregressive-integrated-moving-average-arima"><i class="fa fa-check"></i><b>11.4.4</b> AutoRegressive Integrated Moving Average (ARIMA)  </a></li>
<li class="chapter" data-level="11.4.5" data-path="machinelearning3.html"><a href="machinelearning3.html#multiplicative-seasonal-arima-sarima"><i class="fa fa-check"></i><b>11.4.5</b> Multiplicative Seasonal ARIMA (SARIMA) </a></li>
<li class="chapter" data-level="11.4.6" data-path="machinelearning3.html"><a href="machinelearning3.html#time-series-decomposition"><i class="fa fa-check"></i><b>11.4.6</b> Time-Series Decomposition </a></li>
<li class="chapter" data-level="11.4.7" data-path="machinelearning3.html"><a href="machinelearning3.html#stl-with-aicbic"><i class="fa fa-check"></i><b>11.4.7</b> STL with AIC/BIC</a></li>
<li class="chapter" data-level="11.4.8" data-path="machinelearning3.html"><a href="machinelearning3.html#multivariate-time-series"><i class="fa fa-check"></i><b>11.4.8</b> Multivariate Time-Series</a></li>
<li class="chapter" data-level="11.4.9" data-path="machinelearning3.html"><a href="machinelearning3.html#forecasting-considerations"><i class="fa fa-check"></i><b>11.4.9</b> Forecasting Considerations</a></li>
</ul></li>
<li class="chapter" data-level="11.5" data-path="machinelearning3.html"><a href="machinelearning3.html#recommender-systems"><i class="fa fa-check"></i><b>11.5</b> Recommender Systems </a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="deeplearning1.html"><a href="deeplearning1.html"><i class="fa fa-check"></i><b>12</b> Computational Deep Learning I</a><ul>
<li class="chapter" data-level="12.1" data-path="deeplearning1.html"><a href="deeplearning1.html#simple-perceptron"><i class="fa fa-check"></i><b>12.1</b> Simple Perceptron  </a></li>
<li class="chapter" data-level="12.2" data-path="deeplearning1.html"><a href="deeplearning1.html#adaptive-linear-neuron-adaline"><i class="fa fa-check"></i><b>12.2</b> Adaptive Linear Neuron (ADALINE)  </a></li>
<li class="chapter" data-level="12.3" data-path="deeplearning1.html"><a href="deeplearning1.html#multi-layer-perceptron-mlp"><i class="fa fa-check"></i><b>12.3</b> Multi Layer Perceptron (MLP)  </a><ul>
<li class="chapter" data-level="12.3.1" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed"><i class="fa fa-check"></i><b>12.3.1</b> Forward Feed </a></li>
<li class="chapter" data-level="12.3.2" data-path="deeplearning1.html"><a href="deeplearning1.html#backward-feed"><i class="fa fa-check"></i><b>12.3.2</b> Backward Feed </a></li>
<li class="chapter" data-level="12.3.3" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation"><i class="fa fa-check"></i><b>12.3.3</b> BackPropagation </a></li>
<li class="chapter" data-level="12.3.4" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-example"><i class="fa fa-check"></i><b>12.3.4</b> MLP Example</a></li>
<li class="chapter" data-level="12.3.5" data-path="deeplearning1.html"><a href="deeplearning1.html#activation-function"><i class="fa fa-check"></i><b>12.3.5</b> Activation Function </a></li>
<li class="chapter" data-level="12.3.6" data-path="deeplearning1.html"><a href="deeplearning1.html#mlp-implementation"><i class="fa fa-check"></i><b>12.3.6</b> MLP Implementation</a></li>
<li class="chapter" data-level="12.3.7" data-path="deeplearning1.html"><a href="deeplearning1.html#deep-neural-network-dnn"><i class="fa fa-check"></i><b>12.3.7</b> Deep Neural Network (DNN)  </a></li>
<li class="chapter" data-level="12.3.8" data-path="deeplearning1.html"><a href="deeplearning1.html#vanishing-and-exploding-gradient"><i class="fa fa-check"></i><b>12.3.8</b> Vanishing and Exploding Gradient  </a></li>
<li class="chapter" data-level="12.3.9" data-path="deeplearning1.html"><a href="deeplearning1.html#dead-relu"><i class="fa fa-check"></i><b>12.3.9</b> Dead Relu </a></li>
<li class="chapter" data-level="12.3.10" data-path="deeplearning1.html"><a href="deeplearning1.html#gradient-clipping-gc"><i class="fa fa-check"></i><b>12.3.10</b> Gradient Clipping (GC) </a></li>
<li class="chapter" data-level="12.3.11" data-path="deeplearning1.html"><a href="deeplearning1.html#parameter-initialization"><i class="fa fa-check"></i><b>12.3.11</b> Parameter Initialization </a></li>
<li class="chapter" data-level="12.3.12" data-path="deeplearning1.html"><a href="deeplearning1.html#regularization-by-dropouts"><i class="fa fa-check"></i><b>12.3.12</b> Regularization by Dropouts </a></li>
<li class="chapter" data-level="12.3.13" data-path="deeplearning1.html"><a href="deeplearning1.html#batch-normalization"><i class="fa fa-check"></i><b>12.3.13</b> Batch Normalization </a></li>
<li class="chapter" data-level="12.3.14" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization"><i class="fa fa-check"></i><b>12.3.14</b> Optimization </a></li>
<li class="chapter" data-level="12.3.15" data-path="deeplearning1.html"><a href="deeplearning1.html#interpretability"><i class="fa fa-check"></i><b>12.3.15</b> Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="deeplearning1.html"><a href="deeplearning1.html#convolutional-neural-network-cnn"><i class="fa fa-check"></i><b>12.4</b> Convolutional Neural Network (CNN)  </a><ul>
<li class="chapter" data-level="12.4.1" data-path="deeplearning1.html"><a href="deeplearning1.html#computer-graphics"><i class="fa fa-check"></i><b>12.4.1</b> Computer Graphics</a></li>
<li class="chapter" data-level="12.4.2" data-path="deeplearning1.html"><a href="deeplearning1.html#convolution"><i class="fa fa-check"></i><b>12.4.2</b> Convolution </a></li>
<li class="chapter" data-level="12.4.3" data-path="deeplearning1.html"><a href="deeplearning1.html#stride-and-padding"><i class="fa fa-check"></i><b>12.4.3</b> Stride and Padding  </a></li>
<li class="chapter" data-level="12.4.4" data-path="deeplearning1.html"><a href="deeplearning1.html#kernels-and-filters"><i class="fa fa-check"></i><b>12.4.4</b> Kernels And Filters</a></li>
<li class="chapter" data-level="12.4.5" data-path="deeplearning1.html"><a href="deeplearning1.html#dilation"><i class="fa fa-check"></i><b>12.4.5</b> Dilation </a></li>
<li class="chapter" data-level="12.4.6" data-path="deeplearning1.html"><a href="deeplearning1.html#pooling"><i class="fa fa-check"></i><b>12.4.6</b> Pooling </a></li>
<li class="chapter" data-level="12.4.7" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-architectures"><i class="fa fa-check"></i><b>12.4.7</b> CNN Architectures</a></li>
<li class="chapter" data-level="12.4.8" data-path="deeplearning1.html"><a href="deeplearning1.html#forward-feed-1"><i class="fa fa-check"></i><b>12.4.8</b> Forward Feed </a></li>
<li class="chapter" data-level="12.4.9" data-path="deeplearning1.html"><a href="deeplearning1.html#backpropagation-1"><i class="fa fa-check"></i><b>12.4.9</b> BackPropagation </a></li>
<li class="chapter" data-level="12.4.10" data-path="deeplearning1.html"><a href="deeplearning1.html#optimization-1"><i class="fa fa-check"></i><b>12.4.10</b> Optimization</a></li>
<li class="chapter" data-level="12.4.11" data-path="deeplearning1.html"><a href="deeplearning1.html#normalization"><i class="fa fa-check"></i><b>12.4.11</b> Normalization</a></li>
<li class="chapter" data-level="12.4.12" data-path="deeplearning1.html"><a href="deeplearning1.html#step-decay"><i class="fa fa-check"></i><b>12.4.12</b> Step Decay</a></li>
<li class="chapter" data-level="12.4.13" data-path="deeplearning1.html"><a href="deeplearning1.html#gemm-matrix-multiplication"><i class="fa fa-check"></i><b>12.4.13</b> GEMM (Matrix Multiplication) </a></li>
<li class="chapter" data-level="12.4.14" data-path="deeplearning1.html"><a href="deeplearning1.html#depthwise-separable-convolution-dsc"><i class="fa fa-check"></i><b>12.4.14</b> Depthwise Separable Convolution (DSC)  </a></li>
<li class="chapter" data-level="12.4.15" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-implementation"><i class="fa fa-check"></i><b>12.4.15</b> CNN Implementation</a></li>
<li class="chapter" data-level="12.4.16" data-path="deeplearning1.html"><a href="deeplearning1.html#cnn-application"><i class="fa fa-check"></i><b>12.4.16</b> CNN Application</a></li>
<li class="chapter" data-level="12.4.17" data-path="deeplearning1.html"><a href="deeplearning1.html#summary-7"><i class="fa fa-check"></i><b>12.4.17</b> Summary</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="deeplearning2.html"><a href="deeplearning2.html"><i class="fa fa-check"></i><b>13</b> Computational Deep Learning II</a><ul>
<li class="chapter" data-level="13.1" data-path="deeplearning2.html"><a href="deeplearning2.html#residual-network-resnet"><i class="fa fa-check"></i><b>13.1</b> Residual Network (ResNet)  </a></li>
<li class="chapter" data-level="13.2" data-path="deeplearning2.html"><a href="deeplearning2.html#recurrent-neural-network-rnn"><i class="fa fa-check"></i><b>13.2</b> Recurrent Neural Network (RNN)  </a><ul>
<li class="chapter" data-level="13.2.1" data-path="deeplearning2.html"><a href="deeplearning2.html#vanilla-rnn"><i class="fa fa-check"></i><b>13.2.1</b> Vanilla RNN</a></li>
<li class="chapter" data-level="13.2.2" data-path="deeplearning2.html"><a href="deeplearning2.html#long-short-term-memory-lstm"><i class="fa fa-check"></i><b>13.2.2</b> Long Short-Term Memory (LSTM)  </a></li>
<li class="chapter" data-level="13.2.3" data-path="deeplearning2.html"><a href="deeplearning2.html#gated-recurrent-units-gru"><i class="fa fa-check"></i><b>13.2.3</b> Gated Recurrent Units (GRU)  </a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-rnn"><i class="fa fa-check"></i><b>13.3</b> Deep Stacked RNN </a></li>
<li class="chapter" data-level="13.4" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-stacked-bidirectional-rnn"><i class="fa fa-check"></i><b>13.4</b> Deep Stacked Bidirectional RNN </a></li>
<li class="chapter" data-level="13.5" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-neural-network-tnn"><i class="fa fa-check"></i><b>13.5</b> Transformer Neural Network (TNN)  </a><ul>
<li class="chapter" data-level="13.5.1" data-path="deeplearning2.html"><a href="deeplearning2.html#attention"><i class="fa fa-check"></i><b>13.5.1</b> Attention </a></li>
<li class="chapter" data-level="13.5.2" data-path="deeplearning2.html"><a href="deeplearning2.html#self-attention-and-trainability"><i class="fa fa-check"></i><b>13.5.2</b> Self-Attention and Trainability </a></li>
<li class="chapter" data-level="13.5.3" data-path="deeplearning2.html"><a href="deeplearning2.html#multi-head-attention"><i class="fa fa-check"></i><b>13.5.3</b> Multi-Head Attention </a></li>
<li class="chapter" data-level="13.5.4" data-path="deeplearning2.html"><a href="deeplearning2.html#word-embedding"><i class="fa fa-check"></i><b>13.5.4</b> Word Embedding </a></li>
<li class="chapter" data-level="13.5.5" data-path="deeplearning2.html"><a href="deeplearning2.html#positional-embedding"><i class="fa fa-check"></i><b>13.5.5</b> Positional Embedding </a></li>
<li class="chapter" data-level="13.5.6" data-path="deeplearning2.html"><a href="deeplearning2.html#sequence-alignment"><i class="fa fa-check"></i><b>13.5.6</b> Sequence Alignment</a></li>
<li class="chapter" data-level="13.5.7" data-path="deeplearning2.html"><a href="deeplearning2.html#transformer-architectures"><i class="fa fa-check"></i><b>13.5.7</b> Transformer Architectures </a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="deeplearning2.html"><a href="deeplearning2.html#applications-using-tnn-and-rnn"><i class="fa fa-check"></i><b>13.6</b> Applications using TNN (and RNN)</a><ul>
<li class="chapter" data-level="13.6.1" data-path="deeplearning2.html"><a href="deeplearning2.html#speech-recognition"><i class="fa fa-check"></i><b>13.6.1</b> Speech Recognition </a></li>
<li class="chapter" data-level="13.6.2" data-path="deeplearning2.html"><a href="deeplearning2.html#mel-coefficients-feature-extraction"><i class="fa fa-check"></i><b>13.6.2</b> Mel Coefficients (Feature Extraction) </a></li>
<li class="chapter" data-level="13.6.3" data-path="deeplearning2.html"><a href="deeplearning2.html#connectionist-temporal-classification-ctc"><i class="fa fa-check"></i><b>13.6.3</b> Connectionist Temporal Classification (CTC)  </a></li>
<li class="chapter" data-level="13.6.4" data-path="deeplearning2.html"><a href="deeplearning2.html#model-evaluation"><i class="fa fa-check"></i><b>13.6.4</b> Model Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13.7" data-path="deeplearning2.html"><a href="deeplearning2.html#generative-adversarial-network-gan"><i class="fa fa-check"></i><b>13.7</b> Generative Adversarial Network (GAN)  </a></li>
<li class="chapter" data-level="13.8" data-path="deeplearning2.html"><a href="deeplearning2.html#deep-reinforcement-network-dqn"><i class="fa fa-check"></i><b>13.8</b> Deep Reinforcement Network (DQN)  </a></li>
<li class="chapter" data-level="13.9" data-path="deeplearning2.html"><a href="deeplearning2.html#summary-8"><i class="fa fa-check"></i><b>13.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="distributedcomputation.html"><a href="distributedcomputation.html"><i class="fa fa-check"></i><b>14</b> Distributed Computation</a><ul>
<li class="chapter" data-level="14.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#integration-and-interoperability"><i class="fa fa-check"></i><b>14.1</b> Integration and Interoperability</a></li>
<li class="chapter" data-level="14.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#ml-pipelines"><i class="fa fa-check"></i><b>14.2</b> ML Pipelines</a></li>
<li class="chapter" data-level="14.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-standards"><i class="fa fa-check"></i><b>14.3</b> Open Standards</a><ul>
<li class="chapter" data-level="14.3.1" data-path="distributedcomputation.html"><a href="distributedcomputation.html#predictive-model-markup-language-pmml"><i class="fa fa-check"></i><b>14.3.1</b> Predictive Model Markup Language (PMML)</a></li>
<li class="chapter" data-level="14.3.2" data-path="distributedcomputation.html"><a href="distributedcomputation.html#portable-format-for-analytics-pfa"><i class="fa fa-check"></i><b>14.3.2</b> Portable Format for Analytics (PFA)</a></li>
<li class="chapter" data-level="14.3.3" data-path="distributedcomputation.html"><a href="distributedcomputation.html#open-neural-network-exchange-onnx"><i class="fa fa-check"></i><b>14.3.3</b> Open Neural Network Exchange (ONNX)</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="distributedcomputation.html"><a href="distributedcomputation.html#general-summary"><i class="fa fa-check"></i><b>14.4</b> General Summary</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>15</b> Appendix</a><ul>
<li class="chapter" data-level="15.1" data-path="appendix.html"><a href="appendix.html#appendix-a"><i class="fa fa-check"></i><b>15.1</b> Appendix A</a><ul>
<li class="chapter" data-level="15.1.1" data-path="appendix.html"><a href="appendix.html#trigonometry"><i class="fa fa-check"></i><b>15.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="15.1.2" data-path="appendix.html"><a href="appendix.html#logarithms"><i class="fa fa-check"></i><b>15.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="15.1.3" data-path="appendix.html"><a href="appendix.html#category-theory"><i class="fa fa-check"></i><b>15.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="15.2" data-path="appendix.html"><a href="appendix.html#appendix-b"><i class="fa fa-check"></i><b>15.2</b> Appendix B</a><ul>
<li class="chapter" data-level="15.2.1" data-path="appendix.html"><a href="appendix.html#on-random-chances"><i class="fa fa-check"></i><b>15.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="15.2.2" data-path="appendix.html"><a href="appendix.html#on-replacements"><i class="fa fa-check"></i><b>15.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="15.2.3" data-path="appendix.html"><a href="appendix.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>15.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="15.2.4" data-path="appendix.html"><a href="appendix.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>15.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="15.2.5" data-path="appendix.html"><a href="appendix.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>15.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="15.2.6" data-path="appendix.html"><a href="appendix.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>15.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="15.2.7" data-path="appendix.html"><a href="appendix.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>15.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="15.3" data-path="appendix.html"><a href="appendix.html#appendix-c"><i class="fa fa-check"></i><b>15.3</b> Appendix C</a></li>
<li class="chapter" data-level="15.4" data-path="appendix.html"><a href="appendix.html#appendix-d"><i class="fa fa-check"></i><b>15.4</b> Appendix D</a><ul>
<li class="chapter" data-level="15.4.1" data-path="appendix.html"><a href="appendix.html#lubridate-library"><i class="fa fa-check"></i><b>15.4.1</b> Lubridate Library</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="statistics" class="section level1 hasAnchor">
<h1><span class="header-section-number">Chapter 6</span> Statistical Computation<a href="statistics.html#statistics" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: {
            autoNumber: "AMS",
            formatNumber: function (n) {return '6.'+n}
      } 
  }
});
</script>
<p>In this chapter, we cover the <strong>Computational</strong> side of <strong>Statistics</strong>. By computation, we may refer to the use of computers to perform computational statistics. Here, we reference the great works of Sternstein M. <span class="citation">(<a href="bibliography.html#ref-ref1338s">1996</a>)</span>, Vapnik V. <span class="citation">(<a href="bibliography.html#ref-ref572v">2000</a>)</span>, Hastie T. et al. <span class="citation">(<a href="bibliography.html#ref-ref269t">2016</a>)</span>, Agresti A. et al. <span class="citation">(<a href="bibliography.html#ref-ref260a">2017</a>)</span>, Forsyth D. <span class="citation">(<a href="bibliography.html#ref-ref232d">2018</a>)</span>, and Illowsky B., Dean S. et al. <span class="citation">(<a href="bibliography.html#ref-ref818b">2018</a>)</span>, along with other additional references for consistency.</p>
<p>First, let us mention two kinds of <strong>Statistics</strong>: descriptive statistics and inferential statistics.</p>
<div id="descriptive-statistics" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.1</span> Descriptive Statistics<a href="statistics.html#descriptive-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Descriptive Statistics</strong> is <strong>Statistics</strong> based on describing (in <strong>Summary</strong>) the characteristics of sampled data. By <strong>Summary</strong>, we describe data in <strong>Moments</strong> (e.g., mean, variance or dispersion, skewness, and kurtosis), Central Tendencies, and Quartile Ranks. These <strong>Statistics</strong> are then further described by visual representation. </p>
<div id="visual-representation" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.1</span> Visual Representation<a href="statistics.html#visual-representation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most common visual representation of data is as follows:</p>
<ul>
<li>Histogram and Bar Charts - data is grouped and is presented as bars.</li>
<li>Tabular Form - data is tabulated.</li>
<li>Pie Charts - data is grouped into slices of a pie chart.</li>
<li>Line Graphs - data is presented as a line passing through data points in a graph.</li>
</ul>
<p>These charts are visual representations of specific measurements in terms of counts, percentages, or frequencies.</p>
</div>
<div id="central-tendency" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.2</span> Central Tendency <a href="statistics.html#central-tendency" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Central tendency</strong> is a measure based on common or distinct values of a distribution. There are three measurements: mean, median, and mode. For example, we expect the mean to be zero in a standard normal distribution.</p>

<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb286-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb286-2" data-line-number="2">x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">5000</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb286-3" data-line-number="3"><span class="kw">hist</span>(x, <span class="dt">breaks=</span><span class="dv">20</span>, <span class="dt">prob=</span><span class="ot">TRUE</span>, <span class="dt">main=</span><span class="st">&#39;Standard Normal Distribution&#39;</span>, </a>
<a class="sourceLine" id="cb286-4" data-line-number="4">     <span class="dt">xlab=</span><span class="st">&#39;Standard Deviation&#39;</span>,  <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>))</a>
<a class="sourceLine" id="cb286-5" data-line-number="5"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb286-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">v=</span><span class="dv">0</span>,  <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:uniform2"></span>
<img src="DS_files/figure-html/uniform2-1.png" alt="Standard Normal Distribution" width="70%" />
<p class="caption">
Figure 6.1: Standard Normal Distribution
</p>
</div>

<p>Let us briefly describe the three common central tendencies: <strong>Mean, Median, Mode</strong> <span class="citation">(Sternstein M. <a href="bibliography.html#ref-ref1338s">1996</a>)</span>.   </p>
<p>The <strong>average</strong> of a normal distribution is called the <strong>Mean</strong>.</p>
<pre class="mean"><code>  Values: 1 3 4 5 5 5 6 7 9 
  
  where N = number of values = 9

  Therefore, the mean of (1 + 3 + 4 + 5 + 5 + 5 + 6 + 7 + 9) / N = 5</code></pre>
<p>The <strong>Median</strong> of a normal distribution represents the middle value(s) which is 5 in the example above. We take the average of the middle values in an even count of sampled data.</p>
<pre class="median"><code>  Values: 1 2 3 4
  
  Therefore, the median for the middle values (2,3) is (2 + 3) / 2 = 2.5</code></pre>
<p>The <strong>Mode</strong> of a normal distribution is the value with the highest occurrence or highest frequency. Here, 5 is the mode because it repeats three times. If a sample of data does not have repeating values, then there is no mode.</p>
<pre class="mode"><code>  Values: 1 2 3 3   5 5 5   6 6 7 8 9
  
  Therefore, the mode is 5
  
  Values: 1 2 3
  
  Therefore, the mode is 0</code></pre>
<p>Additionally, it helps to be aware of the following terms, namely <strong>Mean-Centering</strong> or finding the <strong>Zero-Mean</strong>. These terms rely on looking for the central tendency.  </p>
<p>Let us study a dataset with 5 random variables ( X1, X2, X3, X4, X5 ) of 3 observations each using uniform distribution:</p>

<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" data-line-number="1"><span class="kw">options</span>(<span class="dt">scipen =</span> <span class="dv">1</span>, <span class="dt">digits =</span> <span class="dv">4</span>, <span class="dt">width =</span> <span class="dv">80</span>, <span class="dt">fig.align =</span> <span class="st">&quot;center&quot;</span>)</a>
<a class="sourceLine" id="cb290-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb290-3" data-line-number="3">dataset =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">X1 =</span> <span class="kw">runif</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">X2 =</span> <span class="kw">runif</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>),  </a>
<a class="sourceLine" id="cb290-4" data-line-number="4">                      <span class="dt">X3 =</span> <span class="kw">runif</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">X4 =</span> <span class="kw">runif</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb290-5" data-line-number="5">                      <span class="dt">X5 =</span> <span class="kw">runif</span>(<span class="dv">3</span>,<span class="dv">0</span>,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb290-6" data-line-number="6">knitr<span class="op">::</span><span class="kw">kable</span>( <span class="kw">head</span>(dataset, <span class="dv">20</span>), <span class="dt">caption =</span> <span class="st">&#39;Central Tendency&#39;</span>,  </a>
<a class="sourceLine" id="cb290-7" data-line-number="7">              <span class="dt">booktabs =</span> <span class="ot">TRUE</span> )</a></code></pre></div>
<table>
<caption><span id="tab:uniform4">Table 6.1: </span>Central Tendency</caption>
<thead>
<tr class="header">
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.8952</td>
<td align="right">0.5615</td>
<td align="right">0.2804</td>
<td align="right">0.4323</td>
<td align="right">0.7637</td>
</tr>
<tr class="even">
<td align="right">0.6990</td>
<td align="right">0.8107</td>
<td align="right">0.6833</td>
<td align="right">0.8565</td>
<td align="right">0.5609</td>
</tr>
<tr class="odd">
<td align="right">0.9558</td>
<td align="right">0.8678</td>
<td align="right">0.4328</td>
<td align="right">0.8814</td>
<td align="right">0.5153</td>
</tr>
</tbody>
</table>

<p>Using the dataset above, let us convert to a zero-mean matrix (in other words, let us normalize our data):</p>

<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" data-line-number="1">dmatrix =<span class="st">  </span><span class="kw">as.matrix</span>(dataset)</a>
<a class="sourceLine" id="cb291-2" data-line-number="2">dmatrix =<span class="st"> </span><span class="kw">sweep</span>(dmatrix, <span class="dv">2</span>, <span class="kw">colMeans</span>(dmatrix))</a>
<a class="sourceLine" id="cb291-3" data-line-number="3">knitr<span class="op">::</span><span class="kw">kable</span>( <span class="kw">head</span>(dmatrix, <span class="dv">20</span>), <span class="dt">caption =</span> <span class="st">&#39;Zero-mean Matrix&#39;</span>,  </a>
<a class="sourceLine" id="cb291-4" data-line-number="4">              <span class="dt">booktabs =</span> <span class="ot">TRUE</span> )</a></code></pre></div>
<table>
<caption><span id="tab:uniform5">Table 6.2: </span>Zero-mean Matrix</caption>
<thead>
<tr class="header">
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0452</td>
<td align="right">-0.1851</td>
<td align="right">-0.1851</td>
<td align="right">-0.2911</td>
<td align="right">0.1505</td>
</tr>
<tr class="even">
<td align="right">-0.1510</td>
<td align="right">0.0640</td>
<td align="right">0.2177</td>
<td align="right">0.1331</td>
<td align="right">-0.0524</td>
</tr>
<tr class="odd">
<td align="right">0.1058</td>
<td align="right">0.1211</td>
<td align="right">-0.0327</td>
<td align="right">0.1580</td>
<td align="right">-0.0980</td>
</tr>
</tbody>
</table>

<p>To know if our matrix is normalized into zero-mean, we convert the matrix to zero-mean repeatedly. In doing so, we get the same set of values over and over because the mean of the matrix has settled to zero; hence zero-mean.</p>

<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" data-line-number="1">dmatrix =<span class="st">  </span><span class="kw">as.matrix</span>(dataset)</a>
<a class="sourceLine" id="cb292-2" data-line-number="2">dmatrix =<span class="st"> </span><span class="kw">sweep</span>(dmatrix, <span class="dv">2</span>, <span class="kw">colMeans</span>(dmatrix))</a>
<a class="sourceLine" id="cb292-3" data-line-number="3">dmatrix =<span class="st"> </span><span class="kw">sweep</span>(dmatrix, <span class="dv">2</span>, <span class="kw">colMeans</span>(dmatrix))</a>
<a class="sourceLine" id="cb292-4" data-line-number="4">dmatrix =<span class="st"> </span><span class="kw">sweep</span>(dmatrix, <span class="dv">2</span>, <span class="kw">colMeans</span>(dmatrix))</a>
<a class="sourceLine" id="cb292-5" data-line-number="5">knitr<span class="op">::</span><span class="kw">kable</span>( <span class="kw">head</span>(dmatrix, <span class="dv">20</span>), </a>
<a class="sourceLine" id="cb292-6" data-line-number="6">              <span class="dt">caption =</span> <span class="st">&quot;Zero-mean Matrix (Repeated 3 times)&quot;</span>, </a>
<a class="sourceLine" id="cb292-7" data-line-number="7">              <span class="dt">booktabs =</span> <span class="ot">TRUE</span> )</a></code></pre></div>
<table>
<caption><span id="tab:uniform7">Table 6.3: </span>Zero-mean Matrix (Repeated 3 times)</caption>
<thead>
<tr class="header">
<th align="right">X1</th>
<th align="right">X2</th>
<th align="right">X3</th>
<th align="right">X4</th>
<th align="right">X5</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0452</td>
<td align="right">-0.1851</td>
<td align="right">-0.1851</td>
<td align="right">-0.2911</td>
<td align="right">0.1505</td>
</tr>
<tr class="even">
<td align="right">-0.1510</td>
<td align="right">0.0640</td>
<td align="right">0.2177</td>
<td align="right">0.1331</td>
<td align="right">-0.0524</td>
</tr>
<tr class="odd">
<td align="right">0.1058</td>
<td align="right">0.1211</td>
<td align="right">-0.0327</td>
<td align="right">0.1580</td>
<td align="right">-0.0980</td>
</tr>
</tbody>
</table>

<p>We use the <strong>colMeans(.)</strong> function to see if the mean in Table <a href="statistics.html#tab:uniform7">6.3</a> has settled down to zero.</p>

<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" data-line-number="1">dmatrix =<span class="st">  </span><span class="kw">as.matrix</span>(dataset)</a>
<a class="sourceLine" id="cb293-2" data-line-number="2">dmatrix =<span class="st"> </span><span class="kw">sweep</span>(dmatrix, <span class="dv">2</span>, <span class="kw">colMeans</span>(dmatrix))</a>
<a class="sourceLine" id="cb293-3" data-line-number="3"><span class="kw">colMeans</span>(dmatrix)</a></code></pre></div>
<pre><code>##         X1         X2         X3         X4         X5 
##  3.701e-17  0.000e+00  1.850e-17 -3.701e-17  0.000e+00</code></pre>

<p>As it turns out, the results rendered mean values that are extremely small, close to -17 digits in precision - in essence, that is equivalent to zero.</p>
<p>There are other ways to compute the means of complex datasets more optimally. One way is matrix manipulation which we cover in Chapter <strong>2</strong> (<strong>Numerical Linear Algebra I</strong>).</p>
</div>
<div id="variability" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.3</span> Variability <a href="statistics.html#variability" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Variability</strong> is the measure of the variance (or spread) of data. The idea is to measure how close data is to the mean. At times, the standard deviation is preferred over variance because of its interpretability. For example, based on Figure <a href="statistics.html#fig:uniform2">6.1</a>, a four-unit of standard deviation indicates that a data point is at the far tail-end of the distribution and could therefore be deemed as an outlier.</p>
</div>
<div id="kurtosis-and-skewness" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.4</span> Kurtosis and Skewness  <a href="statistics.html#kurtosis-and-skewness" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kurtosis</strong> is used to measure the height (peak) of a distribution. Additionally, we also measure the thickness of the tail of a distribution. For example, Figure <a href="statistics.html#fig:k2">6.2</a> shows how the tail of a distribution gets thicker as its height tapers off. In other words, the more a set of observations crowd around the center, the higher the frequency, and the lesser observations are left away from the center.</p>
<p>There are three types of <strong>kurtosis</strong> distribution:   </p>
<ul>
<li><strong>Leptokurtic</strong> distribution - reflects a slender shape distribution</li>
<li><strong>Platykurtic</strong> distribution - reflects a wider spread (broader shape) distribution</li>
<li><strong>Mesokurtic</strong> distribution - reflects a more normal-like distribution</li>
</ul>

<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb295-2" data-line-number="2">n=<span class="dv">3000</span></a>
<a class="sourceLine" id="cb295-3" data-line-number="3">x =<span class="st"> </span><span class="kw">rt</span>(n, <span class="dt">df=</span><span class="dv">25</span>)</a>
<a class="sourceLine" id="cb295-4" data-line-number="4"><span class="kw">hist</span>(x, <span class="dt">xlim =</span> <span class="kw">range</span>(<span class="op">-</span><span class="dv">4</span>,<span class="dv">4</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>, <span class="fl">0.80</span>), </a>
<a class="sourceLine" id="cb295-5" data-line-number="5">     <span class="dt">prob=</span><span class="ot">TRUE</span>, <span class="dt">breaks=</span><span class="dv">20</span>, <span class="dt">main=</span><span class="st">&#39;t-Distribution&#39;</span>, </a>
<a class="sourceLine" id="cb295-6" data-line-number="6">     <span class="dt">xlab=</span><span class="st">&#39;Standard Deviation&#39;</span>)</a>
<a class="sourceLine" id="cb295-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="fl">0.60</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb295-8" data-line-number="8"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb295-9" data-line-number="9"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">2</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb295-10" data-line-number="10"><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span>.<span class="dv">02</span>, </a>
<a class="sourceLine" id="cb295-11" data-line-number="11">   <span class="kw">c</span>(<span class="st">&quot;Leptokurtic&quot;</span>,<span class="st">&quot;MesoKurtic&quot;</span>,<span class="st">&quot;Platykurtic&quot;</span>), </a>
<a class="sourceLine" id="cb295-12" data-line-number="12">   <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>), </a>
<a class="sourceLine" id="cb295-13" data-line-number="13">   <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:k2"></span>
<img src="DS_files/figure-html/k2-1.png" alt="t-Distribution" width="70%" />
<p class="caption">
Figure 6.2: t-Distribution
</p>
</div>

<p><strong>Skewness</strong> measures the distortion made against a symmetric distribution, e.g., a bell-shaped normal distribution. In other words, this measures the extent to which a normal distribution becomes asymmetric. To demonstrate, we simulate Skewness by using chi-square distribution. Figure <a href="statistics.html#fig:k3">6.3</a> does show an interesting distribution with dramatic Skewness to the left side. We manipulated the shape of the distribution by using degrees of freedom to show Skewness.</p>

<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb296-2" data-line-number="2">n=<span class="dv">1000</span></a>
<a class="sourceLine" id="cb296-3" data-line-number="3">x =<span class="st"> </span><span class="kw">rchisq</span>(n,<span class="dt">df=</span><span class="dv">5</span>, <span class="dt">ncp=</span><span class="dv">0</span>)</a>
<a class="sourceLine" id="cb296-4" data-line-number="4"><span class="kw">hist</span>(x,  <span class="dt">prob=</span><span class="ot">TRUE</span>, <span class="dt">breaks=</span><span class="dv">20</span>, <span class="dt">main=</span><span class="st">&#39;Chi-Square Distribution&#39;</span>, </a>
<a class="sourceLine" id="cb296-5" data-line-number="5">     <span class="dt">xlab=</span><span class="st">&#39;Chi-Square&#39;</span>)</a>
<a class="sourceLine" id="cb296-6" data-line-number="6"><span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">1</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>)</a>
<a class="sourceLine" id="cb296-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">5</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb296-8" data-line-number="8"><span class="kw">curve</span>(<span class="kw">dchisq</span>(x, <span class="dt">df=</span><span class="dv">10</span>),<span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb296-9" data-line-number="9"><span class="kw">legend</span>(<span class="st">&quot;topright&quot;</span>, <span class="dt">inset=</span>.<span class="dv">02</span>,  <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,</a>
<a class="sourceLine" id="cb296-10" data-line-number="10">   <span class="kw">c</span>(<span class="st">&quot;df=1&quot;</span>,<span class="st">&quot;df=5&quot;</span>,<span class="st">&quot;df=10&quot;</span>), </a>
<a class="sourceLine" id="cb296-11" data-line-number="11">   <span class="dt">fill=</span><span class="kw">c</span>(<span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:k3"></span>
<img src="DS_files/figure-html/k3-1.png" alt="Chi-Square Distribution" width="70%" />
<p class="caption">
Figure 6.3: Chi-Square Distribution
</p>
</div>

<p>We have seen just a few - the common ones - of the many kinds of distribution of data. The shape of the distribution leaves clues that help us decide how to proceed with our analysis. It helps to understand the nature of the data we deal with as part of our exploratory data analysis.</p>
</div>
<div id="five-number-summary" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.1.5</span> Five Number Summary  <a href="statistics.html#five-number-summary" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Five Number Summary</strong> refers to the 5 quartiles of a continuous distribution (see Figure <a href="statistics.html#fig:boxplot">6.4</a>):</p>
<ul>
<li><strong>Maximum</strong> - represents the upper whisker in a box plot.</li>
<li><strong>Third Quartile</strong> - represents the upper side of the box in a box plot.</li>
<li><strong>Median</strong> - represents the middle thicker line in a box plot.</li>
<li><strong>First Quartile</strong> - represents the lower side of the box in a box plot.</li>
<li><strong>Minimum</strong> - represents the lower whisker in a box plot.</li>
</ul>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:boxplot"></span>
<img src="boxplot.png" alt="Measuring" width="70%" />
<p class="caption">
Figure 6.4: Measuring
</p>
</div>
<p>The vertical length of the box is called the inter-quartile range (IQR). The minimum and maximum distances from the upper or lower side of the box are computed using: 1.5 * IQR. The other distance for 1.5 * IQR starting from the minimum or maximum whiskers, going outwards, is where outliers lie.</p>
<p>To explain further, Figure <a href="statistics.html#fig:whiskerplot1">6.5</a> illustrates three boxplots, each from one categorical group. The dataset contains three levels (0, 1, 2). The boxplot of each level shows the quartile distribution.</p>

<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb297-2" data-line-number="2">x1 =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">500</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb297-3" data-line-number="3">x2 =<span class="st">  </span><span class="kw">as.factor</span>(<span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">500</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">prob=</span><span class="fl">0.5</span>)  )</a>
<a class="sourceLine" id="cb297-4" data-line-number="4"><span class="kw">boxplot</span>(x1 <span class="op">~</span><span class="st"> </span>x2, <span class="dt">pch=</span><span class="dv">16</span>,  <span class="dt">outcol=</span><span class="st">&quot;deepskyblue&quot;</span>, </a>
<a class="sourceLine" id="cb297-5" data-line-number="5">        <span class="dt">main=</span><span class="st">&quot;Box Plot (Whisker Plot)&quot;</span>, </a>
<a class="sourceLine" id="cb297-6" data-line-number="6">       <span class="dt">xlab=</span><span class="st">&quot;X1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;X2&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:whiskerplot1"></span>
<img src="DS_files/figure-html/whiskerplot1-1.png" alt="Five number Summary" width="70%" />
<p class="caption">
Figure 6.5: Five number Summary
</p>
</div>

<p>Level 1 of the dataset shows three outliers. Level 2 shows only one outlier.</p>
<p>The median, first quartile, and third quartiles of the three levels are all almost aligned. Similarly, the lower (minimum) whiskers are also aligned. Only the upper whiskers are a bit apart. Here, one can see the distribution of categories to be close.</p>
<p>Figure <a href="statistics.html#fig:whiskerplot2">6.6</a> shows another set of boxplots. The boxplots show that Level 1 and Level 2 have values concentrated in the range between 30 and 40 and have a median within the same value range, while Level 0 concentrates around 10-30 in the boxplot with a median within the 10-20 value range.</p>

<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb298-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb298-2" data-line-number="2">x1 =<span class="st"> </span><span class="kw">c</span>( <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">150</span>, <span class="dt">min=</span><span class="dv">1</span>, <span class="dt">max=</span><span class="dv">20</span>), </a>
<a class="sourceLine" id="cb298-3" data-line-number="3">        <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">150</span>, <span class="dt">min=</span><span class="dv">25</span>, <span class="dt">max=</span><span class="dv">35</span>), </a>
<a class="sourceLine" id="cb298-4" data-line-number="4">        <span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">200</span>, <span class="dt">min=</span><span class="dv">30</span>, <span class="dt">max=</span><span class="dv">50</span>) )</a>
<a class="sourceLine" id="cb298-5" data-line-number="5">x2 =<span class="st"> </span><span class="kw">as.factor</span>(<span class="kw">c</span>(<span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">150</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">prob=</span><span class="fl">0.1</span>), </a>
<a class="sourceLine" id="cb298-6" data-line-number="6">                 <span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">150</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">prob=</span><span class="fl">0.45</span>), </a>
<a class="sourceLine" id="cb298-7" data-line-number="7">                 <span class="kw">rbinom</span>(<span class="dt">n=</span><span class="dv">200</span>, <span class="dt">size=</span><span class="dv">2</span>, <span class="dt">prob=</span><span class="fl">0.45</span>) ) )</a>
<a class="sourceLine" id="cb298-8" data-line-number="8"><span class="kw">boxplot</span>(x1 <span class="op">~</span><span class="st"> </span>x2, <span class="dt">pch=</span><span class="dv">16</span>,  <span class="dt">outcol=</span><span class="st">&quot;deepskyblue&quot;</span>, </a>
<a class="sourceLine" id="cb298-9" data-line-number="9">        <span class="dt">main=</span><span class="st">&quot;Box Plot (Whisker Plot)&quot;</span>, </a>
<a class="sourceLine" id="cb298-10" data-line-number="10">        <span class="dt">xlab=</span><span class="st">&quot;X1&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;X2&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:whiskerplot2"></span>
<img src="DS_files/figure-html/whiskerplot2-1.png" alt="Five number Summary" width="70%" />
<p class="caption">
Figure 6.6: Five number Summary
</p>
</div>

<p>How do we then interpret the three boxplots? One thing to point out is that the random variable X2 has a broader number of observations with values between 10 and 30 correlated with the category 0 (or level 0) of random variable X1.</p>
<p>It also emphasizes that as we deal with more random variables, we begin to understand how it can get challenging to plot every combination of every random variable. The so-called âCurse of Dimensionalityâ characterizes an increase of complexity for every random variable that gets included for analysis. Luckily, we have methods available to help us identify whether a random variable is significant (or relevant) or is merely a duplicate that can be excluded from the analysis.</p>
</div>
</div>
<div id="inferential-statistics" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.2</span> Inferential Statistics<a href="statistics.html#inferential-statistics" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Inferential Statistics</strong> is <strong>Statistics</strong> focusing on the interpretation of sampled data with the intent to generalize its representation. For example, we formulate <strong>hypothetical</strong> claims about the nature of data distribution using <strong>Descriptive Statistics</strong> by predicting its <strong>Mean</strong> or <strong>Variance</strong>. </p>
<p>Let us first review the definition of a few terminologies in this section:</p>
<p><strong>Population</strong> refers to a complete set of data.</p>
<p><strong>Sample</strong> refers to a subset of data (or observation) taken randomly from a population.</p>

<div class="sourceCode" id="cb299"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb299-1" data-line-number="1"><span class="co"># Here is a population of ten male and female </span></a>
<a class="sourceLine" id="cb299-2" data-line-number="2"><span class="co"># marsupials from which we sample only 5</span></a>
<a class="sourceLine" id="cb299-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">1228</span>) <span class="co"># set initial seed </span></a>
<a class="sourceLine" id="cb299-4" data-line-number="4">population =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;M&quot;</span>, <span class="st">&quot;F&quot;</span>)</a>
<a class="sourceLine" id="cb299-5" data-line-number="5"><span class="kw">sample</span>(population, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>## [1] &quot;F&quot; &quot;M&quot; &quot;F&quot; &quot;F&quot; &quot;M&quot;</code></pre>

<p><strong>Group</strong> refers to discrete categorical grouping of samples.</p>

<div class="sourceCode" id="cb301"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb301-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1228</span>) <span class="co"># set initial seed </span></a>
<a class="sourceLine" id="cb301-2" data-line-number="2">treatments =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>, <span class="st">&quot;B&quot;</span>, <span class="st">&quot;C&quot;</span>)</a>
<a class="sourceLine" id="cb301-3" data-line-number="3">population_size =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb301-4" data-line-number="4">sample_size =<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb301-5" data-line-number="5"><span class="co"># simulating a population</span></a>
<a class="sourceLine" id="cb301-6" data-line-number="6">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x=</span>treatments, <span class="dt">size =</span> population_size, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb301-7" data-line-number="7">sample =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x=</span>population, <span class="dt">size =</span> sample_size)</a>
<a class="sourceLine" id="cb301-8" data-line-number="8">treatmentA =<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(sample <span class="op">==</span><span class="st"> &quot;A&quot;</span>))</a>
<a class="sourceLine" id="cb301-9" data-line-number="9">treatmentB =<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(sample <span class="op">==</span><span class="st"> &quot;B&quot;</span>))</a>
<a class="sourceLine" id="cb301-10" data-line-number="10">treatmentC =<span class="st"> </span><span class="kw">length</span>(<span class="kw">which</span>(sample <span class="op">==</span><span class="st"> &quot;C&quot;</span>))</a>
<a class="sourceLine" id="cb301-11" data-line-number="11"><span class="kw">c</span>(<span class="st">&quot;A&quot;</span>=treatmentA, <span class="st">&quot;B&quot;</span>=treatmentB, <span class="st">&quot;C&quot;</span>=treatmentC)</a></code></pre></div>
<pre><code>##  A  B  C 
## 11  9 10</code></pre>

<p><strong>Statistical data</strong> refers to data with observable and measurable quality that typically can be quantitatively analyzed based on statistics.</p>
<p>In this section, we take a population sample and perform inferences. There are many ways to infer the data represented in a sample. Our approach covers reviewing the distribution of the sample, analyzing the significance of the differences of samples, analyzing the significance of regression of samples, and possibly dealing with exploratory analysis (which is covered in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).</p>
<p><strong>Sufficient Statistic</strong> refers to a statistic such as a sample mean <span class="math inline">\(\bar{x}\)</span> from a sample distribution that contains enough information to represent a specific model parameter such as the population mean <span class="math inline">\(\mu\)</span> from a population distribution. Formally, the statistic <span class="math inline">\(\bar{x}\)</span> is sufficient if it can be independent of the model parameter <span class="math inline">\(\mu\)</span>. Note that the notation of a sufficient statistic is <span class="math inline">\(t = T(X)\)</span>, and the model parameter is denoted as <span class="math inline">\(\theta\)</span> where T(X) is an estimating function.</p>
<p>In the next several sections under this Chapter, our main focus is <strong>Inferential Statistics</strong>.</p>
</div>
<div id="the-significance-of-difference" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.3</span> The Significance of Difference <a href="statistics.html#the-significance-of-difference" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In this section, let us <strong>evaluate data</strong> that is <strong>sampled from a population</strong>. In some cases, it is virtually impractical to derive parameters such as averages, variances, and standard deviation from a large population. Therefore, in this circumstance, our approach is to sample the population instead and evaluate if the sampled data is a good representation of the entire population data. The phrase <strong>good representation</strong> can be quantified based on <strong>Statistical Significance</strong>. First, we compare the average (the mean) of the sample data against the average (the mean) of the population data. Then conclude to see if there is any significant difference between the two data sets in terms of a chosen set of statistics such as the mean, variance, and others.</p>
<p>It is important to note that some literature tends to explain <strong>Statistical Significance</strong> in the context of a legal setting in which a defendant is assumed to be innocent until sufficient evidence is gathered to prove the defendant guilty beyond a reasonable doubt. Here, we show how that works using a simple example. We base our discussion on additional references, namely Sternstein M. <span class="citation">(<a href="bibliography.html#ref-ref1338s">1996</a>)</span>, Parkhurst D. F. <span class="citation">(<a href="bibliography.html#ref-ref764a">2006</a>)</span>, De Groot A. D. <span class="citation">(<a href="bibliography.html#ref-ref764a">2006</a>)</span>, Lempert R.O <span class="citation">(<a href="bibliography.html#ref-ref748r">2008</a>)</span>, Sabri F. and Gyateng T. <span class="citation">(<a href="bibliography.html#ref-ref756f">2015</a>)</span>.</p>
<div id="hypothesis" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.1</span> Hypothesis<a href="statistics.html#hypothesis" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we review the concept of <strong>Hypothesis</strong>. The term <strong>Hypothesis</strong> is typically associated with other familiar terms such as <strong>supposition</strong>, <strong>proposition</strong>, <strong>assumption</strong>, and <strong>claim</strong>. Here, we declare a statement based on observed data and make a <strong>claim</strong>. For example, we can derive a hypothetical estimate if we cannot possibly know the exact average IQ level of a large population. Suppose we are given a <strong>hypothetical IQ level average</strong> of a large population to be 100. Note that the average IQ level of 100 is just hypothetical, and thus the population mean is unknown. Nonetheless, let us use that to formulate our <strong>Hypothesis</strong>.</p>
<p>There are two types of hypotheses (both need to be stated in our test):</p>
<ul>
<li><strong>Null Hypothesis</strong> - this hypothesis is denoted by <span class="math inline">\(\mathbf{H_0}\)</span>. In this <strong>Hypothesis</strong>, we claim that there is <strong>no significant difference</strong> between the sample and the population ( or between samples from the same population) based on a statistic. For example, we can make a <strong>claim</strong> that the average IQ level of our sample, <span class="math inline">\(\mu\)</span>, is equal to the given <strong>hypothetical IQ level</strong>, <span class="math inline">\(\mu_0 = 100\)</span>. Meaning we claim that there is no difference. </li>
</ul>
<p><span class="math display">\[\begin{align}
\mathbf{H_0: \mu = \mu_0}\ \ \ \ \ \ \ \ \ \ where\ \mu_0 = 100
\end{align}\]</span></p>
<ul>
<li><strong>Alternative Hypothesis</strong> - this hypothesis is denoted by <span class="math inline">\(\mathbf{H_1}\)</span>. In this <strong>hypothesis</strong>, using statistics, we claim a <strong>significant difference</strong> between a sample and a population ( or between samples from the same population). For example, as an alternative hypothesis, we may believe instead that the average IQ level of our sample is greater than the given <strong>hypothetical IQ level</strong> of 100. That means we claim that there is a difference. </li>
</ul>
<p><span class="math display">\[\begin{align}
\mathbf{H_1: \mu \ne \mu_0}\ \ \ \ \ \ \ \ \ \ where\ \mu_0 = 100
\end{align}\]</span></p>
<p>Both null and alternative hypotheses are mutually exclusive, meaning that if one hypothesis is <strong>valid</strong>, then the other must be <strong>invalid</strong>.</p>
<p>Note that <strong>alternative hypothesis</strong> can either be directional or non-directional. For example, if we claim that observation group A is greater than or lesser than observation group B, then this is a <strong>directional alternative hypothesis</strong>. On the other hand, if we compare group A and group B only to know if there is a difference between the two regardless of whether one is better or worse or greater than or lesser than the other, then this is a <strong>non-directional alternative hypothesis</strong>. In the case of our IQ level case above, our <strong>alternative hypothesis</strong> is directional because we claim that our sample mean is greater than the population mean in terms of IQ level.</p>
<p>After formulating our null and alternative hypotheses, the next step is to disprove one of the claims, effectively proving the other.</p>
<p>Our strategy is to <strong>test our claim</strong> on the alternative hypothesis, <span class="math inline">\(\mathbf{H_1}\)</span>, in order to decide on what to do with the null hypothesis based on two outcomes:</p>
<ul>
<li><p>An outcome that <strong>rejects the claim</strong> made on the null hypothesis. It suggests that the sampled data, otherwise our approach, may need more investigation. In this case, the <strong>Alternative Hypothesis</strong> holds.</p></li>
<li><p>An outcome that <strong>fails to reject the claim</strong> made on the null hypothesis. It suggests that the sampled data is a good representation of the population data. Here, if we <strong>fail to reject the claim</strong>, then the <strong>null Hypothesis</strong> holds.</p></li>
</ul>
<p>Also, it helps to be familiar with two types of errors in testing for the <strong>hypothesis</strong>:</p>
<ul>
<li><p><strong>Type I error</strong> - We get this error if our test ends up <strong>rejecting a claim</strong> even though the <strong>null hypothesis</strong> - the claim - is actually <strong>true</strong>. We call this <strong>false positive</strong>. </p></li>
<li><p><strong>Type II error</strong> - We get this error if our test ends up <strong>failing to reject a claim</strong> even though the <strong>null hypothesis</strong> - the claim - is <strong>false</strong>. We call this <strong>false-negative</strong>. </p></li>
</ul>
<p>In the next sections, we discuss methods of testing our hypothetical estimates.</p>
<p>Before we jump to the next sections, however, it may help to introduce three terms that will often describe the relationships of data points:</p>
<ul>
<li><p><strong>Regression</strong> - When we sample a population, we tend to measure our observation based on estimates only. We may not know the exact value of the samples we have observed. Therefore, we can use as many available methods of measurements and perform adjustments if our goal is to get our estimated value as close to the actual value. We rely on <strong>Regression</strong> to see how close our estimated value is to the actual value - if our estimate <strong>regresses to</strong> the actual value. In other words, <strong>Regression</strong> measures the relationship between the dependent and independent variables. Our discussion will primarily focus on coefficients to find any significant effect or the lack thereof.</p></li>
<li><p><strong>Deviance</strong> - In contrast, a population sample with estimates deviating from the true values demonstrates a <strong>deviation</strong>. <strong>Deviation</strong> happens every time our estimate value gets farther away from the true value. Our discussion will also primarily focus on coefficients. </p></li>
<li><p><strong>Variance</strong> - On the other hand, <strong>Variance</strong> is not a measure of whether estimated values regress to or deviate from the true value. Instead, it characterizes the distance between estimates. That means that we are not comparing the estimated and true values. Here, we are comparing two estimate values to see if they are different or not, are related or not. Our discussion will primarily focus on averages to find any significant difference or the lack thereof. </p></li>
</ul>
<p>Note that in the following sections, we will deal with two major types of tests (of estimates):</p>
<ul>
<li>Test to determine the significance of difference - this deals mainly with <strong>Variance</strong>.</li>
<li>Test to determine the significance of Regression - this deals mainly with <strong>Regression</strong> and <strong>Deviation</strong>.</li>
</ul>
<p>Let us now discuss some of the standard tests used in statistics to determine the significance of difference.</p>
</div>
<div id="t-test-true-variance-unknown" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.2</span> T-Test (True Variance unknown) <a href="statistics.html#t-test-true-variance-unknown" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a <strong>T-Test</strong> to test a given hypothesis by computing the <strong>t-value</strong> (or <strong>t-score</strong>) to evaluate our claim <span class="citation">(Ugoni A. and F. Walker B.F. <a href="bibliography.html#ref-ref800a">1995</a>; Abebe T. H. <a href="bibliography.html#ref-ref780t">2020</a>)</span>. This <strong>t-value</strong> is a value of <strong>T statistic</strong> which follows a <strong>Studentâs T-distribution</strong> (W.S. Gossett 1876-1937).</p>
<p>Let us consider three types of samples for our <strong>T-Test</strong>:</p>
<p><strong>One-Sample T-Test</strong>:</p>
<p>A <strong>One-Sample T-Test</strong> uses a sample from a population to determine the significance of the sample mean against a given <strong>known</strong> mean, but with an <strong>unknown</strong> variance. Here, we perform the following test statistic equation against the sample:</p>
<p><span class="math display">\[\begin{align}
t = \frac{observed-expected}{standard\ error}  = \frac{\mu_s - \mu_0} { \sigma_s / \sqrt{n_s}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{t}\)</span> is the test statistic, following a T distribution.</li>
<li><span class="math inline">\(\mathbf{\mu_s}\)</span> is the sample mean - the average (mean) of the sample, <span class="math inline">\(\bar{x}_s\)</span>.</li>
<li><span class="math inline">\(\mathbf{\sigma}_s\)</span> is the sample standard deviation - the standard deviation of the sample.</li>
<li><span class="math inline">\(\mathbf{n_s}\)</span> is the size of the sample data.</li>
<li><span class="math inline">\(\mathbf{\mu_0}\)</span> is a given <strong>hypothesized</strong> population mean. (Note that <span class="math inline">\(\mathbf{\sigma}_p\)</span> - the population standard deviation - is unknown). See <strong>Conjugacy</strong> section in <strong>Bayesian Computation</strong> for <strong>unknown variance</strong>.</li>
</ul>
<p>The sample standard deviation (along with its corresponding sample variance) is expressed as:</p>
<p><span class="math display">\[\begin{align}
\sigma_s = \sqrt{\frac{1}{n-1}\left(\sum_{i=1}^n (x_i - \bar{x})^2\right)}
\ \ \ \ \ \ \ \ \ \ \ where\ \ \sigma_s^2\ \leftarrow\ sample\ variance
\end{align}\]</span></p>
<p>The standard error is expressed as:</p>
<p><span class="math display">\[\begin{align}
SE = \frac{\sigma_{s}}{\sqrt{n}}
\end{align}\]</span></p>
<p>The margin of error is expressed as:</p>
<p><span class="math display">\[\begin{align}
me = t \times SE
\end{align}\]</span></p>
<p>The confidence interval is expressed as:</p>
<p><span class="math display">\[\begin{align}
C.I. = \mu_{s} \pm me
\end{align}\]</span></p>
<p>To illustrate our example of IQ levels once again, suppose we take a sample of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Assume an average IQ level of 100.</p>
<p>Here is what the <strong>t-value</strong> looks like in R-code:</p>

<div class="sourceCode" id="cb303"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb303-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb303-2" data-line-number="2">statistic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb303-3" data-line-number="3">    s =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb303-4" data-line-number="4">    m =<span class="st"> </span><span class="kw">mean</span>(x)</a>
<a class="sourceLine" id="cb303-5" data-line-number="5">    n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb303-6" data-line-number="6">    <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) { s =<span class="st"> </span>s <span class="op">+</span><span class="st"> </span>(x[i] <span class="op">-</span><span class="st"> </span>m)<span class="op">^</span><span class="dv">2</span> }</a>
<a class="sourceLine" id="cb303-7" data-line-number="7">    sd =<span class="st"> </span><span class="kw">sqrt</span>( <span class="dv">1</span><span class="op">/</span><span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">*</span><span class="st"> </span>s)</a>
<a class="sourceLine" id="cb303-8" data-line-number="8">    <span class="kw">list</span>(<span class="st">&quot;mean&quot;</span>=m, <span class="st">&quot;sd&quot;</span>=sd, <span class="st">&quot;sum_squared&quot;</span>=s, <span class="st">&quot;n&quot;</span>=n )</a>
<a class="sourceLine" id="cb303-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb303-10" data-line-number="10">t_test &lt;-<span class="st"> </span><span class="cf">function</span>(x1, <span class="dt">x2 =</span> <span class="ot">NULL</span>, mu, <span class="dt">paired=</span><span class="ot">FALSE</span>) {</a>
<a class="sourceLine" id="cb303-11" data-line-number="11">    <span class="cf">if</span> (<span class="kw">is.null</span>(x2)) {  <span class="co"># one sample t-test</span></a>
<a class="sourceLine" id="cb303-12" data-line-number="12">        stat =<span class="st"> </span><span class="kw">statistic</span>(x1)</a>
<a class="sourceLine" id="cb303-13" data-line-number="13">        t =<span class="st"> </span>(stat<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(stat<span class="op">$</span>sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(stat<span class="op">$</span>n))</a>
<a class="sourceLine" id="cb303-14" data-line-number="14">    } <span class="cf">else</span> </a>
<a class="sourceLine" id="cb303-15" data-line-number="15">    <span class="cf">if</span> (paired <span class="op">==</span><span class="st"> </span><span class="ot">FALSE</span>) { <span class="co"># two sample t-test</span></a>
<a class="sourceLine" id="cb303-16" data-line-number="16">        stat1 =<span class="st"> </span><span class="kw">statistic</span>(x1)</a>
<a class="sourceLine" id="cb303-17" data-line-number="17">        stat2 =<span class="st"> </span><span class="kw">statistic</span>(x2)</a>
<a class="sourceLine" id="cb303-18" data-line-number="18">        pooled_variance =<span class="st"> </span></a>
<a class="sourceLine" id="cb303-19" data-line-number="19"><span class="st">            </span>( stat1<span class="op">$</span>sum_squared <span class="op">+</span><span class="st"> </span>stat2<span class="op">$</span>sum_squared ) <span class="op">/</span><span class="st"> </span></a>
<a class="sourceLine" id="cb303-20" data-line-number="20"><span class="st">            </span>( stat1<span class="op">$</span>n <span class="op">+</span><span class="st"> </span>stat2<span class="op">$</span>n <span class="op">-</span><span class="st"> </span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb303-21" data-line-number="21">        t =<span class="st"> </span>(stat1<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>stat2<span class="op">$</span>mean) <span class="op">/</span></a>
<a class="sourceLine" id="cb303-22" data-line-number="22"><span class="st">            </span>(<span class="kw">sqrt</span>( pooled_variance <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span><span class="op">/</span>stat1<span class="op">$</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>stat2<span class="op">$</span>n)))</a>
<a class="sourceLine" id="cb303-23" data-line-number="23">    } <span class="cf">else</span> { <span class="co"># paired t-test</span></a>
<a class="sourceLine" id="cb303-24" data-line-number="24">        stat =<span class="st"> </span><span class="kw">statistic</span>(x1 <span class="op">-</span><span class="st"> </span>x2) <span class="co"># get the difference</span></a>
<a class="sourceLine" id="cb303-25" data-line-number="25">        t =<span class="st"> </span>(stat<span class="op">$</span>mean <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>(stat<span class="op">$</span>sd <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(stat<span class="op">$</span>n))</a>
<a class="sourceLine" id="cb303-26" data-line-number="26">    }</a>
<a class="sourceLine" id="cb303-27" data-line-number="27">    <span class="kw">list</span>(<span class="st">&quot;statistic&quot;</span>=t)</a>
<a class="sourceLine" id="cb303-28" data-line-number="28">}</a>
<a class="sourceLine" id="cb303-29" data-line-number="29"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb303-30" data-line-number="30">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb303-31" data-line-number="31">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb303-32" data-line-number="32"><span class="co"># we take a sample of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb303-33" data-line-number="33">x =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb303-34" data-line-number="34"><span class="co"># we use a hypothesized average of 100</span></a>
<a class="sourceLine" id="cb303-35" data-line-number="35">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x, <span class="dt">mu=</span><span class="dv">100</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] 3.535</code></pre>

<p>To validate if we get the same result, we use the built-in R function called <strong>ât.test()â</strong> from <strong>stats</strong> package:</p>

<div class="sourceCode" id="cb305"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb305-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(x, <span class="dt">y =</span> <span class="ot">NULL</span>, <span class="dt">mu =</span> <span class="dv">100</span>, </a>
<a class="sourceLine" id="cb305-2" data-line-number="2">                 <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] 3.535</code></pre>

<p><strong>Two-Sample T-Test</strong>:</p>
<p>A <strong>Two-Sample T-Test</strong> uses two samples from a population and performs the following test statistic equation against the two samples:</p>
<p><span class="math display">\[\begin{align}
t = \frac{\bar{x}_1 - \bar{x}_2 } { \sqrt{\sigma_{ss}^2\left(\frac{1}{n_1}+\frac{1}{n_2}\right)}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{t}\)</span> is the test statistic.</li>
<li><span class="math inline">\(\bar{x}_2, \bar{x}_2\)</span> are two sample means</li>
<li><span class="math inline">\(\sigma_{ss}^2\)</span> is the pooled sample variance.</li>
<li><span class="math inline">\(n_1\ and\ n_2\)</span> are the sizes of the two samples respectively.</li>
</ul>
<p>The <strong>pooled sample variance</strong> is a merge of the individual variance computed as: </p>
<p><span class="math display">\[\begin{align}
\sigma_{ss}^2 = \frac{
  \sum_{i=0}^{n_1} (x_i - \bar{x}_1)^2 + \sum_{j=0}^{n_2} (x_j - \bar{x}_2)^2
}{n_1 + n_2 - 2}
\ \ \ \ \ \ \ \ \ \ \ where\ \ \sigma_{ss}^2\ \leftarrow\ pooled\ variance
\end{align}\]</span></p>
<p>To illustrate, suppose we take two separate samples of 25 individuals from a population and take their IQ level. We constrain ourselves with an IQ level ranging between 80 and 140. Here, we do not have a given â<strong>hypothesized</strong>â population mean, <span class="math inline">\(\mathbf{\mu}\)</span>, because we are comparing the mean of two separate samples. Here is what the <strong>t-value</strong> looks like:</p>

<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb307-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb307-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb307-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb307-4" data-line-number="4"><span class="co"># assume a population (though we use the sample)</span></a>
<a class="sourceLine" id="cb307-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb307-6" data-line-number="6"><span class="co"># we take two samples of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb307-7" data-line-number="7">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb307-8" data-line-number="8">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb307-9" data-line-number="9">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x1, <span class="dt">x2=</span>x2)<span class="op">$</span>statistic)</a></code></pre></div>
<pre><code>## [1] 0.441</code></pre>

<p>Let us use <strong>ât.test()â</strong> function to validate:</p>

<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb309-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, </a>
<a class="sourceLine" id="cb309-2" data-line-number="2">                 <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] 0.441</code></pre>

<p><strong>Paired T-Test</strong>:</p>
<p>A <strong>Paired T-Test</strong> uses two samples from a population. Here, the two samples are taken from the same population. However, instead of performing a <strong>Two-Sample T-Test</strong>, we perform a <strong>One-Sample T-Test</strong>. To do that, we generate new sample data using the difference between the two samples.</p>
<p>For example, let us generate two samples of 25 individuals from a population and get the difference. Assume an average IQ level of 100.</p>

<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb311-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb311-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb311-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb311-4" data-line-number="4"><span class="co"># hypothesized population</span></a>
<a class="sourceLine" id="cb311-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb311-6" data-line-number="6"><span class="co"># we take two samples of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb311-7" data-line-number="7">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb311-8" data-line-number="8">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb311-9" data-line-number="9"><span class="co"># we use a hypothesized average of 100</span></a>
<a class="sourceLine" id="cb311-10" data-line-number="10">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>x1, <span class="dt">x2=</span>x2, <span class="dt">mu=</span><span class="dv">100</span>, <span class="dt">paired=</span><span class="ot">TRUE</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>That is equivalent to:</p>

<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb313-1" data-line-number="1">(<span class="dt">tvalue =</span> <span class="kw">t_test</span>(<span class="dt">x1=</span>(x1 <span class="op">-</span><span class="st"> </span>x2), <span class="dt">mu=</span><span class="dv">100</span>, <span class="dt">paired=</span><span class="ot">FALSE</span>)<span class="op">$</span>statistic )</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>and using <strong>ât.test()â</strong> to validate, we get:</p>

<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb315-1" data-line-number="1">(<span class="dt">tvalue =</span> stats<span class="op">::</span><span class="kw">t.test</span>(<span class="dt">x =</span> x1,<span class="dt">y =</span> x2, <span class="dt">mu=</span><span class="dv">100</span>,  </a>
<a class="sourceLine" id="cb315-2" data-line-number="2">                        <span class="dt">paired=</span><span class="ot">TRUE</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]])</a></code></pre></div>
<pre><code>## [1] -17.91</code></pre>

<p>So far, all we did in running the three types of T-tests is to get the <strong>t-value</strong>. The next step is to use the <strong>t-value</strong> to evaluate our <strong>Hypothesis</strong>. </p>
<p>However, let us first take a careful look at Figure <a href="statistics.html#fig:twotailtest">6.7</a> to visualize a two-tail test and Figure <a href="statistics.html#fig:onetailtest">6.8</a> to visualize a one-tail test.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:twotailtest"></span>
<img src="DS_files/figure-html/twotailtest-1.png" alt="Critical Value vs Significance Level - Two-Tail Test" width="70%" />
<p class="caption">
Figure 6.7: Critical Value vs Significance Level - Two-Tail Test
</p>
</div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:onetailtest"></span>
<img src="DS_files/figure-html/onetailtest-1.png" alt="Critical Value vs Significance Level - One-Tail Test" width="70%" />
<p class="caption">
Figure 6.8: Critical Value vs Significance Level - One-Tail Test
</p>
</div>

<p>In the figures, we introduce five concepts.</p>
<p><strong>First</strong>, we introduce the concept of <strong>Confidence level</strong>. There are three commonly used levels of confidence: 99%, 95%, and 90%. For example, a confidence level of 90% means that we are 90% confident that repeated sampling of a population renders the same outcome. </p>
<p><strong>Second</strong>, we introduce the concept of <strong>Significance level</strong> which may complement <strong>confidence level</strong> only in so far as, for example, if the <strong>confidence level</strong> is 99%, then the <strong>significance level</strong> is 1% - or if the confidence level is 90%, then the significance level is 10%. In other words, there are also three commonly used levels of significance: 1%, 5%, 10%. <strong>Significance level</strong> is also denoted as the <strong>alpha</strong> - <span class="math inline">\(\mathbf{\alpha}\)</span> - which corresponds to 0.01, 0.05, 0.10. </p>
<p>For example, an alpha value - or significant level - of 0.10 means a 10% probability that the observed result is at least as extreme as the computed test statistic - e.g., the <strong>t-value</strong> - when the <strong>null hypothesis</strong> is true.</p>
<p>Except just perhaps being a common practice, there is no rule to prevent us from using our own <strong>confidence level</strong> and <strong>significance level</strong> depending on our domain or area of expertise. We may, however, prefer a more stringent level. However, for illustration, let us continue to keep those three levels in our discussions.</p>
<p><strong>Third</strong>, we introduce the concept of a <strong>Critical value</strong>. A <strong>Critical value</strong> can be computed based on the <strong>Significance level</strong>. For example, a significance level of 0.01, 0.05, or 0.10 has the following corresponding computed critical value for a one-tail test: </p>

<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb317-1" data-line-number="1">alpha=<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb317-2" data-line-number="2"><span class="co"># compute for quartile</span></a>
<a class="sourceLine" id="cb317-3" data-line-number="3">lt =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(alpha<span class="op">/</span><span class="dv">2</span>, <span class="dt">df=</span><span class="ot">Inf</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb317-4" data-line-number="4"><span class="co"># reverse alpha then compute for quartile</span></a>
<a class="sourceLine" id="cb317-5" data-line-number="5">ut =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">rev</span>(alpha)<span class="op">/</span><span class="dv">2</span>, <span class="dt">df=</span><span class="ot">Inf</span>),<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb317-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;lower_tail&quot;</span>=<span class="st"> </span>lt, <span class="st">&quot;upper_tail&quot;</span>=ut)</a></code></pre></div>
<pre><code>## $lower_tail
## [1] -2.58 -1.96 -1.64
## 
## $upper_tail
## [1] 1.64 1.96 2.58</code></pre>

<p>A significance level of 0.01, 0.05, and 0.10 respectively for a two-tail test have the following computed critical values:</p>

<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb319-1" data-line-number="1">alpha=<span class="kw">c</span>(<span class="fl">0.01</span>, <span class="fl">0.05</span>, <span class="fl">0.10</span>)</a>
<a class="sourceLine" id="cb319-2" data-line-number="2"><span class="co"># compute for quartile</span></a>
<a class="sourceLine" id="cb319-3" data-line-number="3">lt =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(alpha, <span class="dt">df=</span><span class="ot">Inf</span>), <span class="dv">2</span>) </a>
<a class="sourceLine" id="cb319-4" data-line-number="4"><span class="co"># reverse alpha then compute for quartile</span></a>
<a class="sourceLine" id="cb319-5" data-line-number="5">ut =<span class="st"> </span><span class="kw">round</span>( <span class="kw">qt</span>(<span class="dv">1</span><span class="op">-</span><span class="kw">rev</span>(alpha), <span class="dt">df=</span><span class="ot">Inf</span>),<span class="dv">2</span>) </a>
<a class="sourceLine" id="cb319-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;lower_tail&quot;</span>=<span class="st"> </span>lt, <span class="st">&quot;upper_tail&quot;</span>=ut)</a></code></pre></div>
<pre><code>## $lower_tail
## [1] -2.33 -1.64 -1.28
## 
## $upper_tail
## [1] 1.28 1.64 2.33</code></pre>

<p><strong>Fourth</strong>, we introduce the concept of <strong>p-value</strong>. Let us recall the discussion around <strong>CDF</strong> - cumulative density function. Here, the <strong>p-value</strong> uses the <strong>CDF</strong> of a <strong>T-distribution</strong>. </p>
<p>As an example, let us compute for the <strong>CDF</strong> using the <span class="math inline">\(\mathbf{H_1}\)</span> - our <strong>alternative hypothesis</strong>:</p>
<p><span class="math display">\[
P(\mu &gt; 100 | \mu_0 = 100)
\]</span></p>
<p>We interpret that as the probability that the sample mean is greater than 100, given a true value (true mean) equal to 100.</p>
<p>Let us use our T-distribution <strong>CDF</strong> function <strong>t_cdf(.)</strong> to compute for the <strong>p-value</strong> using the generated <strong>t-value</strong> (note that we sampled 25 individuals, n=25):</p>

<div class="sourceCode" id="cb321"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb321-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb321-2" data-line-number="2"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb321-3" data-line-number="3">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb321-4" data-line-number="4">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">5000</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb321-5" data-line-number="5"><span class="co"># we take a sample of 25 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb321-6" data-line-number="6">x =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">25</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb321-7" data-line-number="7">tvalue =<span class="st"> </span><span class="kw">t_test</span>(<span class="dt">x1=</span>x, <span class="dt">mu=</span><span class="dv">100</span>)<span class="op">$</span>statistic </a>
<a class="sourceLine" id="cb321-8" data-line-number="8">cdf =<span class="st"> </span><span class="kw">t_cdf</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>)</a>
<a class="sourceLine" id="cb321-9" data-line-number="9">pvalue_left =<span class="st"> </span>cdf</a>
<a class="sourceLine" id="cb321-10" data-line-number="10">pvalue_right =<span class="st">  </span>pvalue =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>cdf</a>
<a class="sourceLine" id="cb321-11" data-line-number="11">tvalue  <span class="co"># T value</span></a></code></pre></div>
<pre><code>## [1] 4.519</code></pre>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb323-1" data-line-number="1">pvalue_left <span class="co"># P value (area) towards left of T value</span></a></code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
<div class="sourceCode" id="cb325"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb325-1" data-line-number="1">pvalue_right <span class="co"># P value (area) towards right of T value</span></a></code></pre></div>
<pre><code>## [1] 0.00007054</code></pre>

<p>To validate, let us use the built-in R functions <strong>t.test()</strong> and <strong>pt()</strong> to compute for the <strong>p-value</strong>:</p>

<div class="sourceCode" id="cb327"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb327-1" data-line-number="1">tvalue =<span class="st"> </span><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>)<span class="op">$</span>statistic[[<span class="st">&quot;t&quot;</span>]]</a>
<a class="sourceLine" id="cb327-2" data-line-number="2">pvalue_left =<span class="st"> </span><span class="kw">pt</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb327-3" data-line-number="3">pvalue_right =<span class="st"> </span><span class="kw">pt</span>(tvalue, <span class="dt">df=</span><span class="dv">25-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb327-4" data-line-number="4">tvalue  <span class="co"># T value</span></a></code></pre></div>
<pre><code>## [1] 4.519</code></pre>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb329-1" data-line-number="1">pvalue_left <span class="co"># P value (area) towards left of T value</span></a></code></pre></div>
<pre><code>## [1] 0.9999</code></pre>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb331-1" data-line-number="1">pvalue_right <span class="co"># P value (area) towards right of T value</span></a></code></pre></div>
<pre><code>## [1] 0.00007054</code></pre>

<p>As we can see from the output, we have both <strong>t-value</strong> and <strong>p-value</strong>.</p>
<p><strong>Fifth and last</strong>, we re-introduce the directional and non-directional alternative hypothesis, which we evaluate based on whether we are performing a two-tail or one-tail test. A one-tail test corresponds to evaluating a one-directional hypothesis that tends to compare whether an observation is better or worse, greater or lesser than another observation. A two-tail test evaluates a non-directional alternative that tends to determine if there is a difference between the two observations regardless of direction.</p>
<p>With all those five introductory concepts and to now evaluate our <strong>null hypothesis</strong>, we can use two methods:</p>
<ul>
<li>Use a <strong>t-value</strong> based on a chosen <strong>critical value</strong> as our comfortable threshold (establishing our rejection region).
<ul>
<li>For a two-tail test, a common choice for <strong>critical value</strong> is based on: <span class="math inline">\(\pm 2.58, \pm 1.96, \pm 1.65\)</span>.<br />
</li>
<li>For a one-tail test, we use any of the following: <span class="math inline">\(\pm 2.33, \pm 1.64, \pm 1.28\)</span>.</li>
</ul></li>
<li>Use a <strong>p-value</strong> based on a chosen <strong>significance level</strong> as our comfortable threshold (establishing the area - the alpha - under the curve as our rejection region).
<ul>
<li>For a two-tail test, we can choose any of the given <strong>significance level</strong>: <span class="math inline">\(0.005, 0.025, 0.05\)</span>.<br />
</li>
<li>For a one-tail test, we use any of the following: <span class="math inline">\(0.01, 0.05, 0.10\)</span>.</li>
</ul></li>
</ul>
<p>To illustrate, let us continue to use our <strong>IQ level</strong> samples. Suppose that the average IQ level is 100. Now, let us make a claim, <span class="math inline">\(\mathbf{H_1}\)</span>, that our sample mean is greater than the average IQ level, <span class="math inline">\(\mu\)</span>. Here is what it looks like:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \mu= 100,\ \ \ \ \ \leftarrow \ \ \ \text{null hypothesis}\\
H_1 &amp;: \mu &gt; 100
\end{align*}\]</span></p>
<p>Our computed <strong>t-value</strong> is 4.5191 and our computed <strong>p-value</strong> is 7.054310^{-5}.</p>
<p>If our confidence level is 99% and we are performing a one-tail test, and our alternative hypothesis tends toward the right because we are claiming an IQ level greater than 100, then we have the following:</p>
<p>4.5191 &lt; 2.58</p>
<p>This means that our <strong>t-value</strong> is not in the extreme right-side rejection region (See Figure <a href="statistics.html#fig:onetailtest">6.8</a>). The extreme right-side rejection region is the region in which we reject our <strong>null hypothesis</strong>. Moreover, because our <strong>t-value</strong> is not anywhere in that region, we can conclude that we <strong>fail to reject our claim</strong> that there is no difference given the <strong>null hypothesis</strong> is true. Our alternative hypothesis holds that the IQ level in the sample is greater than 100.</p>
<p>On the other hand, we also see that our <strong>p-value</strong> has the following:</p>
<p>7.054310^{-5} &gt; 0.01</p>
<p>Because our <strong>p-value</strong> is greater than the <strong>significance level</strong> of 0.01, we get the same conclusion - that is, <strong>failing to reject our claim</strong> that there is no difference given the <strong>null hypothesis</strong> is true.</p>
<p>Our three confidence levels for a right-side one-tail fail to reject our claim, given our <strong>null hypothesis</strong> is true. See Table <a href="statistics.html#tab:significancelevel">6.4</a>.</p>

<table>
<caption><span id="tab:significancelevel">Table 6.4: </span>Significance Level</caption>
<thead>
<tr class="header">
<th align="left">Conf Level</th>
<th align="left">T Value</th>
<th align="left">P Value</th>
<th align="left">Significance</th>
<th align="left">Critical</th>
<th align="left">Direction</th>
<th align="left">Analysis</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">99%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.01</td>
<td align="left">t &lt; 2.58</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
<tr class="even">
<td align="left">95%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.05</td>
<td align="left">t &lt; 1.96</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
<tr class="odd">
<td align="left">90%</td>
<td align="left">4.519</td>
<td align="left">0</td>
<td align="left">p &gt; 0.10</td>
<td align="left">t &lt; 1.65</td>
<td align="left">right-side</td>
<td align="left">Fail to Reject</td>
</tr>
</tbody>
</table>

<p>Using the built-in R function <strong>t.test()</strong>, we can implement the same IQ level case in many ways with a summary:</p>

<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb333-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, tends toward left</span></a>
<a class="sourceLine" id="cb333-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt; U0)</span></a>
<a class="sourceLine" id="cb333-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 1
## alternative hypothesis: true mean is less than 100
## 99 percent confidence interval:
##   -Inf 123.5
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb335-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, tends toward right</span></a>
<a class="sourceLine" id="cb335-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &gt; U0)</span></a>
<a class="sourceLine" id="cb335-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 7e-05
## alternative hypothesis: true mean is greater than 100
## 99 percent confidence interval:
##  106.8   Inf
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb337-1" data-line-number="1"><span class="co"># One Sample T-Test, with confidence level 99%, two-sided</span></a>
<a class="sourceLine" id="cb337-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb337-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x=</span>x, <span class="dt">mu=</span><span class="dv">100</span>,  <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.99</span>)</a></code></pre></div>
<pre><code>## 
##  One Sample t-test
## 
## data:  x
## t = 4.5, df = 24, p-value = 0.0001
## alternative hypothesis: true mean is not equal to 100
## 99 percent confidence interval:
##  105.8 124.5
## sample estimates:
## mean of x 
##     115.1</code></pre>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb339-1" data-line-number="1"><span class="co"># Two Sample T-Test, tends toward the right</span></a>
<a class="sourceLine" id="cb339-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &gt; U0)</span></a>
<a class="sourceLine" id="cb339-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.3
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -6.168    Inf
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>
<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb341-1" data-line-number="1"><span class="co"># Two Sample T-Test with pooled variances  </span></a>
<a class="sourceLine" id="cb341-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb341-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2,  <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.7
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -7.831 12.231
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb343-1" data-line-number="1"><span class="co"># Two Sample T-Test, two-sided, with pooled variances</span></a>
<a class="sourceLine" id="cb343-2" data-line-number="2"><span class="co"># with (H0 = U0 vs H1 &lt;&gt; U0)</span></a>
<a class="sourceLine" id="cb343-3" data-line-number="3"><span class="kw">t.test</span>(<span class="dt">x =</span> x1, <span class="dt">y =</span> x2, <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, <span class="dt">var.equal=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  x1 and x2
## t = 0.44, df = 48, p-value = 0.7
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -7.831 12.231
## sample estimates:
## mean of x mean of y 
##     112.8     110.6</code></pre>

<p>A Studentâs T table is provided (See Table <a href="appendix.html#tab:ttable">15.4</a>) as a reference in the Appendix. Given degrees of freedom and significance level, one can cross-reference both parameters to arrive at a <strong>t-value</strong>.</p>
<p>For example:</p>

<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb345-1" data-line-number="1">alpha =<span class="st"> </span><span class="fl">0.01</span></a>
<a class="sourceLine" id="cb345-2" data-line-number="2">n =<span class="st"> </span><span class="dv">5</span>  <span class="co"># df = n - 1</span></a>
<a class="sourceLine" id="cb345-3" data-line-number="3"><span class="kw">qt</span>(<span class="dt">p=</span>alpha, <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)  <span class="co"># left-side one-tail crit value</span></a></code></pre></div>
<pre><code>## [1] -3.747</code></pre>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb347-1" data-line-number="1"><span class="co"># We can use: 1 - qt(p = alpha, df=n-1, lower.tail=TRUE)</span></a>
<a class="sourceLine" id="cb347-2" data-line-number="2"><span class="co"># or we can use the following:</span></a>
<a class="sourceLine" id="cb347-3" data-line-number="3"><span class="kw">qt</span>(<span class="dt">p=</span>alpha, <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)  <span class="co"># right-side one-tail crit value</span></a></code></pre></div>
<pre><code>## [1] 3.747</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb349-1" data-line-number="1"><span class="co"># As for two-tail, we split into two areas:  0.01/2 = 0.005</span></a>
<a class="sourceLine" id="cb349-2" data-line-number="2">two_tail =<span class="st"> </span><span class="kw">qt</span>(<span class="dt">p =</span> alpha <span class="op">/</span><span class="st"> </span><span class="dv">2</span> , <span class="dt">df=</span>n<span class="dv">-1</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb349-3" data-line-number="3"><span class="kw">c</span>(<span class="op">-</span>two_tail, two_tail)  <span class="co"># left and right critical values</span></a></code></pre></div>
<pre><code>## [1] -4.604  4.604</code></pre>

<p>For samples based on multivariate normal distribution, we leave readers to investigate Hotellingâs T^2 statistic.</p>
</div>
<div id="z-test-true-variance-known" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.3</span> Z-Test (True Variance known)<a href="statistics.html#z-test-true-variance-known" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use a <strong>Z-Test</strong> to test a given hypothesis by computing for the <strong>z-score</strong> (or <strong>standard score</strong>) to evaluate our claim. In <strong>Z-Test</strong>, we assume that our <strong>z-score</strong> follows a normal distribution and that the population mean and variance are known. Our <strong>z-score</strong> statistic is therefore expressed as:</p>
<p><span class="math display">\[\begin{align}
z = \frac{observed-expected}{standard\ error} = \frac{ (x - \mu_p)}{\sigma_p }
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{z}\)</span> is the test statistic, following a normal distribution,</li>
<li><span class="math inline">\(\mathbf{\mu_p}\)</span> is a given <strong>hypothesized</strong> population mean, <span class="math inline">\(\mu_p\)</span> = <span class="math inline">\(\mu_0\)</span>,</li>
<li><span class="math inline">\(\mathbf{\sigma_p}\)</span> is a given <strong>hypothesized</strong> population standard deviation.</li>
</ul>
<p>Here, our statistic is based on an individual observation, <span class="math inline">\(\mathbf{x}\)</span>. Equivalently, we can also use the following equation for a group of observations:</p>
<p><span class="math display">\[\begin{align}
z = \frac{ (\mu_s - \mu_p)}{ SE(\mu_s )}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{z}\)</span> is the test statistic, following a normal distribution,</li>
<li><span class="math inline">\(\mathbf{\bar{u}_s}\)</span> is the sample mean, <span class="math inline">\(\bar{x} = \mu_s\)</span>,</li>
<li><span class="math inline">\(\mathbf{\mu_p}\)</span> is a given <strong>hypothesized</strong> population mean, <span class="math inline">\(\mu_p\)</span> = <span class="math inline">\(\mu_0\)</span>.</li>
</ul>
<p>This alternative equation is based on the idea of the <strong>Central Limit Theorem</strong> which states that as the sample size increases (see Figure <a href="numericalprobability.html#fig:tdist">5.30</a>), it comes to a point where the sample mean follows a normal distribution <span class="citation">(Kwak S. G., Kim J. H. <a href="bibliography.html#ref-ref790s">2016</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>. That means that the sampling distribution starts to follow the population distribution closely. The difference is that the <strong>standard deviation</strong> of the sample is computed based on the <strong>standard error</strong> equation below:</p>
<p><span class="math display">\[\begin{align}
SE(\bar{x}) = \frac{\sigma_p^2}{n_s} = \frac{\sigma_p}{\sqrt{n_s}}
\end{align}\]</span></p>
<p>Figure <a href="statistics.html#fig:stdnormaldist">6.9</a> shows a standard normal distribution. The <strong>x-axis</strong> represents the standard deviation which is where the <strong>z-score</strong> is measured - it is a measure of the distance between our standard deviations and the mean, meaning how many standard deviations away we are from the mean. For example, if the <strong>z-score</strong> is positive two, then we are two standard deviations away from the right to the mean, <span class="math inline">\(\bar{x}_s = \mu_s = 0\)</span>. And if our <strong>z-score</strong> is negative two, then we are two standard deviations away from the left to the mean, <span class="math inline">\(\bar{x}_s = \mu_s = 0\)</span>. Therefore the unit of measurement for a <strong>z-score</strong> is in standard deviations.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:stdnormaldist"></span>
<img src="DS_files/figure-html/stdnormaldist-1.png" alt="Standard Normal Distribution" width="70%" />
<p class="caption">
Figure 6.9: Standard Normal Distribution
</p>
</div>

<p>To illustrate, suppose we have a list of 40 observations sampled from a population. We can derive the <strong>z-score</strong> for each data in the sample as such:</p>

<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb351-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb351-2" data-line-number="2">z_score &lt;-<span class="st"> </span><span class="cf">function</span>(x, mu, sd) {</a>
<a class="sourceLine" id="cb351-3" data-line-number="3">  (x <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span>sd</a>
<a class="sourceLine" id="cb351-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb351-5" data-line-number="5"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb351-6" data-line-number="6">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb351-7" data-line-number="7"><span class="co"># hypothesized mean and sd</span></a>
<a class="sourceLine" id="cb351-8" data-line-number="8">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb351-9" data-line-number="9">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">1000</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb351-10" data-line-number="10"><span class="co"># we take a sample of 40 individuals and record their IQ level</span></a>
<a class="sourceLine" id="cb351-11" data-line-number="11"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb351-12" data-line-number="12">( <span class="dt">sample_data =</span> <span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span><span class="dv">40</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) )[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] 116 116 120 119  86 124 121 130 131 121</code></pre>
<div class="sourceCode" id="cb353"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb353-1" data-line-number="1"><span class="co"># The equivalent z-scores for each element in sample x.</span></a>
<a class="sourceLine" id="cb353-2" data-line-number="2"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb353-3" data-line-number="3">(<span class="dt">zscore =</span> <span class="kw">round</span>( <span class="kw">z_score</span>(<span class="dt">x=</span>sample_data, <span class="dt">mu=</span>mu, <span class="dt">sd =</span> sd), <span class="dv">2</span>))[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1]  0.96  0.96  1.20  1.14 -0.84  1.43  1.26  1.79  1.85  1.26</code></pre>

<p>Similarly, we can derive back the raw data given a <strong>z-score</strong> using the following formula:</p>
<p><span class="math display">\[\begin{align}
x = z  \sigma + \mu
\end{align}\]</span></p>
<p>For example, to validate the <strong>z-score</strong> we derived, we use the formula above to compare it with our sample data.</p>

<div class="sourceCode" id="cb355"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb355-1" data-line-number="1"><span class="co"># display only the first 10 items</span></a>
<a class="sourceLine" id="cb355-2" data-line-number="2">( <span class="kw">round</span>( zscore <span class="op">*</span><span class="st"> </span>sd <span class="op">+</span><span class="st"> </span>mu, <span class="dv">0</span>) <span class="op">==</span><span class="st"> </span>sample_data )[<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>]</a></code></pre></div>
<pre><code>##  [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE</code></pre>

<p>Now for a better illustration, let us again use the IQ level case where we continue to assume an IQ level average of 100 and a standard deviation of 16.73. Let us determine the proportion of people with IQ levels greater than 90, lesser than 110, and between 90 and 110.</p>
<p><strong>First</strong>, let us get the z-scores.</p>

<div class="sourceCode" id="cb357"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb357-1" data-line-number="1">x =<span class="st"> </span><span class="kw">list</span>(<span class="st">&quot;min&quot;</span>=<span class="dv">90</span>, <span class="st">&quot;max&quot;</span>=<span class="dv">110</span>)</a>
<a class="sourceLine" id="cb357-2" data-line-number="2">(<span class="dt">zscore =</span> <span class="kw">round</span>( <span class="kw">z_score</span>(<span class="dt">x=</span> <span class="kw">c</span>(x<span class="op">$</span>min, x<span class="op">$</span>max), <span class="dt">mu=</span>mu, <span class="dt">sd =</span> sd), <span class="dv">2</span>))</a></code></pre></div>
<pre><code>## [1] -0.6  0.6</code></pre>

<p><strong>Second</strong>, let us use the build-in R function <strong>pnorm</strong> to get the proportions (or percentage) of the population having an IQ level greater than 90.</p>
<p><span class="math display">\[
P(z &gt; 90) 
\]</span></p>

<div class="sourceCode" id="cb359"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb359-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb359-2" data-line-number="2"><span class="co"># Get proportions using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb359-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb359-4" data-line-number="4">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">1</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb359-5" data-line-number="5"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.57%&quot;</code></pre>
<div class="sourceCode" id="cb361"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb361-1" data-line-number="1"><span class="co"># Get proportions using normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb361-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb361-3" data-line-number="3">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">90</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb361-4" data-line-number="4"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.5%&quot;</code></pre>

<p><strong>Third</strong>, let us get the proportion (or percentage) of population having an IQ level lesser than 110.</p>
<p><span class="math display">\[
P(z &lt; 110)
\]</span></p>

<div class="sourceCode" id="cb363"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb363-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb363-2" data-line-number="2"><span class="co"># Get proportions using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb363-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb363-4" data-line-number="4"><span class="co"># One way to do it</span></a>
<a class="sourceLine" id="cb363-5" data-line-number="5">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb363-6" data-line-number="6"><span class="co"># Another way to do it</span></a>
<a class="sourceLine" id="cb363-7" data-line-number="7">proportion =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb363-8" data-line-number="8"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.57%&quot;</code></pre>
<div class="sourceCode" id="cb365"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb365-1" data-line-number="1"><span class="co"># Get proportions using normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb365-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb365-3" data-line-number="3"><span class="co"># One way to do it</span></a>
<a class="sourceLine" id="cb365-4" data-line-number="4">proportion =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb365-5" data-line-number="5"><span class="co"># Another way to do it</span></a>
<a class="sourceLine" id="cb365-6" data-line-number="6">proportion =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb365-7" data-line-number="7"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;72.5%&quot;</code></pre>

<p><strong>Lastly</strong>, let us get the proportion (or percentage) of population having IQ levels between 90 and 110.</p>
<p><span class="math display">\[
P( 90 &lt; z &lt;  110)
\]</span></p>

<div class="sourceCode" id="cb367"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb367-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb367-2" data-line-number="2"><span class="co"># Get proportion using standard normal distribution (z-score)</span></a>
<a class="sourceLine" id="cb367-3" data-line-number="3">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb367-4" data-line-number="4">proportion1 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">2</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb367-5" data-line-number="5">proportion2 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> zscore[<span class="dv">1</span>], <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb367-6" data-line-number="6"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion1 <span class="op">-</span><span class="st"> </span>proportion2)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , </a>
<a class="sourceLine" id="cb367-7" data-line-number="7">                           <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;45.15%&quot;</code></pre>
<div class="sourceCode" id="cb369"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb369-1" data-line-number="1"><span class="co"># Get proportion using = normal distribution (raw data)</span></a>
<a class="sourceLine" id="cb369-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb369-3" data-line-number="3">proportion1 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">110</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb369-4" data-line-number="4">proportion2 =<span class="st"> </span><span class="kw">pnorm</span>(<span class="dt">q =</span> <span class="dv">90</span>, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb369-5" data-line-number="5"><span class="kw">list</span>(<span class="st">&quot;proportion&quot;</span> =<span class="st"> </span><span class="kw">paste</span>( <span class="kw">round</span>((proportion1 <span class="op">-</span><span class="st"> </span>proportion2)<span class="op">*</span><span class="dv">100</span>, <span class="dv">2</span>) , </a>
<a class="sourceLine" id="cb369-6" data-line-number="6">                           <span class="st">&quot;%&quot;</span>, <span class="dt">sep=</span><span class="st">&quot;&quot;</span> ))</a></code></pre></div>
<pre><code>## $proportion
## [1] &quot;45%&quot;</code></pre>

<p>Consequently, we can also derive the z-score and raw data using the built-in R function <strong>qnorm()</strong> given the probability (proportion). For example:</p>

<div class="sourceCode" id="cb371"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb371-1" data-line-number="1"><span class="co"># Get Z-Score using standard normal distribution</span></a>
<a class="sourceLine" id="cb371-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">0</span>; sd =<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb371-3" data-line-number="3"><span class="kw">round</span>( <span class="kw">qnorm</span>(<span class="dt">p =</span> proportion2, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>), <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] -0.6</code></pre>
<div class="sourceCode" id="cb373"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb373-1" data-line-number="1"><span class="kw">round</span>( <span class="kw">qnorm</span>(<span class="dt">p =</span> proportion1, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>), <span class="dv">1</span>)</a></code></pre></div>
<pre><code>## [1] 0.6</code></pre>
<div class="sourceCode" id="cb375"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb375-1" data-line-number="1"><span class="co"># Get Raw Data using  normal distribution</span></a>
<a class="sourceLine" id="cb375-2" data-line-number="2">mu =<span class="st"> </span><span class="dv">100</span>; sd =<span class="st"> </span><span class="fl">16.73</span></a>
<a class="sourceLine" id="cb375-3" data-line-number="3"><span class="kw">qnorm</span>(<span class="dt">p =</span> proportion2, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 90</code></pre>
<div class="sourceCode" id="cb377"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb377-1" data-line-number="1"><span class="kw">qnorm</span>(<span class="dt">p =</span> proportion1, <span class="dt">mean=</span>mu, <span class="dt">sd=</span>sd, <span class="dt">lower.tail=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## [1] 110</code></pre>

<p>There are two Z-tables for Z-Score that can be found in the Appendix. One table ranges from -3.9 to 0, and another table ranges from 0 to 3.9. The cross-section represents the probability. See Table <a href="appendix.html#tab:ztable1">15.5</a> and Table <a href="appendix.html#tab:ztable2">15.6</a> in the Appendix.</p>
<p>So far, we have not discussed <strong>null hypothesis and alternative hypothesis</strong> in this section. Testing a hypothesis using Z-test also applies given a computation of <strong>z-score</strong> and computation of proportions. The <strong>Critical Values</strong> and <strong>Significance Level</strong> also apply. The <strong>z-score</strong> (similar to t-value) is compared against the chosen <strong>Critical Value</strong> and the <strong>proportion</strong> (similar to p-value) is compared against the chosen <strong>significance Level</strong>. From there, one can conclude whether to reject the claim, the <strong>null hypothesis</strong> - <span class="math inline">\(\mathbf{H_0}\)</span>, stating that there is no difference between observations and population; otherwise, to indicate a failure to reject the claim, <span class="math inline">\(\mathbf{H_0}\)</span>.</p>
<p>Similar to <strong>T-test</strong>, we also can compute for margin of error, which is expressed as:</p>
<p><span class="math display">\[\begin{align}
me = Z \times SE,\ \ \ \ \ where\ SE = \frac{\sigma_p}{\sqrt{n}}
\end{align}\]</span></p>
<p>The confidence interval is expressed as:</p>
<p><span class="math display">\[\begin{align}
C.I. = \bar{x}_{s} \pm me
\end{align}\]</span></p>
<p>We leave readers to investigate the <strong>Two-Sample Z-Test</strong> and <strong>Paired Z-Test</strong> given the following equations:</p>
<p><strong>Two-Sample Z-Test</strong></p>
<p><span class="math display">\[
z = \frac{ (\bar{x}_s - \mu_p)}{\sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma_2^2}{N_2}}} 
\]</span></p>
<p><strong>Paired Z-Test</strong></p>
<p><span class="math display">\[
z = \frac{ (\bar{x}_{s1} - \bar{x}_{s2}) - (\mu_1 - \mu_2)}{\sqrt{\frac{\sigma_1^2}{N_1} + \frac{\sigma_2^2}{N_2}}} 
\]</span></p>
</div>
<div id="f-test-using-f-ratio" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.4</span> F-Test using F-ratio  <a href="statistics.html#f-test-using-f-ratio" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>F-Test</strong> follows an F-distribution <span class="citation">(Ardelean F. A. <a href="bibliography.html#ref-ref809f">2017</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>. It is a test commonly used to compare variances of multiple groups independently sampled from a population (e.g., or from separate experiments) using an <strong>F statistic</strong>. To compute for the <strong>F statistic</strong>, we use the following formula (also known as <strong>ratio of variances</strong> or simply <strong>F-ratio</strong>):</p>
<p><span class="math display">\[\begin{align}
F = \frac{\sigma_1^2} {\sigma_2^2}\ \ \ \ \ where\ \sigma_1 &gt; \sigma_2
\end{align}\]</span></p>
<p>Note that we force a right-side tail by having the variance with greater value be the numerator, making it easier to interpret. For inference, our <strong>null hypothesis</strong> claims that there is no significant difference between the variance of the first sample and the variance of the second sample. That is expressed as:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \sigma_1^2 = \sigma_2^2\ \ \ \ \ \leftarrow \ \ \ \text{null hypothesis}\\
H_1 &amp;: \sigma_1^2 \ne \sigma_2^2\ \ \ \ \ \leftarrow \ \ \ \text{alternative}\\
\end{align*}\]</span></p>
<p>In terms of ratio, the <strong>null hypothesis</strong> is true only if <strong>F-ratio</strong> is 1.</p>
<p>To illustrate, let us review the following naive implementation of <strong>F test</strong> in R code:</p>

<div class="sourceCode" id="cb379"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb379-1" data-line-number="1">f_test &lt;-<span class="st"> </span><span class="cf">function</span>(x1, x2) {</a>
<a class="sourceLine" id="cb379-2" data-line-number="2">  v1 =<span class="st"> </span><span class="kw">var</span>(x1)</a>
<a class="sourceLine" id="cb379-3" data-line-number="3">  v2 =<span class="st"> </span><span class="kw">var</span>(x2)</a>
<a class="sourceLine" id="cb379-4" data-line-number="4">  n1 =<span class="st"> </span><span class="kw">length</span>(x1)  </a>
<a class="sourceLine" id="cb379-5" data-line-number="5">  n2 =<span class="st"> </span><span class="kw">length</span>(x2) </a>
<a class="sourceLine" id="cb379-6" data-line-number="6">  fratio =<span class="st"> </span><span class="dv">0</span>;  p =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb379-7" data-line-number="7">  num =<span class="st"> </span>v1; denom =<span class="st"> </span>v2</a>
<a class="sourceLine" id="cb379-8" data-line-number="8">  df1 =<span class="st"> </span>n1 <span class="op">-</span><span class="st"> </span><span class="dv">1</span>; df2 =<span class="st"> </span>n2 <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb379-9" data-line-number="9">  greater =<span class="st"> </span><span class="ot">TRUE</span></a>
<a class="sourceLine" id="cb379-10" data-line-number="10">  <span class="cf">if</span> (v2 <span class="op">&gt;</span><span class="st"> </span>v1) {</a>
<a class="sourceLine" id="cb379-11" data-line-number="11">    num =<span class="st"> </span>v2; denom =<span class="st"> </span>v1</a>
<a class="sourceLine" id="cb379-12" data-line-number="12">    df1 =<span class="st"> </span>n2 <span class="op">-</span><span class="st"> </span><span class="dv">1</span>; df2 =<span class="st"> </span>n1 <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb379-13" data-line-number="13">    greater =<span class="st"> </span><span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb379-14" data-line-number="14">  } </a>
<a class="sourceLine" id="cb379-15" data-line-number="15">    fratio =<span class="st"> </span>num <span class="op">/</span><span class="st"> </span>denom</a>
<a class="sourceLine" id="cb379-16" data-line-number="16">    p =<span class="st"> </span><span class="kw">pf</span>(fratio, df1, df2, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb379-17" data-line-number="17">  <span class="kw">list</span>(<span class="st">&quot;greater&quot;</span>=greater, <span class="st">&quot;fratio&quot;</span>=fratio, </a>
<a class="sourceLine" id="cb379-18" data-line-number="18">       <span class="st">&quot;num df1&quot;</span> =<span class="st"> </span>df1, <span class="st">&quot;denom df2&quot;</span> =<span class="st"> </span>df2, <span class="st">&quot;pvalue&quot;</span>=p)</a>
<a class="sourceLine" id="cb379-19" data-line-number="19">}</a>
<a class="sourceLine" id="cb379-20" data-line-number="20"><span class="co"># let us constrain IQ range between 80 and 140.</span></a>
<a class="sourceLine" id="cb379-21" data-line-number="21">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb379-22" data-line-number="22"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb379-23" data-line-number="23">population1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb379-24" data-line-number="24"><span class="kw">set.seed</span>(<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb379-25" data-line-number="25">population2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">100</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb379-26" data-line-number="26"><span class="co"># we take two samples and record their IQ level</span></a>
<a class="sourceLine" id="cb379-27" data-line-number="27"><span class="kw">set.seed</span>(<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb379-28" data-line-number="28">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population1, <span class="dt">size=</span><span class="dv">11</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) <span class="co"># group 1</span></a>
<a class="sourceLine" id="cb379-29" data-line-number="29">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population2, <span class="dt">size=</span><span class="dv">26</span>, <span class="dt">replace=</span><span class="ot">FALSE</span>) <span class="co"># group 2</span></a>
<a class="sourceLine" id="cb379-30" data-line-number="30"><span class="co"># get the F statistic</span></a>
<a class="sourceLine" id="cb379-31" data-line-number="31">fscore =<span class="st"> </span><span class="kw">f_test</span>(x1, x2); <span class="kw">t</span>(fscore)</a></code></pre></div>
<pre><code>##      greater fratio num df1 denom df2 pvalue
## [1,] TRUE    1.313  10      25        0.2768</code></pre>

<p>Alternatively, we can use the built-in R function <strong>var.test(.)</strong>. Note that the function always expects the first parameter to be the numerator. Our own <strong>f_test(.)</strong> function uses the greater sample as our numerator. Our result shows that the first sample is the lesser one; therefore, in using <strong>var.test(.)</strong>, we pass the value <strong>less</strong> to the alternative parameter given we pass the second sample to the first parameter.</p>

<div class="sourceCode" id="cb381"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb381-1" data-line-number="1"><span class="co"># first parameter is the second sample, x2</span></a>
<a class="sourceLine" id="cb381-2" data-line-number="2"><span class="co"># therefore, alternative=less</span></a>
<a class="sourceLine" id="cb381-3" data-line-number="3">v1 =<span class="st"> </span><span class="kw">var</span>(x1); v2 =<span class="st"> </span><span class="kw">var</span>(x2)</a>
<a class="sourceLine" id="cb381-4" data-line-number="4"><span class="cf">if</span> (v1 <span class="op">&gt;</span><span class="st"> </span>v2) {</a>
<a class="sourceLine" id="cb381-5" data-line-number="5">  <span class="kw">var.test</span>(x1, x2, <span class="dt">alternative=</span><span class="st">&quot;less&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.90</span>)</a>
<a class="sourceLine" id="cb381-6" data-line-number="6">} <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb381-7" data-line-number="7">  <span class="kw">var.test</span>(x2, x1, <span class="dt">alternative=</span><span class="st">&quot;greater&quot;</span>, <span class="dt">conf.level=</span><span class="fl">0.90</span>)  </a>
<a class="sourceLine" id="cb381-8" data-line-number="8">}</a></code></pre></div>
<pre><code>## 
##  F test to compare two variances
## 
## data:  x1 and x2
## F = 1.3, num df = 10, denom df = 25, p-value = 0.7
## alternative hypothesis: true ratio of variances is less than 1
## 90 percent confidence interval:
##  0.000 2.854
## sample estimates:
## ratio of variances 
##              1.313</code></pre>

<p>We can reference the <strong>F table</strong>, Table <a href="appendix.html#tab:ftable">15.7</a>, in the Appendix to derive the <strong>critical value</strong>. Here, we see that x2 is the greater sample with degrees of freedom at 35, which becomes our numerator. Sample x1 has df at ten, and it becomes our denominator for our <strong>F ratio</strong>. Therefore, our <strong>critical value</strong> in the <strong>F table</strong> corresponds to <strong>2.730</strong> granting our confidence level is at 90% - equivalent to <span class="math inline">\(\alpha = 0.10\)</span>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fdist"></span>
<img src="DS_files/figure-html/fdist-1.png" alt="F Distribution" width="70%" />
<p class="caption">
Figure 6.10: F Distribution
</p>
</div>

<p>Using Figure <a href="statistics.html#fig:fdist">6.10</a>, we see that <strong>F</strong> is lesser than the <strong>critical value</strong>; meaning, <strong>f-score</strong> is outside the <strong>rejection region</strong>. Therefore, we fail to reject the claim, given the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>, is true.</p>
<p>Similarly, if we instead use a <strong>significance level</strong> of 0.05, e.g. <span class="math inline">\(\mathbf{\alpha=0.05}\)</span>, then we see that <strong>p-value</strong> 0.28 is greater than <span class="math inline">\(\alpha = 0.05\)</span>. Therefore, we fail to reject the claim, given the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>, is true.</p>
<p>Let us now discuss a case in which we can derive the <strong>F ratio</strong> by using <strong>ANOVA</strong>.</p>
</div>
<div id="f-test-with-one-way-anova" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.5</span> F-Test with One-Way ANOVA <a href="statistics.html#f-test-with-one-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Anova</strong> stands for <strong>analysis of variance</strong> <span class="citation">(Ardelean F. A. <a href="bibliography.html#ref-ref809f">2017</a>)</span>. There are two ways of performing <strong>ANOVA</strong>.</p>
<ul>
<li>One-Way Anova - we use this to analyze the variation of multiple groups involving one independent variable (one factor) and a dependent variable. The goal is to compare variations between groups (e.g., comparing means if they are closer or farther apart) and within groups.</li>
<li>Two-Way Anova - we use this to analyze the variation of multiple groups involving two independent variables (two factors) and a dependent variable. The goal is to examine the effect of one factor, the other, or both factors (via interaction) against the dependent variable.</li>
</ul>
<p>Note that an <strong>F-Test using ANOVA</strong> is an <strong>Omnibus Test</strong>.</p>
<p>To perform a one-way analysis of variance (One-Way ANOVA), we are to compute a few formulas:</p>
<p><strong>Sum of Squares Total (SST):</strong>  </p>
<p><span class="math display">\[\begin{align}
SS_T = \sum_{j=1}^m \sum_{i=1}^{n_j} (x_{ji} - \bar{x})^2
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>m</strong> is the total number of groups sampled from a population.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{x_{ji}}\)</span> is the ith observation in sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}}\)</span> is the overall grand mean.</li>
</ul>
<p><strong>Sum of Squares between samples (SSB):</strong>  </p>
<p><span class="math display">\[\begin{align}
SS_B = SS_{(treatment)} = \sum_{j=1}^m \sum_{i=1}^{n_j} (\bar{x}_j - \bar{x})^2  = \sum_{j=1}^m n_j  (\bar{x}_j - \bar{x})^2
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{SS_B}\)</span> is also called the sum of squares treatment.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}_j}\)</span> is the jth sample mean.</li>
<li><span class="math inline">\(\mathbf{\bar{x}}\)</span> is the overall grand mean.</li>
</ul>
<p><strong>Sum of Squares within samples (SSW):</strong>  </p>
<p><span class="math display">\[\begin{align}
SS_W = SS_{error} = \sum_{j=1}^m \sum_{i=1}^{n_j} (x_{ji} - \bar{x}_j)^2 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{SS_W}\)</span> is also called the sum of squares error.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{x_{ji}}\)</span> is the ith observation in sample j.</li>
<li><span class="math inline">\(\mathbf{\bar{x}_j}\)</span> is the jth sample mean.</li>
</ul>
<p>Note here that <strong>SST</strong> can also be computed as:</p>
<p><span class="math display">\[\begin{align}
SS_T = SS_B + SS_W
\end{align}\]</span></p>
<p><strong>Mean Squares between samples (MSB):</strong>  </p>
<p><span class="math display">\[\begin{align}
MS_B = s^2_B = \frac{SS_B}{df_B},\ \ \ \ \ \ df_B =  m - 1
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{MS_B}\)</span> is also called the mean of squares treatment.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{df_B}\)</span> is the degrees of freedom between samples.</li>
</ul>
<p><strong>Mean Squares within samples (MSW):</strong>  </p>
<p><span class="math display">\[\begin{align}
MS_W = MS_E = s^2_w = \frac{SS_W}{df_W},\ \ \ \ \ \ df_W = \sum_j^m (n_j - 1)
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{MS_W}\)</span> is also called the within mean square (error).</li>
<li><strong>N</strong> is the overall total observations, e.g., N = mn.</li>
<li><strong>m</strong> is the total number of samples.</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the sample size of sample j.</li>
<li><span class="math inline">\(\mathbf{df_W}\)</span> is the degrees of freedom within samples.</li>
</ul>
<p>Finally, we compute for the <strong>F statistic</strong>: </p>
<p><span class="math display">\[\begin{align}
F = \frac{explained}{unexplained} = \frac{s^2_B}{s^2_W} = \frac{MS_B}{MS_W} 
\end{align}\]</span></p>
<p>All the computation is reflected in the following <strong>One-Way ANOVA</strong> table <span class="citation">(Larson M. G. <a href="bibliography.html#ref-ref827m">2008</a>; Illowsky B, Dean S. <a href="bibliography.html#ref-ref818b">2018</a>)</span>: </p>

<table>
<caption><span id="tab:onewayanova">Table 6.5: </span>One-Way ANOVA</caption>
<thead>
<tr class="header">
<th align="left">Source of Variation</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Sum of Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Between</td>
<td align="left"><span class="math inline">\(df_{B}: m- 1\)</span></td>
<td align="left"><span class="math inline">\(SS_B\)</span></td>
<td align="left"><span class="math inline">\(MS_B: \frac{SSB}{df_B}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MSB}{MSW}\)</span></td>
</tr>
<tr class="even">
<td align="left">Within (Error)</td>
<td align="left"><span class="math inline">\(df_{W}: \sum_j^m (n_j - 1)\)</span></td>
<td align="left"><span class="math inline">\(SS_W\)</span></td>
<td align="left"><span class="math inline">\(MS_W: \frac{SS_W}{df_W}\)</span></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(df_T: \sum_j^m (n_j) - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>Below is a naive implementation of <strong>One-Way ANOVA</strong> in R code:</p>

<div class="sourceCode" id="cb383"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb383-1" data-line-number="1"><span class="co"># Supports different group sizes</span></a>
<a class="sourceLine" id="cb383-2" data-line-number="2">one_way_anova &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, <span class="dt">factor =</span> <span class="ot">NULL</span>, <span class="dt">size =</span> <span class="dv">0</span>) {</a>
<a class="sourceLine" id="cb383-3" data-line-number="3">  <span class="cf">if</span> (<span class="kw">is.null</span>(factor)) {</a>
<a class="sourceLine" id="cb383-4" data-line-number="4">      m =<span class="st"> </span><span class="kw">length</span>(dependent)</a>
<a class="sourceLine" id="cb383-5" data-line-number="5">      x =<span class="st"> </span>dependent</a>
<a class="sourceLine" id="cb383-6" data-line-number="6">  } <span class="cf">else</span> {</a>
<a class="sourceLine" id="cb383-7" data-line-number="7">      groups =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb383-8" data-line-number="8">      m =<span class="st"> </span><span class="kw">length</span>(groups)  <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb383-9" data-line-number="9">      x =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb383-10" data-line-number="10">      <span class="co"># Group factors</span></a>
<a class="sourceLine" id="cb383-11" data-line-number="11">      <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-12" data-line-number="12">          idx =<span class="st"> </span><span class="kw">which</span>(factor <span class="op">==</span><span class="st"> </span>groups[j])</a>
<a class="sourceLine" id="cb383-13" data-line-number="13">          x[[j]] =<span class="st"> </span>dependent[idx]</a>
<a class="sourceLine" id="cb383-14" data-line-number="14">      }</a>
<a class="sourceLine" id="cb383-15" data-line-number="15">  } </a>
<a class="sourceLine" id="cb383-16" data-line-number="16">  <span class="co"># Get the grand mean</span></a>
<a class="sourceLine" id="cb383-17" data-line-number="17">  grand_mean =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-18" data-line-number="18">  r =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-19" data-line-number="19">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-20" data-line-number="20">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-21" data-line-number="21">      r =<span class="st"> </span>r <span class="op">+</span><span class="st"> </span>n</a>
<a class="sourceLine" id="cb383-22" data-line-number="22">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-23" data-line-number="23">        grand_mean =<span class="st"> </span>grand_mean <span class="op">+</span><span class="st"> </span>x[[j]][i]</a>
<a class="sourceLine" id="cb383-24" data-line-number="24">      }</a>
<a class="sourceLine" id="cb383-25" data-line-number="25">  }</a>
<a class="sourceLine" id="cb383-26" data-line-number="26">  grand_mean =<span class="st"> </span>grand_mean <span class="op">/</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb383-27" data-line-number="27">  <span class="co"># sum squared total</span></a>
<a class="sourceLine" id="cb383-28" data-line-number="28">  SST =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-29" data-line-number="29">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-30" data-line-number="30">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-31" data-line-number="31">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-32" data-line-number="32">        SST =<span class="st"> </span>SST <span class="op">+</span><span class="st"> </span>(x[[j]][i] <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-33" data-line-number="33">      }</a>
<a class="sourceLine" id="cb383-34" data-line-number="34">  }</a>
<a class="sourceLine" id="cb383-35" data-line-number="35">  <span class="co"># sum square between</span></a>
<a class="sourceLine" id="cb383-36" data-line-number="36">  SSB =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-37" data-line-number="37">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-38" data-line-number="38">      grp_mean =<span class="st"> </span><span class="kw">mean</span>(x[[j]])</a>
<a class="sourceLine" id="cb383-39" data-line-number="39">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]])  <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-40" data-line-number="40">      SSB =<span class="st"> </span>SSB <span class="op">+</span><span class="st"> </span>n<span class="op">*</span>(grp_mean <span class="op">-</span><span class="st"> </span>grand_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-41" data-line-number="41">  }</a>
<a class="sourceLine" id="cb383-42" data-line-number="42">  <span class="co"># sum square within</span></a>
<a class="sourceLine" id="cb383-43" data-line-number="43">  SSW =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb383-44" data-line-number="44">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-45" data-line-number="45">      grp_mean =<span class="st"> </span><span class="kw">mean</span>(x[[j]])</a>
<a class="sourceLine" id="cb383-46" data-line-number="46">      n =<span class="st"> </span><span class="kw">length</span>(x[[j]]) <span class="co"># size of each group</span></a>
<a class="sourceLine" id="cb383-47" data-line-number="47">      <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb383-48" data-line-number="48">        SSW =<span class="st"> </span>SSW <span class="op">+</span><span class="st"> </span>(x[[j]][i] <span class="op">-</span><span class="st"> </span>grp_mean)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb383-49" data-line-number="49">      }</a>
<a class="sourceLine" id="cb383-50" data-line-number="50">  }</a>
<a class="sourceLine" id="cb383-51" data-line-number="51">  <span class="co"># mean squared between</span></a>
<a class="sourceLine" id="cb383-52" data-line-number="52">  dfB =<span class="st"> </span>m <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb383-53" data-line-number="53">  MSB =<span class="st"> </span>SSB <span class="op">/</span><span class="st"> </span>dfB</a>
<a class="sourceLine" id="cb383-54" data-line-number="54">  <span class="co"># mean squared within</span></a>
<a class="sourceLine" id="cb383-55" data-line-number="55">  dfW =<span class="st"> </span><span class="dv">0</span>  <span class="co"># assume different group sizes; otherwise, use m * (n - 1)</span></a>
<a class="sourceLine" id="cb383-56" data-line-number="56">  <span class="cf">for</span> (j <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb383-57" data-line-number="57">    dfW =<span class="st"> </span>dfW <span class="op">+</span><span class="st"> </span>( <span class="kw">length</span>(x[[j]]) <span class="op">-</span><span class="st"> </span><span class="dv">1</span> )</a>
<a class="sourceLine" id="cb383-58" data-line-number="58">  }</a>
<a class="sourceLine" id="cb383-59" data-line-number="59">  MSW =<span class="st"> </span>SSW <span class="op">/</span><span class="st">  </span>dfW</a>
<a class="sourceLine" id="cb383-60" data-line-number="60">  <span class="co"># F statistic</span></a>
<a class="sourceLine" id="cb383-61" data-line-number="61">  F =<span class="st"> </span>MSB <span class="op">/</span><span class="st"> </span>MSW</a>
<a class="sourceLine" id="cb383-62" data-line-number="62">  <span class="co"># return statistics</span></a>
<a class="sourceLine" id="cb383-63" data-line-number="63">  <span class="kw">c</span>( <span class="st">&quot;SSB&quot;</span>=SSB, <span class="st">&quot;SSW&quot;</span>=SSW, <span class="st">&quot;dfB&quot;</span>=dfB,<span class="st">&quot;dfW&quot;</span>=dfW, </a>
<a class="sourceLine" id="cb383-64" data-line-number="64">     <span class="st">&quot;MSB&quot;</span>=MSB, <span class="st">&quot;MSW&quot;</span>=MSW, <span class="st">&quot;F&quot;</span>=F)</a>
<a class="sourceLine" id="cb383-65" data-line-number="65">}</a></code></pre></div>

<p>Let us consider here two cases. One case would be three nearly distinct populations - forcing three sample means to be farther apart. See below:</p>

<div class="sourceCode" id="cb384"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb384-1" data-line-number="1"><span class="co"># Now let us generate three groups (of population)</span></a>
<a class="sourceLine" id="cb384-2" data-line-number="2"><span class="co"># with different ranges to force different means.</span></a>
<a class="sourceLine" id="cb384-3" data-line-number="3"><span class="co"># The use of different random seed helps to</span></a>
<a class="sourceLine" id="cb384-4" data-line-number="4"><span class="co"># generate independent groups</span></a>
<a class="sourceLine" id="cb384-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb384-6" data-line-number="6">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">70</span>, <span class="dv">120</span>)</a>
<a class="sourceLine" id="cb384-7" data-line-number="7">population1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-8" data-line-number="8"><span class="kw">set.seed</span>(<span class="dv">20</span>)</a>
<a class="sourceLine" id="cb384-9" data-line-number="9">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">130</span>)</a>
<a class="sourceLine" id="cb384-10" data-line-number="10">population2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-11" data-line-number="11"><span class="kw">set.seed</span>(<span class="dv">30</span>)</a>
<a class="sourceLine" id="cb384-12" data-line-number="12">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">90</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb384-13" data-line-number="13">population3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb384-14" data-line-number="14"><span class="co"># let us now sample observations from each group</span></a>
<a class="sourceLine" id="cb384-15" data-line-number="15"><span class="co"># with same sample sizes</span></a>
<a class="sourceLine" id="cb384-16" data-line-number="16"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb384-17" data-line-number="17">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb384-18" data-line-number="18">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population1, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-19" data-line-number="19">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population2, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-20" data-line-number="20">x3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population3, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb384-21" data-line-number="21"><span class="co"># Generate One-Way Anova</span></a>
<a class="sourceLine" id="cb384-22" data-line-number="22">x =<span class="st"> </span><span class="kw">list</span>() ; x[[<span class="dv">1</span>]] =<span class="st"> </span>x1 ; x[[<span class="dv">2</span>]] =<span class="st"> </span>x2; x[[<span class="dv">3</span>]] =<span class="st"> </span>x3</a>
<a class="sourceLine" id="cb384-23" data-line-number="23">(<span class="dt">anova =</span> <span class="kw">one_way_anova</span>(x))</a></code></pre></div>
<pre><code>##      SSB      SSW      dfB      dfW      MSB      MSW        F 
##  540.867 5124.500    2.000   27.000  270.433  189.796    1.425</code></pre>

<p>And to see if we reject the null hypothesis, we evaluate our f-value:</p>

<div class="sourceCode" id="cb386"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb386-1" data-line-number="1">(<span class="dt">Fcrit =</span> <span class="kw">qf</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">df1 =</span> anova[<span class="st">&quot;dfB&quot;</span>], <span class="dt">df2 =</span> anova[<span class="st">&quot;dfW&quot;</span>], </a>
<a class="sourceLine" id="cb386-2" data-line-number="2">            <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 3.354</code></pre>
<div class="sourceCode" id="cb388"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb388-1" data-line-number="1">(<span class="dt">Fvalue =</span> <span class="kw">as.numeric</span>(anova[<span class="st">&quot;F&quot;</span>])) <span class="op">&gt;</span><span class="st"> </span>Fcrit</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>and our p-value:</p>

<div class="sourceCode" id="cb390"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb390-1" data-line-number="1">(<span class="dt">Pvalue =</span> <span class="kw">pf</span>(Fvalue, anova[<span class="st">&quot;dfB&quot;</span>], anova[<span class="st">&quot;dfW&quot;</span>], <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.2581</code></pre>
<div class="sourceCode" id="cb392"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb392-1" data-line-number="1">Pvalue <span class="op">&lt;</span><span class="st"> </span>( <span class="dt">alpha =</span> <span class="fl">0.05</span> )</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>Both outcomes show as <strong>TRUE</strong> meaning, we reject the null hypothesis - there is a significant difference between the groups.</p>
<p>We can also validate using the built-in function called <strong>aov(.)</strong>:</p>

<div class="sourceCode" id="cb394"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb394-1" data-line-number="1">A=<span class="st"> </span><span class="kw">cbind</span>(x1, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb394-2" data-line-number="2">B=<span class="st"> </span><span class="kw">cbind</span>(x2, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb394-3" data-line-number="3">C=<span class="st"> </span><span class="kw">cbind</span>(x3, <span class="kw">rep</span>(<span class="dt">n=</span>sample_size, <span class="dv">3</span>))</a>
<a class="sourceLine" id="cb394-4" data-line-number="4">data =<span class="st"> </span>A</a>
<a class="sourceLine" id="cb394-5" data-line-number="5">data =<span class="st"> </span><span class="kw">rbind</span>(data, B)</a>
<a class="sourceLine" id="cb394-6" data-line-number="6">data =<span class="st"> </span><span class="kw">rbind</span>(data, C)</a>
<a class="sourceLine" id="cb394-7" data-line-number="7"><span class="kw">colnames</span>(data) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;y&quot;</span>, <span class="st">&quot;x&quot;</span>)</a>
<a class="sourceLine" id="cb394-8" data-line-number="8">aov.model =<span class="st"> </span><span class="kw">aov</span>(y <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(x), <span class="dt">data =</span> <span class="kw">as.data.frame</span>(data))</a>
<a class="sourceLine" id="cb394-9" data-line-number="9"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##              Df Sum Sq Mean Sq F value Pr(&gt;F)
## as.factor(x)  2    541     270    1.42   0.26
## Residuals    27   5124     190</code></pre>

<p>We can use <strong>df.residual(.)</strong> to get the <strong>degrees of freedom (dfW)</strong> for the within-group:</p>

<div class="sourceCode" id="cb396"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb396-1" data-line-number="1"><span class="kw">df.residual</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 27</code></pre>

<p>We can use <strong>deviance(.)</strong> to get the <strong>sum square (SSW)</strong> for the within-group:</p>

<div class="sourceCode" id="cb398"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb398-1" data-line-number="1"><span class="kw">deviance</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 5124</code></pre>

<p>Finally, we can get <strong>MSW</strong> by dividing deviance by degrees of freedom:</p>

<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb400-1" data-line-number="1"><span class="kw">deviance</span>(aov.model)<span class="op">/</span><span class="kw">df.residual</span>(aov.model)</a></code></pre></div>
<pre><code>## [1] 189.8</code></pre>

<p>The other case would be to take three samples from the same population - forcing sample means between samples to be closer together. An example of this scenario is when performing regression against a repeated sampling of the same population.</p>

<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb402-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb402-2" data-line-number="2">iq_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">80</span>, <span class="dv">140</span>)</a>
<a class="sourceLine" id="cb402-3" data-line-number="3"><span class="co"># Now let us generate a single population ( a single group )</span></a>
<a class="sourceLine" id="cb402-4" data-line-number="4">population =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> iq_range, <span class="dt">size=</span><span class="dv">500</span>, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb402-5" data-line-number="5"><span class="co"># let us now sample observations from each group</span></a>
<a class="sourceLine" id="cb402-6" data-line-number="6"><span class="co"># with same sample sizes</span></a>
<a class="sourceLine" id="cb402-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb402-8" data-line-number="8">sample_size =<span class="st"> </span><span class="dv">300</span></a>
<a class="sourceLine" id="cb402-9" data-line-number="9">x1 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-10" data-line-number="10">x2 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-11" data-line-number="11">x3 =<span class="st"> </span><span class="kw">sample</span>(<span class="dt">x =</span> population, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb402-12" data-line-number="12"><span class="co"># Generate One-Way Anova</span></a>
<a class="sourceLine" id="cb402-13" data-line-number="13">x =<span class="st"> </span><span class="kw">list</span>() ; x[[<span class="dv">1</span>]] =<span class="st"> </span>x1 ; x[[<span class="dv">2</span>]] =<span class="st"> </span>x2; x[[<span class="dv">3</span>]] =<span class="st"> </span>x3</a>
<a class="sourceLine" id="cb402-14" data-line-number="14">(<span class="dt">anova =</span> <span class="kw">round</span>(<span class="kw">one_way_anova</span>(x),<span class="dv">3</span>))</a></code></pre></div>
<pre><code>##        SSB        SSW        dfB        dfW        MSB        MSW          F 
##    422.909 266582.687      2.000    897.000    211.454    297.194      0.712</code></pre>

<p>And to see if we reject the null hypothesis, we verify critical value first:</p>

<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb404-1" data-line-number="1">(<span class="dt">Fcrit =</span> <span class="kw">qf</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">df1 =</span> anova[<span class="st">&quot;dfB&quot;</span>], <span class="dt">df2 =</span> anova[<span class="st">&quot;dfW&quot;</span>], </a>
<a class="sourceLine" id="cb404-2" data-line-number="2">            <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 3.006</code></pre>
<div class="sourceCode" id="cb406"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb406-1" data-line-number="1">(<span class="dt">Fvalue =</span> <span class="kw">as.numeric</span>(anova[<span class="st">&quot;F&quot;</span>])) <span class="op">&gt;</span><span class="st"> </span>Fcrit</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>and validate with P-value:</p>

<div class="sourceCode" id="cb408"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb408-1" data-line-number="1">(<span class="dt">Pvalue =</span> <span class="kw">pf</span>(Fvalue, anova[<span class="st">&quot;dfB&quot;</span>], anova[<span class="st">&quot;dfW&quot;</span>], <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.4909</code></pre>
<div class="sourceCode" id="cb410"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb410-1" data-line-number="1">Pvalue <span class="op">&lt;</span><span class="st"> </span>( <span class="dt">alpha =</span> <span class="fl">0.05</span> )</a></code></pre></div>
<pre><code>## [1] FALSE</code></pre>

<p>Both outcomes show as <strong>FALSE</strong> meaning, we fail to reject the null hypothesis - there is no difference between the groups.</p>
<p>As can be seen, the <strong>F statistic</strong> is a ratio that reflects the different distributions. Our case demonstrates that if the means are farther from each other between groups, meaning that MSB is greater than MSW, then our <strong>F ratio</strong> is greater than one. On the other hand, if the samples are almost overlapping, meaning that MSB and MSW are almost identical, we see <strong>F ratio</strong> to be closer to one. And if MSW is greater than MSB, it gets closer to zero.</p>
<p>The differences in means are further discussed in the <strong>Post-HOC section</strong>.</p>
<p>In terms of evaluation of <strong>hypothesis</strong>, it is clear that if the <strong>F ratio</strong> is farther away from one, then we reject the <strong>null hypothesis</strong>, <span class="math inline">\(\mathbf{H_0}\)</span>. Otherwise, if <strong>F ratio</strong> is close to or equal to one, we fail to reject the <strong>null hypothesis</strong>, which claims that there is no difference between samples - that their means are somehow equal. Alternatively, the <span class="math inline">\(\mathbf{H_1}\)</span> has <strong>at least one</strong> mean to be different from the others.</p>
<p><span class="math display">\[\begin{align}
H_0: \mu_1 = \mu_2 = \mu_3,\ \ \ \ \ \ \  H_1: \mu_1 \ne \mu_2 \ne \mu_3
\end{align}\]</span></p>
<p>Let us revisit <strong>One-Way ANOVA</strong> when we cover <strong>Tukeyâs method</strong> under the <strong>Post-Hoc Analysis</strong> section using a more practical dataset called <strong>mtcars</strong>. </p>
</div>
<div id="f-test-with-two-way-anova" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.6</span> F-Test with Two-Way ANOVA <a href="statistics.html#f-test-with-two-way-anova" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>One-Way ANOVA</strong>, we show how to perform a test with one independent <strong>nominal</strong> variable. In <strong>Two-Way ANOVA</strong>, we show how to analyze with two independent <strong>nominal</strong> variables (or factors) and a scale-level dependent variable.</p>
<p>For example, suppose we are a dog food company, and we have invited dog owners across the country to participate in our marketing campaign to try our three new brands of dog food (call it A, B, C) specifically made for tiny, adorable cross-breed dogs of the following breed (ref: <a href="https://dogtime.com/dog-breeds/profiles" class="uri">https://dogtime.com/dog-breeds/profiles</a>):</p>
<ul>
<li>Cavachon (Cavalier King Charles Spaniel and Bichon Frise)</li>
<li>Cavapoo (Cavalier King Charles Spaniel and Poodle)</li>
<li>Maltipoo (Maltese and Poodle)</li>
<li>Pomchi (Pomeranian and Chihuahua)</li>
<li>Shichon (Shih Tzu and Bichon Frise) </li>
<li>Shih-Poo (Shih Tzu and Toy Poodle)</li>
<li>Shorkie (Shih Tzhu and Yorkshire Terrier) </li>
<li>Westiepoo (West Highland White Terrier and Poodle)</li>
</ul>
<p>There are 24 combinations to test. We scheme to have four trials for each combination. In other words, we need four subjects from each cross-breed dog type to try one brand of dog food. That makes around 96 volunteered dogs. Each dog is given a 100-gram sample of the new brand (assume no bias - e.g., dog age, dog weight, dog mood, dog health, etc.). Each combination, therefore, makes around 400 grams. Here, we are to measure how many grams are consumed.</p>
<p>Here is what we get (fictitious outcome):</p>

<div class="sourceCode" id="cb412"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb412-1" data-line-number="1"><span class="co"># let us use a fix range of consumed grams between 0 to 1000 grams.</span></a>
<a class="sourceLine" id="cb412-2" data-line-number="2">grams =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb412-3" data-line-number="3">a =<span class="st"> </span><span class="dv">3</span>; b =<span class="st"> </span><span class="dv">8</span>; r =<span class="st"> </span><span class="dv">4</span>;  n =<span class="st"> </span>a <span class="op">*</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>r</a>
<a class="sourceLine" id="cb412-4" data-line-number="4"><span class="co"># let us create three brands of food, with 4 replications  for</span></a>
<a class="sourceLine" id="cb412-5" data-line-number="5"><span class="co"># each cross-breed per brand.</span></a>
<a class="sourceLine" id="cb412-6" data-line-number="6"><span class="kw">set.seed</span>(<span class="dv">1</span>); A =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">2</span>); B =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-8" data-line-number="8"><span class="kw">set.seed</span>(<span class="dv">3</span>); C =<span class="st"> </span><span class="kw">replicate</span>(r, <span class="kw">sample</span>(<span class="dt">x =</span> grams, <span class="dt">size=</span> b, <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb412-9" data-line-number="9">subjects =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dv">8</span>, <span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb412-10" data-line-number="10"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb412-11" data-line-number="11">    subjects[i,<span class="dv">1</span>] =<span class="st"> </span><span class="kw">paste</span>(A[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-12" data-line-number="12">    subjects[i,<span class="dv">2</span>] =<span class="st"> </span><span class="kw">paste</span>(B[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-13" data-line-number="13">    subjects[i,<span class="dv">3</span>] =<span class="st"> </span><span class="kw">paste</span>(C[i,], <span class="dt">collapse=</span><span class="st">&quot;,&quot;</span>)</a>
<a class="sourceLine" id="cb412-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb412-15" data-line-number="15"><span class="kw">colnames</span>(subjects) =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb412-16" data-line-number="16">            <span class="st">&#39;Brand A&#39;</span>, <span class="st">&#39;Brand B&#39;</span>, <span class="st">&#39;Brand C&#39;</span></a>
<a class="sourceLine" id="cb412-17" data-line-number="17">            )</a>
<a class="sourceLine" id="cb412-18" data-line-number="18"><span class="kw">rownames</span>(subjects) =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb412-19" data-line-number="19">            <span class="st">&#39;Cavachon&#39;</span>, <span class="st">&#39;Cavapoo&#39;</span>, <span class="st">&#39;Maltipoo&#39;</span>, <span class="st">&#39;Pomchi&#39;</span>,</a>
<a class="sourceLine" id="cb412-20" data-line-number="20">            <span class="st">&#39;Shichon&#39;</span>, <span class="st">&#39;Shih-Poo&#39;</span>, <span class="st">&#39;Shorkie&#39;</span>, <span class="st">&#39;Westiepoo&#39;</span></a>
<a class="sourceLine" id="cb412-21" data-line-number="21">          )</a>
<a class="sourceLine" id="cb412-22" data-line-number="22">knitr<span class="op">::</span><span class="kw">kable</span>( subjects, </a>
<a class="sourceLine" id="cb412-23" data-line-number="23">    <span class="dt">caption =</span> <span class="st">&#39;Grams consumed by 4 dogs per cross-breed per brand&#39;</span>, </a>
<a class="sourceLine" id="cb412-24" data-line-number="24">    <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-169">Table 6.6: </span>Grams consumed by 4 dogs per cross-breed per brand</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">Brand A</th>
<th align="left">Brand B</th>
<th align="left">Brand C</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="left">26,63,72,26</td>
<td align="left">18,47,98,35</td>
<td align="left">16,58,11,23</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="left">37,6,100,38</td>
<td align="left">70,55,22,49</td>
<td align="left">81,63,71,79</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="left">57,20,38,1</td>
<td align="left">57,55,44,15</td>
<td align="left">38,51,90,60</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="left">91,17,78,38</td>
<td align="left">16,24,7,36</td>
<td align="left">33,51,28,91</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="left">20,69,94,87</td>
<td align="left">95,76,66,97</td>
<td align="left">60,53,23,56</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="left">90,38,21,34</td>
<td align="left">95,18,39,13</td>
<td align="left">61,56,1,76</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="left">95,77,65,48</td>
<td align="left">13,40,84,1</td>
<td align="left">12,87,13,38</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="left">66,50,12,60</td>
<td align="left">84,86,15,16</td>
<td align="left">29,83,9,37</td>
</tr>
</tbody>
</table>

<p>Here, we deal with two factors which we label as <strong>A</strong> and <strong>B</strong>:</p>
<ul>
<li><strong>A</strong> - Dog Food Type</li>
<li><strong>B</strong> - Dog Breed</li>
</ul>
<p>with the following size of each factor:</p>
<ul>
<li><strong>a</strong> - number of food types, where a = 3</li>
<li><strong>b</strong> - number of dog breeds, where b = 8</li>
<li><strong>r</strong> - number of subjects (trials) per combination, where r = 4</li>
<li><strong>n</strong> - grand total number of subjects, where n = abr = <span class="math inline">\(3\times 8 \times 4 = 96\)</span>.</li>
</ul>
<p>With that, let us step through the <strong>Two-Way ANOVA</strong> process.</p>
<p><strong>First</strong>, we need to solve for five <strong>sum of squared values</strong>:</p>
<ul>
<li>Solving sum of squared values for factor A</li>
</ul>

<p><span class="math display">\[\begin{align}
S_A {}&amp;= \frac{ \sum^a (\sum^b\sum^r A)^2 }{b r}  \label{eqn:eqnnumber29} \\
&amp;= \frac{(\sum\text{Brand A})^2 + (\sum\text{Brand B})^2 + (\sum\text{Brand C})^2}{8 \times 4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
   (67 + 58 + 72 + 88 + ... + 81 +  6 + 33 + 73)^2 + \\
   (84 + 80 + 95 + 42 + ... + 92 + 79 + 99 + 82)^2 + \\
   ( 4 + 73 +  4 + 39 + ... + 19 + 28 + 11 + 73)^2
  \end{array}
  \right)
  }
{8 \times 4} \nonumber
\end{align}\]</span>
</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb413-1" data-line-number="1">(<span class="dt">S_A =</span> ( <span class="kw">sum</span>(A)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C)<span class="op">^</span><span class="dv">2</span> ) <span class="op">/</span><span class="st"> </span>(b <span class="op">*</span><span class="st"> </span>r)) </a></code></pre></div>
<pre><code>## [1] 226362</code></pre>
<ul>
<li>Solving sum of squared values for factor B</li>
</ul>

<p><span class="math display">\[\begin{align}
S_B {}&amp;= \frac{ \sum^b (\sum^a\sum^r B)^2 }{a r}  \label{eqn:eqnnumber26} \\
&amp;= \frac{(\sum\text{Cavachon})^2 + (\sum\text{Cavapoo})^2 + ... + (\sum\text{ Westiepoo})^2}{3 \times 4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
  (67 + 58 + 72 + 88 + 84 + 80 + 95 + 42 + 4 + 73 + 4 + 39)^2 + \\
  (38 + 50 + 78 + 43 + 78 + 75 + 49 + 37 + 57 + 54 + 36 + 21)^2 + \\
   ... + \\
  (81 + 6 + 33 + 73 + 92 + 79 + 99 + 82 + 19 + 28 + 11 + 73)^2
  \end{array}
  \right)
  }
{3 \times 4} \nonumber
\end{align}\]</span>
</p>

<div class="sourceCode" id="cb415"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb415-1" data-line-number="1">S_B =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb415-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb415-3" data-line-number="3">    S_B =<span class="st"> </span>S_B <span class="op">+</span><span class="st"> </span>( <span class="kw">sum</span>(A[i,]) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B[i,]) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,]) )<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb415-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb415-5" data-line-number="5">(<span class="dt">S_B =</span> S_B <span class="op">/</span><span class="st"> </span>(a <span class="op">*</span><span class="st"> </span>r))</a></code></pre></div>
<pre><code>## [1] 232082</code></pre>

<ul>
<li>Solving sum of squared values for interaction of A and B</li>
</ul>

<p><span class="math display">\[\begin{align}
S_{AB} {}&amp;= \frac{ \sum^b \sum^a(\sum^r AB)^2 }{r}  \label{eqn:eqnnumber27} \\
&amp;= \frac{
  \left(
  \begin{array}{c}
   (\sum\text{Brand A,Cavachon})^2 + (\sum\text{Brand B, Cavachon})^2 + \\
    ... +\\
    (\sum\text{Brand B,Westiepoo})^2 + (\sum\text{Brand C,Westiepoo})^2
  \end{array}
  \right)
    }{4} \nonumber \\
&amp;= \frac{
  \left(
  \begin{array}{c}
    (67 + 58 + 72 + 88)^2 +  (84 + 80 + 95 + 42)^2  + \\
    ... + \\
    (92 + 79 + 99 + 82)^2 + (19 + 28 + 11 + 73)^2
  \end{array}
  \right)
    }
    {4} \nonumber
\end{align}\]</span>
</p>

<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb417-1" data-line-number="1">S_AB =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb417-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb417-3" data-line-number="3">        S_AB =<span class="st"> </span>S_AB <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(A[i,])<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B[i,])<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,])<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb417-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb417-5" data-line-number="5">(<span class="dt">S_AB =</span> S_AB <span class="op">/</span><span class="st"> </span>r)</a></code></pre></div>
<pre><code>## [1] 246172</code></pre>

<ul>
<li>Solving sum of squared values for each subject</li>
</ul>
<p><span class="math display">\[\begin{align}
S_W {}&amp;= \sum^b \sum^a \sum^r (W)^2 \\
&amp;=  (\text{Brand A,Cavachon, Dog1})^2 + ... + (\text{Brand C,Westiepoo,Dog4})^2  \nonumber \\
&amp;= 67^2 + 58^2 + 72^2 + 88^2 + ... + 19^2 + 28^2 + 11^2 + 73^2 \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb419-1" data-line-number="1">S_W =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb419-2" data-line-number="2"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>b) {</a>
<a class="sourceLine" id="cb419-3" data-line-number="3">    S_W =<span class="st"> </span>S_W <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>( A[i,]<span class="op">^</span><span class="dv">2</span> ) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span> (B[i,]<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C[i,]<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb419-4" data-line-number="4">}</a>
<a class="sourceLine" id="cb419-5" data-line-number="5">S_W</a></code></pre></div>
<pre><code>## [1] 303280</code></pre>

<ul>
<li>Solving sum of squared values for total</li>
</ul>
<p><span class="math display">\[\begin{align}
S_T {}&amp;= (\sum^b \sum^a \sum^r T)^2 \\
&amp;=  \frac{(\text{Brand A,Cavachon, Dog1} + ... + \text{Brand C,Westiepoo, Dog4})^2}
{ 3 \times 8 \times 4} \nonumber \\
&amp;= \frac{(67 + 58 + 72 + 88 + ... + 19 + 28 + 11 + 73)^2}{ 3 \times 8 \times 4 } \nonumber
\end{align}\]</span></p>

<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb421-1" data-line-number="1">(<span class="dt">S_T =</span> ( <span class="kw">sum</span>(A) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(B) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>(C) )<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(a <span class="op">*</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>r))</a></code></pre></div>
<pre><code>## [1] 226010</code></pre>

<p><strong>Second</strong>, let us use the <strong>Two-Way ANOVA Table</strong> (See Table <a href="statistics.html#tab:twowayanova">6.7</a>) <span class="citation">(Larson M. G. <a href="bibliography.html#ref-ref827m">2008</a>; Natoli C. Cory <a href="bibliography.html#ref-ref837c">2017</a>)</span>:</p>

<table>
<caption><span id="tab:twowayanova">Table 6.7: </span>Two-Way ANOVA</caption>
<colgroup>
<col width="15%" />
<col width="17%" />
<col width="20%" />
<col width="27%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Source of Variation</th>
<th align="left">Degrees of Freedom</th>
<th align="left">Sum of Squares</th>
<th align="left">Mean Squares</th>
<th align="left">F</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Between Grps (A)</td>
<td align="left"><span class="math inline">\(df_A: a - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_A: S_A - S_T\)</span></td>
<td align="left"><span class="math inline">\(MS_A: \frac{SS_A}{df_A}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_A}{MS_E}\)</span></td>
</tr>
<tr class="even">
<td align="left">Between Grps (B)</td>
<td align="left"><span class="math inline">\(df_B: b - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_B: S_B - S_T\)</span></td>
<td align="left"><span class="math inline">\(MS_B: \frac{SS_B}{df_B}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_B}{MS_E}\)</span></td>
</tr>
<tr class="odd">
<td align="left">Interaction (AB)</td>
<td align="left"><span class="math inline">\(df_{AB}: (a-1)(b-1)\)</span></td>
<td align="left"><span class="math inline">\(SS_{AB}: S_{AB} - S_A\)</span></td>
<td align="left"><span class="math inline">\(MS_{AB}: \frac{SS_{AB}}{df_{AB}}\)</span></td>
<td align="left"><span class="math inline">\(\frac{MS_{AB}}{MS_E}\)</span></td>
</tr>
<tr class="even">
<td align="left"></td>
<td align="left"></td>
<td align="left"><span class="math inline">\(\text{   }\)</span>- <span class="math inline">\(S_B + S_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Within Grps (Err)</td>
<td align="left"><span class="math inline">\(df_E: ab(r - 1)\)</span></td>
<td align="left"><span class="math inline">\(SS_E: S_W - S_{AB}\)</span></td>
<td align="left"><span class="math inline">\(MS_E: \frac{SS_E}{df_E}\)</span></td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">Total</td>
<td align="left"><span class="math inline">\(df_T: abr - 1\)</span></td>
<td align="left"><span class="math inline">\(SS_T: S_W - S_T\)</span></td>
<td align="left"></td>
<td align="left"></td>
</tr>
</tbody>
</table>

<p>With that, we can patch the values into the table (including the critical values at alpha=0.05):</p>

<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb423-1" data-line-number="1"><span class="co"># Compute for Sum of Squares</span></a>
<a class="sourceLine" id="cb423-2" data-line-number="2">SSA =<span class="st"> </span><span class="kw">round</span>(S_A <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfA =<span class="st"> </span>a <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-3" data-line-number="3">SSB =<span class="st"> </span><span class="kw">round</span>(S_B <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfB =<span class="st"> </span>b <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-4" data-line-number="4">SSAB =<span class="st"> </span><span class="kw">round</span>(S_AB <span class="op">-</span><span class="st"> </span>S_A <span class="op">-</span><span class="st"> </span>S_B <span class="op">+</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfAB =<span class="st"> </span>(a <span class="op">-</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>(b <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb423-5" data-line-number="5">SSW =<span class="st"> </span><span class="kw">round</span>(S_W <span class="op">-</span><span class="st"> </span>S_AB,<span class="dv">2</span>); dfW =<span class="st"> </span>a<span class="op">*</span>b<span class="op">*</span>(r <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb423-6" data-line-number="6">SST =<span class="st"> </span><span class="kw">round</span>(S_W <span class="op">-</span><span class="st"> </span>S_T,<span class="dv">2</span>); dfT =<span class="st"> </span>a<span class="op">*</span>b<span class="op">*</span>r <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb423-7" data-line-number="7"><span class="co"># Compute for Mean Squares</span></a>
<a class="sourceLine" id="cb423-8" data-line-number="8">MSA =<span class="st"> </span><span class="kw">round</span>(SSA<span class="op">/</span>dfA,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-9" data-line-number="9">MSB =<span class="st"> </span><span class="kw">round</span>(SSB<span class="op">/</span>dfB,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-10" data-line-number="10">MSAB =<span class="st"> </span><span class="kw">round</span>(SSAB<span class="op">/</span>dfAB,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-11" data-line-number="11">MSE =<span class="st"> </span><span class="kw">round</span>(SSW <span class="op">/</span><span class="st"> </span>dfW,<span class="dv">2</span>)</a>
<a class="sourceLine" id="cb423-12" data-line-number="12"><span class="co"># Compute for F-statistics</span></a>
<a class="sourceLine" id="cb423-13" data-line-number="13">F_A =<span class="st"> </span><span class="kw">round</span>(MSA<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-14" data-line-number="14">F_B =<span class="st"> </span><span class="kw">round</span>(MSB<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-15" data-line-number="15">F_AB =<span class="st"> </span><span class="kw">round</span>(MSAB<span class="op">/</span>MSE,<span class="dv">4</span>)</a>
<a class="sourceLine" id="cb423-16" data-line-number="16"><span class="co"># Compute for critical values at alpha=0.05 (95% confidence level).</span></a>
<a class="sourceLine" id="cb423-17" data-line-number="17">cv_A =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfA, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-18" data-line-number="18">cv_B =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-19" data-line-number="19">cv_AB =<span class="st"> </span><span class="kw">round</span>(<span class="kw">qf</span>(<span class="fl">0.95</span>, dfAB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-20" data-line-number="20"><span class="co"># Compute for p-value at  alpha=0.05 (95% confidence level).</span></a>
<a class="sourceLine" id="cb423-21" data-line-number="21">pv_A =<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_A, dfA, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-22" data-line-number="22">pv_B=<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_B, dfB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-23" data-line-number="23">pv_AB =<span class="st"> </span><span class="kw">round</span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span><span class="kw">pf</span>(F_AB, dfAB, dfW),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb423-24" data-line-number="24">listing =<span class="st"> </span><span class="kw">c</span>(</a>
<a class="sourceLine" id="cb423-25" data-line-number="25">        <span class="st">&quot;Between Groups (A)&quot;</span>, dfA,   SSA, MSA, F_A, cv_A, pv_A,</a>
<a class="sourceLine" id="cb423-26" data-line-number="26">        <span class="st">&quot;Between Groups (B)&quot;</span>, dfB,   SSB, MSB, F_B, cv_B, pv_B,</a>
<a class="sourceLine" id="cb423-27" data-line-number="27">        <span class="st">&quot;Interaction (AB)&quot;</span>, dfAB, SSAB, MSAB, F_AB, cv_AB, pv_AB,</a>
<a class="sourceLine" id="cb423-28" data-line-number="28">        <span class="st">&quot;Error (Within)&quot;</span>, dfW, SSW, MSE,  <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>,</a>
<a class="sourceLine" id="cb423-29" data-line-number="29">        <span class="st">&quot;Total&quot;</span>, dfT ,SST, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span></a>
<a class="sourceLine" id="cb423-30" data-line-number="30">            )</a>
<a class="sourceLine" id="cb423-31" data-line-number="31">m =<span class="st"> </span><span class="kw">matrix</span>(listing, <span class="dt">nrow=</span><span class="kw">length</span>(listing)<span class="op">/</span><span class="dv">7</span>, <span class="dt">ncol=</span><span class="dv">7</span>, <span class="dt">byrow=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb423-32" data-line-number="32"><span class="kw">colnames</span>(m) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Source of Variation&quot;</span>, <span class="st">&quot;DF&quot;</span>, </a>
<a class="sourceLine" id="cb423-33" data-line-number="33">                  <span class="st">&quot;SS&quot;</span>, <span class="st">&quot;MS&quot;</span>, <span class="st">&quot;F&quot;</span>, <span class="st">&quot;Critical Value&quot;</span>, <span class="st">&quot;P-value&quot;</span> )</a>
<a class="sourceLine" id="cb423-34" data-line-number="34">knitr<span class="op">::</span><span class="kw">kable</span>( m, <span class="dt">caption =</span> <span class="st">&#39;Two-Way ANOVA&#39;</span>, <span class="dt">booktabs =</span> <span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb423-35" data-line-number="35">    <span class="dt">align=</span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&#39;r&#39;</span>,<span class="dt">times=</span><span class="dv">6</span>)), <span class="dt">digits=</span><span class="dv">3</span>, <span class="dt">escape=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<table>
<caption><span id="tab:twowayanova1">Table 6.8: </span>Two-Way ANOVA</caption>
<thead>
<tr class="header">
<th align="right">Source of Variation</th>
<th align="right">DF</th>
<th align="right">SS</th>
<th align="right">MS</th>
<th align="right">F</th>
<th align="right">Critical Value</th>
<th align="right">P-value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">Between Groups (A)</td>
<td align="right">2</td>
<td align="right">352.33</td>
<td align="right">176.16</td>
<td align="right">0.2221</td>
<td align="right">3.124</td>
<td align="right">0.801</td>
</tr>
<tr class="even">
<td align="right">Between Groups (B)</td>
<td align="right">7</td>
<td align="right">6071.96</td>
<td align="right">867.42</td>
<td align="right">1.0936</td>
<td align="right">2.14</td>
<td align="right">0.377</td>
</tr>
<tr class="odd">
<td align="right">Interaction (AB)</td>
<td align="right">14</td>
<td align="right">13738.17</td>
<td align="right">981.3</td>
<td align="right">1.2372</td>
<td align="right">1.832</td>
<td align="right">0.269</td>
</tr>
<tr class="even">
<td align="right">Error (Within)</td>
<td align="right">72</td>
<td align="right">57107.5</td>
<td align="right">793.16</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
<tr class="odd">
<td align="right">Total</td>
<td align="right">95</td>
<td align="right">77269.96</td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
<td align="right"></td>
</tr>
</tbody>
</table>

<p><strong>Finally</strong>, with the three computed <strong>F-statistics</strong>, let us now review our <strong>hypothesis</strong>. At an alpha value of 0.05, we have the following evaluation:</p>
<p>The first <strong>F-statistic</strong> at 0.2221 is greater than 3.124, which is <strong>significant</strong>. Alternatively, our <strong>P-value</strong> at 0.801 is lesser than alpha=0.05; thus it is also <strong>significant</strong>. The other two <strong>F-statistic</strong>, at 1.0936 and 1.2372 respectively, are not significant as they are lesser than their corresponding critical values. That is also proven by their <strong>P-values</strong>, which are both greater than the alpha=0.05.</p>
<p>Therefore, suppose our <strong>null hypotheses</strong> have the corresponding claims:</p>
<ul>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between brands (between groups in A),</li>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between cross-breeds (between groups in B),</li>
<li><span class="math inline">\(\mathbf{H_0}\)</span> - There is no difference between the interaction of brands and cross-breeds.</li>
</ul>
<p>In that case, our <strong>Two-way ANOVA</strong> analysis shows that the first <strong>null hypothesis</strong> is statistically significant. That rejects the <strong>null hypothesis</strong>, which shows a difference between the brands. Also, there is no significant difference in the gram consumption between cross-breeds; neither is there any difference between the interaction of brand and cross-breeds. So to then know which of the brands all the dogs prefer, a starting point is to compare the means of each brand.</p>
<p>The <strong>F distribution</strong> in Figure <a href="statistics.html#fig:fhypo">6.11</a> shows that The <strong>f-value</strong> for <strong>âbetween groupsâ of A</strong> is greater than the <strong>critical value</strong> and therefore it falls within the <strong>rejection region</strong>. That means that the <span class="math inline">\(\mathbf{H_0}\)</span>, <strong>null hypothesis</strong>, is rejected. On the other hand, we fail to reject the <span class="math inline">\(\mathbf{H_0}\)</span>, <strong>null hypothesis</strong>, for both <strong>between-groups of B</strong> and <strong>intersection of A and B</strong>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:fhypo"></span>
<img src="DS_files/figure-html/fhypo-1.png" alt="F Distribution (for Two-Way ANOVA)" width="80%" />
<p class="caption">
Figure 6.11: F Distribution (for Two-Way ANOVA)
</p>
</div>

<p>A faster way to perform a <strong>Two-way ANOVA</strong> without the manual calculations we recently demonstrated is to use the built-in R <strong>aov()</strong>. Here, we use one of the standard datasets in R that comes with library <strong>MASS</strong>: mtcars; other datasets can be listed using the function <strong>data()</strong>. To get information about mtcars, we can use the â?â in R like so:</p>

<blockquote>
<p>&gt; ? mtcars</p>
<p>Description:</p>
<pre><code> The data was extracted from the 1974 _Motor Trend_ US magazine,
 and comprises fuel consumption and 10 aspects of automobile design
 and performance for 32 automobiles (1973-74 models).}</code></pre>
<p>Format:</p>
<pre><code> A data frame with 32 observations on 11 (numeric) variables.

   [, 1]  mpg   Miles/(US) gallon                        
   [, 2]  cyl   Number of cylinders                      
   [, 3]  disp  Displacement (cu.in.)                    
   [, 4]  hp    Gross horsepower                         
   [, 5]  drat  Rear axle ratio                          
   [, 6]  wt    Weight (1000 lbs)                        
   [, 7]  qsec  1/4 mile time                            
   [, 8]  vs    Engine (0 = V-shaped, 1 = straight)      
   [, 9]  am    Transmission (0 = automatic, 1 = manual) 
   [,10]  gear  Number of forward gears                  
   [,11]  carb  Number of carburetors</code></pre>
â¦
</blockquote>

<p>To view the first five records of mtcars, we can use the build-in R function, <strong>head(.)</strong>:</p>

<div class="sourceCode" id="cb426"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb426-1" data-line-number="1"><span class="kw">head</span>(mtcars)</a></code></pre></div>
<pre><code>##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1</code></pre>

<p>To view the internal structure:</p>

<div class="sourceCode" id="cb428"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb428-1" data-line-number="1"><span class="kw">str</span>(mtcars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>

<p>The goal is to examine if each factor, such as the number of cylinders in the car (cyl) or the type of transmission (am), influences the rate of fuel consumption (mpg). Additionally, we examine interactions among factors.</p>
<p>To do that, we use <strong>aov</strong> with an <strong>additive</strong> and an <strong>interaction</strong> formula. Below is an example of an <strong>additive</strong> formula (note that we are using only two factors here - <strong>cyl</strong> and <strong>am</strong>):</p>

<div class="sourceCode" id="cb430"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb430-1" data-line-number="1">(<span class="dt">aov.model =</span> <span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span><span class="kw">as.factor</span>(cyl) <span class="op">+</span><span class="st"> </span><span class="kw">as.factor</span>(am), <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## Call:
##    aov(formula = mpg ~ as.factor(cyl) + as.factor(am), data = mtcars)
## 
## Terms:
##                 as.factor(cyl) as.factor(am) Residuals
## Sum of Squares           824.8          36.8     264.5
## Deg. of Freedom              2             1        28
## 
## Residual standard error: 3.073
## Estimated effects may be unbalanced</code></pre>

<p>It is notable to mention that independent variables have to be <strong>factor</strong> variables; otherwise, they are treated as continuous and not discrete. See below:</p>

<div class="sourceCode" id="cb432"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb432-1" data-line-number="1">(<span class="dt">cyl_factor =</span> <span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##  [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4
## Levels: 4 6 8</code></pre>

<p>We can use the built-in R function called <strong>levels()</strong> to show the list of discrete values of the category (the factor):</p>

<div class="sourceCode" id="cb434"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb434-1" data-line-number="1"><span class="kw">levels</span>(cyl_factor)</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p>Because the variable <strong>cyl</strong> is made into factor variable, there are three distinct levels (n=3), <span class="math inline">\((4,\ 6,\ 8)\)</span> which renders a degree of freedom of 2 = (n - 1).</p>
<p>The other variable has two discrete levels (n = 2): 0 and 1, with a degree of freedom of 1 = (n - 1).</p>

<div class="sourceCode" id="cb436"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb436-1" data-line-number="1">(<span class="dt">am_factor =</span> <span class="kw">as.factor</span>(mtcars<span class="op">$</span>am))</a></code></pre></div>
<pre><code>##  [1] 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 1 1 1 1
## Levels: 0 1</code></pre>

<p>Using the <strong>ANOVA</strong> outcome, a way to summarize the statistic is with the use of the built-in R function, <strong>summary</strong>:</p>

<div class="sourceCode" id="cb438"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb438-1" data-line-number="1"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##                Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## as.factor(cyl)  2    825     412   43.66 2.5e-09 ***
## as.factor(am)   1     37      37    3.89   0.058 .  
## Residuals      28    264       9                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>The summary shows factor <strong>cyl</strong> to be significant with three asterisks &quot;***&quot; significant code only if alpha = 0.001 and that factor <strong>am</strong> is significant with â.â code only if alpha = 0.1.</p>
<p>To also see if there is an interaction between the groups, we use the following <strong>interaction</strong> formula instead:</p>

<div class="sourceCode" id="cb440"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb440-1" data-line-number="1">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor <span class="op">*</span><span class="st"> </span>am_factor, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb440-2" data-line-number="2"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##                      Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## cyl_factor            2    825     412   44.85 3.7e-09 ***
## am_factor             1     37      37    4.00   0.056 .  
## cyl_factor:am_factor  2     25      13    1.38   0.269    
## Residuals            26    239       9                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Notice a third entry in the outcome, <strong>cyl_factor:am_factor</strong>, representing the interaction. However, note that the P-value, <strong>Pr(&gt;F)</strong>, shows no significance. Therefore, we can drop this model and use the <strong>additive</strong> formula instead.</p>
<p>On the other hand, we see that if our alpha is 0.001, then only the <strong>cyl_factor</strong> is significant; therefore, we also can change our formula to only focus on the effect of that single factor variable:</p>

<div class="sourceCode" id="cb442"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb442-1" data-line-number="1">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb442-2" data-line-number="2"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>If our assumption of a <strong>null hypothesis</strong> is that no factors affect the rate of fuel consumption, then we can use the following formula instead:</p>

<div class="sourceCode" id="cb444"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb444-1" data-line-number="1">(<span class="dt">aov.model =</span> <span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span><span class="dv">1</span>, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## Call:
##    aov(formula = mpg ~ 1, data = mtcars)
## 
## Terms:
##                 Residuals
## Sum of Squares       1126
## Deg. of Freedom        31
## 
## Residual standard error: 6.027</code></pre>
<div class="sourceCode" id="cb446"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb446-1" data-line-number="1"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)
## Residuals   31   1126    36.3</code></pre>

</div>
<div id="pearsons-chi-square-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.7</span> Pearsonâs Chi-square Test <a href="statistics.html#pearsons-chi-square-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Chi-square Test</strong>, we deal with multiple independent <strong>nominal or ordinal</strong> variables (or factors). We can use samples from Normal, Binomial, or Poisson distributions. We then compute for the Chi-Square to arrive at a resulting Chi-square distribution for analysis.</p>
<p>Now, to illustrate, let us discuss two types of <strong>Chi-square Test</strong>, namely <span class="math inline">\(\text{One-Factor}\ X^2\ {Test}\)</span> and <span class="math inline">\(\text{Two-Factor}\ X^2\ {Test}\)</span>.</p>
<p><span class="math inline">\(\mathbf{\text{One-Factor}\ X^2\ {Test}}\)</span> (Goodness of Fit Test):</p>
<p>A <span class="math inline">\(\mathbf{\text{One-Factor}\ X^2\ {Test}}\)</span> performs test statistics using the following equation <span class="citation">(McHugh M.L. <a href="bibliography.html#ref-ref847m">2013</a>)</span>:</p>
<p><span class="math display">\[\begin{align}
X^2 = \sum \frac{(observed - expected)^2}{expected} = \sum_i \frac{(O_i - E_i)^2}{E_i}
\end{align}\]</span></p>
<p>Suppose we want to survey the cutest cross-breed dogs preferred by dog owners. So we go out across the country and invite dog owners to participate in our survey. We narrow down the cross-breed type based on the same list of cross-breed dogs as before, but this time with the following <strong>hypothetical</strong> datasets:</p>

<table>
<caption><span id="tab:unnamed-chunk-185">Table 6.9: </span>Observed Cross-Breed Preference</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Observation</th>
<th align="right">Probability</th>
<th align="right">Expected</th>
<th align="right">X^2 = (O - E)^2/E</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="right">310</td>
<td align="right">0.12</td>
<td align="right">333.6</td>
<td align="right">1.67</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="right">400</td>
<td align="right">0.13</td>
<td align="right">361.4</td>
<td align="right">4.123</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="right">240</td>
<td align="right">0.09</td>
<td align="right">250.2</td>
<td align="right">0.416</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="right">400</td>
<td align="right">0.16</td>
<td align="right">444.8</td>
<td align="right">4.512</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="right">420</td>
<td align="right">0.16</td>
<td align="right">444.8</td>
<td align="right">1.383</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="right">430</td>
<td align="right">0.14</td>
<td align="right">389.2</td>
<td align="right">4.277</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="right">300</td>
<td align="right">0.11</td>
<td align="right">305.8</td>
<td align="right">0.11</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="right">280</td>
<td align="right">0.09</td>
<td align="right">250.2</td>
<td align="right">3.549</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="right">2780</td>
<td align="right">1</td>
<td align="right"></td>
<td align="right">20.039</td>
</tr>
</tbody>
</table>

<p>Note that we have assumed some prior probabilities in our calculation which we need to calculate the expected values. For example, the expected value for Cavachon dogs is calculated as such:</p>
<p>E = Total <span class="math inline">\(\times\)</span> Probability = 2780 <span class="math inline">\(\times\)</span> 0.10 = 278.</p>
<p>Relying on the <strong>Chi-square table</strong> (see the Appendix), we see that the <strong>Critical Value</strong> is 20.278 for a confidence level of 95% (alpha=0.05) with a degree of freedom at 7, (n-1).</p>
<div class="sourceCode" id="cb448"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb448-1" data-line-number="1">preference =<span class="st"> </span><span class="kw">c</span>(<span class="dv">310</span>, <span class="dv">400</span>, <span class="dv">240</span>, <span class="dv">400</span>, <span class="dv">420</span>, <span class="dv">430</span>, <span class="dv">300</span>, <span class="dv">280</span>)</a>
<a class="sourceLine" id="cb448-2" data-line-number="2">probability =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.12</span>, <span class="fl">0.13</span>, <span class="fl">0.09</span>, <span class="fl">0.16</span>, <span class="fl">0.16</span>, <span class="fl">0.14</span>, <span class="fl">0.11</span>,<span class="fl">0.09</span>)</a>
<a class="sourceLine" id="cb448-3" data-line-number="3"><span class="kw">chisq.test</span>(preference, <span class="dt">p=</span>probability)</a></code></pre></div>
<pre><code>## 
##  Chi-squared test for given probabilities
## 
## data:  preference
## X-squared = 20, df = 7, p-value = 0.005</code></pre>
<p>The P-value is also given. To validate, let us use <strong>pchisq(.)</strong> using a lower.tail=FALSE, which means that we are interested in the upper tail given the probability condition <span class="math inline">\(P(X^2 &gt; CV)\)</span>; meaning we are looking for the probability that the <strong>chi-square</strong> statistic is greater than the critical value (tabled value).</p>
<div class="sourceCode" id="cb450"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb450-1" data-line-number="1">(<span class="dt">pvalue =</span> <span class="kw">pchisq</span>(<span class="dt">q =</span> chisqr, <span class="dt">df =</span> <span class="dv">7</span>, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.005486</code></pre>
<p>Here, we reject the <strong>null hypothesis</strong> because our P-value is 0.0055 less than alpha=0.05 based on our confidence level. Alternatively, this conclusion is true because the <strong>Chi-square statistic</strong> is greater than the <strong>critical value</strong> (tabled value), <span class="math inline">\(\mathbf{X^2 &gt; CV}\)</span>, which is 20.039 &gt; 14.067. That is more clear as we discuss the next type of test in which we go further with the analysis by using the <strong>Chi-square distribution</strong>.</p>
<p><span class="math inline">\(\mathbf{\text{Two-Factor}\ X^2\ {Test}}\)</span> (Test of Independence):</p>
<p>Let us now have <span class="math inline">\(\mathbf{\text{Two-Factor}\ X^2\ {Test}}\)</span> and compute for the test statistics.</p>
<p>Suppose we want to survey if there is a relationship between the gender of dog owners and the type of dogs preferred. That is to test if two samples are independent (IID). Again, we narrow down to the same list of cross-breed dogs as before, but this time with the following datasets in a <strong>contingency table</strong>:</p>

<table>
<caption><span id="tab:unnamed-chunk-188">Table 6.10: </span>Cross-Breed Preference</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="right">Male</th>
<th align="right">Female</th>
<th align="right">Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Cavachon</td>
<td align="right">140</td>
<td align="right">170</td>
<td align="right">310</td>
</tr>
<tr class="even">
<td align="left">Cavapoo</td>
<td align="right">120</td>
<td align="right">280</td>
<td align="right">400</td>
</tr>
<tr class="odd">
<td align="left">Maltipoo</td>
<td align="right">108</td>
<td align="right">132</td>
<td align="right">240</td>
</tr>
<tr class="even">
<td align="left">Pomchi</td>
<td align="right">200</td>
<td align="right">200</td>
<td align="right">400</td>
</tr>
<tr class="odd">
<td align="left">Shichon</td>
<td align="right">210</td>
<td align="right">210</td>
<td align="right">420</td>
</tr>
<tr class="even">
<td align="left">Shih-Poo</td>
<td align="right">258</td>
<td align="right">172</td>
<td align="right">430</td>
</tr>
<tr class="odd">
<td align="left">Shorkie</td>
<td align="right">75</td>
<td align="right">225</td>
<td align="right">300</td>
</tr>
<tr class="even">
<td align="left">Westiepoo</td>
<td align="right">14</td>
<td align="right">266</td>
<td align="right">280</td>
</tr>
<tr class="odd">
<td align="left">Total</td>
<td align="right">1125</td>
<td align="right">1655</td>
<td align="right">2780</td>
</tr>
</tbody>
</table>

<p>Let us now tabulate the data to calculate our <strong>Chi-square</strong>. The <strong>expected</strong> column is calculated based on:</p>
<p><span class="math display">\[\begin{align}
E_{cell} = \frac{O_j \times O_i}{T}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{O_j}\)</span> is the marginal row total</li>
<li><span class="math inline">\(\mathbf{O_i}\)</span> is the marginal column total</li>
<li><span class="math inline">\(T\)</span> is the grand total</li>
</ul>
<p>For example, to compute the expected value for the Male-Cavachon combination, we perform the following:</p>
<p><span class="math display">\[
E_{male-cavachon} = \frac{310 \times 1125}{2780} = 125.4496
\]</span>
</p>
<table>
<caption><span id="tab:unnamed-chunk-189">Table 6.11: </span>Cross-Breed Preference by Gender</caption>
<thead>
<tr class="header">
<th align="right"><span class="math inline">\(Obs_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(F-Obs_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(Expected_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(Expected_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(X^2_{(M)}\)</span></th>
<th align="right"><span class="math inline">\(X^2_{(F)}\)</span></th>
<th align="right"><span class="math inline">\(X^2\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">140</td>
<td align="right">170</td>
<td align="right">125.45</td>
<td align="right">184.6</td>
<td align="right">1.688</td>
<td align="right">1.147</td>
<td align="right">2.835</td>
</tr>
<tr class="even">
<td align="right">120</td>
<td align="right">280</td>
<td align="right">161.87</td>
<td align="right">238.1</td>
<td align="right">10.831</td>
<td align="right">7.362</td>
<td align="right">18.193</td>
</tr>
<tr class="odd">
<td align="right">108</td>
<td align="right">132</td>
<td align="right">97.12</td>
<td align="right">142.9</td>
<td align="right">1.218</td>
<td align="right">0.828</td>
<td align="right">2.046</td>
</tr>
<tr class="even">
<td align="right">200</td>
<td align="right">200</td>
<td align="right">161.87</td>
<td align="right">238.1</td>
<td align="right">8.982</td>
<td align="right">6.105</td>
<td align="right">15.087</td>
</tr>
<tr class="odd">
<td align="right">210</td>
<td align="right">210</td>
<td align="right">169.96</td>
<td align="right">250.0</td>
<td align="right">9.431</td>
<td align="right">6.411</td>
<td align="right">15.841</td>
</tr>
<tr class="even">
<td align="right">258</td>
<td align="right">172</td>
<td align="right">174.01</td>
<td align="right">256.0</td>
<td align="right">40.539</td>
<td align="right">27.557</td>
<td align="right">68.095</td>
</tr>
<tr class="odd">
<td align="right">75</td>
<td align="right">225</td>
<td align="right">121.40</td>
<td align="right">178.6</td>
<td align="right">17.736</td>
<td align="right">12.056</td>
<td align="right">29.793</td>
</tr>
<tr class="even">
<td align="right">14</td>
<td align="right">266</td>
<td align="right">113.31</td>
<td align="right">166.7</td>
<td align="right">87.039</td>
<td align="right">59.166</td>
<td align="right">146.205</td>
</tr>
<tr class="odd">
<td align="right">1125</td>
<td align="right">1655</td>
<td align="right">NA</td>
<td align="right">NA</td>
<td align="right">177.463</td>
<td align="right">120.632</td>
<td align="right">298.095</td>
</tr>
</tbody>
</table>

<p>Our calculation for the <strong>Degrees of Freedom</strong> is expressed as: <span class="math inline">\(DF = (row - 1)\times(col - 1) = (8-1)\times(2-1) = 7\)</span></p>
<p>Relying on the <strong>Chi-square table</strong> (see the Appendix), the <strong>critical value</strong> is 14.067 for a confidence level of 95% with a degree of freedom at 7. Our <strong>Chi-square statistic</strong> is at 298.0947.</p>
<p>We can use the built-in R function <strong>pchisq</strong> to derive the probability of finding <span class="math inline">\(\mathbf{X^2} \ge CV\)</span>. Here , we have 298.0947 &gt; 14.067:</p>
<div class="sourceCode" id="cb452"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb452-1" data-line-number="1"><span class="kw">pchisq</span>(<span class="dt">q =</span> ct[<span class="dv">9</span>,<span class="dv">7</span>], <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>##     $X^2$ 
## 1.544e-60</code></pre>
<p>The probability is at the extreme right side (right tail), which can also be shown using <strong>PDF</strong> of <strong>Chi-square distribution</strong>.</p>

<div class="sourceCode" id="cb454"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb454-1" data-line-number="1">area &lt;-<span class="st"> </span><span class="cf">function</span>(df, a, b, col) {</a>
<a class="sourceLine" id="cb454-2" data-line-number="2">    area =<span class="st"> </span><span class="kw">seq</span>(a, b, <span class="dt">length.out=</span><span class="dv">50</span>) </a>
<a class="sourceLine" id="cb454-3" data-line-number="3">    x =<span class="st"> </span><span class="kw">c</span>(a, area , b)</a>
<a class="sourceLine" id="cb454-4" data-line-number="4">    y =<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="kw">chi_pdf</span>(area, df), <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb454-5" data-line-number="5">    <span class="kw">polygon</span>(x, y, <span class="dt">col=</span>col, <span class="dt">border=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)  </a>
<a class="sourceLine" id="cb454-6" data-line-number="6">}</a>
<a class="sourceLine" id="cb454-7" data-line-number="7">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">40</span>)</a>
<a class="sourceLine" id="cb454-8" data-line-number="8"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">40</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="fl">0.12</span>), </a>
<a class="sourceLine" id="cb454-9" data-line-number="9">     <span class="dt">xlab=</span><span class="st">&quot;Chi-square value&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;probability density&quot;</span>,</a>
<a class="sourceLine" id="cb454-10" data-line-number="10">     <span class="dt">main=</span><span class="st">&quot;Chi-square Distribution (PDF)&quot;</span>)</a>
<a class="sourceLine" id="cb454-11" data-line-number="11"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>) </a>
<a class="sourceLine" id="cb454-12" data-line-number="12"><span class="co"># Using our CDF implementation of Chi-square PDF</span></a>
<a class="sourceLine" id="cb454-13" data-line-number="13"><span class="kw">curve</span>(<span class="kw">chi_pdf</span>(x, df), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb454-14" data-line-number="14"><span class="co"># Using built-in R function &quot;dchisq&quot;</span></a>
<a class="sourceLine" id="cb454-15" data-line-number="15"><span class="co"># curve(dchisq(x, df), col=&quot;navyblue&quot;, lwd=2, add=TRUE)</span></a>
<a class="sourceLine" id="cb454-16" data-line-number="16"><span class="co"># Chi-square value </span></a>
<a class="sourceLine" id="cb454-17" data-line-number="17"><span class="kw">abline</span>(<span class="dt">v =</span> x2, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb454-18" data-line-number="18"><span class="kw">text</span>(x2, <span class="fl">0.06</span>, <span class="dt">label=</span><span class="kw">paste</span>(<span class="st">&quot;X^2=&quot;</span>, x2), <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-19" data-line-number="19"><span class="co"># Critical Value</span></a>
<a class="sourceLine" id="cb454-20" data-line-number="20"><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">c</span>(cv, <span class="dv">40</span>), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lty=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb454-21" data-line-number="21"><span class="kw">text</span>(cv, <span class="fl">0.06</span>, <span class="dt">label=</span><span class="kw">paste</span>(<span class="st">&quot;cv=&quot;</span>, cv), <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-22" data-line-number="22"><span class="co"># draw the alpha = 0.05</span></a>
<a class="sourceLine" id="cb454-23" data-line-number="23"><span class="kw">area</span>(df, cv, <span class="dv">40</span>, <span class="dt">col=</span><span class="st">&quot;lightblue&quot;</span>)</a>
<a class="sourceLine" id="cb454-24" data-line-number="24"><span class="co"># alpha</span></a>
<a class="sourceLine" id="cb454-25" data-line-number="25"><span class="kw">text</span>(<span class="dv">7</span>, <span class="fl">0.005</span>, <span class="dt">label=</span><span class="st">&quot;alpha=0.05&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb454-26" data-line-number="26"><span class="kw">arrows</span>(<span class="fl">9.5</span>, <span class="fl">0.005</span>, <span class="fl">12.5</span>, <span class="fl">0.005</span>, <span class="dt">length=</span><span class="fl">0.08</span>)</a>
<a class="sourceLine" id="cb454-27" data-line-number="27"><span class="co"># rejection region</span></a>
<a class="sourceLine" id="cb454-28" data-line-number="28"><span class="kw">arrows</span>(cv, <span class="fl">0.02</span>, <span class="dv">40</span>, <span class="fl">0.02</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">code=</span><span class="dv">3</span>, <span class="dt">length=</span><span class="fl">0.08</span>)</a>
<a class="sourceLine" id="cb454-29" data-line-number="29"><span class="kw">text</span>(<span class="dv">31</span>, <span class="fl">0.025</span>, <span class="dt">label=</span><span class="st">&quot;rejection region&quot;</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:chitest1"></span>
<img src="DS_files/figure-html/chitest1-1.png" alt="Chi-square Distribution (PDF)" width="70%" />
<p class="caption">
Figure 6.12: Chi-square Distribution (PDF)
</p>
</div>

<p>It shows that the <strong>Chi-square</strong> value is within the <strong>rejection region</strong> (<span class="math inline">\(\mathbf{X^2} \ge\)</span> 14.067) past greater than the critical value, given a confidence level of 95%; therefore, we reject the <strong>null hypothesis</strong>. Therefore, the <strong>alternative hypothesis</strong> holds.</p>
<p>Our <strong>null hypothesis</strong> as below is rejected:</p>
<p><span class="math display">\[
H_0\ \ \rightarrow \text{there is no association between gender of owner and cross-breed type}
\]</span></p>
<p>Indeed, there is an association which is what the <strong>alternative hypothesis</strong>, <span class="math inline">\(\mathbf{H_1}\)</span>, claims.</p>
</div>
<div id="wilcoxon-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.8</span> Wilcoxon Test  <a href="statistics.html#wilcoxon-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In this section, we discuss non-parametric tests. The idea is that we do not rely on known parameters such as <strong>mean</strong> or <strong>variance</strong>; thus, we cannot assume that our data follows some normal distribution. Here, we do not assume any distribution. Instead, our test is based on rank.</p>
<p>Here, we discuss two kinds of <strong>Wilcoxon tests</strong>, namely <strong>Wilcoxon Rank Sum Test</strong> and <strong>Wilcoxon Signed-Rank Test</strong> <span class="citation">(Harris T and Hardin J.W. <a href="bibliography.html#ref-ref856t">2013</a>)</span>:</p>
<p><strong>Wilcoxon Rank Sum Test</strong>:</p>
<p><strong>Wilcoxon Rank Sum Test</strong> is a non-parametric test. It is also called the <strong>Mann-Whitney test</strong>. To illustrate, let us use the following dataset:</p>

<div class="sourceCode" id="cb455"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb455-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb455-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb455-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb455-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">2</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb455-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb455-6" data-line-number="6"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>)</a>
<a class="sourceLine" id="cb455-7" data-line-number="7">groups</a></code></pre></div>
<pre><code>##         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A    9   10   14   19    8   19   20   15   15     5
## Group B    8    7   15   11   17   12   16   20   11    17</code></pre>

<p>We combine both groups, sort, and then rank each observation.</p>

<div class="sourceCode" id="cb457"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb457-1" data-line-number="1">rank =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,]) ) </a>
<a class="sourceLine" id="cb457-2" data-line-number="2">rank =<span class="st"> </span><span class="kw">rbind</span>( rank, <span class="kw">seq</span>(<span class="dv">1</span>, sample_size<span class="op">*</span><span class="dv">2</span>))</a>
<a class="sourceLine" id="cb457-3" data-line-number="3"><span class="kw">rownames</span>(rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted&quot;</span>, <span class="st">&quot;Ranked&quot;</span> )</a>
<a class="sourceLine" id="cb457-4" data-line-number="4">rank[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display only first 10 columns</span></a></code></pre></div>
<pre><code>##        [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Sorted    5    7    8    8    9   10   11   11   12    14
## Ranked    1    2    3    4    5    6    7    8    9    10</code></pre>

<p>Consecutively identical observations get to share the average of the rank. For example, the first two observations have identical values of 5 and are consecutively ranked 1 and 2; therefore, both will receive a rank of 1.5, derived from (1+2)/2. Moreover, there are three observations with values 11 ranked 10,11,12, respectively; therefore, they will be assigned a rank of 11, derived from (10+11+12)/3.</p>
<p>We then list only the unique observations with their corresponding ranks (this is our rank template):</p>

<div class="sourceCode" id="cb459"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb459-1" data-line-number="1">adjusted_rank =<span class="st"> </span><span class="kw">as.matrix</span>( <span class="kw">aggregate</span>(rank[<span class="dv">2</span>,], <span class="kw">list</span>(rank[<span class="dv">1</span>,]), mean))</a>
<a class="sourceLine" id="cb459-2" data-line-number="2"><span class="kw">colnames</span>(adjusted_rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted Observ.&quot;</span>, <span class="st">&quot;Adjusted Rank&quot;</span>)</a>
<a class="sourceLine" id="cb459-3" data-line-number="3"><span class="kw">t</span>(adjusted_rank)[,<span class="dv">1</span><span class="op">:</span><span class="dv">10</span>] <span class="co"># display only first 10 columns</span></a></code></pre></div>
<pre><code>##                [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Sorted Observ.    5    7  8.0    9   10 11.0   12   14   15    16
## Adjusted Rank     1    2  3.5    5    6  7.5    9   10   12    14</code></pre>

<p>We then assign the adjusted rank back to the original group. An easier way to rank is using the built-in R function <strong>rank()</strong> instead:</p>

<div class="sourceCode" id="cb461"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb461-1" data-line-number="1">builtin.rank =<span class="st"> </span><span class="kw">rank</span>(<span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,]), <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb462"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb462-1" data-line-number="1">ranks =<span class="st"> </span><span class="kw">matrix</span>(builtin.rank, <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">2</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb462-2" data-line-number="2">sum_rank1 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">1</span>])</a>
<a class="sourceLine" id="cb462-3" data-line-number="3">sum_rank2 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb462-4" data-line-number="4">ranked_groups =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], ranks[,<span class="dv">1</span>], groups[<span class="dv">2</span>,], </a>
<a class="sourceLine" id="cb462-5" data-line-number="5">        ranks[,<span class="dv">2</span>]),  <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">4</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb462-6" data-line-number="6">ranked_groups =<span class="st"> </span><span class="kw">rbind</span>(ranked_groups, <span class="kw">c</span>(<span class="ot">NA</span>, sum_rank1, <span class="ot">NA</span>, sum_rank2))</a>
<a class="sourceLine" id="cb462-7" data-line-number="7"><span class="kw">colnames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Rank A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Rank B&quot;</span>)</a>
<a class="sourceLine" id="cb462-8" data-line-number="8"><span class="kw">rownames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>, sample_size), <span class="st">&quot;Sum&quot;</span>)</a>
<a class="sourceLine" id="cb462-9" data-line-number="9">ranked_groups</a></code></pre></div>
<pre><code>##     Group A Rank A Group B Rank B
## 1         9    5.0       8    3.5
## 2        10    6.0       7    2.0
## 3        14   10.0      15   12.0
## 4        19   17.5      11    7.5
## 5         8    3.5      17   15.5
## 6        19   17.5      12    9.0
## 7        20   19.5      16   14.0
## 8        15   12.0      20   19.5
## 9        15   12.0      11    7.5
## 10        5    1.0      17   15.5
## Sum      NA  104.0      NA  106.0</code></pre>

<p>We compute for the <strong>U statistic</strong> for each group using the following formula:</p>
<p><span class="math display">\[\begin{align}
U_{grp} = R_{grp} - \frac{n(n+1)}{2},\ \ \ \ \ where\ R_{grp} = \sum^n_{i=1} rank(x_i)
\end{align}\]</span></p>

<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" data-line-number="1">n =<span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb464-2" data-line-number="2">U_a =<span class="st"> </span>sum_rank1 <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb464-3" data-line-number="3">U_b =<span class="st"> </span>sum_rank2 <span class="op">-</span><span class="st"> </span>n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">2</span> </a>
<a class="sourceLine" id="cb464-4" data-line-number="4"><span class="kw">c</span>(<span class="st">&quot;U_a&quot;</span> =<span class="st"> </span>U_a, <span class="st">&quot;U_b&quot;</span> =<span class="st"> </span>U_b)</a></code></pre></div>
<pre><code>## U_a U_b 
##  49  51</code></pre>

<p>The final U-statistic is based on the group with lesser U-statistic.</p>


<p>Therefore, <span class="math inline">\(U_{stat}\)</span> = 49</p>
<p>As for the hypothesis, let us use a confidence level of 95%.</p>
<p>Given an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A &gt; Group\ B,
\]</span>
our <strong>critical value</strong> is computed using <strong>qwilcox()</strong>:</p>

<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" data-line-number="1">(<span class="dt">U_crit =</span> <span class="dt">U_upper_crit =</span> <span class="kw">qwilcox</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">m =</span> n, <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb466-2" data-line-number="2">                                 <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 72</code></pre>

<p>which shows that <span class="math inline">\(U_{stat}\)</span> &lt; <span class="math inline">\(U_{crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>On the other hand, with an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A &lt; Group\ B,
\]</span></p>
<p>our <strong>critical value</strong> using <strong>qwilcox()</strong> becomes:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" data-line-number="1">(<span class="dt">U_crit =</span> <span class="dt">U_lower_crit =</span> <span class="kw">qwilcox</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">m =</span> n, <span class="dt">n =</span> n, </a>
<a class="sourceLine" id="cb468-2" data-line-number="2">                                 <span class="dt">lower.tail=</span><span class="ot">TRUE</span>))</a></code></pre></div>
<pre><code>## [1] 28</code></pre>
<p>which shows that <span class="math inline">\(U_{stat}\)</span> &gt; <span class="math inline">\(U_{crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>Lastly, with an <strong>alternative hypothesis</strong> as below:</p>
<p><span class="math display">\[
H_1 : Group\ A \ne Group\ B,
\]</span></p>
<p>our <strong>critical value</strong> using <strong>qwilcox()</strong> becomes:</p>

<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;min crit&quot;</span>=U_lower_crit, <span class="st">&quot;max crit&quot;</span>=U_upper_crit)</a></code></pre></div>
<pre><code>## min crit max crit 
##       28       72</code></pre>

<p>which shows that <span class="math inline">\(U_{min\_crit} \le U_{stat} \le U_{max\_crit}\)</span>, rejecting <span class="math inline">\(H_0\)</span>.</p>
<p>In terms of pvalue, for a one-sided upper-tail preference, we get:</p>

<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" data-line-number="1">(<span class="dt">pvalue =</span> <span class="kw">pwilcox</span>(<span class="dt">q =</span> U_stat, <span class="dt">m =</span> n, <span class="dt">n =</span> n) <span class="op">*</span><span class="st"> </span><span class="dv">2</span>)</a></code></pre></div>
<pre><code>## [1] 0.9705</code></pre>

<p>To validate, we can use the built-in R function <strong>wilcox.test()</strong>: </p>

<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb474-1" data-line-number="1"><span class="kw">wilcox.test</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], </a>
<a class="sourceLine" id="cb474-2" data-line-number="2">            <span class="dt">correct=</span><span class="ot">FALSE</span>, <span class="dt">paired=</span><span class="ot">FALSE</span>, <span class="dt">exact=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon rank sum test
## 
## data:  groups[1, ] and groups[2, ]
## W = 49, p-value = 0.9
## alternative hypothesis: true location shift is not equal to 0</code></pre>

<p>Note that we set exact = FALSE. That allows us to perform normal approximation for discrete observations less than 50. Additionally, the function <strong>wilcox.test()</strong> may show message about <strong>P-Value</strong> approximation if observations have ties.</p>
<p><strong>Wilcoxon Signed Rank Test</strong>:</p>
<p><strong>Wilcoxon Signed Rank Test</strong> is the second <strong>Wilcoxon test</strong> we discuss next, and it is also a non-parametric test equivalent to paired <strong>T-Test</strong>. This test utilizes signed-ranks to evaluate the U statistics.  </p>
<p>To illustrate, let us use the same dataset as before but compute for the difference.</p>

<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb476-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb476-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb476-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">2</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb476-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb476-6" data-line-number="6">diff =<span class="st"> </span>groups[<span class="dv">2</span>,] <span class="op">-</span><span class="st"> </span>groups[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb476-7" data-line-number="7">groups =<span class="st"> </span><span class="kw">rbind</span>(groups, diff)</a>
<a class="sourceLine" id="cb476-8" data-line-number="8"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Difference&quot;</span>)</a>
<a class="sourceLine" id="cb476-9" data-line-number="9">groups</a></code></pre></div>
<pre><code>##            [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A       9   10   14   19    8   19   20   15   15     5
## Group B       8    7   15   11   17   12   16   20   11    17
## Difference   -1   -3    1   -8    9   -7   -4    5   -4    12</code></pre>

<p>We then sort and rank the <strong>absolute difference</strong>. The original groups are discarded.</p>

<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" data-line-number="1">diff =<span class="st"> </span><span class="kw">sort</span>( groups[<span class="dv">1</span>,] <span class="op">-</span><span class="st"> </span>groups[<span class="dv">2</span>,])</a>
<a class="sourceLine" id="cb478-2" data-line-number="2">builtin.rank =<span class="st"> </span><span class="kw">rank</span>( <span class="kw">abs</span>( diff ), <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb478-3" data-line-number="3">rank =<span class="st"> </span><span class="kw">rbind</span>( diff, builtin.rank)</a>
<a class="sourceLine" id="cb478-4" data-line-number="4"><span class="kw">rownames</span>(rank) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Difference&quot;</span>, <span class="st">&quot;Rank&quot;</span> )</a>
<a class="sourceLine" id="cb478-5" data-line-number="5">rank</a></code></pre></div>
<pre><code>##            [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Difference  -12   -9   -5 -1.0  1.0    3  4.0  4.0    7     8
## Rank         10    9    6  1.5  1.5    3  4.5  4.5    7     8</code></pre>

<p>Now, for the <strong>signed rank test</strong>, we sum negative groups and positive groups.</p>

<div class="sourceCode" id="cb480"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb480-1" data-line-number="1">mu =<span class="st"> </span><span class="dv">0</span></a>
<a class="sourceLine" id="cb480-2" data-line-number="2">rank_neg =<span class="st"> </span>rank [<span class="dv">2</span>, <span class="kw">which</span>( diff <span class="op">&lt;</span><span class="st"> </span>mu ) ]</a>
<a class="sourceLine" id="cb480-3" data-line-number="3">U_neg =<span class="st">  </span><span class="kw">sum</span>(rank_neg) </a>
<a class="sourceLine" id="cb480-4" data-line-number="4">rank_pos =<span class="st"> </span>rank [<span class="dv">2</span>, <span class="kw">which</span>( diff <span class="op">&gt;</span><span class="st"> </span>mu ) ]</a>
<a class="sourceLine" id="cb480-5" data-line-number="5">U_pos =<span class="st"> </span><span class="kw">sum</span>(rank_pos) </a>
<a class="sourceLine" id="cb480-6" data-line-number="6"><span class="kw">c</span>(<span class="st">&quot;U_neg&quot;</span> =<span class="st"> </span>U_neg, <span class="st">&quot;U_pos&quot;</span> =<span class="st"> </span>U_pos)</a></code></pre></div>
<pre><code>## U_neg U_pos 
##  26.5  28.5</code></pre>

<p>The U-statistic is based on the group with lesser U statistics.</p>


<p>Therefore, <span class="math inline">\(U_{stat}\)</span> = 26.5</p>
<p>As for the hypothesis, let us use a confidence level of 95% with the following claims for a two-tail test:</p>
<p><span class="math display">\[\begin{align*}
H_0 {}&amp;: \text{there is no difference between Group A and B}\\
H_1 &amp;:  \text{there is a difference between  Group A and B}
\end{align*}\]</span></p>
<p>Let us use <strong>qsignrank()</strong> to compute for critical value:</p>

<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" data-line-number="1">U_min =<span class="st"> </span><span class="kw">qsignrank</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">n =</span> n, <span class="dt">lower.tail =</span> <span class="ot">TRUE</span>) </a>
<a class="sourceLine" id="cb482-2" data-line-number="2">U_max =<span class="st"> </span><span class="kw">qsignrank</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">n =</span> n, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb482-3" data-line-number="3"><span class="kw">c</span>(<span class="st">&quot;lower&quot;</span>=U_min, <span class="st">&quot;upper&quot;</span>=U_max)</a></code></pre></div>
<pre><code>## lower upper 
##     9    46</code></pre>

<p>which shows that <span class="math inline">\(U_{min\_crit} \le U_{stat} \le U_{max\_crit}\)</span>, resulting to a rejection of <span class="math inline">\(H_0\)</span>.</p>
<p>Let us now compute for the <strong>z-value</strong> by normalizing and standardizing our U statistic:</p>
<p><span class="math display">\[\begin{align}
Z = \frac{U - E_{H_0}(U)}{\sqrt{VAR_{H_0}(U)}} = \frac{U - \mu_u}{\sigma_u}
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
\mu_u = \frac{n(n+1)}{4}\ \ \ \ \ \ \ \ \sigma_u = \frac{n(n+1)(2n+1)}{24}
\end{align}\]</span></p>

<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" data-line-number="1">n =<span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb484-2" data-line-number="2">mu =<span class="st"> </span>( n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">4</span> )</a>
<a class="sourceLine" id="cb484-3" data-line-number="3">var =<span class="st"> </span>(n <span class="op">*</span><span class="st"> </span>(n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">2</span><span class="op">*</span>n <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))<span class="op">/</span><span class="st"> </span><span class="dv">24</span></a>
<a class="sourceLine" id="cb484-4" data-line-number="4">(<span class="dt">zvalue =</span> (U_stat <span class="op">-</span><span class="st"> </span>mu) <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>(var))</a></code></pre></div>
<pre><code>## [1] -0.1019</code></pre>

<p>And for our <strong>P-value</strong>, we use <strong>pmon()</strong> instead with <strong>Z-value</strong>:</p>

<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb486-1" data-line-number="1"><span class="kw">pnorm</span>( <span class="dt">q =</span> zvalue, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>, <span class="dt">lower.tail=</span><span class="ot">TRUE</span> ) <span class="op">*</span><span class="st"> </span><span class="dv">2</span></a></code></pre></div>
<pre><code>## [1] 0.9188</code></pre>

<p>To validate, we can use the built-in R function <strong>wilcox.test()</strong>:</p>

<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" data-line-number="1"><span class="kw">wilcox.test</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], <span class="dt">alternative=</span><span class="st">&quot;two.sided&quot;</span>, </a>
<a class="sourceLine" id="cb488-2" data-line-number="2">            <span class="dt">correct=</span><span class="ot">FALSE</span>, <span class="dt">paired=</span><span class="ot">TRUE</span>, <span class="dt">exact=</span><span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code>## 
##  Wilcoxon signed rank test
## 
## data:  groups[1, ] and groups[2, ]
## V = 28, p-value = 0.9
## alternative hypothesis: true location shift is not equal to 0</code></pre>

<p>We set the <strong>exact</strong> parameter of the function equal to FALSE to force approximation of the distribution to a normal distribution; though, we do not depend on the distribution. As we can see, the approximation is close to the result of <strong>pnorm(.)</strong>.</p>
<p>There are other <strong>Wilcoxon Tests</strong> available to use. We leave the rest for readers to investigate:</p>
<ul>
<li>Wilcoxon matched-pairs signed-rank test</li>
</ul>
</div>
<div id="kruskal-wallis-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.9</span> Kruskal-Wallis Test <a href="statistics.html#kruskal-wallis-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Kruskal-Wallis</strong> test is also a non-parametric ranked sum test similar to <strong>Mann-Whitney</strong> in that we cannot assume about the distribution (for example, using mean and standard deviation). However, we can also compare two or more groups and rank each observation <span class="citation">(Chan Y. and Walmsley R.P. <a href="bibliography.html#ref-ref865y">1997</a>)</span>.</p>
<p>To illustrate, let us use the same dataset as before.</p>

<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb490-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">5</span>, <span class="dv">20</span>)</a>
<a class="sourceLine" id="cb490-3" data-line-number="3">sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb490-4" data-line-number="4">groups =<span class="st"> </span><span class="kw">t</span>( <span class="kw">replicate</span>(<span class="dt">n=</span><span class="dv">3</span>, <span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, </a>
<a class="sourceLine" id="cb490-5" data-line-number="5">                                  <span class="dt">replace=</span><span class="ot">TRUE</span>)) )</a>
<a class="sourceLine" id="cb490-6" data-line-number="6"><span class="kw">rownames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Group C&quot;</span>)</a>
<a class="sourceLine" id="cb490-7" data-line-number="7">groups</a></code></pre></div>
<pre><code>##         [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]
## Group A   15   11   14   12    7    6    7   11    5    14
## Group B   17   16   18   11   11   13   20   15   13     9
## Group C    8    6   18   20   19    7   10   16   12    13</code></pre>

<p>We then rank the groups in the same manner as shown in <strong>Mann-Whitney test</strong>:</p>

<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" data-line-number="1">builtin.rank =<span class="st"> </span><span class="kw">rank</span>(<span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], groups[<span class="dv">3</span>,]), </a>
<a class="sourceLine" id="cb492-2" data-line-number="2">                    <span class="dt">ties.method=</span><span class="st">&quot;average&quot;</span>)</a>
<a class="sourceLine" id="cb492-3" data-line-number="3">ranks =<span class="st"> </span><span class="kw">matrix</span>(builtin.rank, <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">3</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb492-4" data-line-number="4">R1 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">1</span>]); R2 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">2</span>]); R3 =<span class="st"> </span><span class="kw">sum</span>(ranks[,<span class="dv">3</span>])</a>
<a class="sourceLine" id="cb492-5" data-line-number="5">ranked_groups =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>(groups[<span class="dv">1</span>,], ranks[,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb492-6" data-line-number="6">                          groups[<span class="dv">2</span>,], ranks[,<span class="dv">2</span>],</a>
<a class="sourceLine" id="cb492-7" data-line-number="7">                          groups[<span class="dv">3</span>,], ranks[,<span class="dv">3</span>] ),</a>
<a class="sourceLine" id="cb492-8" data-line-number="8">        <span class="dt">nrow=</span>sample_size, <span class="dt">ncol=</span><span class="dv">6</span>, <span class="dt">byrow=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb492-9" data-line-number="9">ranked_groups =<span class="st"> </span><span class="kw">rbind</span>(ranked_groups,  <span class="kw">c</span>(<span class="ot">NA</span>, R1, <span class="ot">NA</span>, R2, <span class="ot">NA</span>, R3))</a>
<a class="sourceLine" id="cb492-10" data-line-number="10"><span class="kw">colnames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Group A&quot;</span>, <span class="st">&quot;Rank A&quot;</span>, </a>
<a class="sourceLine" id="cb492-11" data-line-number="11">                            <span class="st">&quot;Group B&quot;</span>, <span class="st">&quot;Rank B&quot;</span>,</a>
<a class="sourceLine" id="cb492-12" data-line-number="12">                            <span class="st">&quot;Group C&quot;</span>, <span class="st">&quot;Rank C&quot;</span>)</a>
<a class="sourceLine" id="cb492-13" data-line-number="13"><span class="kw">rownames</span>(ranked_groups) =<span class="st"> </span><span class="kw">c</span>(<span class="kw">seq</span>(<span class="dv">1</span>, sample_size), <span class="st">&quot;Sum&quot;</span>)</a>
<a class="sourceLine" id="cb492-14" data-line-number="14">ranked_groups</a></code></pre></div>
<pre><code>##     Group A Rank A Group B Rank B Group C Rank C
## 1        15   21.5      17   25.0       8    7.0
## 2        11   11.5      16   23.5       6    2.5
## 3        14   19.5      18   26.5      18   26.5
## 4        12   14.5      11   11.5      20   29.5
## 5         7    5.0      11   11.5      19   28.0
## 6         6    2.5      13   17.0       7    5.0
## 7         7    5.0      20   29.5      10    9.0
## 8        11   11.5      15   21.5      16   23.5
## 9         5    1.0      13   17.0      12   14.5
## 10       14   19.5       9    8.0      13   17.0
## Sum      NA  111.5      NA  191.0      NA  162.5</code></pre>

<p>The formula to compute for the K-statistic is as follows:</p>
<p><span class="math display">\[\begin{align}
H = \left(\frac{12}{N(N+1)}\sum^k_{i=1}{\frac{R^2_i}{n_i}}\right) - 3(N + 1)
\end{align}\]</span></p>
<p>Note that the K-statistic assumes a chi-square distribution as shown when using <strong>kruskal.test(.)</strong> function later.</p>

<div class="sourceCode" id="cb494"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb494-1" data-line-number="1">k =<span class="st"> </span><span class="dv">3</span> <span class="co"># total number of groups</span></a>
<a class="sourceLine" id="cb494-2" data-line-number="2">n =<span class="st"> </span>sample_size <span class="co"># size per group</span></a>
<a class="sourceLine" id="cb494-3" data-line-number="3">N =<span class="st"> </span>n <span class="op">*</span><span class="st"> </span>k</a>
<a class="sourceLine" id="cb494-4" data-line-number="4">(<span class="dt">H_stat =</span> (<span class="dv">12</span><span class="op">/</span>(N <span class="op">*</span><span class="st"> </span>(N<span class="op">+</span><span class="dv">1</span>))) <span class="op">*</span><span class="st"> </span>(R1<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>R2<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n <span class="op">+</span><span class="st"> </span>R3<span class="op">^</span><span class="dv">2</span><span class="op">/</span>n) <span class="op">-</span><span class="st"> </span><span class="dv">3</span> <span class="op">*</span><span class="st"> </span>(N <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 4.186</code></pre>

<p>We use <strong>Chi-Square functions</strong> to generate the <strong>Critical value</strong> and <strong>P-value</strong>:</p>

<div class="sourceCode" id="cb496"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb496-1" data-line-number="1">alpha=<span class="fl">0.05</span></a>
<a class="sourceLine" id="cb496-2" data-line-number="2">df =<span class="st"> </span>k <span class="op">-</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb496-3" data-line-number="3">(<span class="dt">H_crit  =</span> <span class="kw">qchisq</span>(<span class="dt">p=</span>alpha, <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 5.991</code></pre>
<div class="sourceCode" id="cb498"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb498-1" data-line-number="1">(<span class="dt">p_value =</span> <span class="kw">pchisq</span>(H_stat, <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>## [1] 0.1233</code></pre>

<p>If our <strong>null hypothesis</strong> is expressed as below (with a confidence level of 95%):</p>
<p><span class="math display">\[
H_0: \text{there is no difference between groups}
\]</span>
then, we reject <span class="math inline">\(H_0\)</span> because the result shows that <span class="math inline">\(H_{stat}\)</span> &lt; <span class="math inline">\(H_{crit}\)</span>. There is significant difference between groups.</p>
<p>To validate, we can use the built-in R function <strong>kruskal.test()</strong>:</p>

<div class="sourceCode" id="cb500"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb500-1" data-line-number="1"><span class="co"># Perform Rank Sum Test</span></a>
<a class="sourceLine" id="cb500-2" data-line-number="2">n =<span class="st"> </span><span class="kw">length</span>(groups[<span class="dv">1</span>,])</a>
<a class="sourceLine" id="cb500-3" data-line-number="3">data =<span class="st"> </span><span class="kw">c</span>(groups[<span class="dv">1</span>,], groups[<span class="dv">2</span>,], groups[<span class="dv">3</span>,])</a>
<a class="sourceLine" id="cb500-4" data-line-number="4">grouping =<span class="st"> </span><span class="kw">as.factor</span> (<span class="kw">c</span>(<span class="kw">rep</span>(<span class="dv">1</span>, n), <span class="kw">rep</span>(<span class="dv">2</span>, n), <span class="kw">rep</span>(<span class="dv">3</span>, n)) )</a>
<a class="sourceLine" id="cb500-5" data-line-number="5"><span class="kw">kruskal.test</span>(data, grouping)</a></code></pre></div>
<pre><code>## 
##  Kruskal-Wallis rank sum test
## 
## data:  data and grouping
## Kruskal-Wallis chi-squared = 4.2, df = 2, p-value = 0.1</code></pre>

</div>
<div id="friedman-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.3.10</span> Friedman Test <a href="statistics.html#friedman-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Friedman</strong> Test is non-parametric test with an <strong>F-statistic</strong> following a <strong>Chi-square distribution</strong>.</p>
<p>We leave readers to investigate <strong>Friedman Test</strong>.</p>
</div>
</div>
<div id="post-hoc-analysis" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.4</span> Post-HOC Analysis <a href="statistics.html#post-hoc-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In our examples, a <strong>null hypothesis</strong> states that there is no difference or no association between groups. If we reject the <strong>null hypothesis</strong>, there is a difference or an association between groups. In certain circumstances, we may need to determine which groups have means that differ and by how much different (how significantly different). Additionally, we may need to run multiple comparison tests when dealing with multiple groups. However, we may come to know that as we run multiple comparisons, the type I error increases (or inflates). Therefore, there is a need to control the inflation while still being able to identify and find differences.</p>
<p>That is where we come to the next process: <strong>Post-HOC</strong> analysis.</p>
<p><strong>Post-HOC</strong> refers to the Latin phrase âafter thisâ (Wikipedia). Statistically, we refer to <strong>Post-HOC</strong> analysis as the next step we take after the result of our experiment.</p>
<p>As a starting point, it helps to be familiar with Figure <a href="statistics.html#fig:testreality">6.13</a>. If our confidence level is at 95%, our threshold for mistakes is at 5% - that is our alpha or type I error - which means that we incorrectly reject a true null hypothesis in about 5 out of 100 observations. On the other hand, our goal is to lean towards a higher <strong>power</strong> - the probability of correctly rejecting a false <strong>null hypothesis</strong>.  </p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:testreality"></span>
<img src="testreality.png" alt="Statistical Error and Power" width="80%" />
<p class="caption">
Figure 6.13: Statistical Error and Power
</p>
</div>
<p>It is essential to be able to measure and control the occurrences of false positives and lean towards achieving more true positives. In particular, we hope to achieve a result in which the <strong>power in significance</strong> has a higher probability than the probability of encountering <strong>type I errors</strong>. For that, let us also review Figure <a href="statistics.html#fig:cohend">6.14</a>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:cohend"></span>
<img src="DS_files/figure-html/cohend-1.png" alt="Effect Size vs Power" width="70%" />
<p class="caption">
Figure 6.14: Effect Size vs Power
</p>
</div>

<p>Figure <a href="statistics.html#fig:cohend">6.14</a> introduces the concept of <strong>effect size</strong> and <strong>power</strong>. As the <strong>effect size</strong> decreases toward the mean of <span class="math inline">\(\mathbf{H_0}\)</span>, then the mean of <span class="math inline">\(\mathbf{H_1}\)</span> - the entire distribution - shifts toward the left, past the critical value; effectively decreasing the right-side tail of <span class="math inline">\(\mathbf{H_1}\)</span> - which also decreases the <strong>power</strong> (or probability). On the other hand, as the <strong>effect size</strong> increases farther away from the mean of <span class="math inline">\(\mathbf{H_0}\)</span>, then the mean of <span class="math inline">\(\mathbf{H_1}\)</span> shifts farther away to the right, also past the critical value; effectively increasing the right-side tail of <span class="math inline">\(\mathbf{H_1}\)</span> - which also increases the <strong>power</strong> (or probability). Note that if the <strong>Power</strong> decreases, the probability of the <strong>Type II error</strong>, <span class="math inline">\(\beta\)</span>, increases. What this means is that the probability of <strong>True Positive outcome</strong>, representing the Power, in which we claim that there is a difference in groups (<span class="math inline">\(\mathbf{H_1}\)</span>), increases as the difference between means, representing the effect size, becomes greater.  </p>
<p><strong>Power of Test</strong> </p>
<p><strong>Power</strong> is defined as the probability that an <strong>alternative hypothesis</strong> holds, granting the <strong>alternative hypothesis</strong> is true. Moreover, it can be expressed as such:</p>
<p><span class="math display">\[\begin{align}
Power = 1 - \beta = P(Reject\ H_0\ |\ H_0\text{ is False})
\end{align}\]</span></p>
<p>Figure <a href="statistics.html#fig:testreality">6.13</a> is a good reference to show where <strong>power</strong> lies.</p>
<p><strong>Effect Size</strong> </p>
<p><strong>Effect size</strong> measures both the magnitude and direction of the <strong>difference or association</strong> between group means. To illustrate, let us introduce two types of groups:</p>
<ul>
<li><p><strong>Experiment Group</strong> - a sample dataset where we expose the subject to treatments; thus, the independent variable is manipulated (adjusted or changed).</p></li>
<li><p><strong>Control Group</strong> - on the other hand, we expose the subjects to normal conditions; thus, the independent variable is constant and controlled.</p></li>
</ul>
<p>When running experiments, it is possible to have multiple experiment groups, each with varying treatments or exposures, while at the same time preserving the normal state of one group, being the control group. After the experiment, we compare the outcomes of each test based on the effect size (magnitude of effect) of the treatments on the subject vs.Â the subjects with no treatment (the control group).</p>
<p>Furthermore, a control group may have two types: a <strong>positive group</strong> and a <strong>negative group</strong>. The positive group validates the effectiveness of the treatment, while the negative group validates the existence of external influences (e.g., contamination).</p>
<p>We can measure <strong>effect size</strong> using Cohenâs d (Cohen 1969) formula:</p>
<p><span class="math display">\[\begin{align}
\text{Effect Size} = d = \frac{(\mu_{experiment})-(\mu_{control})}{\text{standard deviation}}
\end{align}\]</span></p>
<p>Here is a table with the corresponding category (note that other literature may have other ranges for each size category):</p>

<table>
<caption><span id="tab:unnamed-chunk-216">Table 6.12: </span>Effect Size (Cohen 1969)</caption>
<thead>
<tr class="header">
<th align="left">Size (Category)</th>
<th align="left">Effect Size</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Large</td>
<td align="left">0.8</td>
</tr>
<tr class="even">
<td align="left">Medium</td>
<td align="left">0.5</td>
</tr>
<tr class="odd">
<td align="left">Small</td>
<td align="left">0.2</td>
</tr>
</tbody>
</table>

<p>Here, <strong>effect size</strong> is a ratio of standard deviation. An <strong>effect size</strong> of 0.2 implies that the distance of the means of both groups is about 0.2 standard deviation apart. It deviates by that much, and per Cohen, it is categorized as a <strong>small</strong> deviation which is therefore trivial even if ANOVA may show some significance in difference.</p>
<p>The <strong>effect size</strong> cannot easily be adjusted or controlled. On the other hand, perhaps, we can control the occurrence of Type I and type II errors instead.</p>
<p><strong>Error Rate</strong> </p>
<p>There are three common ways to measure error rates and thus be able to control them <span class="citation">(Benjamini Y. and Hochberg Y. <a href="bibliography.html#ref-ref884y">1994</a>)</span>:</p>
<ul>
<li><strong>Family-Wise Error Rate (FWER)</strong> measures the probability of encountering at least one Type I error.</li>
</ul>
<p><span class="math display">\[\begin{align}
FWER = P(\text{at least one type I error}) = 1 - P (type\ I\ error = 0)
\end{align}\]</span></p>
<ul>
<li><strong>False Discovery Rate (FDR)</strong> measures the proportion of incorrectly rejecting a true <strong>null hypothesis</strong> overall rejections. That is a measure of the rate of Type I error. See Figure <a href="statistics.html#fig:testreality">6.13</a>.  </li>
</ul>
<p><span class="math display">\[\begin{align}
FDR = \frac{\text{false positive}}{\text{false and true positive}} = \frac{FP}{FP + TP}
\end{align}\]</span></p>
<ul>
<li><strong>True Discovery Rate (TDR)</strong> measures the proportion of correctly rejecting a false <strong>null hypothesis</strong> over all rejections. This is a measure of the rate of <strong>power in significance</strong>. See Figure <a href="statistics.html#fig:testreality">6.13</a>.  </li>
</ul>
<p><span class="math display">\[\begin{align}
TDR = \frac{\text{true positive}}{\text{false and true positive}} = \frac{TP}{FP + TP}
\end{align}\]</span></p>
<p>When it comes to Control, two terms are being used to describe a method or process:</p>
<ul>
<li><p><strong>conservative method</strong> - a method or process that tends toward enforcing stringent Control over Type I errors. We protect against the occurrence of false positives. While we prefer to have a probability (the p-value) of encountering Type I errors less than the nominal (significance) level (the alpha), too much protection, however, means that we may fail to discover occurrences of significant results.</p></li>
<li><p><strong>anticonservative method</strong> - a method or process that can relax the Control and accept a higher rate (or probability) of encountering at least one Type I error - that makes it towards <strong>more Power</strong> or higher probability of hitting true positives - that is discovering statistically significant results.</p></li>
</ul>
<p>One method that demonstrates a conservative approach is the <strong>Bonferroni Correction</strong>, discussed next.</p>
<div id="bonferroni-correction" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.4.1</span> Bonferroni Correction <a href="statistics.html#bonferroni-correction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Bonferroni Correction</strong> is simply a correction to <strong>P-values</strong>, which relies on FWER. The corrected <strong>P-values</strong> are called the <strong>adjusted P-values</strong> <span class="citation">(Armstrong R. A. <a href="bibliography.html#ref-ref874r">2014</a>; Chen S., Feng Z., and Yi X. <a href="bibliography.html#ref-ref894s">2017</a>)</span>]. To understand the need for correction, let us use a <strong>fictitious dataset</strong> with one factor variable of five groups of size ten each:</p>

<div class="sourceCode" id="cb502"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb502-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb502-2" data-line-number="2">m =<span class="st"> </span><span class="dv">5</span> <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb502-3" data-line-number="3">p =<span class="st"> </span><span class="dv">2</span> <span class="co"># pairwise comparison</span></a>
<a class="sourceLine" id="cb502-4" data-line-number="4">n =<span class="st"> </span>sample_size =<span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb502-5" data-line-number="5">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">100</span>,<span class="dv">300</span>)  <span class="co"># metrics</span></a>
<a class="sourceLine" id="cb502-6" data-line-number="6">groups =<span class="st"> </span><span class="kw">replicate</span>(<span class="dt">n=</span>m, <span class="kw">sample</span>(<span class="dt">x =</span> range, <span class="dt">size=</span> sample_size, </a>
<a class="sourceLine" id="cb502-7" data-line-number="7">                               <span class="dt">replace=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb502-8" data-line-number="8"><span class="kw">colnames</span>(groups) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&#39;A&#39;</span>,<span class="st">&#39;B&#39;</span>,<span class="st">&#39;C&#39;</span>,<span class="st">&#39;D&#39;</span>,<span class="st">&#39;E&#39;</span>)</a>
<a class="sourceLine" id="cb502-9" data-line-number="9">groups</a></code></pre></div>
<pre><code>##         A   B   C   D   E
##  [1,] 153 141 287 196 265
##  [2,] 174 135 142 220 230
##  [3,] 215 238 230 199 257
##  [4,] 282 177 125 137 211
##  [5,] 140 254 153 266 206
##  [6,] 280 200 177 234 258
##  [7,] 289 244 102 259 104
##  [8,] 232 299 176 121 195
##  [9,] 226 176 274 245 247
## [10,] 112 256 168 182 239</code></pre>

<p>With five groups, we are looking at ten pairwise comparisons:</p>
<p><span class="math display">\[
AB\ \ AC\ \ AD\ \ AE\ \ BC\ \ BD\ \ BE\ \ CD\ \ CE\ \ DE
\]</span></p>
<p>The number of comparisons can be calculated using the following:</p>
<p><span class="math display">\[\begin{align}
Comparisons = \frac{m!}{p!(m-p)!} = \frac{m (m-1)}{p}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>m</strong> is the total number of groups</li>
<li><strong>p</strong> is the number of groups to compare. This is 2 for pairwise comparison.</li>
</ul>

<div class="sourceCode" id="cb504"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb504-1" data-line-number="1">(<span class="dt">comparisons =</span> <span class="kw">factorial</span>(m) <span class="op">/</span><span class="st"> </span>( <span class="kw">factorial</span> (p) <span class="op">*</span><span class="st"> </span><span class="kw">factorial</span>(m<span class="op">-</span>p)))</a></code></pre></div>
<pre><code>## [1] 10</code></pre>

<p>We know that <span class="math inline">\(\alpha\)</span> represents the probability of making a <strong>Type I error</strong>:</p>
<p><span class="math display">\[
P(\text{Type I error}) = \alpha = 0.05
\]</span></p>
<p>And inversely, a confidence interval of 95%, <span class="math inline">\(1 - \alpha\)</span>, is the probability of not making a <strong>Type I error</strong>:</p>
<p><span class="math display">\[
P(\text{not a Type I error}) = 1 - \alpha = 0.95
\]</span></p>
<p>Those two probabilities are actual if we are performing one pairwise comparison test. If we are performing more than one test, however, then the probability of making <strong>at least one</strong> <strong>Type I error</strong> is then expressed as:</p>
<p><span class="math display">\[\begin{align}
P(\text{at least one Type I error}) = 1 - P_{(overall)}(\text{not a Type I error})
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
 P_{(overall)}(\text{not a Type I error}) = 
    &amp;P_{AB}(\text{not a Type I error}) \times \nonumber \\ 
    &amp;P_{AC}(\text{not a Type I error}) \times \nonumber \\
    &amp;P_{AD}(\text{not a Type I error}) \times ...\times \nonumber \\
    &amp;P_{DE}(\text{not a Type I error})
\end{align}\]</span></p>
<p>which is equivalent to:</p>
<p><span class="math display">\[\begin{align}
P_{(overall)}(\text{not a Type I error}) = P_{(comparison)}(\text{not a Type I error})^n
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li>n is the number of pairwise comparisons.</li>
</ul>
<p>If we then use a confidence level of 95% for a factor variable with five groups (10 comparisons), we get this:</p>
<p><span class="math display">\[
P_{(overall)}(\text{not a Type I error}) = (0.95)^{10} = 0.4012631 = 40.13\%
\]</span></p>
<p>That shows a 40.13% probability of not making a <strong>Type I error</strong> for ten comparison tests, which also means a 59.87% (1 - 40.13%) probability of making at least one <strong>Type I error</strong>. That is a very high probability and is problematic.</p>
<p><span class="math display">\[
P(\text{at least one Type I error}) = 1 - 0.4012631 = 0.5987369 = 59.87\%
\]</span></p>
<p>To fix that, we apply <strong>Bonferroniâs correction</strong>.</p>
<p>Assume we settle on <span class="math inline">\(\alpha=0.05\)</span>, a <strong>Bonferroni adjustment</strong> is expressed as:</p>
<p><span class="math display">\[
\alpha_c = \frac{\alpha}{\text{no of tests}} = \frac{0.05}{10} = 0.005
\]</span></p>
<p>or for our confidence level of 95%, we see the following adjustment for one comparison test:</p>
<p><span class="math display">\[
1 - \alpha_c = 1 - 0.005 = 0.995\ \ \ \rightarrow \ 99.5\%\ \text{confidence level}
\]</span></p>
<p>Applying the <strong>adjustment</strong> for ten comparison tests, we get the following:</p>
<p><span class="math display">\[\begin{align*}
P(\text{at least one Type I error}) {}&amp;= 1 - ( 1 - \alpha_c)^{10}\\
&amp;= 1 - (1 - 0.005)^{10} = 0.04888987 = 4.89\%
\end{align*}\]</span></p>
<p>To illustrate, let us use the same <strong>mtcars</strong> dataset to perform a pairwise T-test comparison with <strong>Bonferroni correction</strong> using a built-in R function called <strong>pairwise.t.test()</strong>:</p>

<div class="sourceCode" id="cb506"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb506-1" data-line-number="1"><span class="kw">pairwise.t.test</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl, <span class="dt">p.adj=</span><span class="st">&quot;bonferroni&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  mtcars$mpg and mtcars$cyl 
## 
##   4     6   
## 6 4e-04 -   
## 8 3e-09 0.01
## 
## P value adjustment method: bonferroni</code></pre>

<p>The result shows that the following pairs render the corresponding adjusted p-values:</p>
<ul>
<li>6-4 has adj. p-value of 0.00036</li>
<li>8-4 has adj. p-value of 2.6e-09</li>
<li>8-6 has adj. p-value of 0.01246</li>
</ul>
<p>Other adjustments used by the <strong>pairwise.t.test()</strong> includes <strong>Holm Adjustment</strong>:</p>
<div class="sourceCode" id="cb508"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb508-1" data-line-number="1"><span class="kw">pairwise.t.test</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl, <span class="dt">p.adj=</span><span class="st">&quot;holm&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Pairwise comparisons using t tests with pooled SD 
## 
## data:  mtcars$mpg and mtcars$cyl 
## 
##   4     6    
## 6 2e-04 -    
## 8 3e-09 0.004
## 
## P value adjustment method: holm</code></pre>
<p>To see other adjustments, we can use <strong>p.adjust.methods</strong>:</p>
<div class="sourceCode" id="cb510"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb510-1" data-line-number="1">p.adjust.methods</a></code></pre></div>
<pre><code>## [1] &quot;holm&quot;       &quot;hochberg&quot;   &quot;hommel&quot;     &quot;bonferroni&quot; &quot;BH&quot;        
## [6] &quot;BY&quot;         &quot;fdr&quot;        &quot;none&quot;</code></pre>
</div>
<div id="benjamini-hochberg-correction" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.4.2</span> Benjamini-Hochberg Correction <a href="statistics.html#benjamini-hochberg-correction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Benjamini-Hochberg (BH)</strong> method controls the <strong>false discovery rate (FDR)</strong>. It limits the number of false positives. Similar to <strong>Bonferroni correction</strong>, the <strong>BH</strong> correction adjusts p-values <span class="citation">(Chen S., Feng Z., and Yi X. <a href="bibliography.html#ref-ref894s">2017</a>)</span>.</p>
<p>To illustrate, let us directly assume a list of P-values corresponding to the number of experiments.</p>

<div class="sourceCode" id="cb512"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb512-1" data-line-number="1">no_of_experiments=<span class="dv">7</span></a>
<a class="sourceLine" id="cb512-2" data-line-number="2">pvalues =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">round</span>(<span class="kw">runif</span>(no_of_experiments, <span class="dt">min=</span><span class="fl">0.001</span>, <span class="dt">max=</span><span class="fl">0.10</span>),<span class="dv">5</span>)) </a>
<a class="sourceLine" id="cb512-3" data-line-number="3">pvalues =<span class="st"> </span><span class="kw">rbind</span>(pvalues, <span class="kw">seq</span>(<span class="dv">1</span>, no_of_experiments))</a>
<a class="sourceLine" id="cb512-4" data-line-number="4"><span class="kw">rownames</span>(pvalues) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted Pvalues&quot;</span>, <span class="st">&quot;Rank&quot;</span>)</a>
<a class="sourceLine" id="cb512-5" data-line-number="5"><span class="kw">colnames</span>(pvalues) =<span class="st"> </span><span class="kw">paste0</span>(<span class="st">&quot;R&quot;</span>, <span class="kw">seq</span>(<span class="dv">1</span>, no_of_experiments))</a>
<a class="sourceLine" id="cb512-6" data-line-number="6">pvalues</a></code></pre></div>
<pre><code>##                   R1      R2      R3      R4      R5      R6      R7
## Sorted Pvalues 0.008 0.01085 0.02523 0.03231 0.04437 0.04828 0.08626
## Rank           1.000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000</code></pre>

<p>One way to adjust the P-value is to use an <strong>FDR</strong> threshold using this formula:</p>
<p><span class="math display">\[\begin{align}
Adj. p = p \times \frac{R_{(max)}}{R_{p}}
\end{align}\]</span></p>
<p>For example, to adjust the P-value 0.0863 with rank 7, we perform the following calculation:</p>

<div class="sourceCode" id="cb514"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb514-1" data-line-number="1">max_rank =<span class="st"> </span><span class="kw">max</span>(pvalues[<span class="dv">2</span>,])</a>
<a class="sourceLine" id="cb514-2" data-line-number="2">pvalue_rank =<span class="st"> </span>pvalues[<span class="dv">2</span>,<span class="dv">6</span>]</a>
<a class="sourceLine" id="cb514-3" data-line-number="3">(pvalues[<span class="dv">1</span>,<span class="dv">6</span>] <span class="op">*</span><span class="st"> </span>max_rank <span class="op">/</span><span class="st"> </span>pvalue_rank)</a></code></pre></div>
<pre><code>## [1] 0.05633</code></pre>

<p>To adjust all the P-values, we use the following R code:</p>

<div class="sourceCode" id="cb516"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb516-1" data-line-number="1">p =<span class="st"> </span>pvalues[<span class="dv">1</span>,]</a>
<a class="sourceLine" id="cb516-2" data-line-number="2">rank =<span class="st"> </span>pvalues[<span class="dv">2</span>,]</a>
<a class="sourceLine" id="cb516-3" data-line-number="3">max_rank =<span class="st"> </span><span class="kw">max</span>(rank)</a>
<a class="sourceLine" id="cb516-4" data-line-number="4">adj_pvalues =<span class="st"> </span>p <span class="op">*</span><span class="st"> </span>max_rank <span class="op">/</span><span class="st"> </span>rank</a>
<a class="sourceLine" id="cb516-5" data-line-number="5">pvalues =<span class="st"> </span><span class="kw">rbind</span>(pvalues, adj_pvalues)</a>
<a class="sourceLine" id="cb516-6" data-line-number="6"><span class="kw">rownames</span>(pvalues) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Sorted Pvalues&quot;</span>, <span class="st">&quot;Rank&quot;</span>, <span class="st">&quot;Adj Pvalues&quot;</span>)</a>
<a class="sourceLine" id="cb516-7" data-line-number="7">pvalues</a></code></pre></div>
<pre><code>##                   R1      R2      R3      R4      R5      R6      R7
## Sorted Pvalues 0.008 0.01085 0.02523 0.03231 0.04437 0.04828 0.08626
## Rank           1.000 2.00000 3.00000 4.00000 5.00000 6.00000 7.00000
## Adj Pvalues    0.056 0.03798 0.05887 0.05654 0.06212 0.05633 0.08626</code></pre>

</div>
</div>
<div id="multiple-comparison-tests" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.5</span> Multiple Comparison Tests <a href="statistics.html#multiple-comparison-tests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>We now discuss a few popular <strong>multiple comparison tests</strong> in statistics <span class="citation">(Rodger R.S. and Roberts M. <a href="bibliography.html#ref-ref912r">2013</a>; Lee S. and Lee D. K. <a href="bibliography.html#ref-ref903s">2018</a>)</span>. The idea is essentially to compare groups and determine which groups contribute to a significant difference. Note that we can only use any of the <strong>Post-HOC</strong> tests if we determine from our previous analysis, e.g., <strong>ANOVA</strong>, that the <strong>null hypothesis</strong> does not hold.</p>
<p><span class="math display">\[\begin{align}
\mathbf{H_0} {}&amp;: \ \ \ \mu_i = \mu_j\ \ \ \leftarrow\ \ \ rejected\\
\mathbf{H_1} &amp;: \ \ \ \ \mu_i \ne \mu_j\ \ \ \leftarrow\ \ \ this\ holds
\end{align}\]</span></p>
<p>A key point to emphasize here is that if we run our <strong>ANOVA</strong> test against hundred groups (supposition) and have determined that there are significant differences amongst groups, it may help to use one of the following <strong>multiple comparisons</strong> approaches:</p>
<ul>
<li>use a few groups for comparison instead of all of them</li>
<li>or compare all of them</li>
<li>or use one of them as a control group and compare it against the other experimental groups.</li>
<li>or compare a subset of groups orthogonal to another subset of groups.</li>
</ul>
<p>A few of the methods that follow next in our discussion, such as <strong>Tukeyâs Test</strong> and <strong>Dunnettâs Test</strong>, fall under any of those approaches.</p>
<div id="scheffes-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.1</span> Scheffeâs Test <a href="statistics.html#scheffes-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Scheffeâs Test</strong> is a <strong>Post-HOC</strong> multiple comparison test. We use the <strong>mtcars</strong> dataset as before to illustrate a pairwise comparison.</p>
<p><strong>First</strong>, let us determine the groups, e.g. <span class="math inline">\((4,\ 6,\ 8)\)</span>, to be compared from a factor variable, e.g. <strong>cyl</strong> (number of cylinders).</p>

<div class="sourceCode" id="cb518"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb518-1" data-line-number="1">(<span class="dt">groups =</span> <span class="kw">levels</span>(<span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl)))</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p><strong>Second</strong>, let us perform a <strong>One-Way ANOVA</strong> against the corresponding scaled metrics, <strong>mpg</strong> (fuel consumption), using our implementation: </p>

<div class="sourceCode" id="cb520"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb520-1" data-line-number="1">mtc =<span class="st"> </span><span class="kw">list</span>()</a>
<a class="sourceLine" id="cb520-2" data-line-number="2">m =<span class="st"> </span><span class="kw">length</span>(groups)</a>
<a class="sourceLine" id="cb520-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) { mtc[[i]] =<span class="st"> </span>mtcars[mtcars<span class="op">$</span>cyl <span class="op">==</span><span class="st"> </span>groups[i],]<span class="op">$</span>mpg }</a>
<a class="sourceLine" id="cb520-4" data-line-number="4">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtc))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>
<div class="sourceCode" id="cb522"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb522-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p><strong>Third</strong>, compute for the <strong>Critical value</strong> of the F-test (assume a 95% confidence level):</p>

<div class="sourceCode" id="cb524"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb524-1" data-line-number="1">(<span class="dt">cv_anova =</span> <span class="kw">qf</span>(<span class="fl">0.95</span>,  anova[<span class="st">&quot;dfB&quot;</span>],  anova[<span class="st">&quot;dfW&quot;</span>]) )</a></code></pre></div>
<pre><code>## [1] 3.328</code></pre>

<p><strong>Fourth</strong>, compute for the <strong>Critical value</strong> of the <strong>Scheffe Test</strong> using:</p>
<p><span class="math display">\[\begin{align}
F&#39; = dfB \times (\text{Critical value of F-test}) 
\end{align}\]</span></p>
<div class="sourceCode" id="cb526"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb526-1" data-line-number="1">(<span class="dt">cv_scheffe =</span> <span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfB&quot;</span>]) <span class="op">*</span><span class="st"> </span>cv_anova )</a></code></pre></div>
<pre><code>## [1] 6.655</code></pre>
<p><strong>Finally</strong>, with the <strong>ANOVA</strong> result, let us perform a <strong>Post-HOC</strong> analysis using <strong>Scheffeâs Method</strong>:</p>
<p><span class="math display">\[\begin{align}
F_{scheffe} = \frac{(\bar{x}_i - \bar{x}_j)^2}{s^2_w\left(\frac{1}{n_i} + \frac{1}{n_j}\right)}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{\bar{x}_i}\)</span> is the mean of the first group being compared</li>
<li><span class="math inline">\(\mathbf{\bar{x}_j}\)</span> is the mean of the second group being compared</li>
<li><span class="math inline">\(\mathbf{n_i}\)</span> is the group size of the first group being compared</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the group size of the second group being compared</li>
<li><span class="math inline">\(\mathbf{F_{scheffe}}\)</span> is the F statistic of Scheffe Test</li>
<li><span class="math inline">\(\mathbf{s^2_w}\)</span> is the within-group mean square (MSW or MSE) error, the outcome from ANOVA</li>
</ul>
<p>Below is a naive implementation of <strong>Scheffeâs comparison</strong> test in R code:</p>

<div class="sourceCode" id="cb528"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb528-1" data-line-number="1">scheffe.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb528-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(factor)</a>
<a class="sourceLine" id="cb528-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, factor)</a>
<a class="sourceLine" id="cb528-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb528-5" data-line-number="5">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb528-6" data-line-number="6">  scheffe =<span class="st"> </span><span class="kw">c</span>();  rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb528-7" data-line-number="7">  <span class="co"># Critical value for the F-Test at %95</span></a>
<a class="sourceLine" id="cb528-8" data-line-number="8">  cv_anova =<span class="st"> </span><span class="kw">qf</span>(<span class="fl">0.95</span>,  anova[<span class="st">&quot;dfB&quot;</span>],  anova[<span class="st">&quot;dfW&quot;</span>]) </a>
<a class="sourceLine" id="cb528-9" data-line-number="9">  <span class="co"># Critical value for Scheffe Test</span></a>
<a class="sourceLine" id="cb528-10" data-line-number="10">  cv_scheffe=<span class="st"> </span><span class="kw">as.numeric</span>( anova[<span class="st">&quot;dfB&quot;</span>] <span class="op">*</span><span class="st"> </span>cv_anova )</a>
<a class="sourceLine" id="cb528-11" data-line-number="11">  <span class="co"># Get the s2w</span></a>
<a class="sourceLine" id="cb528-12" data-line-number="12">  s2w =<span class="st"> </span>anova[<span class="st">&quot;MSW&quot;</span>]</a>
<a class="sourceLine" id="cb528-13" data-line-number="13">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb528-14" data-line-number="14">    n.i =<span class="st"> </span><span class="kw">length</span>(groups[[i]]);  u.i =<span class="st"> </span><span class="kw">mean</span>(groups[[i]]);  </a>
<a class="sourceLine" id="cb528-15" data-line-number="15">    g.i =<span class="st"> </span>factors[i]</a>
<a class="sourceLine" id="cb528-16" data-line-number="16">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb528-17" data-line-number="17">      <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb528-18" data-line-number="18">        n.j =<span class="st"> </span><span class="kw">length</span>(groups[[j]]);  u.j =<span class="st"> </span><span class="kw">mean</span>(groups[[j]]); </a>
<a class="sourceLine" id="cb528-19" data-line-number="19">        g.j =<span class="st"> </span>factors[j]</a>
<a class="sourceLine" id="cb528-20" data-line-number="20">        d   =<span class="st"> </span>u.j <span class="op">-</span><span class="st"> </span>u.i <span class="co"># difference</span></a>
<a class="sourceLine" id="cb528-21" data-line-number="21">        f_scheffe =<span class="st"> </span>(u.i <span class="op">-</span><span class="st"> </span>u.j)<span class="op">^</span><span class="dv">2</span><span class="op">/</span><span class="st"> </span>( s2w <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.j))  </a>
<a class="sourceLine" id="cb528-22" data-line-number="22">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb528-23" data-line-number="23">        <span class="cf">if</span> (f_scheffe <span class="op">&gt;</span><span class="st"> </span>cv_scheffe) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb528-24" data-line-number="24">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i, signif))</a>
<a class="sourceLine" id="cb528-25" data-line-number="25">        scheffe =<span class="st"> </span><span class="kw">rbind</span>(scheffe, </a>
<a class="sourceLine" id="cb528-26" data-line-number="26">                  <span class="kw">c</span>(<span class="kw">round</span>(d,<span class="dv">5</span>),  f_scheffe, cv_scheffe ))</a>
<a class="sourceLine" id="cb528-27" data-line-number="27">      }</a>
<a class="sourceLine" id="cb528-28" data-line-number="28">    }</a>
<a class="sourceLine" id="cb528-29" data-line-number="29">  }</a>
<a class="sourceLine" id="cb528-30" data-line-number="30">  <span class="kw">colnames</span>(scheffe) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;diff&quot;</span>,  <span class="st">&quot;F&quot;</span>, <span class="st">&quot;critical value&quot;</span> )</a>
<a class="sourceLine" id="cb528-31" data-line-number="31">  <span class="kw">rownames</span>(scheffe) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb528-32" data-line-number="32">  scheffe</a>
<a class="sourceLine" id="cb528-33" data-line-number="33">}</a>
<a class="sourceLine" id="cb528-34" data-line-number="34"><span class="kw">scheffe.comparison</span>(mtcars<span class="op">$</span>mpg, <span class="kw">as.factor</span>( mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##         diff      F critical value
## 6-4*  -6.921 19.723          6.655
## 8-4* -11.564 79.291          6.655
## 8-6*  -4.643  9.683          6.655</code></pre>

<p>Here, an asterisk(*) implies a significant difference between the pair of means because the f-statistic is greater than the critical value.</p>
</div>
<div id="fishers-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.2</span> Fisherâs Test <a href="statistics.html#fishers-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Fisherâs Test</strong>, also called <strong>Tukeyâs LSD (least significant difference)</strong>, is another <strong>Post-HOC</strong> multiple comparison test.</p>
<p><strong>First</strong>, let us perform a <strong>One-Way ANOVA</strong> using our R implementation. Here, we use the <strong>mtcars</strong> dataset as before to illustrate a pairwise comparison:</p>

<div class="sourceCode" id="cb530"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb530-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p>We can validate the result using <strong>aov()</strong>. First and foremost, we convert one of the variables into a factor variable: <strong>number of cylinders (cyl)</strong>.</p>

<div class="sourceCode" id="cb532"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb532-1" data-line-number="1">mtcars_ =<span class="st"> </span>mtcars</a>
<a class="sourceLine" id="cb532-2" data-line-number="2">mtcars_<span class="op">$</span>cyl_factor =<span class="st"> </span><span class="kw">as.factor</span>(mtcars_<span class="op">$</span>cyl)</a>
<a class="sourceLine" id="cb532-3" data-line-number="3">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars_) <span class="co"># One-Way Anova</span></a>
<a class="sourceLine" id="cb532-4" data-line-number="4"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p><strong>Second</strong>, compute for the standard error (SE) per comparison:</p>
<p><span class="math display">\[\begin{align}
SE_{(within\ comparison)} {}&amp;= \sqrt{\frac{MS_W}{n}} = \sqrt{\frac{s^2_W}{n}} = \frac{s_w}{\sqrt{n}}\\
&amp;or \nonumber \\ 
SE_{(within\ comparison)} &amp;=  \sqrt{\frac{MS_W}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}\ \leftarrow \text{if different group sizes}\\
&amp;= s_w \sqrt{\frac{1}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SE_{(within\ comparison)}\)</span> is the standard error per within-comparison</li>
<li><span class="math inline">\(\mathbf{MS_W}\)</span> is the within-group mean square (error)</li>
<li><strong>n</strong> is the group size</li>
<li><span class="math inline">\(\mathbf{n_i}\)</span> is the size of the first group</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the size of the second group</li>
</ul>

<div class="sourceCode" id="cb534"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb534-1" data-line-number="1">groups =<span class="st"> </span><span class="kw">split</span>( mtcars<span class="op">$</span>mpg, <span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl))</a>
<a class="sourceLine" id="cb534-2" data-line-number="2">n<span class="fl">.1</span> =<span class="st"> </span><span class="kw">length</span>(groups[[<span class="dv">1</span>]]); n<span class="fl">.2</span> =<span class="st"> </span><span class="kw">length</span>(groups[[<span class="dv">2</span>]])</a>
<a class="sourceLine" id="cb534-3" data-line-number="3">(<span class="dt">SE =</span> <span class="kw">sqrt</span>(<span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) <span class="op">*</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n<span class="fl">.1</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n<span class="fl">.2</span>))) </a></code></pre></div>
<pre><code>## [1] 1.558</code></pre>

<p><strong>Third</strong>, compute for <strong>Fisherâs LSD</strong> using the following formula:</p>
<p><span class="math display">\[\begin{align}
Fisher&#39;s\ LSD_{i,j} =  t_{\alpha/2}{(  df_w)} \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>LSD</strong> is Fisherâs Significance range value, also called <strong>Fisherâs Criterion</strong></li>
<li><strong>SE</strong> is the standard error</li>
<li><strong>t</strong> is the t-critical value (we can use built-in R function <strong>qt()</strong>)</li>
<li><span class="math inline">\(\mathbf{\alpha}\)</span> is the alpha representing type I error</li>
<li><span class="math inline">\(\mathbf{df_w}\)</span> is degrees of freedom (residual or within-group df)</li>
</ul>
<p>Note: if group sizes are different, then <span class="math inline">\(n_i\)</span> is the size of the first group in pair, <span class="math inline">\(n_j\)</span> is the size of the second group in pair)</p>
<p>Here, we use a built-in R function called <strong>qt()</strong> to calculate <strong>t-critical value</strong>. Then with an assumed 95% confidence level, we now compute for the <strong>LSD</strong>:</p>

<div class="sourceCode" id="cb536"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb536-1" data-line-number="1">t =<span class="st"> </span><span class="kw">qt</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>,  <span class="dt">df =</span> anova[<span class="st">&quot;dfW&quot;</span>] , <span class="dt">lower.tail=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb536-2" data-line-number="2">(<span class="dt">lsd =</span> t <span class="op">*</span><span class="st"> </span>SE) </a></code></pre></div>
<pre><code>## [1] 3.187</code></pre>

<p><strong>Fourth</strong>, determine the significance.</p>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j) &gt; LSD_{i,j}\ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p><strong>Fifth</strong>, repeat the process for every pair.</p>
<p>Here is a naive implementation of <strong>Fisherâs LSD</strong> test using R code:</p>

<div class="sourceCode" id="cb538"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb538-1" data-line-number="1">fisher.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb538-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb538-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, <span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb538-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb538-5" data-line-number="5">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb538-6" data-line-number="6">  fisher =<span class="st"> </span><span class="kw">c</span>();  rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb538-7" data-line-number="7">  <span class="co">#  (t - critical value)</span></a>
<a class="sourceLine" id="cb538-8" data-line-number="8">  dfW =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfW&quot;</span>])</a>
<a class="sourceLine" id="cb538-9" data-line-number="9">  q =<span class="st"> </span><span class="kw">qt</span>(<span class="dt">p =</span> <span class="fl">0.05</span><span class="op">/</span><span class="dv">2</span>, <span class="dt">df =</span> dfW , <span class="dt">lower.tail=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb538-10" data-line-number="10">  msw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) </a>
<a class="sourceLine" id="cb538-11" data-line-number="11">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb538-12" data-line-number="12">        n.i =<span class="st"> </span><span class="kw">length</span>(groups[[i]]); u.i =<span class="st"> </span><span class="kw">mean</span>(groups[[i]]);  </a>
<a class="sourceLine" id="cb538-13" data-line-number="13">        g.i =<span class="st"> </span>factors[i]</a>
<a class="sourceLine" id="cb538-14" data-line-number="14">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb538-15" data-line-number="15">      <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb538-16" data-line-number="16">        n.j =<span class="st"> </span><span class="kw">length</span>(groups[[j]]); u.j =<span class="st"> </span><span class="kw">mean</span>(groups[[j]]); </a>
<a class="sourceLine" id="cb538-17" data-line-number="17">        g.j =<span class="st"> </span>factors[j]</a>
<a class="sourceLine" id="cb538-18" data-line-number="18">        d =<span class="st">  </span><span class="kw">abs</span>(u.j <span class="op">-</span><span class="st"> </span>u.i)  <span class="co"># difference</span></a>
<a class="sourceLine" id="cb538-19" data-line-number="19">        se =<span class="st"> </span><span class="kw">sqrt</span>( (msw <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.j)) <span class="co"># different size</span></a>
<a class="sourceLine" id="cb538-20" data-line-number="20">        t =<span class="st"> </span>d <span class="op">/</span><span class="st"> </span>se <span class="co"># Fisher&#39;s LSD</span></a>
<a class="sourceLine" id="cb538-21" data-line-number="21">        lsd =<span class="st"> </span>q <span class="op">*</span><span class="st"> </span>se <span class="co"># critical range</span></a>
<a class="sourceLine" id="cb538-22" data-line-number="22">        p =<span class="st"> </span><span class="kw">pt</span>( t <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dt">df =</span> dfW, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb538-23" data-line-number="23">        lower =<span class="st"> </span>d <span class="op">-</span><span class="st"> </span>lsd  <span class="co"># lower boundary of conf. interval</span></a>
<a class="sourceLine" id="cb538-24" data-line-number="24">        upper =<span class="st"> </span>d <span class="op">+</span><span class="st"> </span>lsd  <span class="co"># upper boundary of conf. interval</span></a>
<a class="sourceLine" id="cb538-25" data-line-number="25">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb538-26" data-line-number="26">        <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>lsd) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb538-27" data-line-number="27">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i, signif))</a>
<a class="sourceLine" id="cb538-28" data-line-number="28">        fisher =<span class="st"> </span><span class="kw">rbind</span>(fisher, </a>
<a class="sourceLine" id="cb538-29" data-line-number="29">                  <span class="kw">c</span>(se, dfW, <span class="kw">round</span>(d,<span class="dv">5</span>), lower, upper, lsd, q, p))</a>
<a class="sourceLine" id="cb538-30" data-line-number="30">      }</a>
<a class="sourceLine" id="cb538-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb538-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb538-33" data-line-number="33">  <span class="kw">colnames</span>(fisher) =<span class="st"> </span></a>
<a class="sourceLine" id="cb538-34" data-line-number="34"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;dfW&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>,  <span class="st">&quot;Qlsd&quot;</span>,  <span class="st">&quot;Qcrit&quot;</span>, <span class="st">&quot;pvalue&quot;</span>)</a>
<a class="sourceLine" id="cb538-35" data-line-number="35">  <span class="kw">rownames</span>(fisher) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb538-36" data-line-number="36">  fisher</a>
<a class="sourceLine" id="cb538-37" data-line-number="37">}</a>
<a class="sourceLine" id="cb538-38" data-line-number="38"><span class="kw">fisher.comparison</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>##          se dfW   diff lower  upper  Qlsd Qcrit    pvalue
## 6-4* 1.1019  29  6.921 4.667  9.174 2.254 2.045 4.521e-10
## 8-4* 0.9183  29 11.564 9.686 13.442 1.878 2.045 1.857e-17
## 8-6* 1.0550  29  4.643 2.485  6.801 2.158 2.045 4.320e-07</code></pre>

<p>In our result, the absolute difference of each pair of groups is larger than the <strong>Fisherâs LSD criterion (lsd)</strong>, e.g., diff &gt; lsd. Additionally, we also show that the <strong>p-value</strong> is lesser than our alpha, e.g., <span class="math inline">\(p &lt; \alpha\)</span> (at 0.05); therefore, there is a significant difference for all pairs of means, as shown by the asterisk(*) in the table.</p>
</div>
<div id="tukeys-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.3</span> Tukeyâs Test <a href="statistics.html#tukeys-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Tukeyâs Test</strong>, also called <strong>Tukeyâs HSD (honestly significant difference)</strong>, is another <strong>Post-HOC</strong> multiple comparison test. It becomes a <strong>Tukey-Kramerâs Test</strong> if group sizes are not the same (See computation of <strong>standard error (SE)</strong> later).</p>
<p>Here, we use the <strong>mtcars</strong> dataset as before to illustrate a pairwise comparison. Let us convert one of the variables into a factor variable: <strong>number of cylinders (cyl)</strong> while at the same time, view the outcome from <strong>aov()</strong> as a review:</p>

<div class="sourceCode" id="cb540"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb540-1" data-line-number="1">mtcars_ =<span class="st"> </span>mtcars</a>
<a class="sourceLine" id="cb540-2" data-line-number="2">mtcars_<span class="op">$</span>cyl_factor =<span class="st"> </span><span class="kw">as.factor</span>(mtcars_<span class="op">$</span>cyl)</a>
<a class="sourceLine" id="cb540-3" data-line-number="3">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars_) <span class="co"># One-Way Anova</span></a>
<a class="sourceLine" id="cb540-4" data-line-number="4"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p><strong>First</strong>, there are a few assumptions we need to take note of when working with <strong>Tukeyâs Method</strong>:</p>
<ul>
<li><p>assumption of <strong>independence</strong> - samples are independent of each other (no association).</p></li>
<li><p>assumption of <strong>normality</strong> - samples follow a normal distribution. We can use <strong>Shapiro-Wilk</strong> test as an example to test normality. We can use the <strong>Dâagostino-Pearson</strong> test for skewness and kurtosis. There are also other methods available to use. </p></li>
</ul>

<div class="sourceCode" id="cb542"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb542-1" data-line-number="1"><span class="kw">shapiro.test</span>(<span class="kw">residuals</span>(aov.model))</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(aov.model)
## W = 0.97, p-value = 0.5</code></pre>

<ul>
<li>assumption of <strong>homogeneity</strong> - samples have the same variance. We can use <strong>Bartlettâs test</strong> to test <strong>homogeneity</strong> of variance. See also <strong>Games-Howell Test</strong> in the next section.  </li>
</ul>

<div class="sourceCode" id="cb544"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb544-1" data-line-number="1"><span class="kw">bartlett.test</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars_)</a></code></pre></div>
<pre><code>## 
##  Bartlett test of homogeneity of variances
## 
## data:  mpg by cyl_factor
## Bartlett&#39;s K-squared = 8.4, df = 2, p-value = 0.02</code></pre>

<p><strong>Second</strong>, determine the groups, e.g. <span class="math inline">\((4,\ 6,\ 8)\)</span>, to be compared from the factor variable, e.g. <strong>cyl</strong> (number of cylinders).</p>

<div class="sourceCode" id="cb546"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb546-1" data-line-number="1">(<span class="dt">groups =</span> <span class="kw">levels</span>(<span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl)))</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p><strong>Third</strong>, perform a <strong>One-Way ANOVA</strong> using our R implementation: </p>

<div class="sourceCode" id="cb548"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb548-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p><strong>Fourth</strong>, compute for the <strong>standard error (SE)</strong> per comparison: </p>
<p><span class="math display">\[\begin{align}
SE_{(within\ comparison)} {}&amp;= \sqrt{\frac{MS_W}{n}} = \sqrt{\frac{s^2_W}{n}} = \frac{s_w}{\sqrt{n}}\\
&amp;or\nonumber \\ 
SE_{(within\ comparison)} &amp;=  \sqrt{\frac{MS_W}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}\ \leftarrow \text{if different group sizes}\\
&amp;= s_w \sqrt{\frac{1}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)} \label{eqn:eqnnumber28}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(SE_{(within\ comparison)}\)</span> is the standard error per within-comparison</li>
<li><span class="math inline">\(\mathbf{MS_W}\)</span> is the within-group mean square (error)</li>
<li><strong>n</strong> is the group size</li>
<li><span class="math inline">\(\mathbf{n_i}\)</span> is the size of the first group</li>
<li><span class="math inline">\(\mathbf{n_j}\)</span> is the size of the second group</li>
</ul>

<div class="sourceCode" id="cb550"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb550-1" data-line-number="1">n =<span class="st"> </span>sample_size</a>
<a class="sourceLine" id="cb550-2" data-line-number="2">(<span class="dt">SE =</span> <span class="kw">sqrt</span>(<span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) <span class="op">/</span><span class="st"> </span>n)) <span class="co"># using groups of same size</span></a></code></pre></div>
<pre><code>## [1] 1.019</code></pre>

<p><strong>Fifth</strong>, compute for <strong>Tukeyâs HSD</strong> using the following formula: </p>
<p><span class="math display">\[\begin{align}
Q_{hsd} =  q_{\alpha}{( m, df_w)} \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q_{hsd}}\)</span> is Tukeyâs Significance range value, also called <strong>Tukeyâs Criterion</strong></li>
<li><strong>SE</strong> is the standard error</li>
<li><strong>q</strong> is the Tukeyâs studentized range value (we can use built-in R function <strong>qtukey()</strong>)</li>
<li><span class="math inline">\(\mathbf{\alpha}\)</span> is the alpha representing type I error</li>
<li><strong>m</strong> is total number of groups</li>
<li><span class="math inline">\(\mathbf{df_w}\)</span> is degrees of freedom (residual or within-group df)</li>
</ul>
<p>Note: if group sizes are different, then <span class="math inline">\(n_i\)</span> is the size of the first group in pair, <span class="math inline">\(n_j\)</span> is the size of the second group in pair)</p>
<p>Here, we use a built-in R function called <strong>qtukey()</strong> to calculate <strong>Tukeyâs studentized range value</strong>. Then with an assumed 95% confidence level, we now compute for the <strong>HSD</strong>:</p>

<div class="sourceCode" id="cb552"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb552-1" data-line-number="1">m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># Number of Groups</span></a>
<a class="sourceLine" id="cb552-2" data-line-number="2">q =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> m,  <span class="dt">df =</span> anova[<span class="st">&quot;dfW&quot;</span>] , <span class="dt">lower.tail=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb552-3" data-line-number="3">(<span class="dt">hsd =</span> q <span class="op">*</span><span class="st"> </span>SE) <span class="co"># with same group sizes</span></a></code></pre></div>
<pre><code>## [1] 3.56</code></pre>

<p><strong>Sixth</strong>, determine the significance.</p>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j) &gt; Q_{hsd}\ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p><strong>Seventh</strong>, compute for the <strong>Tukeyâs Test statistic (T)</strong> using the following formula:</p>
<p><span class="math display">\[\begin{align}
T = \frac{(\bar{x}_i - \bar{x}_j)}{SE}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>T</strong> is Tukeyâs observed t statistic</li>
<li><span class="math inline">\(\bar{x}_{i}\)</span> is the larger of a pair of group means</li>
<li><span class="math inline">\(\bar{x}_{j}\)</span> is the smaller of a pair of group means</li>
<li><strong>SE</strong> is the standard error</li>
</ul>
<p><strong>Eight</strong>, compute for the <strong>difference of means</strong> and the <strong>confidence interval</strong>:</p>
<p>The <strong>confidence interval</strong> is computed using the following formula:</p>
<p><span class="math display">\[\begin{align}
C.I. = (\bar{x}_{2} - \bar{x}_1) \pm HSD
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}_{2}\)</span> is the larger of a pair of group means</li>
<li><span class="math inline">\(\bar{x}_{1}\)</span> is the smaller of a pair of group means</li>
<li><strong>HSD</strong> is Tukeyâs HSD (honestly significant difference)</li>
</ul>
<p>Here is a naive implementation of <strong>Tukeyâs comparison</strong> test in R code:</p>

<div class="sourceCode" id="cb554"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb554-1" data-line-number="1">tukey.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb554-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb554-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, <span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb554-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb554-5" data-line-number="5">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb554-6" data-line-number="6">  tukey =<span class="st"> </span><span class="kw">c</span>();  rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb554-7" data-line-number="7">  msw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) </a>
<a class="sourceLine" id="cb554-8" data-line-number="8">  dfW =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfW&quot;</span>]) </a>
<a class="sourceLine" id="cb554-9" data-line-number="9">  <span class="co"># tukey&#39;s HSD (tukey criterion - critical value)</span></a>
<a class="sourceLine" id="cb554-10" data-line-number="10">  q =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> m,  <span class="dt">df =</span> dfW , <span class="dt">lower.tail=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb554-11" data-line-number="11">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb554-12" data-line-number="12">        n.i =<span class="st"> </span><span class="kw">length</span>(groups[[i]]); u.i =<span class="st"> </span><span class="kw">mean</span>(groups[[i]]);  </a>
<a class="sourceLine" id="cb554-13" data-line-number="13">        g.i =<span class="st"> </span>factors[i]</a>
<a class="sourceLine" id="cb554-14" data-line-number="14">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb554-15" data-line-number="15">      <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb554-16" data-line-number="16">        n.j =<span class="st"> </span><span class="kw">length</span>(groups[[j]]); u.j =<span class="st"> </span><span class="kw">mean</span>(groups[[j]]); </a>
<a class="sourceLine" id="cb554-17" data-line-number="17">        g.j =<span class="st"> </span>factors[j]</a>
<a class="sourceLine" id="cb554-18" data-line-number="18">        d =<span class="st">  </span><span class="kw">abs</span>(u.j <span class="op">-</span><span class="st"> </span>u.i)  <span class="co"># difference</span></a>
<a class="sourceLine" id="cb554-19" data-line-number="19">        se =<span class="st"> </span><span class="kw">sqrt</span>( (msw <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.j)) <span class="co"># different size</span></a>
<a class="sourceLine" id="cb554-20" data-line-number="20">        t =<span class="st"> </span>d <span class="op">/</span><span class="st"> </span>se <span class="co"># Tukey&#39;s Observed q</span></a>
<a class="sourceLine" id="cb554-21" data-line-number="21">        hsd =<span class="st"> </span>q <span class="op">*</span><span class="st"> </span>se <span class="co"># critical range</span></a>
<a class="sourceLine" id="cb554-22" data-line-number="22">        p =<span class="st"> </span><span class="kw">ptukey</span>( t <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dt">nmeans =</span> m, <span class="dt">df =</span> dfW, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb554-23" data-line-number="23">        lower =<span class="st"> </span>d <span class="op">-</span><span class="st"> </span>hsd  <span class="co"># lower boundary of conf. interval</span></a>
<a class="sourceLine" id="cb554-24" data-line-number="24">        upper =<span class="st"> </span>d <span class="op">+</span><span class="st"> </span>hsd  <span class="co"># upper boundary of conf. interval</span></a>
<a class="sourceLine" id="cb554-25" data-line-number="25">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb554-26" data-line-number="26">        <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>hsd) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb554-27" data-line-number="27">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i, signif))</a>
<a class="sourceLine" id="cb554-28" data-line-number="28">        tukey =<span class="st"> </span><span class="kw">rbind</span>(tukey, </a>
<a class="sourceLine" id="cb554-29" data-line-number="29">                  <span class="kw">c</span>(se, dfW, <span class="kw">round</span>(d,<span class="dv">5</span>), lower, upper, hsd, q, p))</a>
<a class="sourceLine" id="cb554-30" data-line-number="30">      }</a>
<a class="sourceLine" id="cb554-31" data-line-number="31">    }</a>
<a class="sourceLine" id="cb554-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb554-33" data-line-number="33">  <span class="kw">colnames</span>(tukey) =<span class="st"> </span></a>
<a class="sourceLine" id="cb554-34" data-line-number="34"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;dfW&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>,  <span class="st">&quot;Qhsd&quot;</span>,  <span class="st">&quot;Qcrit&quot;</span>, </a>
<a class="sourceLine" id="cb554-35" data-line-number="35">      <span class="st">&quot;pvalue&quot;</span>)</a>
<a class="sourceLine" id="cb554-36" data-line-number="36">  <span class="kw">rownames</span>(tukey) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb554-37" data-line-number="37">  tukey</a>
<a class="sourceLine" id="cb554-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb554-39" data-line-number="39"><span class="kw">tukey.comparison</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>##          se dfW   diff  lower  upper  Qhsd Qcrit    pvalue
## 6-4* 1.1019  29  6.921 3.0722 10.769 3.849 3.493 2.174e-06
## 8-4* 0.9183  29 11.564 8.3565 14.771 3.207 3.493 8.404e-13
## 8-6* 1.0550  29  4.643 0.9581  8.328 3.685 3.493 3.823e-04</code></pre>

<p>In our result, the absolute difference of each pair of groups is larger than the <strong>tukey criterion (hsd)</strong>, e.g., diff &gt; hsd. Additionally, we also show that the <strong>Tukeyâs P-value</strong> is lesser than our alpha, e.g., <span class="math inline">\(p &lt; \alpha\)</span> (at 0.05); therefore, there is a significant difference for all pairs of means, as shown by the asterisk(*) in the table.</p>
<p>Alternatively, we can also use the built-in R function <strong>TukeyHSD()</strong>.</p>
<p>Let us first perform a <strong>One-Way ANOVA</strong>: </p>

<div class="sourceCode" id="cb556"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb556-1" data-line-number="1">mtcars_ =<span class="st"> </span>mtcars</a>
<a class="sourceLine" id="cb556-2" data-line-number="2">mtcars_<span class="op">$</span>cyl_factor =<span class="st"> </span><span class="kw">as.factor</span>(mtcars_<span class="op">$</span>cyl)</a>
<a class="sourceLine" id="cb556-3" data-line-number="3">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars_) <span class="co"># One-Way Anova</span></a>
<a class="sourceLine" id="cb556-4" data-line-number="4"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Then, we can use <strong>TukeyHSD()</strong> to perform a <strong>Tukey multiple comparisons of means</strong> (assuming a confidence level of 95%):</p>

<div class="sourceCode" id="cb558"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb558-1" data-line-number="1">(<span class="dt">tukey_outcome =</span> <span class="kw">TukeyHSD</span>(aov.model, <span class="dt">conf.level=</span><span class="fl">0.95</span>, <span class="dt">ordered=</span><span class="ot">FALSE</span>))</a></code></pre></div>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = mpg ~ cyl_factor, data = mtcars_)
## 
## $cyl_factor
##        diff     lwr     upr  p adj
## 6-4  -6.921 -10.769 -3.0722 0.0003
## 8-4 -11.564 -14.771 -8.3565 0.0000
## 8-6  -4.643  -8.328 -0.9581 0.0112</code></pre>

<p>To visualize the difference, we plot the Tukey outcome like so:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:TukeyHSD"></span>
<img src="DS_files/figure-html/TukeyHSD-1.png" alt="Family-wise Confidence Level" width="70%" />
<p class="caption">
Figure 6.15: Family-wise Confidence Level
</p>
</div>
<p>Note that we also want to compare adjusted P-values corresponding to each pairwise comparison and the difference and confidence intervals. We cover <strong>Bonferroni correction</strong> in a later section to introduce one way of handling adjusted P-values.</p>
</div>
<div id="newman-keul-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.4</span> Newman-Keul Test  <a href="statistics.html#newman-keul-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Newman-Keul Test</strong>, also called <strong>Student-Newman-Keul (SNK) test</strong>, is another <strong>Post-HOC</strong> multiple comparison test similar to <strong>Tukeyâs HSD</strong> done sequentially.</p>
<p><strong>First</strong>, because this is done sequentially, we list the means of groups in descending or ascending order:</p>

<div class="sourceCode" id="cb560"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb560-1" data-line-number="1">group_means =<span class="st"> </span><span class="kw">aggregate</span>(<span class="dt">x =</span> mtcars<span class="op">$</span>mpg, <span class="dt">by =</span> <span class="kw">list</span>(mtcars<span class="op">$</span>cyl), </a>
<a class="sourceLine" id="cb560-2" data-line-number="2">                        <span class="dt">FUN=</span><span class="st">&quot;mean&quot;</span>)</a>
<a class="sourceLine" id="cb560-3" data-line-number="3">means_descend =<span class="st"> </span><span class="kw">sort</span>(<span class="dt">x =</span> group_means<span class="op">$</span>x, <span class="dt">decreasing=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb560-4" data-line-number="4">                     <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb560-5" data-line-number="5"><span class="kw">names</span>(means_descend<span class="op">$</span>x) =<span class="st">  </span>groups[means_descend<span class="op">$</span>ix]</a>
<a class="sourceLine" id="cb560-6" data-line-number="6">(<span class="dt">means_descend =</span> means_descend<span class="op">$</span>x)</a></code></pre></div>
<pre><code>##     4     6     8 
## 26.66 19.74 15.10</code></pre>

<p>We then get the difference between the greater means and the lesser means amongst all of them. Here, we use a table:</p>

<div class="sourceCode" id="cb562"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb562-1" data-line-number="1">n =<span class="st"> </span><span class="kw">length</span>(means_descend)</a>
<a class="sourceLine" id="cb562-2" data-line-number="2">d =<span class="st"> </span><span class="kw">matrix</span>(<span class="st">&#39;.&#39;</span>, n, n)</a>
<a class="sourceLine" id="cb562-3" data-line-number="3"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb562-4" data-line-number="4">  <span class="cf">for</span> (j <span class="cf">in</span> n<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb562-5" data-line-number="5">    <span class="cf">if</span> (means_descend[i] <span class="op">&gt;</span><span class="st"> </span>means_descend[j]) { </a>
<a class="sourceLine" id="cb562-6" data-line-number="6">      d[i, n <span class="op">-</span><span class="st"> </span>j <span class="op">+</span><span class="st"> </span><span class="dv">1</span>] =<span class="st">  </span>means_descend[i] <span class="op">-</span><span class="st"> </span>means_descend[j]</a>
<a class="sourceLine" id="cb562-7" data-line-number="7">    }</a>
<a class="sourceLine" id="cb562-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb562-9" data-line-number="9">}</a>
<a class="sourceLine" id="cb562-10" data-line-number="10"><span class="kw">rownames</span>(d) =<span class="st"> </span><span class="kw">round</span>(means_descend,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb562-11" data-line-number="11"><span class="kw">colnames</span>(d) =<span class="st"> </span><span class="kw">round</span>(<span class="kw">sort</span>(means_descend, <span class="dt">decreasing=</span><span class="ot">FALSE</span>),<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb562-12" data-line-number="12">knitr<span class="op">::</span><span class="kw">kable</span>( d, <span class="dt">caption =</span> <span class="st">&quot;Newman-Keul Diff Table&quot;</span> )</a></code></pre></div>
<table>
<caption><span id="tab:unnamed-chunk-246">Table 6.13: </span>Newman-Keul Diff Table</caption>
<thead>
<tr class="header">
<th align="left"></th>
<th align="left">15.1</th>
<th align="left">19.743</th>
<th align="left">26.664</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">26.664</td>
<td align="left">11.5636363636364</td>
<td align="left">6.92077922077922</td>
<td align="left">.</td>
</tr>
<tr class="even">
<td align="left">19.743</td>
<td align="left">4.64285714285714</td>
<td align="left">.</td>
<td align="left">.</td>
</tr>
<tr class="odd">
<td align="left">15.1</td>
<td align="left">.</td>
<td align="left">.</td>
<td align="left">.</td>
</tr>
</tbody>
</table>

<p><strong>Second</strong>, perform a <strong>One-Way ANOVA</strong> using our R implementation (still using <strong>mtcars</strong> dataset):</p>

<div class="sourceCode" id="cb563"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb563-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p>Here we know that <span class="math inline">\(\mathbf{H_0}\)</span> is rejected, so we continue to investigate the difference.</p>
<p><strong>Third</strong>, calculate the <strong>standard error (SE)</strong> for each comparison with the same formula used for <strong>Tukeyâs Test</strong> (See <strong>SE</strong> formula from <strong>Tukeyâs test</strong>). Each comparison will have a different <strong>SE</strong> as we use mtcars with different group sizes.  </p>
<p><span class="math display">\[\begin{align}
SE_{(within\ comparison)} &amp;=  \sqrt{\frac{MS_W}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}\ \leftarrow \text{if different group sizes}\\
&amp;= s_w \sqrt{\frac{1}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, calculate the <strong>Newman-Keulâs criterion</strong>: </p>
<p><span class="math display">\[\begin{align}
Q_{nk} = q_{\alpha}(r, df) \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q_{nk}}\)</span> is Newman-Keulâs criterion</li>
<li><strong>q</strong> is the Tukey range value (for which we can use <strong>qtukey()</strong> function)</li>
<li><span class="math inline">\(\mathbf{\alpha}\)</span> is the alpha (e.g.Â 0.05)</li>
<li><strong>r</strong> is the rank distance between pairs.
<strong>Fifth</strong>, determine the significance.</li>
</ul>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j)_r &gt; Q_{nk}\ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p>Here is a naive implementation of the <strong>Newman-Keulâs comparison</strong> test in R code:</p>

<div class="sourceCode" id="cb565"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb565-1" data-line-number="1">newmankeul.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(group_means, anova) {</a>
<a class="sourceLine" id="cb565-2" data-line-number="2">  means_descend =<span class="st"> </span><span class="kw">sort</span>(<span class="dt">x =</span> group_means<span class="op">$</span>x, <span class="dt">decreasing=</span><span class="ot">TRUE</span>, </a>
<a class="sourceLine" id="cb565-3" data-line-number="3">                       <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb565-4" data-line-number="4">  group_length =<span class="st"> </span><span class="kw">aggregate</span>(<span class="dt">x =</span> mtcars<span class="op">$</span>mpg, <span class="dt">by =</span> <span class="kw">list</span>(mtcars<span class="op">$</span>cyl), </a>
<a class="sourceLine" id="cb565-5" data-line-number="5">                           <span class="dt">FUN=</span><span class="st">&quot;length&quot;</span>)</a>
<a class="sourceLine" id="cb565-6" data-line-number="6">  <span class="kw">colnames</span>(group_length) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;groups&quot;</span>, <span class="st">&quot;n&quot;</span>)</a>
<a class="sourceLine" id="cb565-7" data-line-number="7">  n =<span class="st"> </span><span class="kw">length</span>(means_descend<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb565-8" data-line-number="8">  nkr =<span class="st"> </span><span class="kw">c</span>(); rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb565-9" data-line-number="9">  msw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) </a>
<a class="sourceLine" id="cb565-10" data-line-number="10">  dfw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfW&quot;</span>]) </a>
<a class="sourceLine" id="cb565-11" data-line-number="11">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb565-12" data-line-number="12">    <span class="cf">for</span> (j <span class="cf">in</span> n<span class="op">:</span><span class="dv">1</span>) {</a>
<a class="sourceLine" id="cb565-13" data-line-number="13">      <span class="cf">if</span> (means_descend<span class="op">$</span>x[i] <span class="op">&gt;</span><span class="st"> </span>means_descend<span class="op">$</span>x[j]) { </a>
<a class="sourceLine" id="cb565-14" data-line-number="14">        d =<span class="st">  </span>means_descend<span class="op">$</span>x[i] <span class="op">-</span><span class="st"> </span>means_descend<span class="op">$</span>x[j]</a>
<a class="sourceLine" id="cb565-15" data-line-number="15">        i.index =<span class="st"> </span>means_descend<span class="op">$</span>ix[i]</a>
<a class="sourceLine" id="cb565-16" data-line-number="16">        j.index =<span class="st"> </span>means_descend<span class="op">$</span>ix[j]</a>
<a class="sourceLine" id="cb565-17" data-line-number="17">        n.i =<span class="st"> </span>group_length<span class="op">$</span>n[i.index]; </a>
<a class="sourceLine" id="cb565-18" data-line-number="18">        g.i =<span class="st"> </span>group_length<span class="op">$</span>groups[i.index]</a>
<a class="sourceLine" id="cb565-19" data-line-number="19">        n.j =<span class="st"> </span>group_length<span class="op">$</span>n[j.index]; </a>
<a class="sourceLine" id="cb565-20" data-line-number="20">        g.j =<span class="st"> </span>group_length<span class="op">$</span>groups[j.index]</a>
<a class="sourceLine" id="cb565-21" data-line-number="21">        rank =<span class="st"> </span>j.index <span class="op">-</span><span class="st"> </span>i.index <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb565-22" data-line-number="22">        se =<span class="st"> </span><span class="kw">sqrt</span>( (msw <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.j))</a>
<a class="sourceLine" id="cb565-23" data-line-number="23">        q =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> rank, <span class="dt">df =</span> dfw, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb565-24" data-line-number="24">        nk =<span class="st"> </span>q <span class="op">*</span><span class="st"> </span>se</a>
<a class="sourceLine" id="cb565-25" data-line-number="25">        t =<span class="st"> </span>d <span class="op">/</span><span class="st"> </span>se <span class="co"># Tukey&#39;s Observed q</span></a>
<a class="sourceLine" id="cb565-26" data-line-number="26">        p =<span class="st"> </span><span class="kw">ptukey</span>( t <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dt">nmeans =</span> rank, <span class="dt">df =</span> dfw, </a>
<a class="sourceLine" id="cb565-27" data-line-number="27">                    <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb565-28" data-line-number="28">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb565-29" data-line-number="29">        <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>nk ) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb565-30" data-line-number="30">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i,signif))</a>
<a class="sourceLine" id="cb565-31" data-line-number="31">        nkr =<span class="st"> </span><span class="kw">rbind</span>(nkr, <span class="kw">c</span>(se, d, rank, dfw,   nk, q, p))</a>
<a class="sourceLine" id="cb565-32" data-line-number="32">      }</a>
<a class="sourceLine" id="cb565-33" data-line-number="33">    }</a>
<a class="sourceLine" id="cb565-34" data-line-number="34">  }</a>
<a class="sourceLine" id="cb565-35" data-line-number="35">  <span class="kw">colnames</span>(nkr) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;rank&quot;</span>, <span class="st">&quot;dfw&quot;</span>, <span class="st">&quot;Qnk&quot;</span>, <span class="st">&quot;Qcrit&quot;</span>, </a>
<a class="sourceLine" id="cb565-36" data-line-number="36">                    <span class="st">&quot;adj. pval&quot;</span>)</a>
<a class="sourceLine" id="cb565-37" data-line-number="37">  <span class="kw">rownames</span>(nkr) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb565-38" data-line-number="38">  nkr</a>
<a class="sourceLine" id="cb565-39" data-line-number="39">}</a>
<a class="sourceLine" id="cb565-40" data-line-number="40">(<span class="dt">nkr =</span> <span class="kw">newmankeul.comparison</span>(group_means, anova))</a></code></pre></div>
<pre><code>##          se   diff rank dfw   Qnk Qcrit adj. pval
## 8-4* 0.9183 11.564    3  29 3.207 3.493 8.404e-13
## 6-4* 1.1019  6.921    2  29 3.187 2.892 7.397e-07
## 8-6* 1.0550  4.643    2  29 3.051 2.892 1.335e-04</code></pre>

<p>We see that all the pairs (comparisons) have an asterisk(*), meaning that each of the mean differences is greater than <span class="math inline">\(\mathbf{Q_{(nk)}}\)</span>, e.g., diff &gt; <span class="math inline">\(Q_{nk}\)</span> - equivalent to being significant. For example, the difference of the pair <strong>8-4</strong> is greater than the computed <strong>Newman-Keulâs criterion</strong>, <span class="math inline">\(\mathbf{NK_{(3)}}\)</span> with a rank of 3, e.g.Â 11.5636 &gt; 3.4926. Therefore, this shows a significant difference between the pair of group means.</p>
</div>
<div id="games-howell-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.5</span> Games-Howell Test <a href="statistics.html#games-howell-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We use this test if there is a violation of <strong>homogeneity of variance</strong>, meaning variances across multiple groups are not constant (not equal). We leave readers to investigate different tests of homogeneity, e.g., <strong>Bartlettâs Test</strong> and <strong>Leveneâs Test</strong>.</p>
<p>Let us continue to use the <strong>mtcars</strong> dataset as before to illustrate a pairwise comparison.</p>
<p><strong>First</strong>, to solve for the <strong>violation of homogeneity</strong> - the unequal variances - we apply <strong>Welchâs correction</strong> using a t-test; hence, this is also called <strong>Welchâs t-test</strong> in which we use the following equation:</p>
<p><span class="math display">\[\begin{align}
t_{(welch)} = \frac{\bar{x}_i - \bar{x}_j}{S_p}\ \ \leftarrow \ \ S_p = 
{\sqrt{\frac{\sigma^2_i}{n_i} + \frac{\sigma^2_j}{n_j}}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>t</strong> is Welchâs T statistic</li>
<li><span class="math inline">\(S_p^2\)</span> is the pooled variance. <span class="math inline">\(S_p\)</span> is the pooled standard deviation.</li>
<li><span class="math inline">\(\bar{x}_i\ ,\ \bar{x}_j\)</span> are means of the first and second group in a pair</li>
<li><span class="math inline">\(\sigma_i\ ,\ \sigma_j\)</span> are standard deviations of the first and second group</li>
<li><span class="math inline">\(n_i\ ,\ n_j\)</span> are sizes of first and second group</li>
</ul>
<p><strong>Second</strong>, compute for the degrees of freedom.</p>
<p>Note that unlike <strong>Tukeyâs Test</strong> in which we use the ANOVAâs within-group degrees of freedom (dfW), here in <strong>Games-Howell Test</strong>, we compute the degrees of freedom from each comparison (pair of group means):</p>
<p><span class="math display">\[\begin{align}
df = \frac{(S_p^2)^2}
{\frac{\left(\frac{\sigma^2_i}{n_i}\right)^2}{n_i-1} + 
 \frac{\left(\frac{\sigma^2_j}{n_j}\right)^2}{n_j-1}}\ \ \leftarrow \ \ S_p^2 = \left(\frac{\sigma^2_i}{n_i} + \frac{\sigma^2_j}{n_j}\right).
\end{align}\]</span></p>
<p><strong>Third</strong>, compute for the pairwise standard error:</p>
<p><span class="math display">\[\begin{align}
SE = \frac{S_p}{\sqrt{2}} = \sqrt{\frac{S^2_p}{2}} = \sqrt{\frac{1}{2}\left(\frac{\sigma^2_i}{n_i} + \frac{\sigma^2_j}{n_j}\right)}
\end{align}\]</span></p>
<p><strong>Fourth</strong>, compute for the <strong>Games-Howell Significance range value</strong></p>
<p><span class="math display">\[\begin{align}
Q_{gh} = q_{\alpha}(m, df) \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q_{gh}}\)</span> is Games-Howellâs criterion</li>
<li><strong>q</strong> is the Tukey range value (for which we can use <strong>qtukey()</strong> function)</li>
<li><span class="math inline">\(\mathbf{\alpha}\)</span> is the alpha (e.g.Â 0.05)</li>
<li><strong>m</strong> is the total number of groups</li>
</ul>
<p><strong>Fifth</strong>, determine the significance.</p>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j) &gt; Q_{gh}\ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p><strong>Finally</strong>, similar to <strong>Tukeyâs confidence interval</strong>, here is the lower and upper estimate of our confidence interval:</p>
<p><span class="math display">\[\begin{align}
C.I. = (\bar{x}_{i} - \bar{x}_j) \pm R 
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}_{i}\)</span> is the larger of a pair of group means</li>
<li><span class="math inline">\(\bar{x}_{j}\)</span> is the smaller of a pair of group means</li>
<li><strong>R</strong> is Games-Howellâs R (ghr)</li>
</ul>
<p>Here is a naive implementation of the <strong>Games-Howell test</strong> in R code:</p>

<div class="sourceCode" id="cb567"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb567-1" data-line-number="1">gameshowell.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb567-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb567-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, <span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb567-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb567-5" data-line-number="5">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb567-6" data-line-number="6">  ghr =<span class="st"> </span><span class="kw">c</span>(); rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb567-7" data-line-number="7">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb567-8" data-line-number="8">    n.i =<span class="st"> </span><span class="kw">length</span>(groups[[i]]); u.i =<span class="st"> </span><span class="kw">mean</span>(groups[[i]]); </a>
<a class="sourceLine" id="cb567-9" data-line-number="9">    s.i =<span class="st"> </span><span class="kw">var</span>(groups[[i]])</a>
<a class="sourceLine" id="cb567-10" data-line-number="10">    g.i =<span class="st"> </span>factors[i]</a>
<a class="sourceLine" id="cb567-11" data-line-number="11">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb567-12" data-line-number="12">      <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb567-13" data-line-number="13">        n.j =<span class="st"> </span><span class="kw">length</span>(groups[[j]]); u.j =<span class="st"> </span><span class="kw">mean</span>(groups[[j]]);  </a>
<a class="sourceLine" id="cb567-14" data-line-number="14">        s.j =<span class="st"> </span><span class="kw">var</span>(groups[[j]])</a>
<a class="sourceLine" id="cb567-15" data-line-number="15">        g.j =<span class="st"> </span>factors[j]</a>
<a class="sourceLine" id="cb567-16" data-line-number="16">        std.i =<span class="st">  </span>s.i <span class="op">/</span><span class="st"> </span>n.i; std.j =<span class="st"> </span>s.j <span class="op">/</span><span class="st"> </span>n.j</a>
<a class="sourceLine" id="cb567-17" data-line-number="17">        d =<span class="st"> </span><span class="kw">abs</span>( u.i <span class="op">-</span><span class="st"> </span>u.j )</a>
<a class="sourceLine" id="cb567-18" data-line-number="18">        s2p =<span class="st"> </span>( std.i <span class="op">+</span><span class="st"> </span>std.j )</a>
<a class="sourceLine" id="cb567-19" data-line-number="19">        se =<span class="st">  </span><span class="kw">sqrt</span>( s2p <span class="op">/</span><span class="st"> </span><span class="dv">2</span> ) </a>
<a class="sourceLine" id="cb567-20" data-line-number="20">        df =<span class="st"> </span>s2p<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>( (std.i<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(n.i <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) <span class="op">+</span><span class="st"> </span>(std.j<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>(n.j <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)) )</a>
<a class="sourceLine" id="cb567-21" data-line-number="21">        t =<span class="st"> </span>d <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>( s2p ) <span class="co"># welch&#39;s t</span></a>
<a class="sourceLine" id="cb567-22" data-line-number="22">        q =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> m, <span class="dt">df =</span> df, <span class="dt">lower.tail =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb567-23" data-line-number="23">        p =<span class="st"> </span><span class="kw">ptukey</span>( t <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dt">nmeans =</span> m, <span class="dt">df =</span> df, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb567-24" data-line-number="24">        gh =<span class="st"> </span>q <span class="op">*</span><span class="st"> </span>se</a>
<a class="sourceLine" id="cb567-25" data-line-number="25">        lower =<span class="st"> </span>d <span class="op">-</span><span class="st"> </span>gh </a>
<a class="sourceLine" id="cb567-26" data-line-number="26">        upper =<span class="st"> </span>d <span class="op">+</span><span class="st"> </span>gh</a>
<a class="sourceLine" id="cb567-27" data-line-number="27">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb567-28" data-line-number="28">        <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>gh ) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb567-29" data-line-number="29">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i,signif))</a>
<a class="sourceLine" id="cb567-30" data-line-number="30">        ghr =<span class="st"> </span><span class="kw">rbind</span>(ghr, <span class="kw">c</span>(se, df, d, lower, upper, gh, q, p ))</a>
<a class="sourceLine" id="cb567-31" data-line-number="31">      }</a>
<a class="sourceLine" id="cb567-32" data-line-number="32">    }</a>
<a class="sourceLine" id="cb567-33" data-line-number="33">  }</a>
<a class="sourceLine" id="cb567-34" data-line-number="34">  <span class="kw">colnames</span>(ghr) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>, <span class="st">&quot;Qgh&quot;</span>, </a>
<a class="sourceLine" id="cb567-35" data-line-number="35">                    <span class="st">&quot;Qcrit&quot;</span>, <span class="st">&quot;adj. pval&quot;</span>)</a>
<a class="sourceLine" id="cb567-36" data-line-number="36">  <span class="kw">rownames</span>(ghr) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb567-37" data-line-number="37">  ghr</a>
<a class="sourceLine" id="cb567-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb567-39" data-line-number="39"><span class="kw">round</span>(<span class="kw">gameshowell.comparison</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl),<span class="dv">3</span>)</a></code></pre></div>
<pre><code>##         se    df   diff lower  upper   Qgh Qcrit adj. pval
## 6-4* 1.037 12.96  6.921 3.047 10.795 3.874 3.736     0.001
## 8-4* 1.076 14.97 11.564 7.609 15.518 3.955 3.674     0.000
## 8-6* 0.620 18.50  4.643 2.409  6.877 2.234 3.601     0.000</code></pre>

</div>
<div id="dunnetts-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.6</span> Dunnettâs Test <a href="statistics.html#dunnetts-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dunnettâs Test</strong> is another <strong>Post-HOC</strong> multiple comparison test. It uses the mean of one fixed <strong>control group</strong> to compare against the means of other <strong>experiment groups</strong>.</p>
<p><strong>First</strong>, let us use the <strong>mtcars</strong> dataset as before with <strong>cyl</strong> as the factor variable of three groups:
</p>
<div class="sourceCode" id="cb569"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb569-1" data-line-number="1">(<span class="dt">groups =</span> <span class="kw">levels</span>(<span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl)))</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p>For illustration, let us use <strong>group 4</strong> for our <strong>control group</strong>, while <strong>groups 6 and 8</strong> are our <strong>experiment groups</strong>. That tells us that the group with cars having four cylinders is our model (or control group) for comparison of fuel consumption against cars that are not with four cylinders.</p>
<p><strong>Second</strong>, run <strong>One-Way ANOVA</strong> for analysis.</p>

<div class="sourceCode" id="cb571"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb571-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p>We know that <span class="math inline">\(\mathbf{H_0}\)</span> is rejected, so we continue to investigate the difference.</p>
<p><strong>Third</strong>, recall the <strong>standard error (SE)</strong> formula we used in <strong>Tukeyâs Test</strong> (See Equation <span class="math inline">\(\ref{eqn:eqnnumber28}\)</span>).</p>
<p><span class="math display">\[\begin{align}
SE_{(within\ comparison)} &amp;=  \sqrt{\frac{MS_W}{2}\left(\frac{1}{n_i}+\frac{1}{n_c}\right)}\ \leftarrow \text{if different group sizes}\\
&amp;= s_w \sqrt{\frac{1}{2}\left(\frac{1}{n_i}+\frac{1}{n_c}\right)}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{n_c}\)</span> is sample size of the control group</li>
<li><span class="math inline">\(\mathbf{n_i}\)</span> is sample size of the ith experiment group</li>
</ul>
<p><strong>Fourth</strong>, compute for Dunnettâs statistic:</p>
<p><span class="math display">\[\begin{align}
Q_{dunn} = q_{\alpha}(n, df) \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q_{dunn}}\)</span> is Dunnettâs criterion</li>
<li><strong>q</strong> is Dunnettâs Critical value ( we can use 3rd party function, <strong>cvSDDT()</strong> )</li>
<li><strong>n</strong> is the nth experiment group</li>
</ul>
<p><strong>Fifth</strong>, to determine significance, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j)_n &gt; Q_{dunn} \ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}_{i}\)</span> is the larger of a pair of group means</li>
<li><span class="math inline">\(\bar{x}_{j}\)</span> is the smaller of a pair of group means</li>
</ul>
<p>Here is a naive implementation of <strong>Dunnettâs comparison</strong> in R code. We use a library called <strong>DunnettTests</strong> for the function <strong>cvsDDT()</strong> to determine Dunnettâs <strong>Critical value</strong>.</p>
<div class="sourceCode" id="cb573"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb573-1" data-line-number="1"><span class="kw">library</span>(DunnettTests)</a></code></pre></div>
<p>Note that the <strong>cvSDDT()</strong> uses <strong>pmvnorm()</strong> for a <strong>multivariate cumulative normal distribution</strong>. This is the function curve in which <strong>cvSDDT()</strong> also uses <strong>uniroot()</strong> for <strong>root finding</strong>. See Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) for <strong>Root Finding</strong> using <strong>Bisection method</strong>. The <strong>roots</strong> become the <strong>critical values</strong> used to construct the <strong>Dunnettâs table</strong>.</p>

<div class="sourceCode" id="cb574"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb574-1" data-line-number="1">dunnett.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb574-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb574-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, <span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb574-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb574-5" data-line-number="5">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb574-6" data-line-number="6">  dunr =<span class="st"> </span><span class="kw">c</span>(); rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb574-7" data-line-number="7">  control =<span class="st"> </span>groups[[<span class="dv">1</span>]]</a>
<a class="sourceLine" id="cb574-8" data-line-number="8">  <span class="co"># use first group as control group</span></a>
<a class="sourceLine" id="cb574-9" data-line-number="9">  n.c =<span class="st"> </span><span class="kw">length</span>(control); u.c =<span class="st"> </span><span class="kw">mean</span>(control); s.c =<span class="st"> </span><span class="kw">var</span>(control)</a>
<a class="sourceLine" id="cb574-10" data-line-number="10">  std.c =<span class="st">  </span>s.c <span class="op">/</span><span class="st"> </span>n.c; g.c =<span class="st"> </span>factors[<span class="dv">1</span>]</a>
<a class="sourceLine" id="cb574-11" data-line-number="11">  msw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>]) </a>
<a class="sourceLine" id="cb574-12" data-line-number="12">  dfW =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfW&quot;</span>]) </a>
<a class="sourceLine" id="cb574-13" data-line-number="13">  q =<span class="st"> </span><span class="kw">cvSDDT</span>(<span class="dt">k=</span>m, <span class="dt">alpha=</span><span class="fl">0.05</span>, <span class="dt">alternative=</span><span class="st">&quot;U&quot;</span>, <span class="dt">corr=</span><span class="fl">0.5</span>, <span class="dt">df=</span>dfW)</a>
<a class="sourceLine" id="cb574-14" data-line-number="14">  <span class="co"># use the rest of group as experiment groups</span></a>
<a class="sourceLine" id="cb574-15" data-line-number="15">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb574-16" data-line-number="16">    n.i =<span class="st"> </span><span class="kw">length</span>(groups[[i]]); u.i =<span class="st"> </span><span class="kw">mean</span>(groups[[i]]); </a>
<a class="sourceLine" id="cb574-17" data-line-number="17">    s.i =<span class="st"> </span><span class="kw">var</span>(groups[[i]])</a>
<a class="sourceLine" id="cb574-18" data-line-number="18">    g.i =<span class="st"> </span>factors[i]</a>
<a class="sourceLine" id="cb574-19" data-line-number="19">    std.i =<span class="st"> </span>s.i <span class="op">/</span><span class="st"> </span>n.i</a>
<a class="sourceLine" id="cb574-20" data-line-number="20">    d =<span class="st"> </span><span class="kw">abs</span>( u.c <span class="op">-</span><span class="st"> </span>u.i )</a>
<a class="sourceLine" id="cb574-21" data-line-number="21">    s2p =<span class="st"> </span>( std.c <span class="op">+</span><span class="st"> </span>std.i )</a>
<a class="sourceLine" id="cb574-22" data-line-number="22">    se =<span class="st"> </span><span class="kw">sqrt</span>( (msw <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.c))</a>
<a class="sourceLine" id="cb574-23" data-line-number="23">    t =<span class="st"> </span>d <span class="op">/</span><span class="st"> </span><span class="kw">sqrt</span>( s2p ) <span class="co"># welch&#39;s t</span></a>
<a class="sourceLine" id="cb574-24" data-line-number="24">    p =<span class="st"> </span><span class="kw">ptukey</span>( t <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>(<span class="dv">2</span>), <span class="dt">nmeans =</span> m, <span class="dt">df =</span> dfW, <span class="dt">lower.tail=</span><span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb574-25" data-line-number="25">    dunn =<span class="st"> </span>q[i] <span class="op">*</span><span class="st"> </span>se</a>
<a class="sourceLine" id="cb574-26" data-line-number="26">    lower =<span class="st"> </span>d <span class="op">-</span><span class="st"> </span>dunn </a>
<a class="sourceLine" id="cb574-27" data-line-number="27">    upper =<span class="st"> </span>d <span class="op">+</span><span class="st"> </span>dunn</a>
<a class="sourceLine" id="cb574-28" data-line-number="28">    signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb574-29" data-line-number="29">    <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>dunn ) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb574-30" data-line-number="30">    rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.i, <span class="st">&quot;-&quot;</span>, g.c,signif))</a>
<a class="sourceLine" id="cb574-31" data-line-number="31">    dunr =<span class="st"> </span><span class="kw">rbind</span>(dunr, <span class="kw">c</span>(se, dfW, d, lower, upper, dunn, q[i], p ))</a>
<a class="sourceLine" id="cb574-32" data-line-number="32">  }</a>
<a class="sourceLine" id="cb574-33" data-line-number="33">  <span class="kw">colnames</span>(dunr) =<span class="st"> </span></a>
<a class="sourceLine" id="cb574-34" data-line-number="34"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>, <span class="st">&quot;Qdunn&quot;</span>,</a>
<a class="sourceLine" id="cb574-35" data-line-number="35">      <span class="st">&quot;Qcrit&quot;</span>,  <span class="st">&quot;adj. pval&quot;</span>)</a>
<a class="sourceLine" id="cb574-36" data-line-number="36">  <span class="kw">rownames</span>(dunr) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb574-37" data-line-number="37">  dunr</a>
<a class="sourceLine" id="cb574-38" data-line-number="38">}</a>
<a class="sourceLine" id="cb574-39" data-line-number="39"><span class="kw">dunnett.comparison</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>##          se df   diff lower  upper Qdunn Qcrit adj. pval
## 6-4* 1.1019 29  6.921 4.726  9.116 2.195 1.992 1.596e-04
## 8-4* 0.9183 29 11.564 9.589 13.538 1.974 2.150 6.646e-08</code></pre>

<p>The result shows that all the experiment groups differ significantly from the control group, as indicated by the asterisk(*).</p>
</div>
<div id="duncans-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.7</span> Duncanâs Test <a href="statistics.html#duncans-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Duncanâs Multiple Range Test</strong> is another <strong>Post-HOC</strong> test that compares differences in group means based on rank. It is a variant of <strong>SNK</strong>.</p>
<p>To illustrate, let us perform <strong>Duncanâs Multiple Range Test</strong>.</p>
<p><strong>First</strong>, let us use the <strong>mtcars</strong> dataset as before with <strong>cyl</strong> as the factor variable of three groups:</p>

<div class="sourceCode" id="cb576"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb576-1" data-line-number="1">(<span class="dt">groups =</span> <span class="kw">levels</span>(<span class="kw">as.factor</span>(mtcars<span class="op">$</span>cyl)))</a></code></pre></div>
<pre><code>## [1] &quot;4&quot; &quot;6&quot; &quot;8&quot;</code></pre>

<p><strong>Second</strong>, rank the means of groups in ascending order.</p>

<div class="sourceCode" id="cb578"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb578-1" data-line-number="1">rank &lt;-<span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb578-2" data-line-number="2">  group_means =<span class="st"> </span><span class="kw">aggregate</span>(<span class="dt">x =</span> dependent, <span class="dt">by =</span> <span class="kw">list</span>(factor), <span class="dt">FUN=</span><span class="st">&quot;mean&quot;</span>)</a>
<a class="sourceLine" id="cb578-3" data-line-number="3">  means_ascend =<span class="st"> </span><span class="kw">sort</span>(<span class="dt">x =</span> group_means<span class="op">$</span>x, <span class="dt">decreasing=</span><span class="ot">FALSE</span>, </a>
<a class="sourceLine" id="cb578-4" data-line-number="4">                      <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb578-5" data-line-number="5">  r =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="kw">length</span>(groups))</a>
<a class="sourceLine" id="cb578-6" data-line-number="6">  <span class="kw">names</span>(r) =<span class="st"> </span>groups[means_ascend<span class="op">$</span>ix]</a>
<a class="sourceLine" id="cb578-7" data-line-number="7">  r =<span class="st"> </span><span class="kw">rbind</span>(r, means_ascend<span class="op">$</span>x)</a>
<a class="sourceLine" id="cb578-8" data-line-number="8">  <span class="kw">rownames</span>(r) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;rank&quot;</span>, <span class="st">&quot;mean&quot;</span>)</a>
<a class="sourceLine" id="cb578-9" data-line-number="9">  r</a>
<a class="sourceLine" id="cb578-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb578-11" data-line-number="11"><span class="kw">rank</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>##         8     6     4
## rank  1.0  2.00  3.00
## mean 15.1 19.74 26.66</code></pre>

<p><strong>Third</strong>, run <strong>One-Way ANOVA</strong> for analysis.</p>

<div class="sourceCode" id="cb580"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb580-1" data-line-number="1">(<span class="dt">anova_outcome =</span> <span class="dt">anova =</span> <span class="kw">one_way_anova</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl))</a></code></pre></div>
<pre><code>##    SSB    SSW    dfB    dfW    MSB    MSW      F 
## 824.78 301.26   2.00  29.00 412.39  10.39  39.70</code></pre>

<p><strong>Fourth</strong>, recall the <strong>standard error (SE)</strong> formula we used in <strong>Tukeyâs Test</strong> (See Equation <span class="math inline">\(\ref{eqn:eqnnumber28}\)</span>).</p>
<p><span class="math display">\[\begin{align}
SE_{(within\ comparison)} &amp;=  \sqrt{\frac{MS_W}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}\ \leftarrow \text{if different group sizes}\\
&amp;= s_w \sqrt{\frac{1}{2}\left(\frac{1}{n_i}+\frac{1}{n_j}\right)}
\end{align}\]</span></p>
<p><strong>Fifth</strong>, compute for Duncanâs statistic:</p>
<p><span class="math display">\[\begin{align}
Q_{dunc} = q_{\alpha}{(r,df_W)} \times SE
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{Q_{dunc}}\)</span> is Duncanâs Significance range value, the <strong>Duncanâs Criterion</strong></li>
<li><strong>SE</strong> is the standard error</li>
<li><strong>q</strong> is Duncanâs range value</li>
<li><span class="math inline">\(\mathbf{\alpha}\)</span> is the alpha representing type I error</li>
<li><strong>r</strong> is the rank distance</li>
<li><span class="math inline">\(\mathbf{df_W}\)</span> is degrees of freedom (residual or within-group df)</li>
</ul>
<p><strong>Sixth</strong>, to determine significance, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
(\bar{x}_i - \bar{x}_j)_n &gt; Q_{dunc} \ \ \ \ \leftarrow\ \ \ \ \text{there is a significant difference}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\bar{x}_{i}\)</span> is the mean of the first group in a pair</li>
<li><span class="math inline">\(\bar{x}_{j}\)</span> is the mean of the second group in a pair</li>
</ul>
<p>Here is a naive implementation of <strong>Duncanâs MRT</strong> in R code. Here, we use the <strong>Tukeyâs studentized range table</strong>:</p>

<div class="sourceCode" id="cb582"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb582-1" data-line-number="1">duncan.comparison &lt;-<span class="st"> </span><span class="cf">function</span>(dependent, factor) {</a>
<a class="sourceLine" id="cb582-2" data-line-number="2">  factors =<span class="st"> </span><span class="kw">levels</span>(<span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb582-3" data-line-number="3">  groups =<span class="st"> </span><span class="kw">split</span>( dependent, <span class="kw">as.factor</span>(factor))</a>
<a class="sourceLine" id="cb582-4" data-line-number="4">  anova =<span class="st"> </span><span class="kw">one_way_anova</span>(groups)</a>
<a class="sourceLine" id="cb582-5" data-line-number="5">  rank =<span class="st"> </span><span class="kw">rank</span>(dependent, factor)</a>
<a class="sourceLine" id="cb582-6" data-line-number="6">  m =<span class="st"> </span><span class="kw">length</span>(groups) <span class="co"># number of groups</span></a>
<a class="sourceLine" id="cb582-7" data-line-number="7">  dunr =<span class="st"> </span><span class="kw">c</span>(); rname =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb582-8" data-line-number="8">  msw =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;MSW&quot;</span>])</a>
<a class="sourceLine" id="cb582-9" data-line-number="9">  dfW =<span class="st"> </span><span class="kw">as.numeric</span>(anova[<span class="st">&quot;dfW&quot;</span>])</a>
<a class="sourceLine" id="cb582-10" data-line-number="10">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb582-11" data-line-number="11">    idx =<span class="st"> </span>factors[i] </a>
<a class="sourceLine" id="cb582-12" data-line-number="12">    group =<span class="st"> </span>groups[[idx]]</a>
<a class="sourceLine" id="cb582-13" data-line-number="13">    n.i =<span class="st"> </span><span class="kw">length</span>(group); u.i =<span class="st"> </span><span class="kw">mean</span>(group); g.i =<span class="st"> </span>idx</a>
<a class="sourceLine" id="cb582-14" data-line-number="14">    r.i =<span class="st"> </span>rank[,idx]</a>
<a class="sourceLine" id="cb582-15" data-line-number="15">    <span class="cf">for</span> (j <span class="cf">in</span> i<span class="op">:</span>m) {</a>
<a class="sourceLine" id="cb582-16" data-line-number="16">      <span class="cf">if</span> (i <span class="op">!=</span><span class="st"> </span>j) {</a>
<a class="sourceLine" id="cb582-17" data-line-number="17">        idx =<span class="st"> </span>factors[j] </a>
<a class="sourceLine" id="cb582-18" data-line-number="18">        group =<span class="st"> </span>groups[[idx]]</a>
<a class="sourceLine" id="cb582-19" data-line-number="19">        n.j =<span class="st"> </span><span class="kw">length</span>(group); u.j =<span class="st"> </span><span class="kw">mean</span>(group); g.j =<span class="st"> </span>idx</a>
<a class="sourceLine" id="cb582-20" data-line-number="20">        r.j =<span class="st"> </span>rank[,idx]</a>
<a class="sourceLine" id="cb582-21" data-line-number="21">        r_dist =<span class="st"> </span>r.i[<span class="st">&quot;rank&quot;</span>] <span class="op">-</span><span class="st"> </span>r.j[<span class="st">&quot;rank&quot;</span>] <span class="op">+</span><span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb582-22" data-line-number="22">        r =<span class="st"> </span>rank[,r_dist]</a>
<a class="sourceLine" id="cb582-23" data-line-number="23">        se =<span class="st"> </span><span class="kw">sqrt</span>( (msw <span class="op">/</span><span class="st"> </span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>n.i <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span>n.j))</a>
<a class="sourceLine" id="cb582-24" data-line-number="24">        q =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> rank[<span class="dv">2</span>,i], <span class="dt">df =</span> dfW, </a>
<a class="sourceLine" id="cb582-25" data-line-number="25">                   <span class="dt">lower.tail=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb582-26" data-line-number="26">        p =<span class="st"> </span><span class="kw">qtukey</span>(<span class="dt">p =</span> <span class="fl">0.05</span>, <span class="dt">nmeans =</span> rank[<span class="dv">2</span>,i], <span class="dt">df =</span> dfW, </a>
<a class="sourceLine" id="cb582-27" data-line-number="27">                   <span class="dt">lower.tail=</span><span class="ot">FALSE</span>) </a>
<a class="sourceLine" id="cb582-28" data-line-number="28">        dunc =<span class="st"> </span>q <span class="op">*</span><span class="st"> </span>se</a>
<a class="sourceLine" id="cb582-29" data-line-number="29">        d =<span class="st"> </span><span class="kw">abs</span>( u.i <span class="op">-</span><span class="st"> </span>u.j )</a>
<a class="sourceLine" id="cb582-30" data-line-number="30">        lower =<span class="st"> </span>d <span class="op">-</span><span class="st"> </span>dunc </a>
<a class="sourceLine" id="cb582-31" data-line-number="31">        upper =<span class="st"> </span>d <span class="op">+</span><span class="st"> </span>dunc</a>
<a class="sourceLine" id="cb582-32" data-line-number="32">        signif =<span class="st"> &#39;&#39;</span></a>
<a class="sourceLine" id="cb582-33" data-line-number="33">        <span class="cf">if</span> (d <span class="op">&gt;</span><span class="st"> </span>dunc ) { signif =<span class="st"> &#39;*&#39;</span> }</a>
<a class="sourceLine" id="cb582-34" data-line-number="34">        rname =<span class="st"> </span><span class="kw">c</span>(rname, <span class="kw">paste0</span>(g.j, <span class="st">&quot;-&quot;</span>, g.i,signif))</a>
<a class="sourceLine" id="cb582-35" data-line-number="35">        dunr =<span class="st"> </span><span class="kw">rbind</span>(dunr, <span class="kw">c</span>(se, dfW, d, lower, upper, r_dist, dunc,  </a>
<a class="sourceLine" id="cb582-36" data-line-number="36">                             q, p ))</a>
<a class="sourceLine" id="cb582-37" data-line-number="37">      }</a>
<a class="sourceLine" id="cb582-38" data-line-number="38">    }</a>
<a class="sourceLine" id="cb582-39" data-line-number="39">  }</a>
<a class="sourceLine" id="cb582-40" data-line-number="40">  <span class="kw">colnames</span>(dunr) =<span class="st"> </span></a>
<a class="sourceLine" id="cb582-41" data-line-number="41"><span class="st">    </span><span class="kw">c</span>(<span class="st">&quot;se&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;diff&quot;</span>, <span class="st">&quot;lower&quot;</span>, <span class="st">&quot;upper&quot;</span>, <span class="st">&quot;dist.&quot;</span>, <span class="st">&quot;Qdunc&quot;</span>, </a>
<a class="sourceLine" id="cb582-42" data-line-number="42">      <span class="st">&quot;Qcrit&quot;</span>, <span class="st">&quot;adj.pval&quot;</span>)</a>
<a class="sourceLine" id="cb582-43" data-line-number="43">  <span class="kw">rownames</span>(dunr) =<span class="st"> </span>rname</a>
<a class="sourceLine" id="cb582-44" data-line-number="44">  dunr</a>
<a class="sourceLine" id="cb582-45" data-line-number="45">}</a>
<a class="sourceLine" id="cb582-46" data-line-number="46"><span class="kw">duncan.comparison</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>cyl)</a></code></pre></div>
<pre><code>##          se df   diff  lower upper dist. Qdunc Qcrit adj.pval
## 6-4* 1.1019 29  6.921  1.155 12.69     2 5.766 5.232    5.232
## 8-4* 0.9183 29 11.564  6.759 16.37     3 4.805 5.232    5.232
## 8-6  1.0550 29  4.643 -1.138 10.42     2 5.781 5.479    5.479</code></pre>

<p>The outcome shows the first and second mean differences to be significant.</p>
<p>Let us validate.</p>
<p><strong>First</strong>, let us generate a summary of ANOVA using <strong>aov()</strong>:</p>

<div class="sourceCode" id="cb584"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb584-1" data-line-number="1">mtcars_<span class="op">$</span>cyl_factor =<span class="st"> </span><span class="kw">as.factor</span>(mtcars_<span class="op">$</span>cyl)</a>
<a class="sourceLine" id="cb584-2" data-line-number="2">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>cyl_factor, <span class="dt">data =</span> mtcars_) <span class="co"># One-Way Anova</span></a>
<a class="sourceLine" id="cb584-3" data-line-number="3"><span class="kw">summary</span>(aov.model)</a></code></pre></div>
<pre><code>##             Df Sum Sq Mean Sq F value Pr(&gt;F)    
## cyl_factor   2    825     412    39.7  5e-09 ***
## Residuals   29    301      10                   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p><strong>Second</strong>, use <strong>duncan.test()</strong> to perform the <strong>Duncanâs test</strong>.</p>
<p>Here, we use a third-party library called <strong>agricolae</strong> which comes with a function called <strong>duncan.test()</strong>.</p>

<div class="sourceCode" id="cb586"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb586-1" data-line-number="1"><span class="kw">library</span>(agricolae)</a></code></pre></div>
<pre><code>## This version of &#39;bslib&#39; is designed to work with &#39;shiny&#39; &gt;= 1.6.0.
##     Please upgrade via install.packages(&#39;shiny&#39;).</code></pre>
<div class="sourceCode" id="cb588"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb588-1" data-line-number="1">(<span class="dt">duncan.model =</span> <span class="kw">duncan.test</span>(aov.model,<span class="st">&quot;cyl_factor&quot;</span>))</a></code></pre></div>
<pre><code>## $statistics
##   MSerror Df  Mean    CV
##     10.39 29 20.09 16.04
## 
## $parameters
##     test     name.t ntr alpha
##   Duncan cyl_factor   3  0.05
## 
## $duncan
## NULL
## 
## $means
##     mpg   std  r  Min  Max   Q25  Q50   Q75
## 4 26.66 4.510 11 21.4 33.9 22.80 26.0 30.40
## 6 19.74 1.454  7 17.8 21.4 18.65 19.7 21.00
## 8 15.10 2.560 14 10.4 19.2 14.40 15.2 16.25
## 
## $comparison
## NULL
## 
## $groups
##     mpg groups
## 4 26.66      a
## 6 19.74      b
## 8 15.10      c
## 
## attr(,&quot;class&quot;)
## [1] &quot;group&quot;</code></pre>

<p><strong>Third</strong>, to visualize, let us plot the variance:</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:duncantest"></span>
<img src="DS_files/figure-html/duncantest-1.png" alt="Groups and Interquantile range" width="70%" />
<p class="caption">
Figure 6.16: Groups and Interquantile range
</p>
</div>

</div>
<div id="meta-analysis-test" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.5.8</span> Meta-Analysis Test <a href="statistics.html#meta-analysis-test" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A single study of observation may sometimes not suffice in analyzing data. There is a benefit when merging data from many different studies at times. This <strong>meta-analysis</strong> of results from different studies may render a different range of values of variables.</p>
<p>Also, specific data are manipulated or manufactured rather than observed from real-world events - we call this data <strong>synthetic data</strong>. We compare the differences and similarities of data properties across studies in analyzing synthetic data. There are two qualities we are looking for in <strong>synthetic data</strong>:</p>
<p><strong>Heterogeneity of variances</strong> refers to the differences in quality or property of data (possibly synthesized and) merged from different studies. </p>
<p><strong>Homogeneity of variances</strong> refers to the similarities in quality or property of data (possibly synthesized and) merged from different studies. </p>
<p>Here, we show a forest plot of <strong>synthetic data</strong> from different studies (See Figure <a href="statistics.html#fig:heterogeniety">6.17</a>) to check if data is either <strong>homogenous</strong> or <strong>heterogenous</strong>.</p>

<div class="sourceCode" id="cb590"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb590-1" data-line-number="1"><span class="kw">library</span>(forestplot)</a>
<a class="sourceLine" id="cb590-2" data-line-number="2"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb590-3" data-line-number="3">beta0 =<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb590-4" data-line-number="4">beta1 =<span class="st"> </span><span class="fl">1.4</span></a>
<a class="sourceLine" id="cb590-5" data-line-number="5">X =<span class="st"> </span><span class="kw">matrix</span> ( <span class="kw">runif</span>(<span class="dv">45</span>, <span class="dt">min=</span><span class="op">-</span><span class="dv">2</span>, <span class="dt">max=</span><span class="dv">2</span>), <span class="dt">nrow=</span><span class="dv">5</span>, <span class="dt">ncol=</span><span class="dv">9</span> )</a>
<a class="sourceLine" id="cb590-6" data-line-number="6">coefs =<span class="st">  </span><span class="kw">matrix</span>(<span class="dv">0</span>, <span class="dt">nrow=</span><span class="dv">9</span>, <span class="dt">ncol=</span><span class="dv">3</span> )</a>
<a class="sourceLine" id="cb590-7" data-line-number="7"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>) {</a>
<a class="sourceLine" id="cb590-8" data-line-number="8">  x =<span class="st"> </span><span class="kw">as.vector</span>( X[,i] )</a>
<a class="sourceLine" id="cb590-9" data-line-number="9">  random_noise =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">5</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb590-10" data-line-number="10">  y =<span class="st">  </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st">  </span>random_noise </a>
<a class="sourceLine" id="cb590-11" data-line-number="11">  model =<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb590-12" data-line-number="12">  coefs[i,] =<span class="st"> </span><span class="kw">c</span>( <span class="dt">Mean =</span> <span class="kw">coef</span>(model)[<span class="dv">2</span>], <span class="dt">Lower =</span> <span class="kw">confint</span>(model)[<span class="dv">2</span>,<span class="dv">1</span>], </a>
<a class="sourceLine" id="cb590-13" data-line-number="13">                 <span class="dt">Upper=</span><span class="kw">confint</span>(model)[<span class="dv">2</span>,<span class="dv">2</span>] ) </a>
<a class="sourceLine" id="cb590-14" data-line-number="14">}</a>
<a class="sourceLine" id="cb590-15" data-line-number="15">labels =<span class="st"> </span><span class="kw">matrix</span>( <span class="kw">c</span>( <span class="st">&quot;Studies&quot;</span>, <span class="kw">paste0</span>(<span class="st">&quot;Study &quot;</span>, <span class="dv">1</span><span class="op">:</span><span class="dv">9</span>), <span class="st">&quot;Coefficients&quot;</span>, </a>
<a class="sourceLine" id="cb590-16" data-line-number="16">                    <span class="kw">round</span>(coefs[,<span class="dv">1</span>],<span class="dv">2</span>)), <span class="dt">nrow=</span><span class="dv">10</span>, <span class="dt">ncol=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb590-17" data-line-number="17">coefs =<span class="st"> </span><span class="kw">rbind</span>(<span class="ot">NA</span>, coefs)</a>
<a class="sourceLine" id="cb590-18" data-line-number="18"><span class="kw">forestplot</span>(<span class="dt">labeltext =</span> labels, </a>
<a class="sourceLine" id="cb590-19" data-line-number="19">        <span class="dt">mean=</span>coefs[,<span class="dv">1</span>], <span class="dt">lower=</span> coefs[,<span class="dv">2</span>], <span class="dt">upper=</span> coefs[,<span class="dv">3</span>], </a>
<a class="sourceLine" id="cb590-20" data-line-number="20">        <span class="dt">title=</span><span class="st">&quot;Meta-Analysis Study&quot;</span> , </a>
<a class="sourceLine" id="cb590-21" data-line-number="21">        <span class="dt">col=</span><span class="kw">fpColors</span>(<span class="dt">box=</span><span class="st">&quot;black&quot;</span>, <span class="dt">line=</span><span class="st">&quot;red&quot;</span>), </a>
<a class="sourceLine" id="cb590-22" data-line-number="22">        <span class="dt">grid =</span> <span class="kw">structure</span>(<span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="dt">gp =</span> <span class="kw">gpar</span>(<span class="dt">col =</span> <span class="st">&quot;grey&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)),</a>
<a class="sourceLine" id="cb590-23" data-line-number="23">        <span class="dt">new_page =</span> <span class="ot">TRUE</span>,  <span class="dt">boxsize=</span><span class="fl">0.25</span>,   <span class="dt">xlab=</span><span class="st">&quot;Coef Index&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heterogeniety"></span>
<img src="DS_files/figure-html/heterogeniety-1.png" alt="Heterogeniety" width="60%" />
<p class="caption">
Figure 6.17: Heterogeniety
</p>
</div>

<p>It shows in the forest plot above that studies 1 through 9 render results that seem reasonably within neighboring means and ranges. However, studies 3 and 9 may require some attention since they have confidence intervals whose ranges are further apart. Thus, that can be considered to have a higher heterogeneity than the other studies - meaning, studies 3 and 9 have more significant differences than the other studies (in the context of the confident interval).</p>
<p>If we are more strict regarding the placement of the blue box (the coefficient mean), then study 9 has a negative value and is placed almost outside the grid between 0 and 3. So in this context, we can say that study 9 has a higher heterogeneity in our being strict about placement.</p>
</div>
</div>
<div id="statistical-modeling" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.6</span> Statistical Modeling <a href="statistics.html#statistical-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Domain (Subject) Knowledge</strong> is key to <strong>modeling</strong>. For example, suppose we try to tackle a known subject in climatology around <strong>climate change</strong> and <strong>global warming</strong>. Suppose we declare the following statement; albeit, admittedly, we are not being savvy around this domain:</p>
<p><em>Global warming is due to one of the following reasons, namely deforestation, greenhouse gasses, or interaction between deforestation and greenhouse gasses.</em></p>
<p>We translate the statement into the following linear equation:</p>
<p><span class="math display">\[\begin{align}
y = \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1 x_2
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>y</strong> is a response variable corresponding to global warming</li>
<li><strong>x1</strong> is a predictor variable corresponding to greenhouse gases</li>
<li><strong>x2</strong> is a predictor variable corresponding to deforestation</li>
<li><strong>x1x2</strong> is an interaction between deforestation and greenhouse gases</li>
</ul>
<p>Our goal is to be able to find a model that can describe the relationship of predictor variables with their additive effect on the response variable.</p>
<div id="model-specification" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.6.1</span> Model Specification <a href="statistics.html#model-specification" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The statement above about <strong>climate change</strong> is an example of describing a domain problem. Our goal is to translate the problem into a statistical model, particularly into a <strong>regression model</strong>. One aspect of <strong>regression modeling</strong> is the formulation of <strong>model specification</strong>.</p>
<p>From a few dictionary sources (e.g., Merriam-webster and Wikipedia), the term <strong>specification</strong> refers to the process of describing and identifying the requirements of a given domain problem. Here, <strong>statistical model specification</strong> refers to just that. However, a true definition of <strong>specification</strong> comes from the word itself - <strong>being specific</strong> or <strong>being detailed</strong>. A model can either be <strong>correctly specified</strong> or <strong>incorrectly specified</strong>. A <strong>specification error</strong> happens when a model is incorrectly specified - which suffers one of two states: <strong>Underspecified</strong> or <strong>Overspecified</strong>. Such <strong>specification errors</strong> may affect bias and variance, which we expound further in Chapter <strong>9</strong> (<strong>Computation Learning I</strong>).</p>
<p>In <strong>Model Specification</strong>, one of our goals is to make sure that our model is <strong>correctly specified</strong>, which is intrinsically achieved as a result of explicitly performing <strong>feature and model selection</strong>, <strong>parameter optimization and regularization</strong> through <strong>validation</strong>and <strong>evaluation and test</strong> through <strong>inference</strong> - among many other considerations. As a starting point, let us discuss preparatory modeling operations, starting with understanding variable interaction and then a discussion of regression analysis. In turn, we also cover the Significance of Regression, including evaluating models by inference.</p>
<p>Note that we end this Chapter covering <strong>Statistical Computation</strong> with a discussion on <strong>Regression</strong>. We defer the topic of <strong>Classification</strong> until we reach Chapter <strong>10</strong> (<strong>Computation Learning II</strong>).</p>
</div>
<div id="statistical-interaction" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.6.2</span> Statistical Interaction <a href="statistics.html#statistical-interaction" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Most of our discussion in <strong>Linear regression</strong> includes simple <strong>additive linear combination</strong>, e.g.:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ ...\ + \beta_n x_n + \epsilon
\end{align}\]</span></p>
<p>We also discussed <strong>Polynomial regression</strong>, e.g.</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2^2 + \ ...\ + \beta_n x_n^n + \epsilon
\end{align}\]</span></p>
<p>In this section, we discuss <strong>interactions</strong> of predictor variables, e.g.:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_1x_2 + \epsilon
\end{align}\]</span></p>
<p>Notice that the first three terms of the linear combination form a simple linear equation. The fourth term however comprises the independent variable <span class="math inline">\(\mathbf{x_1}\)</span> and <span class="math inline">\(\mathbf{x_2}\)</span>. We call this <strong>interaction</strong>. In determining <strong>significance of regression</strong> for interactions, our <strong>null hypothesis</strong> may be stated as such:</p>
<p><span class="math display">\[\begin{align}
H_0 {}&amp;: \beta_3 = 0 \ \ \ \ \text{there is no effect of the interaction of}\ \mathbf{x_1}\ and\  \mathbf{x_2} \\
H_1 &amp;: \beta_3 \ne 0\ \ \ \ \text{there is an effect of the interaction of}\ \mathbf{x_1}\ and\  \mathbf{x_2} 
\end{align}\]</span></p>
<p>To illustrate, let us generate three independent variables, { <span class="math inline">\(\mathbf{x1,\ x2,\ x3}\)</span> }:</p>

<div class="sourceCode" id="cb591"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb591-1" data-line-number="1">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb591-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb591-3" data-line-number="3"><span class="kw">set.seed</span>(<span class="dv">1020</span>)</a>
<a class="sourceLine" id="cb591-4" data-line-number="4">x1 =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb591-5" data-line-number="5"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb591-6" data-line-number="6">x2 =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb591-7" data-line-number="7"><span class="kw">set.seed</span>(<span class="dv">3020</span>)</a>
<a class="sourceLine" id="cb591-8" data-line-number="8">x3 =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb591-9" data-line-number="9">e =<span class="st"> </span><span class="kw">rnorm</span>(sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co"># residual</span></a>
<a class="sourceLine" id="cb591-10" data-line-number="10">y =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">rnorm</span>(sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span>e )</a></code></pre></div>

<p>We then create four linear models with the following combination:</p>

<div class="sourceCode" id="cb592"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb592-1" data-line-number="1">lm1.model =<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>x2 <span class="op">+</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x2 <span class="op">+</span><span class="st"> </span>x2<span class="op">:</span>x3 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x3 <span class="op">+</span><span class="st"> </span>x1<span class="op">:</span>x2<span class="op">:</span>x3 )</a>
<a class="sourceLine" id="cb592-2" data-line-number="2">lm2.model =<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span>x2 <span class="op">*</span><span class="st"> </span>x3) </a>
<a class="sourceLine" id="cb592-3" data-line-number="3">lm3.model =<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x1 <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x1<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span><span class="kw">I</span>(x1<span class="op">^</span><span class="dv">3</span>))</a></code></pre></div>

<p>The first model, <strong>lm1.model</strong>, and the second model, <strong>lm2.model</strong>, are the same but different formulae. The first model uses a formula with arbitrary interactions. The second formula uses a much simpler and more convenient format which expands to include every possible combination of additive and interaction terms. For example, an interaction term has the format as the following: <strong>x1:x2, x2:x3, x1:x3, x1:x2:x3</strong>.</p>
<p>Notice that both models render the same summarized coefficients.
</p>
<div class="sourceCode" id="cb593"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb593-1" data-line-number="1"><span class="kw">summary</span>(lm1.model)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    2.945      2.119   1.390   0.1898
## x1            -8.243      5.260  -1.567   0.1430
## x2            -5.422      4.019  -1.349   0.2022
## x3            -6.802      4.152  -1.638   0.1273
## x1:x2         16.006      9.822   1.630   0.1291
## x2:x3         14.235      8.781   1.621   0.1309
## x1:x3         21.916     13.267   1.652   0.1245
## x1:x2:x3     -37.256     24.178  -1.541   0.1493</code></pre>
<div class="sourceCode" id="cb595"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb595-1" data-line-number="1"><span class="kw">summary</span>(lm2.model)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)    2.945      2.119   1.390   0.1898
## x1            -8.243      5.260  -1.567   0.1430
## x2            -5.422      4.019  -1.349   0.2022
## x3            -6.802      4.152  -1.638   0.1273
## x1:x2         16.006      9.822   1.630   0.1291
## x1:x3         21.916     13.267   1.652   0.1245
## x2:x3         14.235      8.781   1.621   0.1309
## x1:x2:x3     -37.256     24.178  -1.541   0.1493</code></pre>

<p>Also, in terms of <strong>significance of effect</strong>, none of the coefficients show any <strong>significance</strong> given the data.</p>
<p>The third model uses the notation <strong>I(.)</strong> to preserve the literal syntax and therefore does not perform a square or cube computation.</p>

<div class="sourceCode" id="cb597"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb597-1" data-line-number="1"><span class="kw">summary</span>(lm3.model)<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##                     Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)           -2.023      1.066  -1.898  0.08012
## x1                    69.667     41.699   1.671  0.11866
## I(x1^2)             -607.596    448.989  -1.353  0.19904
## I(x1^3)             2373.538   1870.448   1.269  0.22670
## x1:I(x1^3)         -4444.136   3525.578  -1.261  0.22963
## I(x1^2):I(x1^3)     3906.938   3062.678   1.276  0.22440
## x1:I(x1^2):I(x1^3) -1295.589   1000.864  -1.294  0.21802</code></pre>
<div class="sourceCode" id="cb599"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb599-1" data-line-number="1"><span class="kw">anova</span>(lm3.model)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: y
##                    Df Sum Sq Mean Sq F value Pr(&gt;F)  
## x1                  1   3.52    3.52    2.17  0.164  
## I(x1^2)             1   2.11    2.11    1.31  0.274  
## I(x1^3)             1   5.33    5.33    3.29  0.093 .
## x1:I(x1^3)          1   0.91    0.91    0.56  0.467  
## I(x1^2):I(x1^3)     1   0.06    0.06    0.04  0.845  
## x1:I(x1^2):I(x1^3)  1   2.71    2.71    1.68  0.218  
## Residuals          13  21.03    1.62                 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Notice that we can produce seven terms with only three independent variables (with seven coefficients including the intercept) based on the additional combination of their interactions. Therefore, if we have many more independent variables, the combination gets exponential. We may also have to determine if all independent variables are significant (meaningful), or we review each of them and perhaps exclude those that do not contribute to the effect.</p>
<p>In Chapters <strong>9</strong> (<strong>Computational Learning I</strong>) and <strong>10</strong> (<strong>Computational Learning II</strong>), we discuss <strong>Regression</strong> and <strong>Classification</strong> techniques such as <strong>Regression Trees</strong> that allow us to handle a large number of independent variables that create complex interactions.</p>
</div>
<div id="dummy-variables" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.6.3</span> Dummy Variables <a href="statistics.html#dummy-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>To extend the concept of <strong>Interaction</strong>, we introduce the notion of <strong>Dummy variables</strong>, also called <strong>indicator variables</strong>. These variables act as <strong>switches</strong>, and they apply to <strong>Categorical variables</strong>. We also call them <strong>Binary variables</strong> if the possible number of values of the categorical variables is two.</p>
<p>For example, for a categorical variable with two levels (e.g., female and male), we create one dummy variable:</p>
<p><span class="math display">\[
v1 = \begin{cases} 1 &amp; \text{female} \\ 0 &amp; \text{male} \end{cases}
\]</span>
For a categorical variable with three levels (e.g., weekly, monthly, yearly), we create three dummy variables:</p>
<p><span class="math display">\[
v1 = \begin{cases} 1 &amp; \text{weekly} \\ 0 &amp; \text{non-weekly} \end{cases}\ \ \ \ \
v2 = \begin{cases} 1 &amp; \text{monthly} \\ 0 &amp; \text{non-monthy} \end{cases}\ \ \ \ \
v3 = \begin{cases} 1 &amp; \text{yearly} \\ 0 &amp; \text{non-yearly} \end{cases}
\]</span>
We also introduce the concept of <strong>reference level</strong> in which our linear model uses only two dummy variables for a three-level categorical variable. Two <strong>dummy</strong> variables are chosen to represent two levels, and the other level becomes a reference level.</p>
<p>First, let us create a dataset with three predictor variables, namely <strong>productivity</strong>, <strong>gender</strong>, and <strong>timeline (tl)</strong> and a response variable, namely <strong>label</strong>. The <strong>gender</strong> and <strong>timeline (tl)</strong> variables are factors.</p>

<div class="sourceCode" id="cb601"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb601-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb601-2" data-line-number="2">N =<span class="st"> </span><span class="dv">30</span> </a>
<a class="sourceLine" id="cb601-3" data-line-number="3">gender =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;female&quot;</span>, <span class="st">&quot;male&quot;</span>)</a>
<a class="sourceLine" id="cb601-4" data-line-number="4">sched =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;weekly&quot;</span>, <span class="st">&quot;monthly&quot;</span>, <span class="st">&quot;yearly&quot;</span>)</a>
<a class="sourceLine" id="cb601-5" data-line-number="5">sample.gender =<span class="st"> </span><span class="kw">sample</span>(gender, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb601-6" data-line-number="6">sample.sched =<span class="st"> </span><span class="kw">sample</span>(sched, <span class="dt">size=</span>N, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb601-7" data-line-number="7">prod =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, N) <span class="op">+</span><span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n =</span> N, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) </a>
<a class="sourceLine" id="cb601-8" data-line-number="8">label =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, N) <span class="op">+</span><span class="st">  </span><span class="kw">rnorm</span>(<span class="dt">n =</span> N, <span class="dt">mean=</span><span class="dv">2</span>, <span class="dt">sd=</span><span class="dv">2</span>)  <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb601-9" data-line-number="9"><span class="st">    </span><span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(sample.gender)) <span class="op">*</span><span class="st"> </span></a>
<a class="sourceLine" id="cb601-10" data-line-number="10"><span class="st">    </span><span class="kw">as.numeric</span>(<span class="kw">as.factor</span>(sample.sched))</a>
<a class="sourceLine" id="cb601-11" data-line-number="11">dataset =<span class="st"> </span><span class="kw">data.frame</span>( <span class="dt">productivity =</span> prod,  <span class="dt">gender =</span> sample.gender, </a>
<a class="sourceLine" id="cb601-12" data-line-number="12">                      <span class="dt">tl =</span> sample.sched, <span class="dt">label =</span> label) <span class="co">#</span></a>
<a class="sourceLine" id="cb601-13" data-line-number="13"><span class="kw">head</span>(dataset)</a></code></pre></div>
<pre><code>##   productivity gender      tl  label
## 1       0.1875   male  yearly 12.818
## 2       1.2563 female  weekly  6.502
## 3       4.0953   male  yearly  7.803
## 4       6.4354 female  yearly 15.403
## 5       5.3881 female  weekly 10.910
## 6       6.2906 female monthly  9.737</code></pre>

<p>Let us list the levels for both <strong>gender</strong> and <strong>timeline (tl)</strong> variables:</p>

<div class="sourceCode" id="cb603"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb603-1" data-line-number="1"><span class="kw">levels</span>(dataset<span class="op">$</span>gender)</a></code></pre></div>
<pre><code>## [1] &quot;female&quot; &quot;male&quot;</code></pre>
<div class="sourceCode" id="cb605"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb605-1" data-line-number="1"><span class="kw">levels</span>(dataset<span class="op">$</span>tl)</a></code></pre></div>
<pre><code>## [1] &quot;monthly&quot; &quot;weekly&quot;  &quot;yearly&quot;</code></pre>

<p>Using the linear model function <strong>lm(.)</strong>, let us review the coefficients given only <strong>productivity</strong> and <strong>gender</strong> as predictors:</p>

<div class="sourceCode" id="cb607"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb607-1" data-line-number="1">(<span class="dt">model =</span> <span class="kw">lm</span>(label <span class="op">~</span><span class="st"> </span>productivity <span class="op">+</span><span class="st"> </span>gender, <span class="dt">data =</span> dataset))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = label ~ productivity + gender, data = dataset)
## 
## Coefficients:
##  (Intercept)  productivity    gendermale  
##         5.52          0.95          1.97</code></pre>

<p>We notice three coefficients: one for the intercept, one for the <strong>productivity</strong> predictor, and one for a newly created <strong>dummy variable</strong> labeled <strong>gendermale</strong>. This dummy variable belongs to the <strong>male gender</strong> level. The <strong>female gender</strong> is chosen as a <strong>reference level</strong> because it is the first level alphabetically. The general formula for our model is like so:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 v_1 + \epsilon
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(x_1\)</span> is the <strong>predictor variable</strong> for <strong>productivity</strong>,</li>
<li><span class="math inline">\(v_1\)</span> is a <strong>dummy variable</strong> for <strong>male gender</strong> category or level,</li>
<li><span class="math inline">\(\beta0\)</span> is the coefficient for the intercept,</li>
<li><span class="math inline">\(\beta1\)</span> is the coefficient for <span class="math inline">\(x_1\)</span>,</li>
<li><span class="math inline">\(\beta2\)</span> is the coefficient for <span class="math inline">\(v_1\)</span>.</li>
</ul>
<p>Here, <span class="math inline">\(v_1\)</span> is like a switch (or indicator) variable. If the observation shows that the gender is male, then <span class="math inline">\(v_1\)</span> becomes one, and thus we end up with the following <strong>regression</strong> formula:</p>
<p><span class="math display">\[\begin{align}
y = (\beta_0 + \beta_2) +  \beta_1 x_1  + \epsilon
\end{align}\]</span></p>
<p>Otherwise, if <span class="math inline">\(v_1\)</span> is zero, then we get the following <strong>regression</strong> formula:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 +  \beta_1 x_1  + \epsilon
\end{align}\]</span></p>
<p>Now, let review our linear model when we use both <strong>productivity</strong> and <strong>timeline</strong> as predictors:</p>

<div class="sourceCode" id="cb609"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb609-1" data-line-number="1">(<span class="dt">model =</span> <span class="kw">lm</span>(label <span class="op">~</span><span class="st"> </span>productivity <span class="op">+</span><span class="st"> </span>tl, <span class="dt">data =</span> dataset))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = label ~ productivity + tl, data = dataset)
## 
## Coefficients:
##  (Intercept)  productivity      tlweekly      tlyearly  
##         2.51          1.04          3.39          4.43</code></pre>

<p>We notice four coefficients: one for the intercept, one for the <strong>productivity</strong> predictor, and two newly created <strong>dummy variables</strong> called <strong>timelineweekly</strong> and <strong>timelineyearly</strong>. These dummy variables belong to the <strong>weekly</strong> and <strong>yearly</strong> timeline levels. The <strong>monthly timeline</strong> is chosen to be a <strong>reference level</strong> because it is the first level alphabetically. The general formula for our model is like so:</p>
<p><span class="math display">\[\begin{align}
y = \beta_0 + \beta_1 x_1 + \beta_2 v_1 + \beta_3 v_2 + \epsilon
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(x_1\)</span> is the <strong>predictor variable</strong> for <strong>productivity</strong>,</li>
<li><span class="math inline">\(v_1\)</span> is a <strong>dummy variable</strong> for <strong>weekly timeline</strong> category or level,</li>
<li><span class="math inline">\(v_2\)</span> is a <strong>dummy variable</strong> for <strong>yearly timeline</strong> category or level,</li>
<li><span class="math inline">\(\beta0\)</span> is the coefficient for the intercept,</li>
<li><span class="math inline">\(\beta1\)</span> is the coefficient for <span class="math inline">\(x_1\)</span>,</li>
<li><span class="math inline">\(\beta2\)</span> is the coefficient for <span class="math inline">\(v_1\)</span>,</li>
<li><span class="math inline">\(\beta3\)</span> is the coefficient for <span class="math inline">\(v_2\)</span>.</li>
</ul>
<p>We then have the following regression formulas:</p>

<p><span class="math display">\[
\underbrace{y = (\beta_0 + \beta_2) +  \beta_1 x_1 + \epsilon}_\text{weekly observation}\ \ \ \ \ \ \
\underbrace{y = (\beta_0 + \beta_3) +  \beta_1 x_1 + \epsilon}_\text{yearly observation}\ \ \ \ \ \ \ 
\underbrace{y = (\beta_0 + \beta_2 + \beta_3) +  \beta_1 x_1 + \epsilon}_\text{monthly observation}
\]</span>
</p>
<p>Lastly, let review our linear model when we use all predictors:</p>

<div class="sourceCode" id="cb611"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb611-1" data-line-number="1">(<span class="dt">model =</span> <span class="kw">lm</span>(label <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> dataset))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = label ~ ., data = dataset)
## 
## Coefficients:
##  (Intercept)  productivity    gendermale      tlweekly      tlyearly  
##         2.35          1.02          1.16          3.39          3.95</code></pre>

<p>Here, both <strong>female gender</strong> and <strong>monthly timeline</strong> become the <strong>reference levels</strong>. The general formula with three dummy variables becomes:</p>
<p><span class="math display">\[
y = \beta_0 + \beta_1 x_1 + \beta_2 v_1 + \beta_3 v_2 + \beta_4 v_3 + \epsilon
\]</span>
where:</p>
<ul>
<li><span class="math inline">\(x_1\)</span> is the <strong>predictor variable</strong> for <strong>productivity</strong>,</li>
<li><span class="math inline">\(v_1\)</span> is a <strong>dummy variable</strong> for <strong>male gender</strong> category or level,</li>
<li><span class="math inline">\(v_2\)</span> is a <strong>dummy variable</strong> for <strong>weekly timeline</strong> category or level,</li>
<li><span class="math inline">\(v_3\)</span> is a <strong>dummy variable</strong> for <strong>yearly timeline</strong> category or level,</li>
<li><span class="math inline">\(\beta0\)</span> is the coefficient for the intercept, ,</li>
<li><span class="math inline">\(\beta1\)</span> is the coefficient for <span class="math inline">\(x_1\)</span>,</li>
<li><span class="math inline">\(\beta2\)</span> is the coefficient for <span class="math inline">\(v_1\)</span>,</li>
<li><span class="math inline">\(\beta3\)</span> is the coefficient for <span class="math inline">\(v_2\)</span>.</li>
<li><span class="math inline">\(\beta4\)</span> is the coefficient for <span class="math inline">\(v_3\)</span>.</li>
</ul>
<p>For an observation with <strong>female gender</strong> and <strong>weekly timeline</strong> we have the following <strong>regression</strong> formula:</p>
<p><span class="math display">\[
y = (\beta_0 + \beta_3) + \beta_1 x_1 + \epsilon
\]</span></p>
<p>We leave readers to generate the other <strong>regression formulas</strong>.</p>
<p>Regarding <strong>significance of regression</strong>, we can see below that the <strong>productivity</strong> predictor is significant with P-value less than the chosen alpha, which is <span class="math inline">\(\alpha = 0.001\)</span>. It also shows that the dummy variables <strong>gendermale</strong> and <strong>timelineyearly</strong> are significant at their own chosen alpha (see significance code).</p>

<div class="sourceCode" id="cb613"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb613-1" data-line-number="1">(<span class="dt">p =</span> <span class="kw">summary</span>(model))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = label ~ ., data = dataset)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.838 -1.844 -0.297  1.551  5.372 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    2.3461     1.4350    1.63   0.1146    
## productivity   1.0211     0.0616   16.58  5.4e-15 ***
## gendermale     1.1617     1.0667    1.09   0.2865    
## tlweekly       3.3898     1.2541    2.70   0.0122 *  
## tlyearly       3.9514     1.3003    3.04   0.0055 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.66 on 25 degrees of freedom
## Multiple R-squared:  0.924,  Adjusted R-squared:  0.912 
## F-statistic: 76.1 on 4 and 25 DF,  p-value: 1.25e-13</code></pre>

<p>On the other hand, the <strong>weekly timeline</strong> level has 1.2% chance of not being meaningful.</p>
<p>See Figure <a href="statistics.html#fig:dummyvar">6.18</a> for the plot of our dataset. In the illustration, we emphasize the <strong>timeline</strong> categories.</p>

<div class="sourceCode" id="cb615"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb615-1" data-line-number="1">model =<span class="st"> </span><span class="kw">lm</span>(label <span class="op">~</span><span class="st"> </span>productivity <span class="op">+</span><span class="st"> </span>tl, <span class="dt">data =</span> dataset)</a>
<a class="sourceLine" id="cb615-2" data-line-number="2">model<span class="op">$</span>coefficients</a></code></pre></div>
<pre><code>##  (Intercept) productivity     tlweekly     tlyearly 
##        2.508        1.037        3.386        4.431</code></pre>

<p>Let us generate the intercepts for each timeline and the slope to plot the regression lines.</p>

<div class="sourceCode" id="cb617"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb617-1" data-line-number="1">slope=<span class="st">  </span>model<span class="op">$</span>coefficients[<span class="dv">2</span>] </a>
<a class="sourceLine" id="cb617-2" data-line-number="2"><span class="co"># B0, if V1 and V2 are zero</span></a>
<a class="sourceLine" id="cb617-3" data-line-number="3">intrcpt.monthly =<span class="st"> </span>model<span class="op">$</span>coefficients[<span class="dv">1</span>]  </a>
<a class="sourceLine" id="cb617-4" data-line-number="4"><span class="co"># B0 + B2, if V1 = 1</span></a>
<a class="sourceLine" id="cb617-5" data-line-number="5">intrcpt.weekly  =<span class="st"> </span>intrcpt.monthly <span class="op">+</span><span class="st"> </span>model<span class="op">$</span>coefficients[<span class="dv">3</span>] </a>
<a class="sourceLine" id="cb617-6" data-line-number="6"><span class="co"># B0 + B3, if V2 = 1</span></a>
<a class="sourceLine" id="cb617-7" data-line-number="7">intrcpt.yearly  =<span class="st"> </span>intrcpt.monthly <span class="op">+</span><span class="st"> </span>model<span class="op">$</span>coefficients[<span class="dv">4</span>]  </a></code></pre></div>
<div class="sourceCode" id="cb618"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb618-1" data-line-number="1">col=<span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>, <span class="st">&quot;brown&quot;</span>)</a>
<a class="sourceLine" id="cb618-2" data-line-number="2"><span class="kw">plot</span>(label <span class="op">~</span><span class="st"> </span>productivity , <span class="dt">data =</span> dataset, </a>
<a class="sourceLine" id="cb618-3" data-line-number="3">     <span class="dt">col =</span> col[<span class="kw">as.numeric</span>(tl)], <span class="dt">pch =</span> <span class="kw">as.numeric</span>(tl),</a>
<a class="sourceLine" id="cb618-4" data-line-number="4">     <span class="dt">xlab=</span><span class="st">&quot;predictors&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;response&quot;</span>,</a>
<a class="sourceLine" id="cb618-5" data-line-number="5">     <span class="dt">main=</span><span class="st">&quot;Dummy Variable (Timeline)&quot;</span> )</a>
<a class="sourceLine" id="cb618-6" data-line-number="6"><span class="kw">legend</span>(<span class="st">&quot;bottomright&quot;</span>,  <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;weekly&quot;</span>,  <span class="st">&quot;monthly&quot;</span>, <span class="st">&quot;yearly&quot;</span>),</a>
<a class="sourceLine" id="cb618-7" data-line-number="7">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;black&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;brown&quot;</span>),  <span class="dt">pch=</span><span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">1</span>,<span class="dv">3</span>),  <span class="dt">cex=</span><span class="fl">0.8</span>)</a>
<a class="sourceLine" id="cb618-8" data-line-number="8"><span class="kw">abline</span>(intrcpt.weekly, slope, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb618-9" data-line-number="9"><span class="kw">abline</span>(intrcpt.monthly, slope, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb618-10" data-line-number="10"><span class="kw">abline</span>(intrcpt.yearly, slope, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:dummyvar"></span>
<img src="DS_files/figure-html/dummyvar-1.png" alt="Dummy Variable (Timeline)" width="70%" />
<p class="caption">
Figure 6.18: Dummy Variable (Timeline)
</p>
</div>

<p>We leave readers to experiment with the interaction of <strong>gender</strong> and <strong>timeline</strong>.</p>
</div>
<div id="model-selection" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.6.4</span> Model Selection <a href="statistics.html#model-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Model selection</strong>, in the context of <strong>Linear Regression</strong>, is about determining relevant independent variables. There are complementary techniques such as <strong>regularization</strong> and <strong>primary component analysis (PCA)</strong> that allow the determination of relevant independent variables. Such techniques are discussed in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) for <strong>Feature Selection</strong> as part of <strong>Feature Engineering</strong>. Here, we discuss one technique not so much in evaluating independent variables but by modeling a combination of additive and interactive relationships of covariates. Then, we evaluate the resulting models and determine which model may be deemed the best (<strong>correctly specified</strong>) based on using <strong>AIC</strong> and <strong>BIC</strong>.</p>
<p><strong>Aikike Information Criterion (AIC)</strong>  </p>
<p>In <strong>F-Test with Two-Way Anova</strong>, we perform <strong>significance of difference</strong> analysis using dataset <strong>mtcars</strong>. Here, we use the same dataset and perform <strong>model selection</strong> using a <strong>step-wise</strong> greedy iterative <strong>leave-one-out cross-validation (LOOCV)</strong> approach with the following equation:</p>
<p><span class="math display">\[\begin{align}
LOOCV(formula_{\{inititial\}}) = \underset{aic}{\mathrm{argmin}}\ AIC(formula_{\{inititial\}}).
\end{align}\]</span></p>
<p><strong>AIC</strong> is computed based on the following equation:</p>
<p><span class="math display">\[\begin{align}
AIC = -2\cdot \log_e\ \mathcal{L} + p \cdot 2 = n\cdot  \log_e\ \left(\frac{RSS}{n}\right) + p\cdot k \ \ \ \ \ \ where\ k = 2
\end{align}\]</span></p>
<p>and where:</p>
<ul>
<li><strong>p</strong> is the number of <span class="math inline">\(\beta\)</span> coefficients</li>
<li><span class="math inline">\(\mathbf{\log_e\mathcal{L}}\)</span> is the log likelihood</li>
</ul>
<p>Note that <strong>Likehood</strong> is discussed in much depth in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under <strong>Likelihood</strong> Subsection under <strong>Bayes Theorem</strong> Section. .</p>
<p>Here is a sample implementation of <strong>AIC</strong> using a built-in function called <strong>step()</strong>.</p>

<div class="sourceCode" id="cb619"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb619-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st">  </span>disp <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec , <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb619-2" data-line-number="2">selected.model =<span class="st"> </span><span class="kw">step</span>(initial.model, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, </a>
<a class="sourceLine" id="cb619-3" data-line-number="3">                      <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=65.47
## mpg ~ disp + hp + drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - disp  1       4.0 174 64.2
## &lt;none&gt;              170 65.5
## - hp    1      11.9 182 65.6
## - qsec  1      12.7 183 65.8
## - drat  1      15.5 186 66.3
## - wt    1      81.4 252 76.0
## 
## Step:  AIC=64.21
## mpg ~ hp + drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - hp    1       9.4 184 63.9
## - qsec  1       9.6 184 63.9
## &lt;none&gt;              174 64.2
## - drat  1      12.0 186 64.3
## + disp  1       4.0 170 65.5
## - wt    1     113.9 288 78.3
## 
## Step:  AIC=63.89
## mpg ~ drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## &lt;none&gt;              184 63.9
## - drat  1      11.9 195 63.9
## + hp    1       9.4 174 64.2
## + disp  1       1.5 182 65.6
## - qsec  1      85.7 269 74.2
## - wt    1     275.7 459 91.2</code></pre>

<p>First of all, tracing is enabled so that we see each step that <strong>AIC</strong> takes. The idea is to minimize the <strong>CV</strong> value using the <strong>LOOCV</strong> approach.</p>
<p>In step 1, we see an initial <strong>AIC</strong> score of 65.47 based on the following initial formula: <strong>mpg ~ disp + hp + drat + wt + qsec</strong>. From there, it tries to evaluate all possible combination based on the initial formula. Note that the entry with <strong>&lt;none&gt;</strong> is the entry with the current formula: <strong>mpg ~ disp + hp + drat + wt + qsec</strong>.</p>
<p>It first tries to remove the variable <strong>disp</strong> given the notation (- disp) and then computes for RSS and AIC. It finds that if it drops the <strong>disp</strong> variable from the formula, ending up with only this: <strong>mpg ~ hp + drat + wt + qsec</strong>, then the <strong>AIC score</strong> is 64.2. This score is better (smaller) than the current score of 65.47.</p>
<p>Then it continues to try the next combination by dropping the <strong>hp</strong> variable only. This generates the score of 65.6 with the following formula: <strong>mpg ~ disp + drat + wt + qsec</strong>. It finds that the score is higher than the current.</p>
<p>AIC performs further validations and tries dropping other variables one at a time. After which, it determines that the best combination with the lowest score at 64.2 is one in which it has to drop the <strong>disp</strong> variable to get the following final formula for the next step: <strong>mpg ~ hp + drat + wt + qsec</strong>.</p>
<p>Therefore, in step 2, we have a score of 64.2 with the formula: *mpg ~ hp + drat + wt + qsec**. Then, the process starts again with the evaluation until it finds a score lower than the current one in step 2.</p>
<p><strong>AIC</strong> makes three steps, and each one is scored with an <strong>AIC score</strong>. It determines that after step 3, there is no other AIC score lower than 63.89 with the final formula: mpg ~ drat + wt + qsec. It stops the iteration.</p>
<p>We now display the best model selected by <strong>AIC step</strong>. The best model selected is based on the lowest <strong>AIC</strong> score. In this case, the lowest score is 63.89 in step 3.</p>

<div class="sourceCode" id="cb621"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb621-1" data-line-number="1">selected.model</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ drat + wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)         drat           wt         qsec  
##      11.394        1.656       -4.398        0.946</code></pre>

<p>To illustrate how we computed for the lowest <strong>AIC score</strong> and ultimately arrived at the best model, let us recall the <strong>Least-Squares</strong> equation:</p>
<p><span class="math display">\[\begin{align}
RSS(\beta_{least}) = |y - X\beta|^2 = \sum_{i=1}^n (y_{i} - x_{i}\beta)^2,
\end{align}\]</span></p>
<p>Our objective function is expressed as such:</p>
<p><span class="math display">\[\begin{align}
\hat{\beta}_{least} = \underset{\beta}{\mathrm{argmin}}\ RSS(\beta_{least}).
\end{align}\]</span></p>
<p>Let us show two ways to generate the <strong>RSS</strong> and, ultimately, the <strong>AIC</strong> score for step 3.</p>
<p><strong>First</strong>, we can use the built-in function <strong>lm(.)</strong> like so (note that we use the formula obtained for the best model for demonstration):</p>

<div class="sourceCode" id="cb623"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb623-1" data-line-number="1">y.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb623-2" data-line-number="2"><span class="kw">coef</span>(y.model)</a></code></pre></div>
<pre><code>## (Intercept)        drat          wt        qsec 
##     11.3945      1.6561     -4.3978      0.9462</code></pre>
<div class="sourceCode" id="cb625"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb625-1" data-line-number="1">RSS =<span class="st"> </span><span class="kw">sum</span>( <span class="kw">resid</span>(y.model)<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb625-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;RSS (using manual)&quot;</span>=RSS, </a>
<a class="sourceLine" id="cb625-3" data-line-number="3">  <span class="st">&quot;RSS (using deviance function)&quot;</span>=<span class="kw">deviance</span>(y.model))</a></code></pre></div>
<pre><code>##            RSS (using manual) RSS (using deviance function) 
##                         183.5                         183.5</code></pre>

<p><strong>Second</strong>, let us do it the manual way using <strong>matrix equation</strong> (or we can also use the equation above for least square):</p>

<div class="sourceCode" id="cb627"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb627-1" data-line-number="1">A =<span class="st"> </span><span class="kw">with</span>(mtcars, <span class="kw">cbind</span>(<span class="dv">1</span> , drat , wt, qsec))</a>
<a class="sourceLine" id="cb627-2" data-line-number="2">y =<span class="st"> </span>mtcars<span class="op">$</span>mpg</a>
<a class="sourceLine" id="cb627-3" data-line-number="3">beta.hat =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>y <span class="co"># matrix equation</span></a>
<a class="sourceLine" id="cb627-4" data-line-number="4"><span class="kw">colnames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;coefficients&quot;</span>)</a>
<a class="sourceLine" id="cb627-5" data-line-number="5"><span class="kw">rownames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span>, <span class="st">&quot;drat&quot;</span> ,  <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;qsec&quot;</span>)</a>
<a class="sourceLine" id="cb627-6" data-line-number="6"><span class="kw">t</span>(beta.hat)</a></code></pre></div>
<pre><code>##              intercept  drat     wt   qsec
## coefficients     11.39 1.656 -4.398 0.9462</code></pre>
<div class="sourceCode" id="cb629"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb629-1" data-line-number="1">y.hat =<span class="st"> </span>( beta.hat[<span class="dv">1</span>,] <span class="op">+</span><span class="st">  </span>beta.hat[<span class="dv">2</span>,] <span class="op">*</span><span class="st"> </span>mtcars<span class="op">$</span>drat <span class="op">+</span></a>
<a class="sourceLine" id="cb629-2" data-line-number="2"><span class="st">         </span>beta.hat[<span class="dv">3</span>,] <span class="op">*</span><span class="st"> </span>mtcars<span class="op">$</span>wt <span class="op">+</span><span class="st"> </span>beta.hat[<span class="dv">4</span>,] <span class="op">*</span><span class="st"> </span>mtcars<span class="op">$</span>qsec )</a>
<a class="sourceLine" id="cb629-3" data-line-number="3"><span class="co"># or y.hat = A %*% beta.hat</span></a>
<a class="sourceLine" id="cb629-4" data-line-number="4">(<span class="dt">RSS =</span> <span class="kw">sum</span>( (y <span class="op">-</span><span class="st"> </span>y.hat)<span class="op">^</span><span class="dv">2</span> ))</a></code></pre></div>
<pre><code>## [1] 183.5</code></pre>

<p><strong>Finally</strong>, let us compute for the <strong>AIC score</strong> at step 3. This uses natural log and therefore we use base <strong>exp(1)</strong> = 2.7183:</p>

<div class="sourceCode" id="cb631"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb631-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(mtcars)</a>
<a class="sourceLine" id="cb631-2" data-line-number="2">p =<span class="st"> </span><span class="kw">length</span>(beta.hat) <span class="co"># no of. coefficients (including intercept)</span></a>
<a class="sourceLine" id="cb631-3" data-line-number="3">k =<span class="st"> </span><span class="dv">2</span> <span class="co"># used by BIC</span></a>
<a class="sourceLine" id="cb631-4" data-line-number="4">base =<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>) <span class="co"># natural log</span></a>
<a class="sourceLine" id="cb631-5" data-line-number="5">(<span class="dt">AIC =</span> n <span class="op">*</span><span class="st"> </span><span class="kw">log</span>( RSS <span class="op">/</span><span class="st"> </span>n, base ) <span class="op">+</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>k)  </a></code></pre></div>
<pre><code>## [1] 63.89</code></pre>

<p>Another way to obtain <strong>AIC</strong> score is to use the built-in R function called <strong>AIC(.)</strong>:</p>

<div class="sourceCode" id="cb633"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb633-1" data-line-number="1"><span class="kw">AIC</span>(y.model)</a></code></pre></div>
<pre><code>## [1] 156.7</code></pre>

<p>This call to <strong>AIC()</strong> undergoes comprehensive cross-validation, and so the <strong>AIC</strong> score is based on the lowest score from this exhaustive validation.</p>
<p>We can also compute the <strong>AIC</strong> score by hand like so (note, we can use the built-in function called <strong>logLik()</strong> to compute for log-likelihood):</p>

<div class="sourceCode" id="cb635"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb635-1" data-line-number="1">n      =<span class="st"> </span><span class="kw">nrow</span>(mtcars)</a>
<a class="sourceLine" id="cb635-2" data-line-number="2">p      =<span class="st"> </span><span class="kw">length</span>(beta.hat) <span class="co"># coefficients</span></a>
<a class="sourceLine" id="cb635-3" data-line-number="3">df     =<span class="st"> </span>n <span class="op">-</span><span class="st"> </span>p</a>
<a class="sourceLine" id="cb635-4" data-line-number="4">se     =<span class="st"> </span><span class="kw">sqrt</span>( RSS <span class="op">/</span><span class="st"> </span>df) <span class="co"># standard error</span></a>
<a class="sourceLine" id="cb635-5" data-line-number="5">sd.hat =<span class="st"> </span>se <span class="op">*</span><span class="st"> </span><span class="kw">sqrt</span>((n<span class="op">-</span>p)<span class="op">/</span>n)</a>
<a class="sourceLine" id="cb635-6" data-line-number="6">mu.hat =<span class="st"> </span><span class="kw">fitted</span>(y.model)</a>
<a class="sourceLine" id="cb635-7" data-line-number="7">logLikelihood =<span class="st"> </span><span class="kw">sum</span>(<span class="kw">dnorm</span>(mtcars<span class="op">$</span>mpg,</a>
<a class="sourceLine" id="cb635-8" data-line-number="8">                <span class="dt">mean =</span> mu.hat, <span class="dt">sd =</span> sd.hat, <span class="dt">log=</span><span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb635-9" data-line-number="9"><span class="co"># or logLikelihood = as.numeric(logLik(y.model))</span></a>
<a class="sourceLine" id="cb635-10" data-line-number="10">(<span class="dt">AIC =</span> <span class="dv">-2</span> <span class="op">*</span><span class="st"> </span><span class="kw">as.numeric</span>( logLikelihood ) <span class="op">+</span><span class="st"> </span>k <span class="op">*</span><span class="st"> </span>(p <span class="op">+</span><span class="st"> </span><span class="dv">1</span>))</a></code></pre></div>
<pre><code>## [1] 156.7</code></pre>

<p><strong>Bayesian Information Criterion (BIC)</strong>  </p>
<p><strong>BIC</strong> is similar to <strong>AIC</strong> in that it is also a step-wise iterative method. The difference is with the use of <strong>k</strong>.</p>
<p><span class="math display">\[\begin{align}
BIC = -2\cdot \log_e\ \mathcal{L} + p \cdot ln(n) = n\cdot \log_e\left(\frac{RSS}{n}\right) + p\cdot k
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>k</strong> is <span class="math inline">\(ln(n)\)</span>,</li>
<li><strong>p</strong> is the number of <span class="math inline">\(\beta\)</span> coefficients, and</li>
<li><span class="math inline">\(\mathbf{\log_e\mathcal{L}}\)</span> is the log likelihood.</li>
</ul>
<p>Note that we discuss <strong>likelihood</strong> in-depth in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under the <strong>Likelihood</strong> Sub-section.</p>
<p>Here is a sample implementation of <strong>AIC</strong> using a built-in function called <strong>step(.)</strong>.</p>

<div class="sourceCode" id="cb637"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb637-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(mtcars)</a>
<a class="sourceLine" id="cb637-2" data-line-number="2">selected.model =<span class="st"> </span><span class="kw">step</span>(initial.model, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, </a>
<a class="sourceLine" id="cb637-3" data-line-number="3">                      <span class="dt">k=</span><span class="kw">log</span>(n), <span class="dt">trace =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=74.26
## mpg ~ disp + hp + drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - disp  1       4.0 174 71.5
## - hp    1      11.9 182 73.0
## - qsec  1      12.7 183 73.1
## - drat  1      15.5 186 73.6
## &lt;none&gt;              170 74.3
## - wt    1      81.4 252 83.3
## 
## Step:  AIC=71.53
## mpg ~ hp + drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - hp    1       9.4 184 69.8
## - qsec  1       9.6 184 69.8
## - drat  1      12.0 186 70.2
## &lt;none&gt;              174 71.5
## + disp  1       4.0 170 74.3
## - wt    1     113.9 288 84.2
## 
## Step:  AIC=69.75
## mpg ~ drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - drat  1      11.9 195 68.3
## &lt;none&gt;              184 69.8
## + hp    1       9.4 174 71.5
## + disp  1       1.5 182 73.0
## - qsec  1      85.7 269 78.6
## - wt    1     275.7 459 95.6
## 
## Step:  AIC=68.31
## mpg ~ wt + qsec
## 
##        Df Sum of Sq RSS   AIC
## &lt;none&gt;              195  68.3
## + drat  1        12 184  69.8
## + hp    1         9 186  70.2
## + disp  1         0 195  71.8
## - qsec  1        83 278  76.1
## - wt    1       733 929 114.7</code></pre>

<p>Notice that <strong>BIC</strong>, in this particular case only, iterates one more step, the fourth step, which has an <strong>AIC</strong> score of 68.31. It is, however, higher than the score of 61.89 we obtained previously.</p>
<p>We now display the best model selected by <strong>BIC step</strong>. The best model selected is based on the lowest <strong>BIC</strong> score. In this case, the lowest score is 68.31 in step 4.</p>

<div class="sourceCode" id="cb639"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb639-1" data-line-number="1">selected.model</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)           wt         qsec  
##      19.746       -5.048        0.929</code></pre>

<p>Let us compute for <strong>RSS</strong> using the modelâs selected formula:</p>

<div class="sourceCode" id="cb641"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb641-1" data-line-number="1">y.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st">  </span>wt <span class="op">+</span><span class="st"> </span>qsec, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb641-2" data-line-number="2"><span class="kw">coef</span>(y.model)</a></code></pre></div>
<pre><code>## (Intercept)          wt        qsec 
##     19.7462     -5.0480      0.9292</code></pre>
<div class="sourceCode" id="cb643"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb643-1" data-line-number="1">(<span class="dt">RSS =</span> <span class="kw">sum</span>( <span class="kw">resid</span>(y.model)<span class="op">^</span><span class="dv">2</span> ))</a></code></pre></div>
<pre><code>## [1] 195.5</code></pre>

<p>Then we compute for the <strong>AIC score</strong> in step 4. This uses natural log and therefore we use base <strong>exp(1)</strong> = 2.7183:</p>

<div class="sourceCode" id="cb645"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb645-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(mtcars)</a>
<a class="sourceLine" id="cb645-2" data-line-number="2">base =<span class="st"> </span><span class="kw">exp</span>(<span class="dv">1</span>) <span class="co"># natural log</span></a>
<a class="sourceLine" id="cb645-3" data-line-number="3">p =<span class="st"> </span><span class="kw">length</span>(<span class="kw">coef</span>(y.model)) <span class="co"># no. of coefficients (including intercept)</span></a>
<a class="sourceLine" id="cb645-4" data-line-number="4">k =<span class="st"> </span><span class="kw">log</span>(n, base) <span class="co"># used by BIC</span></a>
<a class="sourceLine" id="cb645-5" data-line-number="5">(<span class="dt">AIC =</span> n <span class="op">*</span><span class="st"> </span><span class="kw">log</span>( RSS <span class="op">/</span><span class="st"> </span>n, base ) <span class="op">+</span><span class="st"> </span>p <span class="op">*</span><span class="st"> </span>k)</a></code></pre></div>
<pre><code>## [1] 68.31</code></pre>

<p>Similarly, we can obtain <strong>BIC</strong> score using the built-in R function called <strong>BIC(.)</strong>:</p>

<div class="sourceCode" id="cb647"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb647-1" data-line-number="1"><span class="kw">BIC</span>(y.model)</a></code></pre></div>
<pre><code>## [1] 162.6</code></pre>

<p>In terms of the <strong>step-wise</strong> iteration method itself, there are three ways to step through model selection: <strong>forward step, backward step, or both</strong>. We require an initial model as a starting point for the iteration. The initial model, in part, dictates how many steps to take - of course, if the best model is closer from the beginning, then a forward step is preferable and much faster. However, that is if we have some sense of the estimated step.</p>
<p>Below is a <strong>forward step</strong> direction used starting from the <strong>intercept</strong> and forward up to the final formula provided: ~disp + drat + wt + qsec:</p>

<div class="sourceCode" id="cb649"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb649-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span><span class="dv">1</span> , <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb649-2" data-line-number="2">selected.model =<span class="st"> </span><span class="kw">step</span>(initial.model,  </a>
<a class="sourceLine" id="cb649-3" data-line-number="3">            <span class="dt">scope=</span>  <span class="op">~</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec,</a>
<a class="sourceLine" id="cb649-4" data-line-number="4">            <span class="dt">direction =</span> <span class="st">&quot;forward&quot;</span>, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=115.9
## mpg ~ 1
## 
##        Df Sum of Sq  RSS   AIC
## + wt    1       848  278  73.2
## + disp  1       809  317  77.4
## + drat  1       522  604  98.0
## + qsec  1       197  929 111.8
## &lt;none&gt;              1126 115.9
## 
## Step:  AIC=73.22
## mpg ~ wt
## 
##        Df Sum of Sq RSS  AIC
## + qsec  1      82.9 196 63.9
## + disp  1      31.6 247 71.4
## &lt;none&gt;              278 73.2
## + drat  1       9.1 269 74.2
## 
## Step:  AIC=63.91
## mpg ~ wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## + drat  1      11.9 184 63.9
## &lt;none&gt;              196 63.9
## + disp  1       0.0 196 65.9
## 
## Step:  AIC=63.89
## mpg ~ wt + qsec + drat
## 
##        Df Sum of Sq RSS  AIC
## &lt;none&gt;              184 63.9
## + disp  1      1.51 182 65.6</code></pre>

<p>Below is a <strong>backward step</strong> direction used starting from the provided formula, ~disp + drat + wt + qsec, and then steps back down to a formula with only the intercept. Notice that it does not have to reach the intercept because, after two steps, it has found a model with the lowest AIC score.</p>

<div class="sourceCode" id="cb651"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb651-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>drat <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec , <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb651-2" data-line-number="2">selected.model =<span class="st"> </span><span class="kw">step</span>(initial.model,  <span class="dt">scope=</span>  <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb651-3" data-line-number="3">            <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">trace =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>## Start:  AIC=65.63
## mpg ~ disp + drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## - disp  1       1.5 184 63.9
## &lt;none&gt;              182 65.6
## - drat  1      13.4 196 65.9
## - qsec  1      61.7 244 73.0
## - wt    1     109.3 291 78.7
## 
## Step:  AIC=63.89
## mpg ~ drat + wt + qsec
## 
##        Df Sum of Sq RSS  AIC
## &lt;none&gt;              184 63.9
## - drat  1      11.9 195 63.9
## - qsec  1      85.7 269 74.2
## - wt    1     275.7 459 91.2</code></pre>

<p>In a case where we want to have a more comprehensive iteration for the best model, we can set the trace to false and get the final result like so:</p>

<div class="sourceCode" id="cb653"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb653-1" data-line-number="1">initial.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>., <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb653-2" data-line-number="2">( <span class="dt">selected.model =</span> <span class="kw">step</span>(initial.model,  <span class="dt">scope=</span>  <span class="op">~</span><span class="st"> </span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb653-3" data-line-number="3">            <span class="dt">direction =</span> <span class="st">&quot;backward&quot;</span>, <span class="dt">k=</span><span class="dv">2</span>, <span class="dt">trace =</span> <span class="ot">FALSE</span>) )</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ wt + qsec + am, data = mtcars)
## 
## Coefficients:
## (Intercept)           wt         qsec           am  
##        9.62        -3.92         1.23         2.94</code></pre>
<div class="sourceCode" id="cb655"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb655-1" data-line-number="1"><span class="kw">anova</span>(selected.model)</a></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Response: mpg
##           Df Sum Sq Mean Sq F value  Pr(&gt;F)    
## wt         1    848     848  140.21   2e-12 ***
## qsec       1     83      83   13.70 0.00093 ***
## am         1     26      26    4.33 0.04672 *  
## Residuals 28    169       6                    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>We can use the following built-in function called <strong>extractAIC(.)</strong> to derive the lowest AIC score immediately for an exhaustive search.</p>

<div class="sourceCode" id="cb657"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb657-1" data-line-number="1"><span class="kw">extractAIC</span>(selected.model, <span class="dt">k=</span><span class="kw">log</span>(n))</a></code></pre></div>
<pre><code>## [1]  4.00 67.17</code></pre>

<p>Additionally, we also can plot the search path taken by AIC. See Figure <a href="statistics.html#fig:aicsearch">6.19</a>.</p>

<div class="sourceCode" id="cb659"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb659-1" data-line-number="1">n =<span class="st"> </span><span class="kw">nrow</span>(mtcars)</a>
<a class="sourceLine" id="cb659-2" data-line-number="2">selected.model =<span class="st"> </span><span class="kw">step</span>(initial.model, <span class="dt">direction =</span> <span class="st">&quot;both&quot;</span>, </a>
<a class="sourceLine" id="cb659-3" data-line-number="3">                      <span class="dt">k=</span><span class="kw">log</span>(n), <span class="dt">trace =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb659-4" data-line-number="4"><span class="kw">plot</span>(selected.model<span class="op">$</span>anova<span class="op">$</span>AIC, <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">65</span>,<span class="dv">88</span>),</a>
<a class="sourceLine" id="cb659-5" data-line-number="5">     <span class="dt">xlab=</span><span class="st">&quot;steps&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;AIC score&quot;</span>,</a>
<a class="sourceLine" id="cb659-6" data-line-number="6">     <span class="dt">main=</span><span class="st">&quot;Model Selection (Using LOOCV-BIC)&quot;</span>)</a>
<a class="sourceLine" id="cb659-7" data-line-number="7"><span class="kw">lines</span>(selected.model<span class="op">$</span>anova<span class="op">$</span>AIC, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb659-8" data-line-number="8"><span class="kw">text</span>(selected.model<span class="op">$</span>anova<span class="op">$</span>AIC <span class="dv">-2</span>, </a>
<a class="sourceLine" id="cb659-9" data-line-number="9">     <span class="dt">labels=</span><span class="kw">round</span>(selected.model<span class="op">$</span>anova<span class="op">$</span>AIC,<span class="dv">1</span>))</a>
<a class="sourceLine" id="cb659-10" data-line-number="10"><span class="kw">grid</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:aicsearch"></span>
<img src="DS_files/figure-html/aicsearch-1.png" alt="Model Selection  (Using LOOCV-BIC)" width="70%" />
<p class="caption">
Figure 6.19: Model Selection (Using LOOCV-BIC)
</p>
</div>

<p>As we can see now, <strong>model selection</strong> is also about <strong>coefficient selection</strong>. The independent variables with coefficients that can contribute to a low <strong>AIC</strong> score get to be included in the model. In effect, this is also about <strong>variable selection</strong>.</p>
<p>We continue this discussion in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under <strong>Feature Selection</strong> Section.</p>
</div>
</div>
<div id="regression-analysis" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.7</span> Regression Analysis <a href="statistics.html#regression-analysis" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Apart from handling <strong>model selection</strong>, it helps to also analyze the quality of regression, especially around the selected model and its goodness of fit. This section discusses the distribution property of residuals and their relationship with the fitted outcome.</p>
<div id="assumptions" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.1</span> Assumptions<a href="statistics.html#assumptions" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In statistics, we need to be able to measure statistics and understand assumptions and the violations against these assumptions.</p>
<p>We look at the following:</p>
<ul>
<li>Correlation</li>
<li>Homoscedasticity vs.Â Heteroscedasticity</li>
<li>Normality and Leverage</li>
<li>Collinearity</li>
</ul>
</div>
<div id="correlation-coefficients" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.2</span> Correlation Coefficients <a href="statistics.html#correlation-coefficients" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We introduce three common correlation coefficients in this section that deal with measuring the strength of the relationship or association between two quantities.</p>
<p><strong>Pearsonâs Rank Correlation Coefficients</strong> measures the strength of the relationship between two continuous variables. The measure is expressed as such: </p>
<p><span class="math display">\[\begin{align}
r_{xy} = \frac{S_{R_xR_y}}{\sqrt{S_{R_x} S_{R_y}}} = \frac{\sum(R_x - \bar{R}_x)(R_y-\bar{R}_y)}{\sqrt{\sum(R_x-\bar{R}_x)^2\sum(R_y - \bar{R}_y)^2}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{R_x}\)</span> is xâs rank</li>
<li><span class="math inline">\(\mathbf{R_y}\)</span> is yâs rank</li>
</ul>
<p>and where:</p>
<p><span class="math display">\[\begin{align}
S_{R_xR_y} {}&amp;=  n \sum_{i=1}^n R_{x_i} R_{y_i} - \sum_{i=1}^n R_{x_i} \sum_{i=1}^n R_{y_i} \\
S_{R_x} &amp;=  n\sum_{i=1}^n R_{x_i}^2 - (\sum_{i=1}^n R_{x_i})^2 \\
S_{R_y} &amp;=  n \sum_{i=1}^n R_{y_i}^2 -(\sum_{i=1}^n R_{y_i})^2
\end{align}\]</span></p>
<p><strong>Spearmanâs Rank Correlation Coefficients</strong> measures the strength of the monotonic relationship between two continuous variables. A monotonic relationship means that as one variable increases continuously, the other variable also increases or decreases. The variable cannot increase along the way and then decreases. It also cannot decrease along the way then increases. </p>
<p><span class="math display">\[\begin{align}
r_s = 1 - \frac{6\left( \sum_{i=1}^n D_{i=1}^2\right)}{n(n^2 - 1)}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>D</strong> is the difference in rank within pairs</li>
</ul>
<p><strong>Kendallâs Rank Correlation</strong> measures the strength of the ordinal relationship between two measured variables. It is also called <strong>Kendall Tau (<span class="math inline">\(\tau\)</span>) Rank Correlation</strong> in which tau (<span class="math inline">\(\tau\)</span>) is the coefficient. </p>
<p><span class="math display">\[\begin{align}
\tau = \frac{n_c - n_d}{\frac{1}{2}n(n - 1)}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\mathbf{n_c}\)</span> - number of concordant pairs</li>
<li><span class="math inline">\(\mathbf{n_d}\)</span> - number of discordant pairs</li>
</ul>
<p>Note that a <strong>concordant</strong> pair is a pair of observations in which a subject is higher on both pairs or lower on both pairs, e.g. ( S &gt; X and S &gt; Y ) or ( S &lt; X and S &lt; Y ). On the other hand, a <strong>discordant</strong> pair is a pair of observations in which a subject is higher on one observation of a pair and lower on the other, e.g. ( S &gt; X and S &lt; Y ) or ( S &lt; X and S &gt; Y ).</p>
<p>We leave readers to investigate two other types of the tau (<span class="math inline">\(\tau\)</span>) coefficients: Tau-a and Tau-b.</p>
<p>To illustrate, suppose we have the following dataset (See Table <a href="statistics.html#tab:spearsman">6.14</a>):</p>

<table>
<caption><span id="tab:spearsman">Table 6.14: </span>Correlation Coefficient Test</caption>
<thead>
<tr class="header">
<th align="left">Category</th>
<th align="left">Sample A</th>
<th align="left">Sample B</th>
<th align="left">Rank A</th>
<th align="left">Rank B</th>
<th align="left">Diff in Rank</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">(R) Red</td>
<td align="left">4</td>
<td align="left">5</td>
<td align="left">2</td>
<td align="left">2</td>
<td align="left">0</td>
</tr>
<tr class="even">
<td align="left">(G) Green</td>
<td align="left">2</td>
<td align="left">3</td>
<td align="left">3</td>
<td align="left">4</td>
<td align="left">-1</td>
</tr>
<tr class="odd">
<td align="left">(B) Blue</td>
<td align="left">1</td>
<td align="left">4</td>
<td align="left">4</td>
<td align="left">3</td>
<td align="left">1</td>
</tr>
<tr class="even">
<td align="left">(O) Orange</td>
<td align="left">7</td>
<td align="left">6</td>
<td align="left">1</td>
<td align="left">1</td>
<td align="left">0</td>
</tr>
</tbody>
</table>

<p>The highest score gets the top rank as one.</p>
<p>We can use a built-in function called <strong>cor(.)</strong> to compute for the <strong>Pearson</strong>, <strong>Spearman</strong>, and <strong>Kendall</strong> correlation coefficients:</p>

<div class="sourceCode" id="cb660"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb660-1" data-line-number="1">RankA =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb660-2" data-line-number="2">RankB =<span class="st"> </span><span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">3</span>, <span class="dv">1</span>)</a>
<a class="sourceLine" id="cb660-3" data-line-number="3">pearson =<span class="st"> </span><span class="kw">cor</span>(RankA, RankB, <span class="dt">method=</span><span class="st">&quot;pearson&quot;</span>, <span class="dt">use=</span><span class="st">&quot;all.obs&quot;</span>)</a>
<a class="sourceLine" id="cb660-4" data-line-number="4">spearman =<span class="st"> </span><span class="kw">cor</span>(RankA, RankB, <span class="dt">method=</span><span class="st">&quot;spearman&quot;</span>, <span class="dt">use=</span><span class="st">&quot;all.obs&quot;</span>)</a>
<a class="sourceLine" id="cb660-5" data-line-number="5">kendall =<span class="st"> </span><span class="kw">cor</span>(RankA, RankB, <span class="dt">method=</span><span class="st">&quot;kendall&quot;</span>, <span class="dt">use=</span><span class="st">&quot;all.obs&quot;</span>)</a>
<a class="sourceLine" id="cb660-6" data-line-number="6"><span class="kw">round</span>( <span class="kw">c</span>(<span class="st">&quot;pearson&quot;</span>=pearson, <span class="st">&quot;spearman&quot;</span>=spearman, <span class="st">&quot;kendall&quot;</span>=kendall), <span class="dv">2</span>)</a></code></pre></div>
<pre><code>##  pearson spearman  kendall 
##     0.80     0.80     0.67</code></pre>

<p>We also can use another built-in function called <strong>cor.test(.)</strong>:</p>

<div class="sourceCode" id="cb662"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb662-1" data-line-number="1"><span class="kw">cor.test</span>(RankA, RankB, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  RankA and RankB
## t = 1.9, df = 2, p-value = 0.1
## alternative hypothesis: true correlation is greater than 0
## 95 percent confidence interval:
##  -0.4977  1.0000
## sample estimates:
## cor 
## 0.8</code></pre>
<div class="sourceCode" id="cb664"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb664-1" data-line-number="1"><span class="kw">cor.test</span>(RankA, RankB, <span class="dt">method =</span> <span class="st">&quot;spearman&quot;</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Spearman&#39;s rank correlation rho
## 
## data:  RankA and RankB
## S = 2, p-value = 0.2
## alternative hypothesis: true rho is greater than 0
## sample estimates:
## rho 
## 0.8</code></pre>
<div class="sourceCode" id="cb666"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb666-1" data-line-number="1"><span class="kw">cor.test</span>(RankA, RankB, <span class="dt">method =</span> <span class="st">&quot;kendall&quot;</span>, <span class="dt">alternative =</span> <span class="st">&quot;greater&quot;</span>)</a></code></pre></div>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  RankA and RankB
## T = 5, p-value = 0.2
## alternative hypothesis: true tau is greater than 0
## sample estimates:
##    tau 
## 0.6667</code></pre>

<p>Note that a coefficient closer to 1 or -1 shows a very strong (positive or negative) correlation. A value of zero means no correlation at all.</p>
<p>Let us perform manual computation:</p>

<div class="sourceCode" id="cb668"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb668-1" data-line-number="1">pairs =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;(R,G)&quot;</span>, <span class="st">&quot;(R,B)&quot;</span>, <span class="st">&quot;(R, O)&quot;</span>, <span class="st">&quot;(G,B)&quot;</span>, <span class="st">&quot;(G,O)&quot;</span>, <span class="st">&quot;(B,O)&quot;</span>)</a>
<a class="sourceLine" id="cb668-2" data-line-number="2">RankA =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;2&lt;3&quot;</span>, <span class="st">&quot;2&lt;4&quot;</span>, <span class="st">&quot;2&gt;1&quot;</span>, <span class="st">&quot;3&lt;4&quot;</span>, <span class="st">&quot;3&gt;1&quot;</span>, <span class="st">&quot;4&gt;1&quot;</span>)</a>
<a class="sourceLine" id="cb668-3" data-line-number="3">RankB =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;2&lt;4&quot;</span>, <span class="st">&quot;2&lt;3&quot;</span>, <span class="st">&quot;2&gt;1&quot;</span>, <span class="st">&quot;4&gt;3&quot;</span>, <span class="st">&quot;4&gt;1&quot;</span>, <span class="st">&quot;3&gt;1&quot;</span>)</a>
<a class="sourceLine" id="cb668-4" data-line-number="4">concord =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;T&quot;</span>, <span class="st">&quot;T&quot;</span>, <span class="st">&quot;T&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;T&quot;</span>, <span class="st">&quot;T&quot;</span>)</a>
<a class="sourceLine" id="cb668-5" data-line-number="5">discord =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;T&quot;</span>, <span class="st">&quot;&quot;</span>, <span class="st">&quot;&quot;</span>)</a>
<a class="sourceLine" id="cb668-6" data-line-number="6">X =<span class="st"> </span><span class="kw">rbind</span>(pairs, RankA)</a>
<a class="sourceLine" id="cb668-7" data-line-number="7">X =<span class="st"> </span><span class="kw">rbind</span>(X,RankB)</a>
<a class="sourceLine" id="cb668-8" data-line-number="8">X =<span class="st"> </span><span class="kw">rbind</span>(X, concord)</a>
<a class="sourceLine" id="cb668-9" data-line-number="9">X =<span class="st"> </span><span class="kw">rbind</span>(X, discord)</a>
<a class="sourceLine" id="cb668-10" data-line-number="10">X</a></code></pre></div>
<pre><code>##         [,1]    [,2]    [,3]     [,4]    [,5]    [,6]   
## pairs   &quot;(R,G)&quot; &quot;(R,B)&quot; &quot;(R, O)&quot; &quot;(G,B)&quot; &quot;(G,O)&quot; &quot;(B,O)&quot;
## RankA   &quot;2&lt;3&quot;   &quot;2&lt;4&quot;   &quot;2&gt;1&quot;    &quot;3&lt;4&quot;   &quot;3&gt;1&quot;   &quot;4&gt;1&quot;  
## RankB   &quot;2&lt;4&quot;   &quot;2&lt;3&quot;   &quot;2&gt;1&quot;    &quot;4&gt;3&quot;   &quot;4&gt;1&quot;   &quot;3&gt;1&quot;  
## concord &quot;T&quot;     &quot;T&quot;     &quot;T&quot;      &quot;&quot;      &quot;T&quot;     &quot;T&quot;    
## discord &quot;&quot;      &quot;&quot;      &quot;&quot;       &quot;T&quot;     &quot;&quot;      &quot;&quot;</code></pre>

<p>We have four observations ( n = 4)</p>
<p>Our <strong>Pearson</strong> rank coefficient is computed like so:</p>
<p><span class="math display">\[\begin{align*}
Sxy {}&amp;= 4(2\times2 + 3\times4 + 4\times3 + 1\times1) - (2+3+4+1)(2+4+3+1) \\
&amp;= 4\times29 - 10\times 10\\
&amp;= 16\\
\\
Sx &amp;= \sqrt{4\times (2^2+3^2+4^2+1^2) - (2+3+4+1)^2} = \sqrt{20}  \\
\\
Sy &amp;= \sqrt{4\times (2^2+4^2+3^2+1^2) - (2+4+3+1)^2} = \sqrt{20} 
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align}
r_{xy} = \frac{S_{xy}}{S_x S_y} = \frac{16}{\sqrt{20} \times \sqrt{20}} = 0.80
\end{align}\]</span></p>
<p>Our <strong>Spearman</strong> rank coefficient is computed like so:</p>
<p><span class="math display">\[\begin{align}
r_s = 1 - \frac{6\left( \sum_{i=1}^n D_{i=1}^2\right)}{n(n^2 - 1)} =
1 - \frac{6 (0^2 + (-1)^2+1^2+0^2)}{4(4^2-1)} = \frac{6 \times 2}{4\times 15} = 1 - 0.20 = 0.80
\end{align}\]</span></p>
<p>Our <strong>Kendall</strong> tau (<span class="math inline">\(\tau\)</span>) coefficient is computed like so:</p>
<p><span class="math display">\[\begin{align}
\tau = \frac{n_c - n_d}{\frac{1}{2}n(n - 1)} = \frac{5-1}{\frac{1}{2} 4(4 - 1)} = \frac{4}{6} = 0.67
\end{align}\]</span></p>
<p>All three correlation coefficients obtained indicate a positive correlation.</p>
</div>
<div id="homoscedasticity-and-heteroscedasticity" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.3</span> Homoscedasticity and Heteroscedasticity  <a href="statistics.html#homoscedasticity-and-heteroscedasticity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Data comes in many shapes and forms. Ideally, the distribution of data follows constant variance and normal distribution. Here, there are two shapes that we need to consider; but it is essential to emphasize that we are analyzing the type of distribution of residuals (also known as errors, noise, or perturbation) in the data instead of the type of distribution of the data itself.</p>
<p>In this section, we start with the concept of <strong>Scedasticity</strong>, which refers to the distribution of residuals. There are two types of residual distributions:</p>
<p><strong>Homoscedasticity</strong> refers to the uneven spread of residuals. It shows a constant distribution of residuals, following a normal distribution.</p>
<p>We create a sample data set with random noise - the residual.</p>

<div class="sourceCode" id="cb670"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb670-1" data-line-number="1">beta0 =<span class="st"> </span><span class="fl">0.5</span></a>
<a class="sourceLine" id="cb670-2" data-line-number="2">beta1 =<span class="st"> </span><span class="fl">1.2</span></a>
<a class="sourceLine" id="cb670-3" data-line-number="3">x =<span class="st"> </span><span class="kw">runif</span>(<span class="dt">n=</span><span class="dv">500</span>, <span class="dt">min=</span><span class="dv">0</span>, <span class="dt">max=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb670-4" data-line-number="4">random_noise =<span class="st"> </span><span class="kw">rnorm</span>( <span class="dv">500</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb670-5" data-line-number="5"><span class="co"># Simulating homoscedastic</span></a>
<a class="sourceLine" id="cb670-6" data-line-number="6">expected_y =<span class="st"> </span>beta0 <span class="op">+</span><span class="st"> </span>beta1 <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>random_noise <span class="op">*</span><span class="st"> </span>x </a></code></pre></div>

<p>Notice that the residual points follow a <strong>uniform-like</strong> or <strong>box-like</strong> shape.</p>
<p>Let us plot a case where the residual follows a <strong>homoscedastic</strong> distribution. See Figure <a href="statistics.html#fig:homoscedastic">6.20</a>.</p>

<div class="sourceCode" id="cb671"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb671-1" data-line-number="1">y.model =<span class="st"> </span><span class="kw">lm</span>(expected_y <span class="op">-</span><span class="st"> </span><span class="kw">I</span>(random_noise <span class="op">*</span><span class="st"> </span>x) <span class="op">+</span><span class="st"> </span>random_noise <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb671-2" data-line-number="2"><span class="kw">plot</span>( <span class="kw">fitted</span>(y.model), <span class="kw">resid</span>(y.model), <span class="dt">lwd=</span><span class="dv">1</span>,   <span class="dt">cex=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb671-3" data-line-number="3">      <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>),  </a>
<a class="sourceLine" id="cb671-4" data-line-number="4">      <span class="dt">main=</span><span class="st">&quot;Homoscedasticity&quot;</span>,  </a>
<a class="sourceLine" id="cb671-5" data-line-number="5">      <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Residual&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Fitted&quot;</span>  )</a>
<a class="sourceLine" id="cb671-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:homoscedastic"></span>
<img src="DS_files/figure-html/homoscedastic-1.png" alt="Constant Variance" width="70%" />
<p class="caption">
Figure 6.20: Constant Variance
</p>
</div>

<p><strong>Heteroscedasticity</strong> refers to the uneven spread of residuals. It does not show a constant distribution of residuals and therefore does not follow a normal distribution.</p>
<p>Let us plot a case where the residual follows a <strong>heteroscedastic</strong> distribution. See Figure <a href="statistics.html#fig:heteroscedastic">6.21</a>.</p>

<div class="sourceCode" id="cb672"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb672-1" data-line-number="1">y.model =<span class="st"> </span><span class="kw">lm</span>(expected_y <span class="op">~</span><span class="st"> </span>x)</a>
<a class="sourceLine" id="cb672-2" data-line-number="2"><span class="kw">plot</span>( <span class="kw">fitted</span>(y.model), <span class="kw">resid</span>(y.model), <span class="dt">lwd=</span><span class="dv">1</span>,   <span class="dt">cex=</span><span class="dv">1</span>,</a>
<a class="sourceLine" id="cb672-3" data-line-number="3">      <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>),  </a>
<a class="sourceLine" id="cb672-4" data-line-number="4">      <span class="dt">main=</span><span class="st">&quot;Heteroscedasticity&quot;</span>,  </a>
<a class="sourceLine" id="cb672-5" data-line-number="5">      <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Residual&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Fitted&quot;</span>  )</a>
<a class="sourceLine" id="cb672-6" data-line-number="6"><span class="kw">abline</span>(<span class="dt">h=</span><span class="dv">0</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:heteroscedastic"></span>
<img src="DS_files/figure-html/heteroscedastic-1.png" alt="Uneven Variance" width="70%" />
<p class="caption">
Figure 6.21: Uneven Variance
</p>
</div>

<p>Notice that the residual points follow a <strong>funnel-like</strong> shape. Other shapes could show as a <strong>polynomial-like</strong> shape. If the shape does not follow a <strong>box-like</strong> shape or the <strong>red line</strong> curves around a non-normal distribution, the residual then follows a <strong>heteroscedastic</strong> distribution. Additionally, if a residual distribution is not centered along the y-axis, it may also be interpreted as being <strong>heteroscedastic</strong>.</p>
</div>
<div id="normality-and-leverage" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.4</span> Normality and Leverage  <a href="statistics.html#normality-and-leverage" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Normality</strong> is a measure of how data follows a normal distribution. We discuss the <strong>Normal Q-Q</strong> plot in a later section under <strong>Diagnostic Plots</strong> to have a visual view of Normality. However, we can also use the <strong>Shapiro-Wilk</strong> test, which we introduced in <strong>Tukeyâs</strong> test or the <strong>Kolmogorov-Smirnov</strong> test to measure Normality.  </p>
<p>Here is a sample of the <strong>Shapiro-Wilk</strong> test.</p>

<div class="sourceCode" id="cb673"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb673-1" data-line-number="1">aov.model =<span class="st"> </span><span class="kw">aov</span>(mpg <span class="op">~</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>wt, <span class="dt">data =</span> mtcars) <span class="co"># One-Way Anova</span></a>
<a class="sourceLine" id="cb673-2" data-line-number="2"><span class="kw">shapiro.test</span>(<span class="kw">residuals</span>(aov.model))</a></code></pre></div>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  residuals(aov.model)
## W = 0.89, p-value = 0.004</code></pre>

<p>A <strong>W</strong> value closer to 1 indicates that our data set adequately follows a normal distribution; otherwise, our data set departs from the distribution. To test how much data departs from the normal distribution, we can also use the <strong>Anderson-Darling</strong> test. </p>

<div class="sourceCode" id="cb675"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb675-1" data-line-number="1"><span class="kw">library</span>(nortest)</a>
<a class="sourceLine" id="cb675-2" data-line-number="2">x =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span><span class="dv">20</span>, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb675-3" data-line-number="3"><span class="kw">ad.test</span>(x)</a></code></pre></div>
<pre><code>## 
##  Anderson-Darling normality test
## 
## data:  x
## A = 0.35, p-value = 0.4</code></pre>

<p>An <strong>A</strong> value is the test statistic and measures how much our data departs from a normal distribution.</p>
<p><strong>Leverage</strong> measures the distance between each observation to the mean of all observations. This measure of the distance of each observation is also known as the <strong>hat-value</strong> of the observation and can be used to identify outliers. We discuss outliers further in terms of <strong>Cookâs distance</strong>. </p>
<p><strong>Leverage</strong> is computed using the following equation:</p>
<p><span class="math display">\[\begin{align}
h_i = \frac{1}{n} + \frac{(x_i - \bar{x})^2}{SS_E}\ \ \ \ \
where\ \ \ \ SS_E = \sum_{j=1}^n(x_j - \bar{x})^2
\end{align}\]</span></p>
<p>To illustrate, let us prepare our simple dataset and generate a simple linear regression model like so:</p>

<div class="sourceCode" id="cb677"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb677-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb677-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb677-3" data-line-number="3">range1 =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">200</span>)</a>
<a class="sourceLine" id="cb677-4" data-line-number="4">x =<span class="st"> </span><span class="kw">sample</span>(range1, <span class="dt">size=</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb677-5" data-line-number="5">e =<span class="st"> </span><span class="kw">rnorm</span>(sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="co"># residual</span></a>
<a class="sourceLine" id="cb677-6" data-line-number="6">y =<span class="st"> </span><span class="kw">sort</span>( <span class="kw">rnorm</span>(sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">+</span><span class="st"> </span>e )</a>
<a class="sourceLine" id="cb677-7" data-line-number="7">model =<span class="st"> </span><span class="kw">lm</span>( y <span class="op">~</span><span class="st"> </span>x)</a></code></pre></div>

<p>Now, let us compute for the <strong>leverage</strong> (or hatvalue). The following R implementation illustrates a calculation for <strong>simple linear regression</strong> only.</p>

<div class="sourceCode" id="cb678"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb678-1" data-line-number="1">my.hatvalues &lt;-<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb678-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">length</span>(x)</a>
<a class="sourceLine" id="cb678-3" data-line-number="3">  x.mean =<span class="st"> </span><span class="kw">mean</span>(x)</a>
<a class="sourceLine" id="cb678-4" data-line-number="4">  hat.values =<span class="st"> </span><span class="kw">c</span>()</a>
<a class="sourceLine" id="cb678-5" data-line-number="5">  sse =<span class="st"> </span><span class="kw">sum</span>( (x <span class="op">-</span><span class="st"> </span>x.mean)<span class="op">^</span><span class="dv">2</span> )</a>
<a class="sourceLine" id="cb678-6" data-line-number="6">  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="op">:</span>n) {</a>
<a class="sourceLine" id="cb678-7" data-line-number="7">    hat.values[i] =<span class="st"> </span><span class="dv">1</span> <span class="op">/</span><span class="st"> </span>n <span class="op">+</span><span class="st"> </span>(x[i] <span class="op">-</span><span class="st"> </span>x.mean)<span class="op">^</span><span class="dv">2</span> <span class="op">/</span><span class="st"> </span>sse</a>
<a class="sourceLine" id="cb678-8" data-line-number="8">  }</a>
<a class="sourceLine" id="cb678-9" data-line-number="9">  <span class="kw">names</span>(hat.values) =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, n)</a>
<a class="sourceLine" id="cb678-10" data-line-number="10">  hat.values</a>
<a class="sourceLine" id="cb678-11" data-line-number="11">}</a>
<a class="sourceLine" id="cb678-12" data-line-number="12"><span class="co"># let us display only the 1st five hatvalues</span></a>
<a class="sourceLine" id="cb678-13" data-line-number="13"><span class="kw">head</span>(<span class="kw">my.hatvalues</span>(x), <span class="dt">n=</span><span class="dv">5</span>)</a></code></pre></div>
<pre><code>##       1       2       3       4       5 
## 0.07113 0.05592 0.06421 0.05001 0.13954</code></pre>

<p>Let us validate by using a built-in R function called <strong>hatvalues()</strong> and <strong>influence()</strong>:</p>

<div class="sourceCode" id="cb680"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb680-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">hatvalues</span>(model), <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##       1       2       3       4       5 
## 0.07113 0.05592 0.06421 0.05001 0.13954</code></pre>
<div class="sourceCode" id="cb682"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb682-1" data-line-number="1"><span class="kw">head</span>(<span class="kw">influence</span>(model)<span class="op">$</span>hat, <span class="dv">5</span>)</a></code></pre></div>
<pre><code>##       1       2       3       4       5 
## 0.07113 0.05592 0.06421 0.05001 0.13954</code></pre>

<p>For <strong>Multi-linear Regression</strong>, we use the following <strong>Matrix equation</strong> from Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>):</p>

<p><span class="math display">\[\begin{align}
Recall: \hat{\beta} = (A^T \cdot A)^{-1} \cdot A^T \cdot y\ \ \ \ \leftarrow\ \ \ \ \ \ y = A\hat{\beta} \label{eqn:eqnnumber3a}
\end{align}\]</span>
</p>
<p>Therefore, we got:</p>

<p><span class="math display">\[\begin{align}
H = A \cdot (A^T \cdot A)^{-1} \cdot A^T = A \beta y^{-1}
\end{align}\]</span>
</p>
<p>We leave readers to investigate using <strong>Leverage</strong> in a <strong>multi-linear regression</strong>.</p>
</div>
<div id="collinearity" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.5</span> Collinearity <a href="statistics.html#collinearity" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Collinearity</strong> is a measure of how much independent variables are correlated. We discuss this topic in more detail in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).</p>
</div>
<div id="dispersion" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.6</span> Dispersion <a href="statistics.html#dispersion" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Dispersion</strong> describes the variation of data distribution. A <strong>Normal distribution</strong> tends to have an observed constant <strong>variance</strong> (<span class="math inline">\(\sigma^2\)</span>) all through the <strong>mean</strong> (<span class="math inline">\(\mu\)</span>) of linear regression. A <strong>Poisson distribution</strong> tends to have an observed <strong>variance</strong> equal to the <strong>mean</strong>. As the <strong>mean</strong> increases or decreases, so does the <strong>variance</strong>. On the other hand, a distribution with an observed <strong>variance</strong> that is not constant but is higher than the expected <strong>variance</strong> indicates signs of <strong>Overdispersion</strong>; otherwise, it shows signs of <strong>Underdispersion</strong>.</p>
<p><strong>Dispersion</strong> is measured as a ratio of <strong>deviance</strong> to the <strong>degrees of freedom (df)</strong>, so that if the ratio is higher than 1 (e.g., ratio&gt;1:1), then it indicates some overdispersion.</p>
<p>One common way to handle Overdispersion is by using <strong>Quasi-Distribution</strong> or <strong>Quasi-Likelihood</strong>. An example is using <strong>Quasi-Binomial</strong> distribution and <strong>Quasi-Poisson</strong> distribution in <strong>Generalized Linear Modeling (GLM)</strong>. The intuition behind <strong>GLM</strong> is discussed in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Logistic Regression</strong> Subsection under <strong>Regression</strong> Section.</p>
<p>A <strong>Quasi-Distribution</strong> or a <strong>Quasi-Likelihood</strong> has a revised variant of its <strong>PDF</strong> equation. For example, the <strong>PDF</strong> of a <strong>Binomial distribution</strong> has the following equation:</p>
<p><span class="math display">\[\begin{align}
P(X = x| n, \rho) = \binom{n}{x} \rho^x(1 - \rho)^{n - x}
\end{align}\]</span></p>
<p>A <strong>Quasi-Binomial</strong> has the following <strong>PDF</strong> equation:</p>
<p><span class="math display">\[\begin{align}
P(X = x| n, \rho) = \binom{n}{x} \rho(\rho + k\psi)^{x-1}(1 - \rho - x\psi)^{n -x}
\end{align}\]</span></p>
<p>The added symbol <strong>psi</strong> (<span class="math inline">\(\psi\)</span>) represents a parameter used to describe and explain the additional variance observed in the data distribution.</p>
<p><strong>Dispersion</strong> is further discussed in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Poisson Regression</strong> Subsection under <strong>Regression</strong> Section.</p>
</div>
<div id="diagnostic-plots" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.7.7</span> Diagnostic Plots<a href="statistics.html#diagnostic-plots" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are six diagnostic plots presented here to help us discuss linear regression analysis:</p>
<ul>
<li><strong>Residuals vs.Â Fitted plot</strong> - Measure non-linearity and exposes outliers.</li>
<li><strong>Normal Q-Q plot</strong> - Measure data distribution against normal distribution.</li>
<li><strong>Scale-Location plot</strong> - Measure variance (homoscedasticity vs.Â heteroscedasticity).</li>
<li><strong>Cookâs Distance plot</strong> - Measure of influence based on Cookâs distance formula.</li>
<li><strong>Residual vs.Â Leverage plot</strong> - Measure influence based on Residual variance.</li>
<li><strong>Cookâs Distance vs.Â Leverage plot</strong> - Standardized Residuals vs.Â Leverage.</li>
</ul>
<p>Let us generate a regression model for diagnostics. Here, we induce two outliers to explain leverage and Cookâs distance. See Figure <a href="statistics.html#fig:outliers">6.22</a>.</p>

<div class="sourceCode" id="cb684"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb684-1" data-line-number="1">beta<span class="fl">.0</span> =<span class="st"> </span><span class="fl">1.0</span></a>
<a class="sourceLine" id="cb684-2" data-line-number="2">beta<span class="fl">.1</span> =<span class="st"> </span><span class="fl">1.3</span></a>
<a class="sourceLine" id="cb684-3" data-line-number="3">sample_size =<span class="st"> </span>n =<span class="st"> </span><span class="dv">50</span></a>
<a class="sourceLine" id="cb684-4" data-line-number="4">e =<span class="st"> </span><span class="kw">rnorm</span>(n, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd =</span> <span class="dv">1</span>) <span class="op">*</span><span class="st"> </span><span class="dv">3</span> <span class="co"># residual</span></a>
<a class="sourceLine" id="cb684-5" data-line-number="5">x.true.fit =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, n)   <span class="co"># x baseline</span></a>
<a class="sourceLine" id="cb684-6" data-line-number="6">y.true.fit =<span class="st"> </span>beta<span class="fl">.0</span> <span class="op">+</span><span class="st"> </span>beta<span class="fl">.1</span> <span class="op">*</span><span class="st"> </span>x.true.fit <span class="co"># y baseline</span></a>
<a class="sourceLine" id="cb684-7" data-line-number="7">x.observed =<span class="st"> </span>x.true.fit <span class="op">+</span><span class="st"> </span>e <span class="co"># add noise to gen. predictor</span></a>
<a class="sourceLine" id="cb684-8" data-line-number="8">y.observed =<span class="st"> </span>y.true.fit <span class="op">+</span><span class="st"> </span>e <span class="co"># add noise to gen. response </span></a>
<a class="sourceLine" id="cb684-9" data-line-number="9">reg.model =<span class="st"> </span><span class="kw">lm</span>(y.observed <span class="op">~</span><span class="st"> </span>x.observed)</a>
<a class="sourceLine" id="cb684-10" data-line-number="10">y.expected =<span class="st"> </span><span class="kw">fitted</span>(reg.model) <span class="co"># fitted expectation</span></a>
<a class="sourceLine" id="cb684-11" data-line-number="11"><span class="co"># create outliers</span></a>
<a class="sourceLine" id="cb684-12" data-line-number="12">outlier.idx =<span class="st"> </span><span class="kw">c</span>(<span class="dv">49</span>,<span class="dv">50</span>)</a>
<a class="sourceLine" id="cb684-13" data-line-number="13">outlier.val =<span class="st"> </span><span class="kw">c</span>(<span class="dv">130</span>, <span class="dv">150</span>)</a>
<a class="sourceLine" id="cb684-14" data-line-number="14">y.with.outliers =<span class="st"> </span>y.observed</a>
<a class="sourceLine" id="cb684-15" data-line-number="15">y.with.outliers[outlier.idx] =<span class="st"> </span>outlier.val</a>
<a class="sourceLine" id="cb684-16" data-line-number="16">outlier.model =<span class="st"> </span><span class="kw">lm</span>(y.with.outliers <span class="op">~</span><span class="st"> </span>x.observed)</a>
<a class="sourceLine" id="cb684-17" data-line-number="17">y.outlier.expected =<span class="st"> </span><span class="kw">fitted</span>(outlier.model) <span class="co"># fitted expectation</span></a>
<a class="sourceLine" id="cb684-18" data-line-number="18">ymax =<span class="st"> </span><span class="kw">max</span>(y.true.fit) <span class="op">+</span><span class="st"> </span><span class="dv">10</span></a>
<a class="sourceLine" id="cb684-19" data-line-number="19">xmax =<span class="st"> </span><span class="kw">max</span>(x.observed)</a>
<a class="sourceLine" id="cb684-20" data-line-number="20"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>, <span class="dv">50</span>), <span class="dt">ylim =</span> <span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">160</span>), </a>
<a class="sourceLine" id="cb684-21" data-line-number="21">     <span class="dt">xlab =</span> <span class="st">&quot;indepdendent&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;dependent&quot;</span>,</a>
<a class="sourceLine" id="cb684-22" data-line-number="22">     <span class="dt">main=</span><span class="st">&quot;Linear Regression&quot;</span>)</a>
<a class="sourceLine" id="cb684-23" data-line-number="23"><span class="kw">grid</span>()</a>
<a class="sourceLine" id="cb684-24" data-line-number="24">x.i =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, n)</a>
<a class="sourceLine" id="cb684-25" data-line-number="25"><span class="kw">points</span>(x.i, y.observed, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</a>
<a class="sourceLine" id="cb684-26" data-line-number="26"><span class="kw">lines</span>(x.i, y.true.fit, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</a>
<a class="sourceLine" id="cb684-27" data-line-number="27"><span class="kw">abline</span>(reg.model, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb684-28" data-line-number="28"><span class="kw">abline</span>(outlier.model, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb684-29" data-line-number="29"><span class="kw">points</span>(outlier.idx, outlier.val, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb684-30" data-line-number="30"><span class="kw">legend</span>(<span class="dv">1</span>, <span class="dv">150</span>, </a>
<a class="sourceLine" id="cb684-31" data-line-number="31">   <span class="kw">c</span>( <span class="st">&quot;true fit&quot;</span>,  <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;fit with outliers&quot;</span> ),</a>
<a class="sourceLine" id="cb684-32" data-line-number="32">     <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>), </a>
<a class="sourceLine" id="cb684-33" data-line-number="33">     <span class="dt">horiz=</span><span class="ot">FALSE</span>, <span class="dt">cex=</span><span class="fl">0.8</span>,   <span class="dt">lty=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>,<span class="dv">2</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:outliers"></span>
<img src="DS_files/figure-html/outliers-1.png" alt="Linear Regression (Outlier)" width="70%" />
<p class="caption">
Figure 6.22: Linear Regression (Outlier)
</p>
</div>

<p>Notice that with the existence of two outliers, the regression line is pulled slightly towards the outliers, indicating influence.</p>
<p><strong>Residuals vs Fitted plot</strong></p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis1">6.23</a>.</p>

<div class="sourceCode" id="cb685"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb685-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb685-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb685-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">1</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis1"></span>
<img src="DS_files/figure-html/lranalysis1-1.png" alt="Linear Regression Analysis" width="70%" />
<p class="caption">
Figure 6.23: Linear Regression Analysis
</p>
</div>
<div class="sourceCode" id="cb686"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb686-1" data-line-number="1"><span class="co"># we also can use the plot and abline to produce the same plots</span></a>
<a class="sourceLine" id="cb686-2" data-line-number="2"><span class="co"># plot( fitted(reg.model2), resid(reg.model2), ...)</span></a>
<a class="sourceLine" id="cb686-3" data-line-number="3"><span class="co"># abline(h = 0, lwd=2, col=&quot;red&quot;)</span></a></code></pre></div>

<p>There are two plots in the figure: one without outliers and one with outliers. As can be seen, the plot can give us the location of outliers. Because we only deal with a small number of residuals, linearity is not entirely visible. Linearity is more visible, however, for residuals with extreme outliers, as shown in the second plot.</p>
<p><strong>Normal Q-Q plot</strong> </p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis2">6.24</a>.</p>

<div class="sourceCode" id="cb687"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb687-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb687-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb687-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">2</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis2"></span>
<img src="DS_files/figure-html/lranalysis2-1.png" alt="Linear Regression Analysis" width="80%" />
<p class="caption">
Figure 6.24: Linear Regression Analysis
</p>
</div>
<div class="sourceCode" id="cb688"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb688-1" data-line-number="1"><span class="co"># we also can use qqnorm and qqline to produce the same plots</span></a>
<a class="sourceLine" id="cb688-2" data-line-number="2"><span class="co"># qqnorm( resid(reg.model), main = &quot;Normal Q-Q&quot;)  # data points</span></a>
<a class="sourceLine" id="cb688-3" data-line-number="3"><span class="co"># qqline( resid(outlier.model), lwd = 2, col = &quot;red&quot;)</span></a></code></pre></div>

<p>A Normality test is when residuals all fall along a straight line, representing Normality. A typical good fit is when the plot shows a normal distribution of residuals forming along a straight line in analyzing our linear regression. Residuals tend to crowd at the center and spread out. Extreme residuals dissipate as they spread out, but they still fall closer along the straight line while being extreme. Residuals that are outliers, however, tend to fall off the straight line, as seen in the second plot.</p>
<p><strong>Scale-Location plot</strong> </p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis3">6.25</a>.</p>

<div class="sourceCode" id="cb689"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb689-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb689-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb689-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">3</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis3"></span>
<img src="DS_files/figure-html/lranalysis3-1.png" alt="Linear Regression Analysis" width="70%" />
<p class="caption">
Figure 6.25: Linear Regression Analysis
</p>
</div>

<p>In the previous section, we discuss <strong>homoscedasticity</strong> - a residual that follows a constant normal distribution. While we have a smaller number of residuals, <strong>homoscedasticity</strong> may not be as pronounced as when we deal with a larger number of residuals; though, still, the first plot shows the red line passing through an even constant distribution of the residuals. However, the second plot is bent - but only because of the influence of the outliers. Still, the residuals (except for the outliers) are evenly dispersed across the red bent line.</p>
<p><strong>Cookâs Distance plot</strong> </p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis4">6.26</a>.</p>

<div class="sourceCode" id="cb690"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb690-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb690-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">4</span>)</a>
<a class="sourceLine" id="cb690-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">4</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis4"></span>
<img src="DS_files/figure-html/lranalysis4-1.png" alt="Linear Regression Analysis" width="70%" />
<p class="caption">
Figure 6.26: Linear Regression Analysis
</p>
</div>

<p>In the second plot, we can see that the outliers are more visible than the other residuals. On the other hand, the other residuals become more refined because of the outliers. If we remove the outliers, we see a more coarse plot similar to the first plot. Let us discuss <strong>Cookâs distance</strong> to show how we evaluate and remove the outliers.</p>
<p>The formula for <strong>Cookâs distance</strong> is as follows:</p>
<p><span class="math display">\[\begin{align}
D_i^{(cook&#39;s\ distance)} = H_i^{(leverage)} \times R_i^{2(studentized\ residual)} \times \frac{1}{k}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>k</strong> is the number of coefficients (including the interface)</li>
</ul>
<p><strong>H</strong> is the leverage:</p>
<p><span class="math display">\[\begin{align}
H_i^{(leverage)} = \left(\frac{h_i}{ 1 - h_i}\right)
\end{align}\]</span></p>
<p><strong>R</strong> is the <strong>Studentized Residual</strong> expressed as: </p>
<p><span class="math display">\[\begin{align}
R_i = \frac{ e_i }{ SE_i^{(standard\ error)}}
\end{align}\]</span></p>
<p><strong>SE</strong> is the estimated <strong>Standard deviation</strong> or <strong>Standard Error</strong> expressed as:</p>
<p><span class="math display">\[\begin{align}
SE_i^{(standard\ error)} = \sqrt{MSE \times ( 1 - h_i)} \approx \sqrt{\sigma^2 \times ( 1 - h_i)}
\end{align}\]</span></p>
<p>and <strong>MSE</strong> is the <strong>mean squared error</strong> which we use in place of <span class="math inline">\(\sigma^2\)</span> which is unknown:</p>
<p><span class="math display">\[\begin{align}
MSE = \frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{(n - k)}
\end{align}\]</span></p>
<p>We can use a built-in function called <strong>cooks.distance()</strong> to compute for the <strong>Cookâs distance</strong>. We then apply a <strong>threshold</strong> for eliminating outliers. Here, we use the following cut-off:</p>
<p><span class="math display">\[\begin{align}
cutoff = \frac{4}{n}
\end{align}\]</span></p>

<div class="sourceCode" id="cb691"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb691-1" data-line-number="1">cutoff =<span class="st"> </span><span class="dv">4</span> <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(y.with.outliers)</a>
<a class="sourceLine" id="cb691-2" data-line-number="2">cooks.dist =<span class="st"> </span><span class="kw">cooks.distance</span>(reg.model)</a>
<a class="sourceLine" id="cb691-3" data-line-number="3">outlier.idx =<span class="st"> </span><span class="kw">which</span>(<span class="kw">ifelse</span>( cooks.dist <span class="op">&lt;</span><span class="st"> </span>cutoff, <span class="ot">FALSE</span>, <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb691-4" data-line-number="4">cooks =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Cooks.Dist&quot;</span> =<span class="st"> </span>cooks.dist[outlier.idx],</a>
<a class="sourceLine" id="cb691-5" data-line-number="5">  <span class="st">&quot;Cut-Off&quot;</span> =<span class="st"> </span>cutoff,  <span class="st">&quot;Outlier Index&quot;</span> =<span class="st"> </span>outlier.idx,</a>
<a class="sourceLine" id="cb691-6" data-line-number="6">  <span class="st">&quot;Outlier&quot;</span>=<span class="st"> </span>y.with.outliers[outlier.idx] )</a>
<a class="sourceLine" id="cb691-7" data-line-number="7"><span class="kw">round</span>(cooks,<span class="dv">2</span>)</a></code></pre></div>
<pre><code>##    Cooks.Dist.14          Cut-Off Outlier Index.14          Outlier 
##             0.08             0.08            14.00            28.80</code></pre>

<p>Here is a naive implementation of <strong>Cookâs Distance</strong> in R code:</p>

<div class="sourceCode" id="cb693"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb693-1" data-line-number="1">cooks &lt;-<span class="st"> </span><span class="cf">function</span>(model ) {</a>
<a class="sourceLine" id="cb693-2" data-line-number="2">  hi =<span class="st"> </span><span class="kw">hatvalues</span>(model)</a>
<a class="sourceLine" id="cb693-3" data-line-number="3">  e.resid =<span class="st">  </span><span class="kw">resid</span>(model)</a>
<a class="sourceLine" id="cb693-4" data-line-number="4">  n =<span class="st"> </span><span class="kw">length</span>(<span class="kw">length</span>(e.resid)) <span class="co">#</span></a>
<a class="sourceLine" id="cb693-5" data-line-number="5">  k =<span class="st"> </span><span class="kw">length</span>( <span class="kw">coef</span>(model)) <span class="co"># no. coeffs including intercept</span></a>
<a class="sourceLine" id="cb693-6" data-line-number="6">  e.resid.df =<span class="st"> </span><span class="kw">df.residual</span>(model) <span class="co"># (n - k)</span></a>
<a class="sourceLine" id="cb693-7" data-line-number="7">  leverage =<span class="st"> </span>hi <span class="op">/</span><span class="st"> </span>( <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>hi)</a>
<a class="sourceLine" id="cb693-8" data-line-number="8">  e2 =<span class="st"> </span>(e.resid)<span class="op">^</span><span class="dv">2</span></a>
<a class="sourceLine" id="cb693-9" data-line-number="9">  mse =<span class="st"> </span><span class="kw">sum</span>((e.resid)<span class="op">^</span><span class="dv">2</span>) <span class="op">/</span><span class="st"> </span>e.resid.df <span class="co"># estimate standard error</span></a>
<a class="sourceLine" id="cb693-10" data-line-number="10">  s.e =<span class="st"> </span><span class="kw">sqrt</span>( mse <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>hi))</a>
<a class="sourceLine" id="cb693-11" data-line-number="11">  <span class="co"># studentized residual (standard residual)</span></a>
<a class="sourceLine" id="cb693-12" data-line-number="12">  r.s =<span class="st"> </span>e.resid <span class="op">/</span><span class="st"> </span>s.e  <span class="co"># also can use rstandard(model)</span></a>
<a class="sourceLine" id="cb693-13" data-line-number="13">  cooks.dist =<span class="st"> </span>leverage <span class="op">*</span><span class="st"> </span>( r.s<span class="op">^</span><span class="dv">2</span>) <span class="op">*</span><span class="st"> </span>(<span class="dv">1</span><span class="op">/</span>k)</a>
<a class="sourceLine" id="cb693-14" data-line-number="14">  cooks.dist</a>
<a class="sourceLine" id="cb693-15" data-line-number="15">}</a>
<a class="sourceLine" id="cb693-16" data-line-number="16">cooks.dist =<span class="st"> </span><span class="kw">cooks</span>(reg.model )</a>
<a class="sourceLine" id="cb693-17" data-line-number="17">outlier.idx =<span class="st"> </span><span class="kw">which</span>(<span class="kw">ifelse</span>( cooks.dist <span class="op">&lt;</span><span class="st"> </span>cutoff, <span class="ot">FALSE</span>, <span class="ot">TRUE</span>))</a>
<a class="sourceLine" id="cb693-18" data-line-number="18">cooks =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;Cooks.Dist&quot;</span> =<span class="st"> </span>cooks.dist[outlier.idx],</a>
<a class="sourceLine" id="cb693-19" data-line-number="19">  <span class="st">&quot;Cut-Off&quot;</span> =<span class="st"> </span>cutoff,  <span class="st">&quot;Outlier Index&quot;</span> =<span class="st"> </span>outlier.idx,</a>
<a class="sourceLine" id="cb693-20" data-line-number="20">  <span class="st">&quot;Outlier&quot;</span>=<span class="st"> </span>y.with.outliers[outlier.idx] )</a>
<a class="sourceLine" id="cb693-21" data-line-number="21"><span class="kw">round</span>(cooks,<span class="dv">2</span>)</a></code></pre></div>
<pre><code>##    Cooks.Dist.14          Cut-Off Outlier Index.14          Outlier 
##             0.08             0.08            14.00            28.80</code></pre>

<p><strong>Residual vs Leverage plot</strong></p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis5">6.27</a>.</p>

<div class="sourceCode" id="cb695"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb695-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb695-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb695-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">5</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis5"></span>
<img src="DS_files/figure-html/lranalysis5-1.png" alt="Linear Regression Analysis" width="80%" />
<p class="caption">
Figure 6.27: Linear Regression Analysis
</p>
</div>

<p>We measure <strong>leverage</strong> in terms of how far each data is from the mean. We know that the mean is where most typical data crowds, and therefore, any data farther away from the mean are candidate <strong>leverage points</strong> that may have influence. Most - if not all - outliers can influence our model and thus are candidates to be removed. However, there may also be data that are not outliers yet has some influence on the model. The second plot shows a few outliers.</p>
<p>We evaluate the lower and upper right corners outside the dotted lines for this plot. For example, there is one outlier outside the upper right boundary with a threshold of 1. In addition, we see another outlier outside the 0.5 boundary. They indicate high leverage with the potential to influence.</p>
<p><strong>Cookâs Distance vs Leverage plot</strong></p>
<p>For this diagnostic plot, see Figure <a href="statistics.html#fig:lranalysis6">6.28</a>.</p>

<div class="sourceCode" id="cb696"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb696-1" data-line-number="1"><span class="kw">par</span>(<span class="dt">mfrow =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>))</a>
<a class="sourceLine" id="cb696-2" data-line-number="2"><span class="kw">plot</span>(reg.model, <span class="dt">which=</span><span class="dv">6</span>)</a>
<a class="sourceLine" id="cb696-3" data-line-number="3"><span class="kw">plot</span>(outlier.model, <span class="dt">which=</span><span class="dv">6</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lranalysis6"></span>
<img src="DS_files/figure-html/lranalysis6-1.png" alt="Linear Regression Analysis" width="80%" />
<p class="caption">
Figure 6.28: Linear Regression Analysis
</p>
</div>

<p>The dotted lines are numbered from zero to a number (right to left) that could reach as high as 6 in the case of the second plot. These dotted lines are boundaries - the higher a data point reaches a boundary, the more highly influential it is. Any residual that gets promoted to the status of being a <strong>leverage point</strong> has the potential to influence. For example, the outlier at index 50 has a <strong>Cookâs distance</strong> value between 5 and 6. Ideally, even at 1, it should already be considered highly influential.</p>
<p>As the last topic in this section, one useful function is <strong>influence.measures()</strong>. We leave readers to experiment with the use of the function. Running the function tends to output a long list of results for a large number of observations.</p>

<div class="sourceCode" id="cb697"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb697-1" data-line-number="1"><span class="kw">influence.measures</span>(outlier.model, <span class="dt">infl =</span> <span class="kw">influence</span>(outlier.model))</a></code></pre></div>

</div>
</div>
<div id="the-significance-of-regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.8</span> The Significance of Regression <a href="statistics.html#the-significance-of-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Recall in previous sections that we deal with <strong>significance of difference</strong> of groups. In our examples, our <strong>null hypothesis</strong> claims that there is no difference between groups, while our <strong>alternative hypothesis</strong> supports the claim that there is a difference between groups:</p>
<p><span class="math display">\[\begin{align}
H_0 : \mu = \mu_0\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
H_1 : \mu \ne \mu_0
\end{align}\]</span></p>
<p>In this section, we focus on <strong>significance of regression</strong> in which we want to study the relationship, association, or effect of <strong>explanatory variables</strong> on <strong>response variables</strong>. We do this by <strong>examining the coefficients</strong> that are associated with the explanatory variables and then determining the significance based on a test of effect. Here, we are also required to formulate our own <strong>null hypothesis</strong> and <strong>alternative hypothesis</strong> using a <strong>linear equation</strong> starting with the following <strong>simple linear equation</strong>:</p>
<p><span class="math display">\[\begin{align}
y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the response (dependent) variable</li>
<li><span class="math inline">\(B_0\)</span> and <span class="math inline">\(B_1\)</span> are coefficients</li>
<li><span class="math inline">\(x_i\)</span> is the explanatory (predictor or independent) variable</li>
<li><span class="math inline">\(e_i\)</span> is the unexplained residual</li>
</ul>
<p>Our <strong>null hypothesis</strong> claims that our response variable does not depend on the predictor variable.</p>
<p><span class="math display">\[\begin{align}
H_0 {}&amp;: \beta_1 = 0,\ \ \ \  \ y_i = \beta_0 + \epsilon_i\\
\nonumber \\
H_1 &amp;: \beta_1 \ne 0,\ \ \ \  \ y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{align}\]</span></p>
<p>While we can use <strong>ANOVA</strong> and <strong>Sum of Squares</strong> to measure <strong>Variance</strong>, in <strong>Significance of Regression</strong>, it also helps consider the effect of predictor variables on response variables. What we want to do is to settle over the least amount of error. Therefore, we measure error based on the distance between two data points deviating from one another. In the following sections, we introduce a few tests of <strong>null hypothesis</strong> for <strong>linear regression</strong>, starting with <strong>simple linear regression</strong>.</p>
<div id="simple-linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.1</span> Simple Linear Regression<a href="statistics.html#simple-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Simple Linear Regression</strong>, we deal with one continuous independent variable denoted as <span class="math inline">\(\mathbf{x_i}\)</span> and one continuous dependent variable denoted as <span class="math inline">\(\hat{y}\)</span>. Being linear, we try to find the best fit of a line into the data and determine if we can even fit a line - is there a relationship between the dependent and independent variables?</p>
<p>We need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any random chance (from an independent random variable).</p>
<p>To illustrate, let us continue to use the <strong>mtcars</strong> dataset. However, this time, we intend to know if there is an effect of the displacement, <strong>disp</strong>, to fuel consumption, <strong>mpg</strong>. We use a built-in R function called <strong>lm(.)</strong> to generate a linear model;</p>

<div class="sourceCode" id="cb698"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb698-1" data-line-number="1">(<span class="dt">simple.model =</span> <span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp  
##     29.5999      -0.0412</code></pre>

<p>Notice in the model that we have two coefficients: <span class="math inline">\(\beta_0\)</span> = 29.6 and <span class="math inline">\(\beta_1\)</span> = -0.041.</p>
<p>Let us run a summary against the linear model using <strong>summary(.)</strong>:</p>

<div class="sourceCode" id="cb700"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb700-1" data-line-number="1"><span class="kw">summary</span>(simple.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -4.892 -2.202 -0.963  1.627  7.231 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 29.59985    1.22972   24.07  &lt; 2e-16 ***
## disp        -0.04122    0.00471   -8.75  9.4e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.25 on 30 degrees of freedom
## Multiple R-squared:  0.718,  Adjusted R-squared:  0.709 
## F-statistic: 76.5 on 1 and 30 DF,  p-value: 9.38e-10</code></pre>

<p>Based on the summary, we can say that the displacement variable significantly affects fuel consumption. That is asserted because of the triple asterisk (***) for the coefficient of the variable <strong>disp</strong>. The code indicates a significant effect at <span class="math inline">\(\alpha=0.001\)</span>.</p>
<p>To determine if such a predictor variable <strong>regresses to</strong> a response variable, we may plot our dataset and determine if we can model the regression based on measuring the <strong>closeness of distance</strong> of each observed data point denoted as <span class="math inline">\(y_{i}\)</span> to a fitted line denoted as <span class="math inline">\(f(x_{i})\)</span>. In other words, we should be able to see the linear relationship pattern between <strong>disp</strong> and <strong>mpg</strong>:</p>

<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb702-1" data-line-number="1"><span class="kw">plot</span>(mpg <span class="op">~</span><span class="st"> </span>disp, <span class="dt">data =</span> mtcars, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb702-2" data-line-number="2">     <span class="dt">main=</span><span class="st">&quot;Effect of Displacement to Fuel Consumption&quot;</span>)</a>
<a class="sourceLine" id="cb702-3" data-line-number="3"><span class="kw">abline</span>(simple.model, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>) <span class="co"># fit a line, f(xi)</span></a>
<a class="sourceLine" id="cb702-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb702-5" data-line-number="5"><span class="kw">arrows</span>(<span class="dv">200</span>,<span class="dv">23</span>, <span class="dv">250</span>,<span class="dv">28</span>, <span class="dt">code=</span><span class="dv">1</span>, <span class="dt">length=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb702-6" data-line-number="6"><span class="kw">text</span>(<span class="dv">350</span>, <span class="dv">28</span>, <span class="dt">label=</span><span class="st">&quot;fitted line, f(x), the linear model&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-304"></span>
<img src="DS_files/figure-html/unnamed-chunk-304-1.png" alt="Smoothing" width="70%" />
<p class="caption">
Figure 6.29: Smoothing
</p>
</div>

<p>In the plot, we fit a line to show the linear regression. The <span class="math inline">\(B_1\)</span> is negative which indicates a decreasing effect to the fuel consumption; meaning, for every unit increase of <span class="math inline">\(x\)</span> (disp), there is a 0.041 unit decrease of <span class="math inline">\(y\)</span> (mpg).</p>
We will see how to apply the formulas when we get into <strong>multilinear regression</strong>. Meanwhile, Figure <a href="statistics.html#fig:statistics">6.30</a> shows the relationships of the formulas:

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:statistics"></span>
<img src="DS_files/figure-html/statistics-1.png" alt="Statistics" width="0%" />
<p class="caption">
Figure 6.30: Statistics
</p>
</div>

<p>Note, based on Figure <a href="statistics.html#fig:statistics">6.30</a>, that <strong>RSS</strong> compares the regression line against the horizontal line - the null hypothesis (<strong>H0</strong>). Similarly, <strong>R squared</strong> is compared the same.</p>
<p><strong>RSS (Residual Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(y\)</span>. At times, in other literature, it may read as <strong>SSR</strong> (Sum of Squares Residual) or <strong>SSE</strong> (Sum of Squares Error).</p>
<p><span class="math display">\[\begin{align}
RSS = \sum_{i=1}^{n}(y_{i} - \hat{y}_i)^2
\end{align}\]</span></p>
<p><strong>ESS (Explained Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(\hat{y}\)</span> and <span class="math inline">\(\bar{y}\)</span>. We may read this as <strong>RSS</strong> (Regression Sum of Squares) in other literature. Alternatively, it may read as <strong>SSR</strong> (Sum of Squares Regression) or <strong>SSE</strong> (Sum of Squares Explained).</p>
<p><span class="math display">\[\begin{align}
ESS = \sum_{i=1}^{n}(\hat{y}_i - \bar{y})^2
\end{align}\]</span></p>
<p><strong>TSS (Total Sum of Squares)</strong>  </p>
<p>That is the sum of squares of the difference between <span class="math inline">\(y\)</span> and <span class="math inline">\(\bar{y}\)</span>. We may sometimes read this as <strong>SST</strong> (Sum of Squares Total).</p>
<p><span class="math display">\[\begin{align}
TSS = \sum_{i=1}^{n}(y_{i} - \bar{y})^2
\end{align}\]</span></p>
<p>The total sum of squares combines RSS (residual sum of squares) and ESS (explained sum of squares).</p>
<p><span class="math display">\[\begin{align}
TSS = RSS + ESS
\end{align}\]</span></p>
<p>We have the following implementation of the measures for the regression, for which we can demonstrate the usage in the next section:</p>

<div class="sourceCode" id="cb703"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb703-1" data-line-number="1">reg.summary &lt;-<span class="st"> </span><span class="cf">function</span>(A, coefficients, y) {</a>
<a class="sourceLine" id="cb703-2" data-line-number="2">  n =<span class="st"> </span><span class="kw">nrow</span>(A)</a>
<a class="sourceLine" id="cb703-3" data-line-number="3">  m =<span class="st"> </span><span class="kw">length</span>(coefficients)</a>
<a class="sourceLine" id="cb703-4" data-line-number="4">  y_hat =<span class="st"> </span>A <span class="op">%*%</span><span class="st"> </span>coefficients</a>
<a class="sourceLine" id="cb703-5" data-line-number="5">  df =<span class="st"> </span>n <span class="op">-</span><span class="st"> </span>m  <span class="co"># degrees of freedom</span></a>
<a class="sourceLine" id="cb703-6" data-line-number="6">  rss =<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span>y_hat)<span class="op">^</span><span class="dv">2</span>) <span class="co"># residual sum squared</span></a>
<a class="sourceLine" id="cb703-7" data-line-number="7">  ess =<span class="st"> </span><span class="kw">sum</span>((y_hat <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span>) <span class="co"># explained sum squared</span></a>
<a class="sourceLine" id="cb703-8" data-line-number="8">  tss =<span class="st"> </span><span class="kw">sum</span>((y <span class="op">-</span><span class="st"> </span><span class="kw">mean</span>(y))<span class="op">^</span><span class="dv">2</span>) <span class="co"># total sum squared</span></a>
<a class="sourceLine" id="cb703-9" data-line-number="9">  se =<span class="st"> </span><span class="kw">sqrt</span>( rss <span class="op">/</span><span class="st"> </span>df ) <span class="co"># standard error</span></a>
<a class="sourceLine" id="cb703-10" data-line-number="10">  r2 =<span class="st"> </span>ess<span class="op">/</span><span class="st"> </span>tss <span class="co"># multiple r-squared</span></a>
<a class="sourceLine" id="cb703-11" data-line-number="11">  adj.r2 =<span class="st"> </span><span class="dv">1</span> <span class="op">-</span><span class="st"> </span>((<span class="dv">1</span><span class="op">-</span>r2) <span class="op">*</span><span class="st"> </span>(n<span class="dv">-1</span>) <span class="op">/</span><span class="st"> </span>(n <span class="op">-</span><span class="st"> </span>m))</a>
<a class="sourceLine" id="cb703-12" data-line-number="12">  out =<span class="st"> </span><span class="kw">c</span>(n, m, df, rss, ess, tss, se, r2, adj.r2)</a>
<a class="sourceLine" id="cb703-13" data-line-number="13">  <span class="kw">names</span>(out) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;m&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;RSS&quot;</span>, <span class="st">&quot;ESS&quot;</span>, <span class="st">&quot;TSS&quot;</span>, <span class="st">&quot;SE&quot;</span>, </a>
<a class="sourceLine" id="cb703-14" data-line-number="14">                 <span class="st">&quot;R-squared&quot;</span>, <span class="st">&quot;Adj.R2&quot;</span>)</a>
<a class="sourceLine" id="cb703-15" data-line-number="15">  out</a>
<a class="sourceLine" id="cb703-16" data-line-number="16">}</a></code></pre></div>

</div>
<div id="multilinear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.2</span> Multilinear Regression <a href="statistics.html#multilinear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In <strong>Multilinear Regression</strong>, we deal with more than one continuous independent variable, denoted as <strong>X</strong>, and one continuous dependent variable, denoted as <span class="math inline">\(\hat{y}\)</span>, using the following <strong>multilinear equation</strong>:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = \beta^T X
\end{align}\]</span></p>
<p>which can be expanded like so:</p>
<p><span class="math display">\[\begin{align}
y_i = \beta_0 + \beta_1 x_{1,i} +  \beta_2 x_{2,i}\ + ... +\ \beta_n x_{n,i} +  \epsilon_i
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> is the response (dependent) variable</li>
<li><span class="math inline">\(B_0\)</span> â¦ <span class="math inline">\(B_n\)</span> are coefficients</li>
<li><span class="math inline">\(x_1\)</span> â¦ <span class="math inline">\(X_n\)</span> are the explanatory (predictor or independent) variables</li>
<li><span class="math inline">\(e_i\)</span> is the unexplained residual</li>
</ul>
<p>Our <strong>null hypothesis</strong> claims that our response variable does not depend on any predictor variable. Otherwise, our <strong>alternative hypothesis</strong> claims that our response variable depends on at least one predictor variable.</p>
<p><span class="math display">\[\begin{align}
H_0 {}&amp;: \beta_1 =  \beta_2 =  \beta_3 =\ ...\ =\ \beta_n  = 0,\ \ \ \  \ y_i = \beta_0 + \epsilon_i\\
\nonumber \\
H_1 &amp;: at\ least\ one\ \ \beta_j \ne 0,\ \ \ \  \ y_i = \beta_0 + \beta_1 x_{1,i} +  \beta_2 x_{2,i}\ + ... +\ \beta_n x_{n,i} +  \epsilon_i
\end{align}\]</span></p>
<p>We need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from multiple independent random variables).</p>
<p>To illustrate, we now use more than one predictor variable from the <strong>mtcars</strong> dataset and model a multilinear regression. We choose four continuous predictor variables: displacement (disp), horsepower (hp), weight (wt), and 1/4 mile time (qsec).</p>

<div class="sourceCode" id="cb704"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb704-1" data-line-number="1"><span class="kw">str</span>(mtcars)</a></code></pre></div>
<pre><code>## &#39;data.frame&#39;:    32 obs. of  11 variables:
##  $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...
##  $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...
##  $ disp: num  160 160 108 258 360 ...
##  $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...
##  $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...
##  $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...
##  $ qsec: num  16.5 17 18.6 19.4 17 ...
##  $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...
##  $ am  : num  1 1 1 0 0 0 0 0 0 0 ...
##  $ gear: num  4 4 4 3 3 3 3 4 4 4 ...
##  $ carb: num  4 4 1 1 2 1 4 2 2 4 ...</code></pre>

<p>We generate our multilinear model:</p>

<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb706-1" data-line-number="1">(<span class="dt">multi.model =</span>  <span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp <span class="op">+</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>wt <span class="op">+</span><span class="st"> </span>qsec, <span class="dt">data =</span> mtcars))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp           hp           wt         qsec  
##    27.32964      0.00267     -0.01867     -4.60912      0.54416</code></pre>

<p>and derive the following coefficients:</p>
<p><span class="math inline">\(\beta_0\)</span> = 27.3296,
<span class="math inline">\(\beta_1\)</span> = 0.0027,
<span class="math inline">\(\beta_2\)</span> = -0.0187,
<span class="math inline">\(\beta_3\)</span> = -4.6091,
<span class="math inline">\(\beta_4\)</span> = 0.5442</p>
<p>We can also use a built-in function called <strong>coef()</strong>:</p>

<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb708-1" data-line-number="1"><span class="kw">coef</span>(multi.model)</a></code></pre></div>
<pre><code>## (Intercept)        disp          hp          wt        qsec 
##   27.329638    0.002666   -0.018666   -4.609123    0.544160</code></pre>

<p>Recall <strong>Polynomial Regression</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). We use the same matrix manipulation to get the list of coefficients, where A is the matrix of equations (See also Equation <span class="math inline">\(\ref{eqn:eqnnumber3a}\)</span>).</p>
<p><span class="math display">\[\begin{align*}
\hat{\beta} \approx (A^T \cdot A)^{-1} \cdot A^T \cdot y 
\end{align*}\]</span></p>
<p>For example:</p>

<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb710-1" data-line-number="1">A =<span class="st"> </span><span class="kw">with</span>(mtcars, <span class="kw">cbind</span>(<span class="dv">1</span>, disp, hp, wt, qsec))</a>
<a class="sourceLine" id="cb710-2" data-line-number="2">y =<span class="st"> </span>mtcars<span class="op">$</span>mpg</a>
<a class="sourceLine" id="cb710-3" data-line-number="3">beta.hat =<span class="st"> </span><span class="kw">solve</span>(<span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>A) <span class="op">%*%</span><span class="st"> </span><span class="kw">t</span>(A) <span class="op">%*%</span><span class="st"> </span>y</a>
<a class="sourceLine" id="cb710-4" data-line-number="4"><span class="kw">colnames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;coefficients&quot;</span>)</a>
<a class="sourceLine" id="cb710-5" data-line-number="5"><span class="kw">rownames</span>(beta.hat) =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;intercept&quot;</span>, <span class="st">&quot;disp&quot;</span>, <span class="st">&quot;hp&quot;</span>, <span class="st">&quot;wt&quot;</span>, <span class="st">&quot;qsec&quot;</span>)</a>
<a class="sourceLine" id="cb710-6" data-line-number="6"><span class="kw">t</span>(beta.hat)</a></code></pre></div>
<pre><code>##              intercept     disp       hp     wt   qsec
## coefficients     27.33 0.002666 -0.01867 -4.609 0.5442</code></pre>

<p>Note that there are other improvements to matrix manipulation we can numerically use to compute the coefficients. Please refer to Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>).</p>
<p>To get the approximate response outcome, <span class="math inline">\(\hat{y}\)</span>, we use the following equation:</p>
<p><span class="math display">\[\begin{align}
\hat{y} = A\times\hat{\beta} \\
\nonumber \\
y_hat = A %*% beta.hat
\end{align}\]</span></p>
<p>And to get the <strong>Standard Residual Error</strong> and <strong>R-squared</strong>, we use the following equations:</p>
<p><span class="math display">\[\begin{align}
SE = \frac{RSS}{df}\ \ \ \ \ \ \leftarrow\ \ \ df = n - m,
\ \ \ \ \ \ \ \ \ \ \ \ \ \ R^2 = \frac{ESS}{TSS}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>RSS</strong> is the residual sum of squares</li>
<li><strong>n</strong> is size of the sample</li>
<li><strong>m</strong> is the number of coefficients (including intercept)</li>
<li><strong>df</strong> is the degrees of freedom</li>
</ul>

<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb712-1" data-line-number="1">reg.out =<span class="st"> </span><span class="kw">round</span>( <span class="kw">reg.summary</span>(A, beta.hat, y), <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb712-2" data-line-number="2">reg.out[<span class="kw">c</span>(<span class="st">&quot;n&quot;</span>, <span class="st">&quot;m&quot;</span>, <span class="st">&quot;df&quot;</span>, <span class="st">&quot;RSS&quot;</span>, <span class="st">&quot;ESS&quot;</span>, <span class="st">&quot;TSS&quot;</span>, <span class="st">&quot;SE&quot;</span>)]</a></code></pre></div>
<pre><code>##        n        m       df      RSS      ESS      TSS       SE 
##   32.000    5.000   27.000  185.635  940.412 1126.047    2.622</code></pre>
<div class="sourceCode" id="cb714"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb714-1" data-line-number="1">reg.out[<span class="kw">c</span>(<span class="st">&quot;R-squared&quot;</span>, <span class="st">&quot;Adj.R2&quot;</span>)]</a></code></pre></div>
<pre><code>## R-squared    Adj.R2 
##    0.8351    0.8107</code></pre>

<p>Here is the summary:</p>

<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb716-1" data-line-number="1">(<span class="dt">summary.out =</span> <span class="kw">summary</span>(multi.model))</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = mpg ~ disp + hp + wt + qsec, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.866 -1.582 -0.379  1.171  5.647 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 27.32964    8.63903    3.16   0.0038 **
## disp         0.00267    0.01074    0.25   0.8058   
## hp          -0.01867    0.01561   -1.20   0.2423   
## wt          -4.60912    1.26585   -3.64   0.0011 **
## qsec         0.54416    0.46649    1.17   0.2536   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.62 on 27 degrees of freedom
## Multiple R-squared:  0.835,  Adjusted R-squared:  0.811 
## F-statistic: 34.2 on 4 and 27 DF,  p-value: 3.31e-10</code></pre>

<p>As shown, the predictor variable <strong>wt</strong> has a significant effect to <strong>mpg</strong> at <span class="math inline">\(\alpha=0.01\)</span>.</p>
<p>Also, notice that our <strong>R-squared</strong> is at <span class="math inline">\(\sigma\)</span> = 0.8351. Any value closer to one indicates a better fit. However, while we consider this coefficient of determinant to measure the goodness of fit, it becomes less reliable as the number of predictors increases. </p>
<p>Let us calculate for the adjusted <strong>R-squared</strong> using the equation below (with the intent to have an adjusted <span class="math inline">\(R^2\)</span> event at a high number of predictors - although, here, we are not going to perform that): </p>
<p><span class="math display">\[\begin{align}
adj.R^2 = 1 - \frac{(1-R^2)(n-1)}{n-m }
\end{align}\]</span></p>

<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb718-1" data-line-number="1">r2 =<span class="st"> </span><span class="kw">as.numeric</span>( reg.out[<span class="kw">c</span>(<span class="st">&quot;R-squared&quot;</span>)] )</a>
<a class="sourceLine" id="cb718-2" data-line-number="2">n  =<span class="st"> </span><span class="kw">as.numeric</span>(reg.out[<span class="kw">c</span>(<span class="st">&quot;n&quot;</span>)])</a>
<a class="sourceLine" id="cb718-3" data-line-number="3">m  =<span class="st"> </span><span class="kw">as.numeric</span>(reg.out[<span class="kw">c</span>(<span class="st">&quot;m&quot;</span>)])</a>
<a class="sourceLine" id="cb718-4" data-line-number="4">( <span class="dt">adj.r2 =</span> <span class="dv">1</span> <span class="op">-</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>r2)<span class="op">*</span>(n <span class="op">-</span><span class="st"> </span><span class="dv">1</span>)<span class="op">/</span>(n <span class="op">-</span><span class="st"> </span>m) )</a></code></pre></div>
<pre><code>## [1] 0.8107</code></pre>

</div>
<div id="logistic-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.3</span> Logistic Regression <a href="statistics.html#logistic-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We review <strong>Logistic Regression</strong> in the context of <strong>Generalized Linear Model (GLM)</strong>, emphasizing the use of a <strong>link function</strong>. Note that <strong>GLM</strong> and <strong>link function</strong> are covered in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>).  </p>
<p>In <strong>Logistic Regression</strong>, we deal with independent variables denoted by <span class="math inline">\(X\)</span> and a <strong>logit-transformed</strong> dependent variable that follows a binomial probability denoted by <span class="math inline">\(\hat{p}\)</span>. We express the distribution like so:</p>
<p><span class="math display">\[y \sim Bern(\hat{p})\]</span></p>
<p>The intuition behind <strong>Logistic Regression</strong> starts with two functions:</p>
<p><strong>Link Function (Logit Function)</strong>: </p>
<p>Note that a <strong>Logit Function</strong> is the inverse of a <strong>Sigmoid Function</strong> (also known as <strong>Logistic Function</strong>).</p>
<p><span class="math display">\[\begin{align}
\text{logit}(\hat{p}) = \log_e\left(\frac{\hat{p}}{1 - \hat{p}}\right) = z
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where z in }[-\infty,\infty]
\end{align}\]</span></p>
<p><strong>Inverse Link Function (Sigmoid Function):</strong></p>
<p><span class="math display">\[\begin{align}
\text{logit}^{-1}(z) = \hat{p} 
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \text{where }\hat{p}\text{ in }[0,1]
\end{align}\]</span></p>
<p>and where: <span class="math inline">\(z = \beta^TX\ \ \ \  \leftarrow\ \ \ \ \beta_0 + \sum_{i=1}^n\beta_i x_i\)</span>.</p>
<p>The <strong>Inverse Link Function</strong>, also called <strong>Inverse Logit Function</strong>, can be expanded and be shown to be inversely related to <strong>Logit function</strong> like so:</p>
<p><span class="math display">\[\begin{align}
\hat{p} = \frac{\text{exp}(z)}{1 + \text{exp}(z)} = \frac{1}{1 + \text{exp}(-z)} 
= \frac{1}{1 + \text{exp}(-\text{logit}(z))} 
\end{align}\]</span></p>
<p>where: <span class="math inline">\(\hat{p}\)</span> is interpreted as the <strong>probability</strong> of observing a successful event.</p>
<p><span class="math display">\[\begin{align}
\underbrace{\hat{p} = P(y = 1| x)}_{\text{successful event}}\ \ \ \ \ \ \ \ \ 
\underbrace{\hat{q} = 1 - \hat{p} =P(y = 0| x) }_{\text{failed event}}
\end{align}\]</span></p>
<p>The <strong>Logit Function</strong>, on the other hand, can be interpreted as the logarithm of the odds ratio of observing a successful event over observing a failed event. This function allows us to model regression linearly.</p>
<p><span class="math display">\[\begin{align}
\text{logit}(\hat{p}) = \log_e\left(\frac{\hat{p}}{1 - \hat{p}}\right) 
= \log_e\left(\frac{\hat{p}}{\hat{q}}\right)  = \beta^TX
\end{align}\]</span></p>
<p>Both functions can be implemented such that we have <strong>inverse.logit(.)</strong> and <strong>logit(.)</strong>.</p>

<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb720-1" data-line-number="1">ln &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="kw">log</span>(x, <span class="kw">exp</span>(<span class="dv">1</span>))}  <span class="co"># exp(1) = 2.718282</span></a>
<a class="sourceLine" id="cb720-2" data-line-number="2">inverse.logit &lt;-<span class="st"> </span><span class="cf">function</span>(z) {  <span class="dv">1</span> <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span><span class="kw">exp</span>(<span class="op">-</span>z)) }</a>
<a class="sourceLine" id="cb720-3" data-line-number="3">logit &lt;-<span class="st"> </span><span class="cf">function</span>(p) { <span class="kw">ln</span>(p <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p))  }</a></code></pre></div>

<p>Here, the base of our log is e = 2.7183.</p>
<p>Alternatively, R comes with two functions, namely <strong>plogis(.)</strong> and <strong>qlogis(.)</strong> respectively, e.g.:</p>

<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb721-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;inverse.logit&quot;</span> =<span class="st"> </span><span class="kw">inverse.logit</span>(<span class="dv">8</span>), <span class="st">&quot;plogis&quot;</span> =<span class="st"> </span><span class="kw">plogis</span>(<span class="dv">8</span>))</a></code></pre></div>
<pre><code>## inverse.logit        plogis 
##        0.9997        0.9997</code></pre>
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb723-1" data-line-number="1"><span class="kw">c</span>(<span class="st">&quot;logit&quot;</span> =<span class="st"> </span><span class="kw">logit</span>(<span class="fl">0.90</span>), <span class="st">&quot;qlogis&quot;</span> =<span class="st"> </span><span class="kw">qlogis</span>(<span class="fl">0.90</span>))</a></code></pre></div>
<pre><code>##  logit qlogis 
##  2.197  2.197</code></pre>

<p>To visualize the logistic distribution using the plogis function, we show Figure <a href="statistics.html#fig:plogis">6.31</a>.</p>

<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb725-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb725-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;Z (Log-Odds)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;plogis(z)&quot;</span>,</a>
<a class="sourceLine" id="cb725-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Inverse Logit (Log-Odds as Input)&quot;</span>)</a>
<a class="sourceLine" id="cb725-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb725-5" data-line-number="5"><span class="co"># Using plogis outcome</span></a>
<a class="sourceLine" id="cb725-6" data-line-number="6">z =<span class="st"> </span>x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>, <span class="dv">5</span>, <span class="dt">length.out=</span><span class="dv">50</span>) <span class="co"># z is log-odds.</span></a>
<a class="sourceLine" id="cb725-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">plogis</span>(x), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:plogis"></span>
<img src="DS_files/figure-html/plogis-1.png" alt="Logistic Regression (CDF)" width="70%" />
<p class="caption">
Figure 6.31: Logistic Regression (CDF)
</p>
</div>

<p>To visualize the logistic distribution using qlogis function, we show Figure <a href="statistics.html#fig:qlogis">6.32</a>. The range is [<span class="math inline">\(-\infty, \infty\)</span>].</p>

<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb726-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">0</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>), </a>
<a class="sourceLine" id="cb726-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;P (Probability)&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;qlogis(p)&quot;</span>, </a>
<a class="sourceLine" id="cb726-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Logit (Probabilities as Input)&quot;</span>)</a>
<a class="sourceLine" id="cb726-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb726-5" data-line-number="5"><span class="co"># Using qlogis outcome</span></a>
<a class="sourceLine" id="cb726-6" data-line-number="6">p =<span class="st"> </span>x =<span class="st">  </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>) <span class="co"># p is sequence of probabilities</span></a>
<a class="sourceLine" id="cb726-7" data-line-number="7"><span class="kw">curve</span>(<span class="kw">qlogis</span>(x), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">add=</span><span class="ot">TRUE</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:qlogis"></span>
<img src="DS_files/figure-html/qlogis-1.png" alt="Logit - Quantile" width="70%" />
<p class="caption">
Figure 6.32: Logit - Quantile
</p>
</div>

<p>To complement our understanding of <strong>Logistic Regression</strong>, let us review the concept of <strong>Proportionality</strong> and the <strong>Odds</strong> of observing a successful event. For example, suppose we flip a coin 200 times, then calculate the proportion of our experiment landing on heads:</p>

<div class="sourceCode" id="cb727"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb727-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb727-2" data-line-number="2">range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb727-3" data-line-number="3">overall.outcome =<span class="st"> </span><span class="dv">200</span></a>
<a class="sourceLine" id="cb727-4" data-line-number="4">tosses =<span class="st"> </span><span class="kw">sample</span>(range, <span class="dt">size=</span>overall.outcome, <span class="dt">replace=</span><span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb727-5" data-line-number="5">heads =<span class="st"> </span><span class="kw">length</span>( <span class="kw">which</span>( tosses <span class="op">==</span><span class="st"> </span><span class="dv">1</span>) ) <span class="co"># no of times coin landed on head</span></a>
<a class="sourceLine" id="cb727-6" data-line-number="6">tails =<span class="st"> </span><span class="kw">length</span>( <span class="kw">which</span>( tosses <span class="op">==</span><span class="st"> </span><span class="dv">0</span>) ) <span class="co"># no of times coin landed on tail</span></a>
<a class="sourceLine" id="cb727-7" data-line-number="7"><span class="kw">c</span>(<span class="st">&quot;head&quot;</span> =<span class="st"> </span>heads <span class="op">/</span><span class="st"> </span>overall.outcome, <span class="st">&quot;tail&quot;</span> =<span class="st"> </span>tails <span class="op">/</span><span class="st"> </span>overall.outcome)</a></code></pre></div>
<pre><code>## head tail 
## 0.53 0.47</code></pre>

<p>Flipping a coin 200 times shows that the <strong>proportion</strong> of landing on heads overall outcome is around 0.53 and that the proportion of landing on tails overall outcome is around 0.47. So the total proportionally should sum to 1.</p>
<p>The <strong>odds</strong> of heads over tails is around 1.1277, and the <strong>odds</strong> of tails over heads is around 0.8868.</p>

<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb729-1" data-line-number="1">p =<span class="st"> </span>heads <span class="op">/</span><span class="st"> </span>overall.outcome</a>
<a class="sourceLine" id="cb729-2" data-line-number="2"><span class="kw">c</span>(<span class="st">&quot;odds of heads&quot;</span> =<span class="st"> </span>p <span class="op">/</span><span class="st"> </span>(<span class="dv">1</span> <span class="op">-</span><span class="st"> </span>p),   <span class="st">&quot;odds of tails&quot;</span> =<span class="st"> </span>(<span class="dv">1</span><span class="op">-</span>p) <span class="op">/</span><span class="st"> </span>p)</a></code></pre></div>
<pre><code>## odds of heads odds of tails 
##        1.1277        0.8868</code></pre>

<p>The result can be equivalently achieved using a very simple <strong>odds ratio</strong> formula:</p>

<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb731-1" data-line-number="1"><span class="kw">c</span>( <span class="st">&quot;odds of heads&quot;</span> =<span class="st"> </span>heads<span class="op">/</span>tails,  <span class="st">&quot;odds of tails&quot;</span> =<span class="st"> </span>tails <span class="op">/</span><span class="st"> </span>heads)</a></code></pre></div>
<pre><code>## odds of heads odds of tails 
##        1.1277        0.8868</code></pre>

<p>To be able to transform the result of our <strong>odds</strong> to a symmetrical range, e.g. [<span class="math inline">\(-\infty,\infty\)</span>], we use <strong>log-odds</strong>.</p>

<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb733-1" data-line-number="1"><span class="kw">c</span>( <span class="st">&quot;log-odds of heads&quot;</span> =<span class="st"> </span><span class="kw">ln</span>(heads<span class="op">/</span>tails),  </a>
<a class="sourceLine" id="cb733-2" data-line-number="2">   <span class="st">&quot;odds of tails&quot;</span>     =<span class="st"> </span><span class="kw">ln</span>(tails <span class="op">/</span><span class="st"> </span>heads))</a></code></pre></div>
<pre><code>## log-odds of heads     odds of tails 
##            0.1201           -0.1201</code></pre>

<p>Now, in terms of <strong>Logistic Regression</strong>, we need to determine if the <span class="math inline">\(H_0\)</span> supports the claim that the predicted outcome does not regress - is not close - to the actual value by any number of random chances (from independent random variables).</p>
<p>To illustrate, we choose a simple continuous independent variable, namely fuel consumption (<strong>mpg</strong>), from the <strong>mtcars</strong> dataset and model a logistic regression. Our dependent variable is <strong>dichotomous</strong> (binary), namely V/Straight Engine (vs).</p>
<p>In determining whether an independent variable <strong>regresses</strong> to a dependent variable, we use the <strong>logit link function</strong> to obtain the estimated <strong>logit</strong> values using <strong>glm(.)</strong>, generating our logistic model in the process. Note that we discuss <strong>Generalized Linear Models (GLM)</strong> in detail in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Regression</strong> Section.</p>

<div class="sourceCode" id="cb735"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb735-1" data-line-number="1"><span class="co"># binomial family with logit link</span></a>
<a class="sourceLine" id="cb735-2" data-line-number="2">logit.model =<span class="st"> </span><span class="kw">glm</span>(vs <span class="op">~</span><span class="st"> </span>mpg, <span class="dt">data=</span>mtcars, <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span>logit)) </a>
<a class="sourceLine" id="cb735-3" data-line-number="3">logit.hat =<span class="st"> </span>logit.model<span class="op">$</span>fitted.values</a></code></pre></div>

<p>The model produces fitted values called <strong>logit values</strong> for the log odds. In other words, the values are the logarithm of the odds ratio of <strong>Straight Engine</strong> to <strong>V Engine</strong> for the <strong>vs</strong> dependent variable. See Figure <a href="statistics.html#fig:logit">6.33</a>.</p>

<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb736-1" data-line-number="1">sort.mpg =<span class="st"> </span><span class="kw">sort</span>(mtcars<span class="op">$</span>mpg, <span class="dt">index.return =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb736-2" data-line-number="2">x =<span class="st"> </span>sort.mpg<span class="op">$</span>x</a>
<a class="sourceLine" id="cb736-3" data-line-number="3">y =<span class="st"> </span>logit.hat[sort.mpg<span class="op">$</span>ix]</a>
<a class="sourceLine" id="cb736-4" data-line-number="4"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb736-5" data-line-number="5">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb736-6" data-line-number="6">     <span class="dt">ylab =</span> <span class="st">&quot;logit(p)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Logit&quot;</span>)</a>
<a class="sourceLine" id="cb736-7" data-line-number="7"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb736-8" data-line-number="8"><span class="kw">points</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>)</a>
<a class="sourceLine" id="cb736-9" data-line-number="9"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb736-10" data-line-number="10"><span class="kw">points</span>(mtcars<span class="op">$</span>mpg, mtcars<span class="op">$</span>vs, <span class="dt">col=</span></a>
<a class="sourceLine" id="cb736-11" data-line-number="11">         <span class="kw">ifelse</span>(mtcars<span class="op">$</span>vs <span class="op">==</span><span class="st"> </span><span class="dv">1</span>, <span class="st">&quot;darkgreen&quot;</span>, <span class="st">&quot;brown&quot;</span>), <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb736-12" data-line-number="12"><span class="kw">arrows</span>(<span class="fl">24.5</span>, <span class="fl">0.81</span>, <span class="fl">27.5</span>,<span class="fl">0.6</span>, <span class="dt">code=</span><span class="dv">1</span>, <span class="dt">length=</span><span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb736-13" data-line-number="13"><span class="kw">text</span>(<span class="dv">30</span>,  <span class="fl">0.55</span>, <span class="dt">label=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;logit(p)=&quot;</span>,beta<span class="op">^</span>T, <span class="st">&quot;X&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:logit"></span>
<img src="DS_files/figure-html/logit-1.png" alt="Logit (Estimated Values)" width="80%" />
<p class="caption">
Figure 6.33: Logit (Estimated Values)
</p>
</div>

<p>Note that the logit model in the figure seems to show a <strong>sigmoid</strong> curve. However, that is not the case. The model produces an outcome that follows a linear logit. That becomes more apparent if we feed the model with missing values and predict the outcome. See Figure <a href="statistics.html#fig:linearlogistic">6.34</a>.</p>

<div class="sourceCode" id="cb737"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb737-1" data-line-number="1">x =<span class="st"> </span>mpg.predictor =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">40</span>, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb737-2" data-line-number="2">predicted.logit =<span class="st">  </span><span class="kw">predict.glm</span>(logit.model, </a>
<a class="sourceLine" id="cb737-3" data-line-number="3">                               <span class="kw">data.frame</span>(<span class="dt">mpg =</span> mpg.predictor))</a></code></pre></div>
<div class="sourceCode" id="cb738"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb738-1" data-line-number="1">y =<span class="st"> </span>predicted.logit  </a>
<a class="sourceLine" id="cb738-2" data-line-number="2"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb738-3" data-line-number="3">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb738-4" data-line-number="4">     <span class="dt">ylab =</span> <span class="st">&quot;logit(p)&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Logit (Linear)&quot;</span>)</a>
<a class="sourceLine" id="cb738-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb738-6" data-line-number="6"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb738-7" data-line-number="7"><span class="kw">text</span>(<span class="dv">20</span>,  <span class="dv">3</span>, <span class="dt">label=</span><span class="kw">expression</span>(<span class="kw">paste</span>(<span class="st">&quot;logit(p)=&quot;</span>, beta<span class="op">^</span>T, <span class="st">&quot;X&quot;</span>)))</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:linearlogistic"></span>
<img src="DS_files/figure-html/linearlogistic-1.png" alt="Logit (Linear)" width="80%" />
<p class="caption">
Figure 6.34: Logit (Linear)
</p>
</div>

<p>Next, we convert the log odds into the inverse form (which becomes the <strong>P-hat</strong> <span class="math inline">\(\mathbf{\hat{p}}\)</span>). See Figure <a href="statistics.html#fig:inverseform">6.35</a>. The figure does show the expected <strong>sigmoid</strong> curve.</p>

<div class="sourceCode" id="cb739"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb739-1" data-line-number="1">p.hat =<span class="st"> </span>y =<span class="st"> </span><span class="kw">inverse.logit</span>(predicted.logit)</a></code></pre></div>
<div class="sourceCode" id="cb740"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb740-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">col=</span><span class="st">&quot;deepskyblue&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>,</a>
<a class="sourceLine" id="cb740-2" data-line-number="2">     <span class="dt">xlim =</span> <span class="kw">range</span>(x), <span class="dt">ylim =</span> <span class="kw">range</span>(y), <span class="dt">xlab =</span> <span class="st">&quot;mpg (Predictor)&quot;</span>, </a>
<a class="sourceLine" id="cb740-3" data-line-number="3">     <span class="dt">ylab =</span> <span class="st">&quot;inverse.logit(z) = p.hat&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Inverse Logit (Sigmoid)&quot;</span>)</a>
<a class="sourceLine" id="cb740-4" data-line-number="4"><span class="kw">abline</span>(simple.model, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>) <span class="co">#  </span></a>
<a class="sourceLine" id="cb740-5" data-line-number="5"><span class="kw">grid</span>(<span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb740-6" data-line-number="6"><span class="kw">lines</span>(x, y, <span class="dt">pch=</span><span class="dv">20</span>, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb740-7" data-line-number="7"><span class="kw">text</span>(<span class="dv">25</span>,  <span class="fl">0.6</span>, <span class="dt">label=</span><span class="st">&quot;Sigmoid&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:inverseform"></span>
<img src="DS_files/figure-html/inverseform-1.png" alt="Inverse Logit (Sigmoid)" width="70%" />
<p class="caption">
Figure 6.35: Inverse Logit (Sigmoid)
</p>
</div>

<p>The coefficients of the fitted values show as follows:</p>

<div class="sourceCode" id="cb741"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb741-1" data-line-number="1">beta_<span class="dv">0</span> =<span class="st"> </span><span class="kw">as.numeric</span>(logit.model<span class="op">$</span>coefficients[<span class="dv">1</span>])  </a>
<a class="sourceLine" id="cb741-2" data-line-number="2">beta_<span class="dv">1</span> =<span class="st"> </span><span class="kw">as.numeric</span>(logit.model<span class="op">$</span>coefficients[<span class="dv">2</span>])  </a></code></pre></div>

<p><span class="math inline">\(\beta_0\)</span> = -8.8331 and <span class="math inline">\(\beta_1\)</span> = 0.4304.</p>
<p>If we review Figure <a href="statistics.html#fig:linearlogistic">6.34</a>, <span class="math inline">\(\beta_0\)</span> is the intercept that touches the y-axis.</p>
<p>We can re-formulate our log-odds model, recalculating <span class="math inline">\(\mathbf{z_i}\)</span>.</p>
<p><span class="math inline">\(z_i\)</span> = <span class="math inline">\(\beta_0\)</span> + <span class="math inline">\(\beta_1\)</span> <span class="math inline">\(x_i\)</span> = -8.8331 + 0.4304 <span class="math inline">\(x_i\)</span></p>

<div class="sourceCode" id="cb742"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb742-1" data-line-number="1">x.observed =<span class="st"> </span>new.mpg =<span class="st"> </span><span class="dv">30</span></a>
<a class="sourceLine" id="cb742-2" data-line-number="2">(<span class="dt">z =</span> beta_<span class="dv">0</span> <span class="op">+</span><span class="st"> </span>beta_<span class="dv">1</span> <span class="op">*</span><span class="st"> </span>x.observed)</a></code></pre></div>
<pre><code>## [1] 4.079</code></pre>
<div class="sourceCode" id="cb744"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb744-1" data-line-number="1">(<span class="dt">z =</span> <span class="kw">as.vector</span>(<span class="kw">predict.glm</span>(logit.model, </a>
<a class="sourceLine" id="cb744-2" data-line-number="2">                           <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">mpg =</span> x.observed))))</a></code></pre></div>
<pre><code>## [1] 4.079</code></pre>

<p>In terms of <strong>statistical significance</strong>, let us summarize the fitted model using <strong>glm(.)</strong>.</p>

<div class="sourceCode" id="cb746"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb746-1" data-line-number="1"><span class="kw">summary</span>(logit.model)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = vs ~ mpg, family = binomial(link = logit), data = mtcars)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.213  -0.512  -0.228   0.640   1.698  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   -8.833      3.162   -2.79   0.0052 **
## mpg            0.430      0.158    2.72   0.0066 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 43.860  on 31  degrees of freedom
## Residual deviance: 25.533  on 30  degrees of freedom
## AIC: 29.53
## 
## Number of Fisher Scoring iterations: 6</code></pre>

<p>In the summary of the model, we see that the observed data (<strong>mpg</strong>) is also <strong>statistically significant</strong> at alpha <span class="math inline">\(\alpha = 0.01\)</span> (e.g., less than one in a hundred chance of being wrong). In other words, the <strong>fuel consumption</strong> has a significant association with the type of engine (<strong>vs</strong>) with 99% confidence.</p>
<p>To illustrate further, let us this time consider a <strong>multiple logistic regression</strong> - that is, with multiple independent variables. Let us use a few new functions; for example, <strong>mvrnorm(.)</strong> from a library called <strong>MASS</strong> to generate a multivariate normal distribution, <strong>rbinom(.)</strong> to generate our binomial response variable, and finally <strong>glm(.)</strong> using a generalized linear model with binomial family. Here is what it looks like:</p>

<div class="sourceCode" id="cb748"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb748-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb748-2" data-line-number="2">binary =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb748-3" data-line-number="3">range =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">5</span>,<span class="dv">5</span>, <span class="dt">length.out=</span><span class="dv">100</span>) </a>
<a class="sourceLine" id="cb748-4" data-line-number="4">sample_size =<span class="st"> </span><span class="dv">100</span></a>
<a class="sourceLine" id="cb748-5" data-line-number="5">predictors =<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb748-6" data-line-number="6"><span class="co"># simulate an unexplained residual (Gaussian)</span></a>
<a class="sourceLine" id="cb748-7" data-line-number="7">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dv">0</span>, <span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">20</span> </a>
<a class="sourceLine" id="cb748-8" data-line-number="8">mu =<span class="st"> </span><span class="kw">rep</span>(<span class="dv">0</span>, predictors)</a>
<a class="sourceLine" id="cb748-9" data-line-number="9">sigma =<span class="st"> </span><span class="kw">diag</span>(<span class="dv">1</span>, predictors, predictors)</a>
<a class="sourceLine" id="cb748-10" data-line-number="10">x_observed =<span class="st"> </span>x =<span class="st">  </span>MASS<span class="op">::</span><span class="kw">mvrnorm</span>(<span class="dt">n =</span> sample_size, <span class="dt">mu=</span>mu, <span class="dt">Sigma=</span>sigma)</a>
<a class="sourceLine" id="cb748-11" data-line-number="11">y_observed =<span class="st"> </span><span class="kw">inverse.logit</span>(x_observed) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb748-12" data-line-number="12"><span class="co"># try some initial beta values and inject some bias</span></a>
<a class="sourceLine" id="cb748-13" data-line-number="13">b0 =<span class="st"> </span><span class="fl">2.3</span>; b1 =<span class="st"> </span><span class="dv">3</span>; b2 =<span class="st"> </span><span class="dv">1</span>; b3 =<span class="st"> </span><span class="fl">1.5</span> </a>
<a class="sourceLine" id="cb748-14" data-line-number="14">z =<span class="st"> </span>b0 <span class="op">+</span><span class="st"> </span>b1 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">1</span>] <span class="op">+</span><span class="st"> </span>b2 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">2</span>] <span class="op">+</span><span class="st"> </span>b3 <span class="op">*</span><span class="st"> </span>x[,<span class="dv">3</span>]</a>
<a class="sourceLine" id="cb748-15" data-line-number="15">y_expected =<span class="st"> </span><span class="kw">rbinom</span>(<span class="dt">n =</span> sample_size, <span class="dt">size=</span><span class="dv">1</span>, <span class="dt">prob=</span><span class="kw">inverse.logit</span>(z))</a>
<a class="sourceLine" id="cb748-16" data-line-number="16">x_obs =<span class="st"> </span>x_observed; y_exp =<span class="st"> </span>y_expected</a>
<a class="sourceLine" id="cb748-17" data-line-number="17">(<span class="dt">glm.model =</span> <span class="kw">glm</span>(y_exp  <span class="op">~</span><span class="st"> </span>x_obs,  <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;logit&quot;</span>)))</a></code></pre></div>
<pre><code>## 
## Call:  glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;))
## 
## Coefficients:
## (Intercept)       x_obs1       x_obs2       x_obs3  
##        2.26         4.04         0.99         1.95  
## 
## Degrees of Freedom: 99 Total (i.e. Null);  96 Residual
## Null Deviance:       120 
## Residual Deviance: 49.9  AIC: 57.9</code></pre>

<p>Here, we see four <strong>beta hat</strong> coefficients (including the intercept) which allows the logistic model to fit:</p>
<p><span class="math inline">\(\hat{\beta}_0\)</span> = 2.2559 <span class="math inline">\(\hat{\beta}_1\)</span> = 4.0386 <span class="math inline">\(\hat{\beta}_2\)</span> = 0.9902 <span class="math inline">\(\hat{\beta}_3\)</span> = 1.948.</p>
<p>Let us compare the outcome of running <strong>summary()</strong> vs <strong>anova()</strong>:</p>

<div class="sourceCode" id="cb750"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb750-1" data-line-number="1"><span class="kw">summary</span>(glm.model)</a></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = y_exp ~ x_obs, family = binomial(link = &quot;logit&quot;))
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.1530  -0.0481   0.0826   0.3347   2.4757  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    2.256      0.572    3.94 0.000081 ***
## x_obs1         4.039      0.998    4.05 0.000052 ***
## x_obs2         0.990      0.427    2.32   0.0205 *  
## x_obs3         1.948      0.664    2.93   0.0033 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 120.43  on 99  degrees of freedom
## Residual deviance:  49.94  on 96  degrees of freedom
## AIC: 57.94
## 
## Number of Fisher Scoring iterations: 7</code></pre>

<p>The outcome of our summary shows that the first coefficient <span class="math inline">\(x\_obs1\)</span> has three asterisks (***), indicating that this coefficient is statistically significant at <span class="math inline">\(\alpha=0.001\)</span>. The second coefficient <span class="math inline">\(x\_obs1\)</span> has one asterisk (*) indicating that this coefficient is significant at <span class="math inline">\(\alpha=0.05\)</span>. The third coefficient is significant at <span class="math inline">\(\alpha=0.10\)</span>. In other words, the first predictor has a significant effect on the outcome with 99% confidence. The second predictor has a significant association (or effect) on the outcome with 95% confidence. Lastly, the third predictor has a significant association (or effect) on the outcome with 90% confidence.</p>
<p>The result rejects the <strong>null hypothesis</strong> at their respective alphas.</p>

<div class="sourceCode" id="cb752"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb752-1" data-line-number="1"><span class="kw">anova</span>(glm.model, <span class="dt">test=</span><span class="st">&quot;Chisq&quot;</span>)</a></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model: binomial, link: logit
## 
## Response: y_exp
## 
## Terms added sequentially (first to last)
## 
## 
##       Df Deviance Resid. Df Resid. Dev Pr(&gt;Chi)    
## NULL                     99      120.4             
## x_obs  3     70.5        96       49.9  3.4e-15 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>

<p>Overall, an <strong>ANOVA</strong> using the <strong>Chi-square</strong> test shows that the observed predictors have a significant association or effect on the outcome with 99% confidence (at <span class="math inline">\(\alpha=0.001\)</span>).</p>
<p>One that is almost identical to the <strong>logit</strong> link function is what we call the <strong>probit</strong> link function. In the <strong>inverse-logit</strong> (CDF) function, we generate an S-shaped non-linear curve and determine the dichotomous nature of the outcome in which we say that the upper half of the curve corresponds to the positive half (e.g., YES, TRUE, ONE, and so on) versus the lower half corresponds to the negative half (e.g., NO, FALSE, ZERO, and so on). On the other hand, a bell-shaped non-linear curve is generated by the <strong>inverse-probit</strong> (CDF) function, which is a Gaussian Cumulative Density function.</p>
<p>When using <strong>glm()</strong>, we merely replace the link with <strong>probit</strong> like so:</p>

<div class="sourceCode" id="cb754"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb754-1" data-line-number="1">(<span class="dt">probit.model =</span> <span class="kw">glm</span>(dependent.y <span class="op">~</span><span class="st"> </span>independent.x, </a>
<a class="sourceLine" id="cb754-2" data-line-number="2">                    <span class="dt">family=</span><span class="kw">binomial</span>(<span class="dt">link=</span><span class="st">&quot;probit&quot;</span>)))</a></code></pre></div>

<p>We continue to expand on the concept of <strong>Logistic Regression</strong> and <strong>Generalized Regression Models (GLM)</strong> using <strong>glm(.)</strong> in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Poisson Regression</strong> Subsection under <strong>Regression</strong>.</p>
</div>
<div id="poisson-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.4</span> Poisson Regression <a href="statistics.html#poisson-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Similar to <strong>Logistic Regression</strong>, we also review <strong>Poisson Regression</strong> in the context of <strong>Generalized Linear Model (GLM)</strong>, emphasizing the use of a <strong>link function</strong>.</p>
<p><strong>Poisson Regression</strong> models a regression that describes the number of incidents occurring during a <strong>follow-up period</strong> (per hour, per day, per week). <strong>Poisson models</strong> are suitable for data in which, as examples, we count the number of tweets a person submits per day, the number of eye blinks per minute, or the number of cars paying in a toll plaza every hour.</p>
<p>Here, we deal with Poisson distribution, which is expressed as such:</p>
<p><span class="math display">\[y \sim Pois(\hat{\lambda}) \equiv N(\mu = \hat{\lambda}, \epsilon)\]</span></p>
<p>The intuition behind <strong>Poisson Regression</strong> also starts with two functions similar to <strong>Logistic Regression</strong>.</p>
<p><strong>Link Function</strong>: </p>
<p>A <strong>Link Function</strong> allows us to model regression in linear fashion.</p>
<p><span class="math display">\[\begin{align}
\text{link}(\hat{\lambda}) = \log_e\left(y\right) = z
\end{align}\]</span></p>
<p><strong>Inverse Link Function:</strong></p>
<p><span class="math display">\[\begin{align}
\text{link}^{-1}(z) = \hat{\lambda} =  \text{exp}(z)
\end{align}\]</span></p>
<p>where: <span class="math inline">\(z = \beta^TX\ \ \ \  \leftarrow\ \ \ \ \beta_0 + \sum_{i=1}^n\beta_i x_i\)</span>.</p>
<p>The Lambda, denoted by <span class="math inline">\(\lambda\)</span>, represents the mean of a <strong>Poisson distribution</strong>. The expectation is that lambda grows in an exponential non-linear fashion, as seen in the <strong>Inverse Link Function</strong>. However, our goal is to linearize the function; thus, we have the <strong>Link Function</strong>.</p>
<p>A detailed discussion of <strong>Poisson Regression</strong> is covered in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Regression</strong> Section.</p>
</div>
<div id="cox-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.5</span> Cox Regression <a href="statistics.html#cox-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p><strong>Cox Regression</strong>, also known as <strong>Cox proportional hazard</strong> regression, models a regression describing the length of time an event occurs, also called <strong>time-to-event (TTE)</strong> occurrence. That applies to cases such as survival rate and hazard rate of subjects <span class="citation">(Cox D. R. <a href="bibliography.html#ref-ref921d">1972</a>)</span>.</p>
<p>Let us review Figure <a href="statistics.html#fig:survival">6.36</a> to illustrate.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:survival"></span>
<img src="DS_files/figure-html/survival-1.png" alt="Survival Probability" width="70%" />
<p class="caption">
Figure 6.36: Survival Probability
</p>
</div>

<p>Suppose from the figure that we split a group of subjects into two environments - one environment with treatment and another environment with no treatment. We then measure the probability of survival of the two groups.</p>
<p>Below is a summary of the <strong>survival probability</strong> in the figure:</p>

<pre><code>##                                           T1   T2   T3   T4 T5
## Total Subjects in Treatment             9.00 7.00 6.00 2.00  1
## Total Subjects not in Treatment        11.00 9.00 9.00 6.00  4
## Treated Subjects Failing to Survive     2.00 1.00 4.00 1.00  1
## Untreated Subjects Failing to Survive   2.00 0.00 3.00 2.00  4
## Survival Probab. of Treated Subjects    0.78 0.67 0.22 0.11  0
## Survival Probab. of Untreated Subjects  0.82 0.82 0.55 0.36  0</code></pre>

<p>Survival probability is the probability of surviving through time (t) and is expressed as:</p>
<p><span class="math display">\[\begin{align}
S(t) = \prod_{i = 1}^t \frac{n_i - d_i}{n_i}
\end{align}\]</span></p>
<p>For example, there are a total of 9 subjects in treatment at time one, <strong>T1</strong> and a total of 11 subjects that are not in treatment. Now, at time three, <strong>T3</strong>, <strong>because some subjects failed to survive previous to</strong> <strong>T3</strong>, we are left with 6 subjects in treatment and 9 subjects not in treatment. At time one, <strong>T1</strong>, we have 2 subjects in treatment that failed to survive and 2 subjects not in treatment that failed to survive. At time three, <strong>T3</strong>, we have 4 subjects in treatment that failed to survive and 3 subjects not in treatment that failed to survive. The survival probability at time one, <strong>T1</strong>, of the group with treatment and the group with no treatment are 0.78 and 0.82, respectively. At time three, <strong>T3</strong>, the survival probabilities of the group with treatment and the group without treatment are 0.22 and 0.55, respectively.</p>
<p>We calculate the survival probability at <strong>T1</strong> like so:</p>

<p><span class="math display">\[\begin{align}
S_{(treated)}(t=1) = \left(\frac{9-4}{9}\right) = 0.56
\ \ \ \ \ \ \ \ 
S_{(not treated)}(t=1) = \left(\frac{11 - 0}{11}\right) = 1.00
\end{align}\]</span>
</p>
<p>The survival probability at <strong>T3</strong> is calculated like so:</p>
<p><span class="math display">\[\begin{align*}
S_{(treated)}(t=3) {}&amp;= \left(\frac{9-4}{9}\right)_{T1} \times
 \left(\frac{5-0}{5}\right)_{T2}  \times
 \left(\frac{5-3}{5}\right)_{T3}  = 0.22\\
S_{(not\ treated)}(t=3) &amp;= \left(\frac{11-0}{11}\right)_{T1} \times
 \left(\frac{11-1}{11}\right)_{T2}  \times
 \left(\frac{10-4}{10}\right)_{T3}  = 0.55
\end{align*}\]</span></p>
<p>Also, it helps to understand the <strong>Hazard Ratio</strong> (or risk ratio) which is expressed as:</p>
<p><span class="math display">\[\begin{align}
R(t) = \frac{risk_{(treatment)}}{risk_{(control)}}
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><strong>treatment</strong> refers to the exposure of a group to treatment (this is an experimental group).</li>
<li><strong>control</strong> refers to a baseline group in a sample not exposed to treatment (this is a control group).</li>
</ul>
<p>For example, suppose we have a sample of plants which we expose to a type of soil with special treatment. Our experiment is to determine if the plant survives under such conditions.</p>

<div class="sourceCode" id="cb756"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb756-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">142</span>)</a>
<a class="sourceLine" id="cb756-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb756-3" data-line-number="3"><span class="co"># T - with treatment, N - with no treatment</span></a>
<a class="sourceLine" id="cb756-4" data-line-number="4">treatment_range =<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;T&quot;</span>, <span class="st">&quot;N&quot;</span>) </a>
<a class="sourceLine" id="cb756-5" data-line-number="5">population =<span class="st"> </span><span class="kw">sample</span>( treatment_range, <span class="dt">size =</span> <span class="dv">300</span>, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb756-6" data-line-number="6">sample =<span class="st">  </span><span class="kw">sample</span>( population , <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">FALSE</span> )</a>
<a class="sourceLine" id="cb756-7" data-line-number="7">treated =<span class="st"> </span>sample[<span class="kw">which</span>(sample<span class="op">==</span><span class="st">&quot;T&quot;</span>)]</a>
<a class="sourceLine" id="cb756-8" data-line-number="8">not_treated =<span class="st"> </span>sample[<span class="kw">which</span>(sample<span class="op">==</span><span class="st">&quot;N&quot;</span>)]</a></code></pre></div>

<p>In terms of risk ratio, we have the following computation:</p>
<div class="sourceCode" id="cb757"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb757-1" data-line-number="1">(<span class="dt">risk_ratio =</span> <span class="kw">length</span>(treated) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(not_treated))</a></code></pre></div>
<pre><code>## [1] 1.222</code></pre>
<p>A risk ratio gives the following interpretation:</p>
<p><span class="math display">\[\begin{align*}
R {}&amp;= 1\ \ \ \ \text{exposure to treatment has no impact on risk of an incident}\\
R &amp;&gt; 1\ \ \ \ \text{suggests an increased risk of an incident in the treatment group} \\
R &amp;&lt; 1\ \ \ \ \ \text{suggests a reduced risk of an incident in the treatment group}
\end{align*}\]</span></p>
<p>Because <strong>cox regression</strong> covers a wide range of topics, we only cover here <strong>significance of regression</strong>.</p>
<p>The Cox regression model at time (t) is expressed as:</p>
<p><span class="math display">\[\begin{align}
ln \frac{h(t|x_1, x_2,...,x_n)}{h_0(t)} = \sum_{i=1}^n \beta_i x_i 
\end{align}\]</span></p>
<p>which is derived from the following equation:</p>
<p><span class="math display">\[\begin{align}
h(t) = h_0(t|x_1, x_2,...,x_n)\times exp \left(\sum_{i=1}^n \beta_i x_i \right)
\end{align}\]</span></p>
<p>where:</p>
<ul>
<li><span class="math inline">\(h(\mathbf{t|x_1, x_2,...,x_n)}\)</span> is the expected incidence (hazard) at time (t) given independent variables.</li>
<li><span class="math inline">\(h_0(t)\)</span> is the baseline incidence (hazard) at time (t) when <span class="math inline">\(x_1 = x_2 = ... = x_n = 0\)</span>.</li>
<li><span class="math inline">\(\mathbf{\beta_i}\)</span> are the coefficients,</li>
<li><span class="math inline">\(\mathbf{x_i}\)</span> are the predictive variables.</li>
</ul>
<p>To illustrate, we create a dataset with survival range, environment type, and status. The idea is to understand the survival pattern of a specie exposed to three different types of environments where status indicates the following (1=survived, 0=failed to survive):</p>

<div class="sourceCode" id="cb759"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb759-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb759-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">300</span></a>
<a class="sourceLine" id="cb759-3" data-line-number="3">survival_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, <span class="dv">50</span>)</a>
<a class="sourceLine" id="cb759-4" data-line-number="4">status_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb759-5" data-line-number="5">environment_range =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb759-6" data-line-number="6">time =<span class="st"> </span><span class="kw">sample</span>( survival_range, <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-7" data-line-number="7">status =<span class="st">  </span><span class="kw">sample</span>( status_range , <span class="dt">size =</span>sample_size, <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-8" data-line-number="8">environment =<span class="st"> </span><span class="kw">sample</span>( environment_range , <span class="dt">size =</span>sample_size, </a>
<a class="sourceLine" id="cb759-9" data-line-number="9">                      <span class="dt">replace=</span><span class="ot">TRUE</span> )</a>
<a class="sourceLine" id="cb759-10" data-line-number="10">data =<span class="st"> </span><span class="kw">cbind</span>(time, status) </a>
<a class="sourceLine" id="cb759-11" data-line-number="11">data =<span class="st"> </span><span class="kw">cbind</span>(data,  environment  )</a>
<a class="sourceLine" id="cb759-12" data-line-number="12">data =<span class="st"> </span><span class="kw">as.data.frame</span>(data)</a>
<a class="sourceLine" id="cb759-13" data-line-number="13">data<span class="op">$</span>environment =<span class="st"> </span><span class="kw">as.factor</span>(data<span class="op">$</span>environment)</a>
<a class="sourceLine" id="cb759-14" data-line-number="14"><span class="kw">head</span>(data)</a></code></pre></div>
<pre><code>##   time status environment
## 1   33      0           3
## 2   20      0           3
## 3   31      0           2
## 4   24      0           3
## 5    7      0           3
## 6    4      1           1</code></pre>

<p>Let us use a built-in function called <strong>coxph(.)</strong> from <strong>survival</strong> library to model our <strong>Cox regression</strong>:</p>

<div class="sourceCode" id="cb761"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb761-1" data-line-number="1"><span class="kw">library</span>(survival)</a>
<a class="sourceLine" id="cb761-2" data-line-number="2">(<span class="dt">cox.model =</span> <span class="kw">coxph</span>( <span class="kw">Surv</span>(time, status) <span class="op">~</span><span class="st"> </span>environment,  <span class="dt">data =</span> data ))</a></code></pre></div>
<pre><code>## Call:
## coxph(formula = Surv(time, status) ~ environment, data = data)
## 
##              coef exp(coef) se(coef)    z   p
## environment2 -0.1       0.9      0.2 -0.5 0.6
## environment3  0.2       1.2      0.2  1.0 0.3
## 
## Likelihood ratio test=2  on 2 df, p=0.3
## n= 300, number of events= 152</code></pre>

<p>Now, we summarize the model for analysis:</p>

<div class="sourceCode" id="cb763"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb763-1" data-line-number="1"><span class="kw">summary</span>(cox.model)</a></code></pre></div>
<pre><code>## Call:
## coxph(formula = Surv(time, status) ~ environment, data = data)
## 
##   n= 300, number of events= 152 
## 
##                coef exp(coef) se(coef)     z Pr(&gt;|z|)
## environment2 -0.104     0.901    0.194 -0.54     0.59
## environment3  0.204     1.226    0.209  0.98     0.33
## 
##              exp(coef) exp(-coef) lower .95 upper .95
## environment2     0.901      1.110     0.616      1.32
## environment3     1.226      0.816     0.814      1.85
## 
## Concordance= 0.528  (se = 0.027 )
## Likelihood ratio test= 2.31  on 2 df,   p=0.3
## Wald test            = 2.36  on 2 df,   p=0.3
## Score (logrank) test = 2.38  on 2 df,   p=0.3</code></pre>

<p>The <strong>z</strong> is a wald statistics computed based on <span class="math inline">\(z = \frac{coef}{se(coef)}\)</span>. In the example above, our <strong>p-value</strong> is greater than our typical alpha values, and so our <strong>null hypothesis</strong> holds in which our coefficient does not have any significant effect (or the different environment does not contribute to survivability).</p>
<p><span class="math display">\[\begin{align}
H_0 {}&amp;: \beta_1 = 0 \\
H_1 &amp;: \beta_1 \ne 0
\end{align}\]</span></p>
<p>However, if we are to consider <strong>inference for regression</strong>, which we discuss further in the next section, we may at least be able to predict the probability of survivability.</p>
<p>We use a built-in function called <strong>survfit(.)</strong> to fit our <strong>cox model</strong>, then we capture a predicted probability at time (t) (where i=20):</p>

<div class="sourceCode" id="cb765"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb765-1" data-line-number="1">p =<span class="st"> </span><span class="kw">survfit</span>(cox.model)</a>
<a class="sourceLine" id="cb765-2" data-line-number="2">i =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb765-3" data-line-number="3">t =<span class="st"> </span>p<span class="op">$</span>time[i];  r =<span class="st"> </span>p<span class="op">$</span>n.risk[i]; e =<span class="st"> </span>p<span class="op">$</span>n.event[i]; s =<span class="st"> </span>p<span class="op">$</span>surv[i]</a>
<a class="sourceLine" id="cb765-4" data-line-number="4">l =<span class="st"> </span>p<span class="op">$</span>lower[i]; u =<span class="st"> </span>p<span class="op">$</span>upper[i]; c =<span class="st"> </span>p<span class="op">$</span>conf.int</a>
<a class="sourceLine" id="cb765-5" data-line-number="5"><span class="kw">t</span>(<span class="kw">as.matrix</span>(<span class="kw">round</span>(<span class="kw">c</span>(<span class="st">&quot;survival prob.&quot;</span>=<span class="kw">round</span>(s,<span class="dv">2</span>), <span class="st">&quot;time&quot;</span>=<span class="st"> </span>t, </a>
<a class="sourceLine" id="cb765-6" data-line-number="6">    <span class="st">&quot;lower&quot;</span>=l, <span class="st">&quot;upper&quot;</span>=u, <span class="st">&quot;conf&quot;</span>=c), <span class="dv">2</span>)))</a></code></pre></div>
<pre><code>##      survival prob. time lower upper conf
## [1,]           0.81   20  0.76  0.86 0.95</code></pre>

<p>Let us plot the survival probability (see Figure <a href="statistics.html#fig:coxreg">6.37</a>).</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:coxreg"></span>
<img src="DS_files/figure-html/coxreg-1.png" alt="Kaplan-Meier Estimate" width="70%" />
<p class="caption">
Figure 6.37: Kaplan-Meier Estimate
</p>
</div>

<p>In the next few sections, let us cover <strong>Polynomial</strong> regressions as we continue to emphasize on significance of regression.</p>
</div>
<div id="polynomial-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.6</span> Polynomial Regression <a href="statistics.html#polynomial-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>So far, we have discussed data points that follow <strong>normal distribution</strong> and use <strong>linear regression</strong> to fit a model to data. We also discussed data points that follow <strong>exponential distribution</strong> and use <strong>generalized linear regression</strong> to fit.</p>
<p>Now in cases in which the distribution of data may not necessarily reflect a simple straight line or a simple curve line but rather curvier or deeply curvy, it helps to carefully obtain a model that is not too <strong>wiggly</strong> causing an overfit, or that is not approaching a straight line causing an underfit. Both situations offset the power of inference or prediction.</p>
<p>Therefore, in this section, we discuss the importance of fitting a model - ensuring our model regresses to the actual data set. The more representative our model is, the more we can use it for prediction in general. However, an overfitted model may not be able to apply to a new data set; likewise, one that is underfitted may just as well not apply.</p>
<p>This section talks about higher-degree polynomials as an extended discussion of <strong>simple linear regression</strong> - one-degree polynomial.</p>
<p>A good understanding of this section requires a good review of <strong>B-spline regression</strong> in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>). It may help to re-iterate the four knobs that we can use to manipulate and influence the curvature of B-splines, especially when fitting a model:</p>
<ul>
<li>the coefficients</li>
<li>the number of knots</li>
<li>the placements of knots</li>
<li>the basis function</li>
</ul>
<p>For illustration, let us use <strong>Chebyshev polynomial of the first kind</strong> as a template for our next discussion. <strong>Chebyshev polynomial</strong> is introduced in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under the <strong>Higher Degree Polynomials</strong> Section. Out of four polynomials, let us use the quintic polynomial and generate the response variables: one for observed, one for expected:</p>

<div class="sourceCode" id="cb767"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb767-1" data-line-number="1">quadratic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb767-2" data-line-number="2">cubic     &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x }</a>
<a class="sourceLine" id="cb767-3" data-line-number="3">quartic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>  }</a>
<a class="sourceLine" id="cb767-4" data-line-number="4">quintic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">16</span><span class="op">*</span>x<span class="op">^</span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">20</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb767-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>)</a>
<a class="sourceLine" id="cb767-6" data-line-number="6">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">5</span>  <span class="co"># simulate Gaussian residual</span></a>
<a class="sourceLine" id="cb767-7" data-line-number="7">y_observed =<span class="st"> </span><span class="kw">quintic</span>(x) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb767-8" data-line-number="8">y_expected =<span class="st"> </span><span class="kw">quintic</span>(x) </a></code></pre></div>

<p>Here, to create a model that fits the quintic polynomial above for <strong>Polynomial regression</strong>, let us use a new function called <strong>bs(.)</strong> from the <strong>splines</strong> library. The <strong>bs</strong> stands for B-spline. Notice that we use five degrees of freedom. That is to attempt to render enough <strong>wiggles</strong> or <strong>oscillation</strong> to form a quintic polynomial curve, as we see after plotting.</p>

<div class="sourceCode" id="cb768"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb768-1" data-line-number="1"><span class="kw">library</span>(splines)</a>
<a class="sourceLine" id="cb768-2" data-line-number="2">poly.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">5</span>))</a></code></pre></div>

<p>Now, let us plot (See Figure <a href="statistics.html#fig:bsplineregress">6.38</a>). Notice the polynomial generated by <span class="math inline">\(bs(x,df=5)\)</span>. It can generate a quintic polynomial that fits close to the actual.</p>

<div class="sourceCode" id="cb769"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb769-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb769-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb769-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Chebyshev polynomial of the first kind (Quintic poly)&quot;</span>)</a>
<a class="sourceLine" id="cb769-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb769-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-7" data-line-number="7"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(poly.model), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb769-8" data-line-number="8"><span class="kw">legend</span>(<span class="dv">0</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb769-9" data-line-number="9">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;poly.model&quot;</span>),</a>
<a class="sourceLine" id="cb769-10" data-line-number="10">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bsplineregress"></span>
<img src="DS_files/figure-html/bsplineregress-1.png" alt="Chebyshev polynomial of the first kind (Quintic poly)" width="70%" />
<p class="caption">
Figure 6.38: Chebyshev polynomial of the first kind (Quintic poly)
</p>
</div>

<p>Let us evaluate the <strong>significance of regression</strong> for our polynomial model:</p>

<div class="sourceCode" id="cb770"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb770-1" data-line-number="1"><span class="kw">summary</span>(poly.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y_expected ~ bs(x, df = 5))
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.377 -0.098  0.000  0.098  0.377 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -0.6227     0.0520   -12.0   &lt;2e-16 ***
## bs(x, df = 5)1   3.4213     0.0992    34.5   &lt;2e-16 ***
## bs(x, df = 5)2  -3.6255     0.0669   -54.2   &lt;2e-16 ***
## bs(x, df = 5)3   4.8710     0.0910    53.5   &lt;2e-16 ***
## bs(x, df = 5)4  -2.1758     0.0740   -29.4   &lt;2e-16 ***
## bs(x, df = 5)5   1.2455     0.0757    16.4   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.118 on 144 degrees of freedom
## Multiple R-squared:  0.973,  Adjusted R-squared:  0.972 
## F-statistic: 1.04e+03 on 5 and 144 DF,  p-value: &lt;2e-16</code></pre>

<p>Notice that we base the number of coefficients (excluding intercept) on the number of degrees of freedom provided to our model. Looking at the geometrical fit of our model, it may seem that the fitted model using bs(x, df=5) may not be close enough to the actual polynomial curve - it does not regress enough. Nonetheless, our coefficients are significant at <span class="math inline">\(\alpha = 0.001\)</span>. We may accept the model or continue to adjust and improve.</p>
<p>In the later section, we show how to adjust the model by covering <strong>power of prediction</strong> under <strong>Inference for Regression</strong>.</p>
</div>
<div id="b-splines-and-natural-splines" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.7</span> B-Splines and Natural Splines  <a href="statistics.html#b-splines-and-natural-splines" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We extend our discussion of <strong>B-Splines</strong> and <strong>Natural Splines</strong> from Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under <strong>Polynomial Regression</strong> Section and <strong>Polynomial Interpolation</strong> Section.</p>
<p>In the previous section, we show how to interpret the <strong>significance of regression</strong>. In this section, we continue <strong>B-spline regression</strong> from the perspective of <strong>prediction</strong>. We can attain Better prediction if only we can adjust a <strong>B-spline model</strong> to fit better. Here, we show two adjustments to fit a <strong>B-SPline</strong> and one adjustment to fit a <strong>Natural Spline</strong>. From there, we obtain three models which we can use to predict.</p>
<p>Let us continue to use the same dataset (using one of the <strong>Chebyshev polynomials</strong>): </p>

<div class="sourceCode" id="cb772"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb772-1" data-line-number="1">quadratic &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">-</span><span class="st"> </span><span class="dv">1</span> }</a>
<a class="sourceLine" id="cb772-2" data-line-number="2">cubic     &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">4</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">-</span><span class="st"> </span><span class="dv">3</span><span class="op">*</span>x }</a>
<a class="sourceLine" id="cb772-3" data-line-number="3">quartic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) {  <span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">4</span> <span class="op">-</span><span class="st"> </span><span class="dv">8</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span>  }</a>
<a class="sourceLine" id="cb772-4" data-line-number="4">quintic   &lt;-<span class="st"> </span><span class="cf">function</span>(x) { <span class="dv">16</span><span class="op">*</span>x<span class="op">^</span><span class="dv">5</span> <span class="op">-</span><span class="st"> </span><span class="dv">20</span><span class="op">*</span>x<span class="op">^</span><span class="dv">3</span> <span class="op">+</span><span class="st"> </span><span class="dv">5</span><span class="op">*</span>x  }</a>
<a class="sourceLine" id="cb772-5" data-line-number="5">x =<span class="st"> </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>)</a>
<a class="sourceLine" id="cb772-6" data-line-number="6">e =<span class="st"> </span><span class="kw">rnorm</span>(<span class="kw">length</span>(x), <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>) <span class="op">/</span><span class="st"> </span><span class="dv">5</span>  <span class="co"># simulate Gaussian residual</span></a>
<a class="sourceLine" id="cb772-7" data-line-number="7">y_observed =<span class="st"> </span><span class="kw">quintic</span>(x) <span class="op">+</span><span class="st"> </span>e</a>
<a class="sourceLine" id="cb772-8" data-line-number="8">y_expected =<span class="st"> </span><span class="kw">quintic</span>(x) </a></code></pre></div>

<p>Let us now try to adjust:</p>
<ul>
<li>use df=3 (3 degrees of freedom) - for the sake of showing insufficient oscillation (underfit):</li>
</ul>

<div class="sourceCode" id="cb773"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb773-1" data-line-number="1">adj1.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">3</span>))</a></code></pre></div>

<ul>
<li>still use df=5 (5 degrees of freedom) with the following number of knots and placements - to show that the wrong number of knots and placement could create greater displacement of the oscillation (with possible signs of rank deficiency - see next topic around knob adjustments):</li>
</ul>

<div class="sourceCode" id="cb774"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb774-1" data-line-number="1">x.df5.knots4 =<span class="st"> </span><span class="kw">bs</span>(x, <span class="dt">df=</span><span class="dv">5</span>, <span class="dt">knots=</span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.6</span>, <span class="fl">-0.2</span>, <span class="dv">5</span>, <span class="fl">0.9</span>))</a>
<a class="sourceLine" id="cb774-2" data-line-number="2">adj2.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span>x.df5.knots4)</a></code></pre></div>

<ul>
<li>use another function called <strong>ns()</strong> - <strong>natural spline</strong> with the following parameters - to show that even a natural spline, we can also find ways to fit a model:</li>
</ul>

<div class="sourceCode" id="cb775"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb775-1" data-line-number="1">adj3.model =<span class="st"> </span><span class="kw">lm</span>(y_expected <span class="op">~</span><span class="st"> </span><span class="kw">ns</span>(x, <span class="dt">df=</span><span class="dv">5</span>))</a></code></pre></div>

<p>Let us try to plot (this time coloring the actual fit with grey color to emphasize the other adjusted models). See Figure <a href="statistics.html#fig:knobsregress">6.39</a>.</p>

<div class="sourceCode" id="cb776"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb776-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb776-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb776-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Fitting Adjusted B-Spline Models&quot;</span>)</a>
<a class="sourceLine" id="cb776-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb776-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb776-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb776-7" data-line-number="7"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj1.model), <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-8" data-line-number="8"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj2.model), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-9" data-line-number="9"><span class="kw">lines</span>(x, <span class="kw">fitted</span>(adj3.model), <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb776-10" data-line-number="10"><span class="kw">legend</span>(<span class="fl">0.1</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb776-11" data-line-number="11">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;adj1.model&quot;</span>, </a>
<a class="sourceLine" id="cb776-12" data-line-number="12">              <span class="st">&quot;adj2.model&quot;</span>,  <span class="st">&quot;adj3.model&quot;</span>),</a>
<a class="sourceLine" id="cb776-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knobsregress"></span>
<img src="DS_files/figure-html/knobsregress-1.png" alt="Fitting Adjusted B-Spline Models" width="70%" />
<p class="caption">
Figure 6.39: Fitting Adjusted B-Spline Models
</p>
</div>

<p>As one can imagine, there are more ways than just one to generate a linear model with multiple knobs available to adjust, allowing us to try to tune a polynomial model so it can fit well to a given data.</p>
<p>Now, using the adjusted models, we demonstrate three predictions:</p>

<div class="sourceCode" id="cb777"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb777-1" data-line-number="1"><span class="co"># predict via interpolation</span></a>
<a class="sourceLine" id="cb777-2" data-line-number="2">x.new =<span class="st">  </span><span class="kw">seq</span>(<span class="op">-</span><span class="dv">1</span>, <span class="dv">1</span>, <span class="dt">length.out=</span><span class="dv">150</span>) </a>
<a class="sourceLine" id="cb777-3" data-line-number="3">y1.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj1.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a>
<a class="sourceLine" id="cb777-4" data-line-number="4">y2.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj2.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a></code></pre></div>
<pre><code>## Warning in predict.lm(adj2.model, newdata = data.frame(x = x.new)): prediction
## from a rank-deficient fit may be misleading</code></pre>
<div class="sourceCode" id="cb779"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb779-1" data-line-number="1">y3.predict =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(adj3.model, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">x =</span> x.new))</a></code></pre></div>

<p>We should also notice that the second prediction renders a warning about <strong>rank deficiency</strong>. If we are to run a summary of the model, it turns out that the estimated value of one of the <strong>coefficients</strong> is not available due to <strong>singularity</strong>, effectively rendering a linear combination with one being deficient in forming a <strong>full rank</strong>.   </p>

<div class="sourceCode" id="cb780"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb780-1" data-line-number="1"><span class="kw">summary</span>(adj2.model)</a></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y_expected ~ x.df5.knots4)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.659 -0.271 -0.013  0.285  0.972 
## 
## Coefficients: (1 not defined because of singularities)
##               Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -1.271      0.194   -6.54  1.0e-09 ***
## x.df5.knots41    4.207      0.349   12.05  &lt; 2e-16 ***
## x.df5.knots42   -0.342      0.218   -1.57     0.12    
## x.df5.knots43    1.607      0.308    5.22  6.2e-07 ***
## x.df5.knots44    2.650      0.271    9.80  &lt; 2e-16 ***
## x.df5.knots45   -0.261      0.250   -1.05     0.30    
## x.df5.knots46    2.929      0.348    8.43  3.5e-14 ***
## x.df5.knots47       NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.362 on 143 degrees of freedom
## Multiple R-squared:  0.75,   Adjusted R-squared:  0.739 
## F-statistic: 71.3 on 6 and 143 DF,  p-value: &lt;2e-16</code></pre>

<p>Let us plot the predictions (see Figure <a href="statistics.html#fig:knobsregress1">6.40</a>).</p>

<div class="sourceCode" id="cb782"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb782-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>), </a>
<a class="sourceLine" id="cb782-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (quintic)&quot;</span>,</a>
<a class="sourceLine" id="cb782-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Plotting Predicted B-Spline Lines&quot;</span>)</a>
<a class="sourceLine" id="cb782-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb782-5" data-line-number="5"><span class="kw">points</span>(x, y_observed, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb782-6" data-line-number="6"><span class="kw">lines</span>(x, y_expected, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb782-7" data-line-number="7"><span class="kw">lines</span>(x.new, y1.predict, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-8" data-line-number="8"><span class="kw">lines</span>(x.new, y2.predict, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-9" data-line-number="9"><span class="kw">lines</span>(x.new, y3.predict, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb782-10" data-line-number="10"><span class="kw">legend</span>(<span class="fl">0.1</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb782-11" data-line-number="11">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;expected fit&quot;</span>, <span class="st">&quot;y1.predict&quot;</span>, </a>
<a class="sourceLine" id="cb782-12" data-line-number="12">              <span class="st">&quot;y2.predict&quot;</span>,  <span class="st">&quot;y3.predict&quot;</span>),</a>
<a class="sourceLine" id="cb782-13" data-line-number="13">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:knobsregress1"></span>
<img src="DS_files/figure-html/knobsregress1-1.png" alt="Plotting Predicted B-Spline Lines" width="80%" />
<p class="caption">
Figure 6.40: Plotting Predicted B-Spline Lines
</p>
</div>

<p>Notice that the curves in the plot correspond to those in Figure <a href="statistics.html#fig:knobsregress1">6.40</a>. That is only because the predicted curves are already the fitted curves. Any new data always predicts a value in the y-axis inside the fitted curve.</p>
</div>
<div id="spline-smoothing" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.8</span> Spline Smoothing <a href="statistics.html#spline-smoothing" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>There are times when curves seem rough - rather than smooth. Hence, another form of adjustment to make is by smoothing splines. In terms of the formula, we reference Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under <strong>B-Spline Regression</strong> Subsection under <strong>Approximating Polynomial Functions by Interpolation</strong> Section.</p>
<p>Note here that if done optimally, smoothing a curve allows us to also <strong>fit a smooth spline</strong>.</p>
<p>To illustrate, we intentionally reduce our dataset to a sample size of maybe only 10. That creates a crooked line instead of a smooth line. We use a function in R called <strong>approx(.)</strong> to simulate a crooked line.</p>

<div class="sourceCode" id="cb783"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb783-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb783-2" data-line-number="2">sample_size =<span class="st"> </span><span class="dv">20</span></a>
<a class="sourceLine" id="cb783-3" data-line-number="3">x =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>,sample_size)</a>
<a class="sourceLine" id="cb783-4" data-line-number="4">y =<span class="st"> </span><span class="kw">rnorm</span>(<span class="dt">n=</span>sample_size, <span class="dt">mean=</span><span class="dv">0</span>, <span class="dt">sd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb783-5" data-line-number="5">crooked.line =<span class="st"> </span><span class="kw">approx</span>(x, y, <span class="dt">method=</span><span class="st">&quot;linear&quot;</span>, <span class="dt">n=</span>sample_size)</a></code></pre></div>

<p>We then apply a few smoothing adjustments to the crooked line by using two built-in R functions called <strong>spline()</strong> and <strong>smooth.spline()</strong> to demonstrate our case.</p>

<div class="sourceCode" id="cb784"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb784-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb784-2" data-line-number="2">spline =<span class="st"> </span><span class="kw">spline</span>(x, y, <span class="dt">n=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb784-3" data-line-number="3">spline1.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">0.30</span>)</a>
<a class="sourceLine" id="cb784-4" data-line-number="4">spline2.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">1.00</span>)</a>
<a class="sourceLine" id="cb784-5" data-line-number="5">spline3.model =<span class="st"> </span><span class="kw">smooth.spline</span>(x, y, <span class="dt">df =</span> <span class="dv">9</span>)</a></code></pre></div>

<p>We are now ready to predict.</p>

<div class="sourceCode" id="cb785"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb785-1" data-line-number="1">x.interpolate =<span class="st"> </span><span class="kw">seq</span>(<span class="dv">1</span>, sample_size, <span class="dt">length.out=</span><span class="dv">100</span>)</a>
<a class="sourceLine" id="cb785-2" data-line-number="2">smooth1.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline1.model, x.interpolate)</a>
<a class="sourceLine" id="cb785-3" data-line-number="3">smooth2.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline2.model, x.interpolate)</a>
<a class="sourceLine" id="cb785-4" data-line-number="4">smooth3.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(spline3.model, x.interpolate)</a></code></pre></div>

<p>Let us plot our prediction (see Figure <a href="statistics.html#fig:splineapprox">6.41</a>).</p>

<div class="sourceCode" id="cb786"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb786-1" data-line-number="1"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb786-2" data-line-number="2">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (approx)&quot;</span>,</a>
<a class="sourceLine" id="cb786-3" data-line-number="3">     <span class="dt">main=</span><span class="st">&quot;Fitting a Smooth Spline&quot;</span>)</a>
<a class="sourceLine" id="cb786-4" data-line-number="4"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb786-5" data-line-number="5"><span class="kw">lines</span>(crooked.line, <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)</a>
<a class="sourceLine" id="cb786-6" data-line-number="6"><span class="kw">lines</span>(spline, <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-7" data-line-number="7"><span class="kw">lines</span>(smooth1.line, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-8" data-line-number="8"><span class="kw">lines</span>(smooth2.line, <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-9" data-line-number="9"><span class="kw">lines</span>(smooth3.line, <span class="dt">col=</span><span class="st">&quot;purple&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb786-10" data-line-number="10"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb786-11" data-line-number="11"><span class="kw">legend</span>(<span class="dv">7</span>, <span class="fl">-0.5</span>, </a>
<a class="sourceLine" id="cb786-12" data-line-number="12">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;crooked.line&quot;</span>, <span class="st">&quot;spline&quot;</span>, </a>
<a class="sourceLine" id="cb786-13" data-line-number="13">              <span class="st">&quot;smooth1.line&quot;</span>,  <span class="st">&quot;smooth2.line&quot;</span>,</a>
<a class="sourceLine" id="cb786-14" data-line-number="14">              <span class="st">&quot;smooth3.line&quot;</span>),</a>
<a class="sourceLine" id="cb786-15" data-line-number="15">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;brown&quot;</span>, </a>
<a class="sourceLine" id="cb786-16" data-line-number="16">           <span class="st">&quot;darksalmon&quot;</span>, <span class="st">&quot;purple&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:splineapprox"></span>
<img src="DS_files/figure-html/splineapprox-1.png" alt="Fitting a Smooth Spline" width="80%" />
<p class="caption">
Figure 6.41: Fitting a Smooth Spline
</p>
</div>

<p>The spline shows a smooth curve that travels almost through all the points touched by the crooked line. Therefore, we can say that spline is just a smoothing method for a crooked line. However, modeling a spline to fit our data gets overfitting this way.</p>
<p>The spline1.model has a smooth parameter (spar) of 0.30. That is a way to force the spline to fit about 30% through the data. So it does not overfit much. On the other hand, spline2.model has a spar of 1.0, which simulates a straight <strong>linear regression</strong> fitting. As we can see, fitting this way could end up underfitting.</p>
<p>Overfitting may risk including outliers while, at the same time, the model cannot be generalized in usage. On the other hand, underfitting may eliminate outliers but exclude significant influencers.</p>
<p>Overall, spline1.model is a better polynomial regression model than the others but may still be improved.</p>
<p>Other parameters can be tuned using the <strong>smooth.spline(.)</strong> function. To view a list of parameters, we can just run it like so (notice how it calculates the other parameters by default):</p>

<div class="sourceCode" id="cb787"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb787-1" data-line-number="1"><span class="co"># output of a spline model</span></a>
<a class="sourceLine" id="cb787-2" data-line-number="2"><span class="kw">smooth.spline</span>(x, y, <span class="dt">spar=</span><span class="fl">0.30</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, spar = 0.3)
## 
## Smoothing Parameter  spar= 0.3  lambda= 0.00001489
## Equivalent Degrees of Freedom (Df): 12.87
## Penalized Criterion (RSS): 5.869
## GCV: 2.308</code></pre>

<p>The output includes two performance metrics:</p>
<ul>
<li><strong>RSS</strong> - calculates a <strong>Penalized criterion</strong>. Recall <strong>residual sum squares</strong> covered in <strong>least squares</strong>.</li>
<li><strong>GCV</strong> - generalized cross-validation. Recall <strong>bandwidth selection</strong> criterion covered in <strong>KDE</strong>. Similarly, we use <strong>GCV</strong> for <strong>parameter selection</strong> for <strong>optimal spline smoothing</strong>. There are three parameters shown: <strong>Spar</strong>, <strong>Lambda</strong>, and <strong>Degrees of Freedom</strong>.</li>
</ul>
<p>We can use degrees of freedom like so (notice how it calculates the other parameters by default). Here, fewer degrees of freedom generate a <strong>linear regression</strong>:</p>

<div class="sourceCode" id="cb789"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb789-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y, <span class="dt">df=</span><span class="dv">6</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, df = 6)
## 
## Smoothing Parameter  spar= 0.5182  lambda= 0.0005617 (12 iterations)
## Equivalent Degrees of Freedom (Df): 6.001
## Penalized Criterion (RSS): 25.63
## GCV: 2.616</code></pre>

<p>Notice in Figure <a href="statistics.html#fig:splineapprox">6.41</a>, the parameter spar=0.30 generates a polynomial curve that closes matches one with the parameter df=6.873 and vice-versa. A <strong>linear regression</strong> happens if spar=1.00 or df=2.</p>
<p>Next, we also can use lambda like so:</p>

<div class="sourceCode" id="cb791"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb791-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y,  <span class="dt">lambda=</span> <span class="fl">0.0001300587</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, lambda = 0.0001300587)
## 
## Smoothing Parameter  spar= NA  lambda= 0.0001301
## Equivalent Degrees of Freedom (Df): 8.163
## Penalized Criterion (RSS): 18.58
## GCV: 2.653</code></pre>

<p>We can use a combination like so (spar and lambda are mutually exclusive):</p>

<div class="sourceCode" id="cb793"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb793-1" data-line-number="1"><span class="kw">smooth.spline</span>(x, y,  <span class="dt">df=</span><span class="fl">6.78</span>, <span class="dt">lambda=</span> <span class="fl">0.00013</span>)</a></code></pre></div>
<pre><code>## Call:
## smooth.spline(x = x, y = y, df = 6.78, lambda = 0.00013)
## 
## Smoothing Parameter  spar= NA  lambda= 0.00013
## Equivalent Degrees of Freedom (Df): 8.164
## Penalized Criterion (RSS): 18.58
## GCV: 2.653</code></pre>

<p>Note that any parameter adjustments affect the <strong>power of prediction</strong>. Our optimal choices should allow a more general model to fit a broader set of cases.</p>
</div>
<div id="loess-and-lowess" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.8.9</span> LOESS and LOWESS  <a href="statistics.html#loess-and-lowess" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Two common smoothers discussed in other literature are <strong>LOESS</strong> and <strong>LOWESS</strong>. We discussed the theory and math in Chapter <strong>3</strong> (<strong>Numerical Linear Algebra II</strong>) under the <strong>Polynomial Smoothing</strong> Section.</p>
<p>Here, we show a simple use of both smoothers in R code (this time, we once again use the built-in R function <strong>predict(.)</strong> for inference):</p>

<div class="sourceCode" id="cb795"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb795-1" data-line-number="1"><span class="kw">set.seed</span>(<span class="dv">2020</span>)</a>
<a class="sourceLine" id="cb795-2" data-line-number="2"><span class="co"># degree of polynomial is 2</span></a>
<a class="sourceLine" id="cb795-3" data-line-number="3">loess.model =<span class="st"> </span><span class="kw">loess</span>(y <span class="op">~</span><span class="st"> </span>x, <span class="dt">span=</span><span class="fl">0.70</span>, <span class="dt">degree=</span><span class="dv">2</span>) </a>
<a class="sourceLine" id="cb795-4" data-line-number="4">loess.line =<span class="st"> </span>stats<span class="op">::</span><span class="kw">predict</span>(loess.model)</a>
<a class="sourceLine" id="cb795-5" data-line-number="5"><span class="kw">plot</span>(<span class="ot">NULL</span>, <span class="dt">xlim=</span><span class="kw">range</span>(<span class="dv">1</span>,sample_size), <span class="dt">ylim=</span><span class="kw">range</span>(<span class="op">-</span><span class="dv">3</span>,<span class="dv">3</span>), </a>
<a class="sourceLine" id="cb795-6" data-line-number="6">     <span class="dt">xlab=</span><span class="st">&quot;x-axis&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;y-axis (approx)&quot;</span>,</a>
<a class="sourceLine" id="cb795-7" data-line-number="7">     <span class="dt">main=</span><span class="st">&quot;Fitting using LOWESS and LOESS&quot;</span>)</a>
<a class="sourceLine" id="cb795-8" data-line-number="8"><span class="kw">grid</span>(<span class="dt">lty=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="st">&quot;lightgrey&quot;</span>)</a>
<a class="sourceLine" id="cb795-9" data-line-number="9"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">0.20</span>), <span class="dt">col=</span><span class="st">&quot;navyblue&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-10" data-line-number="10"><span class="kw">lines</span>(<span class="kw">lowess</span>(x, y, <span class="dt">f=</span><span class="fl">1.70</span>), <span class="dt">col=</span><span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-11" data-line-number="11"><span class="kw">lines</span>(smooth1.line, <span class="dt">col=</span><span class="st">&quot;brown&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-12" data-line-number="12"><span class="kw">lines</span>(loess.line , <span class="dt">col=</span><span class="st">&quot;darksalmon&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>, <span class="dt">lty=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb795-13" data-line-number="13"><span class="kw">points</span>(x, y, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>, <span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb795-14" data-line-number="14"><span class="kw">legend</span>(<span class="dv">7</span>, <span class="dv">-1</span>, </a>
<a class="sourceLine" id="cb795-15" data-line-number="15">    <span class="dt">legend=</span><span class="kw">c</span>( <span class="st">&quot;lowess(f=0.20)&quot;</span>, <span class="st">&quot;lowess(f=1.70)&quot;</span>, </a>
<a class="sourceLine" id="cb795-16" data-line-number="16">              <span class="st">&quot;smooth1.line&quot;</span>,  <span class="st">&quot;loess.line(span=0.70)&quot;</span>),</a>
<a class="sourceLine" id="cb795-17" data-line-number="17">    <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;navyblue&quot;</span>, <span class="st">&quot;red&quot;</span>, <span class="st">&quot;brown&quot;</span>, <span class="st">&quot;darksalmon&quot;</span>), </a>
<a class="sourceLine" id="cb795-18" data-line-number="18">    <span class="dt">lty=</span><span class="dv">1</span>,  <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:loessapprox"></span>
<img src="DS_files/figure-html/loessapprox-1.png" alt="Fitting using LOWESS and LOESS" width="80%" />
<p class="caption">
Figure 6.42: Fitting using LOWESS and LOESS
</p>
</div>

<p>We use the <strong>loess(.)</strong> function to model a fit for our dataset. We then use the generated model to <strong>predict</strong> a new set of data (equivalently performing interpolation) which we then use to plot a <strong>smooth curve</strong>.</p>
<p>In the previous discussion, spline1.model is chosen to be the better smoothing model to fit our data. Now, in Figure <a href="statistics.html#fig:loessapprox">6.42</a>, we compare spline1.model with loess.model to show how both are trying to fit data. Even though it may show that spline1.model is still a better model than loess.model; we can still tune loess.model to get a good fit. We leave readers to experiment and perhaps tune the smoothing span (span) parameter.</p>
<p>On the other hand, a lowess model with f=0.70 renders a line and is slightly smooth; but still able to fit our data. We can increase the parameter, but while it may not <strong>regress</strong> ideally, it nevertheless tries to do so, risking underfitting.</p>
<p>Also, the span parameter in <strong>loess(.)</strong> function settles into a <strong>linear regression</strong> similar to a higher <strong>f</strong> parameter in <strong>lowess(.)</strong> function. Otherwise, if tuned to ideal lower values, both parameters can fit a polynomial model.</p>
</div>
</div>
<div id="inference-for-regression" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.9</span> Inference for Regression<a href="statistics.html#inference-for-regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><strong>Prediction</strong> and <strong>Inference</strong> require a fitted model. Here, the better the fit, the better the Prediction or Inference.</p>
<div id="goodness-of-fit-linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.9.1</span> Goodness of Fit (Linear Regression) <a href="statistics.html#goodness-of-fit-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The quality of a good fit depends on specific criteria based on the metrics used.</p>
<p>The assumption is that our linear model fits data that follows a <strong>Normal distribution</strong>. Evaluating <strong>goodness of fit</strong> for linear regression is based on two common metrics (out of many others).</p>
<p><strong>Coefficient of Determinant</strong> <span class="math inline">\(\mathbf{(R^2)}\)</span><strong>:</strong> </p>
<p><strong>Coefficient of Determination</strong> measures the proportion of the expected outcome over the observed outcome and is expressed as:</p>
<p><span class="math display">\[\begin{align}
R^2 = \frac{\sum_{i=1}^{n}(\hat{y}_{i} - \bar{y})^2}{\sum_{i=1}^{n}(y_{i} - \bar{y})^2} = \frac{|\hat{y}_{i} - \bar{y}|^2}{|y_{i} - \bar{y}|^2}
\end{align}\]</span></p>
<p>Because this is a proportion of both outcomes, a value close to 1 means that the expected outcome is very close to the observed outcome. That means that the model has a higher power of Prediction closer to one and a lower power of Prediction closer to zero.</p>
<p>Another variation of this equation is as follows:</p>
<p><span class="math display">\[\begin{align}
R^2 = \frac{VAR_{(explained)}}{VAR_{(tot)}} 
= \frac{\text{Explained Sum of Squares}}{\text{Total Sum of Squares}} 
= \frac{ESS}{TSS} = 1 - \frac{RSS}{TSS}
\end{align}\]</span></p>
<p><strong>Root Mean Square Error (RMSE):</strong>  </p>
<p><strong>RMSE</strong> measures the degree of error between the expected and observed outcomes. We calculate the degree of error as the standard deviation of the error (or residual).</p>
<p><span class="math display">\[\begin{align}
RMSE = \sqrt{\frac{1}{n}\sum_{i=1}^n\left(\hat{y}_i - y_i\right)}
\end{align}\]</span></p>
<p>Perhaps, we can think of this in analogy to <strong>standard normal distribution</strong> with spread (or variance) in which the variance is transformed into a square root, effectively becoming a measure of standard deviation. Similarly, <strong>RMSE</strong> transforms into a square root of <strong>MSE</strong>. In other words, <strong>RMSE</strong> is the square root of the variance of residuals. And it measures regression - or the closeness of distance from the data point to the fitted line.</p>
<p>The equation is better understood if expressed this way:</p>
<p><span class="math display">\[\begin{align}
RMSE = \sqrt{MSE}
\end{align}\]</span></p>
<p>where <strong>MSE</strong> represents the variance of the residuals and is expressed as:</p>
<p><span class="math display">\[\begin{align}
MSE = \frac{1}{n}\sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2
\end{align}\]</span></p>
<p>From that line of thinking, we can say that a residual with one unit of standard deviation (closer to zero - or closer to the mean or the typical value) indicates higher power of prediction; otherwise, a higher standard deviation means that the model has lower power of prediction because the expected outcome is moving away from the mean (typical value).</p>
<p>We revisit <strong>MSE</strong> in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under <strong>Regularization</strong> Section as an <strong>L2-loss</strong> regularization technique.</p>
</div>
<div id="goodness-of-fit-non-linear-regression" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.9.2</span> Goodness of Fit (Non-Linear Regression) <a href="statistics.html#goodness-of-fit-non-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We start with the premise that the following equation is true:</p>
<p><span class="math display">\[\begin{align}
y = \beta^TX\ + \epsilon\ \ \ \ \ \ \ where\ \epsilon \sim \mathcal{N}(0, \sigma^2I)
\end{align}\]</span></p>
<p>The <strong>noise</strong> denoted by the epsilon (<span class="math inline">\(\epsilon\)</span>) symbol describes either a <strong>normal distribution</strong> for <strong>linear regression</strong> or an <strong>exponential distribution</strong> for <strong>non-linear regression</strong>. The estimators for <span class="math inline">\(\hat{\beta}\)</span> and <span class="math inline">\(\hat{\sigma}^2\)</span> generated by <strong>optimization</strong> play a role in how our fit <span class="math inline">\(\hat{y}\)</span> regresses towards <span class="math inline">\(y\)</span>.</p>
<p>For <strong>Non-Linear Regression</strong> such as <strong>Logistic and Poisson Regressions</strong>, we assume that the non-linear regression model fits data that follows an <strong>Exponential distribution</strong> such as <strong>Binomial or Poisson distribution</strong>. Evaluating <strong>Goodness of Fit</strong> for non-linear regression can be measured using three related tests <span class="citation">(R. F Engle <a href="bibliography.html#ref-ref2149r">1984</a>)</span> because of their association with the <strong>Chi-square</strong> distribution. See Figure <a href="statistics.html#fig:likelihoodratio">6.43</a>.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:likelihoodratio"></span>
<img src="likelihoodratio.png" alt="Goodness of Fit" width="80%" />
<p class="caption">
Figure 6.43: Goodness of Fit
</p>
</div>
<p>Figure <a href="statistics.html#fig:likelihoodratio">6.43</a> illustrates two models. The first model is characterized by its parameter, namely <span class="math inline">\(\hat{\theta}\)</span>. This model is also called the <strong>unrestrictive/unconstrained</strong> model or the <strong>full</strong> (<span class="math inline">\(H_1\)</span>) model. The second model is characterized by its parameter, namely <span class="math inline">\(\theta_0\)</span>. This model is also called the <strong>restrictive/constrained</strong> model or the <strong>null</strong> (<span class="math inline">\(H_0\)</span>) model.</p>
<p>We can obtain the <span class="math inline">\(\hat{\theta}\)</span> model using <strong>MLE</strong> procedures discussed in Chapter <strong>7</strong> (<strong>Bayesian Computation I</strong>) under the <strong>Bayesian Inference</strong> Section so that the <strong>MLE</strong> for a <strong>binomial distribution</strong> as an example is obtained based on the derived equation for <strong>MLE</strong>:</p>
<p><span class="math display">\[\begin{align}
\hat{\theta}_{(MLE)} = \frac{x}{n}\ \ \ \text{for point-estimate}.
\end{align}\]</span></p>
<p>As always, our hypothesis consists of the <strong>null</strong> hypothesis and its <strong>alternate</strong> hypothesis:</p>
<p><span class="math display">\[\begin{align}
H_0: \theta = \theta_0\ \ \ \ \ \ \ \ \ \ H_1: \theta \ne \theta_0
\end{align}\]</span></p>
<p>Here, we resort to a <strong>null</strong> distribution to represent <span class="math inline">\(\theta_0\)</span>, which we then compare against our sample distribution denoted by <span class="math inline">\(\hat{\theta}\)</span>. If the <strong>null</strong> distribution is not consistent with - or if it is far from - our sample distribution, e.g., <span class="math inline">\(\theta_0 \ne \hat{\theta}\)</span>, then it is likely that we reject <span class="math inline">\(H_0\)</span> based on a statistic, e.g., using the p-value. To measure how far <span class="math inline">\(\theta_0\)</span> is from <span class="math inline">\(\hat{\theta}\)</span>, we use any of the three tests below, starting with <strong>likelihood ratio</strong>.</p>
<p><strong>Likelihood Ratio Test:</strong> </p>
<p>The <strong>LR test</strong> compares the fit of a <strong>full</strong> model (<span class="math inline">\(\hat{\theta}\)</span>) to the fit of a <strong>null</strong> model (<span class="math inline">\(\theta_0\)</span>). The test is expressed as such:</p>
<p><span class="math display">\[\begin{align}
LR(x) = -2 \log_e \left(\frac{\mathcal{L}(\theta_0)}{\mathcal{L}(\hat{\theta})}\right) = 
2\left[\log_e \mathcal{L}(\hat{\theta}) - \log_e \mathcal{L}(\theta_{0})\right]\ \sim\ \ \mathcal{X}^2(\nu)
\end{align}\]</span></p>
<p>The test result follows a <strong>Chi-square distribution</strong> constrained by <span class="math inline">\(\nu\)</span> degrees of freedom. We take the result and find the p-value from a p-value table and evaluate if the <strong>full</strong> model is statistically significant, meaning it fits significantly better a model than the <strong>null</strong> model, e.g. (p &lt; 0.01). If the <strong>full</strong> model moves away from the <strong>null model</strong>, we reject the <strong>null</strong> hypothesis.</p>
<p><strong>Wald Test:</strong> </p>
<p><strong>Wald Test</strong> performs an evaluation against the <strong>unrestrictive</strong> model, namely (<span class="math inline">\(\hat{\theta}\)</span>). It is expressed as such:</p>
<p><span class="math display">\[\begin{align}
WT = \frac{(\hat{\theta} - \theta_{0})^2}{var(\hat{\theta})}\ \sim\ \ \mathcal{X}^2(\nu)
\end{align}\]</span></p>
<p>The denominator in the equation indicates <strong>variance</strong> of the <strong>full model</strong> which requires <span class="math inline">\(\sigma^2\)</span>. Note that <span class="math inline">\(\sigma^2\)</span> is usually unknown in which case, we can estimate the statistic given the following:</p>
<p><span class="math display">\[\begin{align}
var(\hat{\theta}) =  \underbrace{I(\hat{\theta})^{-1}}_{\text{Fisher Information}}
= - \mathbb{E}[\mathcal{L}&#39;&#39;(\hat{\theta})]^{-1}
\ \ \ \ \ \ where:
\underbrace{\mathcal{L}&#39;&#39;(\hat{\theta}) =
\left(\frac{\partial^2\mathcal{L}}{\partial\hat{\theta}^2}\right)}_{
\text{Observed Fisher Information}
}
\end{align}\]</span></p>
<p>Secondly, knowing that <strong>standard error (SE)</strong> is the square root of <strong>variance</strong>, then we have:</p>
<p><span class="math display">\[\begin{align}
\sqrt{var(\hat{\theta})} = SE(\hat{\theta}) = \sqrt{I(\hat{\theta})^{-1}}
\end{align}\]</span></p>
<p>Therefore, we get:</p>
<p><span class="math display">\[\begin{align}
WT = \frac{(\hat{\theta} - \theta_{0})^2}{I(\hat{\theta})^{-1}}\ \sim\ \ \mathcal{X}^2(\nu)
\end{align}\]</span></p>
<p>Similarly, the test result follows a <strong>Chi-square distribution</strong> constrained by <span class="math inline">\(\nu\)</span> degrees of freedom. We take the result and find the corresponding p-value for analysis.</p>
<p>We leave readers to investigate <strong>Cramer Rao Lower Bound</strong> relevant to <strong>Fisher Information</strong>.</p>
<p><strong>Score Test (Lagrange Multiplier Test):</strong>  </p>
<p><strong>Score Test</strong> evaluates the <strong>restrictive</strong> model, namely (<span class="math inline">\(\theta_0\)</span>). That is because the <strong>slope</strong> of the <strong>unrestrictive</strong> model tends toward zero - at the top of the curve. The test formula is expressed as such:</p>
<p><span class="math display">\[\begin{align}
LM = \frac{S(\theta_{0})^2}{I\left(\theta_{0}\right)}\ \sim\ \ \mathcal{X}^2(\nu)
\end{align}\]</span></p>
<p>where <strong>S</strong> describes the <strong>score equation</strong> and <strong>I</strong> describes the <strong>Fisher Information</strong>: </p>
<p><span class="math display">\[\begin{align}
S(\theta_0) = \underbrace{\mathcal{L}&#39;(\theta_0) = 
\frac{\partial \mathcal{L}}{\partial \theta_0}}_{\text{score equation}}
\ \ \ \ \ \ \ \ \ 
I(\theta_0) = var(\theta_0)
\end{align}\]</span></p>
<p>Just as the same, the test result follows a <strong>Chi-square distribution</strong> constrained by <span class="math inline">\(\nu\)</span> degrees of freedom. We then analyze its corresponding p-value.</p>
<p>The paper from Daniel F. Kohler <span class="citation">(<a href="bibliography.html#ref-ref1015d">1982</a>)</span> explains the following simplified equation based on the assumptions of a linear model and certain constraints:</p>
<p><span class="math display">\[
LR = n \log_e \hat{\theta}\ \ \ \ \ \ \ \ \
WT = n ( \hat{\theta}  - 1)\ \ \ \ \ \ \ \ \
LM = n (1 - 1/\hat{\theta})
\]</span></p>
<p>From which, we get the known inequality relationship:</p>
<p><span class="math display">\[
LM \le LR \le WT 
\]</span></p>
<p>Other <strong>tests</strong> to consider are <strong>Deviance test</strong>, <strong>Pearsonâs chi-square test</strong>, <strong>Hosmer-Lemeshow test</strong>. These tests are introduced further in Chapter <strong>10</strong> (<strong>Computational Learning II</strong>) under <strong>Logistic Regression</strong> Section.</p>
</div>
<div id="confidence-interval" class="section level3 hasAnchor">
<h3><span class="header-section-number">6.9.3</span> Confidence interval <a href="statistics.html#confidence-interval" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>In cases where we are not confident about our <strong>point estimates</strong>, it may help to lay out a range of values within which we may fairly be sure that the actual value lies within such range. Then, in <strong>predicting</strong> for the value, we can use a confidence level, e.g., 95%, so that our average fit falls within <span class="math inline">\(\pm\)</span> (plus-minus) of our confidence interval.</p>

<div class="sourceCode" id="cb796"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb796-1" data-line-number="1">simple.model =<span class="st"> </span><span class="kw">lm</span>(mpg <span class="op">~</span><span class="st"> </span>disp, <span class="dt">data =</span> mtcars)</a>
<a class="sourceLine" id="cb796-2" data-line-number="2">new_x =<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">disp =</span> <span class="kw">c</span>(<span class="dv">301</span>, <span class="dv">302</span>))</a>
<a class="sourceLine" id="cb796-3" data-line-number="3"><span class="kw">predict.lm</span>(<span class="dt">object =</span> simple.model, <span class="dt">newdata =</span> new_x, </a>
<a class="sourceLine" id="cb796-4" data-line-number="4">            <span class="dt">interval=</span><span class="kw">c</span>(<span class="st">&quot;confidence&quot;</span>), <span class="dt">level=</span><span class="fl">0.95</span>)</a></code></pre></div>
<pre><code>##     fit   lwr   upr
## 1 17.19 15.84 18.55
## 2 17.15 15.79 18.51</code></pre>

<p>We can then readily provide our interpretation with 95% confidence that our findings fall within the average range between a lower and upper boundary.</p>
<p>In <strong>predicting</strong> for the value, we also can use a prediction interval. However, the lower and upper bound applies only to the value of our observation and not the average (or expectation) of our observation. See below:</p>

<div class="sourceCode" id="cb798"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb798-1" data-line-number="1"><span class="kw">predict.lm</span>(<span class="dt">object =</span> simple.model, <span class="dt">newdata =</span> new_x, </a>
<a class="sourceLine" id="cb798-2" data-line-number="2">           <span class="dt">interval=</span><span class="kw">c</span>(<span class="st">&quot;prediction&quot;</span>))</a></code></pre></div>
<pre><code>##     fit   lwr   upr
## 1 17.19 10.42 23.97
## 2 17.15 10.37 23.93</code></pre>

</div>
</div>
<div id="summary-4" class="section level2 hasAnchor">
<h2><span class="header-section-number">6.10</span> Summary<a href="statistics.html#summary-4" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This chapter builds intuition around Statistical Analysis and the required math. However, we have yet to describe what a statistical modeling is all about. If we search for a meaningful yet straightforward description, we can go with the description from Stobierski T. <span class="citation">(<a href="bibliography.html#ref-ref115t">2019</a>)</span>, which also follows a similar description from Wikipedia. We should note that a typical description of Statistical modeling is about âapplying statistical analysisâ (or applying the mathematical model) to observed data. There is the math, and there is the data. Below, we have the most simple regression model, and all we need to capture is the most fitting value as a model weight for our parameter (<span class="math inline">\(\theta\)</span>).</p>
<p><span class="math display">\[
\hat{y} = \theta x + \epsilon
\]</span></p>
<p>We then use the model for inference.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="numericalprobability.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="bayesian.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
