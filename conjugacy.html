<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.4 Conjugacy | The Power and Art of Approximation</title>
  <meta name="description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  <meta name="generator" content="bookdown 0.32 and GitBook 2.6.7" />

  <meta property="og:title" content="3.4 Conjugacy | The Power and Art of Approximation" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.4 Conjugacy | The Power and Art of Approximation" />
  
  <meta name="twitter:description" content="Enthused by the promising future of self-learning machines and the continuous advancement of technology, we write this book to cover a compendium of analytical and numerical techniques conflated into a common idea that highlights the fundamental requirements of Data Science and Machine Learning (ML) Engineering. In this book, we review and give brief insights into numerous fundamental ideas around methods of approximation conceived by great experts. We aim to share them with those new to Data Science who are just beginning to develop an inclination toward this field but may not know where to begin. In addition, we hope to introduce some essential aspects of Data Science in a more progressive and possibly structured manner. This book avoids being specific to a target audience depending on interest. The premise is that Data Science can be for everybody, whether one is an engineer, a researcher within a particular domain, or, for that matter, an undergraduate student just trying to get into this field. While we note that our common theme across the book is intuition, contemplating more on basic operations than mathematical rigor, it is essential to revive our understanding of mathematical concepts first. That is founded upon the idea that we express most of what we do in Data Science in the language of mathematics, more numerically inclined in fact than analytical - meaning, we live to decide based on close approximation in many situations. Therefore, it is just right to have a historical perspective of the mathematical foundations which Machine Learning algorithms may have come about - if not at least what they depend upon fundamentally. For that reason, we cover a list of mathematical concepts that are no doubt valuable to eventually get us to Machine Learning concepts. However, only a particular elementary and introductory portion of each field of mathematics is covered as we emphasize only relevant and essential areas. That said, this book comes in three volumes. Volumes I and II of this book briefly cover common topics in Linear Algebra, Numerical Analysis, Statistical Analysis, and Bayesian Analysis. The third part (or volume III) of this book covers Machine Learning and Deep Learning in detail." />
  

<meta name="author" content="Raymond Michael Ofiaza OrdoÃ±a" />


<meta name="date" content="2023-02-12" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bayes-theorem.html"/>
<link rel="next" href="information-theory.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">The Power and Art of Approximation</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="0.1" data-path="acknowledgment-and-motivations.html"><a href="acknowledgment-and-motivations.html"><i class="fa fa-check"></i><b>0.1</b> Acknowledgment and Motivations</a></li>
<li class="chapter" data-level="0.2" data-path="caveat.html"><a href="caveat.html"><i class="fa fa-check"></i><b>0.2</b> Caveat</a></li>
<li class="chapter" data-level="0.3" data-path="about-the-author.html"><a href="about-the-author.html"><i class="fa fa-check"></i><b>0.3</b> About the Author</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i>Introduction</a></li>
<li class="chapter" data-level="" data-path="mathematical-notation.html"><a href="mathematical-notation.html"><i class="fa fa-check"></i>Mathematical Notation</a><ul>
<li class="chapter" data-level="0.4" data-path="notation.html"><a href="notation.html"><i class="fa fa-check"></i><b>0.4</b> Notation</a></li>
<li class="chapter" data-level="0.5" data-path="number-system.html"><a href="number-system.html"><i class="fa fa-check"></i><b>0.5</b> Number System</a></li>
<li class="chapter" data-level="0.6" data-path="implementation.html"><a href="implementation.html"><i class="fa fa-check"></i><b>0.6</b> Implementation</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="numericalprobability.html"><a href="numericalprobability.html"><i class="fa fa-check"></i><b>1</b> Probability and Distribution</a><ul>
<li class="chapter" data-level="1.1" data-path="approximation-based-on-random-chances.html"><a href="approximation-based-on-random-chances.html"><i class="fa fa-check"></i><b>1.1</b> Approximation based on Random Chances </a></li>
<li class="chapter" data-level="1.2" data-path="distribution.html"><a href="distribution.html"><i class="fa fa-check"></i><b>1.2</b> Distribution</a></li>
<li class="chapter" data-level="1.3" data-path="mass-and-density.html"><a href="mass-and-density.html"><i class="fa fa-check"></i><b>1.3</b> Mass and Density  </a></li>
<li class="chapter" data-level="1.4" data-path="probability.html"><a href="probability.html"><i class="fa fa-check"></i><b>1.4</b> Probability  </a></li>
<li class="chapter" data-level="1.5" data-path="probability-density-function-pdf.html"><a href="probability-density-function-pdf.html"><i class="fa fa-check"></i><b>1.5</b> Probability Density Function (PDF)  </a></li>
<li class="chapter" data-level="1.6" data-path="probability-mass-function-pmf.html"><a href="probability-mass-function-pmf.html"><i class="fa fa-check"></i><b>1.6</b> Probability Mass function (PMF)  </a></li>
<li class="chapter" data-level="1.7" data-path="cumulative-distribution-function-cdf.html"><a href="cumulative-distribution-function-cdf.html"><i class="fa fa-check"></i><b>1.7</b> Cumulative Distribution Function (CDF)  </a></li>
<li class="chapter" data-level="1.8" data-path="special-functions.html"><a href="special-functions.html"><i class="fa fa-check"></i><b>1.8</b> Special Functions</a><ul>
<li class="chapter" data-level="1.8.1" data-path="special-functions.html"><a href="special-functions.html#gamma-function"><i class="fa fa-check"></i><b>1.8.1</b> Gamma function </a></li>
<li class="chapter" data-level="1.8.2" data-path="special-functions.html"><a href="special-functions.html#incomplete-gamma-function"><i class="fa fa-check"></i><b>1.8.2</b> Incomplete Gamma function </a></li>
<li class="chapter" data-level="1.8.3" data-path="special-functions.html"><a href="special-functions.html#digamma-function"><i class="fa fa-check"></i><b>1.8.3</b> Digamma Function </a></li>
<li class="chapter" data-level="1.8.4" data-path="special-functions.html"><a href="special-functions.html#beta-function"><i class="fa fa-check"></i><b>1.8.4</b> Beta function </a></li>
<li class="chapter" data-level="1.8.5" data-path="special-functions.html"><a href="special-functions.html#incomplete-beta-function"><i class="fa fa-check"></i><b>1.8.5</b> Incomplete Beta function </a></li>
<li class="chapter" data-level="1.8.6" data-path="special-functions.html"><a href="special-functions.html#regularized-beta-function"><i class="fa fa-check"></i><b>1.8.6</b> Regularized Beta function  </a></li>
<li class="chapter" data-level="1.8.7" data-path="special-functions.html"><a href="special-functions.html#hypergeometric-function"><i class="fa fa-check"></i><b>1.8.7</b> Hypergeometric function </a></li>
<li class="chapter" data-level="1.8.8" data-path="special-functions.html"><a href="special-functions.html#continued-fraction"><i class="fa fa-check"></i><b>1.8.8</b> Continued Fraction </a></li>
<li class="chapter" data-level="1.8.9" data-path="special-functions.html"><a href="special-functions.html#dirac-delta-function"><i class="fa fa-check"></i><b>1.8.9</b> Dirac Delta Function </a></li>
<li class="chapter" data-level="1.8.10" data-path="special-functions.html"><a href="special-functions.html#kronecker-delta-function"><i class="fa fa-check"></i><b>1.8.10</b> Kronecker Delta Function </a></li>
</ul></li>
<li class="chapter" data-level="1.9" data-path="distributiontypes.html"><a href="distributiontypes.html"><i class="fa fa-check"></i><b>1.9</b> Types of Distribution</a><ul>
<li class="chapter" data-level="1.9.1" data-path="distributiontypes.html"><a href="distributiontypes.html#bernoulli-distribution"><i class="fa fa-check"></i><b>1.9.1</b> Bernoulli distribution </a></li>
<li class="chapter" data-level="1.9.2" data-path="distributiontypes.html"><a href="distributiontypes.html#binomial-distribution"><i class="fa fa-check"></i><b>1.9.2</b> Binomial distribution </a></li>
<li class="chapter" data-level="1.9.3" data-path="distributiontypes.html"><a href="distributiontypes.html#multinomial-distribution"><i class="fa fa-check"></i><b>1.9.3</b> Multinomial distribution </a></li>
<li class="chapter" data-level="1.9.4" data-path="distributiontypes.html"><a href="distributiontypes.html#geometric-distribution"><i class="fa fa-check"></i><b>1.9.4</b> Geometric distribution </a></li>
<li class="chapter" data-level="1.9.5" data-path="distributiontypes.html"><a href="distributiontypes.html#beta-distribution"><i class="fa fa-check"></i><b>1.9.5</b> Beta distribution </a></li>
<li class="chapter" data-level="1.9.6" data-path="distributiontypes.html"><a href="distributiontypes.html#dirichlet-distribution"><i class="fa fa-check"></i><b>1.9.6</b> Dirichlet distribution </a></li>
<li class="chapter" data-level="1.9.7" data-path="distributiontypes.html"><a href="distributiontypes.html#exponential-distribution"><i class="fa fa-check"></i><b>1.9.7</b> Exponential distribution </a></li>
<li class="chapter" data-level="1.9.8" data-path="distributiontypes.html"><a href="distributiontypes.html#gamma-distribution"><i class="fa fa-check"></i><b>1.9.8</b> Gamma distribution </a></li>
<li class="chapter" data-level="1.9.9" data-path="distributiontypes.html"><a href="distributiontypes.html#inverse-gamma-distribution"><i class="fa fa-check"></i><b>1.9.9</b> Inverse Gamma distribution </a></li>
<li class="chapter" data-level="1.9.10" data-path="distributiontypes.html"><a href="distributiontypes.html#weibull-distribution"><i class="fa fa-check"></i><b>1.9.10</b> Weibull distribution </a></li>
<li class="chapter" data-level="1.9.11" data-path="distributiontypes.html"><a href="distributiontypes.html#poisson-distribution"><i class="fa fa-check"></i><b>1.9.11</b> Poisson distribution </a></li>
<li class="chapter" data-level="1.9.12" data-path="distributiontypes.html"><a href="distributiontypes.html#pareto-distribution"><i class="fa fa-check"></i><b>1.9.12</b> Pareto distribution </a></li>
<li class="chapter" data-level="1.9.13" data-path="distributiontypes.html"><a href="distributiontypes.html#normal-distribution"><i class="fa fa-check"></i><b>1.9.13</b> Normal distribution </a></li>
<li class="chapter" data-level="1.9.14" data-path="distributiontypes.html"><a href="distributiontypes.html#wald-distribution"><i class="fa fa-check"></i><b>1.9.14</b> Wald Distribution </a></li>
<li class="chapter" data-level="1.9.15" data-path="distributiontypes.html"><a href="distributiontypes.html#log-normal-distribution"><i class="fa fa-check"></i><b>1.9.15</b> Log-normal Distribution </a></li>
<li class="chapter" data-level="1.9.16" data-path="distributiontypes.html"><a href="distributiontypes.html#uniform-distribution"><i class="fa fa-check"></i><b>1.9.16</b> Uniform Distribution </a></li>
<li class="chapter" data-level="1.9.17" data-path="distributiontypes.html"><a href="distributiontypes.html#t-distribution"><i class="fa fa-check"></i><b>1.9.17</b> T-Distribution </a></li>
<li class="chapter" data-level="1.9.18" data-path="distributiontypes.html"><a href="distributiontypes.html#f-distribution"><i class="fa fa-check"></i><b>1.9.18</b> F-Distribution </a></li>
<li class="chapter" data-level="1.9.19" data-path="distributiontypes.html"><a href="distributiontypes.html#chi-square-distribution"><i class="fa fa-check"></i><b>1.9.19</b> Chi-square Distribution </a></li>
<li class="chapter" data-level="1.9.20" data-path="distributiontypes.html"><a href="distributiontypes.html#wishartdistribution"><i class="fa fa-check"></i><b>1.9.20</b> Wishart distribution</a></li>
<li class="chapter" data-level="1.9.21" data-path="distributiontypes.html"><a href="distributiontypes.html#lkj-distribution"><i class="fa fa-check"></i><b>1.9.21</b> LKJ distribution </a></li>
<li class="chapter" data-level="1.9.22" data-path="distributiontypes.html"><a href="distributiontypes.html#mixture-distribution"><i class="fa fa-check"></i><b>1.9.22</b> Mixture distribution </a></li>
<li class="chapter" data-level="1.9.23" data-path="distributiontypes.html"><a href="distributiontypes.html#non-parametric-distribution"><i class="fa fa-check"></i><b>1.9.23</b> Non-parametric distribution </a></li>
<li class="chapter" data-level="1.9.24" data-path="distributiontypes.html"><a href="distributiontypes.html#multi-dimensional-density"><i class="fa fa-check"></i><b>1.9.24</b> Multi-dimensional Density </a></li>
</ul></li>
<li class="chapter" data-level="1.10" data-path="summary.html"><a href="summary.html"><i class="fa fa-check"></i><b>1.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="statistics.html"><a href="statistics.html"><i class="fa fa-check"></i><b>2</b> Statistical Computation</a><ul>
<li class="chapter" data-level="2.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html"><i class="fa fa-check"></i><b>2.1</b> Descriptive Statistics</a><ul>
<li class="chapter" data-level="2.1.1" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#visual-representation"><i class="fa fa-check"></i><b>2.1.1</b> Visual Representation</a></li>
<li class="chapter" data-level="2.1.2" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#central-tendency"><i class="fa fa-check"></i><b>2.1.2</b> Central Tendency </a></li>
<li class="chapter" data-level="2.1.3" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#variability"><i class="fa fa-check"></i><b>2.1.3</b> Variability </a></li>
<li class="chapter" data-level="2.1.4" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#kurtosis-and-skewness"><i class="fa fa-check"></i><b>2.1.4</b> Kurtosis and Skewness  </a></li>
<li class="chapter" data-level="2.1.5" data-path="descriptive-statistics.html"><a href="descriptive-statistics.html#five-number-summary"><i class="fa fa-check"></i><b>2.1.5</b> Five Number Summary  </a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="inferential-statistics.html"><a href="inferential-statistics.html"><i class="fa fa-check"></i><b>2.2</b> Inferential Statistics</a></li>
<li class="chapter" data-level="2.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html"><i class="fa fa-check"></i><b>2.3</b> The Significance of Difference </a><ul>
<li class="chapter" data-level="2.3.1" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#hypothesis"><i class="fa fa-check"></i><b>2.3.1</b> Hypothesis</a></li>
<li class="chapter" data-level="2.3.2" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#t-test-true-variance-unknown"><i class="fa fa-check"></i><b>2.3.2</b> T-Test (True Variance unknown) </a></li>
<li class="chapter" data-level="2.3.3" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#z-test-true-variance-known"><i class="fa fa-check"></i><b>2.3.3</b> Z-Test (True Variance known)</a></li>
<li class="chapter" data-level="2.3.4" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-using-f-ratio"><i class="fa fa-check"></i><b>2.3.4</b> F-Test using F-ratio  </a></li>
<li class="chapter" data-level="2.3.5" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-one-way-anova"><i class="fa fa-check"></i><b>2.3.5</b> F-Test with One-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.6" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#f-test-with-two-way-anova"><i class="fa fa-check"></i><b>2.3.6</b> F-Test with Two-Way ANOVA </a></li>
<li class="chapter" data-level="2.3.7" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#pearsons-chi-square-test"><i class="fa fa-check"></i><b>2.3.7</b> Pearsonâs Chi-square Test </a></li>
<li class="chapter" data-level="2.3.8" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#wilcoxon-test"><i class="fa fa-check"></i><b>2.3.8</b> Wilcoxon Test  </a></li>
<li class="chapter" data-level="2.3.9" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#kruskal-wallis-test"><i class="fa fa-check"></i><b>2.3.9</b> Kruskal-Wallis Test </a></li>
<li class="chapter" data-level="2.3.10" data-path="the-significance-of-difference.html"><a href="the-significance-of-difference.html#friedman-test"><i class="fa fa-check"></i><b>2.3.10</b> Friedman Test </a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html"><i class="fa fa-check"></i><b>2.4</b> Post-HOC Analysis </a><ul>
<li class="chapter" data-level="2.4.1" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#bonferroni-correction"><i class="fa fa-check"></i><b>2.4.1</b> Bonferroni Correction </a></li>
<li class="chapter" data-level="2.4.2" data-path="post-hoc-analysis.html"><a href="post-hoc-analysis.html#benjamini-hochberg-correction"><i class="fa fa-check"></i><b>2.4.2</b> Benjamini-Hochberg Correction </a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html"><i class="fa fa-check"></i><b>2.5</b> Multiple Comparison Tests </a><ul>
<li class="chapter" data-level="2.5.1" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#scheffes-test"><i class="fa fa-check"></i><b>2.5.1</b> Scheffeâs Test </a></li>
<li class="chapter" data-level="2.5.2" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#fishers-test"><i class="fa fa-check"></i><b>2.5.2</b> Fisherâs Test </a></li>
<li class="chapter" data-level="2.5.3" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#tukeys-test"><i class="fa fa-check"></i><b>2.5.3</b> Tukeyâs Test </a></li>
<li class="chapter" data-level="2.5.4" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#newman-keul-test"><i class="fa fa-check"></i><b>2.5.4</b> Newman-Keul Test  </a></li>
<li class="chapter" data-level="2.5.5" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#games-howell-test"><i class="fa fa-check"></i><b>2.5.5</b> Games-Howell Test </a></li>
<li class="chapter" data-level="2.5.6" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#dunnetts-test"><i class="fa fa-check"></i><b>2.5.6</b> Dunnettâs Test </a></li>
<li class="chapter" data-level="2.5.7" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#duncans-test"><i class="fa fa-check"></i><b>2.5.7</b> Duncanâs Test </a></li>
<li class="chapter" data-level="2.5.8" data-path="multiple-comparison-tests.html"><a href="multiple-comparison-tests.html#meta-analysis-test"><i class="fa fa-check"></i><b>2.5.8</b> Meta-Analysis Test </a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="statistical-modeling.html"><a href="statistical-modeling.html"><i class="fa fa-check"></i><b>2.6</b> Statistical Modeling </a><ul>
<li class="chapter" data-level="2.6.1" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-specification"><i class="fa fa-check"></i><b>2.6.1</b> Model Specification </a></li>
<li class="chapter" data-level="2.6.2" data-path="statistical-modeling.html"><a href="statistical-modeling.html#statistical-interaction"><i class="fa fa-check"></i><b>2.6.2</b> Statistical Interaction </a></li>
<li class="chapter" data-level="2.6.3" data-path="statistical-modeling.html"><a href="statistical-modeling.html#dummy-variables"><i class="fa fa-check"></i><b>2.6.3</b> Dummy Variables </a></li>
<li class="chapter" data-level="2.6.4" data-path="statistical-modeling.html"><a href="statistical-modeling.html#model-selection"><i class="fa fa-check"></i><b>2.6.4</b> Model Selection </a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="regression-analysis.html"><a href="regression-analysis.html"><i class="fa fa-check"></i><b>2.7</b> Regression Analysis </a><ul>
<li class="chapter" data-level="2.7.1" data-path="regression-analysis.html"><a href="regression-analysis.html#assumptions"><i class="fa fa-check"></i><b>2.7.1</b> Assumptions</a></li>
<li class="chapter" data-level="2.7.2" data-path="regression-analysis.html"><a href="regression-analysis.html#correlation-coefficients"><i class="fa fa-check"></i><b>2.7.2</b> Correlation Coefficients </a></li>
<li class="chapter" data-level="2.7.3" data-path="regression-analysis.html"><a href="regression-analysis.html#homoscedasticity-and-heteroscedasticity"><i class="fa fa-check"></i><b>2.7.3</b> Homoscedasticity and Heteroscedasticity  </a></li>
<li class="chapter" data-level="2.7.4" data-path="regression-analysis.html"><a href="regression-analysis.html#normality-and-leverage"><i class="fa fa-check"></i><b>2.7.4</b> Normality and Leverage  </a></li>
<li class="chapter" data-level="2.7.5" data-path="regression-analysis.html"><a href="regression-analysis.html#collinearity"><i class="fa fa-check"></i><b>2.7.5</b> Collinearity </a></li>
<li class="chapter" data-level="2.7.6" data-path="regression-analysis.html"><a href="regression-analysis.html#dispersion"><i class="fa fa-check"></i><b>2.7.6</b> Dispersion </a></li>
<li class="chapter" data-level="2.7.7" data-path="regression-analysis.html"><a href="regression-analysis.html#diagnostic-plots"><i class="fa fa-check"></i><b>2.7.7</b> Diagnostic Plots</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html"><i class="fa fa-check"></i><b>2.8</b> The Significance of Regression </a><ul>
<li class="chapter" data-level="2.8.1" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>2.8.1</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="2.8.2" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#multilinear-regression"><i class="fa fa-check"></i><b>2.8.2</b> Multilinear Regression </a></li>
<li class="chapter" data-level="2.8.3" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#logistic-regression"><i class="fa fa-check"></i><b>2.8.3</b> Logistic Regression </a></li>
<li class="chapter" data-level="2.8.4" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#poisson-regression"><i class="fa fa-check"></i><b>2.8.4</b> Poisson Regression </a></li>
<li class="chapter" data-level="2.8.5" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#cox-regression"><i class="fa fa-check"></i><b>2.8.5</b> Cox Regression </a></li>
<li class="chapter" data-level="2.8.6" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#polynomial-regression"><i class="fa fa-check"></i><b>2.8.6</b> Polynomial Regression </a></li>
<li class="chapter" data-level="2.8.7" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#b-splines-and-natural-splines"><i class="fa fa-check"></i><b>2.8.7</b> B-Splines and Natural Splines  </a></li>
<li class="chapter" data-level="2.8.8" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#spline-smoothing"><i class="fa fa-check"></i><b>2.8.8</b> Spline Smoothing </a></li>
<li class="chapter" data-level="2.8.9" data-path="the-significance-of-regression.html"><a href="the-significance-of-regression.html#loess-and-lowess"><i class="fa fa-check"></i><b>2.8.9</b> LOESS and LOWESS  </a></li>
</ul></li>
<li class="chapter" data-level="2.9" data-path="inference-for-regression.html"><a href="inference-for-regression.html"><i class="fa fa-check"></i><b>2.9</b> Inference for Regression</a><ul>
<li class="chapter" data-level="2.9.1" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-linear-regression"><i class="fa fa-check"></i><b>2.9.1</b> Goodness of Fit (Linear Regression) </a></li>
<li class="chapter" data-level="2.9.2" data-path="inference-for-regression.html"><a href="inference-for-regression.html#goodness-of-fit-non-linear-regression"><i class="fa fa-check"></i><b>2.9.2</b> Goodness of Fit (Non-Linear Regression) </a></li>
<li class="chapter" data-level="2.9.3" data-path="inference-for-regression.html"><a href="inference-for-regression.html#confidence-interval"><i class="fa fa-check"></i><b>2.9.3</b> Confidence interval </a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="summary-1.html"><a href="summary-1.html"><i class="fa fa-check"></i><b>2.10</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>3</b> Bayesian Computation I</a><ul>
<li class="chapter" data-level="3.1" data-path="probability-1.html"><a href="probability-1.html"><i class="fa fa-check"></i><b>3.1</b> Probability </a><ul>
<li class="chapter" data-level="3.1.1" data-path="probability-1.html"><a href="probability-1.html#marginal-probability"><i class="fa fa-check"></i><b>3.1.1</b> Marginal Probability </a></li>
<li class="chapter" data-level="3.1.2" data-path="probability-1.html"><a href="probability-1.html#joint-probability"><i class="fa fa-check"></i><b>3.1.2</b> Joint Probability </a></li>
<li class="chapter" data-level="3.1.3" data-path="probability-1.html"><a href="probability-1.html#conditional-probability"><i class="fa fa-check"></i><b>3.1.3</b> Conditional Probability </a></li>
<li class="chapter" data-level="3.1.4" data-path="probability-1.html"><a href="probability-1.html#negation-probability"><i class="fa fa-check"></i><b>3.1.4</b> Negation Probability </a></li>
<li class="chapter" data-level="3.1.5" data-path="probability-1.html"><a href="probability-1.html#combination-of-probabilities"><i class="fa fa-check"></i><b>3.1.5</b> Combination of Probabilities</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="probability-rules.html"><a href="probability-rules.html"><i class="fa fa-check"></i><b>3.2</b> Probability Rules</a><ul>
<li class="chapter" data-level="3.2.1" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-probability"><i class="fa fa-check"></i><b>3.2.1</b> Law of Total Probability</a></li>
<li class="chapter" data-level="3.2.2" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-expectation"><i class="fa fa-check"></i><b>3.2.2</b> Law of Total Expectation </a></li>
<li class="chapter" data-level="3.2.3" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-variance"><i class="fa fa-check"></i><b>3.2.3</b> Law of Total Variance </a></li>
<li class="chapter" data-level="3.2.4" data-path="probability-rules.html"><a href="probability-rules.html#law-of-total-covariance"><i class="fa fa-check"></i><b>3.2.4</b> Law of Total Covariance </a></li>
<li class="chapter" data-level="3.2.5" data-path="probability-rules.html"><a href="probability-rules.html#law-of-large-numbers"><i class="fa fa-check"></i><b>3.2.5</b> Law of Large Numbers </a></li>
<li class="chapter" data-level="3.2.6" data-path="probability-rules.html"><a href="probability-rules.html#central-limit-theorem"><i class="fa fa-check"></i><b>3.2.6</b> Central Limit Theorem </a></li>
<li class="chapter" data-level="3.2.7" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-independence"><i class="fa fa-check"></i><b>3.2.7</b> Rule of Independence </a></li>
<li class="chapter" data-level="3.2.8" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-exchangeability"><i class="fa fa-check"></i><b>3.2.8</b> Rule of Exchangeability </a></li>
<li class="chapter" data-level="3.2.9" data-path="probability-rules.html"><a href="probability-rules.html#rule-of-expectation-and-variance"><i class="fa fa-check"></i><b>3.2.9</b> Rule of Expectation and Variance</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html"><i class="fa fa-check"></i><b>3.3</b> Bayes Theorem </a><ul>
<li class="chapter" data-level="3.3.1" data-path="bayes-theorem.html"><a href="bayes-theorem.html#naÃ¯ve-bayes"><i class="fa fa-check"></i><b>3.3.1</b> NaÃ¯ve Bayes </a></li>
<li class="chapter" data-level="3.3.2" data-path="bayes-theorem.html"><a href="bayes-theorem.html#likelihood"><i class="fa fa-check"></i><b>3.3.2</b> Likelihood</a></li>
<li class="chapter" data-level="3.3.3" data-path="bayes-theorem.html"><a href="bayes-theorem.html#posterior-probability"><i class="fa fa-check"></i><b>3.3.3</b> Posterior Probability  </a></li>
<li class="chapter" data-level="3.3.4" data-path="bayes-theorem.html"><a href="bayes-theorem.html#prior-probability"><i class="fa fa-check"></i><b>3.3.4</b> Prior Probability  </a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="conjugacy.html"><a href="conjugacy.html"><i class="fa fa-check"></i><b>3.4</b> Conjugacy</a><ul>
<li class="chapter" data-level="3.4.1" data-path="conjugacy.html"><a href="conjugacy.html#precision"><i class="fa fa-check"></i><b>3.4.1</b> Precision </a></li>
<li class="chapter" data-level="3.4.2" data-path="conjugacy.html"><a href="conjugacy.html#conjugate-prior"><i class="fa fa-check"></i><b>3.4.2</b> Conjugate Prior </a></li>
<li class="chapter" data-level="3.4.3" data-path="conjugacy.html"><a href="conjugacy.html#normal-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.3</b> Normal-Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.4" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.4</b> Normal-Inverse Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.5" data-path="conjugacy.html"><a href="conjugacy.html#multivariate-normal-conjugacy"><i class="fa fa-check"></i><b>3.4.5</b> Multivariate Normal Conjugacy </a></li>
<li class="chapter" data-level="3.4.6" data-path="conjugacy.html"><a href="conjugacy.html#normal-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.6</b> Normal Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.7" data-path="conjugacy.html"><a href="conjugacy.html#normal-inverse-wishart-conjugacy"><i class="fa fa-check"></i><b>3.4.7</b> Normal-Inverse Wishart Conjugacy </a></li>
<li class="chapter" data-level="3.4.8" data-path="conjugacy.html"><a href="conjugacy.html#normal-lkj-conjugacy"><i class="fa fa-check"></i><b>3.4.8</b> Normal-LKJ Conjugacy </a></li>
<li class="chapter" data-level="3.4.9" data-path="conjugacy.html"><a href="conjugacy.html#binomial-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.9</b> Binomial-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.10" data-path="conjugacy.html"><a href="conjugacy.html#geometric-beta-conjugacy"><i class="fa fa-check"></i><b>3.4.10</b> Geometric-Beta Conjugacy </a></li>
<li class="chapter" data-level="3.4.11" data-path="conjugacy.html"><a href="conjugacy.html#poisson-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.11</b> Poisson-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.12" data-path="conjugacy.html"><a href="conjugacy.html#exponential-gamma-conjugacy"><i class="fa fa-check"></i><b>3.4.12</b> Exponential-Gamma Conjugacy </a></li>
<li class="chapter" data-level="3.4.13" data-path="conjugacy.html"><a href="conjugacy.html#multinomial-dirichlet-conjugacy"><i class="fa fa-check"></i><b>3.4.13</b> Multinomial-Dirichlet Conjugacy </a></li>
<li class="chapter" data-level="3.4.14" data-path="conjugacy.html"><a href="conjugacy.html#hyperparameters"><i class="fa fa-check"></i><b>3.4.14</b> Hyperparameters </a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="information-theory.html"><a href="information-theory.html"><i class="fa fa-check"></i><b>3.5</b> Information Theory </a><ul>
<li class="chapter" data-level="3.5.1" data-path="information-theory.html"><a href="information-theory.html#information"><i class="fa fa-check"></i><b>3.5.1</b> Information </a></li>
<li class="chapter" data-level="3.5.2" data-path="information-theory.html"><a href="information-theory.html#entropy"><i class="fa fa-check"></i><b>3.5.2</b> Entropy </a></li>
<li class="chapter" data-level="3.5.3" data-path="information-theory.html"><a href="information-theory.html#gini-index"><i class="fa fa-check"></i><b>3.5.3</b> Gini Index </a></li>
<li class="chapter" data-level="3.5.4" data-path="information-theory.html"><a href="information-theory.html#information-gain"><i class="fa fa-check"></i><b>3.5.4</b> Information Gain </a></li>
<li class="chapter" data-level="3.5.5" data-path="information-theory.html"><a href="information-theory.html#mutual-information"><i class="fa fa-check"></i><b>3.5.5</b> Mutual Information </a></li>
<li class="chapter" data-level="3.5.6" data-path="information-theory.html"><a href="information-theory.html#kullback-leibler-divergence"><i class="fa fa-check"></i><b>3.5.6</b> Kullback-Leibler Divergence  </a></li>
<li class="chapter" data-level="3.5.7" data-path="information-theory.html"><a href="information-theory.html#jensens-inequality"><i class="fa fa-check"></i><b>3.5.7</b> Jensenâs Inequality</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="bayesianinference.html"><a href="bayesianinference.html"><i class="fa fa-check"></i><b>3.6</b> Bayesian Inference</a><ul>
<li class="chapter" data-level="3.6.1" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-likelihood-mle"><i class="fa fa-check"></i><b>3.6.1</b> Maximum Likelihood (MLE)  </a></li>
<li class="chapter" data-level="3.6.2" data-path="bayesianinference.html"><a href="bayesianinference.html#maximum-a-posteriori-map"><i class="fa fa-check"></i><b>3.6.2</b> Maximum A-posteriori (MAP)  </a></li>
<li class="chapter" data-level="3.6.3" data-path="bayesianinference.html"><a href="bayesianinference.html#laplace-approximation"><i class="fa fa-check"></i><b>3.6.3</b> Laplace Approximation </a></li>
<li class="chapter" data-level="3.6.4" data-path="bayesianinference.html"><a href="bayesianinference.html#expectation-maximization-em"><i class="fa fa-check"></i><b>3.6.4</b> Expectation-Maximization (EM)  </a></li>
<li class="chapter" data-level="3.6.5" data-path="bayesianinference.html"><a href="bayesianinference.html#variational-inference"><i class="fa fa-check"></i><b>3.6.5</b> Variational Inference </a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bayesian2.html"><a href="bayesian2.html"><i class="fa fa-check"></i><b>4</b> Bayesian Computation II</a><ul>
<li class="chapter" data-level="4.1" data-path="bayesian-models.html"><a href="bayesian-models.html"><i class="fa fa-check"></i><b>4.1</b> Bayesian Models </a><ul>
<li class="chapter" data-level="4.1.1" data-path="bayesian-models.html"><a href="bayesian-models.html#belief-propagation"><i class="fa fa-check"></i><b>4.1.1</b> Belief Propagation </a></li>
<li class="chapter" data-level="4.1.2" data-path="bayesian-models.html"><a href="bayesian-models.html#expectation-propagation"><i class="fa fa-check"></i><b>4.1.2</b> Expectation Propagation </a></li>
<li class="chapter" data-level="4.1.3" data-path="bayesian-models.html"><a href="bayesian-models.html#markov-chain"><i class="fa fa-check"></i><b>4.1.3</b> Markov Chain </a></li>
<li class="chapter" data-level="4.1.4" data-path="bayesian-models.html"><a href="bayesian-models.html#hidden-markov-model"><i class="fa fa-check"></i><b>4.1.4</b> Hidden Markov Model  </a></li>
<li class="chapter" data-level="4.1.5" data-path="bayesian-models.html"><a href="bayesian-models.html#dynamic-system-model"><i class="fa fa-check"></i><b>4.1.5</b> Dynamic System Model</a></li>
<li class="chapter" data-level="4.1.6" data-path="bayesian-models.html"><a href="bayesian-models.html#bayes-filter"><i class="fa fa-check"></i><b>4.1.6</b> Bayes Filter </a></li>
<li class="chapter" data-level="4.1.7" data-path="bayesian-models.html"><a href="bayesian-models.html#kalman-filter"><i class="fa fa-check"></i><b>4.1.7</b> Kalman Filter </a></li>
<li class="chapter" data-level="4.1.8" data-path="bayesian-models.html"><a href="bayesian-models.html#extended-kalman-filter"><i class="fa fa-check"></i><b>4.1.8</b> Extended Kalman Filter </a></li>
<li class="chapter" data-level="4.1.9" data-path="bayesian-models.html"><a href="bayesian-models.html#unscented-kalman-filter"><i class="fa fa-check"></i><b>4.1.9</b> Unscented Kalman Filter </a></li>
<li class="chapter" data-level="4.1.10" data-path="bayesian-models.html"><a href="bayesian-models.html#particle-filter"><i class="fa fa-check"></i><b>4.1.10</b> Particle Filter </a></li>
<li class="chapter" data-level="4.1.11" data-path="bayesian-models.html"><a href="bayesian-models.html#ensemble-kalman-filter"><i class="fa fa-check"></i><b>4.1.11</b> Ensemble Kalman Filter </a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html"><i class="fa fa-check"></i><b>4.2</b> Simulation and Sampling</a><ul>
<li class="chapter" data-level="4.2.1" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-estimation"><i class="fa fa-check"></i><b>4.2.1</b> Monte Carlo Estimation </a></li>
<li class="chapter" data-level="4.2.2" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#monte-carlo-simulation"><i class="fa fa-check"></i><b>4.2.2</b> Monte Carlo Simulation </a></li>
<li class="chapter" data-level="4.2.3" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#markov-chain-monte-carlo"><i class="fa fa-check"></i><b>4.2.3</b> Markov Chain Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.4" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#metropolis-hastings-monte-carlo"><i class="fa fa-check"></i><b>4.2.4</b> Metropolis-Hastings Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.5" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#hamiltonian-monte-carlo"><i class="fa fa-check"></i><b>4.2.5</b> Hamiltonian Monte Carlo  </a></li>
<li class="chapter" data-level="4.2.6" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#gibbs-sampling"><i class="fa fa-check"></i><b>4.2.6</b> Gibbs Sampling </a></li>
<li class="chapter" data-level="4.2.7" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#importance-sampling"><i class="fa fa-check"></i><b>4.2.7</b> Importance Sampling </a></li>
<li class="chapter" data-level="4.2.8" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#rejection-sampling"><i class="fa fa-check"></i><b>4.2.8</b> Rejection Sampling </a></li>
<li class="chapter" data-level="4.2.9" data-path="simulation-and-sampling.html"><a href="simulation-and-sampling.html#jags-modeling"><i class="fa fa-check"></i><b>4.2.9</b> JAGS Modeling </a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html"><i class="fa fa-check"></i><b>4.3</b> Bayesian Analysis</a><ul>
<li class="chapter" data-level="4.3.1" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#autocorrelation"><i class="fa fa-check"></i><b>4.3.1</b> Autocorrelation </a></li>
<li class="chapter" data-level="4.3.2" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#predictive-probability"><i class="fa fa-check"></i><b>4.3.2</b> Predictive Probability </a></li>
<li class="chapter" data-level="4.3.3" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#posterior-interval"><i class="fa fa-check"></i><b>4.3.3</b> Posterior Interval </a></li>
<li class="chapter" data-level="4.3.4" data-path="bayesian-analysis.html"><a href="bayesian-analysis.html#bayes-factor"><i class="fa fa-check"></i><b>4.3.4</b> Bayes Factor </a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="summary-2.html"><a href="summary-2.html"><i class="fa fa-check"></i><b>4.4</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="machinelearning1.html"><a href="machinelearning1.html"><i class="fa fa-check"></i><b>5</b> Computational Learning I</a><ul>
<li class="chapter" data-level="5.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html"><i class="fa fa-check"></i><b>5.1</b> Observation and Measurement</a><ul>
<li class="chapter" data-level="5.1.1" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-measurements"><i class="fa fa-check"></i><b>5.1.1</b> Levels of Measurements</a></li>
<li class="chapter" data-level="5.1.2" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-categorical-measurements"><i class="fa fa-check"></i><b>5.1.2</b> Levels of Categorical measurements</a></li>
<li class="chapter" data-level="5.1.3" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#levels-of-continuous-measurements"><i class="fa fa-check"></i><b>5.1.3</b> Levels of Continuous measurements</a></li>
<li class="chapter" data-level="5.1.4" data-path="observation-and-measurement.html"><a href="observation-and-measurement.html#discrete-vs-continuous-measurements"><i class="fa fa-check"></i><b>5.1.4</b> Discrete vs Continuous measurements</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="input-data.html"><a href="input-data.html"><i class="fa fa-check"></i><b>5.2</b> Input Data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="input-data.html"><a href="input-data.html#structured-data"><i class="fa fa-check"></i><b>5.2.1</b> Structured Data</a></li>
<li class="chapter" data-level="5.2.2" data-path="input-data.html"><a href="input-data.html#non-structured-data"><i class="fa fa-check"></i><b>5.2.2</b> Non-Structured Data</a></li>
<li class="chapter" data-level="5.2.3" data-path="input-data.html"><a href="input-data.html#statistical-data"><i class="fa fa-check"></i><b>5.2.3</b> Statistical Data</a></li>
<li class="chapter" data-level="5.2.4" data-path="input-data.html"><a href="input-data.html#real-time-and-near-real-time-data"><i class="fa fa-check"></i><b>5.2.4</b> Real-Time and Near Real-Time Data</a></li>
<li class="chapter" data-level="5.2.5" data-path="input-data.html"><a href="input-data.html#oltp-and-datawarehouse"><i class="fa fa-check"></i><b>5.2.5</b> OLTP and Datawarehouse</a></li>
<li class="chapter" data-level="5.2.6" data-path="input-data.html"><a href="input-data.html#data-lake"><i class="fa fa-check"></i><b>5.2.6</b> Data lake</a></li>
<li class="chapter" data-level="5.2.7" data-path="input-data.html"><a href="input-data.html#natural-language-nl"><i class="fa fa-check"></i><b>5.2.7</b> Natural Language (NL)</a></li>
<li class="chapter" data-level="5.2.8" data-path="input-data.html"><a href="input-data.html#multimedia-md"><i class="fa fa-check"></i><b>5.2.8</b> Multimedia (MD)</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="primitive-methods.html"><a href="primitive-methods.html"><i class="fa fa-check"></i><b>5.3</b> Primitive Methods</a><ul>
<li class="chapter" data-level="5.3.1" data-path="primitive-methods.html"><a href="primitive-methods.html#weighting"><i class="fa fa-check"></i><b>5.3.1</b> Weighting</a></li>
<li class="chapter" data-level="5.3.2" data-path="primitive-methods.html"><a href="primitive-methods.html#smoothing"><i class="fa fa-check"></i><b>5.3.2</b> Smoothing</a></li>
<li class="chapter" data-level="5.3.3" data-path="primitive-methods.html"><a href="primitive-methods.html#normalizing"><i class="fa fa-check"></i><b>5.3.3</b> Normalizing</a></li>
<li class="chapter" data-level="5.3.4" data-path="primitive-methods.html"><a href="primitive-methods.html#standardizing"><i class="fa fa-check"></i><b>5.3.4</b> Standardizing </a></li>
<li class="chapter" data-level="5.3.5" data-path="primitive-methods.html"><a href="primitive-methods.html#centering"><i class="fa fa-check"></i><b>5.3.5</b> Centering </a></li>
<li class="chapter" data-level="5.3.6" data-path="primitive-methods.html"><a href="primitive-methods.html#scaling"><i class="fa fa-check"></i><b>5.3.6</b> Scaling </a></li>
<li class="chapter" data-level="5.3.7" data-path="primitive-methods.html"><a href="primitive-methods.html#transforming"><i class="fa fa-check"></i><b>5.3.7</b> Transforming</a></li>
<li class="chapter" data-level="5.3.8" data-path="primitive-methods.html"><a href="primitive-methods.html#clipping"><i class="fa fa-check"></i><b>5.3.8</b> Clipping </a></li>
<li class="chapter" data-level="5.3.9" data-path="primitive-methods.html"><a href="primitive-methods.html#regularizing"><i class="fa fa-check"></i><b>5.3.9</b> Regularizing</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="distance-metrics.html"><a href="distance-metrics.html"><i class="fa fa-check"></i><b>5.4</b> Distance Metrics</a><ul>
<li class="chapter" data-level="5.4.1" data-path="distance-metrics.html"><a href="distance-metrics.html#cosine-similarity"><i class="fa fa-check"></i><b>5.4.1</b> Cosine Similarity</a></li>
<li class="chapter" data-level="5.4.2" data-path="distance-metrics.html"><a href="distance-metrics.html#manhattan-and-euclidean-distance"><i class="fa fa-check"></i><b>5.4.2</b> Manhattan and Euclidean Distance  </a></li>
<li class="chapter" data-level="5.4.3" data-path="distance-metrics.html"><a href="distance-metrics.html#minkowski-and-chebyshev-supremum-distance"><i class="fa fa-check"></i><b>5.4.3</b> Minkowski and Chebyshev (Supremum) Distance  </a></li>
<li class="chapter" data-level="5.4.4" data-path="distance-metrics.html"><a href="distance-metrics.html#jaccard-similarity-and-distance"><i class="fa fa-check"></i><b>5.4.4</b> Jaccard (Similarity and Distance) </a></li>
<li class="chapter" data-level="5.4.5" data-path="distance-metrics.html"><a href="distance-metrics.html#hamming-distance"><i class="fa fa-check"></i><b>5.4.5</b> Hamming Distance </a></li>
<li class="chapter" data-level="5.4.6" data-path="distance-metrics.html"><a href="distance-metrics.html#mahalanobis-distance"><i class="fa fa-check"></i><b>5.4.6</b> Mahalanobis Distance </a></li>
<li class="chapter" data-level="5.4.7" data-path="distance-metrics.html"><a href="distance-metrics.html#precision-and-accuracy"><i class="fa fa-check"></i><b>5.4.7</b> Precision and Accuracy  </a></li>
<li class="chapter" data-level="5.4.8" data-path="distance-metrics.html"><a href="distance-metrics.html#auc-on-roc"><i class="fa fa-check"></i><b>5.4.8</b> AUC on ROC </a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>5.5</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="5.5.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-cleaning-wrangling"><i class="fa fa-check"></i><b>5.5.1</b> Data Cleaning (Wrangling)  </a></li>
<li class="chapter" data-level="5.5.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#association"><i class="fa fa-check"></i><b>5.5.2</b> Association</a></li>
<li class="chapter" data-level="5.5.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#pattern-discovery"><i class="fa fa-check"></i><b>5.5.3</b> Pattern Discovery</a></li>
<li class="chapter" data-level="5.5.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#null-invariance"><i class="fa fa-check"></i><b>5.5.4</b> Null Invariance </a></li>
<li class="chapter" data-level="5.5.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation-and-collinearity"><i class="fa fa-check"></i><b>5.5.5</b> Correlation and Collinearity  </a></li>
<li class="chapter" data-level="5.5.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#covariance"><i class="fa fa-check"></i><b>5.5.6</b> Covariance </a></li>
<li class="chapter" data-level="5.5.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#outliers-leverage-influence"><i class="fa fa-check"></i><b>5.5.7</b> Outliers, Leverage, Influence   </a></li>
<li class="chapter" data-level="5.5.8" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#dominating-factors"><i class="fa fa-check"></i><b>5.5.8</b> Dominating Factors </a></li>
<li class="chapter" data-level="5.5.9" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#missingness-and-imputation"><i class="fa fa-check"></i><b>5.5.9</b> Missingness and Imputation  </a></li>
<li class="chapter" data-level="5.5.10" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confounding-variable"><i class="fa fa-check"></i><b>5.5.10</b> Confounding Variable </a></li>
<li class="chapter" data-level="5.5.11" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#data-leakage"><i class="fa fa-check"></i><b>5.5.11</b> Data Leakage </a></li>
<li class="chapter" data-level="5.5.12" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#one-hot-encoding"><i class="fa fa-check"></i><b>5.5.12</b> One Hot Encoding </a></li>
<li class="chapter" data-level="5.5.13" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#winsorization-and-trimming"><i class="fa fa-check"></i><b>5.5.13</b> Winsorization and Trimming  </a></li>
<li class="chapter" data-level="5.5.14" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#discretization"><i class="fa fa-check"></i><b>5.5.14</b> Discretization </a></li>
<li class="chapter" data-level="5.5.15" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#stratification"><i class="fa fa-check"></i><b>5.5.15</b> Stratification </a></li>
<li class="chapter" data-level="5.5.16" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#fine-and-coarse-classing"><i class="fa fa-check"></i><b>5.5.16</b> Fine and Coarse Classing</a></li>
<li class="chapter" data-level="5.5.17" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#embedding"><i class="fa fa-check"></i><b>5.5.17</b> Embedding </a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="featureengineering.html"><a href="featureengineering.html"><i class="fa fa-check"></i><b>5.6</b> Feature Engineering</a><ul>
<li class="chapter" data-level="5.6.1" data-path="featureengineering.html"><a href="featureengineering.html#machine-learning-features"><i class="fa fa-check"></i><b>5.6.1</b> Machine Learning Features</a></li>
<li class="chapter" data-level="5.6.2" data-path="featureengineering.html"><a href="featureengineering.html#dimensionality-reduction"><i class="fa fa-check"></i><b>5.6.2</b> Dimensionality Reduction </a></li>
<li class="chapter" data-level="5.6.3" data-path="featureengineering.html"><a href="featureengineering.html#principal-component-analysis"><i class="fa fa-check"></i><b>5.6.3</b> Principal Component Analysis  </a></li>
<li class="chapter" data-level="5.6.4" data-path="featureengineering.html"><a href="featureengineering.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>5.6.4</b> Linear Discriminant Analysis (LDA)  </a></li>
<li class="chapter" data-level="5.6.5" data-path="featureengineering.html"><a href="featureengineering.html#feature-construction"><i class="fa fa-check"></i><b>5.6.5</b> Feature Construction </a></li>
<li class="chapter" data-level="5.6.6" data-path="featureengineering.html"><a href="featureengineering.html#featureselection"><i class="fa fa-check"></i><b>5.6.6</b> Feature Selection</a></li>
<li class="chapter" data-level="5.6.7" data-path="featureengineering.html"><a href="featureengineering.html#feature-transformation"><i class="fa fa-check"></i><b>5.6.7</b> Feature Transformation </a></li>
<li class="chapter" data-level="5.6.8" data-path="featureengineering.html"><a href="featureengineering.html#model-specification-1"><i class="fa fa-check"></i><b>5.6.8</b> Model Specification </a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="general-modeling.html"><a href="general-modeling.html"><i class="fa fa-check"></i><b>5.7</b> General Modeling</a><ul>
<li class="chapter" data-level="5.7.1" data-path="general-modeling.html"><a href="general-modeling.html#training-learning"><i class="fa fa-check"></i><b>5.7.1</b> Training (Learning)</a></li>
<li class="chapter" data-level="5.7.2" data-path="general-modeling.html"><a href="general-modeling.html#validation-tuning"><i class="fa fa-check"></i><b>5.7.2</b> Validation (Tuning) </a></li>
<li class="chapter" data-level="5.7.3" data-path="general-modeling.html"><a href="general-modeling.html#testing-assessing"><i class="fa fa-check"></i><b>5.7.3</b> Testing (Assessing) </a></li>
<li class="chapter" data-level="5.7.4" data-path="general-modeling.html"><a href="general-modeling.html#cross-validation-cv"><i class="fa fa-check"></i><b>5.7.4</b> Cross-Validation (CV)  </a></li>
<li class="chapter" data-level="5.7.5" data-path="general-modeling.html"><a href="general-modeling.html#bias-and-variance"><i class="fa fa-check"></i><b>5.7.5</b> Bias and Variance </a></li>
<li class="chapter" data-level="5.7.6" data-path="general-modeling.html"><a href="general-modeling.html#loss-and-cost-functions"><i class="fa fa-check"></i><b>5.7.6</b> Loss and Cost Functions  </a></li>
<li class="chapter" data-level="5.7.7" data-path="general-modeling.html"><a href="general-modeling.html#global-and-local-minima"><i class="fa fa-check"></i><b>5.7.7</b> Global and Local Minima  </a></li>
<li class="chapter" data-level="5.7.8" data-path="general-modeling.html"><a href="general-modeling.html#regularization"><i class="fa fa-check"></i><b>5.7.8</b> Regularization</a></li>
</ul></li>
<li class="chapter" data-level="5.8" data-path="supervised-vs.unsupervised-learning.html"><a href="supervised-vs.unsupervised-learning.html"><i class="fa fa-check"></i><b>5.8</b> Supervised vs.Â Unsupervised Learning  </a></li>
<li class="chapter" data-level="5.9" data-path="summary-3.html"><a href="summary-3.html"><i class="fa fa-check"></i><b>5.9</b> Summary</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>6</b> Appendix</a><ul>
<li class="chapter" data-level="6.1" data-path="appendix-a.html"><a href="appendix-a.html"><i class="fa fa-check"></i><b>6.1</b> Appendix A</a><ul>
<li class="chapter" data-level="6.1.1" data-path="appendix-a.html"><a href="appendix-a.html#trigonometry"><i class="fa fa-check"></i><b>6.1.1</b> Trigonometry</a></li>
<li class="chapter" data-level="6.1.2" data-path="appendix-a.html"><a href="appendix-a.html#logarithms"><i class="fa fa-check"></i><b>6.1.2</b> Logarithms</a></li>
<li class="chapter" data-level="6.1.3" data-path="appendix-a.html"><a href="appendix-a.html#category-theory"><i class="fa fa-check"></i><b>6.1.3</b> Category Theory</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="appendix-b.html"><a href="appendix-b.html"><i class="fa fa-check"></i><b>6.2</b> Appendix B</a><ul>
<li class="chapter" data-level="6.2.1" data-path="appendix-b.html"><a href="appendix-b.html#on-random-chances"><i class="fa fa-check"></i><b>6.2.1</b> On Random chances</a></li>
<li class="chapter" data-level="6.2.2" data-path="appendix-b.html"><a href="appendix-b.html#on-replacements"><i class="fa fa-check"></i><b>6.2.2</b> On Replacements</a></li>
<li class="chapter" data-level="6.2.3" data-path="appendix-b.html"><a href="appendix-b.html#on-permutations-and-combinations"><i class="fa fa-check"></i><b>6.2.3</b> On Permutations and Combinations</a></li>
<li class="chapter" data-level="6.2.4" data-path="appendix-b.html"><a href="appendix-b.html#on-conditional-probabilities"><i class="fa fa-check"></i><b>6.2.4</b> On Conditional Probabilities</a></li>
<li class="chapter" data-level="6.2.5" data-path="appendix-b.html"><a href="appendix-b.html#the-arithmetic-of-probabilities"><i class="fa fa-check"></i><b>6.2.5</b> The Arithmetic of Probabilities</a></li>
<li class="chapter" data-level="6.2.6" data-path="appendix-b.html"><a href="appendix-b.html#on-dependent-and-independent-events"><i class="fa fa-check"></i><b>6.2.6</b> On Dependent and Independent Events</a></li>
<li class="chapter" data-level="6.2.7" data-path="appendix-b.html"><a href="appendix-b.html#on-mutual-exclusivity"><i class="fa fa-check"></i><b>6.2.7</b> On Mutual Exclusivity</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="appendix-c.html"><a href="appendix-c.html"><i class="fa fa-check"></i><b>6.3</b> Appendix C</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliography.html"><a href="bibliography.html"><i class="fa fa-check"></i>Bibliography</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Power and Art of Approximation</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="conjugacy" class="section level2 hasAnchor">
<h2><span class="header-section-number">3.4</span> Conjugacy<a href="conjugacy.html#conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section and the subsections listing conjugate priors include additional references from Daniel Fink <span class="citation">(<a href="bibliography.html#ref-ref437d">1997</a>)</span>, Ben Lambart <span class="citation">(<a href="bibliography.html#ref-ref457b">2014</a>)</span>, and Deetorah <span class="citation">(<a href="bibliography.html#ref-ref447d">2013</a>)</span> as the basis for a long list of Conjugacy Prior derivations.</p>
<p>When dealing with bayesian probabilities, we may think of data distribution as normal distribution by default. So if we see the following <strong>NaÃ¯ve Bayes</strong> formula: </p>
<p><span class="math display">\[\begin{align}
posterior \propto likelihood \times prior,
\end{align}\]</span></p>
<p>It may be appropriate if only we highlight the type of distribution used. So let us do that:</p>
<p><span class="math display">\[\begin{align}
\underbrace{posterior}_\text{normal} \propto \underbrace{likelihood}_\text{normal} \times \underbrace{prior}_\text{normal}
\end{align}\]</span></p>
<p>Both the <strong>posterior</strong> and <strong>prior</strong> components of a <strong>Bayesian</strong> formula are considered conjugate if they share the same family of probability distributions. It means that the parameters of both components follow the same family of distribution, e.g., a normal distribution.</p>
<div id="precision" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.1</span> Precision <a href="conjugacy.html#precision" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Before discussing a few conjugate combinations, let us introduce the concept of <strong>Precision</strong>.</p>
<p>A normal distribution has two common parameters: mean and variance. However, there is one other parameter that we can use for normal distribution called <strong>precision</strong> (let us use the <strong>L</strong>ambda <span class="math inline">\(\Lambda\)</span> symbol for this) which is the inverse of variance, <span class="math inline">\(\Lambda = \frac{1}{\sigma^2} = (\sigma^2)^{-1}\)</span>. <strong>Precision</strong> has an advantage over variance in that it has an <strong>additive property</strong>, which is useful for multivariate normal distribution.</p>
<p>For <strong>multivariate normal distribution</strong>, we have the following adjusted <strong>normal density function</strong>:</p>
<p><span class="math display">\[\begin{align}
f(x; \theta) = \frac{1}{\sqrt{(2\pi)^d \Sigma}} exp\left[-\frac{1}{2}\underbrace{(x - \mu)^T{\Sigma}^{-1}(x - \mu)}_\text{squared mahalanobis distance}\right]
\end{align}\]</span></p>
<p>where:</p>
<p><span class="math display">\[\begin{align}
{\Sigma}^{-1} = \sum uu^T\ \ \ \ \ \ \  \text{(an inverse covariance positive definite matrix)}
\end{align}\]</span></p>
<p>Note that <strong>Squared Mahalanobis distance</strong> measures the distance between distributions (e.g.Â between <strong>x</strong> and <span class="math inline">\(\mu\)</span>). See a brief definition in Chapter <strong>9</strong> (<strong>Computational Learning I</strong>) under <strong>Distance metrics</strong> Section.</p>
</div>
<div id="conjugate-prior" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.2</span> Conjugate Prior <a href="conjugacy.html#conjugate-prior" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A way for a <strong>posterior</strong> to characterize a <strong>proper density</strong> is to have a <strong>prior</strong> density be applied to a <strong>sampling density (likelihood)</strong>. Therefore, it is essential to be able to choose an appropriate prior density that leads to a proper posterior density. We need to choose a prior distribution that fits a good distribution for the posterior. We call this chosen prior distribution a <strong>conjugate prior</strong>.</p>
<p>In <strong>machine learning</strong>, it is common to regard <strong>prior</strong> as a weighing factor denoted as <span class="math inline">\(\omega(.)\)</span> or <span class="math inline">\(\pi(.)\)</span>, e.g.:</p>

<p><span class="math display">\[\begin{align}
P(\mu|x) \propto P(\mu) \times Lik(\mu|x)
\rightarrow P(\mu|x) \propto \mathcal{\pi}(\mu) \times Lik(\mu|x)
\end{align}\]</span>
</p>
</div>
<div id="normal-normal-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.3</span> Normal-Normal Conjugacy <a href="conjugacy.html#normal-normal-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Normal density</strong> distribution for <strong>posterior</strong> given that a <strong>Normal density</strong> is <strong>conjugate prior</strong> for a <strong>Normal likelihood</strong> where the mean <span class="math inline">\(\mu\)</span> is unknown. For a notation, we use the following:</p>

<p><span class="math display">\[\begin{align}
P(\mu|x) \propto P(\mu) \times Lik(\mu|x)
\ \ \ \ \ \ \text{for } -\infty &lt; \mu\ &lt; \infty
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\mu, \sigma^2 \sim \mathcal{N}(\mu, \sigma^2)\ \ \ \ \ \text{where}\ \sigma^2\ \text{ is known, but}\ \mu\ \text{is unknown}
\end{align}\]</span>
</p>
<p>For <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, \sigma^2|x) {}&amp;\equiv P_X(X=x|\mu, \sigma^2)\\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right]
\end{align}\]</span>
</p>
<p>For <strong>joint-density</strong> likelihood (where <strong>n</strong> is sample size):</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, \sigma^2|x_1,...,x_n) &amp;\equiv P_X(x_1,...,x_n|\mu, \sigma^2) \\
&amp;= \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right]\\
&amp;= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right]
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Normal</strong> distribution for the <span class="math inline">\(\mu\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\mu \sim \mathcal{N}(\mu_0, \sigma^2_0)\ \ \ \ \ \text{where}\ \mu_0\ \text{and}\ \sigma^2_0 \text{ are known } \mathbf{hyperparameters} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\mu) = P(\mu; \mu_0, \sigma^2_0) = \frac{1}{\sqrt{2\pi\sigma^2_0}} exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right] 
\end{align}\]</span>
</p>
<p>For <strong>posterior</strong>, we want to arrive at a <strong>Normal density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\mu, \sigma^2|x \sim \mathcal{N}(\mu_1, \sigma^2_1) 
\end{align}\]</span>
</p>
<p>However, first, let us derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\mu\)</span> by <strong>dropping the constants</strong> that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents).</p>
<p>For a <strong>Normal posterior</strong> with <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\sigma^2|x) {}&amp;\propto  
\underbrace { \frac{1}{\sqrt{2\pi\sigma^2_0}} exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right]}_\text{normal prior} \times 
\underbrace { \frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right] }_\text{normal likelihood}\\
P(\mu,\sigma^2|x) &amp;\propto exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right] \times
exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right]\ \ \text{(drop constant)} \\
&amp;\propto exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}
-\frac{(x - \mu)^2}{2\sigma^2}\right] \\
&amp; \propto exp\left[-\left(\frac{\sigma^2}{\sigma^2}\right)\frac{(\mu - \mu_0)^2}{2\sigma^2_0} 
-\left(\frac{\sigma^2_0}{\sigma^2_0}\right)\frac{(x - \mu)^2}{2\sigma^2}\right] \\
&amp;\propto exp\left[-\frac{\sigma^2(\mu - \mu_0)^2}{2\sigma^2_0\sigma^2}
-\frac{\sigma^2_0(x - \mu)^2}{2\sigma^2\sigma^2_0}\right] \\
&amp;\propto exp\left[\frac{
      -\sigma^2(\mu^2 - 2\mu\mu_0 + \mu^2_0) + -\sigma^2_0(x^2 - 2x\mu + \mu^2)}
      {2\sigma^2\sigma^2_0}\right] \\
&amp;\propto exp\left[\frac{
      (-\mu^2\sigma^2 + 2\mu\mu_0\sigma^2 - \mu^2_0\sigma^2) + (-x^2\sigma^2_0 + 2x\mu\sigma^2_0 - \mu^2\sigma^2_0)}
      {2\sigma^2\sigma^2_0}\right] 
\end{align}\]</span></p>

<p>Let us pull out contents of the exponent and drop the constants with respect to <span class="math inline">\(\mu\)</span>, then simplify further:</p>

<p><span class="math display">\[\begin{align}
{}&amp;\rightarrow \frac{
      (-\mu^2\sigma^2 + 2\mu\mu_0\sigma^2) + (2x\mu\sigma^2_0 - \mu^2\sigma^2_0)}
      {2\sigma^2\sigma^2_0} \\
&amp;\rightarrow \frac{1}{2}\frac{
      -\mu^2(\sigma^2 + \sigma^2_0) +2\mu(\mu_0\sigma^2 + x\sigma^2_0)} 
      {\sigma^2\sigma^2_0} \\
&amp;\rightarrow \frac{1}{2}\frac{
      -\mu^2(\sigma^2 + \sigma^2_0) +2\mu(\mu_0\sigma^2 + x\sigma^2_0)} 
      {\sigma^2\sigma^2_0} \left(\frac{\frac{1}{\sigma^2 + \sigma^2_0}}{\frac{1}{\sigma^2 + \sigma^2_0}}\right)\\
&amp;\rightarrow -\frac{1}{2}\left(\frac{
      \mu^2 - 2\mu
         \frac{ \mu_0\sigma^2 + x\sigma^2_0 }{\sigma^2 + \sigma^2_0}
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + \sigma^2_0}  } \right)\ \ \
  \rightarrow -\frac{1}{2}\left(\frac{
       \left( \mu - 
        \frac{ \mu_0\sigma^2 + x\sigma^2_0 }{\sigma^2 + \sigma^2_0}\right)^2
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + \sigma^2_0}  } \right) 
\end{align}\]</span>
</p>
<p>If we then put back the equation inside the exponent, we then have the following:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\sigma^2|x) \propto 
 exp\left[-\frac{(\mu - \mu_1)^2}{2\sigma^2_1}\right] = 
 exp 
 \left[
   -\frac{1}{2}\left(\frac{
      \left(  \mu - 
         \frac{ \mu_0\sigma^2 + x\sigma^2_0}{\sigma^2 + \sigma^2_0}
         \right)^2
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + n\sigma^2_0}  } \right)
 \right]
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Normal posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\mu_1 = \frac{ \mu_0\sigma^2 + x\sigma^2_0}{\sigma^2 + \sigma^2_0} =
\frac{\left(\frac{\mu_0}{\sigma^2_0} + \frac{x}{\sigma^2}\right)}
      {\left(\frac{1}{\sigma^2_0} + \frac{1}{\sigma^2}\right) }
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\sigma^2_1 = \frac{\sigma^2\sigma^2_0}{\sigma^2 + \sigma^2_0} = 
\left(\frac{1}{\sigma^2_0} + \frac{1}{\sigma^2}\right)^{-1}
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\mu,\sigma^2|x \sim ~ N(\alpha_1, \beta_1)\ \ \ \rightarrow 
N\left[ \sigma^2_1\left(\frac{\mu_0}{\sigma^2_0} + \frac{x}{\sigma^2}\right),
 \left(\frac{1}{\sigma^2_0} + \frac{1}{\sigma^2}\right)^{-1} \right] 
\end{align}\]</span>
</p>
<p>For a <strong>Normal posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\sigma^2|x_1,...,x_n) {}&amp;\propto  
\underbrace{ \frac{1}{\sqrt{2\pi\sigma^2_0}} exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right]}_\text{normal prior} \times
\underbrace{ \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right]}_\text{normal likelihood}\\
P(\mu,\sigma^2|x_1,...,x_n) &amp;\propto exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}\right] \times
exp\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right]\ \\
&amp;\propto exp\left[-\frac{(\mu - \mu_0)^2}{2\sigma^2_0}
-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] \\
&amp; \propto exp\left[-\left(\frac{\sigma^2}{\sigma^2}\right)\frac{(\mu - \mu_0)^2}{2\sigma^2_0} 
-\left(\frac{\sigma^2_0}{\sigma^2_0}\right)\frac{\sum_{i=1}^n(x_i  - \mu)^2}{2\sigma^2}\right] \\
&amp;\propto exp\left[-\frac{\sigma^2(\mu - \mu_0)^2}{2\sigma^2_0\sigma^2}
-\frac{\sigma^2_0\sum_{i=1}^n(x_i  - \mu)^2}{2\sigma^2\sigma^2_0}\right] \\
&amp;\propto exp\left[\frac{
      -\sigma^2(\mu^2 - 2\mu\mu_0 + \mu^2_0) + -\sigma^2_0(\sum_{i=1}^nx_i^2 - 2n\bar{x}\mu + n\mu^2)}
      {2\sigma^2\sigma^2_0}\right] 
\end{align}\]</span>
<span class="math display">\[\begin{align}
&amp;\propto exp\left[\frac{
      (-\mu^2\sigma^2 + 2\mu\mu_0\sigma^2 - \mu^2_0\sigma^2) + (-\sigma^2_0 \sum_{i=1}^nx_i^2 + 2n \bar{x}\mu\sigma^2_0 - n\mu^2\sigma^2_0)}
      {2\sigma^2\sigma^2_0}\right] 
\end{align}\]</span>
</p>
<p>Let us pull out contents of the exponent and drop the constants with respect to <span class="math inline">\(\mu\)</span>:</p>

<p><span class="math display">\[\begin{align}
{}&amp;\rightarrow \frac{
      (-\mu^2\sigma^2 + 2\mu\mu_0\sigma^2) + (2n\bar{x}\mu\sigma^2_0 - n\mu^2\sigma^2_0)}
      {2\sigma^2\sigma^2_0} \\
&amp;\rightarrow \frac{
      -\mu^2(\sigma^2 + n\sigma^2_0) + 2\mu(\mu_0\sigma^2 + \bar{x}n\sigma^2_0)}
      {2\sigma^2\sigma^2_0} \\
&amp;\rightarrow \frac{
      -\mu^2(\sigma^2 + n\sigma^2_0) + 2\mu(\mu_0\sigma^2 + \bar{x}n\sigma^2_0)}
      {2\sigma^2\sigma^2_0}  \left(\frac{\frac{1}{\sigma^2 + n\sigma^2_0}}{\frac{1}{\sigma^2 + n\sigma^2_0}}\right)\\
&amp;\rightarrow -\frac{1}{2}\left(\frac{
      \mu^2 - 
         2\mu \frac{ \mu_0\sigma^2 + n\bar{x}\sigma^2_0}{\sigma^2 + n\sigma^2_0}
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + n\sigma^2_0}  } \right)\ \ \ 
  \rightarrow\ \ \
   -\frac{1}{2}\left(\frac{
       \left( \mu - 
          \frac{ \mu_0\sigma^2 + n\bar{x}\sigma^2_0}{\sigma^2 + n\sigma^2_0}
         \right)^2
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + n\sigma^2_0}  } \right) 
\end{align}\]</span>
</p>
<p>If we then put back the equation inside the exponent, we then have the following:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\sigma^2|x1,...,x_n) \propto 
 exp\left[-\frac{(\mu - \mu_1)^2}{2\sigma^2_1}\right] = 
 exp \left[
   -\frac{1}{2}\left(\frac{
       \left( \mu - 
         \frac{ \mu_0\sigma^2 + n\bar{x}\sigma^2_0}{\sigma^2 + n\sigma^2_0}
         \right)^2
      } 
      {\frac{\sigma^2\sigma^2_0}{\sigma^2 + n\sigma^2_0}  } \right)
 \right]
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Normal posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\mu_1 {}= \frac{ \mu_0\sigma^2 + n\bar{x}\sigma^2_0}{\sigma^2 + n\sigma^2_0} =
\frac{\left(\frac{\mu_0}{\sigma^2_0} + \frac{n\bar{x}}{\sigma^2}\right)}
      {\left(\frac{1}{\sigma^2_0} + \frac{n}{\sigma^2}\right) }
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\sigma^2_1 = \frac{\sigma^2\sigma^2_0}{\sigma^2 + n\sigma^2_0} = 
\left(\frac{1}{\sigma^2_0} + \frac{n}{\sigma^2}\right)^{-1}
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\mu,\sigma^2|x_1,...,x_n \sim ~ N(\alpha_1, \beta_1)\ \ \ \rightarrow 
N\left[ \sigma^2_1\left(\frac{\mu_0}{\sigma^2_0} + \frac{n\bar{x}}{\sigma^2}\right),
 \left(\frac{1}{\sigma^2_0} + \frac{n}{\sigma^2}\right)^{-1} \right] 
\end{align}\]</span>
</p>
</div>
<div id="normal-inverse-gamma-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.4</span> Normal-Inverse Gamma Conjugacy <a href="conjugacy.html#normal-inverse-gamma-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor an <strong>Inverse Gamma density</strong> distribution for <strong>posterior</strong> given that an <strong>Inverse Gamma density</strong> is <strong>conjugate prior</strong> for a <strong>Normal likelihood</strong> where the variance <span class="math inline">\(\sigma^2\)</span> is unknown. For a notation, we use the following:</p>

<p><span class="math display">\[\begin{align}
P(\sigma^2|x) \propto P(\sigma^2) \times Lik(\sigma^2|x)
\ \ \ \ \ \ \text{for } -\infty &lt; \sigma^2\ &lt; \infty 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\mu,\sigma^2 \sim \mathcal{N}(\mu, \sigma^2)\ \ \ \ \ \text{where}\ \mu\ \text{ is known, but}\ \sigma^2\ \text{is unknown}
\end{align}\]</span>
</p>
<p>For <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu,\sigma^2|x) {}&amp;\equiv P(X=x|\mu,\sigma^2)\\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right]
\end{align}\]</span>
</p>
<p>For <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, \sigma^2|x_1,...,x_n) &amp;\equiv P_X(x_1,...,x_n|\mu,\sigma^2) \\
&amp;= \prod_{i=1}^n\frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x_i - \mu)^2}{2\sigma^2}\right]\\
&amp;= \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left[-\frac{\sum_{i=1}^n (x_i - \mu)^2}{2\sigma^2}\right]
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose an <strong>Inverse Gamma</strong> distribution for the <span class="math inline">\(\sigma^2\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\sigma^2 \sim Inv.\ Gamma(\alpha_0,\beta_0)\ \ \ \ \ \text{where}\ \alpha_0\ \text{and}\ \beta_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\sigma^2) = P(\sigma^2; \alpha_0, \beta_0) =
\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right)\ \ \text{(inverse)}
\end{align}\]</span>
</p>
<p>For <strong>posterior</strong>, we want to arrive at an <strong>Inv. Gamma density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\sigma^2|x \sim Inv.\ Gamma(\alpha_1,\beta_1)
\end{align}\]</span>
</p>
<p>But first, let us derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\sigma^2\)</span> by <strong>dropping the constants</strong> that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents).</p>
<p>For an <strong>Inv. Gamma posterior</strong> with <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\sigma^2|x) {}&amp;\propto  \underbrace{ \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right) }_\text{inverse-gamma prior} \times 
\underbrace{ \frac{1}{\sqrt{2\pi\sigma^2}} exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right]}_\text{normal likelihood} \\
&amp;\propto\frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right) \times
(2\pi)^{-\frac{1}{2}} (\sigma^2)^{-\frac{1}{2}}  exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right]\\
P(\mu,\sigma^2|x) 
&amp;\propto \left[{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right) \right] \times
 (\sigma^2)^{-\frac{1}{2}}  exp\left[-\frac{(x - \mu)^2}{2\sigma^2}\right] \ \text{(drop const.)} \\
&amp;\propto \left[{(\sigma^2)}^{-(\alpha_0+\frac{1}{2} + 1)}
   exp\left( 
   -\frac{1}{\sigma^2} \left(\beta_0 + \frac{(x - \mu)^2}{2} \right)
   \right) 
 \right] 
\end{align}\]</span>
</p>
<p>Notice that given an <strong>Inv. Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 = \alpha_0  + \frac{1}{2}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = \left(\beta_0 + \frac{(x - \mu)^2}{2} \right)
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x \sim Inv. Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow 
Inv. Gamma \left[\alpha_0  + \frac{1}{2}, \left(\beta_0 + \frac{(x - \mu)^2}{2} \right) \right]
\end{align}\]</span>
</p>
<p>For an <strong>Inv. Gamma posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\mu&amp;,\sigma^2|x_1,...,x_n) \propto \nonumber \\
&amp;\underbrace{ \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right)}_\text{inverse-gamma prior}
\times 
\underbrace{ \left(\frac{1}{\sqrt{2\pi\sigma^2}}\right)^n exp\left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] }_\text{normal likelihood} \\
&amp;\propto   \frac{\beta_0^{\alpha_0}}{\Gamma(\alpha_0)}{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right) \times
(2\pi)^{-\frac{n}{2}} (\sigma^2)^{-\frac{n}{2}}  exp\left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] 
\end{align}\]</span>
<span class="math display">\[\begin{align}
P(\mu,\sigma^2|x_1,...,x_n) 
&amp;\propto \left[{(\sigma^2)}^{-(\alpha_0+1)}exp\left({-\frac{\beta_0}{\sigma^2}}\right) \right] \times
 (\sigma^2)^{-\frac{n}{2}}  exp\left[-\frac{\sum_{i=1}^n(x_i - \mu)^2}{2\sigma^2}\right] \\
&amp;\propto \left[{(\sigma^2)}^{-(\alpha_0+\frac{n}{2} + 1)}
   exp\left( 
   -\frac{1}{\sigma^2} \left(\beta_0 + \frac{\sum_{i=1}^n(x_i - \mu)^2}{2} \right)
   \right) 
 \right] 
\end{align}\]</span>
</p>
<p>Notice that given an <strong>Inv. Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 = \alpha_0  + \frac{n}{2}
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 =  \beta_0 + \frac{\sum_{i=1}^n(x_i - \mu)^2}{2}  
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x_1,...,x_n \sim Inv. Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow 
Inv. Gamma \left[\alpha_0  + \frac{n}{2},  \beta_0 + \frac{\sum_{i=1}^n(x_i - \mu)^2}{2}  \right ]
\end{align}\]</span>
</p>
</div>
<div id="multivariate-normal-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.5</span> Multivariate Normal Conjugacy <a href="conjugacy.html#multivariate-normal-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Multivariate Normal (MVN)</strong> distribution for <strong>posterior</strong> given that <strong>MVN density</strong> is <strong>conjugate prior</strong> for the mean parameter, <span class="math inline">\(\mu\)</span>. For a notation, we use the following:</p>

<p><span class="math display">\[\begin{align}
P(\mu|x) \propto P(\mu) \times Lik(\mu|x)
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following <strong>p-variate normal</strong> distribution:</p>

<p><span class="math display">\[\begin{align}
X_m|\mu,\Sigma \sim \mathcal{N}_p(\mu, \Sigma)\ \ \ \ \ \text{where}\ \mu\ \text{is unknown and }\ \Sigma \text{ is  known }
\end{align}\]</span>
</p>
<p>and where:</p>

<p><span class="math display">\[\begin{align*}
X = \left[\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end{array}\right]_\text{(nxp)}
 \ \ \ \ 
\mu = \left[\begin{array}{c}\bar{x}_1 \\ \bar{x}_2 \\ \vdots \\ \bar{x}_p \end{array}\right]
= \left[\begin{array}{c}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p \end{array}\right]_\text{(1xp)}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\Sigma = \left[\begin{array}{rrrr}\sigma^2_{1} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_{2} &amp; \cdots &amp; \sigma_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \cdots &amp; \sigma^2_{p} \end{array}\right]_\text{(pxp)}
\end{align*}\]</span>
</p>
<p>The notation <span class="math inline">\(\Sigma\)</span> above represents a <strong>p-variate</strong> <strong>positive-definite</strong> variance-covariance matrix.</p>
<p>For <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, {}&amp;\Sigma|x_1,...,x_n) \equiv P_X(x_1,...,x_n|\mu,\Sigma) \\
&amp;= \prod_{i=1}^n\frac{|\Sigma|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left[-\frac{1}{2}(x_i - \mu)^T\Sigma^{-1}(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Sigma|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\sum_{i=1}^n (x_i - \mu)^T\Sigma^{-1}(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Sigma|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{n}{2} (\bar{x} - \mu)^T\Sigma^{-1}(\bar{x} - \mu)\right]\\
&amp;= \left(\frac{|\Sigma|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{n}{2} (\bar{x}^T\Sigma^{-1}\bar{x} - \bar{x}^T\Sigma^{-1}\mu - \mu^T\Sigma^{-1}\bar{x} + \mu^T\Sigma^{-1}\mu )\right]
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Multivariate normal</strong> distribution for the <span class="math inline">\(\mu\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\mu \sim \mathcal{N}_p(\mu_0,\Sigma_0)\ \ \ \ \ \text{where}\ \mu_0\ \text{and}\ \Sigma_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\mu) = P(\mu; \mu_0, \Sigma_0) {}&amp;= \frac{|\Sigma_0|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left[-\frac{1}{2}(\mu - \mu_0)^T\Sigma_0^{-1}(\mu - \mu_0)\right]\\
&amp;= \frac{|\Sigma_0|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} 
exp\left[-\frac{1}{2}\left( \mu^T \Sigma_0^{-1} \mu - \mu^T\Sigma_0^{-1} \mu_0 -  \mu_0^T\Sigma_0^{-1} \mu  + {\mu_0}^T \Sigma_0^{-1}\mu_0  \right)\right]
\end{align}\]</span>
</p>
<p>For <strong>posterior</strong>, we want to arrive at a <strong>Multivariate normal density</strong> given some observed data:</p>

<p><span class="math display">\[\begin{align}
\mu|X \sim \mathcal{N}_p(\mu_1,\Sigma_1) 
\end{align}\]</span>
</p>
<p>But first, let us derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\mu\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>
<p>For a <strong>multivariate normal posterior</strong>:
</p>
<p><span class="math display">\[\begin{align}
P(\mu|x) &amp;= 
\underbrace{\frac{|\Sigma_0|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} 
exp\left[-\frac{1}{2}\left( \mu^T \Sigma_0^{-1} \mu - \mu^T\Sigma_0^{-1} \mu_0 -  {\mu_0}^T\Sigma_0^{-1} \mu  + {\mu_0}^T \Sigma_0^{-1}\mu_0 \right)\right]}_\text{MVN prior} \nonumber \\
&amp;\times \underbrace{\left(\frac{|\Sigma|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n 
exp\left[-\frac{n}{2} (\bar{x}^T\Sigma^{-1}\bar{x} - \bar{x}^T\Sigma^{-1}\mu - \mu^T\Sigma^{-1}\bar{x} + \mu^T\Sigma^{-1}\mu ) \right] }_\text{MVN likelihood}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow &amp; \text{(drop constants not relevant to } \mu \text{. Also, recall matrix transposition properties)} \nonumber \\
&amp;\propto  exp\left[-\frac{1}{2}\left( \mu^T \Sigma_0^{-1} \mu -  \mu^T\Sigma_0^{-1} \mu_0 - {\mu_0} ^T\Sigma_0^{-1} \mu \right)\right]\\
&amp;\times exp\left[-\frac{n}{2} \left( -  \bar{x} ^T\Sigma^{-1}\mu -  \mu^T\Sigma^{-1}\bar{x}  +   \mu^T\Sigma^{-1}\mu\right)\right] \\
 &amp;\propto  exp\biggl[-\frac{1}{2}\biggl(\left[ \mu^T \Sigma_0^{-1} \mu -  \mu^T\Sigma_0^{-1} \mu_0 - {\mu_0} ^T\Sigma_0^{-1}\mu \right]  + \left[  -  n\bar{x} ^T\Sigma^{-1}\mu -  n\mu^T\Sigma^{-1}\bar{x}  +   n\mu^T\Sigma^{-1}\mu\right]\biggr)\biggr]\\
  &amp;\propto  exp\biggl[-\frac{1}{2}\biggl( \mu^T \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right] \mu -  \mu^T \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right]  - \left[\Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right]^T\mu
  \biggr)\biggr]\\
  &amp;\propto  exp\biggl[-\frac{1}{2}\biggl( \mu^T \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right] \mu -  2\mu^T \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right]  
  \biggr)\biggr]\\
&amp;\propto  exp\biggl[-\frac{1}{2}\biggl(
    \left(\mu -   \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right]^{-1}
            \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right] \right)^T
            \left( \Sigma_0^{-1} + n\Sigma^{-1} \right) \nonumber \\
&amp;\ \ \ \ \ \ \ \ \ \ \ \left(\mu -   \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right]^{-1}
            \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right] \right)     
  \biggr)\biggr]
\end{align}\]</span>
</p>
<p>Notice that given an <strong>MVN posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\Sigma_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\Sigma_1 =  \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right]^{-1}\ \ \ \ \ \ \ \ \ \ \ \ 
\mu_1 = \Sigma_1 \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right]
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>p-variate posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\mu|x \sim \mathcal{N}_p(\mu_1,\Sigma_1) \ \ \ \rightarrow 
\mathcal{N}_p \left(  \Sigma_1 \left[ \Sigma_0^{-1}\mu_0 + \Sigma^{-1}n\bar{x}\right],\  \left[ \Sigma_0^{-1} + n\Sigma^{-1}\right]^{-1} \right)
\end{align}\]</span>
</p>
</div>
<div id="normal-wishart-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.6</span> Normal Wishart Conjugacy <a href="conjugacy.html#normal-wishart-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Normal Wishart</strong> distribution for <strong>posterior</strong> given that <strong>Normal Wishart density</strong> is <strong>conjugate prior</strong> for <strong>mean</strong>, <span class="math inline">\(\mu\)</span>, and <strong>positive-definite</strong> <strong>precision</strong> matrix parameter, <span class="math inline">\(\Lambda\)</span>, of a <strong>Multivariate normal likelihood</strong>. For a notation, we use the following:</p>

<p><span class="math display">\[\begin{align}
P(\mu, \Lambda|x) \propto P(\mu,\Lambda) \times Lik(\mu, \Lambda|x)
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following <strong>p-variate normal</strong> distribution:</p>

<p><span class="math display">\[\begin{align}
X|\mu,\Lambda \sim \mathcal{N}_p(X; \mu, \Lambda)  \ \ \ \ \ \text{where}\ \mu\ and\ \Lambda\ \text{are unknown }
\end{align}\]</span>
</p>
<p>Recall the below structure. See Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>) under <strong>Wishart Distribution</strong> section:</p>

<p><span class="math display">\[\begin{align*}
X = \left[\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end{array}\right]_\text{(nxp)}
 \ \ \ \ 
\mu = \left[\begin{array}{c}\bar{x}_1 \\ \bar{x}_2 \\ \vdots \\ \bar{x}_p \end{array}\right]
= \left[\begin{array}{c}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p \end{array}\right]_\text{(1xp)}
\end{align*}\]</span></p>
<p><span class="math display">\[\begin{align*}
\Lambda = \left[\begin{array}{rrrr}\sigma^2_{1} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_{2} &amp; \cdots &amp; \sigma_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \cdots &amp; \sigma^2_{p} \end{array}\right]_\text{(pxp)}^{(-1)}
\end{align*}\]</span>
</p>
<p>For <strong>multivariate joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, \Lambda|x_1,...,x_n) &amp;\equiv P_X(x_1,...,x_n|\mu,\Lambda) \\
&amp;= \prod_{i=1}^n\frac{|\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left[-\frac{1}{2}(x_i - \mu)^T\Lambda(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\sum_{i=1}^n (x_i - \mu)^T\Lambda(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\sum_{i=1}^n \Lambda(x_i - \mu)(x_i - \mu)^T\right]\\
&amp;= \left(\frac{|\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\Lambda\left(\sum_{i=1}^n x_i x_i^T - \mu (n\bar{x})^T - (n\bar{x})\mu^T + n\mu\mu^T\right)\right]
\end{align}\]</span>
</p>
<p>It helps to be aware of the following mathematical manipulation (derivation not included):</p>

<p><span class="math display">\[\begin{align}
\sum_{i=1}^n (x_i - \mu)^T\Lambda(x_i - \mu) = tr( \Sigma \Lambda)\ \ \ \ \  where\ \ \ \Sigma = \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Normal-Wishart</strong> distribution for both <span class="math inline">\(\Lambda\)</span> and <span class="math inline">\(\mu\)</span> parameters. Recall description of <strong>Central Wishart</strong> notation under <strong>Wishart distribution</strong> in Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>.</p>

<p><span class="math display">\[\begin{align}
\Lambda {}&amp;\sim \mathcal{W}_p(v_0, \Sigma_0)\ \ \ \ \ \text{where}\ v_0\ and\ \Sigma_0 \text{ are known } \mathbf{hyperparameters}\\
\nonumber \\
\mathcal{\pi}(\Lambda) &amp;= P(\Lambda; \nu_0, \Sigma_0) 
    =  \frac{|\Lambda|^{\frac{\nu_0-p-1}{2}} exp\left[-\frac{1}{2}tr(\Sigma_0^{-1}\Lambda)\right]}
    {2^{\frac{\nu_0 p}{2}}|\Sigma_0|^{\frac{\nu_0}{2}}\ \Gamma_p\left(\frac{\nu_0}{2}\right)}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mu|\Lambda {}&amp;\sim \mathcal{N}_p(\alpha_0, (\beta_0\Lambda)^{-1})\ \ \ \ \ \text{where}\ \alpha_0\ and\ \beta_0\Lambda \text{ are known } \mathbf{hyperparameters}\\
\nonumber  \\
\mathcal{\pi}(\mu|\Lambda) &amp;= P(\mu; \alpha_0, (\beta_0\Lambda)^{-1})
    =  \frac{|\beta_0\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left[-\frac{1}{2}(\mu - \alpha_0)^T\beta_0\Lambda(\mu - \alpha_0)\right]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow &amp; \text{(join distributions and drop constants)} \nonumber \\
\nonumber \\
P(\mu, \Lambda) 
&amp;\propto  \underbrace{ |\beta_0\Lambda|^{\frac{1}{2}} 
  exp\left[-\frac{1}{2}(\mu -     \alpha_0)^T\beta_0\Lambda(\mu - \alpha_0)\right]}_\text{gaussian prior}
      \times
       \underbrace{ |\Lambda|^{\frac{\nu_0-p-1}{2}}  
  exp\left[-\frac{1}{2}tr(\Sigma_0^{-1}\Lambda)\right]}_\text{wishart  prior}\\
&amp;\propto    |\beta_0|^\frac{1}{2} |\Lambda|^{\frac{1}{2}}  |\Lambda|^{\frac{\nu_0-p-1}{2}}  exp\left[-\frac{\beta_0}{2}\Lambda(\mu - \alpha_0)(\mu - \alpha_0)^T + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \right] \\
&amp;\propto   |\beta_0|^\frac{1}{2} |\Lambda|^{\frac{\nu_0-p}{2}}  exp\left[-\frac{\beta_0}{2}\Lambda(\mu\mu^T - \mu{\alpha_0}^T - {\alpha_0}\mu^T + \alpha_0{\alpha_0}^T) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \right]
\end{align}\]</span>
</p>
<p>where:</p>
<ul>
<li><span class="math inline">\(\alpha_0\)</span> is hyper mean for <span class="math inline">\(\mu\)</span>.</li>
<li><span class="math inline">\(\beta_0\Lambda\)</span> is hyper precision for <span class="math inline">\(\mu\)</span>. See Figure  as reference model.</li>
<li><span class="math inline">\(\Lambda\)</span> is <strong>positive-definite</strong> precision matrix that follows a <strong>wishart</strong> distribution.</li>
</ul>
<p>For <strong>posterior</strong>, we want to join the likelihood and the normal-wishart prior:</p>

<p><span class="math display">\[\begin{align}
\mu, \Lambda|x \sim \ \mathcal{NW}_p(\mu, \Lambda; \alpha_1, (\beta_1\Lambda), v_1, \Sigma_1)
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\Lambda\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents).</p>
<p>For a <strong>Normal-Wishart posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P{}&amp;(\mu,\Lambda|X) \propto  \underbrace{ |\beta_0|^\frac{1}{2} |\Lambda|^{\frac{\nu_0-p}{2}}  exp\left[-\frac{\beta_0}{2}\Lambda(\mu\mu^T - \mu{\alpha_0}^T - {\alpha_0}\mu^T + \alpha_0{\alpha_0}^T) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \right] }_\text{normal-wishart prior} \nonumber \\
&amp;\times 
\underbrace{\left(\frac{|\Lambda|^{\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\Lambda\left(\sum_{i=1}^n x_i {x_i}^T - \mu (n\bar{x})^T - (n\bar{x})\mu^T + n\mu\mu^T\right)\right]}_\text{normal likelihood} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow &amp; \text{(drop constants and simplify. Recall matrix transposition properties)} \nonumber \\
&amp;\propto   |\Lambda|^{\frac{\nu_0-p}{2}} |\Lambda|^{\frac{n}{2}} exp\biggl[-\frac{\beta_0}{2}\Lambda(\mu\mu^T - \mu{\alpha_0}^T - {\alpha_0}\mu^T + \alpha_0{\alpha_0}^T) +  -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \nonumber \\
&amp;+ -\frac{1}{2}\Lambda\biggl(\sum_{i=1}^n x_i {x_i}^T - \mu (n\bar{x})^T - (n\bar{x})\mu^T + n\mu\mu^T\biggr) \biggr] \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((\beta_0 + n) \mu\mu^T - (\alpha_0\beta_0 + n\bar{x})\mu^T  - \mu(\alpha_0\beta_0 + n\bar{x})^T + \beta_0\alpha_0{\alpha_0}^T  + \sum_{i=1}^n x_i {x_i}^T \biggr)  + \nonumber \\
&amp; -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow &amp; \text{(inject placeholders)} \nonumber \\
\rightarrow  &amp;\text{let}\ a = (\beta_0 + n)\ and\ b = (\alpha_0\beta_0 + n\bar{x})\\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((a) \mu\mu^T - (b)\mu^T  - \mu(b)^T + \beta_0\alpha_0{\alpha_0}^T  + \sum_{i=1}^n x_i {x_i}^T \biggr)  + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow  &amp;\text{(add terms so that}\ bb^T - bb^T = 0\ \text{)} \nonumber \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((a) \mu\mu^T-(b)\mu^T  - \mu(b)^T \mathbf{\ + bb^T - bb^T} + \beta_0\alpha_0{\alpha_0}^T  + \sum_{i=1}^n x_i {x_i}^T \biggr) -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]\\
\rightarrow &amp; \text{(add factor so that}\ \frac{a}{a} = 1\ \text{)} \nonumber \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(\frac{a}{a}\biggl[(a) \mu\mu^T - (b)\mu^T  - \mu(b)^T  bb^T-bb^T \biggr] + \beta_0\alpha_0{\alpha_0}^T  + \sum_{i=1}^n x_i {x_i}^T \biggr) \nonumber \\
&amp;-\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow  &amp;\text{(split exponentials and re-arrange terms)} \nonumber \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((a) \biggl[ \mu\mu^T - \frac{1}{(a)}(b)\mu^T  - \frac{1}{(a)}\mu(b)^T + \frac{1}{(a)}bb^T \biggr] \biggr) \biggr] \times \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(- \frac{1}{(a)}bb^T + \beta_0\alpha_0{\alpha_0}^T + \sum_{i=1}^n x_i {x_i}^T  \biggr) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]\\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((a) \biggl( \mu - \frac{(b)}{(a)} \biggr)\biggl( \mu - \frac{(b)}{(a)} \biggr)^T  \biggr) \biggr] \times \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(- \frac{1}{(a)}bb^T + \beta_0\alpha_0{\alpha_0}^T +  \sum_{i=1}^n x_i {x_i}^T \biggr)  + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\rightarrow &amp; \text{(substitute placeholder)} \nonumber \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber  \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((\beta_0 + n) \biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)\biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)^T  \biggr) \biggr] \times \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(- \frac{(\alpha_0\beta_0 + n\bar{x})(\alpha_0\beta_0 + n\bar{x})^T }{(\beta_0 + n)} + \beta_0\alpha_0{\alpha_0}^T  +  \sum_{i=1}^n x_i {x_i}^T \biggr) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span>
</p>
<p>Let us extract terms from the second exponential equation to simplify.</p>
<p>First, add terms so that <span class="math inline">\(- n\bar{x}\bar{x}^T + n\bar{x}\bar{x}^T = 0\)</span>, and then simplify (a.l.a sum of squares).</p>

<p><span class="math display">\[\begin{align}
&amp;\rightarrow  \sum_{i=1}^nx_i {x_i}^T \mathbf{\ - n\bar{x}\bar{x}^T +  n\bar{x}\bar{x}^T}  \\
&amp;\rightarrow  \sum_{i=1}^n\biggl(x_i {x_i}^T - \bar{x}\bar{x}^T\biggr) + \mathbf{n\bar{x}\bar{x}^T} \\
&amp;\rightarrow  \sum_{i=1}^n\biggl(x_i {x_i}^T - \bar{x}{x_i}^T - x_i\bar{x}^T + \bar{x}\bar{x}^T \biggr) +  \mathbf{\ n\bar{x}\bar{x}^T} \\
&amp;\rightarrow \sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T + n\bar{x}\bar{x}^T
\end{align}\]</span>
</p>
<p>Let <strong>S</strong> = <span class="math inline">\(\sum_{i=1}^n (x_i - \bar{x})(x_i - \bar{x})^T\)</span>.</p>
<p>Therefore, we get:</p>

<p><span class="math display">\[\begin{align}
P{}&amp;(\mu,\Lambda|X) \propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((\beta_0 + n) \biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)\biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)^T  \biggr) \biggr] \times \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(- \frac{(\alpha_0\beta_0 + n\bar{x})(\alpha_0\beta_0 + n\bar{x})^T }{(\beta_0 + n)} + \beta_0\alpha_0{\alpha_0}^T  +  \mathbf{n\bar{x}\bar{x}^T + S} \biggr) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]
\end{align}\]</span>
</p>
<p>Second, add a factor so that <span class="math inline">\(\frac{(\beta_0 + n)}{(\beta_0 + n)} = 1\)</span>, and then expand the first three terms of the second exponential equation.</p>

<p><span class="math display">\[\begin{align}
&amp;\rightarrow \biggl(- \frac{(\alpha_0\beta_0 + n\bar{x})(\alpha_0\beta_0 + n\bar{x})^T }{(\beta_0 + n)} + \beta_0\alpha_0{\alpha_0}^T  + n \bar{x}\bar{x}^T\biggr) \\
&amp;\rightarrow - \frac{(\alpha_0\beta_0 + n\bar{x})(\alpha_0\beta_0 + n\bar{x})^T }{(\beta_0 + n)} + \beta_0\alpha_0{\alpha_0}^T \frac{(\beta_0 + n)}{(\beta_0 + n)} + n \bar{x}\bar{x}^T \frac{(\beta_0 + n)}{(\beta_0 + n)}  \\
&amp;\rightarrow \frac{-(\alpha_0\beta_0 + n\bar{x})(\alpha_0\beta_0 + n\bar{x})^T  + 
     \beta_0^2\alpha_0{\alpha_0}^T  +  n \beta_0\alpha_0{\alpha_0}^T + n \beta_0 \bar{x}\bar{x}^T + n^2 \bar{x}\bar{x}^T
      }{(\beta_0 + n)}  \\
&amp;\rightarrow \frac{ - (\beta_0^2\alpha_0\alpha_0^T + n\beta_0\alpha_0\bar{x}T + n\beta_0\bar{x}\alpha_0^T + n^2\bar{x}\bar{x}^T ) +  \beta_0^2\alpha_0{\alpha_0}^T  +  n \beta_0\alpha_0{\alpha_0}^T +n \beta_0 \bar{x}\bar{x}^T +n^2 \bar{x}\bar{x}^T }{(\beta_0 + n)}\\
&amp;\rightarrow \frac{ - n\beta_0\alpha_0\bar{x}^T - n\beta_0\bar{x}{\alpha_0}^T +  n \beta_0\alpha_0{\alpha_0}^T +  n \beta_0 \bar{x}\bar{x}^T }{(\beta_0 + n)}\\
&amp;\rightarrow \frac{ n\beta_0(\bar{x}\bar{x}^T - \alpha_0\bar{x}^T - \bar{x}{\alpha_0}^T + \alpha_0{\alpha_0}^T   ) }{(\beta_0 + n)}\\
&amp;\rightarrow \frac{ n\beta_0}{(\beta_0 + n)}(\bar{x} - \alpha_0)(\bar{x} - \alpha_0  )^T 
\end{align}\]</span>
</p>
<p>Finally, we get the following <strong>gaussian-wishart</strong> equation:</p>

<p><span class="math display">\[\begin{align}
P(\mu,\Lambda|X) {}&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}} \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl((\beta_0 + n) \biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)\biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)^T  \biggr) \biggr] \times \nonumber \\
&amp;exp\biggl[-\frac{1}{2}\Lambda\biggl(  \frac{ n\beta_0}{(\beta_0 + n)}(\bar{x} - \alpha_0)( \bar{x} - \alpha_0 )^T + S \biggr) + -\frac{1}{2}tr(\Sigma_0^{-1}\Lambda) \biggr]\\
\nonumber \\
&amp;\propto    |\Lambda|^{\frac{\nu_0+n-p}{2}}   \nonumber \\
&amp;\underbrace{ exp\biggl[-\frac{(\beta_0 + n) }{2}\biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr)^T\Lambda\biggl( \mu - \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)} \biggr) \biggr]}_\text{gaussian} \times \nonumber \\
&amp;\underbrace{exp\biggl[-\frac{1}{2}tr\biggl(  \frac{ n\beta_0}{(\beta_0 + n)}(\bar{x} - \alpha_0)( \bar{x} - \alpha_0 )^T + S  + \Sigma_0^{-1} \biggr) \Lambda \biggr]}_\text{wishart}
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Normal Wishart posterior</strong> distribution, it becomes apparent that the parameters correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 &amp;=  \frac{(\alpha_0\beta_0 + n\bar{x})}{(\beta_0 + n)}\\
\beta_1 &amp;= \beta_0 + n \\
\nu_1 {}&amp;= \nu_0 + n \\
\Sigma_1 &amp;=  \frac{ n\beta_0}{(\beta_0 + n)}(\bar{x} - \alpha_0)( \bar{x} - \alpha_0 )^T + S  + \Sigma_0^{-1} 
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong> using a <strong>gaussian-wishart</strong> joint distribution:</p>

<p><span class="math display">\[\begin{align}
\mu, \Lambda|x\sim \ \mathcal{NW}_p(\alpha_1, \beta_1, \nu_1,\Sigma_1) 
= \mathcal{N}_p(\alpha_1, \beta_1)\times\mathcal{W}_p(\nu_1, \Sigma_1)
\end{align}\]</span>
</p>
</div>
<div id="normal-inverse-wishart-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.7</span> Normal-Inverse Wishart Conjugacy <a href="conjugacy.html#normal-inverse-wishart-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor an <strong>Inverse Wishart</strong> distribution for <strong>posterior</strong> given that <strong>Inverse Wishart density</strong> is <strong>conjugate prior</strong> for a <strong>positive-definite</strong> <strong>covariance matrix</strong> parameter, <span class="math inline">\(\Lambda\)</span>, of a <strong>Multivariate Normal Likelihood</strong>; the same as we use <strong>Inverse Gamma density</strong> as <strong>conjugate prior</strong> for the variance parameter, <span class="math inline">\(\sigma^2\)</span>, of a <strong>Univariate Normal Likelihood</strong>. For a notation, we use the following:</p>

<p><span class="math display">\[\begin{align}
P(\Lambda|x) \propto P(\Lambda) \times Lik(\Lambda|x)
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following <strong>p-variate normal</strong> distribution:</p>

<p><span class="math display">\[\begin{align}
X|\mu,\Lambda \sim \mathcal{N}_p(X; \mu, \Lambda)  \ \ \ \ \ \text{where}\ \mu\ \text{is known and }\ \Lambda \text{ is unknown } 
\end{align}\]</span>
</p>
<p>Recall the below structure. See Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>) under <strong>Wishart Distribution</strong> Section as reference:</p>

<p><span class="math display">\[\begin{align}
X = \left[\begin{array}{rrrr}x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p}\\ x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np} \end{array}\right]_\text{(nxp)}
 \ \ \ \ 
\mu = \left[\begin{array}{c}\bar{x}_1 \\ \bar{x}_2 \\ \vdots \\ \bar{x}_p \end{array}\right]
= \left[\begin{array}{c}\mu_1 \\ \mu_2 \\ \vdots \\ \mu_p \end{array}\right]_\text{(1xp)} \label{eqn:eqnnumber311}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\Lambda = \left[\begin{array}{rrrr}\sigma^2_{1} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1p}\\ \sigma_{21} &amp; \sigma^2_{2} &amp; \cdots &amp; \sigma_{2p} \\ \vdots &amp;  \vdots &amp;  \ddots &amp;  \vdots \\ \sigma_{p1} &amp; \sigma_{p2} &amp; \cdots &amp; \sigma^2_{p} \end{array}\right]_\text{(pxp)} \label{eqn:eqnnumber312}
\end{align}\]</span>
</p>
<p>For <strong>multivariate joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\mu, \Lambda|x_1,...,x_n) &amp;\equiv P_X(x_1,...,x_n|\mu,\Lambda) \\
&amp;= \prod_{i=1}^n\frac{|\Lambda|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}} exp\left[-\frac{1}{2}(x_i - \mu)^T\Lambda^{-1}(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Lambda|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n exp\left[-\frac{1}{2}\sum_{i=1}^n (x_i - \mu)^T\Lambda^{-1}(x_i - \mu)\right]\\
&amp;= \left(\frac{|\Lambda|^{-\frac{1}{2}}}{(2\pi)^{\frac{p}{2}}}\right)^n 
exp\left[-\frac{1}{2}tr(\Lambda^{-1} \Sigma)\right]
\end{align}\]</span>
</p>
<p>It helps to be aware of the following mathematical manipulation (derivation not included):</p>

<p><span class="math display">\[\begin{align}
\sum_{i=1}^n (x_i - \mu)^T\Lambda^{-1}(x_i - \mu) = tr( \Lambda^{-1} \Sigma)\ \ \ \ \  where\ \ \ \Sigma = \sum_{i=1}^n (x_i - \mu)(x_i - \mu)^T
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose an <strong>Inverse Wishart</strong> distribution for the <span class="math inline">\(\Lambda\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\Lambda \sim IW_p(v_0, \Sigma_0)\ \ \ \ \ \text{where}\ v_0\ and\ \Sigma_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\Lambda) = P(\Lambda; \nu_0, \Sigma_0) 
    =  \frac{ |\Lambda|^{-\frac{\nu_0 +p+1}{2}}  exp\left[-\frac{1}{2}tr(\Lambda^{-1}\Sigma_0)\right]}
    {2^{\frac{\nu_0 p}{2}}|\Sigma_0|^{-\frac{\nu_0}{2}}\ \Gamma_p\left(\frac{\nu_0}{2}\right)}
\end{align}\]</span>
</p>
<p>Recall description of <strong>Inverse Wishart</strong> notation in Chapter <strong>5</strong> (<strong>Numerical Probability and Distribution</strong>) under <strong>Wishart Distribution</strong> Section.</p>
<p>For <strong>posterior</strong>, we want to arrive at an <strong>Inverse Wishart density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\Lambda|x \sim \ \mathcal{W}^{-1}_p(v_1,\Sigma_1)
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\Lambda\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution (e.g., expressed by the exponents).</p>
<p>For an <strong>Inverse Wishart posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
\mathcal{W}(\Lambda|x_{1:n}) {}&amp;\propto  \underbrace{  \frac{ |\Lambda|^{-\frac{\nu_0 +p+1}{2}}  exp\left[-\frac{1}{2}tr(\Lambda^{-1}\Sigma_0)\right]}
    {2^{\frac{\nu_0 p}{2}}|\Sigma_0|^{-\frac{\nu_0}{2}}\ \Gamma_p\left(\frac{\nu_0}{2}\right)} }_\text{wishart prior} \times 
\underbrace{  \left(\frac{|\Lambda|^{\frac{1}{2}}}{\sqrt{2\pi}}\right)^n exp\left[\Lambda^{-1}\Sigma  \right]}_\text{normal likelihood} \\
\rightarrow  &amp;\text{(drop constants)} \nonumber \\
&amp;\propto |\Lambda|^{-\frac{(\nu_0+n)+p+1}{2}}  exp\left[-\frac{1}{2}tr(\Lambda^{-1}\Sigma_0)\right] \times   exp\left[\Lambda^{-1} \Sigma \right] \\
&amp;\propto |\Lambda|^{-\frac{(\nu_0+n)+ p + 1}{2}} exp\left[-\frac{1}{2}  tr\left( \Lambda^{-1}\Sigma_0 + \Sigma^{-1}  \Sigma \right) \right] \\
&amp;\propto |\Lambda|^{-\frac{(\nu_0+n) + p+1}{2}} exp\left[-\frac{1}{2}  tr\left( \Lambda^{-1} ( \Sigma_0 +  \Sigma)\right)  \right] 
\end{align}\]</span>
</p>
<p>Notice that given an <strong>Inverse Wishart posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(v_1\)</span> and <span class="math inline">\(\Sigma_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\Sigma_1 = (\Sigma_0 + \Sigma)^{-1}\ \ \ \ \ \ \ \
\nu_1 = \nu_0 + n 
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\Lambda|x_1,...,x_n \sim \ \mathcal{IW}_p(\nu_1,\Sigma_1)\ \ \ \rightarrow \mathcal{W}^{-1}_p(\nu_0 + n\ ,\ (\Sigma_0 + \Sigma)^{-1})
\end{align}\]</span>
</p>
<p>For <strong>MAP</strong> we can use the following equation:</p>

<p><span class="math display">\[\begin{align}
\Lambda_{(map)} = \frac{\Lambda_1}{\nu_1 + p + 1}
\end{align}\]</span>
</p>
</div>
<div id="normal-lkj-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.8</span> Normal-LKJ Conjugacy <a href="conjugacy.html#normal-lkj-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor an <strong>LKJ</strong> distribution for <strong>posterior</strong>.</p>
<p>We leave readers to investigate this conjugacy as a modern alternative to the <strong>normal Wishart</strong> conjugacy. While <strong>normal Wishart conjugacy</strong> operates on the covariance of <strong>MVN</strong>, <strong>normal LKJ conjugacy</strong> operates on correlation.</p>
</div>
<div id="binomial-beta-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.9</span> Binomial-Beta Conjugacy <a href="conjugacy.html#binomial-beta-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Beta density</strong> distribution for <strong>posterior</strong> given that <strong>Beta density</strong> is <strong>conjugate prior</strong> for a <strong>Binomial likelihood</strong>. For a notation, let us use the following:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x) \propto P(\rho) \times Lik(n,\rho|x)
\ \ \ \ \ \ \text{for } 0 \le \rho\ \le 1 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|n,\rho \sim Binom(n,\rho)
\end{align}\]</span>
</p>
<p>For <strong>marginal</strong> (Bernoulli) likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(n,\rho|x) \equiv P(X=x|n,\rho) = \binom{n}{x}\rho^x(1 - \rho)^{n-x}
\end{align}\]</span>
</p>
<p>For <strong>joint</strong> (Binomial) likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(n_1,...,n_m,\rho|x_1,...,x_m) \equiv P_X(x_1,...,x_m|n_1,...,n_m,\rho) = \prod_{i=1}^m \binom{n_i}{x_i}\rho^{x_i}(1 - \rho)^{n_i - x_i}
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Beta</strong> distribution for the <span class="math inline">\(\rho\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\rho \sim Beta(\alpha_0,\beta_0)\ \ \ \ \ \text{where}\ \alpha_0\ \text{and}\ \beta_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\rho) = P(\rho; \alpha_0, \beta_0) = \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1}
\end{align}\]</span>
</p>
<p>For <strong>posterior</strong>, we want to arrive at a <strong>Beta density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\rho|x \sim Beta(\alpha_1,\beta_1) 
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\rho\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>
<p>For a <strong>Beta posterior</strong> with <strong>marginal</strong> (Bernoulli) likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x)  {}&amp;\propto  
\underbrace{ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1}}_\text{beta prior} \times
\underbrace{ \binom{n}{x}\rho^x(1 - \rho)^{n-x} }_\text{binomial likelihood} \\
\rightarrow  &amp;\text{(drop constants)} \nonumber \\
P(\rho|x)  &amp;\propto \left(\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1} \right) \times  \left( \rho^x(1 - \rho)^{n-x} \right) \\
&amp;\propto \left(\rho^{(\alpha_0 + x) - 1}(1 - \rho)^{(\beta_0 + n - x) - 1} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Beta posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 = \alpha_0 + x
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = \beta_0 + (n - x)
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x \sim Beta(\alpha_1, \beta_1)\ \ \ \rightarrow Beta(\alpha_0 + x,\ \beta_0 + (n - x))
\end{align}\]</span>
</p>
<p>For a <strong>Beta posterior</strong> with <strong>joint</strong> (Binomial) likelihood:</p>

<p><span class="math display">\[\begin{align}
P(n_{1:m},\rho|x_1,...,x_m)  {}&amp;\propto  
\underbrace{ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1} 
 }_\text{beta prior} \times
\underbrace{ \prod_{i=1}^m  \binom{n_i}{x_i}\rho^{x_i}(1 - \rho)^{n-x_i} }_\text{binomial likelihood} \\
\rightarrow &amp; \text{(drop constants)} \nonumber \\
P(n_{1:m},\rho|x_1,...,x_m)  &amp;\propto \left(\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1} \right) \times  \left( \rho^{\sum_{i=1}^m x_i}(1 - \rho)^{\sum_{i=1}^m n_i-\sum_{i=1}^m x_i} \right) \\
&amp;\propto \left(\rho^{(\sum_{i=1}^m x_i + \alpha_0) - 1}(1 - \rho)^{(\sum_{i=1}^m n_i + \beta_0 - \sum_{i=1}^m x_i) - 1} \right) \\
&amp;\propto \left(\rho^{(\alpha_0 + m \bar{x} ) - 1}(1 - \rho)^{( \beta_0 + ( m \bar{n} - m \bar{x})) - 1} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Beta posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 = \alpha_0 + m \bar{x} 
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = \beta_0 + ( m \bar{n} - m \bar{x})
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x_1,...,x_n \sim Beta(\alpha_1, \beta_1)\ \ \ \rightarrow Beta(\alpha_0 + m \bar{x},\ \beta_0 + ( m \bar{n} - m \bar{x}))
\end{align}\]</span>
</p>
</div>
<div id="geometric-beta-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.10</span> Geometric-Beta Conjugacy <a href="conjugacy.html#geometric-beta-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Beta density</strong> distribution for <strong>posterior</strong> given that <strong>Beta density</strong> is <strong>conjugate prior</strong> for a <strong>Geometric likelihood</strong>. For a notation, let us use the following:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x) \propto P(\rho) \times Lik(\rho|x) 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\rho \sim Geo(\rho)
\end{align}\]</span>
</p>
<p>For <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\rho|x) \equiv P(X=x|\rho) = \rho(1 - \rho)^{x - 1}
\end{align}\]</span>
</p>
<p>For <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\rho|x_1,...,x_n) \equiv P_X(x_1,...,x_n|\rho) = \prod_{i=1}^n \rho(1 - \rho)^{x - 1}
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Beta</strong> distribution for the <span class="math inline">\(\rho\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\rho \sim Beta(\alpha_0,\beta_0)\ \ \ \ \ \text{where}\ \alpha_0\ \text{and}\ \beta_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\rho)  = P(\rho; \alpha_0, \beta_0) = \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1}
\end{align}\]</span>
</p>
<p>For <strong>posterior</strong>, we want to arrive at a <strong>Beta density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\rho|x \sim Beta(\alpha_1,\beta_1) 
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\rho\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>
<p>For a <strong>Beta posterior</strong> with <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x)  {}&amp;\propto  
\underbrace{ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1}}_\text{beta prior} \times \underbrace{ \rho(1 - \rho)^{x - 1} }_\text{geometric likelihood} \\
\rightarrow &amp; \text{(drop constants)} \nonumber \\
P(\rho|x)  &amp;\propto \left(\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1} \right) \times  \left( \rho (1 - \rho)^{x - 1} \right) \\
&amp;\propto \left(\rho^{(1 + \alpha_0) - 1}(1 - \rho)^{(x + \beta_0 - 1) - 1} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Beta posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_1 = 1 + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = x + \beta_0  - 1
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x \sim Beta(\alpha_1, \beta_1)\ \ \ \rightarrow Beta(1 + \alpha_0,\ x + \beta_0 - 1 )
\end{align}\]</span>
</p>
<p>For a <strong>Beta posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x_1,...,x_n)  {}&amp;\propto  
\underbrace{ \frac{1}{\mathcal{B}(\alpha_0,\beta_0)}\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1}}_\text{beta prior} \times \underbrace{ \prod_{i=1}^n \rho(1 - \rho)^{x_i - 1}}_\text{geometric likelihood} \\
\rightarrow &amp; \text{(drop constants)} \nonumber \\
P(\rho|x_1,...,x_n)  &amp;\propto \left(\rho^{\alpha_0-1}(1 - \rho)^{\beta_0 - 1} \right) \times  \left( \rho^n(1 - \rho)^{\sum_{i=1}^n x_i - n} \right) \\
&amp;\propto \left(\rho^{(n + \alpha_0) - 1}(1 - \rho)^{(\sum_{i=1}^n x_i + \beta_0 - n ) - 1} \right) \\
&amp;\propto \left(\rho^{(n + \alpha_0) - 1}(1 - \rho)^{(n \bar{x} + \beta_0 - n) - 1} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Beta posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>
<p><span class="math display">\[\begin{align}
\alpha_1 = n + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = n \bar{x} + \beta_0 - n
\end{align}\]</span></p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x_1,...,x_n \sim Beta(\alpha_1, \beta_1)\ \ \ \rightarrow Beta(n + \alpha_0,\ n \bar{x} + \beta_0 - n)
\end{align}\]</span>
</p>
</div>
<div id="poisson-gamma-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.11</span> Poisson-Gamma Conjugacy <a href="conjugacy.html#poisson-gamma-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Gamma density</strong> distribution for <strong>posterior</strong> given that <strong>Gamma density</strong> is <strong>conjugate prior</strong> for a <strong>Poisson likelihood</strong>. For a notation, let us use the following:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x) \propto P(\lambda) \times Lik(\lambda|x) 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\lambda \sim Pois(\lambda)
\end{align}\]</span>
</p>
<p>For <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\lambda|x) \equiv P(X=x|\lambda) = \frac{1}{x!} \lambda^x e^{-\lambda}
\end{align}\]</span>
</p>
<p>For <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\lambda|x_1,...,x_n) \equiv P_X(x_1,...,x_n|\lambda) = \prod_{i=1}^n \frac{1}{x!} \lambda^x e^{-\lambda}
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Gamma</strong> distribution for the <span class="math inline">\(\lambda\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\lambda \sim Gamma(\alpha_0,\beta_0)\ \ \ \ \ \text{where}\ \alpha_0\ \text{and}\ \beta_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\lambda) = 
P(\lambda; \alpha_0, \beta_0) = \frac{1}{\beta_0^{\alpha_0} \Gamma(\alpha_0)} \lambda^{\alpha_0-1} e^ {-\frac{\lambda}{\beta_0}} = 
\frac{\beta^{\alpha_0}}{\Gamma(\alpha_0)}\lambda^{\alpha_0-1}e^{-\beta_0 \lambda}
\end{align}\]</span>
</p>
<p>For a <strong>posterior</strong>, we want to arrive at a <strong>Gamma density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\lambda|x \sim Gamma(\alpha_1, \beta_1) 
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\lambda\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>
<p>For a <strong>Gamma posterior</strong> with <strong>marginal</strong> (Bernoulli) likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x)  {}&amp;\propto  
\underbrace{ \left(\frac{\beta_0^{\alpha_0}}{ \Gamma(\alpha_0)}\right)
\lambda^{\alpha_0-1} e^ {-\beta_0 \lambda} }_\text{gamma prior} \times
\underbrace{ \left(\frac{1}{x!}\right) \lambda^x e^{-\lambda} }_\text{poisson likelihood} \\
\rightarrow  &amp;\text{(drop constants)} \nonumber \\
P(\lambda|x)  &amp;\propto \left( \lambda^{\alpha_0-1} e^ {-\beta_0 \lambda}\right) \times \left( \lambda^x e^{-\lambda} \right) \\
&amp;\propto \left(\lambda^{x + \alpha_0-1} e^ {-\lambda - \beta_0 \lambda} \right) \\
&amp;\propto \left(\lambda^{(x + \alpha_0)-1} e^ {-(1 + \beta_0) \lambda} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>
<p><span class="math display">\[\begin{align}
\alpha_1 = x + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = 1 + \beta_0
\end{align}\]</span></p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\lambda|x \sim Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow Gamma(x + \alpha_0, 1 + \beta_0)
\end{align}\]</span>
</p>
<p>For a <strong>Gamma posterior</strong> with <strong>joint</strong> (Binomial) likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x)  {}&amp;\propto  
\underbrace{ \left(\frac{\beta_0^{\alpha_0}}{ \Gamma(\alpha_0)}\right)
\lambda^{\alpha_0-1} e^ {-\beta_0 \lambda}}_\text{gamma prior}  \times
\underbrace{ \prod_{i=1}^n \left(\frac{1}{x_i!}\right) \lambda^{x_i} e^{-\lambda} }_\text{poisson likelihood}\\
\rightarrow &amp; \text{(drop constants)} \nonumber \\
P(\lambda|x)  &amp;\propto \left( \lambda^{\alpha_0-1} e^ {- \beta_0 \lambda}\right) \times \left( \lambda^{n \bar{x}} e^{-n \lambda} \right),\ \ \ \ \ n \bar{x} = \sum_{i=1}^n x_i \\
&amp;\propto \left( \lambda^{(n \bar{x} +\alpha_0)-1} e^ {-(n + \beta_0) \lambda}\right)
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>
<p><span class="math display">\[\begin{align}
\alpha_1 = n \bar{x} + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = n + \beta_0
\end{align}\]</span></p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\lambda|x_1,...,x_n \sim Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow Gamma(n \bar{x} + \alpha_0, n + \beta_0)
\end{align}\]</span>
</p>
</div>
<div id="exponential-gamma-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.12</span> Exponential-Gamma Conjugacy <a href="conjugacy.html#exponential-gamma-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Gamma density</strong> distribution for <strong>posterior</strong> given that <strong>Gamma density</strong> is <strong>conjugate prior</strong> for an <strong>Exponential likelihood</strong>. For a notation, let us use the following:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x) \propto P(\lambda) \times Lik(\lambda|x) 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\lambda \sim Expo(\lambda)
\end{align}\]</span>
</p>
<p>For <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\lambda|x) \equiv P(X=x|\lambda) = \lambda e^{-\lambda x}
\end{align}\]</span>
</p>
<p>For <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
Lik_X(\lambda|x_1,...,x_n) \equiv P_X(x_1,...,x_n|\lambda) = \prod_{i=1}^n \lambda e^{-\lambda x}
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Gamma</strong> distribution for the <span class="math inline">\(\lambda\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\lambda \sim Gamma(\alpha_0,\beta_0)\ \ \ \ \ \text{where}\ \alpha_0\ \text{and}\ \beta_0 \text{ are known } \mathbf{hyperparameters}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\lambda) = P(\lambda; \alpha_0, \beta_0) = \frac{1}{\beta_0^{\alpha_0} \Gamma(\alpha_0)} \lambda^{\alpha_0-1} e^ {-\frac{\lambda}{\beta_0}} = 
\frac{\beta^{\alpha_0}}{\Gamma(\alpha_0)}\lambda^{\alpha_0-1}e^{-\beta_0 \lambda}
\end{align}\]</span>
</p>
<p>For a <strong>posterior</strong>, we want to arrive at a <strong>Gamma density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\lambda|x \sim Gamma(\alpha_1, \beta_1) 
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\lambda\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>
<p>For a <strong>Gamma posterior</strong> with <strong>marginal-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x)  {}&amp;\propto  
\underbrace{ \left(\frac{\beta_0^{\alpha_0}}{ \Gamma(\alpha_0)}\right)
\lambda^{\alpha_0-1} e^ {-\beta_0 \lambda}}_\text{gamma prior} \times
\underbrace{ \lambda  e^{-\lambda x} }_\text{exponential likelihood}\\
\rightarrow &amp; \text{(drop constants)} \nonumber \\
P(\lambda|x)  &amp;\propto \left( \lambda^{\alpha_0-1} e^ {-\beta_0 \lambda}\right) \times \left( \lambda  e^{-\lambda x} \right) \\
&amp;\propto \left(\lambda^{(1 + \alpha_0) - 1} e^ {-(1 + \beta_0) \lambda} \right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>
<p><span class="math display">\[\begin{align}
\alpha_1 = 1 + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = 1 + \beta_0
\end{align}\]</span></p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\lambda|x \sim Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow Gamma(1 + \alpha_0, 1 + \beta_0)
\end{align}\]</span>
</p>
<p>For a <strong>Gamma posterior</strong> with <strong>joint-density</strong> likelihood:</p>

<p><span class="math display">\[\begin{align}
P(\lambda|x)  {}&amp;\propto  
\underbrace{ \left(\frac{\beta_0^{\alpha_0}}{ \Gamma(\alpha_0)}\right)
\lambda^{\alpha_0-1} e^ {-\beta_0 \lambda} }_\text{gamma prior} \times
\underbrace{ \prod_{i=1}^n \lambda e^{-\lambda x_i}}_\text{exponential likelihood} \\
\rightarrow &amp;\text{(drop constants)} \nonumber \\
P(\lambda|x)  &amp;\propto \left( \lambda^{\alpha_0-1} e^ {- \beta_0 \lambda}\right) \times \left( \lambda^{n}  e^{-\lambda n \bar{x}} \right),\ \ \ \ \ n \bar{x} = \sum_{i=1}^n x_i \\
&amp;\propto \left( \lambda^{(n  +\alpha_0)-1} e^ {-(n \bar{x} + \beta_0) \lambda}\right)
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Gamma posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\alpha_1\)</span> and <span class="math inline">\(\beta_1\)</span> correspond to the following:</p>
<p><span class="math display">\[\begin{align}
\alpha_1 = n  + \alpha_0
\ \ \ \ \ \ \ \ \ \ \ \ \ \
\beta_1 = n \bar{x} + \beta_0
\end{align}\]</span></p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\lambda|x_1,...,x_n \sim Gamma(\alpha_1, \beta_1)\ \ \ \rightarrow Gamma(n  + \alpha_0, n \bar{x}+ \beta_0)
\end{align}\]</span>
</p>
</div>
<div id="multinomial-dirichlet-conjugacy" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.13</span> Multinomial-Dirichlet Conjugacy <a href="conjugacy.html#multinomial-dirichlet-conjugacy" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The idea is to be able to tailor a <strong>Dirichlet density</strong> distribution for <strong>posterior</strong> given that <strong>Dirichlet density</strong> is <strong>conjugate prior</strong> for a <strong>Multinomial (or Categorical) likelihood</strong>. For a notation, let us use the following:</p>

<p><span class="math display">\[\begin{align}
P(\rho|x) \propto P(\rho) \times Lik(n, \rho|x) 
\end{align}\]</span>
</p>
<p>For <strong>likelihood</strong>, we have the following distribution:</p>

<p><span class="math display">\[\begin{align}
x|\rho\sim Multi(n,\rho)
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
Lik_{X_{1:k}}(n, \rho|x_1,...,x_k)  \equiv P_{X_{1:k}}(x_1,...,x_k|n, \rho) {}&amp;= 
\frac{n!}{x_1! \times ... \times x_k!} 
 \rho_1^{x_1} \times ... \times \rho_k^{x_k}\\
&amp;= \frac{n!}{\prod_{i=1}^k x_i!} \prod_{i=1}^k \rho_i^{x_i}
\end{align}\]</span>
</p>
<p>For <strong>prior</strong>, we choose a <strong>Dirichlet</strong> distribution for the <span class="math inline">\(\lambda\)</span> parameter:</p>

<p><span class="math display">\[\begin{align}
\rho \sim Dir(\alpha_{0_{1:k}})\ \ \ \ \ \text{where}\ \alpha_{0_{1:k}}\ \text{ are known } \mathbf{hyperparameters} 
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
\mathcal{\pi}(\rho) = P(\rho; \alpha_{0_{1:k}}) = \frac{1}{\mathcal{B}(\alpha_{0_{1:k}})} \prod_{i=1}^k \rho_i^{\alpha_{0_i}-1},\ \ \ \ \ where\ \alpha_{0_{1:k}} = (\alpha_{0_1},...,\alpha_{0_k})
\end{align}\]</span>
</p>
<p>and where:</p>

<p><span class="math display">\[\begin{align}
\mathcal{B}(\alpha_{0_{1:k}}) = 
\frac{\Gamma(\alpha_{0_1})\times ...\times \Gamma(\alpha_{0_k})}{\Gamma(\alpha_{0_1} + ... + \alpha_{0_k})}
\end{align}\]</span>
</p>
<p>For a <strong>posterior</strong>, we want to arrive at a <strong>Dirichlet density</strong> given an observed data:</p>

<p><span class="math display">\[\begin{align}
\rho|x_1,...,x_k \sim Dir(\alpha_{1_{1:k}}),\ \ \ \ \ where\ \alpha_{1_{1:k}} = (\alpha_{1_1},...,\alpha_{1_k})
\end{align}\]</span>
</p>
<p>However, let us first derive the <strong>posterior density</strong> with respect to <span class="math inline">\(\rho\)</span> by dropping the constants that do not affect the shape or proportionality of the distribution.</p>

<p><span class="math display">\[\begin{align}
P(\rho|x_1,...,x_k)  {}&amp;\propto 
\underbrace { \frac{1}{\mathcal{B}(\alpha_0)} \prod_{i=1}^k \rho_i^{\alpha_{0_i}-1} }_\text{dirichlet prior} \times
\underbrace{ \frac{n!}{\prod_{i=1}^k x_i!} \prod_{i=1}^k \rho_i^{x_i} }_\text{multinomial likelihood}\\
\rightarrow &amp;\text{(drop constants)} \nonumber \\
P(\rho|x_1,...,x_k)  &amp;\propto 
\left( \prod_{i=1}^k \rho_i^{\alpha_{0_i}-1}  \right) \times 
\left( \prod_{i=1}^k \rho_i^{x_i} \right) \\
&amp;\propto \left( \prod_{i=1}^k \rho_i^{(x_i + \alpha_{0_i}) - 1}\right) 
\end{align}\]</span>
</p>
<p>Notice that given a <strong>Dirichlet posterior</strong> distribution, it becomes apparent that the parameters <span class="math inline">\(\rho_1\)</span> corresponds to the following:</p>

<p><span class="math display">\[\begin{align}
\alpha_{1_{1:k}} = \sum_{i=1}^k (x_i + \alpha_{0_i})
\end{align}\]</span>
</p>
<p>Therefore, we arrive at the following reparameterized <strong>posterior distribution</strong>:</p>

<p><span class="math display">\[\begin{align}
\rho|x_1,...,x_k \sim Dir(\alpha_{1_{1:k}})\ \ \ \rightarrow Dir(\sum_{i=1}^k (x_i + \alpha_{0_i}))
\end{align}\]</span>
</p>
<p>Application of this conjugacy becomes apparent in <strong>Variational Bayes</strong> section.</p>
</div>
<div id="hyperparameters" class="section level3 hasAnchor">
<h3><span class="header-section-number">3.4.14</span> Hyperparameters <a href="conjugacy.html#hyperparameters" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>A <strong>proper prior</strong> is as essentially fitting as the chosen quantities of its <strong>hyperparameters</strong>. Depending on its <strong>hyperparameters</strong>, a <strong>prior</strong> may stretch within the spectrum of being <strong>weakly informed</strong> and <strong>well informed</strong>.</p>
<p>The idea that a <strong>prior</strong> does not have information may be contested because there is always truly information about a <strong>prior</strong>. Therefore, we can say that <strong>non-informative prior</strong> is a misnomer. From that perspective, we also use <strong>weakly informative prior</strong>. Morever, we can use other terms such as <strong>vague prior</strong>, <strong>objective prior</strong>, <strong>imprecise prior</strong>, and <strong>insufficient prior</strong>.</p>
<p>One way to complement and at the same time improve our <strong>weakly informative prior knowledge</strong> is to keep accumulating evidence and to keep seeking prior knowledge from domain experts.</p>
<p>Consequently, our goal is to achieve a <strong>well-behaved proper</strong> posterior. However, with only an initial piece of evidence to use on hand and minimal expert knowledge, we can use a <strong>uniform or flat</strong> prior instead. A <strong>uniform prior</strong> is improper in that it integrates infinitely; however, it leads to a proper posterior when combined with <strong>likelihood</strong>.</p>
<p>An example set of hyperparameter quantities used for <strong>Normal</strong> flat prior is:</p>
<p><span class="math display">\[\begin{align}
\mu \sim U(a = 0, b = 1) = Beta(\alpha_0 = 1, \beta_0  = 1)\ \ \text{(weakly-informed prior)}
\end{align}\]</span></p>
<p>An example set of hyperparameter quantities used for <strong>Beta</strong> flat prior is:</p>
<p><span class="math display">\[\begin{align}
\rho \sim Beta\left(\alpha_0 = \frac{1}{2}, \beta_0 = \frac{1}{2}\right)\ \ \text{(weakly-informed prior)},
\ \ \ \ \alpha_0 &gt; 0,\ \beta_0 &gt; 0 
\end{align}\]</span></p>
<p>An example set of hyperparameter quantities used for <strong>Inverse Gamma</strong> flat prior is:</p>
<p><span class="math display">\[\begin{align}
\sigma^2 \sim Inv. Gamma \left(\alpha_0 =  1, \beta_0 =  1 \right) \ \ \text{(weakly-informed prior)}
\end{align}\]</span></p>
<p>An example set of hyperparameter quantities used for <strong>Gamma</strong> flat prior is:</p>
<p><span class="math display">\[\begin{align}
\lambda \sim Gamma\left(\alpha_0 =  \frac{1}{2}, \beta_0 =  \frac{1}{2}\right)\ \ \text{(weakly-informed prior)}
\end{align}\]</span></p>
<p>Figure  shows graph of the <strong>prior distribution</strong> with corresponding <strong>hyperparameter</strong>.</p>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:flatprior"></span>
<img src="DS_files/figure-html/flatprior-1.png" alt="Flat Prior (Hyperparameter)" width="70%" />
<p class="caption">
Figure 3.8: Flat Prior (Hyperparameter)
</p>
</div>

<p>We leave readers to investigate the topic around <strong>Jeffreyâs prior</strong> for the chosen <strong>hyperparameters</strong> above.</p>
<p>Also, we illustrate the use of <strong>flat prior</strong> and <strong>hyperparameter</strong> in the <strong>Bayesian modeling</strong> section.</p>
<p>In summary, Table  lists conjugate priors for a few distribution families corresponding to their <strong>likelihood</strong> distributions. That includes the corresponding <strong>hyperparameters</strong>.</p>

<table>
<caption><span id="tab:conjugacy">Table 3.3: </span>Conjugate Prior-Posterior</caption>
<colgroup>
<col width="13%" />
<col width="40%" />
<col width="17%" />
<col width="28%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Conjugacy Family</th>
<th align="left">General Notation</th>
<th align="left">Prior HyperParameter</th>
<th align="left">Likelihood</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\(\mu&amp;#124;\sigma^2 \sim N(\mu,\sigma^2)\)</span></td>
<td align="left"><span class="math inline">\(\mu_0, \sigma^2_0\)</span></td>
<td align="left">Normal (unknown <span class="math inline">\(\mu\)</span>)</td>
</tr>
<tr class="even">
<td align="left">Inverse Gamma</td>
<td align="left"><span class="math inline">\(\sigma^2&amp;#124;\mu \sim Inv.\Gamma(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Normal (unknown <span class="math inline">\(\sigma^2\)</span>)</td>
</tr>
<tr class="odd">
<td align="left">Normal</td>
<td align="left"><span class="math inline">\(\mu,\sigma^2 \sim N(\mu,\sigma^2)\)</span></td>
<td align="left"><span class="math inline">\(\mu_0, \sigma^2_0\)</span></td>
<td align="left">Normal (unknown <span class="math inline">\(\mu\)</span>,<span class="math inline">\(\sigma^2\)</span>)</td>
</tr>
<tr class="even">
<td align="left">Gamma</td>
<td align="left"><span class="math inline">\(\mu,\sigma^2 \sim \Gamma(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Normal (unknown <span class="math inline">\(\mu\)</span>,<span class="math inline">\(\sigma^2\)</span>)</td>
</tr>
<tr class="odd">
<td align="left">Beta</td>
<td align="left"><span class="math inline">\(\rho \sim Beta(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Binomial</td>
</tr>
<tr class="even">
<td align="left">Beta</td>
<td align="left"><span class="math inline">\(\rho \sim Beta(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Geometric</td>
</tr>
<tr class="odd">
<td align="left">Gamma</td>
<td align="left"><span class="math inline">\(\lambda \sim \Gamma(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Poisson</td>
</tr>
<tr class="even">
<td align="left">Gamma</td>
<td align="left"><span class="math inline">\(\lambda \sim \Gamma(\alpha,\beta)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0, \beta_0\)</span></td>
<td align="left">Exponential</td>
</tr>
<tr class="odd">
<td align="left">Dirichlet</td>
<td align="left"><span class="math inline">\(\rho \sim Dir(\alpha)\)</span></td>
<td align="left"><span class="math inline">\(\alpha_0\)</span></td>
<td align="left">Multinomial</td>
</tr>
</tbody>
</table>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bayes-theorem.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="information-theory.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "sepia",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["DS.pdf", "DS.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
